Datum;Titel;Text;Link;Quelle;Autor
  29. Oktober 2020;Neue Trends im Natural Language Processing – Wie NLP massentauglich wird;"NLP (engl. für Natural Language Processing) beschreibt allgemein das computergestützte Verarbeiten von menschlicher Sprache. Dies umfasst neben der geschriebenen auch die gesprochene Sprache. Die Ziele, die mit NLP verfolgt werden, lassen sich in zwei übergeordnete Kategorien einordnen: Verstehen von Sprache und Erzeugen von Sprache. Die technische Herausforderung ist bei beiden Zielen, unstrukturierte Informationen in Form von Texten in ein Format zu transferieren, das maschinell verarbeitet werden kann. Das bedeutet konkret, dass Texte in einem Zahlenformat repräsentiert werden müssen, welches der Computer verstehen kann. Noch vor wenigen Jahren war dies nur für einige wenige Technologiekonzerne möglich. Dabei hatten diese Firmen drei entscheidende Vorteile:Zugang zu riesigen Mengen an unstrukturierten TextdatenFachleute, die in der Lage sind, cutting-edge Technologien zu entwickelnRechenkapazitäten, um die Menge an unstrukturierten Daten verarbeiten zu könnenIn diesem Artikel zeigen wir verschiedene Anwendungsgebiete von NLP und erklären, wie sich innerhalb nur weniger Jahre die Markteintrittsbarrieren so weit gesenkt haben, dass heute jedes Unternehmen NLP für eigene Lösungen nutzen kann.InhaltsverzeichnisIn welchen Bereichen kann NLP eingesetzt werden?SequenzklassifizierungFrage-Antwort ModelleGenerierung von TextenErkennen von SatzgliedernZusammenfassungenÜbersetzungenWie haben sich die NLP Modelle entwickelt?Wieso ist NLP so relevant geworden?Wie ist NLP so gut anwendbar geworden?Einstiegsbarriere: DatenverfügbarkeitEinstiegsbarriere: RechenleistungEinstiegsbarriere: TalentakquisitionWelche Herausforderung bestehen weiterhin?ModellDatenRechenleistungWas können wir in der Zukunft von NLP erwarten?ZusammenfassungIn welchen Bereichen kann NLP eingesetzt werden?Computergestützte Sprachverarbeitung ist ein sehr abstrakter Begriff. Verständlicher wird der Begriff, wenn man ihn in die Anwendungsgebiete herunterbricht. Dabei wird für jeden Teilbereich ein anderes, spezialisiertes Modell für die Sprachverarbeitung angewandt. Zu beachten ist dabei, dass Aufgaben, die einem Menschen eher leichtfallen, wie z.B. das Erkennen von Emotionen, auch für eine Maschine im NLP Bereich tendenziell eher einfach sind.. Andersherum sind auch kompliziertere Aufgaben, wie das Übersetzen von Texten, für den Computer eine tendenziell schwieriger zu lösende Aufgabe. Die sechs wichtigsten Anwendungen von NLP und die damit lösbaren Businessprobleme werden nachfolgend beleuchtet.SequenzklassifizierungDas klassische Beispiel für NLP ist die Sequenzklassifizierung. Ziel ist es, Textsequenzen einer von mehreren vorher definierten Klassen zuzuordnen. Ein Beispiel für Klassen sind Emotionen (freudig, wütend, erheitert, usw.). Dem Computer wird ein Text vorgelegt und er muss selbstständig entscheiden, welche Emotion der Autor mit seinem Text ausdrücken wollte. Weitere Beispiele sind das Zuordnen eines Texts zu einem bekannten Autor oder das Klassifizieren von Dokumentarten.Bei der Sequenzklassifizierung ist zu beachten, dass eine Sequenz aus einem Text beliebiger Länge bestehen kann. Eine Sequenz kann aus einem einzigen Wort (Sequenz von Buchstaben), einem Satz, einem Paragraphen, oder aber aus einem kompletten Dokument bestehen. Ein Beispiel für eine kürzere Sequenz wäre eine Urlaubsbewertung.Anwendungsbeispiel 1:Ein Reiseportal möchte spezifisch Kunden mit einer negativen Urlaubserfahrung in einer Marketingkampagne ansprechen. Dazu werden vorhandene Kundenbewertungen in drei Klassen unterteilen – Positiv, Neutral und Negativ. Jede Bewertung wird automatisch einer dieser Klassen zugeordnet.Wobei längere Dokumente beispielsweise Postsendungen beliebiger Art sein könnten.Anwendungsbeispiel 2:Die Logistik-Abteilung einer internationalen Firma prozessiert verschiedene Dokumentarten in unterschiedlichen Teams. Dazu werden aktuell Postsendungen manuell selektiert und sortiert. Zukünftig werden diese automatisch einer Kategorie zuordnen. Als Kategorien könnten Eingehende Rechnungen, Lieferscheine sowie andere Anfragen definiert werdenFrage-Antwort ModelleBei Frage-Antwort Problemen wird dem Computer eine möglichst große Anzahl an Textkorpora zur Verfügung gestellt. Ziel ist es, auf Fragen, die von einem Person verfasst wurden, eine inhaltlich korrekte Antwort zu geben, die auf Informationen der Textkorpora basiert. Die Schwierigkeit dieser Aufgabe variiert, je nachdem wie konkret die benötigten Informationen im Text sind. Die einfachste Lösung ist die komplette Extraktion vorhandener Textstellen. Darauf aufbauend kann die extrahierte Information in eine grammatikalisch korrekte Antwort verpackt werden. Am komplexesten zu implementieren sind logische Schlussfolgerungen basierend auf den vorhandenen Informationen.Ein gegebener Text könnte beispielsweise die Struktur eines Firmengeländes beschreiben. Es werden dabei Gebäude A, B und C erwähnt. Eine mögliche Frage könnte lauten „Wie viele Gebäude sind auf dem Firmengelände vorhanden?“ Eine logische Schlussfolgerung des Computers wäre die Erkenntnis, dass das Gelände aus insgesamt 3 Gebäuden besteht, ohne, dass die Zahl an sich erwähnt wurde.Anwendungsbeispiel 3:Eine mittelständische Firma beobachtet seit längerer Zeit einen kontinuierlichen Anstieg an Kundenanfragen. Bei vielen dieser Anfragen handelt es sich um Auskunftsanfragen zu Informationen. Die Firma beschließt, einen Chatbot zu entwickeln. Basierend auf internen Supportdokumenten können Kunden nun selbstständig, automatisch Fragen an den Chatbot stellen und beantworten lassen.Generierung von TextenBasierend auf einem gegebenen Text soll möglichst genau das nächste passende Wort vorhergesagt werden. Der so entstandene Text kann wiederum zur Vorhersage von einem weiteren Wort benutzt werden. Dieser Vorgang kann beliebig oft wiederholt werden, um beliebig lange Texte zu erzeugen. Dabei ist es möglich, Texte mit beliebigen Sprachfeinheiten zu generieren. So kann ein bestimmter Akzent oder Dialekt modelliert werden, aber auch eine einfache oder komplexere Sprache verwendet werden, je nach Zielgruppe. Die größte Herausforderung ist, dass Text sowohl inhaltlich als auch sprachlich fehlerfrei sein sollte.Anwendungsbeispiel 4:Ein Hersteller eines Dokumentenverwaltungssystems möchte das Auffinden von Dokumenten einfacher gestalten. In der eingebauten Suchmaske wird der Suchbegriff automatisch mit weiteren passenden Wörtern ergänzt.Erkennen von SatzgliedernBei dem Erkennen von Satzgliedern, auch Named Entity Recognition (NER) ist das Ziel, ein oder mehrere Wörter in einem Satz einer Klasse zuzuordnen. Als mögliche Satzglieder können grammatikalische Einheiten definiert werden. Dabei wird ein Satz gegliedert nach Subjekt, Prädikat, Objekt usw. Oft werden anstelle von grammatikalischen Einheiten benutzerdefinierte Entitäten gewählt. Oft sind die gesuchten Entitäten z.B. Orte, natürliche Personen, Firmen oder Zeiten. Wenn ein Mensch eine Entscheidung treffen muss, zu welcher Kategorie ein Satzglied gehört, greifen wir automatisch auf Regeln (wie z.B. Grammatikregeln) zurück. Bei NER soll der Computer lernen, auf ähnliche Entscheidungsregeln zurückzugreifen. Allerdings werden diese Regeln nicht explizit vorgegeben, stattdessen muss der Computer sich diese selbstständig erarbeiten.Anwendungsbeispiel 5:Ein Hedgefonds analysiert automatisch die eingereichten vierteljährlichen Berichte welche bei der Börsenaufsicht eingereicht werden. Ziel ist es, automatisch Zusammenfassungen der Geschäftsaktivität der Firma zu erstellen. So besteht die Liste der zu extrahierenden Identitäten aus Geschäftsart, Geschäftsbereich, Geschäftsführer usw.ZusammenfassungenDie Aufgabe, eine Zusammenfassung eines Textes zu erstellen, kann man sich exakt wie die Aufgabenstellung früher im Deutschunterricht vorstellen. Das Ziel ist es, eine möglichst echt wirkende, menschliche Zusammenfassung mit allen relevanten Inhalten zu erstellen. Dabei müssen selbstverständlich geltende Rechtschreib- und Grammatikregeln eingehalten werden. Die Herausforderung dabei ist es, dem Computer beizubringen wichtige und relevante Inhalte von unwichtigen Inhalten zu trennen.Anwendungsbeispiel 6:Eine Onlinenachrichtenagentur hat durch Analyse des Nutzungsverhalten der Website herausgefunden das immer weniger Leute die Artikel komplett bis zum Ende durchlesen. Um Lesern das extrahieren von relevanten Informationen zu erleichtern, soll automatisch eine Zusammenfassung für vorhandene und neuen Artikeln erstellt werden. Die Zusammenfassung soll in Länge und Sprachkomplexität abhängig vom Nutzerprofil erstellt werden.ÜbersetzungenBeim Anwendungsfall einer Textübersetzung soll der Text von einer Sprache in eine andere transferieren werden, unter Einhaltung der geltenden Rechtschreib- und Grammatikregeln und ohne den Inhalt zu verändern. Dabei hat der Computer ähnliche Probleme mit dem Übersetzen von Texten wie ein Mensch. Es muss stets die Balance zwischen inhaltlicher und grammatikalischer Korrektheit gehalten werden, ohne sich dabei vom Originaltext zu entfernen.Anwendungsbeispiel 7:Ein national agierender Zulieferbetrieb möchte seinen Absatzmarkt international erweitern. Dazu müssen alle vorhandenen technischen Spezifikationen in die Sprache der Zielmärkte übersetzt werden. Eine besondere Herausforderung besteht in der Übersetzung von technischen, branchenspezifischen Vokabular.Wie haben sich die NLP-Modelle entwickelt?Die Geschichte von NLP-Modellen lässt sich in drei Epochen unterteilen: Naive Modelle, statische Vektormodelle und dynamische Vektormodelle.In den Anfängen von NLP Modellen wurde versucht, den Sinn von Texten durch das Zählen von Wörtern oder Wortpaaren zu ermitteln. Dazu war ein sehr intensives Vorbearbeiten der Texte notwendig. Das eigentliche Rechnen der Modelle, basierend auf den Zählständen, ist (mit heutigen Computern) äußerst schnell zu bewerkstelligen. Allerdings geht durch das Zählen der Wörter jeglicher Kontext verloren.Der nächste Entwicklungsschritt waren statische Vektormodelle. Der Gedanke hinter diesen Modellen ist, dass jedes Wort durch einen Vektor, also eine Zahlenreihe, repräsentiert wird. Diese Zahlenreihen werden meist durch Zuhilfenahme von Deep Learning Modellen berechnet. Sie dienen anschließend als Eingabe für ein weiteres Modell, z.B. wiederum ein Deep Learning Modell, dass die Zahlenreihe nutzt, um damit die eigentliche Aufgabe, z.B. die Klassifikation der Texte, zu lösen. Durch Zuhilfenahme der Vektoren war es möglich, den Kontext von Wörtern besser zu erfassen. D.h. bei dem Berechnen eines Vektors für ein Wort werden andere, dieses Wort umgebende Wörter, mitbetrachtet. Allerdings sind die Vektoren für ein gleich geschriebenes Wort noch identisch, unabhängig von der eigentlichen Bedeutung. Bei dem unten gezeigten Beispiel wäre der Vektor für ‚Bank‘ jeweils der gleiche.Ich sitze auf der Bank. (Bank = Sitzgelegenheit)Ich bringe mein Geld zur Bank. (Bank = Geldhaus)Das berechnen der Vektoren sowie des Modells ist sehr zeit- und rechenintensiv. Allerdings gestaltet sich die Vorhersage, durch den fehlenden Kontext der Vektoren, noch sehr effizient.Die aktuellste Generation von NLP Modellen ist ähnlich der zweiten Generation, allerdings werden nun Vektoren mit Bezug auf den Kontext des Wortes berechnet. Somit würde in dem obenstehenden Beispiel für die Sitzbank ein anderer Vektor berechnet werden als für das Geldhaus. Das macht sowohl die Berechnung des Modells als auch die Vorhersage sehr rechenintensiv.Wieso ist NLP so relevant geworden?Den Beginn der „New-Area of NLP“ hat Google Ende 2018 mit dem sogenannten BERT Modell eingeläutet (hier geht es zum offiziellen GitHub repository). Seitdem erscheinen monatlich Anpassungen und Weiterentwicklungen des Modells von Universitäten, aber auch anderen Firmen wie Facebook und natürlich von Google selbst. Die Mehrheit dieser Modelle steht der breiten Masse kostenfrei zur Verfügung – fast immer ist die Verwendung auch für den kommerziellen Zweck freigegeben.Die Performance dieser neusten Generation von NLP Modellen ist in vielen Bereichen auf Augenhöhe mit, oder bereits über, den Ergebnissen, die von Menschen erzielt werden können.Die Forschung hat Datensätze für verschiedene Aufgaben und Teilbereiche der Sprachverarbeitung entwickelt. Diese Aufgaben wurden zunächst von Menschen gelöst, um einen Referenzwert zu schaffen, der von Computern geschlagen werden soll. Mittlerweile sind NLP Modelle in der Lage, in fast allen Bereichen nahezu menschliche Ergebnisse zu liefern.Zu beachten ist, dass die Datensätze sehr generalistisch sind. Jeder dieser Benchmark-Datensätze versucht in seinem Teilbereich eine möglichst große Abdeckung zu erlangen, um eine möglichste gute, generelle Aussage über die Performance zu treffen. Businessprobleme hingegen sind meist deutlich konkreter. So kann es sein, dass ein Modell sehr gut in der Lage ist, die generelle Stimmung von Texten aller Art zu erfassen und somit eine gute, hohe Bewertung in diesem Bereich erlangt.Ein Businessproblem könnte sein, dass die Stimmung von Kundenbeiträgen in sozialen Netzwerken oder von eingehenden Emails von Kundenbeschwerden zu bewerten. Von einem menschlichen Standpunkt aus gesehen, sind beide Aufgaben sehr ähnlich. Für eine Maschine kann es einen großen Unterscheid machen, ob es sich um kurze, informellen Texte, wie Beiträge aus sozialen Medien, oder um längere, formelle Texte, wie E-Mails, handelt. Eine Evaluation der Modelle auf das Business-Problem ist unerlässlich.Wie ist NLP so gut anwendbar geworden?Bis vor wenigen Jahren gab es in der Entwicklung von künstlicher Intelligenz, und speziell für den Teilbereich NLP, drei grundlegende Probleme, die die Entwicklung und Adaption dieser Modelle erschwert hat. Die Probleme hingen mit der Ressourcenallokation in den drei Bereichen Daten, Rechenleistung und Humankapital zusammen. Alle drei Punkte wurden durch das Vortrainieren von Modellen deutlich entschärft.Die großen, relevanten Firmen in der NLP-Modellentwicklung investieren in diese Ressourcen und stellen im Anschluss diese vortrainierten Modelle meist kostenfrei zur Verfügung. Die Modelle sind bereits sehr gut im generellen Verstehen von Texten, aber lassen meist noch Raum für Verbesserungen bei spezifischen Problemen. Der Löwenanteil an Ressourcen wird jedoch für den ersten Teil, dem generalistischen Repräsentieren von Text, benötigt. Diese vortrainierten Modelle lassen sich nun mit Verhältnis wenig Aufwand auf bestimmte Businessprobleme feinabstimmen. Durch das Feinabstimmten können exzellente Ergebnisse mit minimalstem Aufwand und geringen Kosten erzielt werden.Einstiegsbarriere: DatenverfügbarkeitMit zunehmender Komplexität der Modelle wächst der Bedarf an Daten, welche für das Training benötigt werden, exponentiell. Die Performance dieser Modelle kommt durch das Betrachten des Kontextes eines Wortes zustande. Folglich ist es notwendig, dass ein Modell möglichst viele Wörter in möglichst vielen Kombinationen sieht. Durch das Internet gibt es Zugriff auf sehr große Textsammlungen. So wurde das vorhin erwähnte BERT Modell auf diversen Büchern mit zusammen etwa 800 Millionen Wörtern sowie der kompletten englischsprachigen Plattform Wikipedia mit etwa 2,5 Milliarden Wörtern trainiert.Einstiegsbarriere: RechenleistungAus dem immer weiter ansteigenden Bedarf an Daten sowie der ansteigenden Modellkomplexität ergibt sich ein immer größerer Bedarf an Rechenleistung. Dabei lassen sich einige Punkte beobachten. Die Leistung von Computern steigt jedes Jahr massiv an, man spricht von einer Verdoppelung der Rechenleistung etwa alle zwei Jahre. Gleichzeitig wird Rechenleistung immer günstiger. Seit dem Siegeszug von Cloudanbietern wurde der Zugang zu Rechenleistung demokratisiert. Extrem performante und spezialisierte Computercluster sind nun nicht mehr nur für große Firmen, sondern für jedermann bei einer minutengenauen Abrechnung verfügbar.Einstiegsbarriere: TalentakquisitionIn den Anfängen von KI war es notwendig, entweder selbst ein kompetitives Entwicklungsteam innerhalb der eigenen Organisation aufzubauen oder die komplette Entwicklung von spezialisierten Firmen einzukaufen. Dadurch war es erforderlich, finanziell sehr stark in Vorleistung zu gehen, um nach einer oft mehrjährigen Entwicklungszeit ein fertiges KI-Produkt in Betrieb nehmen zu können. Oft sind solche Projekte fehlgeschlagen oder haben einen zu geringen Mehrwert gestiftet. Finanzielle Investitionen mit solch einem Risikoprofil waren meist nur für große multinationale Unternehmen möglich. Die meisten der neu entwickelten NLP-Modelle sind heutzutage frei zugänglich, auch für kommerzielle Zwecke. Somit ist es möglich, innerhalb von Wochen, anstatt Monaten oder Jahren, einen Machbarkeitsnachweis zu bekommen. Die Einführungszeit eines kompletten Produktes hat sich von Jahren auf Monate reduziert. Iterationen in der Produktentwicklung sind nun sehr schnell möglich, mit einem geringen initialen Investment.Welche Herausforderung bestehen weiterhin?Viele der ursprünglich vorhandenen Probleme wurden entschärft oder komplett gelöst. Diese Entwicklungen haben vor allem den Zeitaufwand bis zum Abschluss einer Machbarkeitsstudie extrem verkürzt.ModellAktuell sind vortrainierte Modelle von einer Vielzahl an Firmen und Anbietern vorhanden. Diese werden laufend weiterentwickelt und oft wird nach einigen Monaten eine neuere, verbesserte Version veröffentlicht. Außerdem werden von ein- und demselben Modell zum gleichen Zeitpunkt mehrere Versionen veröffentlicht. Diese unterscheiden sich oftmals in der Komplexität oder der Sprache.Es ist extrem wichtig, in der Anfangsphase eines NLP-Projekts Modelle zu sondieren und zu evaluieren. Grundsätzlich lässt sich die Performance in zwei verschiedene Dimensionen aufteilen: Die Qualität der Ergebnisse und die Ausführungsgeschwindigkeit.Die Qualität von Modellergebnisse zu beurteilen, ist je nach Aufgabe häufig anspruchsvoll. Bei der Klassifizierung von Emotionen lässt sich meistens eindeutig bestimmen, ob das Modell richtig oder falsch lag. Das Beurteilen von Zusammenfassungen ist deutlich schwieriger. Es ist sehr wichtig in der Anfangsphase eines Projektes ein Gütemaß zu bestimmen, das technisch umsetzbar ist, aber auch das Businessproblem widerspiegelt.Die zweite Dimension der Modellperformance ist die Ausführungsgeschwindigkeit. Diese umfasst sowohl die benötigte Zeit für das Training als auch für die Vorhersage. Dabei ist es sehr wichtig, frühzeitig den Anspruch an das Modell mit allen Projektpartnern abzustimmen. So hat ein Modell, welches live und innerhalb von Millisekunden Anfragen beantworten muss, andere Eigenschaften als ein Modell, welches einmal pro Tag über Nacht Ergebnisse berechnet.DatenDas Thema Daten ist generell bei KI und vor allem im Bereich NLP ein zweischneidiges Schwert. Einerseits sind Daten generell vorhanden und es existieren Computersysteme, die in der Lage sind, diese zu verarbeiten. Durch das Vortrainieren von Modellen wird uns ein Großteil der Arbeit mit Daten abgenommen. Andererseits sind vortrainierte Modelle immer darauf ausgelegt, möglichst gut in einer Vielzahl von Aufgaben zu funktionieren. Oft liefern die vortrainieren Modelle ohne eine Feinabstimmung bereits gute, aber keine herausragenden Ergebnisse. Die Feinabstimmung erfolgt meist in zwei Dimensionen. Das Modell muss zunächst auf Spracheigenheiten und Feinheiten angepasst werden – das kann spezielles Vokabular sein, aber auch Slang und Dialekt. So gibt es einen großen Unterschied zwischen Beiträgen aus den Sozialen Medien und Anleitungen für produktionstechnische Verfahren. Die zweite Dimension bezieht sich auf die eigentliche Aufgabenstellung. Um eine herausragende Leistung zu erlangen, müssen Modelle immer auf das Businessproblem abgestimmt werden. Ein Modell, das übersetzen kann, unterscheidet sich erheblich von einem Modell, das Emotionen klassifizieren kann. Für diese Feinabstimmung werden Texte/Daten benötigt, welche auf die Zielsprache und das Zielproblem abgestimmt sind. Die Texte müssen aufbereitet und dem Modell zugeführt werden. Je nach Komplexität und Qualität der Daten kann dies noch immer ein aufwendiger Prozess sein.RechenleistungDer Fakt, dass Computer immer besser werden und Rechenleistung immer günstiger wird, ist einer der Hauptgründe für die Adaption von KI. Wie bereits mehrfach erwähnt, ist es durch vortrainierte Modelle nicht mehr notwendig, den Löwenanteil der Rechenleistung selbst zu erbringen. Computerleistung wird lediglich für die Datenverarbeitung und die Feinabstimmung der Modelle benötigt. Dies ist ein Bruchteil von der Rechenleistung, die für das komplette Training von Anfang bis Ende benötigt wird. Dennoch ist es meist mehr als ein Standard-Computer in einer angemessenen Zeit bewerkstelligen könnte. Daher wird für die Feinabstimmung meist auf Cloudcomputing zurückgegriffen. Cloudressourcen werden in der Regel minutengenau abgerechnet und sind daher sehr kostengünstig. Allerdings unterscheidet sich der Ablauf eines Trainings mittels Cloudcomputing deutlich von einem Training in einem Standard-Rechenzentrum, weswegen in diesem Bereich Wissen in der eigenen Organisation entweder aufgebaut oder von externen Dienstleistern eingekauft werden muss.Was können wir in der Zukunft von NLP erwarten?Vom gesamten Bereich der künstlichen Intelligenz wird an NLP aktuell am aktivsten geforscht. Es sind in den nächsten Monaten und Jahren noch einige interessante Entwicklungen zu erwarten und derzeit zeichnen sich zwei Entwicklungen mit sehr interessanten praktischen Implikationen ab.Wir erwarten kurz- bis mittelfristig den praktischen Einsatz von sogenannten Zero-Shot Modellen. Diese Modelle sind für ein gewisses Aufgabengebiet wie Sequenzklassifikation trainiert. Die Neuheit ist, dass diese Modelle sehr gute Ergebnisse liefern, ohne jeweils domänenspezifische Daten gesehen zu haben. Somit entwickeln diese Modelle eine Art „generelle“ Intelligenz. Damit wird die Feinabstimmung von Modellen deutlich einfacher oder entfällt komplett.Der nächste zu erwartende Schritt sind sogenannte General Purpose Modelle. Diese Art von Modellen können jedes Aufgabengebiet auf ungesehenen Daten lösen, wodurch die komplette Feinabstimmung entfallen würde. Erste Versuche mit diesen Modellen scheinen sehr gute Ergebnisse zu liefern, allerdings sind die Modelle extrem groß und stellen sehr hohe Anforderungen an die Rechenleistung. Damit ist die Inbetriebnahme dieser Modelle aktuell äußerst schwer und teuer. Noch gibt es nahezu keine praktischen Einsatzgebiete. Wir erwarten in den nächsten Jahren deutliche Sprünge hinsichtlich Praktikabilität und Performance.ZusammenfassungDie jüngsten Entwicklungen in dem Bereich der Sprachprozessierung sind beeindruckend und schnell zugleich. Den Startschuss der neusten Entwicklungen gab Google mit der Veröffentlichung des BERT-Modells vor knapp zwei Jahren und seitdem werden in einem Wochenrhythmus neue Modelle von Firmen und Universitäten rund um die Welt veröffentlicht. Diese Modelle verbessern oft die Ergebnisse von vorhandenen Problemen oder ermöglichen es, vorhandene Ressourcen effizienter einzusetzen. Probleme welche vor zwei Jahren noch als unlösbar galten sind nun oft sehr gut lösbar und auch im Hinblick auf einzusetzende Ressourcen und Entwicklungszeit erschwinglich. Die notwendige Zeit, eine Machbarkeitsstudie zu erstellen, wurde extrem verkürzt.Über den AutorDominique LadeI am a data scientist at STATWORX. I enjoy the whole journey from defining the business problem till the final product is delivered to the client. If it is in the context of financial data – even better..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/neue-trends-im-natural-language-processing-wie-nlp-massentauglich-wird/;Statworx;  Dominique Lade
  27. Oktober 2020;Whitepaper: KI-Weiterbildung in Unternehmen;"Management SummaryDie Weiterbildung von Mitarbeitenden im Bereich Künstlicher Intelligenz (KI) ist eine der zentralen Aufgaben für eine erfolgreiche Etablierung von KI im eigenen Unternehmen. Zwar spielen auch andere Faktoren, wie eine unternehmensweite KI-Strategie oder eine breite Unterstützung von KI-Projekten durch die verschiedenen Führungsebenen, eine wichtige Rolle für den Erfolg von KI-Initiativen, doch ohne ein unternehmensweites Grundverständnis der Thematik sowie ein geschultes Data Science bzw. KI-Team, kann keine Initiative langfristig etabliert werden.Akzeptanz von KI durch fundierte WeiterbildungVielen Mitarbeitenden in Organisationen fehlt aktuell noch die Vorstellungskraft in Bezug auf künstliche Intelligenz. Noch sind die Chancen, die durch die Anwendung der Technologie entstehen für viele Personen zu abstrakt. Durch konkrete Erfahrungen in der Anwendung von KI-Lösungen sowie durch gezielte Schulungsmaßnahmen kann das Bewusstsein für die Chancen von KI in der Belegschaft geschärft werden. Gleichzeitig ist eine notwendig, dass Führungskräfte ihre KI-Strategie klar kommunizieren und einen offenen Umgang mit Ängsten oder gar Vorurteilen pflegen, damit Mitarbeitende KI im Arbeitsalltag akzeptieren und sich bestenfalls sogar dafür begeistern. Um dieses Ziel zu erreichen, empfiehlt sich ein unternehmensweites Aufklärungs- bzw. Weiterbildungsprogramm zu initiieren, das die Belegschaft systematisch und zielgruppengerecht auf die zu erwartende Zukunft vorbereitet. Nur so lässt sich ein organisationsweites Grundverständnis sowie die mit KI einhergehenden Chancen und Risiken aufbauen, was letztlich zu der notwendigen Akzeptanz der Technologie in der Belegschaft führt.Die drei Kernrollen des KI-TeamsNeben einem unternehmensweiten, eher grundsätzlich ausgerichteten Weiterbildungsprogramm müssen Data Science bzw. KI-Teams individueller geschult werden. Für die drei Kernrollen des KI-Teams – Data Scientist, Business Analyst und Data Engineer – müssen, neben technischen Weiterbildungen, eine Vielzahl flankierender Themen wie agiles, kollaboratives Arbeiten oder rechtliche und ethische Aspekte von KI vermittelt werden. Weiterhin gibt es spezifisches Wissen und Fähigkeiten, die für jede der drei Rollen separat notwendig sein können.In diesem Whitepaper erläutern wir die Aufgaben &amp; Pflichten der einzelnen Rollen eines Data Science und KI-Teams. Darauf aufbauen zeigen wir auf, weshalb interne Weiterbildung und Unterstützung externer Dienstleister der Schlüssel zum Aufbau einer erfolgreichen KI-Strategie darstellen.Sie haben Interesse am Thema KI-Weiterbildung?Hier können Sie sich das gesamte Whitepaper „KI-Weiterbildung in Unternehmen“ kostenfrei herunterladen.Über den AutorAlexander NiltopI am a statistician at STATWORX and don't just want to understand the fascinating world of statistics but explain it easily, too. I hope, it worked!.e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/whitepaper-ki-weiterbildung-in-unternehmen/;Statworx;  Alexander Niltop
  22. Oktober 2020;5 Technologien;"Management SummaryIn diesem Beitrag sollen fünf Technologien vorgestellt werden, die jeder Data Engineer für seine tägliche Arbeit kennen und beherrschen sollte. Aufgeführt werden Spark als Data Processing Tool im Big Data Umfeld, Kafka als Streaming Platform, Airflow und Serverless-Architektur zur Koordinierung bzw. Orchestrierung. Zuvor werden Stellenwert und Rolle von SQL (Structured Query Language) und relationalen Datenbanken besprochen.Trotz des stetigen Wandels hat sich SQL eine Sonderposition herausgearbeitet und findet sich auch in neuen Entwicklungen als Schnittstelle wieder. Mitnichten ist es aber so, dass wie vor Jahrzehnten ein stabiles Wissen von dieser Query-Sprache ausreicht, um das Gros der Datenarbeiten bewältigen zu können. Dafür ist zum einen die Datenlandschaft, zum anderen die verarbeiteten Daten mittlerweile zu heterogen geworden. Darüber hinaus reicht es in vielen Fällen nicht mehr aus, Daten nach einem zeitlich fixierten Schema zu aktualisieren oder die zu prozessierenden Datenmengen sind schlicht nicht mehr von klassischen ETL (Extract-Transform-Load) Prozessen in Datenbanken zu schultern.Data Streaming Plattformen wie Apache Kafka sind eine Antwort auf die Real-time-Problematik, dabei hat es die Möglichkeit mit den Anforderungen zu skalieren und für Ausfallsicherheit zu sorgen. Bezüglich der Möglichkeit des Prozessierens enorm großer Datenmengen ist Spark das Tool der Wahl. Ebenso wie Kafka kann es mit den Anforderungen skalieren. Ferner bietet es eine großzügige Auswahl an Implementierungssprachen an.Auf die Heterogenität der eingesetzten Data Stores und Prozessierungsframeworks kann man mit zwei Ansätzen antworten: eine zentrale (globale) Orchestrierung, die die verschiedenen Komponenten bedienen kann, wie es Airflow auf beeindruckende Weise tut; eine dezentrale (lokale) Lösung, die nur auf spezifische Signale einzelner Komponenten reagiert, wie es der Serverless-Ansatz vorsieht. Alle gängigen Cloud-Anbieter können einen solchen Service vorweisen. Zusammenfassend findet man hier also eine Mischung an Tools als Antwort auf die aktuellen Fragen im Data Engineering.IntroDie Datenlandschaft war in den letzten Jahrzehnten geprägt von einer zunehmenden Dynamik. Galten bis zum Jahrtausendwechsel Data Warehouses und relationale Datenbank-Managementsysteme (RDBMS) noch als der Goldstandard für die Datenhaltung und -aufbereitung, so durchbrach vor allem die Verbreitung und die Dynamik des Internets diese Alleinstellung. Zum einen vervielfachte sich damit die Datenmenge, zum anderen galt nun das Interesse vermehrt auch semi-strukturierten und unstrukturierteren Daten. Man war also im Big Data Zeitalter angekommen. Der nächste Schub wurde durch die von mobilen Endgeräten und Sensoren generierten Datenflüsse (Stichwort: Internet of Things) verursacht. Es ging nun nicht mehr nur darum, den erneut enorm gestiegenen Datenaufwand zu bewältigen, sondern Ereignisse aus vielen Datenpunkten in Echtzeit zu erkennen und darauf reagieren zu können. Schlussendlich hat das Cloud Computing zusätzliches Potential für Datenanalysen und -verarbeitung geborgen, indem Infrastruktur nun in vielerlei Hinsicht günstig zur Verfügung steht und diverse Technologien mit geringer Einstiegsschwelle genutzt werden können. Aus der anfänglich monolithischen Welt der Datenbanken ist eine heterogene und dynamische Datenlandschaft geworden, die Data Engineers benötigt, um den Anforderungen gerecht zu werden.Anhand von fünf Technologien soll mit diesem Artikel eine Orientierung zur Lösung aktueller Problemstellungen im Bereich „Data Engineering“ gegeben werden. In den Gebieten „Batch Processing“ und „Streaming“ werden die etablierten Technologien Apache Spark und Apache Kafka vorgestellt. Mit Apache Airflow und der Serverless-Technologie Lambda werden zwei unterschiedliche Konzepte präsentiert, um Abläufe zu steuern. Zuletzt und entgegen der Stoßrichtung der Einleitung findet sich auch ein Kapitel über SQL und relationale Datenbanken.* Unter Data Engineering ist hier ein sehr enger Begriff verstanden, nämlich die Tätigkeit, eine kontinuierliche Datenversorgung und -aufbereitung herzustellen.SQL und relationale DatenbanksystemeSQL ist eine Abkürzung für structured query language und ist fester Bestandteil von relationalen Datenbanksystemen. Entgegen des allgemeinen Tones und trotz diverser (Weiter-)Entwicklungen von NoSQL1-Lösungen, spielen SQL-Systeme weiterhin eine tragende Rolle in der modernen Datenarchitektur. Das zeigt auch eine Umfrage aus dem Jahr 2019, die sich folgendermaßen zusammenfassen lässt: Der Einsatz von NoSQL-Systemen ist in den allermeisten Fällen keine Abkehr, sondern ein Zusatz zu bestehenden Systemen, die auf SQL setzen.Auch in modernen Frameworks spiegelt sich die Popularität von SQL wider: So zeugen die Entwicklung in Spark und Kafka (siehe beide in den nachstehenden Absätzen) vom Stellenwert, der SQL eingeräumt wird. Schließlich gibt es Tools, die SQL-Queries für NoSQL-Systeme kompatibel machen, hier beispielhaft zu nennen Apache Drill.Ein Grund für die Popularität von SQL, neben seinen Vorzügen als einfach und semantisch an die englische Sprache angelehnt, liegt in seiner weiten Verbreitung. Auch außerhalb des Datenbankmilieus finden sich Analysten und Beschäftigte im Reporting, die SQL beherrschen.SELECT      DEPARTMENT,      MANAGER,      COUNT(USER_ID)FROM TBL_EMPLOYEEWHERE IS_EXTERNAL = 1GROUP BY DEPARTMENT, MANAGERKlar lesbar führt diese Query eine Selektion, Filterung und Gruppierung von Daten aus.Relationale Datenbanken sind eng mit SQL verbunden. Diese ausgereifte Technologie sticht vor allem durch Konsistenz und Dauerhaftigkeit hervor. Zudem muss das Tabellenschema vor dem ersten Schreiben definiert sein, was zwar zu Erwartbarkeit und Sicherheit führt, aber auch als aufwendig in der Verwaltung und starr angesehen werden kann. Der Umgang mit Daten, deren Struktur nicht explizit angegeben werden kann oder wechselhaft ist, kann sich also in relationalen Datenbanken als beschwerlich gestalten. Gleiches gilt für komplex-strukturierte Daten, da auch diese in Tabellen und Relationen eingepasst werden müssen, damit sie von der Datenbank effektiv behandelt werden können.Vorzüge von relationalen Datenbanken:Eine seit den 1970er Jahren entwickelte und damit ausgereifte Technologie, die von vielen Experten beherrscht wirdEine starke Typisierung und der Definitionszwang a priori von Tabellenschemata garantieren Erwartbarkeit und SicherheitMit SQL eine weitverbreitete, verständliche Query-SpracheDie Implementierung des ACID-Schemas, das Konsistenz, Sicherheit und Dauerhaftigkeit von Datenständen garantiertRedundanzarmer Speicherverbrauch durch Normalisierung und ReferenzmöglichkeitenSparkFast schon ein Klassiker und nun bereits zur Version 3 gereift, ist Spark der Standard, um sehr große Datenvolumen effizient zu prozessieren. Die Performance steht vor allem auf zwei Säulen: Zum einen werden Verarbeitungsschritte auf eine Schar an Worker-Nodes verteilt, was große Datenmengen parallelisiert bearbeitbar macht. Zum anderen ist es ein intelligentes System, um Zwischenergebnisse im Arbeitsspeicher zu halten, um Berechnungsstrecken abzukürzen und Zugriffszeiten zu verkürzen.Die Bearbeitung durch das Spark-Cluster wird erst dann ausgelöst, wenn die Strecke End-to-End (von den Ausgangsdaten über Transformationen bis zum Endprodukt) definiert ist. Wie angedeutet, versucht Spark die Aufgabenlast möglichst parallelisiert auf den Worker-Nodes auszuführen. Gleich einem Query-Optimizer in einer relationalen Datenbank, sucht Spark einen möglichst performanten Weg, die Definition in einzelne Schritte zu zerlegen und diese wiederum als Tasks an die Worker zu verteilen.Anhand eines einfachen Beispiels soll dieses Schema veranschaulicht werden. Dabei wird angenommen, dass Daten aus einer großen Menge Dateien prozessiert werden, die in einem Textformat vorliegen. Es werden diverse Transformation durchgeführt, um die wichtigen Informationen zu extrahieren. Aus einer zweiten Datenquelle, die ebenfalls in einem rohen Zustand im Textformat vorliegt, sollen weitere Informationen entnommen werden und an das Zwischenprodukt angereichert werden.Die unten dargestellte Abbildung, die Spark intern generiert, kann folgendes beobachtet werden:Die Ausgangsdaten, die als Textdateien vorliegen, werden zu Anbeginn der Stages 8 und 10 eingelesen und anschließend transformiert, was sich in den Aktionen „map“ und „distinct“ widerspiegelt. Zuletzt werden die Ergebnisse mit einer „join“ Aktion (wie man sie auch von SQL kennt) in Stage 10 vereint.Ein weiterer Vorteil von Spark ist die Kompatibilität mit verschiedenen Datenformaten. So erlaubt Spark Lese- und Schreibvorgänge in unter anderem JSON, XML, CSV, Avro, kann aber auch ebenso gut mit Datenbankverbindungen umgehen – seien es nun klassische RDBMS oder Big-Data Datenbanken wie Hive. Zwar basiert die Engine auf Java, längst aber hat Python (in Form der PySpark Bibliothek) als primäre Implementierungssprache die mit Abstand weiteste Verbreitung2. Entwicklungen wie die von Koalas, welches die beliebte Datenanalyse-Bibliothek Pandas in Spark integriert, unterstreicht dies zusätzlich. Weitere unterstütze Sprachen sind R, Scala und SQL.Sparks primäre Stärke ist die Abarbeitungen definierter Prozessierungs-Jobs, in allen Facetten und Farben. Für Use-Cases hingegen, die geringe Latenzen auf Datenanfragen erwarten, sollte man besser auf andere Tools setzen, wie etwa Presto, Impala oder Dremio.Vorzüge von Spark:Verarbeitet große Datenmengen (TB-Bereich) sehr effizientSkaliert durch Hinzufügen weiterer Worker Nodes zum ClusterUnterstützt viele Datenformate und Anbindungen zu DatenbankenProzessierungs-Abfragen lassen sich in SQL, R, Scala, Java und Python schreibenSpark gibt es in allen möglichen Ausgaben und Größen:Jupyter Notebooks mit PySpark und lokalem Spark Context (zum Beispiel als Docker-Container)On-premise Lösungen wie Cloudera oder HortonworksNotebook zentrierte Cloud-Lösung von DatabricksCluster in der Cloud: EMR in AWS, HDInsights in Azure, Dataproc in GCPKafkaKafka ist eine verteilte, fehler-tolerante und performante Streaming-Plattform. Wenn es darum geht, hohes Datenaufkommen in Echtzeit zu verarbeiten, kommt häufig Kafka ins Spiel. Die Plattform wird auch für Use Cases genutzt, in denen eine große Anzahl heterogener Systeme mit gegenseitigen Abhängigkeiten auftreten. Anders als bei gewöhnlichen Data Stores, ist ein Stream per Definition nicht erschöpft und Operationen darauf sind ebenso kontinuierlich.Messages werden als Key-Value-Paare behandelt und in Topics gegliedert. Topics wiederum sind in Partitionen unterteilt, die redundant und damit gegen Ausfälle gesichert auf mehreren Nodes gehalten werden. Und schließlich gibt es noch die Teilnehmer an beiden Enden: Konsumenten (Consumers) und Produzenten (Producers), die aus den Topics lesen bzw. in jene schreiben.In der Abbildung sieht man, welche Stelle Kafka im System einnimmt, nämlich als Datenhub: Von unten liefern die Quellsysteme hauptsächlich ins Kafka-System (Producer) und von oben werden die (mithin aufbereiteten) Streams konsumiert (Consumer).Durch die Streaming API und die Entwicklung von ksql, das es ermöglicht, direkt SQL Queries auf Streams auszuführen, steht eine high-level Interaktionsmöglichkeit zur Verfügung, um mit Streams zu interagieren und aus bestehenden neue zu bauen. Dabei sind alle Transformationen vertreten, die man auch aus anderen Processing Frameworks (wie Spark) oder Datenbank-SQL-Dialekten kennt: Filtern, Gruppieren, Joinen, Mappen, etc. Darüber hinaus gibt es aber auch die Streams immanente Zeitachse, die mit Windowing Funktionen behandelt werden kann, also die Fragestellungen danach, wie oft Events pro Zeitabschnitt stattgefunden haben, z.B. Klicks auf Elementen von Web Pages.Die Vorzüge von Kafka nochmal auf einem Blick:Ermöglicht Analysen und Operationen in real timeIntegriert heterogene Systeme zu einem zentralen Datenhub (Producer und Consumer API)Skalierbar und ausfallsicher durch redundantes Halten von PartitionenStellt mit ksqldb eine Technologie bereit, um mit SQL Queries Streams zu transformierenEin gutes Starter-Kit findet man hier. In allen gängigen Cloud Plattformen gibt es auch Services (Amazon MSK, Azure HD Insight, …).AirflowEin weiteres Themenfeld ist die Strukturierung des Ablaufs der einzelnen Datenaufbereitungs- und Datenverarbeitungsschritte. Besonders bei einer hohen Anzahl an beteiligten und einer hohen Heterogenität in den Komponenten ist es höchst ratsam ein Orchestrierungstool einzusetzen, um einen stabilen Ablauf zu garantieren. Für moderne Orchestrierungstools gibt es dabei einen breiten Anforderungskatalog, der in folgende Kategorien eingeteilt werden kann:Kontrollierbarkeit: Die beteiligten Komponenten und Schritte sind wohldefiniert. Die Ausführung soll nachvollziehbar sein, sowohl im Laufen als auch im Nachgang.Integrierbarkeit: Diverse und meist sehr heterogene Komponenten sollen ansprechbar und ihre Ausführung soll beobachtbar sein.Reaktivität: Ergebnisse einzelner Schritte sollen ausgewertet werden können und in den Ablaufprozess einfließen. So soll zum Beispiel definiert sein, was im Fall des Scheiterns eines ganzen Schrittes passieren soll: eine Wiederholung, eine Abweichung vom normalen Prozessweg, ein Ignorieren?Skalierbarkeit: Das Wachsen der Ablaufstruktur kann durch die Erweiterung der Ausführungsinfrastruktur möglichst einfach aufgefangen werden.Apache Airflow erweist sich für diese Anforderungen als optimaler Kandidat. Komplexe Abläufe werden als directed acyclic graph (DAG) beschrieben, die Schritte sind als Knoten durch Bedingungen untereinander verknüpft. Diese DAGs bilden eine ausführbare Einheit, ihre Ausführung lässt sich automatisieren. Die Beschreibung wird rein in Python geschrieben. Das hat den Vorteil, dass die Entwicklung schnell geht und Operatoren gegebenenfalls leicht selbst geschrieben werden können. Zumeist ist das jedoch gar nicht nötig, da in Apache Airflow bereits mit eine Fülle an diversen und gängigen Operatoren implementiert ist. Darunter fallen: Triggern von Spark Jobs, Bereitstellung von Cloud Infrastruktur und das Ausführen von Datenbank-Queries.Im Beispiel unten sieht man einen beispielhaften DAG-Ablauf, bei dem mehrere Stränge parallel ausgeführt werden und zuletzt zusammengeführt werden.In der Abbildung sieht man, wie DAGs mehrere Richtungen nehmen können. Der letzte Schritt wertet die Ergebnisse von beiden Vorgängern aus und führt die Stränge wieder zusammen.Beim Thema Skalierbarkeit liefert Airflow zwei Lösungswege mit: Der Celery Executor verteilt Tasks über einen Message Broker auf die dort registrierten Arbeiterprozesse. Kubernetes lässt sich als Ausführungsplattform über den Kubernetes Executor anschließen.Zuletzt soll noch die Verwaltung von Zugängen zu Fremdsystemen (seien es nun Datenbanken oder Cloud Computing Instanzen) angesprochen werden. Dafür gibt es den sogenannten Secrets Manager, um übersichtlich, zentral und verschlüsselt die Verbindungsschlüssel zu den Systemen hinterlegen kann.Die Vorzüge von Airflow auf einem Blick:Komplexe Abläufe werden in DAGs beschrieben und angesteuertAirflow bietet ein Füllhorn an Operatoren zur Kommunikation mit FremdsystemenDie Ausführung kann skalierbar konfiguriert werden mithilfe von Celery und Kubernetes ExecutorsSecrets von Datenbanken und anderen Systemen werden von Airflow zentral verwaltet und können im Code referenziert werdenEs gibt bereits fertige Docker Images, um Airflow auszuprobieren. Die Installation und Einrichtung ist aber auch recht einfach. Darüber hinaus bietet die Google Cloud auch den verwalteten Airflow Service Cloud Composer an, mit dem man sofort loslegen kann.ServerlessZuletzt soll hier noch ein komplett anderer Ansatz der Ablaufsteuerung angeführt werden, ein Ansatz der sich der Event-Driven Architektur zuordnen lässt. Statt einer zentralen Steuereinheit, die die Abläufe orchestriert, wird hierbei dezentral definiert, mit welchen Abläufen auf spezifische Events reagiert werden soll. Das kann zum Beispiel eine Veränderung im Data Lake sein, ein Infrastrukturevent wie die Bereitstellung einer Recheninstanz oder auch schlicht ein Zeit-Event. Auf Seiten der Funktionsaufrufe sind typische Beispiele das Anstoßen einer Data Ingestion, das Ausführen eines Tabellenupdates oder das Senden einer Benachrichtigung. Durch lokale Konfiguration der Reaktivität, wird hiermit also ein Ablaufkomplex zusammengebaut.In der Abbildung sieht man das Schema der Serverless-Architektur, entnommen von OpenWhisk, einer Open-Source Serverless Plattform. Ein Feed stellt den Eventfluss zur Verfügung. Eine Regel verbindet das Auftreten eines Events (Trigger) mit der Aktion (Action).Eine weitere Besonderheit ist das Serverless-Paradigma. Demnach entfällt der Aufwand für die Bereitstellung von Rechenressourcen zu sorgen. Stattdessen bestimmt die unterliegende Umgebung (also z.B. eine Cloud-Infrastruktur), wie Ressourcen allokiert werden und wo Code ausgeführt wird. Ein Vertreter einer solchen Applikation zur Verwaltung und Deployment von Ressourcen ist Kubernetes. Dem Entwickler wird also die Annehmlichkeit geboten, seinen Fokus ganz auf die Entwicklung zu legen. Zudem ist die Ausführung sprachagnostisch und alle gängigen modernen Programmiersprachen werden unterstützt. Damit können Ausführungen in verschiedenen Sprachen koexistieren und auch miteinander gekoppelt werden. Einige Beispiele für den Einsatz:Eine neue Datei ist im ADL (Azure Data Lake) abgelegt worden und eine Data Ingestion soll angestoßen werden.Ein Tabellen-Inhalt der No-SQL-Datenbank DynamoDB wurde verändert und dies soll ins Archiv geschrieben werden (das entspricht Triggers in Datenbanken).Zeitereignisse: Jede Stunde sollen Daten zum Modelltraining bereitgestellt werden (vergleichbar mit cron jobs in der serverzentrierten Welt).Die Vorteile nochmal auf einem Blick:Keine Aufwände für Infrastrukturbereitstellung, stattdessen Konzentration auf Implementierung der Funktionalität.Event-Driven statt zentraler Orchestrierung.Funktionen können in verschiedenen Programmiersprachen geschrieben werden .Funktionen verschiedener Sprache können koexistieren.Die Anbieter findet man meistens in der Cloud: Lambda in AWS, Cloud Functions in GCP, Azure Functions in der Azure Cloud – ohne das Open-Source-Projekt OpenWhisk verschweigen zu wollen.ZusammenfassungDem Wandel und den Brüchen zum Trotz bildet SQL weiterhin die zentrale Schnittstelle zu Daten, sowohl was Queries als auch die Prozessierung betrifft. Womöglich überdauert SQL sogar noch die Welt der relationalen Datenbanken, aus der sie ursprünglich kam.Andererseits lässt sich ebenso feststellen, dass es längst nicht mehr genügt, SQL zu beherrschen, um modernen Problemstellungen gerecht zu werden, was in der vormaligen Welt der Data Warehouses galt. Verlässt man den bereitgestellten Standard an Funktionalitäten in Spark oder Kafka, den SQL mitliefert, um spezifische Funktionen zu implementieren (man spricht hier von User Defined Functions), so benötigt es zusätzliches Wissen in Programmiersprachen wie Python oder Java. Ebenso erfordert der Umgang mit Airflow oder der Serverless-Technologie, wie in den vorherigen Abschnitten gezeigt wurde, die Beherrschung der Programmiersprache Python – oder im Fall von Serverless der einer anderen modernen Sprache.Wählt man fünf Technologien aus, fallen im Gegenzug andere herunter, die dennoch hier Erwähnung finden sollen:MongoDB als Vertreter von Dokumentdatenbanken (NoSQL), die mit Flexibilität und Skalierbarkeit glänzen können.Apache Cassandra als Vertreter von Key-Value-Datenbanken (NoSQL), das vor allem für hohe Skalierbarkeit steht.Apache Flink als Data Streaming Framework in Ergänzung zu Apache Kafka.Apache Beam als Abstraktionsschicht zur Datenpipeline-Definition, die dann in Apache Flink oder in Apache Spark ausgeführt werden können.1 NoSQL bedeutet in erster Linie die Abkehr vom SQL Paradigma und solche Datenbanken bilden keine homogene Klasse. Stattdessen kann man grob drei Unterklassen definieren: Graphendatenbanken, Dokumentendatenbanken und Key-Value-Datenbanken.2 laut Studie mit einem Anteil von 70% in NotebooksÜber den AutorAndre MünchI am a data engineer at STATWORX. I love to have challenges to setup data structure and compose components to integrate Data Science models into productive environments..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/5-technologien-die-jeder-data-engineer-kennen-sollte/;Statworx;  Andre Münch
  20. Oktober 2020;Whitepaper: 7 Trends für Data Science im Jahr 2021;"Management SummaryIm letzten Jahrzehnt hat sich das Interesse am Bereich Data Science nicht nur enorm erhöht, sondern es sich auch erheblich verändert und entwickelt. Angesichts der neuen technologischen Fortschritte und des stetigen Datenzuwachses erwarten wir, dass sich Data Science auch in Zukunft weiterentwickeln wird. Unsere Recherchen lassen insbesondere die folgenden sieben Data Science Trends für das nächste Jahr erkennen:Beantwortung der Frage nach dem WarumModellierungsunsicherheitAutomatisierung von Data ScienceDemokratisierung von Data ScienceData Science OperationsEthik von Künstlicher IntelligenzSpezialisierung von Data Science RollenDie ersten fünf Themen können in zwei Haupttrends unterteilt werden, und die Punkte Nr. 6 und 7 können als daraus resultierende Trends betrachtet werden.Kausalität und Unsicherheit von ML/KI-ModellenDer erste Haupttrend, der die ersten beiden Punkte umfasst, besteht darin, neue Lösungen mittels Machine Learning (ML) und künstlicher Intelligenz (KI) zu finden, um wichtige Probleme zu lösen, die mit den derzeitigen ML/KI-Modellen noch nicht gelöst werden können: Beantwortung der Frage nach dem Warum (d.h. Modellierung von Ursache und Wirkung) und Modellierung der Unsicherheit. Nur wenn Modelle Kausalität und Unsicherheit erfassen können, können sie wirklich intelligent werden und uns zuverlässige und sichere Entscheidungen und Handlungen liefern. Daher investieren sowohl akademische Institutionen als auch große Technologieunternehmen stark in die Bereiche Machine Learning und KI. Die nächste Herausforderung für Data Scientists wird es sein, diese Modelle zu verstehen und sie in der Praxis anzuwenden, um datengetriebenen Mehrwert für Unternehmen und Organisationen zu schaffen.Anwendung von ML/KI-ModellenDer zweite Haupttrend, der die Punkte 3 bis 5 einschließt, besteht darin, aktuelle ML/KI-Modelle in Produktion zu bringen, um datengetriebenen Mehrwert zu schaffen. Die Demokratisierung und Automatisierung von Data Science machen aktuelle ML/KI-Modelle einem viel breiteren Publikum zugänglich, sodass Techniken und Lösungen von Data Science Operations, um diese Modelle in Produktion zu bringen, zu den wichtigsten Triebkräften für Data Science Lösungen werden. Infolgedessen werden sich neue Fragen bezüglich der Ethik von KI ergeben. Diese Fragen beziehen sich darauf, sicherzustellen, dass diese Technologien (im weiteren Sinne) Menschen oder anderen moralisch relevanten Wesen keinen Schaden zufügen. Um auf die oben erwähnten Trends zu reagieren, erwarten wir eine notwendige Spezialisierung der Data Science Rollen. Dabei wird erwartet, dass die Nachfrage nach Rollen wie Data Engineers und ML Engineers in den kommenden Jahren erheblich zunehmen wird.Um Ihre Organisation bestmöglich auf diese aufkommenden Data Science Trends vorzubereiten, haben wir in unserem Whitepaper für jeden Trend drei Schlüsselmaßnahmen identifiziert, die Sie bereits heute ergreifen können. Machen Sie sich bereit und nutzen Sie die vielversprechende Zukunftsperspektive von Data Science, um einen nachhaltigen datengetriebenen Mehrwert für Ihr Unternehmen zu schaffen.Sie haben Interesse am Thema Data Science Implementierung in Unternehmen?Hier können Sie sich das gesamte Whitepaper „7 Trends für Data Science im Jahr 2021“ kostenfrei herunterladen.Über den AutorLivia EichenbergerI am a data scientist at STATWORX and especially interested in Causal Machine Learning. I love the logic in data science and the beauty of neat and structured code!.e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/whitepaper-7-trends-fuer-data-science-im-jahr-2021/;Statworx;  Livia Eichenberger
  15. Oktober 2020;Generative Adversarial Networks: Wie mit Neuronalen Netzen Daten generiert werden können;"Management SummaryNeuronale Netze haben sich in den letzten Jahren immer weiter zur Kerntechnologie im Bereich Machine Learning und AI entwickelt. Neben den klassischen Anwendungen der Daten-Klassifizierung und -Regression können sie auch dazu verwendet werden, neue, realistische Datenpunkte zu erzeugen. Die etablierte Modell-Architektur für die Datengenerierung sind sogenannte Generative Adversarial Networks (GANs). GANs bestehen aus zwei neuronalen Netzen, dem Generator- und dem Diskriminatornetzwerk. Die beiden Netzwerke werden iterativ gegeneinander trainiert. Hierbei versucht der Generator einen realistischen Datenpunkt zu erzeugen, während der Diskriminator lernt, echte und synthetische Datenpunkte zu unterscheiden. In diesem Minimax-Spiel verbessert der Generator seine Performance so weit, bis die erzeugten Datenpunkte nicht mehr von echten Datenpunkten zu unterscheiden sind.Die Erzeugung synthetischer Daten mittels GANs hat eine Vielzahl von spannenden Anwendungsmöglichkeiten in der Praxis:Die Bild-SyntheseDie ersten Erfolge von GANs wurden auf Basis von Bilddaten erreicht. Heute sind spezifische GAN-Architekturen fähig, Bilder von Gesichtern zu generieren, die kaum von echten unterscheidbar sind.Die Musik-SyntheseNeben Bildern können GAN-Modelle auch verwendet werden, um Musik zu generieren. Im Bereich der Musik-Synthese wurden sowohl für das Erzeugen von realistischen Wellenformen als auch für die Generierung ganzer Melodien vielversprechende Resultate erreicht.Super-ResolutionEin weiteres Beispiel für den erfolgreichen Einsatz von GANs ist die Super-Resolution. Hierbei wird versucht, die Auflösung eines Bilds zu verbessern bzw. die Größe des Bildes verlustfrei zu erhöhen. Mittels GANs können schärfere Übergänge zwischen verschiedenen Bildbereichen als mit z.B. Interpolation erreicht und die allgemeine Bildqualität verbessert werden.DeepfakesMittels Deepfake Algorithmen können Gesichter in Bildern und Videoaufnahmen durch andere Gesichter ersetzt werden. Dabei wird der Algorithmus auf möglichst vielen Daten der Zielperson trainiert. Danach kann damit die Mimik einer dritten Person auf die zu imitierende Person transferiert werden, und es entsteht ein „Deepfake“.Generierung zusätzlicher TrainingsdatenGANs werden auch dazu verwendet, die im maschinellen Lernen oft nur in geringen Mengen verfügbaren Trainingsdaten zu vermehren (auch Augmentieren genannt). Dabei wird ein GAN auf dem Trainingsdatensatz kalibriert, damit anschließend beliebig viele, neue Trainingsbeispiele generiert werden können. Das Ziel ist, mit den zusätzlich generierten Daten die Performance bzw. Generalisierbarkeit von ML-Modellen zu verbessern.Anonymisierung von DatenLetztlich können mittels GANs auch Daten anonymisiert werden. Dies ist vor allem für Datensätze, die personenbezogene Daten enthalten, ein wichtiger Anwendungsfall. Der Vorteil von GANs gegenüber anderen Ansätzen zur Datenanonymisierung ist, dass die statistischen Eigenschaften des Datensets erhalten bleiben. Dies ist wiederum wichtig für die Performance anderer Machine Learning Modelle, die auf diesen Daten trainiert werden. Da das Verarbeiten von personenbezogenen Daten heute im Bereich des maschinellen Lernens eine große Hürde darstellt, sind GANs für die Anonymisierung von Datensätzen ein vielversprechender Ansatz und haben das Potential, für Firmen die Tür zur Verarbeitung personenbezogener Daten zu öffnen.In diesem Artikel wird die Funktionsweise von GANs erläutert. Zudem werden Use Cases diskutiert, die mithilfe von GANs umgesetzt werden können, und aktuelle Trends vorgestellt, die sich im Bereich von generativen Netzwerken abzeichnen.EinleitungDie größten Fortschritte der letzten Jahre im Bereich der künstlichen Intelligenz wurden durch die Anwendung von neuronalen Netzen erzielt. Diese haben sich insbesondere bei der Verarbeitung von unstrukturierten Daten, wie Texten oder Bildern, als äußerst zuverlässiger Ansatz zur Regression und Klassifizierung erwiesen. Ein bekanntes Beispiel ist die Klassifizierung von Bildern in zwei oder mehr Gruppen. Auf dem sogenannten ImageNet-Datensatz 1 mit insgesamt 1000 Klassen erreichen Neuronale Netze eine Top-5 Accuracy von 98.7%. Das bedeutet, dass für 98.7% der Bilder die richtige Klasse in den fünf besten Modellvorhersagen enthalten ist. Auch im Bereich der natürlichen Sprachverabreitung wurden in den letzten Jahren bahnbrechende Resultate durch die Anwendung von neuronalen Netzen erzielt. Die Transformer-Architektur wurde beispielsweise sehr erfolgreich für verschiedene Sprachverarbeitungsprobleme wie Question-Answering, Named Entity Recognition und Sentimentanalyse verwendet.Weniger bekannt ist, dass neuronale Netze auch dazu verwendet werden können die zugrundeliegende Verteilung einer Datenmenge zu erlernen, um somit neue, realistische Beispieldaten zu generieren. Als zielführende Modell-Architektur für diese Problemstellung haben sich in den letzten Jahren Generative Adversarial Networks (GANs) etabliert. Durch deren Anwendung ist es möglich, synthetische Datenpunkte zu erzeugen, die die gleichen statistischen Eigenschaften wie die zugrundeliegenden Trainingsdaten aufweisen. Dies eröffnet viele spannende Anwendungsfälle, von denen nachfolgend einige vorgestellt werden.Generative Adversarial NetworksGANs sind eine neue Klasse von Algorithmen im Bereich des maschinellen Lernens. Wie zuvor erläutert, handelt es sich dabei um Modelle, die neue, realistische Datenpunkte generieren können, nachdem sie auf einem bestimmten Datensatz trainiert wurden. GANs bestehen im Kern aus zwei neuronalen Netzen, die für spezifische Tasks im Lernprozess zuständig sind. Der Generator ist dafür verantwortlich, einen zufällig generierten Input in ein realistisches Sample der zu lernenden Verteilung zu transformieren. Im Gegensatz dazu ist es die Aufgabe des Diskriminators, echte von generierten Datenpunkten zu unterscheiden. Eine einfache Analogie ist das Wechselspiel eines Fälschers und eines Polizisten. Der Fälscher versucht möglichst realistische Münzfälschungen zu kreieren, während der Polizist die Fälschungen von echten Münzen unterscheiden will. Je besser die Nachbildungen des Fälschers sind, desto besser muss der Polizist die Münzen unterscheiden können, um die gefälschten Exemplare zu erkennen. Andererseits müssen die Fälschungen immer besser werden, da der Polizist mit seiner Erfahrung gefälschte Münzen zuverlässiger erkennen kann. Es ist einleuchtend, dass im Verlaufe dieses Prozesses sowohl der Fälscher als auch der Polizist in ihrer jeweiligen Aufgabe besser werden. Dieser iterative Prozess bildet auch die Grundlage für das Training der GAN-Modelle. Der Fälscher entspricht hier dem Generator, während der Polizist die Rolle des Diskriminators einnimmt.Wie werden Generator- und Diskriminator-Netzwerke trainiert?In einem ersten Schritt wird der Diskriminator fixiert. Das bedeutet, dass in diesem Schritt für das Diskriminatornetzwerk keine Anpassungen der Parameter vorgenommen werden. Für eine bestimmte Anzahl an Trainingsschritten wird dann der Generator trainiert. Der Generator wird, wie für neuronale Netze üblich, mittels Backpropagation trainiert. Sein Ziel ist, dass die aus dem zufälligen Input entstehenden „gefälschten“ Outputs von dem aktuellen Diskriminator als echte Beispiele klassifiziert werden. Es ist wichtig zu erwähnen, dass die Trainingsfortschritte vom aktuellen Stand des Diskriminators abhängen.Im zweiten Schritt wird der Generator fixiert und ausschließlich der Diskriminator trainiert, indem er sowohl echte als auch generierte Beispiele mit entsprechenden Labels als Trainingsinput verarbeitet. Das Ziel des Trainingsprozesses ist, dass der Generator so realistische Beispiele zu erschaffen lernt, dass der Diskriminator sie nicht mehr von echten unterscheiden kann. Zum besseren Verständnis ist der GAN-Trainingsprozess in der Abbildung 1 schematisch abgebildet.Abbildung 1 – Architektur eines GANsHerausforderungen des TrainingsprozessesAufgrund der Natur des alternierenden Trainingsprozesses können verschiedene Probleme beim Training eines GANs auftreten. Eine häufige Herausforderung ist, dass das Feedback des Diskriminators schlechter wird, je besser der Generator im Verlauf des Trainings wird.Man kann sich den Prozess folgendermaßen vorstellen: Wenn der Generator Beispiele generieren kann, die von echten Beispielen nicht mehr unterscheidbar sind, bleibt dem Diskriminator nichts anderes übrig, als zu raten aus welcher Klasse das jeweilige Beispiel stammt. Wenn man also das Training nicht rechtzeitig beendet, ist es möglich, dass die Qualität des Generators und des Diskriminators aufgrund der zufälligen Rückgabewerten des Diskriminators wieder abnimmt.Ein weiteres häufig auftretendes Problem ist der sogenannte „Mode-Collapse“. Dieser tritt auf, wenn das Netzwerk anstatt die Eigenschaften der zugrundeliegenden Daten zu lernen, sich einzelne Beispiele dieser Daten merkt oder nur Beispiele mit geringer Variabilität generiert. Einige Ansätze, um diesem Problem entgegenzuwirken, sind das Verarbeiten mehrerer Beispiele gleichzeitig (in Batches) oder das simultane Zeigen vergangener Beispiele, damit der Diskriminator mangelnde Unterscheidbarkeit der Beispiele quantifizieren kann.Use Cases für GANsDie Erzeugung von synthetischen Daten mittels GANs hat eine Vielzahl von spannenden Anwendungsmöglichkeiten in der Praxis. Die ersten eindrucksvollen Resultate von GANs wurden auf Basis von Bilddaten erreicht, dicht gefolgt von der Generierung von Audiodaten. In den letzten Jahren wurden GANs jedoch auch erfolgreich auf andere Datentypen, wie tabellarische Daten, angewendet. Im Folgenden werden ausgewählte Anwendungen vorgestellt.1. Die Bild-SyntheseEine der bekanntesten Anwendungen von GANs ist die Bild-Synthese. Hierbei wird ein GAN auf Basis eines großen Bilddatensatzes trainiert. Dabei erlernt das Generatornetzwerk die wichtigen gemeinsamen Merkmale und Strukturen der Bilder. Ein Beispiel ist die Generierung von Gesichtern. Die Portraits in der Abbildung 2 wurden mittels speziellen GAN-Netzwerken erzeugt, die für Bilddaten optimierte Faltungsnetzwerke, „Convolutional Neural Networks“ (CNN), verwenden. Natürlich können auch andere Arten von Bildern mittels GANs erzeugt werden, wie zum Beispiel handgeschriebene Zeichen, Fotos von Gegenständen oder Häusern. Die grundlegende Voraussetzung für gute Resultate ist ein ausreichend großer Datensatz.Ein praktischer Anwendungsfall dieser Technologie findet sich im Online Retail. Hier werden mittels GAN Fotos von Models in spezifischen Kleidungsstücken oder bestimmten Posen erzeugt.Abbildung 2 – GAN-generierte Gesichter2. Die Musik-SyntheseIm Gegensatz zur Erzeugung von Bildern beinhaltet die Musik-Synthese eine temporale Komponente. Da Audio-Wellenformen sehr periodisch sind und das menschliche Ohr sehr empfindlich gegen Abweichungen dieser Wellenformen ist, hat die Erhaltung der Signalperiodizität einen hohen Stellenwert für GANs, die Musik generieren. Dies hat zu GAN-Modellen geführt, die anstatt der Wellenform die Magnituden und Frequenzen des Tons generieren. Beispiele für GANs zur Musiksynthese sind das GANSynth 3, welches für das Generieren von realistischen Wellenformen optimiert ist, und das MuseGAN 4 – seinerseits spezialisiert auf das Generieren von ganzen Melodiesequenzen. Im Internet finden sich diverse Quellen, die von GANs erzeugte Musikstücke präsentieren. 5, 6 Es ist zu erwarten, dass GANs in den nächsten Jahren für eine hohe Disruption in der Musikbranche führen werden.Die Abbildung 3 zeigt Ausschnitte der vom MuseGAN generierten Musik für verschiedene Trainingsschritte. Zu Beginn des Trainings (step 0) ist der Output rein zufällig und im letzten Schritt (step 7900) erkennt man, dass z.B. der Bass eine typische Basslinie spielt und die Streicher vorwiegend gehaltene Akkorde spielen.Abbildung 3 – MuseGAN Trainingsverlauf3. Die Super-ResolutionDer Prozess der Wiedergewinnung eines hochauflösenden Bildes aus einem tiefer aufgelösten Bild heißt Super-Resolution. Die Schwierigkeit hierbei besteht darin, dass es sehr viele mögliche Lösungen für die Wiederherstellung des Bildes gibt.Ein klassischer Ansatz für Super-Resolution ist Interpolation. Dabei werden die für die höhere Auflösung fehlenden Pixel mittels einer vorgegebenen Funktion von den benachbarten Pixeln abgeleitet. Dabei gehen aber typischerweise viele Details verloren und die Übergänge zwischen verschiedenen Bildteilen verschwimmen. Da GANs die Hauptmerkmale von Bilddaten gut erlernen können, sind sie auch für das Problem der Super-Resolution ein geeigneter Ansatz. Dabei wird für das Generator-Netzwerk nicht mehr ein zufälliger Input verwendet, sondern das niedrigaufgelöste Bild. Der Diskriminator lernt dann generierte Bilder höherer Auflösung von den echten Bildern höherer Auflösung zu unterscheiden. Das trainierte GAN-Netzwerk kann dann auf neuen tief aufgelösten Bildern verwendet werden.Ein State of the Art GAN für Super-Resolution ist das SRGAN 7. In der Abbildung 4 sind die Resultate für verschiedene Super-Resolution-Ansätze für ein Beispielbild abgebildet. Das SRGAN (zweites Bild von rechts) liefert das schärfste Resultat. Besonders Details wie Wassertropfen oder die Oberflächenstruktur des Kopfschmucks werden vom SRGAN erfolgreich rekonstruiert.Abbildung 4 – Beispiel Super-Resolution für 4x Upscaling (von links nach rechts: bikubische Interpolation, SRResNet, SRGAN, Originalbild)4. Deepfakes: Audio und VideoMittels Deepfake-Algorithmen können Gesichter in Bildern und Videoaufnahmen durch andere Gesichter ersetzt werden. Die Resultate sind mittlerweile so gut, dass die Deepfakes nur schwer von echten Aufnahmen unterschieden werden können. Um einen Deepfake herzustellen wird wird der Algorithmus zunächst auf möglichst vielen Daten der Zielperson trainiert, damit anschließend die Mimik einer dritten Person auf die zu imitierende Person transferiert werden kann. In der Abbildung 5 sehen Sie den Vorher-nachher-Vergleich eines Deepfakes. Dabei wurde das Gesicht des Schauspielers Matthew McConaughey (Bildausschnit aus dem Film „Interstellar“) (links) durch das Gesicht des Tech-Unternehmers Elon Musk ersetzt (rechts).Diese Technik lässt sich beispielsweise auch auf Audiodaten anwenden. Dabei wird das Modell auf eine Zielstimme trainiert, um dann die Stimme in der Originalaufnahme durch die Zielstimme zu ersetzen.Abbildung 5 – Deepfake Sample Images5. Generierung zusätzlicher TrainingsdatenUm tiefe neuronale Netze mit vielen Parametern trainieren zu können, werden i.d.R. sehr große Datenmengen benötigt. Oft ist es nur schwer oder gar nicht möglich, an eine ausreichend große Datenmengen zu gelangen oder diese zu erheben. Ein Ansatz, um auch mit weniger Daten ein gutes Resultat zu erzielen, ist die Data Augmentation. Dabei werden die vorhandenen Datenpunkte leicht abgeändert, um somit neue Trainingsbeispiele zu kreieren. Häufig wird dies bspw. im Bereich von Computer Vision angewendet, indem Bilder rotiert oder mittels Zoom neue Bildausschnitte abgeleitet werden. Ein neuerer Ansatz für Data Augmentation ist das Verwenden von GANs. Die Idee ist, dass GANs die Verteilung der Trainingsdaten erlernen und somit theoretisch unendlich viele neue Beispiele generiert werden können. Für die Erkennung von Krankheiten auf Tomographie-Bildern wurde dieser Ansatz beispielsweise erfolgreich umgesetzt 9. GANs können also dazu verwendet werden, neue Trainingsdaten für Probleme zu generieren, bei denen nicht genügend große Datenmengen für das Training vorhanden sind.In vielen Fällen ist das Erheben größerer Datenmengen auch mit erheblichen Kosten verbunden, insbesondere dann, wenn die Daten für das Training manuell gekennzeichnet werden müssen. In diesen Situationen können GANs verwendet werden, um die anfallenden Kosten einer zusätzlichen Datenakquise zu reduzieren.Ein weiteres technisches Problem beim Training von Machine Learning Modellen, bei dem GANs Abhilfe schaffen können, sind Imbalanced Datasets. Dabei handelt es sich um Datensätze mit verschiedenen Klassen, die unterschiedlich häufig repräsentiert sind. Um mehr Trainingsdaten der unterrepräsentierten Klasse zu erhalten, kann man für dieser Klasse ein GAN trainieren, um weitere synthetisch generierte Datenpunkte zu erzeugen, die dann im Training verwendet werden. Beispielsweise gibt es deutlich weniger Mikroskop-Aufnahmen von krebsbefallenen Zellen als von gesunden. GANs ermöglichen in diesem Fall, bessere Modelle zur Erkennung von Krebszellen zu trainieren und können so Ärzte bei der Krebsdiagnose unterstützen.6. Anonymisierung von DatenEin weiterer interessanter Anwendungsbereich von GANs ist die Anonymisierung von Datensätzen. Klassische Ansätze für die Anonymisierung sind das Entfernen von Identifikator-Spalten oder das zufällige Ändern ihrer Werte. Resultierende Probleme sind bspw., dass mit entsprechendem Vorwissen trotzdem Rückschlüsse auf die Personen hinter den personenbezogenen Daten gezogen werden können. Auch die Änderung der statistischen Eigenschaften des Datensatzes durch das Fehlen oder Abändern bestimmter Informationen kann den Nutzen der Daten schmälern. GANs können auch dazu verwendet werden, anonymisierte Datensätze zu generieren. Sie werden dabei so trainiert, dass die persönlichen Daten im generierten Datensatz nicht mehr identifiziert, aber Modelle trotzdem ähnlich gut damit trainiert werden können 10. Erklären kann man diese gleichbleibende Qualität der Modelle damit, dass die zugrundeliegenden statistischen Eigenschaften des ursprünglichen Datensatzes von dem GAN gelernt, und somit auch erhalten werden.Anonymisierung mithilfe von GANs kann neben tabellarischen Daten auch spezifisch für Bilder angewendet werden. Dabei werden Gesichter oder andere persönliche Merkmale/Elemente auf dem Bild durch generierte Varianten ersetzt. Dies erlaubt, Computer Vision Modelle mit realistisch aussehenden Daten zu trainieren, ohne mit Datenschutzproblemen konfrontiert zu werden. Oft wird die Qualität der Modelle deutlich schlechter, wenn wichtige Merkmale eines Bilds, wie beispielsweise ein Gesicht, verpixelt oder weichgezeichnet in den Trainingsprozess einbezogen werden.Fazit und AusblickDurch die Anwendung von GANs kann die Verteilung von Datensätzen beliebiger Art gelernt werden. Wie oben erläutert werden GANs bereits auf verschiedenste Problemstellungen erfolgreich angewendet. Da GANs erst 2014 entdeckt wurden und großes Potential bewiesen haben, wird aktuell sehr intensiv daran geforscht. Die Lösung der oben erwähnten Probleme beim Training, wie z.B. „Mode-Collapse“, sind in der Forschung weit verbreitet. Es wird unter anderem an alternativen Loss-Funktionen und allgemein stabilisierenden Trainingsverfahren geforscht. Ein weiteres aktives Forschungsgebiet ist die Konvergenz von GAN-Netzwerken. Aufgrund der Fortschritte des Generators und Diskriminators während des Trainingsprozesses ist es sehr wichtig, zum richtigen Zeitpunkt das Training zu beenden, um den Generator nicht weiter auf schlechten Diskriminator-Ergebnissen zu trainieren. Um das Training zu stabilisieren, wird zudem an Ansätzen geforscht, die Diskriminator-Inputs mit Rauschen versehen, um die Anpassung des Diskriminators während des Trainings zu limitieren.Ein modifizierter Ansatz zur Generierung neuer Trainingsdaten sind Generative Teaching Networks. Dabei wird der Fokus des Trainings nicht primär auf das Erlernen der Datenverteilung gelegt, sondern man versucht direkt zu lernen, welche Daten das Training am schnellsten vorantreiben, ohne zwingend Ähnlichkeit mit den Originaldaten vorzuschreiben. 11 Am Beispiel von handgeschriebenen Zahlen konnte gezeigt werden, dass neuronale Netzwerke mit diesen künstlichen Inputdaten schneller lernen können als mit den Originaldaten. Dieser Ansatz lässt sich auch auf andere Datenarten als Bilddaten anwenden. Im Bereich der Anonymisierung ist man bisher imstande, Teile eines Datensatzes mit Persönlichkeitsschutzgarantien zuverlässig zu generieren. Diese Anonymisierungsnetzwerke können weiterentwickelt werden, um mehr Typen von Datensätzen abzudecken.Im Bereich von GANs werden sehr schnell theoretische Fortschritte gemacht, die bald den Weg in die Praxis finden werden. Da das Verarbeiten von personenbezogenen Daten heute im Bereich des maschinellen Lernens eine große Hürde darstellt, sind GANs für die Anonymisierung von Datensätzen ein vielversprechender Ansatz und haben das Potenzial, für Unternehmen die Tür zur Verarbeitung personenbezogener Daten zu öffnen.Quellenhttp://www.image-net.org/https://arxiv.org/abs/1710.10196https://openreview.net/pdf?id=H1xQVn09FXhttps://arxiv.org/abs/1709.06298https://salu133445.github.io/musegan/resultshttps://storage.googleapis.com/magentadata/papers/gansynth/index.htmlhttps://arxiv.org/abs/1609.04802https://github.com/iperov/DeepFaceLabhttps://arxiv.org/abs/1803.01229https://arxiv.org/abs/1806.03384https://eng.uber.com/generative-teaching-networks/Über den AutorStephan MüllerI am a data scientist at STATWORX and I am fascinated by the mathematical machinery which Machine Learning algorithms rely on. My life outside work revolves around sports, in particular field hockey, swimming and running..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/generative-adversarial-networks-wie-mit-neuronalen-netzen-daten-generiert-werden-koennen/;Statworx;  Stephan Müller
  13. Oktober 2020;Whitepaper: Change-Management für Data Science – Wie erreicht man ein Data Mindset;"Management SummaryData Science und künstliche Intelligenz sind der Schlüssel zur zukünftigen Wertschöpfung und Wettbewerbsfähigkeit vieler Organisationen. Um das Potenzial dieser Technologien voll auszuschöpfen, genügt jedoch nicht allein die Investition in Tools und Know-how. Zur Schaffung tatsächlicher Innovation müssen die neuen Technologien mit Branchenexpertise kombiniert und in das operative Geschäft integriert werden. Die einhergehende nachhaltige Transformation organisationaler Strukturen und Prozesse muss durch die Workforce getragen werden. Daher ist ein organisationsweites, grundsätzliches Umdenken hin zu datenbasierten Denk- und Handlungsweisen, dem Data Mindset, notwendig. Aktuell finden sich die Vordenker und Befürworter dieses Wandels vor allem in Leadership-Positionen von Unternehmen. Daher ist im Rahmen von KI-Initiativen effizientes Change-Management zur Transformation des Denkens im Top-Down-Prozess dringend geboten.Erfolgreiches Change-Management zur Etablierung des Data Mindsets setzt klares Leadership Commitment voraus und ist durch sechs zentrale Schlüsselfaktoren gekennzeichnet:Faktor – Reason:Argumentative Darlegung der Notwendigkeit des Wandels.Faktor – Vision:Die Vermittlung, der zukünftigen Mehrwerte des Wandels, sowohl für die Organisation als Ganzes als auch die Individuen.Faktor – Dialog:Offener Austausch mit der Workforce hinsichtlich Feedback und Vorbehalten bezüglich des Wandels.Faktor– Education &amp; Empowerment:Den Zugang zu differenzierten Weiterbildungsmaßnahmen sowie Tools und technischen Ressourcen.Faktor – Evangelize:Überzeugungsarbeit durch Kommunikation, Kooperation und Wissenstransfer mit Pionieren der Anwendung sowie erfolgreiche Umsetzung exemplarischer Leuchtturmprojekte.Faktor – Iteration:Agile und insbesondere iterative (Re-)Organisation von Handlungs-, Entscheidungs- und Planungsprozessen.In diesem Whitepaper präsentieren wir Ihnen die oben genannten Schlüsselfaktoren für ein erfolgreiches Change-Management zur Etablierung des Data Mindsets. Wir zeigen genau auf, worauf Sie achten müssen, um innerhalb Ihrer Data Science und KI-Initiative den Prozess des Umdenkens erfolgreich anzustoßen.Sie haben Interesse am Thema Data Science Implementierung in Unternehmen?Hier können Sie sich das gesamte Whitepaper „Change-Management für Data Science“ kostenfrei herunterladen.Über den AutorLea WaniekI am a data scientist at STATWORX, apart from machine learning, I love to play around with RMarkdown and ggplot2, making data science beautiful inside and out..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/whitepaper-change-management-fuer-data-science/;Statworx;  Lea Waniek
  8. Oktober 2020;Finetuning von Tesseract-OCR für deutsche Rechnungen;Management SummaryOCR (Optical Character Recognition) ist eine große Herausforderung für viele Unternehmen. Am OCR-Markt tummeln sich diverse Open Source sowie kommerzielle Anbieter. Ein bekanntes Open Source Tool für OCR ist Tesseract, das mittlerweile von Google bereitgestellt wird. Tesseract ist aktuell in der Version 4 verfügbar, die die OCR Extraktion mittels rekurrenten neuronalen Netzen durchführt. Die OCR Performance von Tesseract ist nach wie vor jedoch volatil und hängt von verschiedenen Faktoren ab. Eine besondere Herausforderung ist die Anwendung von Tesseract auf Dokumente, die aus verschiedenen Strukturen aufgebaut sind, z.B. Texten, Tabellen und Bildern. Eine solche Dokumentenart stellen bspw. Rechnungen dar, die OCR Tools aller Anbieter nach wie vor besondere Herausforderungen stellen.In diesem Beitrag wird demonstriert, wie ein Finetuning der Tesseract-OCR (Optical Character Recognition) Engine auf einer kleinen Stichprobe von Daten bereits eine erhebliche Verbesserung der OCR-Leistung auf Rechnungsdokumenten bewirken kann. Dabei ist der dargestellte Prozess nicht ausschließlich auf Rechnungen anwendbar sondern auf beliebige Dokumentenarten.Es wird ein Anwendungsfall definiert, der auf eine korrekte Extraktion des gesamten Textes (Wörter und Zahlen) aus einem fiktiven, aber realistischen deutschen Rechnungsdokument abzielt. Es wird hierbei angenommen, dass die extrahierten Informationen für nachgelagerte Buchhaltungszwecke bestimmt sind. Daher wird eine korrekte Extraktion der Zahlen sowie des Euro-Zeichens als kritisch angesehen.Die OCR-Leistung von zwei Tesseract-Modellen für die deutsche Sprache wird verglichen: das Standardmodell (nicht getuned) und eine finegetunete Variante. Das Standardmodell wird aus dem Tesseract OCR GitHub Repository bezogen. Das feinabgestimmte Modell wird mit denen in diesem Artikel beschriebenen Schritten entwickelt. Eine zweite deutsche Rechnung ähnlich der ersten wird für die Feinabstimmung verwendet. Sowohl das Standardmodell als auch das getunte Modell werden auf der gleichen Out-of-Sample Rechnung bewertet, um einen fairen Vergleich zu gewährleisten.Die OCR-Leistung des Tesseract Standardmodells ist bei Zahlen vergleichsweise schlecht. Dies gilt insbesondere für Zahlen, die den Zahlen 1 und 7 ähnlich sind. Das Euro-Symbol wird in 50% der Fälle falsch erkannt, sodass das Ergebnis für eine etwaig nachgelagerte Buchhaltungsanwendung ungeeignet ist.Das getunte Modell zeigt eine ähnliche OCR-Leistung für deutsche Wörter. Die OCR-Leistung bei Zahlen verbessert sich jedoch deutlich. Alle Zahlen und jedes Euro-Symbol werden korrekt extrahiert. Es zeigt sich, dass eine Feinabstimmung mit minimalem Aufwand und einer geringen Menge an Schulungsdaten eine große Verbesserung der Erkennungsleistung erzielen kann. Dadurch wird Tesseract OCR mit seiner Open-Source-Lizenzierung zu einer attraktiven Lösung im Vergleich zu proprietärer OCR-Software. Weiterhin werden abschließende Empfehlungen für das Finetuning von Tesseract LSTM-Modellen dargestellt, für den Fall, dass mehr Trainingsdaten vorliegen.Download des Tesseract Docker ContainersDer gesamte Finetuning-Prozess des LSTM-Modells von Tesseract wird im Folgenden ausführlich erörtert. Da die Installation und Anwendung von Tesseract kompliziert werden kann, haben wir einen Docker Container vorbereitet, der alle nötigen Installationen bereits enthält. Anrede*HerrFrauVorname* Nachname* Firma* Geschäftliche E-Mail*  Ich stimme der Verarbeitung meiner hier angegebenen Daten zum Zwecke des PDF Downloads zu. Die Datenschutzerklärung habe ich zur Kenntnis genommen.;https://www.statworx.com/de/blog/finetuning-von-tesseract-ocr-fuer-deutsche-rechnungen/;Statworx;  Denis Gontcharov
  6. Oktober 2020;Whitepaper: Ein Reifegradmodell für Künstliche Intelligenz;"Management SummaryKünstliche Intelligenz gilt heute als einer der bedeutendsten Megatrends des neuen Jahrzehnts. Allein in Europa könnten bis 2030 durch die Anwendung von KI Produktivitätserhöhungen von 20%-38% realisiert werden 1.Um künstliche Intelligenz im eigenen Unternehmen erfolgreich etablieren zu können, sollte zunächst verstanden werden, wie sich der Status Quo der KI-Adoption darstellt. Hierzu haben wir bei STATWORX ein Modell entwickelt, das basierend auf unserem KI-Strategie-Ansatz in den sechs Dimensionen (1) Daten, (2) Use Cases, (3) Infrastruktur, (4) Organisation, (5) Team &amp; Skills sowie (6) Governance den KI-Reifegrad in Organisationen oder Bereichen / Abteilungen bestimmen kann. Das Unternehmen kann dann in einen von vier möglichen Reifegraden eingestuft werden: ad-hoc, opportunistisch, integriert oder transformierend. In diesem Whitepaper werden sowohl die Hintergründe der Prüfung des KI-Reifegrades als auch Methoden zur Bestimmung des konkreten Reifegrades vorgestellt. Darüber hinaus werden Schritte aufgezeigt, die Sie bei einer systematischen Erhöhung des KI-Reifegrades unterstützen.Problemdarstellung: Warum KI-Reifegrade messen?Die Digitalisierung hat mittlerweile in den meisten Organisationen Fuß gefasst. Täglich werden große Mengen an Daten generiert und gespeichert. Die Möglichkeiten, durch KI neue Produkte und Services zu entwickeln, die Produktivität zu steigern und den Profit zu erhöhen, ist für viele Unternehmen äußerst attraktiv. 9 von 10 Führungskräfte sehen KI hierbei klar als Geschäftsmöglichkeit 2. Bei der Betrachtung des Marktes zeigt sich, trotz der erheblichen Möglichkeiten solcher intelligenter Technologien, ein ambivalentes Bild: Während in manchen Unternehmen die Adaption von Anwendungen der künstlichen Intelligenz bereits im vollen Gange ist und strategische und operative Maßnahmen zur Skalierung der Technologie ergriffen werden, hinken andere Firmen in diesen Themen noch stark hinterher. Analog zur allgemeinen digitalen Transformation werden Pioniere ihre Position festigen können, während andere Wettbewerber abgehängt werden. Bereits im Jahr 2017 konnten 38% der Befragten einer Umfrage von 3000 Führungskräften eine KI-Strategie im Unternehmen vorweisen, 23% der Befragten gaben an, KI zu pilotieren 2. Um eine erfolgreiche KI-Adaption zu ermöglichen, muss jedes Unternehmen intern zunächst einige Feststellungen treffen: Wo steht das Unternehmen bei der Anwendung von intelligenten Technologien und Fähigkeiten der künstlichen Intelligenz? Wohin soll die Anwendung von KI kurz-, mittel- und langfristig führen?In diesem Whitepaper möchten wir Ihnen unsere Methode zur Ermittlung sowie Erhöhung des KI-Reifegrades präsentieren. Nachfolgend finden Sie nicht nur die Parameter für die erfogreiche Messung der Reife, sondern darüber hinaus auch konkrete Zielbilder und Maßnahmen, welche Ihnen helfen eine KI-Roadmap zu definieren.Sie haben Interesse am Thema Reifegradmodell für Künstliche Intelligenz?Hier können Sie sich das gesamte Whitepaper „Ein Reifegradmodell für Künstliche Intelligenz“ kostenfrei herunterladen.Über den AutorJannik KlaukeAt STATWORX I work as a data science consultant and I'm passionate about all things data. My biggest interest is combining technical topics with business strategy..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/ein-reifegradmodell-fuer-kuenstliche-intelligenz/;Statworx;  Jannik Klauke
  1. Oktober 2020;Machine Learning Modelle mit Hilfe von Docker Containern bereitstellen;"EinleitungKünstliche Intelligenz (KI) ist für deutsche Firmen keineswegs eine Zukunftsvorstellung mehr. Laut einer Umfrage von Deloitte unter ca. 2.700 KI-Experten aus neun Ländern, geben über 90 Prozent der Befragten an, dass ihr Unternehmen Technologien aus einem der Bereiche Machine Learning (ML), Deep Learning, Natural Language Processing (NLP) und Computer Vision nutzt oder plant zu nutzen. Dieser hohe Anteil lässt sich nicht nur dadurch erklären, dass die Firmen das Potenzial von KI erkannt haben. Vielmehr stehen auch deutlich mehr standardisierte Lösungen für den Einsatz dieser Technologien zur Verfügung. Diese Entwicklung hat dazu geführt, dass die Einstiegshürde in den vergangenen Jahren immer weiter gesunken ist.Das Problem mit StandardlösungenSo bieten zum Beispiel die drei großen Cloud Anbieter – Amazon Web Services (AWS), Google Cloud Plattform (GCP) und Microsoft Azure – für bestimmte Problemstellungen standardisierte Lösungen an (z.B. Objekterkennung auf Bildern, Übersetzung von Texten, und automatisiertes Machine Learning). Bislang lassen sich aber noch nicht alle Probleme mit Hilfe solcher standardisierten Anwendungen lösen. Dafür kann es unterschiedliche Gründe geben: Der häufigste Grund ist, dass die verfügbaren Standardlösungen nicht auf die gewünschte Fragestellung passen. So ist zum Beispiel im Bereich NLP die Klassifikation von ganzen Texten häufig als eine Standardlösung verfügbar. Soll hingegen eine Klassifikation nicht auf Text-, sondern Wort-Ebene stattfinden, werden dazu andere Modelle benötigt, die nicht immer als Teil von Standardlösungen verfügbar sind. Und selbst wenn diese zur Verfügung stehen, sind die möglichen Kategorien in der Regel vordefiniert und lassen sich nicht weiter anpassen. Ein Service also, der für die Klassifikation von Wörtern in die Kategorien Ort, Person und Zeit gebaut wurde, kann nicht ohne weiteres für die Klassifikation von Wörtern in den Kategorien Kunde, Produkt und Preis verwendet werden. Viele Unternehmen sind deshalb auch weiterhin darauf angewiesen, eigene ML Modelle zu entwickeln. Da die Entwicklung von Modellen oftmals auf lokalen Rechnern erfolgt, muss sichergestellt werden, dass diese Modelle nicht nur dem Entwickler zur Verfügung stehen. Nach der Entwicklung eines Modells besteht eine wichtige Herausforderung darin, das Modell unterschiedlichen Nutzern zur Verfügung zu stellen, da nur dann auch ein Mehrwert für das Unternehmen entsteht.Mit MLOps Modelle nutzbar machenBei ML &amp; KI Projekten im Unternehmen treten sowohl bei der Entwicklung als auch bei der Bereitstellung eigene Herausforderungen in Erscheinung. Während die Entwicklung oftmals an der mangelnden Verfügbarkeit geeigneter Daten scheitert, kann die Bereitstellung daran scheitern, dass ein Modell nicht mit der Produktionsumgebung kompatibel ist. So werden beispielsweise Machine Learning Modelle zum überwiegenden Teil mit Open Source Sprachen oder neuen ML-Frameworks (z.B. Dataiku oder H2O) entwickelt, während eine operative Produktionsumgebung oftmals mit proprietärer und langjährig erprobter Software arbeitet. Die enge Verzahnung dieser beiden Welten stellt oftmals beide Komponenten vor gewichtige Herausforderungen. Deshalb ist es wichtig, die Entwicklung von ML Modellen mit der Arbeit der IT Operations zu verknüpfen. Diesen Prozess bezeichnet man als MLOps, da hier Data Scientists mit der IT zusammenarbeiten, um Modelle produktiv nutzbar zu machen.MLOpsist eine ML-Entwicklungskultur und -praxis, deren Ziel es ist, die Entwicklung von ML-Systemen (Dev) und den Betrieb von ML-Systemen (Ops) zu verbinden. In der Praxis bedeutet MLOps, auf Automatisierung und Überwachung zu setzen. Dieses Prinzip erstreckt sich auf alle Schritte der ML-Systemkonfiguration wie Integration, Testen, Freigabe, Bereitstellung und Infrastrukturverwaltung. Der Code eines Modells ist dabei einer von vielen weiteren Bestandteilen, wie in Abbildung 1 veranschaulicht wird. Die Abbildung zeigt neben dem ML Code auch andere Schritte des MLOps Prozesses auf und verdeutlicht, dass der ML Code selbst einen relativ kleinen Anteil des gesamten Prozesses ausmacht.Abbildung 1: Wichtige Bestandteile des MLOps ProzessesWeitere Aspekte von MLOps sind z.B. die kontinuierliche Bereitstellung und Qualitätsprüfung der Daten, oder das Testen des Modells und ggf. die Fehlerbehebung (sog. Debugging). Docker Container haben sich bei der Bereitstellung von eigens entwickelten ML Modellen als Kerntechnologie herauskristallisiert und werden deshalb in diesem Beitrag vorgestellt.Warum Docker Container?Die Herausforderung bei der Bereitstellung von ML Modellen ist, dass ein Modell in einer bestimmten Version einer Programmiersprache geschrieben ist. Diese Sprache ist im Regelfall nicht in der Produktionsumgebung verfügbar und muss deshalb erst installiert werden. Zudem verfügt das Modell über eigene Bibliotheken, Laufzeiten und andere technische Abhängigkeiten, die ebenfalls erst in der Produktionsumgebung installiert werden müssten. Docker löst dieses Problem über sogenannte Container, in denen sich Anwendungen samt aller Bestandteile isoliert verpacken und als eigene Services bereitstellen lassen. Diese Container enthalten alle Komponenten, die die Anwendung bzw. das ML Modell zum Ausführen benötigt inklusive Code, Bibliotheken, Laufzeiten und Systemtools. Über Container lassen sich deshalb auch eigene Modelle und Algorithmen in jeder Umgebung bereitstellen und zwar ohne Sorge, dass bspw. fehlende oder inkompatible Bibliotheken zu Fehlern führen.Abbildung 2: Vergleich von Docker Containern und virtuellen MaschinenVor dem Siegeszug von Docker waren lange Zeit virtuelle Maschinen das Tool der Wahl, um Anwendungen und ML Modelle isoliert bereitzustellen. Allerdings hat sich herausgestellt, dass Docker über diverse Vorteile gegenüber virtuellen Maschinen verfügt. Dies sind insbesondere verbesserte Ressourcennutzung, Skalierbarkeit und die schnellere Bereitstellung neuer Software. Im Folgenden sollen die drei Punkte detaillierter beleuchtet werden.Verbesserte RessourcennutzungAbbildung 2 vergleicht schematisch, wie Applikationen in Docker Containern bzw. auf virtuellen Maschinen laufen können. Virtuelle Maschinen verfügen über ihr eigenes Gastbetriebssystem, auf welchem unterschiedliche Applikationen laufen. Die Virtualisierung des Gastbetriebssystems auf der Hardwareebene nimmt viel Rechenleistung und Speicherplatz in Anspruch. Deshalb können auf einer virtuellen Maschine, bei gleichbleibender Effizienz, weniger Applikationen gleichzeitig laufen.Docker Container teilen sich hingegen das Betriebssystem des Hosts und benötigen kein eigenes Betriebssystem. Applikationen in Docker Containern fahren deshalb schneller hoch und nehmen, durch das geteilte Betriebssystems des Hosts, weniger Rechenleistung und Speicherplatz in Anspruch. Diese geringere Ressourcenauslastung ermöglicht es auf einem Server mehrere Applikationen parallel laufen zu lassen, was die Nutzungsrate eines Servers verbessert.SkalierbarkeitEinen weiteren Vorteil bieten Container im Bereich der Skalierung: Soll innerhalb des Unternehmens ein ML Modell häufiger genutzt werden, muss die Anwendung die zusätzlichen Anfragen bedienen können. Glücklicherweise lassen sich ML Modelle mit Docker leicht skalieren indem zusätzliche Container mit der gleichen Anwendung hochgefahren werden. Insbesondere Kubernetes, eine Open Source Technologie zur Container-Orchestrierung und skalierbaren Bereitstellung von Web-Services, eignet sich aufgrund seiner Kompatibilität mit Docker für flexible Skalierung. Mit Kubernetes können Web-Services basierend auf der aktuellen Auslastung flexibel und automatisiert hoch- oder runterskaliert werden.Bereitstellung neuer SoftwareVon Vorteil ist außerdem, dass Container sich nahtlos von lokalen Entwicklungsmaschinen auf Produktionsmaschinen schieben lassen. Deshalb sind sie einfach auszutauschen, wenn z.B. eine neue Version des Modells bereitgestellt werden soll. Die Isolierung des Codes und aller Abhängigkeiten in einem Container führt zudem zu einer stabileren Umgebung, in der das Modell betrieben werden kann. Dadurch treten Fehler aufgrund von bspw. falschen Versionen einzelner Bibliotheken weniger häufig auf und lassen sich gezielter beheben.Das Modell wird innerhalb eines Containers dabei als Web-Service bereitgestellt, den andere Nutzer und Anwendungen über gewöhnliche Internetprotokolle (z.B. HTTP) ansprechen können. Auf diesem Weg kann das Modell als Web-Service von anderen Systemen und Nutzern angefragt werden, ohne dass diese selbst spezifische technischen Voraussetzungen erfüllen müssen. Es müssen also nicht erst Bibliotheken oder die Programmiersprache des Modells installiert werden, um das Modell nutzbar zu machen.Neben Docker existieren noch weitere Containertechnologien wie rkt und Mesos, wobei Docker mit seiner nutzerfreundlichen Bedienung und ausführlichen Dokumentation neuen Entwicklern einen leichten Einstieg ermöglicht. Durch die große Nutzerbasis existieren für viele Standardanwendungen Vorlagen, die mit geringem Aufwand in Containern hochgefahren werden können. Gleichzeitig dienen diese kostenlosen Vorlagen als Basis für Entwicklungen eigener Anwendungen.Nicht zuletzt wegen dieser Vorteile wird Docker im MLOps Prozess mittlerweile als Best Practice angesehen. Der Prozess der Modellentwicklung gleicht zunehmend dem Softwareentwicklungsprozess, nicht zuletzt auch wegen Docker. Deutlich wird dies daran, dass containerbasierte Anwendungen durch Standardtools zur stetigen Integration und Bereitstellung (CI/CD) von Web-Services unterstützt werden.Welche Rolle spielen Docker Container in der MLOps Pipeline?Wie bereits erwähnt, handelt es sich bei MLOps um einen komplexen Prozess der kontinuierlichen Bereitstellung von ML Modellen. Zentrale Bestandteile eines solchen Systems sind in Abbildung 1 veranschaulicht. Der MLOps Prozess ähnelt stark dem DevOps Prozess, da auch die Entwicklung von Systemen des maschinellen Lernens eine Form der Softwareentwicklung ist. Standardkonzepte aus dem DevOps Bereich, wie stetige Integration von neuem Code und Bereitstellung neuer Software, finden sich deshalb im MLOps Prozess wieder. Dabei kommen neue, ML-spezifische Bestandteile wie das kontinuierliche Modelltraining und die Modell- und Datenvalidierung hinzu.Grundsätzlich gilt es als Best Practice die Entwicklung von ML Modellen in einer MLOps Pipeline einzubetten. Die MLOps Pipeline umfasst alle Schritte von der Bereitstellung und Transformation von Daten über das Modelltraining bis hin zur kontinuierlichen Bereitstellung fertiger Modelle auf Produktionsservern. Der Code für jeden Schritt in der Pipeline wird hierbei in einem Docker Container verpackt und die Pipeline startet die Container in einer festgelegten Reihenfolge. Docker Container zeigen besonders hier ihre Stärke, da durch die Isolierung des Codes innerhalb einzelner Container kontinuierlich Codeänderungen an den entsprechenden Stellen der Pipeline eingearbeitet werden können, ohne dass die gesamte Pipeline ausgetauscht werden muss. Dadurch sind die Kosten für die Wartung der Pipeline relativ gering. Die großen Cloud Anbieter (GCP, AWS und Microsoft Azure) bieten zudem Services an, mit denen Docker Container automatisch gebaut, bereitgestellt und als Web-Services gehostet werden können. Um die Skalierung von Containern zu erleichtern und möglichst flexibel zu gestalten, bieten Cloud Anbieter ebenfalls vollständig verwaltete Kubernetes Produkte an. Für die Verwendung von ML Modellen im Unternehmen bedeutet diese Flexibilität Kostenersparnisse, da eine ML Anwendung einfach runterskaliert wird für den Fall, dass die Nutzungsrate sinkt. Gleichermaßen kann eine höhere Nachfrage durch Bereitstellung weiterer Container gewährleistet werden, ohne dass der Container mit dem Modell gestoppt werden muss. Nutzer der Anwendung erfahren so keine unnötige Downtime.FazitFür die Entwicklung von Machine Learning Modellen und MLOps Pipelines stellen Docker Container eine Kerntechnologie dar. Die Vorteile bestehen in der Portabilität, Modularisierung und Isolierung des Codes von Modellen, geringem Wartungsaufwand bei Integration in Pipelines, schnellere Bereitstellung neuer Versionen des Modells und Skalierbarkeit über serverlose Cloudprodukte zur Bereitstellung von Containern. Bei STATWORX haben wir das Potenzial von Docker Containern erkannt und nutzen diese aktiv. Mit diesem Wissen unterstützten wir unsere Kunden bei der Verwirklichung ihrer Machine Learning und KI-Projekte. Sie wollen Docker in Ihrer MLOps Pipeline nutzen? Unsere Academy bietet Remote Trainings zu Data Science mit Docker, sowie kostenlose Webinare zu den Themen MLOps und Docker an.Über den AutorThomas AlcockI am a data scientist at STATWORX. The most interesting thing about data science is to find performative and explainable solutions to new problems..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/machine-learning-modelle-mit-hilfe-von-docker-containern-bereitstellen/;Statworx;  Thomas Alcock
  29. September 2020;Whitepaper: Die 6 Kernelemente einer KI-Strategie;"Künstliche Intelligenz (KI) hat sich zu einem der zentralen Treiber für den digitalen Wandel von Wirtschaft und Gesellschaft entwickelt. Unternehmen stehen vor der Herausforderung, KI als Bestandteil der Unternehmensstrategie etablieren zu müssen, um zukünftig wettbewerbsfähig zu bleiben. Gemäß einer aktuellen Bitkom Studie zum Thema KI aus dem Jahr 2020 mit über 600 Befragten, sprechen Unternehmen künstlicher Intelligenz zwar eine herausragende Bedeutung zu, tun sich aber schwer damit, die Technologie erfolgreich in der Organisation wertschöpfend zu verankern. Ein wichtiger Grund dafür ist das Fehlen einer KI-Strategie bei der Etablierung der Technologie in der Organisation.Auf Basis unserer langjährigen KI-Projekterfahrung haben wir 6 Kernelemente identifiziert, die Unternehmen bei der Entwicklung einer KI-Strategie berücksichtigen müssen:Daten: Durch ein systematisches Sammeln, Verwalten und Bereitstellen von Daten wird der fundamentale Baustein einer KI-Initiative gelegt. Daten sind hierbei als strategisches Asset zu betrachten und entsprechend zu behandeln.Use Cases: Durch einen klaren Prozess zur Identifikation, Priorisierung und Umsetzung von KI Use Cases wird eine bessere Skalierbarkeit der KI-Initiative in der gesamten Organisation gefördert.Infrastruktur: Die IT Infrastruktur der Organisation muss flexibel genug sein, alle KI-relevanten Lasten abbilden zu können. Dabei sollte die Zukunftsfähigkeit der eigenen On-Premises Infrastruktur beleuchtet und gegen eine (teilweise) cloudbasierte Infrastruktur abgewogen werden.Organisation: Auf Organisationsebene müssen Rahmenbedingungen für die optimale Implementierung und Skalierung von KI geschaffen werden. Hierzu zählen agile Prozesse sowie die Verankerung von datenbasierten Entscheidungen in der Kultur des Unternehmens.Team &amp; Skills: Zur Umsetzung von KI-Initiativen wird erfahrenes Personal aus verschiedenen Bereichen/Rollen benötigt. Dabei sind die Personalbedarfe an den jeweiligen KI-Reifegrad der Organisation anzupassen.Governance: Für die zuvor genannten Kernelemente müssen solide Governancemechanismen entwickelt und etabliert werden. Hierzu zählen im KI-Kontext insb. Data Governance aber auch Erklärbarkeit und Nachvollziehbarkeit von KI-Modellen.Durch die strukturierte Betrachtung dieser sechs Elemente können die wichtigsten strategischen Entscheidungen im Hinblick auf eine erfolgreiche Etablierung von KI-Initiativen abgeleitet werden. Dabei existieren verschiedene zeitliche und inhaltliche Abhängigkeiten zwischen den Kernelementen, die bei der Entwicklung der Strategie berücksichtigt werden müssen. Zur internen Ausgestaltung der sechs Kernelemente haben wir jeweils fünf Leitfragen erarbeitet, die Sie bei der Diskussion und Entscheidungsfindung unterstützen.In meinem Whitepaper “Die 6 Kernelemente einer KI-Strategie” erkläre ich anhand der fünf Leitfragen, wie Sie Ihr Unternehmen auf die Implementierung von KI ideal vorbereiten können.Sie haben Interesse am Thema KI-Strategie?Hier können Sie sich das gesamte Whitepaper „Die 6 Kernelemente einer KI-Strategie“ kostenfrei herunterladen.Über den AutorSebastian HeinzI am the founder and CEO of STATWORX. I enjoy writing about machine learning and AI, especially about neural networks and deep learning. In my spare time, I love to cook, eat and drink as well as traveling the world..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/whitepaper-die-6-kernelemente-einer-ki-strategie/;Statworx;  Sebastian Heinz
  24. September 2020;STATWORX 2.0 – Das neue Headquarter in Frankfurt ist eröffnet;"Something big is coming … mit diesem Teaser wurde das STATWORX-Team nun mehrere Monate unter Hochspannung gehalten, bis am 03. Juli 2020 endlich die Pforten des neuen Büros von STATWORX am Hauptstandort Frankfurt am Main geöffnet wurden. Beim gemütlichen Grillen konnte sich das Team nach einer langen Corona-bedingten Home Office Phase wieder persönlich austauschen und die neuen Räumlichkeiten auf sich wirken lassen. Doch wie war der Weg bis ins neue Office? Eins steht fest: it was a piece of work.„Hilfe, wir brauchen mehr Platz“ lautete bei STATWORX das Motto, als bereits 2018 die Entscheidung fiel, sich nach neuen Office Räumlichkeiten umzusehen. Unser bisheriger Standort im Frankfurter Westend war seit Ende 2016 der offizielle Hauptsitz. Im Rahmen der Expansion des Geschäfts folgten 10 weitere Desks in WeWork und Design Offices in der Frankfurter Innenstadt. Dass dies keine dauerhafte Lösung bleiben konnte, war klar.Frankfurt ist ein attraktiver Wirtschaftsstandort und bietet ein großes Angebot an verfügbaren Büroflächen – doch leider oftmals mit dem Fokus auf einen stark verzimmerten Innenausbau. Für STATWORX sollte es loftartig anmuten, mit großzügigen Bereichen für informelles und offenes Arbeiten; in denen der Austausch von Wissen und Kreativität gefördert werden soll. Nach einer fast zweijährigen Suche fiel unsere Entscheidung auf eine insg. 1.400qm große Einheit in der Hanauer Landstr. 150, unweit der Europäischen Zentralbank gelegen. Das Areal um den Ostbahnhof herum steht seit dem Einzug der EZB für Aufbruch, Innovation und Vorankommen. Ein Spirit, mit dem wir uns sehr gut identifizieren können.Im ca. 500qm großen Erdgeschoss trifft man auf die Empfangs- und Konferenzbereiche der Fläche. Insgesamt 4 Konferenzräume, z.B. für unsere offenen Data Science Kurse, unsere Bootcamp-Reihe sowie für interne Weiterbildungsveranstaltungen, fassen insgesamt bis zu 100 Personen. Dem vorgeschaltet ist die großzügige Lobby, die unter anderem für unsere quartalsweise stattfindenden Townhall-Meetings konzipiert wurde; und aufgrund der STATBAR in Zukunft wohl auch Schauplatz vieler After Work Events des Teams sein wird. Cheers! Besonders stolz sind wir auf unsere in Kooperation mit Poliform Berlin geplante (Event-)Küche. Die auf einem Betonboden ruhende Küche befindet sich in einem knapp 70qm großen Raum und wird künftig das kommunikative Herzstück im Erdgeschoss bilden.Die Regeletage im ersten Stock unserer Fläche umfasst rund 900qm. Hier herrscht ein Hot Desking Konzept vor, das insbesondere den Erkenntnissen zum Arbeitsalltag der neuerlichen Zeit Rechnung trägt. Muss ein zeitgemäßes Büro für jedes Teammitglied stets einen kompletten Arbeitsplatz vorhalten? Wir finden, spätestens seit Corona, nein. Arbeiten im Jahr 2020 stellt sich immer mehr als eine wenig abgegrenzte Kombination aus Arbeit, Bewegung, Anwesenheit, Home Office und aus spontanen Zusammenkünften dar. Dabei sind eine strenge Verzimmerung von Räumen sowie die Isolation von Menschen in diesen Räumen zum Zwecke des Arbeitens erklärtes Negativbeispiel eines modernen Arbeitsumfelds gewesen. So zielt unser Open Space Bereich des Büros auf Kommunikation ab. Interaktion, Arbeit und Austausch sollen fließend ineinander übergehen. Da dies natürlich nicht für jede Situation des Arbeitstages passend ist, gibt es zusätzlich 5 Gruppenbüros für insg. 20 Teammitglieder sowie die Büros des Managements und die der Geschäftsführung. Immer wieder finden sich in der Fläche Bereiche zum informellen Arbeiten, beispielsweise in Sesseln, auf Sofas oder in einem Think Tank- bzw. Ideenraum, in dem im Stehen konzipiert und gebrainstormed werden kann. In diesem Raum ist beispielsweise die komplette Wand beschreibbar, da sie in einer speziell dafür vorgesehenen Farbe ausgeführt wurde. Besonders interessant war die Planung des Coding Raums, in dem in dunkler „Hacking“-Atmosphäre unsere Mitarbeitenden in besonders ruhiger Umgebung programmieren können. Privat und zurückgezogen sind die Telefonräume konzipiert, die einen Ort für längere Telefonkonferenzen bieten, die nicht am Schreibtisch stattfinden sollen.Den Namen STATBib hat das Team unserem Bibliotheksbereich gegeben, in dem die umfangreiche Literatursammlung des Unternehmens endlich seinen gebührenden Platz findet. Runde Tische aus geräucherter Eiche mit dunkelgrünen Pendelleuchten darüber versprühen den Charme und die Geborgenheit einer Bibliothek aus vergangenen Zeiten.A propos Dunkelgrün: Bei der Planung des Interieurs des neuen STATWORX Offices wurde bewusst auf weiße Wände verzichtet. So gibt es in der gesamten Einheit keine einzige weiße Wand oder Decke. Als Hauptverantwortlicher des Projekts „Neues Office“ war es unserem CFO Julius ein persönliches Anliegen, dass die Grenzen zwischen Arbeiten und Wohlfühlen im neuen Büro verschwimmen. Das ging einher mit der Absage an weiß als typische, langweilige Bürofarbe. Farben sind für die Wirkung der Räumlichkeiten, in denen wir uns bewegen, essenziell. Das Farbkonzept basiert auf erdigen Farben mit grünen Untertönen, die das Gefühl von Behaglichkeit, Besonnenheit, aber auch von Motivation und Energie vermitteln. Bei der Wahl der Möbel kamen überwiegend natürliche Materialien zum Einsatz: viel Echtholz, viel Stoff, viel schwarz – gepaart mit Designklassikern aus unterschiedlichen Epochen und Ländern.All diese kreativen Entscheidungen spiegeln einen wichtigen Teil unserer Unternehmenskultur wider. Diese folgt der Leitlinie eines kollegialen Miteinanders und unserem Versprechen an Arbeit auf höchstem Niveau. Hierzu tragen ein ästhetisch durchdachtes und qualitativ wertiges Arbeitsumfeld genauso bei wie Kompetenz unserer Mitarbeitenden.Für das Team einen schönen Ort des Arbeitens und des Zusammentreffens zu realisieren, waren mein persönlicher Anspruch und stetiger Motivator über die gesamte Projektdauer hinweg.Du bist neugierig geworden auf unser neues Office? Wir freuen uns, ab sofort wieder viele neue Gesichter, Kunden, Freunde und altbekannte Partner in unserem neuen Headquarter in der Hanauer Landstr. 150 in Frankfurt begrüßen zu dürfen. Welcome to STATWORX!Über den AutorJulius HeinzJulius is our CFO and he's been accompanying the financial and strategic growth of the company since 2015. He is very passionate about architecture, photography and furniture..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/statworx-2-0-das-neue-headquarter-in-frankfurt-ist-eroeffnet/;Statworx;  Julius Heinz
  5. August 2020;Why You Should Use Containerized Microservices When Deploying Your Data Science Application;"Here at STATWORX, a Data Science and AI consulting company, we thrive on creating data-driven solutions that can be acted on quickly and translate into real business value. We provide many of our solutions in some form of web application to our customers, to allow and support them in their data-driven decision-making.Containerization Allows Flexible SolutionsAt the start of a project, we typically need to decide where and how to implement the solution we are going to create. There are several good reasons to deploy the designed solutions directly into our customer IT infrastructure instead of acquiring an external solution. Often our data science solutions use sensitive data. By deploying directly to the customers‘ infrastructure, we make sure to avoid data-related compliance or security issues. Furthermore, it allows us to build pipelines that automatically extract new data points from the source and incorporate them into the solution so that it is always up to date.However, this also imposes some constraints on us. We need to work with the infrastructure provided by our customers. On the one hand, that requires us to develop solutions that can exist in all sorts of different environments. On the other hand, we need to adapt to changes in the customers‘ infrastructure quickly and efficiently. All of this can be achieved by containerizing our solutions.The Advantages of ContainerizationContainerization has evolved as a lightweight alternative to virtualization. It involves packaging up software code and all its dependencies in a „container“ so that the software can run on practically any infrastructure. Traditionally, an application was developed in a specific computing development environment and then transferred to the production environment, often resulting in many bugs and errors; Especially when these environments were not mirroring each other. For example, when an application is transferred from a local desktop computer to a virtual machine or from a Linux to a Windows operating system.A container platform like Docker allows us to store the whole application with all the necessary code, system tools, libraries, and settings in a container that can be shipped to and work uniformly in any environment. We can develop our applications dockerized and do not have to worry about the specific infrastructure environment provided by our customers.There are some other advantages that come with using Docker in comparison to traditional virtual machines for the deployment of data science applications.Efficiency – As the container shares the machines‘ OS system kernel and does not require a Guest OS per application, it uses the provided infrastructure more efficiently, resulting in lower infrastructure costs.Speed – The start of a container does not require a Guest OS reboot; it can be started, stopped, replicated, and destroyed in seconds. That speeds up the development process, the time to market, and the operational speed. Releasing new software or updates has never been so fast: Bugs can be fixed, and new features implemented in hours or days.Scalability – Horizontal scaling allows to start and stop additional container depending on the current demand.Security – Docker provides the strongest default isolation capabilities in the industry. Containers run isolated from each other, which means that if one crashes, other containers serving the same applications will still be running.The Key Benefits of a Microservices ArchitectureIn connection with the use of Docker for delivering data science solutions, we use another emerging method. Instead of providing a monolithic application that comes with all the required functionalities of an application, we create small, independent services that communicate with each other and together embody the complete application. Usually, we develop WebApps for our customers. As shown in the graphic, the WebApp will communicate directly with the different other backend microservices. Each one is designed for a specific task and has an exposed REST API that allows for different HTTP requests.Furthermore, the backend microservices are indirectly exposed to the mobile app. An API Gateway routes the requests to the desired microservices. It can also provide an API endpoint that invokes several backend microservices and aggregates the results. Moreover, it can be used for access control, caching, and load balancing. If suitable, you might also decide to place an API Gateway between the WebApp and the backend microservices.In summary, splitting the application into small microservices has several advantages for us:Agility – As services operate independently, we can update or fix bugs for a specific microservice without redeploying the entire application.Technology freedom – Different microservices can be based on different technologies or languages, thus allowing us to use the best of all worlds.Fault isolation – If an individual microservice becomes unavailable, it will not crash the entire application. Only the function provided by the specific microservice will not be provided.Scalability – Services can be scaled independently. It is possible to scale the services which do the work without scaling the application.Reusability of service – Often, the functionalities of the services we create are also requested by other departments and other cases. We then expose application user interfaces so that the services can also be used independently of the focal application.Containerized Microservices – The Best of Both Worlds!The combination of docker with a clean microservices architecture allows us to combine the mentioned advantages. Each microservice lives in its own Docker container. We deliver fast solutions that are consistent across environments, efficient in terms of resource consumption, and easily scalable and updatable. We are not bound to a specific infrastructure and can adjust to changes quickly and efficiently.ConclusionOften the deployment of a data science solution is one of the most challenging tasks within data science projects. But without a proper deployment, there won’t be any business value created. Hopefully, I was able to help you figure out how to optimize the implementation of your data science application. If you need further help bringing your data science solution into production, feel free to contact us!Sourceshttps://docs.microsoft.com/de-de/learn/modules/principles-cloud-computing/2-what-is-cloud-computinghttps://www.ibm.com/cloud/learn/containerizationhttps://www.docker.com/resources/what-containerhttps://cloud.google.com/containershttps://blog.kumina.nl/2017/04/the-benefits-of-containers-and-container-technology/https://docs.microsoft.com/en-us/dotnet/architecture/microservices/architect-microservice-container-applications/direct-client-to-microservice-communication-versus-the-api-gateway-patternhttps://www.nginx.com/blog/introduction-to-microservices/Über den AutorJan FischerI am a data scientist at STATWORX. I always enjoyed to think critically about complex problems, understand and find a solution. Fortunately, STATWORX pays me for that!.e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/why-you-should-use-containerized-microservices-when-deploying-your-data-science-application/;Statworx;  Jan Fischer
  29. Juli 2020;How to Build a Machine Learning API with Python and Flask;"Did you ever want to make your machine learning model available to other people, but didn’t know how? Or maybe you just heard about the term API, and want to know what’s behind it? Then this post is for you!Here at STATWORX, we use and write APIs daily. For this article, I wrote down how you can build your own API for a machine learning model that you create and the meaning of some of the most important concepts like REST. After reading this short article, you will know how to make requests to your API within a Python program. So have fun reading and learning!What is an API?API is short for Application Programming Interface. It allows users to interact with the underlying functionality of some written code by accessing the interface. There is a multitude of APIs, and chances are good that you already heard about the type of API, we are going to talk about in this blog post: The web API.This specific type of API allows users to interact with functionality over the internet. In this example, we are building an API that will provide predictions through our trained machine learning model. In a real-world setting, this kind of API could be embedded in some type of application, where a user enters new data and receives a prediction in return. APIs are very flexible and easy to maintain, making them a handy tool in the daily work of a Data Scientist or Data Engineer.An example of a publicly available machine learning API is Time Door. It provides Time Series tools that you can integrate into your applications. APIs can also be used to make data available, not only machine learning models.And what is REST?Representational State Transfer (or REST) is an approach that entails a specific style of communication through web services. When using some of the REST best practices to implement an API, we call that API a „REST API“. There are other approaches to web communication, too (such as the Simple Object Access Protocol: SOAP), but REST generally runs on less bandwidth, making it preferable to serve your machine learning models.In a REST API, the four most important types of requests are:GETPUTPOSTDELETEFor our little machine learning application, we will mostly focus on the POST method, since it is very versatile, and lots of clients can’t send GET methods.It’s important to mention that APIs are stateless. This means that they don’t save the inputs you give during an API call, so they don’t preserve the state. That’s significant because it allows multiple users and applications to use the API at the same time, without one user request interfering with another.The ModelFor this How-To-article, I decided to serve a machine learning model trained on the famous iris dataset. If you don’t know the dataset, you can check it out here. When making predictions, we will have four input parameters: sepal length, sepal width, petal length, and finally, petal width. Those will help to decide which type of iris flower the input is.For this example I used the scikit-learn implementation of a simple KNN (K-nearest neighbor) algorithm to predict the type of iris:# model.pyfrom sklearn import datasetsfrom sklearn.model_selection import train_test_splitfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.metrics import accuracy_scorefrom sklearn.externals import joblibimport numpy as npdef train(X,y):    # train test split    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)    knn = KNeighborsClassifier(n_neighbors=1)    # fit the model    knn.fit(X_train, y_train)    preds = knn.predict(X_test)    acc = accuracy_score(y_test, preds)    print(f'Successfully trained model with an accuracy of {acc:.2f}')    return knnif __name__ == '__main__':    iris_data = datasets.load_iris()    X = iris_data'data'    y = iris_data'target'    labels = {0 : 'iris-setosa',              1 : 'iris-versicolor',              2 : 'iris-virginica'}    # rename integer labels to actual flower names    y = np.vectorize(labels.__getitem__)(y)    mdl = train(X,y)    # serialize model    joblib.dump(mdl, 'iris.mdl')As you can see, I trained the model with 70% of the data and then validated with 30% out of sample test data. After the model training has taken place, I serialize the model with the joblib library. Joblib is basically an alternative to pickle, which preserves the persistence of scikit estimators, which include a large number of numpy arrays (such as the KNN model, which contains all the training data). After the file is saved as a joblib file (the file ending thereby is not important by the way, so don’t be confused that some people call it .model or .joblib), it can be loaded again later in our application.The API with Python and FlaskTo build an API from our trained model, we will be using the popular web development package Flask and Flask-RESTful. Further, we import joblib to load our model and numpy to handle the input and output data.In a new script, namely app.py, we can now set up an instance of a Flask app and an API and load the trained model (this requires saving the model in the same directory as the script):from flask import Flaskfrom flask_restful import Api, Resource, reqparsefrom sklearn.externals import joblibimport numpy as npAPP = Flask(__name__)API = Api(APP)IRIS_MODEL = joblib.load('iris.mdl')The second step now is to create a class, which is responsible for our prediction. This class will be a child class of the Flask-RESTful class Resource. This lets our class inherit the respective class methods and allows Flask to do the work behind your API without needing to implement everything.In this class, we can also define the methods (REST requests) that we talked about before. So now we implement a Predict class with a .post() method we talked about earlier.The post method allows the user to send a body along with the default API parameters. Usually, we want the body to be in JSON format. Since this body is not delivered directly in the URL, but as a text, we have to parse this text and fetch the arguments. The flask _restful package offers the RequestParser class for that. We simply add all the arguments we expect to find in the JSON input with the .add_argument() method and parse them into a dictionary. We then convert it into an array and return the prediction of our model as JSON.class Predict(Resource):    @staticmethod    def post():        parser = reqparse.RequestParser()        parser.add_argument('petal_length')        parser.add_argument('petal_width')        parser.add_argument('sepal_length')        parser.add_argument('sepal_width')        args = parser.parse_args()  # creates dict        X_new = np.fromiter(args.values(), dtype=float)  # convert input to array        out = {'Prediction': IRIS_MODEL.predict(X_new)0}        return out, 200You might be wondering what the 200 is that we are returning at the end: For APIs, some HTTP status codes are displayed when sending requests. You all might be familiar with the famous 404 - page not found code. 200 just means that the request has been received successfully. You basically let the user know that everything went according to plan.In the end, you just have to add the Predict class as a resource to the API, and write the main function:API.add_resource(Predict, '/predict')if __name__ == '__main__':    APP.run(debug=True, port='1080')The '/predict' you see in the .add_resource() call, is the so-called API endpoint. Through this endpoint, users of your API will be able to access and send (in this case) POST requests. If you don’t define a port, port 5000 will be the default.You can see the whole code for the app again here:# app.pyfrom flask import Flaskfrom flask_restful import Api, Resource, reqparsefrom sklearn.externals import joblibimport numpy as npAPP = Flask(__name__)API = Api(APP)IRIS_MODEL = joblib.load('iris.mdl')class Predict(Resource):    @staticmethod    def post():        parser = reqparse.RequestParser()        parser.add_argument('petal_length')        parser.add_argument('petal_width')        parser.add_argument('sepal_length')        parser.add_argument('sepal_width')        args = parser.parse_args()  # creates dict        X_new = np.fromiter(args.values(), dtype=float)  # convert input to array        out = {'Prediction': IRIS_MODEL.predict(X_new)0}        return out, 200API.add_resource(Predict, '/predict')if __name__ == '__main__':    APP.run(debug=True, port='1080')Run the APINow it’s time to run and test our API!To run the app, simply open a terminal in the same directory as your app.py script and run this command.python run app.pyYou should now get a notification, that the API runs on your localhost in the port you defined. There are several ways of accessing the API once it is deployed. For debugging and testing purposes, I usually use tools like Postman. We can also access the API from within a Python application, just like another user might want to do to use your model in their code.We use the requests module, by first defining the URL to access and the body to send along with our HTTP request:import requestsurl = 'http://127.0.0.1:1080/predict'  # localhost and the defined port + endpointbody = {    ""petal_length"": 2,    ""sepal_length"": 2,    ""petal_width"": 0.5,    ""sepal_width"": 3}response = requests.post(url, data=body)response.json()The output should look something like this:Out1: {'Prediction': 'iris-versicolor'}That’s how easy it is to include an API call in your Python code! Please note that this API is just running on your localhost. You would have to deploy the API to a live server (e.g., on AWS) for others to access it.ConclusionIn this blog article, you got a brief overview of how to build a REST API to serve your machine learning model with a web interface. Further, you now understand how to integrate simple API requests into your Python code. For the next step, maybe try securing your APIs? If you are interested in learning how to build an API with R, you should check out this post. I hope that this gave you a solid introduction to the concept and that you will be building your own APIs immediately. Happy coding!Über den AutorJannik KlaukeAt STATWORX I work as a data science consultant and I'm passionate about all things data. My biggest interest is combining technical topics with business strategy..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/how-to-build-a-machine-learning-api-with-python-and-flask/;Statworx;  Jannik Klauke
  22. Juli 2020;Archive Existing RDS Files;"IntroductionWhen working on data science projects in R, exporting internal R objects as files on your hard drive is often necessary to facilitate collaboration. Here at STATWORX, we regularly export R objects (such as outputs of a machine learning model) as .RDS files and put them on our internal file server. Our co-workers can then pick them up for further usage down the line of the data science workflow (such as visualizing them in a dashboard together with inputs from other colleagues).Over the last couple of months, I came to work a lot with RDS files and noticed a crucial shortcoming: The base R saveRDS function does not allow for any kind of archiving of existing same-named files on your hard drive. In this blog post, I will explain why this might be very useful by introducing the basics of serialization first and then showcasing my proposed solution: A wrapper function around the existing base R serialization framework.Be wary of silent file replacements!In base R, you can easily export any object from the environment to an RDS file with:saveRDS(object = my_object, file = ""path/to/dir/my_object.RDS"")However, including such a line somewhere in your script can carry unintended consequences: When calling saveRDS multiple times with identical file names, R silently overwrites existing, identically named .RDS files in the specified directory. If the object you are exporting is not what you expect it to be — for example due to some bug in newly edited code — your working copy of the RDS file is simply overwritten in-place. Needless to say, this can prove undesirable.If you are familiar with this pitfall, you probably used to forestall such potentially troublesome side effects by commenting out the respective lines, then carefully checking each time whether the R object looked fine, then executing the line manually. But even when there is nothing wrong with the R object you seek to export, it can make sense to retain an archived copy of previous RDS files: Think of a dataset you run through a data prep script, and then you get an update of the raw data, or you decide to change something in the data prep (like removing a variable). You may wish to archive an existing copy in such cases, especially with complex data prep pipelines with long execution time.Don’t get tangled up in manual renamingYou could manually move or rename the existing file each time you plan to create a new one, but that’s tedious, error-prone, and does not allow for unattended execution and scalability. For this reason, I set out to write a carefully designed wrapper function around the existing saveRDS call, which is pretty straightforward: As a first step, it checks if the file you attempt to save already exists in the specified location. If it does, the existing file is renamed/archived (with customizable options), and the „updated“ file will be saved under the originally specified name.This approach has the crucial advantage that the existing code that depends on the file name remaining identical (such as readRDS calls in other scripts) will continue to work with the latest version without any needs for adjustment! No more saving your objects as „models_2020-07-12.RDS“, then combing through the other scripts to replace the file name, only to repeat this process the next day. At the same time, an archived copy of the — otherwise overwritten — file will be kept.What are RDS files anyways?Before I walk you through my proposed solution, let’s first examine the basics of serialization, the underlying process behind high-level functions like saveRDS.Simply speaking, serialization is the „process of converting an object into a stream of bytes so that it can be transferred over a network or stored in a persistent storage.“Stack Overflow: What is serialization?There is also a low-level R interface, serialize, which you can use to explore (un-)serialization first-hand: Simply fire up R and run something like serialize(object = c(1, 2, 3), connection = NULL). This call serializes the specified vector and prints the output right to the console. The result is an odd-looking raw vector, with each byte separately represented as a pair of hex digits. Now let’s see what happens if we revert this process:s &lt;- serialize(object = c(1, 2, 3), connection = NULL)print(s)# &gt;  1 58 0a 00 00 00 03 00 03 06 00 00 03 05 00 00 00 00 05 55 54 46 2d 38 00 00 00 0e 00# &gt; 29 00 00 03 3f f0 00 00 00 00 00 00 40 00 00 00 00 00 00 00 40 08 00 00 00 00 00 00unserialize(s)# &gt; 1 2 3The length of this raw vector increases rapidly with the complexity of the stored information: For instance, serializing the famous, although not too large, iris dataset results in a raw vector consisting of 5959 pairs of hex digits!Besides the already mentioned saveRDS function, there is also the more generic save function. The former saves a single R object to a file. It allows us to restore the object from that file (with the counterpart readRDS), possibly under a different variable name: That is, you can assign the contents of a call to readRDS to another variable. By contrast, save allows for saving multiple R objects, but when reading back in (with load), they are simply restored in the environment under the object names they were saved with. (That’s also what happens automatically when you answer „Yes“ to the notorious question of whether to „save the workspace image to ~/.RData“ when quitting RStudio.)Creating the archivesObviously, it’s great to have the possibility to save internal R objects to a file and then be able to re-import them in a clean session or on a different machine. This is especially true for the results of long and computationally heavy operations such as fitting machine learning models. But as we learned earlier, one wrong keystroke can potentially erase that one precious 3-hour-fit fine-tuned XGBoost model you ran and carefully saved to an RDS file yesterday.Digging into the wrapperSo, how did I go about fixing this? Let’s take a look at the code. First, I define the arguments and their defaults: The object and file arguments are taken directly from the wrapped function, the remaining arguments allow the user to customize the archiving process: Append the archive file name with either the date the original file was archived or last modified, add an additional timestamp (not just the calendar date), or save the file to a dedicated archive directory. For more details, please check the documentation here. I also include the ellipsis ... for additional arguments to be passed down to saveRDS. Additionally, I do some basic input handling (not included here).save_rds_archive &lt;- function(object,                             file = """",                             archive = TRUE,                             last_modified = FALSE,                             with_time = FALSE,                             archive_dir_path = NULL,                             ...) {The main body of the function is basically a series of if/else statements. I first check if the archive argument (which controls whether the file should be archived in the first place) is set to TRUE, and then if the file we are trying to save already exists (note that „file“ here actually refers to the whole file path). If it does, I call the internal helper function create_archived_file, which eliminates redundancy and allows for concise code.if (archive) {    # check if file exists    if (file.exists(file)) {      archived_file &lt;- create_archived_file(file = file,                                            last_modified = last_modified,                                            with_time = with_time)Composing the new file nameIn this function, I create the new name for the file which is to be archived, depending on user input: If last_modified is set, then the mtime of the file is accessed. Otherwise, the current system date/time (= the date of archiving) is taken instead. Then the spaces and special characters are replaced with underscores, and, depending on the value of the with_time argument, the actual time information (not just the calendar date) is kept or not.To make it easier to identify directly from the file name what exactly (date of archiving vs. date of modification) the indicated date/time refers to, I also add appropriate information to the file name. Then I save the file extension for easier replacement (note that „.RDS“, „.Rds“, and „.rds“ are all valid file extensions for RDS files). Lastly, I replace the current file extension with a concatenated string containing the type info, the new date/time suffix, and the original file extension. Note here that I add a „$“ sign to the regex which is to be matched by gsub to only match the end of the string: If I did not do that and the file name would be something like „my_RDS.RDS“, then both matches would be replaced.# create_archived_file.Rcreate_archived_file &lt;- function(file, last_modified, with_time) {  # create main suffix depending on type  suffix_main &lt;- ifelse(last_modified,                        as.character(file.info(file)$mtime),                        as.character(Sys.time()))  if (with_time) {    # create clean date-time suffix    suffix &lt;- gsub(pattern = "" "", replacement = ""_"", x = suffix_main)    suffix &lt;- gsub(pattern = "":"", replacement = ""-"", x = suffix)    # add ""at"" between date and time    suffix &lt;- paste0(substr(suffix, 1, 10), ""_at_"", substr(suffix, 12, 19))  } else {    # create date suffix    suffix &lt;- substr(suffix_main, 1, 10)  }  # create info to paste depending on type  type_info &lt;- ifelse(last_modified,                      ""_MODIFIED_on_"",                      ""_ARCHIVED_on_"")  # get file extension (could be any of ""RDS"", ""Rds"", ""rds"", etc.)  ext &lt;- paste0(""."", tools::file_ext(file))  # replace extension with suffix  archived_file &lt;- gsub(pattern = paste0(ext, ""$""),                        replacement = paste0(type_info,                                             suffix,                                             ext),                        x = file)  return(archived_file)}Archiving the archives?By way of example, with last_modified = FALSE and with_time = TRUE, this function would turn the character file name „models.RDS“ into „models_ARCHIVED_on_2020-07-12_at_11-31-43.RDS“. However, this is just a character vector for now — the file itself is not renamed yet. For this, we need to call the base R file.rename function, which provides a direct interface to your machine’s file system. I first check, however, whether a file with the same name as the newly created archived file string already exists: This could well be the case if one appends only the date (with_time = FALSE) and calls this function several times per day (or potentially on the same file if last_modified = TRUE). Somehow, we are back to the old problem in this case. However, I decided that it was not a good idea to archive files that are themselves archived versions of another file since this would lead to too much confusion (and potentially too much disk space being occupied). Therefore, only the most recent archived version will be kept. (Note that if you still want to keep multiple archived versions of a single file, you can set with_time = TRUE. This will append a timestamp to the archived file name up to the second, virtually eliminating the possibility of duplicated file names.) A warning is issued, and then the already existing archived file will be overwritten with the current archived version.The last puzzle piece: Renaming the original fileTo do this, I call the file.rename function, renaming the „file“ originally passed by the user call to the string returned by the helper function. The file.rename function always returns a boolean indicating if the operation succeeded, which I save to a variable temp to inspect later. Under some circumstances, the renaming process may fail, for instance due to missing permissions or OS-specific restrictions. We did set up a CI pipeline with GitHub Actions and continuously test our code on Windows, Linux, and MacOS machines with different versions of R. So far, we didn’t run into any problems. Still, it’s better to provide in-built checks.It’s an error! Or is it?The problem here is that, when renaming the file on disk failed, file.rename raises merely a warning, not an error. Since any causes of these warnings most likely originate from the local file system, there is no sense in continuing the function if the renaming failed. That’s why I wrapped it into a tryCatch call that captures the warning message and passes it to the stop call, which then terminates the function with the appropriate message.Just to be on the safe side, I check the value of the temp variable, which should be TRUE if the renaming succeeded, and also check if the archived version of the file (that is, the result of our renaming operation) exists. If both of these conditions hold, I simply call saveRDS with the original specifications (now that our existing copy has been renamed, nothing will be overwritten if we save the new file with the original name), passing along further arguments with ....        if (file.exists(archived_file)) {          warning(""Archived copy already exists - will overwrite!"")        }        # rename existing file with the new name        # save return value of the file.rename function        # (returns TRUE if successful) and wrap in tryCatch        temp &lt;- tryCatch({file.rename(from = file,                                      to = archived_file)        },        warning = function(e) {          stop(e)        })      }      # check return value and if archived file exists      if (temp &amp; file.exists(archived_file)) {        # then save new file under specified name        saveRDS(object = object, file = file, ...)      }    }These code snippets represent the cornerstones of my function. I also skipped some portions of the source code for reasons of brevity, chiefly the creation of the „archive directory“ (if one is specified) and the process of copying the archived file into it. Please refer to our GitHub for the complete source code of the main and the helper function.Finally, to illustrate, let’s see what this looks like in action:x &lt;- 5y &lt;- 10z &lt;- 20## save to RDSsaveRDS(x, ""temp.RDS"")saveRDS(y, ""temp.RDS"")## ""temp.RDS"" is silently overwritten with y## previous version is lostreadRDS(""temp.RDS"")#&gt; 1 10save_rds_archive(z, ""temp.RDS"")## current version is updatedreadRDS(""temp.RDS"")#&gt; 1 20## previous version is archivedreadRDS(""temp_ARCHIVED_on_2020-07-12.RDS"")#&gt; 1 10Great, how can I get this?The function save_rds_archive is now included in the newly refactored helfRlein package (now available in version 1.0.0!) which you can install directly from GitHub:# install.packages(""devtools"")devtools::install_github(""STATWORX/helfRlein"")Feel free to check out additional documentation and the source code there. If you have any inputs or feedback on how the function could be improved, please do not hesitate to contact me or raise an issue on our GitHub.ConclusionThat’s it! No more manually renaming your precious RDS files — with this function in place, you can automate this tedious task and easily keep a comprehensive archive of previous versions. You will be able to take another look at that one model you ran last week (and then discarded again) in the blink of an eye. I hope you enjoyed reading my post — maybe the function will come in handy for you someday!Über den AutorLukas FeickI am a data scientist at STATWORX. I have always enjoyed using data-driven approaches to tackle complex real-world problems, and to help people gain better insights..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/archive-existing-rds-files/;Statworx;  Lukas Feick
  20. Juli 2020;Neues Office in Zürich – Unlocking the Swiss AI Potential!;"Fast gleichzeitig mit unseren Kollegen in Frankfurt sind auch wir, das Schweizer STATWORX Team in Zürich, Anfang Juli in ein neues Büro umgezogen. Die neuen Räumlichkeiten im hippen Industrieviertel von Zürich bieten den perfekten Rahmen für das weitere Wachstum und den geplanten Ausbau des Schweizer Standortes von STATWORX.Zurück zu den AnfängenWie es der Zufall will, wurde STATWORX Schweiz direkt gegenüber vom neuen Bürogebäude ins Leben gerufen. Zwei Jahre nach der Gründung von STATWORX in Frankfurt hat unser CEO, Sebastian Heinz, im Jahr 2013 hier am Turbinenplatz diese Zweigniederlassung eröffnet, um die vielen Schweizer Unternehmen direkt vor Ort beraten zu können. Mit der ersten Erweiterung des Teams im Jahr 2018 wurde der Standort gewechselt und so arbeitete STATWORX Schweiz bis Ende Juni 2020 im Zentrum von Zürich.Im neuen Office, am Puls des Kreis 5, haben wir nun genügend Platz für die weitere Entwicklung und dem damit verbundenen Teamzuwachs von STATWORX Schweiz. Der moderne Open-Space mit Sichtbeton, weiten Glassfronten und tropischen Pflanzen bietet den perfekten Raum für kreatives und konzentriertes Arbeiten. Die Dachterrasse sorgt für einen perfekten Erholungsort für die Pausen zwischendurch.AI Potential der SchweizIm Laufe der Jahre haben wir bereits mit vielen Schweizer Unternehmen gemeinsam an deren Data Science, Machine Learning und AI Herausforderungen gearbeitet. Dabei sind wir industrieübergreifend aktiv gewesen: ob Versicherung, Finanzdienstleistung, Transport, Retail oder Telekommunikation – wir haben in nahezu allen Branchen bereits Erfahrungen gesammelt. Auch die Themen waren vielfältig und spannend: von der Prognose der Kundenabwanderung im Versicherungsumfeld, einer Datenstrategie im öffentlichen Transport, über Anomalieerkennung in Telekommunikationsnetzen bis zur Prognose von Abflugverspätungen ist so ziemlich alles dabei gewesen, was das Herz jedes Data Scientist höher schlagen lässt.Auch in Zukunft möchten wir mit unserer Expertise und breitgefächerten Erfahrung dazu beitragen, das Thema Data Science und AI in der Schweiz voranzutreiben und mitzugestalten. Wir sind davon überzeugt, dass wir mit unserem integrativen End-2-End Ansatz viele Prozesse, Dienstleistungen und Produkte in Unternehmen verbessern und effizienter gestalten können. Um das AI Potential der Schweiz zu erschließen, werden wir in naher Zukunft ein maßgeschneidertes Portfolio an Initiativen und Workshops lancieren – stay tuned!Über den AutorLivia EichenbergerI am a data scientist at STATWORX and especially interested in Causal Machine Learning. I love the logic in data science and the beauty of neat and structured code!.e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/neues-office-in-zuerich-unlocking-the-swiss-ai-potential/;Statworx;  Livia Eichenberger
  15. Juli 2020;Model Regularization – The Bayesian Way;"IntroductionSometimes here at STATWORX, we have impromptu discussions about statistical methods. In one such discussion, one of my colleagues decided to declare (albeit jokingly) Bayesian statistics unnecessary. That made me ask myself: Why would I ever use Bayesian models in the context of a standard regression problem? Existing approaches such as Ridge Regression are just as good, if not better. However, the Bayesian approach has the advantage that it lets you regularize your model to prevent overfitting and meaningfully interpret the regularization parameters. Contrary to the usual way of looking at ridge regression, the regularization parameters are no longer abstract numbers but can be interpreted through the Bayesian paradigm as derived from prior beliefs. In this post, I’ll show you the formal similarity between a generalized ridge estimator and the Bayesian equivalent.A (very brief) Primer on Bayesian StatsTo understand the Bayesian regression estimator, a minimal amount of knowledge about Bayesian statistics is necessary, so here’s what you need to know (if you don’t already): In Bayesian statistics, we think about model parameters (i.e., regression coefficients) probabilistically. In other words, the data given to us is fixed, and the parameters are considered random. That runs counter to the standard frequentist perspective in which the underlying model parameters are treated as fixed. At the same time, the data are considered random realizations of the stochastic process driven by those fixed model parameters. The end goal of Bayesian analysis is to find the posterior distribution, which you may remember from Bayes Rule:    While  is our likelihood and  is a normalizing constant,  is our prior which does not depend on the data, . In classical statistics,  is set to 1 (an improper reference prior) so that when the posterior ‚probability‘ is maximized, really just the likelihood is maximized because it’s the only part that still depends on . However, in Bayesian statistics, we use an actual probability distribution in place of , a Normal distribution, for example. So let’s consider the case of a regression problem, and we’ll assume that our target, , and our prior follow normal distributions. That leads us to conjugate Bayesian analysis, in which we can neatly write down an equation for the posterior distribution. In many cases, this is not possible, and for this reason, Markov Chain Monte Carlo methods were invented to sample from the posterior – taking a frequentist approach, ironically.We’ll make the usual assumption about the data:  is i.i.d.  for all observations . This gives us our standard likelihood for the Normal distribution. Now we can specify the prior for the parameter we’re trying to estimate, . If we choose a Normal prior (conditional on the variance, ) for the vector or weights in , i.e.  and an inverse-Gamma prior over the variance parameter it can be shown that the posterior distribution for  is Normally distributed with mean    If you’re interested in a proof of this result check out Jackman (2009, p.526).Let’s look at it piece by piece: is our standard OLS estimator,  is the mean vector of (multivariate normal) prior distribution, so it lets us specify what we think the average values of each of our model parameters are is the covariance matrix and contains our respective uncertainties about the model parameters. The inverse of the variance is called the precisionWhat we can see from the equation is that the mean of our posterior is a precision weighted average of our prior mean (information not based on data) and the OLS estimator (based solely on the data). The second term in parentheses indicates that we are taking the uncertainty weighted prior mean, , and adding it to the weighted OLS estimator, . Imagine for a moment that  . Then    That would mean that we are infinitely uncertain about our prior beliefs that the mean vector of our prior distribution would vanish, contributing nothing to our posterior! Likewise, if our uncertainty decreases (and the precision thus increases), the prior mean, , would contribute more to the posterior mean.After this short primer on Bayesian statistics, we can now formally compare the Ridge estimator with the above Bayesian estimator. But first, we need to take a look at a more general version of the Ridge estimator.Generalizing the Ridge estimatorA standard tool used in many regression problems, the standard Ridge estimator is derived by solving a least-squares problem from the following loss function:    While minimizing this gives us the standard Ridge estimator you have probably seen in textbooks on the subject, there’s a slightly more general version of this loss function:    Let’s derive the estimator by first re-writing the loss function in terms of matrices:    Differentiating with respect to the parameter vector, we end up with this expression for the gradient:    So, Minimizing over  we get this expression for the generalized ridge estimator:    The standard Ridge estimator can be recovered by setting . Usually we regard  as an abstract parameter that regulates the penalty size and  as a vector of values (one for each predictor) that increases the loss the further these coefficients deviate from these values. When  the coefficients are pulled towards zero.Let’s take a look at how the estimator behaves when the parameters, , and  change. We’ll define a meaningful ‚prior‘ for our example and then vary the penalty parameter. As an example, we’ll use the diamonds dataset from the ggplot2 package and model the price as a linear function of the number of carats, in each diamond, the depth, table, x, y and z attributesAs we can see from the plot, both with and without a prior, the coefficient estimates change rapidly for the first few increases in the penalty size. We also see that the ’shrinkage‘ effect holds from the upper plot: as the penalty increases, the coefficients tend towards zero, some faster than others. The plot on the right shows how the coefficients change when we set a sensible ‚prior‘. The coefficients still change, but they now tend towards the ‚prior‘ we specified. That’s because  penalizes deviations from our , which means that larger values for the penalty pull the coefficients towards . You might be asking yourself how this compares to the Bayesian estimator. Let’s find out!Comparing the Ridge and Bayesian EstimatorNow that we’ve seen both the Ridge and the Bayesian estimators, it’s time to compare them. We discovered, that the Bayesian estimator contains the OLS estimator. Since we know its form, let’s substitute it and see what happens:    This form makes the analogy much clearer: corresponds to , the matrix of precisions. In other words, since  is the identity matrix, the ridge estimator assumes no covariances between the regression coefficients and a constant precision across all coefficients (recall that  is a scalar) corresponds to , which makes sense, since the vector  is the mean of our prior distribution, which essentially pulls the estimator towards it, just like  ’shrinks‘ the coefficients towards its values. This ‚pull‘ depends on the uncertainty captured by  or  in the ridge estimator.That’s all well and good, but let’s see how changing the uncertainty in the Bayesian case compares to the behavior of the ridge estimator. Using the same data and the same model specification as above, we’ll set the covariance matrix  matrix to equal  and then change lambda. Remember, smaller values of  now imply a more significant contribution of the prior (less uncertainty), and therefore increasing them makes the prior less important.The above plots match out understanding so far: With a prior mean of zeros, the coefficients are shrunken towards zero, as in the ridge regression case when the prior dominates, i.e., when the precision is high. And when a previous mean is set, the coefficients tend towards it as the precision increases. So much for the coefficients, but what about the performance? Let’s have a look!Performance comparisonLastly, we’ll compare the predictive performance of the two models. Although we could treat the parameters in the model as hyperparameters, which we would need to tune, this would defy the purpose of using prior knowledge. Instead, let’s choose a previous specification for both models, and then compare the performance on a holdout set (30% of the data). While we can use the simple  as our predictor for the Ridge model, the Bayesian model provides us with a full posterior predictive distribution, which we can sample from to get model predictions. To estimate the model I used the brmspackage.RMSEMAEMAPEBayesian Linear Model1625.381091.3644.15Ridge Estimator1756.011173.5043.44Overall, both models perform similarly, although some error metrics slightly favor one model over the other. Judging by these errors, we could certainly improve our models by specifying a more appropriate probability distribution for our target variable. After all, prices can not be negative, yet our models can and do produce negative predictions.RecapIn this post, I’ve shown you how the ridge estimator compares to the Bayesian conjugate linear model. Now you understand the connection between the two models and how a Bayesian approach can provide a more readily interpretable way of regularizing your model. Normally  would be considered a penalty size, but now it can be interpreted as a measure of prior uncertainty. Similarly, the parameter vector  can be seen as a vector of prior means for our model parameters in the extended ridge model. As far as the Bayesian approach goes, we also can use prior distributions to implement expert knowledge in your estimation process. This regularizes your model and allows for incorporation of external information in your model. If you are interested in the code, check it out at our GitHub page!ReferencesJackman S. 2009. Bayesian Analysis for the Social Sciences. West Sussex: Wiley.Über den AutorThomas AlcockI am a data scientist at STATWORX. The most interesting thing about data science is to find performative and explainable solutions to new problems..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/model-regularization-the-bayesian-way/;Statworx;  Thomas Alcock
  14. Juli 2020;Off To New Adventures: STATWORX Office Soft Opening;"For nearly a whole year, the STATcrew waited for this magic moment: The STATWORX Office Opening Day! On July 3rd, we celebrated the opening of our new office in the Eastend of Frankfurt.Exploring The New SpaceThe day started with a tour of our new building. Everyone was excited to see the brand new office since our CFO Julius put so much effort into transforming the space into a modern, welcoming place to work. On the tour, the crew was eager to examine the whole area with its lovely details, tech gadgets, lounge places, conference rooms, and much more.Talking StrategyAfterward, we had our quarterly town hall meeting to get everyone up-to-date on company news. For several hours we talked about communication, strategy, and goals for the coming months.Firing up the grill on the patio outside the office. Time For Food and SnacksFinally, after a lot of insightful talks, we fired up the grill to have a tasty barbecue with cold beers and delicious wine. Since we’ve all been working from home for the last couple of months, we had a lot of catching up to do.Who has the coolest new hairstyle? How are the new parents and their babies doing? Are weddings still happening? Of course, there was a lot of work-talk, too, but we certainly loved seeing all those familiar faces again.Back to WorkOne of the most significant changes is the sheer amount of conference rooms we have now! And not only that: The rooms are all different, some with sofas and armchairs, others with bar tables and stools. It’s really fun to try out every room and to find new stuff and details in each one. We’ve been working in our new office for over a week now, and the motivation is as high as ever. With our heads full ideas and on the lookout for new challenges, we’re off to new adventures!Über den AutorAnne-Marie AntwerpenJournalism, Social Networks and Media have influenced my life for over a decade now. As a PR &amp; Communications Manager at STATWORX, I can finally combine my love for the many facets of the internet with my job!.e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/off-to-new-adventures-statworx-office-soft-opening/;Statworx;  Anne-Marie Antwerpen
  8. Juli 2020;Two Patterns To Secure REST APIs;"In this blog post, I want to present two models of how to secure a REST API. Both models work with JSON Web Tokens (JWT). We suppose that token creation has already happened somewhere else, e.g., on the customer side. We at STATWORX are often interested in the verification part itself.The first model extends a running unsecured interface without changing any code in it. This model does not even rely on Web Tokens and can be applied in a broader context. The second model demonstrates how token verification can be implemented inside a Flask application.About JWTJSON Web Tokens are more or less JSON objects. They contain authority information like the issuer, the type of signing, information on their validity span, and custom information like the username, user groups and further personal display data. To be more precise: it consists of three parts: the header, the payload, and the signature. All three parts are JSON objects, are base64url encoded and put together in the following fashion, where the dots separate the three parts:base64url(header).base64url(payload).base64url(signature)All information is transparent and readable to whoever the token has (just decode the parts). Its security and validity stem from its digital signing, the signing guarantees that a Web Token is from its issuer.There are two different ways of digital signing: with a secret or with a public-private-key pair in various forms and algorithms. This choice also influences the verification possibilities: The symmetric algorithm with a password can only be verified by the owner of the password – the issuer – whereas, for asymmetric algorithms, the public part can be distributed and, therefore, used for verification.Besides this feature, JWT offers additional advantages and features. Here is a brief overviewDecoupling: The application or the API will not need to implement a secure way to exchange passwords and verify them against a backend.Purpose oriented: Tokens can be issued for a particular purpose (defined within the payload) and are only valid for this purpose.Less password exchange: Tokens can be used multiple times without password interactions.Expiration: Tokens expire. Even in the event of theft and criminal intent, the amount of information that can be obtained is limited to its validity span.More information about JWT and also a debugger can be found at https://jwt.ioAuthorization HeaderMoving over to consideration on how Authorization is exchanged within HTTP requests.To authenticate against a server, you can use the Request Header Authorization, of which various types exist. Down below are two common examples:Basic  Authorization: Basic QWxhZGRpbjpvcGVuIHNlc2FtZQ==where the latter part is a base64 encoded string of user:passwordBearer  Authorization: Bearer eyJhbGciOiJIUzI1NiJ9.eyJpbmZvIjoiSSdtIGEgc2lnbmVkIHRva2VuIn0.rjnRMAKcaRamEHnENhg0_Fqv7Obo-30U4bcI_v-nfEMwhere the latter part is a JWT.Note that this information is totally transparent on transfer unless HTTPS protocol is used.Pattern 1 – Sidecar VerificationLet’s start with the first pattern that I’d like to introduce.In this section, I’ll introduce a setup with nginx and use its feature of sub-requests. This is especially useful in cases where the implementation of an API can not be changed.Nginx is a versatile tool. It acts as a web server, a load balancer, a content cache, and a reversed proxy. With the latest Web Server Survey of April 2020, nginx is the most used web server for public web sites.By using nginx and sub-requests, each incoming request has to go through nginx, and the header, including the Authorization part, is passed to a sub-module (calling it Auth Service). This sub-module then checks the validity of the Authorization before sending it through to the actual REST API or declining it. This decision process depends on the following status codes of the Auth Service:20x: nginx passes the request over to the actual resource service401 or 403: nginx denies the access and sends the response from the authentication service insteadAs you might have noticed, this setup does not exclusively build on JWTs, and therefore other authorization types can be used.Schema of request handling: 1) pass over to Auth Service for verification, 2) pass over to REST API if the verification was successful.Nginx configurationTwo configuration amendments need to be taken to have nginx configured for token validation:Add an (internal) directive to the authorization service which verifies the requestAdd authentication parameters to the directive that needs to be securedThe authorization directive is configured like thislocation /auth {        internal;        proxy_pass                          http://auth-service/verify;        proxy_pass_request_body off;        proxy_set_header                Content-Length """";        proxy_set_header                X-Original-URI $request-uri;}This cuts off the body and sends the rest of the request to the authorization service.Next, the configuration of the directive which forwards to your REST API:location /protected {        auth_request    auth/;        proxy_pass      http://rest-service;}Note that auth_request points to the authorization directive we have set up before.Please see the Nginx documentation for additional information on the sub-request pattern.Basic Implementation of the Auth ServiceIn the sub-module, we use the jwcrypto library for the token operations. It offers all features and algorithms needed for the authentication task. Many other libraries can do similar things, which can be found at Jwt.io.Suppose the token was created by an asymmetric cryptographical algorithm like RSA, and you have access to the public key (here called: public.pem)Following our configuration on nginx, we will add the directive /verify that does the verification job:from jwcrypto import jwt, jwkfrom flask import Flaskapp = Flask(__name__)# Load the public keywith open('public.pem', 'rb') as pemfile:        public_key = jwk.JWK.from_pem(pemfile.read())def parse_token(auth_header):      # your implementation of Authorization Header extraction    pass@app.route(""/verify"")def verify():      token_raw = parse_token(request.headers""Authorization"")      try:            # decode token        token = jwt.JWT(jwt=token_raw, key=public_key)        return 200, ""this is a secret information""    except:          return 403, """"The script consists of three parts: Reading the public key with the start of the API, extracting the header information (not given here), and the actual verification that is embedded in a try-catch expression.Pattern 2 – Verify within the APIIn this section, we will implement the verification within our Flask API.There are packages in the Flask universe available like flask_jwt; however, they do not offer the full scope of features and, in particular, not for our case here. Instead, we again use the library jwcrypto like before.It is further assumed that you have access to the public key again, but this time, a specific directive is secured (here called: /secured ) – and also, access should only be granted to admin users.from jwcrypto import jwt, jwkfrom flask import Flaskapp = Flask(__name__)# Load the public keywith open('public.pem', 'rb') as pemfile:        public_key = jwk.JWK.from_pem(pemfile.read())@app.route(""/secured"")def secured():      token_raw = parse_token(request.headers""Authorization"")      try:            # decode token        token = jwt.JWT(jwt=token_raw, key=public_key)        if ""admin"" in token.claims""groups"":              return 200, ""this is a secret information""        else:              return 403, """"    except:          return 403, """"The setup is the same as in the previous example, but an additional check was added: whether the user is part of the admin group.SummaryTwo patterns were discussed, which both use Web Tokens to authenticate the request. While the first pattern’s charm lies in the decoupling of Authorization and API functionality, the second approach is more compact. It perfectly fits situations where the number of APIs is low, or the overhead caused by a separate service is too high. Instead, the sidecar pattern is perfect when you provide several APIs and like to unify the Authorization in one separate service.Web Tokens are used in popular authorization schemes like OAuth and OpenID Connect. In another blog post, I will focus on how they work and how they connect to the verification I presented.I hope you have enjoyed the post. Happy coding!Über den AutorAndre MünchI am a data engineer at STATWORX. I love to have challenges to setup data structure and compose components to integrate Data Science models into productive environments..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/two-patterns-to-secure-rest-apis/;Statworx;  Andre Münch
  1. Juli 2020;A Social Network Simulation In The Tidyverse;"„There is no way you know Thomas! What a coincidence! He’s my best friend’s saxophone teacher! This cannot be true. Here we are, at the other end of the world and we meet? What are the odds?“ Surely, not only us here at STATWORX have experienced similar situations, be it in a hotel’s lobby, on the far away hiking trail or in the pub in that city you are completely new to. However, the very fact that this story is so suspiciously relatable might indicate that the chances of being socially connected to a stranger by a short chain of friends of friends isn’t too low after all.Lots of research has been done in this field, one particular popular result being the 6-Handshake-Rule. It states that most people living on this planet are connected by a chain of six handshakes or less. In the general setting of graphs, in which edges connect nodes, this is often referred to as the so-called small-world-effect. That is to say, the typical number of edges needed to get from node A to node B grows logarithmically in population size (i.e., # nodes). Note that, up until now, no geographic distance has been included in our consideration, which seems inadequate as it plays a significant role in social networks.When analyzing data from social networks such as Facebook or Instagram, three observations are especially striking:Individuals who are geographically farther away from each other are less likely to connect, i.e., people from the same city are more likely to connect.Few individuals have extremely many connections. Their number of connections follows a heavy-tailed Pareto distribution. Such individuals interact as hubs in the network. That could be a celebrity or just a really popular kid from school.Connected individuals tend to share a set of other individuals they are both connected to (e.g., „friend cliques“). This is called the clustering property.A model that explains these observationsClearly, due to the characteristics of social networks mentioned above, only a model that includes geographic distances of the individuals makes sense. Also, to account for the occurrence of hubs, research has shown that reasonable models attach a random weight to each node (which can be regarded as the social attractiveness of the respective individual). A model that accounts for all three properties is the following: First, randomly place nodes in space with a certain intensity , which can be done with a Poisson process. Then, with an independent uniformly distributed weight  attached to each node , every two nodes get connected by an edge with a probability    where  is the dimension of the model (here:  as we’ll simulate the model on the plane), model parameter  controls the impact of the weights, model parameter  squishes the overall input to the profile function , which is a monotonously decreasing, normalized function that returns a value between  and .That is, of course, what we want because its output shall be a probability. Take a moment to go through the effects of different  and  on . A higher  yields a smaller input value for  and thereby a higher connection probability. Similarly, a high  entails a lower  (as ) and thus a higher connection probability. All this comprises a scale-free random connection model, which can be seen as a generalization of the model by Deprez and Würthrich. So much about the theory. Now that we have a model, we can use this to generate synthetic data that should look similar to real-world data. So let’s simulate!Obtain data through simulationFrom here on, the simulation is pretty straight forward. Don’t worry about specific numbers at this point.library(tidyverse)library(fields)library(ggraph)library(tidygraph)library(igraph)library(Matrix)# Create a vector with plane dimensions. The random nodes will be placed on the plane.plane &lt;- c(1000, 1000)poisson_para &lt;- .5 * 10^(-3) # Poisson intensity parameterbeta &lt;- .5 * 10^3gamma &lt;- .4# Number of nodes is Poisson(gamma)*AREA - distributedn_nodes &lt;- rpois(1, poisson_para * plane1 * plane2)weights &lt;- runif(n_nodes) # Uniformly distributed weights# The Poisson process locally yields node positions that are completely random.x = plane1 * runif(n_nodes)y = plane2 * runif(n_nodes)phi &lt;- function(z) { # Connection function  pmin(z^(-1.8), 1)} What we need next is some information on which nodes are connected. That means, we need to first get the connection probability by evaluating  for each pair of nodes and then flipping a biased coin, accordingly. This yields a  encoding, where  means that the two respective nodes are connected and  that they’re not. We can gather all the information for all pairs in a matrix that is commonly known as the adjacency matrix.# Distance matrix needed as inputdist_matrix &lt;-rdist(tibble(x,y))weight_matrix &lt;- outer(weights, weights, FUN=""*"") # Weight matrixcon_matrix_prob &lt;- phi(1/beta * weight_matrix^gamma*dist_matrix^2)# Evaluationcon_matrix &lt;- Matrix(rbernoulli(1,con_matrix_prob), sparse=TRUE) # Samplingcon_matrix &lt;- con_matrix * upper.tri(con_matrix) # Transform to symmetric matrixadjacency_matrix &lt;- con_matrix + t(con_matrix)Visualization with ggraphIn an earlier post we praised visNetwork as our go-to package for beautiful interactive graph visualization in R. While this remains true, we also have lots of love for tidyverse, and ggraph (spoken „g-giraffe“) as an extension of ggplot2 proves to be a comfortable alternative for non-interactive graph plots, especially when you’re already familiar with the grammar of graphics. In combination with tidygraph, which lets us describe a graph as two tidy data frames (one for the nodes and one for the edges), we obtain a full-fledged tidyverse experience. Note that tidygraph is based on a graph manipulation library called igraph from which it inherits all functionality and „exposes it in a tidy manner“. So before we get cracking with the visualization in ggraph, let’s first tidy up our data with tidygraph!Make graph data tidy again!Let’s attach some new columns to the node dataframe which will be useful for visualization. After we created the tidygraph object, this can be done in the usual dplyr fashion after using activate(nodes)and activate(edges)for accessing the respective dataframes.# Create Igraph objectgraph &lt;- graph_from_adjacency_matrix(adjacency_matrix, mode=""undirected"")# Make a tidygraph object from it. Igraph methods can still be called on it.tbl_graph &lt;- as_tbl_graph(graph)hub_id &lt;- which.max(degree(graph))# Add spacial positions, hub distance and degree information to the nodes.tbl_graph &lt;- tbl_graph %&gt;%  activate(nodes) %&gt;%  mutate(    x = x,    y = y,    hub_dist = replace_na(bfs_dist(root = hub_id), Inf),    degree = degree(graph),    friends_of_friends = replace_na(local_ave_degree(), 0),    cluster = as.factor(group_infomap())  )Tidygraph supports most of igraphs methods, either directly or in the form of wrappers. This also applies to most of the functions used above. For example breadth-first search is implemented as the bfs_* family, wrapping igraph::bfs(), the group_graphfamily wraps igraphs clustering functions and local_ave_degree() wraps igraph::knn().Let’s visualize!GGraph is essentially built around three components: Nodes, Edges and Layouts. Nodes that are connected by edges compose a graph which can be created as an igraph object. Visualizing the igraph object can be done in numerous ways: Remember that nodes usually are not endowed with any coordinates. Therefore, arranging them in space can be done pretty much arbitrarily. In fact, there’s a specific research branch called graph drawing that deals with finding a good layout for a graph for a given purpose.Usually, the main criteria of a good layout are aesthetics (which is often interchangeable with clearness) and capturing specific graph properties. For example, a layout may force the nodes to form a circle, a star, two parallel lines, or a tree (if the graph’s data allows for it). Other times you might want to have a layout with a minimal number of intersecting edges. Fortunately, in ggraph all the layouts from igraph can be used.We start with a basic plot by passing the data and the layout to ggraph(), similar to what you would do with ggplot() in ggplot2. We can then add layers to the plot. Nodes can be created by using geom_node_point()and edges by using geom_edge_link(). From then on, it’s full-on ggplot2-style.# Add coord_fixed() for fixed axis ratio!basic &lt;- tbl_graph %&gt;%  ggraph(layout = tibble(V(.)$x, V(.)$y)) +  geom_edge_link(width = .1) +  geom_node_point(aes(size = degree, color = degree)) +  scale_color_gradient(low = ""dodgerblue2"", high = ""firebrick4"") +  coord_fixed() +  guides(size = FALSE)To see more clearly what nodes are essential to the network, the degree, which is the number of edges a node is connected with, was highlighted for each node. Another way of getting a good overview of the graph is to show a visual decomposition of the components. Nothing easier than that!cluster &lt;- tbl_graph %&gt;%  ggraph(layout = tibble(V(.)$x, V(.)$y)) +  geom_edge_link(width = .1) +  geom_node_point(aes(size = degree, color = cluster)) +  coord_fixed() +  theme(legend.position = ""none"")Wouldn’t it be interesting to visualize the reach of a hub node? Let’s do it with a facet plot:# Copy of tbl_graph with columns that indicate weather in n - reach of hub.reach_graph &lt;- function(n) {  tbl_graph %&gt;%    activate(nodes) %&gt;%    mutate(      reach = n,      reachable = ifelse(hub_dist &lt;= n, ""reachable"", ""non_reachable""),      reachable = ifelse(hub_dist == 0, ""Hub"", reachable)    )}# Tidygraph allows to bind graphs. This means binding rows of the node and edge dataframes.evolving_graph &lt;- bind_graphs(reach_graph(0), reach_graph(1), reach_graph(2), reach_graph(3))evol &lt;- evolving_graph %&gt;%  ggraph(layout = tibble(V(.)$x, V(.)$y)) +  geom_edge_link(width = .1, alpha = .2) +  geom_node_point(aes(size = degree, color = reachable)) +  scale_size(range = c(.5, 2)) +  scale_color_manual(values = c(""Hub"" = ""firebrick4"",                                ""non_reachable"" = ""#00BFC4"",                                ""reachable"" = ""#F8766D"","""")) +  coord_fixed() +  facet_nodes(~reach, ncol = 4, nrow = 1, labeller = label_both) +  theme(legend.position = ""none"")A curious observationAt this point, there are many graph properties (including the three above but also cluster sizes and graph distances) that are worth taking a closer look at, but this is beyond the scope of this blogpost. However, let’s look at one last thing. Somebody just recently told me about a very curious fact about social networks that seems paradoxical at first: Your average friend on Facebook (or Instagram) has way more friends than the average user of that platform.It sounds odd, but if you think about it for a second, it is not too surprising. Sampling from the pool of your friends is very different from sampling from all users on the platform (entirely at random). It’s exactly those very prominent people who have a much higher probability of being among your friends. Hence, when calculating the two averages, we receive very different results.As can be seen, the model also reflects that property: In the small excerpt of the graph that we simulate, the average node has a degree of around 5 (blue intercept). The degree of connected nodes is over 10 on average (red intercept).ConclusionIn the first part, I introduced a model that describes the features of real-life data of social networks well. In the second part, we obtained artificial data from that model and used it to create an igraph object (by means of the adjacency matrix). The latter can then be transformed into a tidygraph object, allowing us to easily make manipulation on the node and edge tibble to calculate any graph statistic (e.g., the degree) we like. Further, the tidygraph object is then used for conveniently visualizing the network through Ggraph. I hope that this post has sparked your interest in network modeling and has given you an idea of how seamlessly graph manipulation and visualization with Tidygraph and Ggraph merge into the usual tidyverse workflow. Have a wonderful day!Über den AutorFelix PlaggeAs a data scientist at STATWORX I am always keen on finding relevant structure in data. Also, I'm passionate about the mathematical fundament of machine learning algorithms..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/a-social-network-simulation-in-the-tidyverse/;Statworx;  Felix Plagge
  15. Mai 2020;How To Dockerize ShinyApps;"In my previous blog post, I have shown you how to run your R-scripts inside a docker container. For many of the projects we work on here at STATWORX, we end up using the RShiny framework to build our R-scripts into interactive applications. Using containerization for the deployment of ShinyApps has a multitude of advantages. There are the usual suspects such as easy cloud deployment, scalability, and easy scheduling, but it also addresses one of RShiny’s essential drawbacks: Shiny creates only a single R session per app, meaning that if multiple users access the same app, they all work with the same R session, leading to a multitude of problems. With the help of Docker, we can address this issue and start a container instance for every user, circumventing this problem by giving every user access to their own instance of the app and their individual corresponding R session.If you’re not familiar with building R-scripts into a docker image or with Docker terminology, I would recommend you to first read my previous blog post.So let’s move on from simple R-scripts and run entire ShinyApps in Docker now!The SetupSetting up a projectIt is highly advisable to use RStudio’s project setup when working with ShinyApps, especially when using Docker. Not only do projects make it easy to keep your RStudio neat and tidy, but they also allow us to use the renv package to set up a package library for our specific project. This will come in especially handy when installing the needed packages for our app to the Docker image.For demonstration purposes, I decided to use an example app created in a previous blog post, which you can clone from the STATWORX GitHub repository. It is located in the „example-app“ subfolder and consists of the three typical scripts used by ShinyApps (global.R, ui.R, and server.R) as well as files belonging to the renv package library. If you choose to use the example app linked above, then you won’t have to set up your own RStudio Project, you can instead open „example-app.Rproj“, which opens the project context I have already set up. If you choose to work along with an app of your own and haven’t created a project for it yet, you can instead set up your own by following the instructions provided by RStudio.Setting up a package libraryThe RStudio project I provided already comes with a package library stored in the renv.lock file. If you prefer to work with your own app, you can create your own renv.lock file by installing the renv package from within your RStudio project and executing renv::init(). This initializes renv for your project and creates a renv.lock file in your project root folder. You can find more information on renv over at RStudio’s introduction article on it.The DockerfileThe Dockerfile is once again the central piece of creating a Docker image. We now aim to repeat this process for an entire app where we previously only built a single script into an image. The step from a single script to a folder with multiple scripts is small, but there are some significant changes needed to make our app run smoothly.# Base image https://hub.docker.com/u/rocker/FROM rocker/shiny:latest# system libraries of general use## install debian packagesRUN apt-get update -qq &amp;&amp; apt-get -y --no-install-recommends install \    libxml2-dev \    libcairo2-dev \    libsqlite3-dev \    libmariadbd-dev \    libpq-dev \    libssh2-1-dev \    unixodbc-dev \    libcurl4-openssl-dev \    libssl-dev## update system librariesRUN apt-get update &amp;&amp; \    apt-get upgrade -y &amp;&amp; \    apt-get clean# copy necessary files## app folderCOPY /example-app ./app## renv.lock fileCOPY /example-app/renv.lock ./renv.lock# install renv &amp; restore packagesRUN Rscript -e 'install.packages(""renv"")'RUN Rscript -e 'renv::restore()'# expose portEXPOSE 3838# run app on container startCMD ""R"", ""-e"", ""shiny::runApp('/app', host = '0.0.0.0', port = 3838)""The base imageThe first difference is in the base image. Because we’re dockerizing a ShinyApp here, we can save ourselves a lot of work by using the rocker/shiny base image. This image handles the necessary dependencies for running a ShinyApp and comes with multiple R packages already pre-installed.Necessary filesIt is necessary to copy all relevant scripts and files for your app to your Docker image, so the Dockerfile does precisely that by copying the entire folder containing the app to the image.We can also make use of renv to handle package installation for us. This is why we first copy the renv.lock file to the image separately. We also need to install the renv package separately by using the Dockerfile’s ability to execute R-code by prefacing it with RUN Rscript -e. This package installation allows us to then call renv directly and restore our package library inside the image with renv::restore(). Now our entire project package library will be installed in our Docker image, with the exact same version and source of all the packages as in your local development environment. All this with just a few lines of code in our Dockerfile.Starting the App at RuntimeAt the very end of our Dockerfile, we tell the container to execute the following R-command:shiny::runApp('/app', host = '0.0.0.0', port = 3838)The first argument allows us to specify the file path to our scripts, which in our case is ./app. For the exposed port, I have chosen 3838, as this is the default choice for RStudio Server, but can be freely changed to whatever suits you best.With the final command in place every container based on this image will start the app in question automatically at runtime (and of course close it again once it’s been terminated).The Finishing TouchesWith the Dockerfile set up we’re now almost finished. All that remains is building the image and starting a container of said image.Building the imageWe open the terminal, navigate to the folder containing our new Dockerfile, and start the building process:docker build -t my-shinyapp-image . Starting a containerAfter the building process has finished, we can now test our newly built image by starting a container:docker run -d --rm -p 3838:3838 my-shinyapp-imageAnd there it is, running on localhost:3838.OutlookNow that you have your ShinyApp running inside a Docker container, it is ready for deployment! Having containerized our app already makes this process a lot easier; there are further tools we can employ to ensure state of the art security, scalability, and seamless deployment. Stay tuned until next time, when we’ll go deeper into the full range of RShiny and Docker capabilities by introducing ShinyProxy.Über den AutorOliver GuggenbühlI am a data scientist at STATWORX and love telling stories with data - the ShinyR the better!.e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/how-to-dockerize-shinyapps/;Statworx;  Oliver Guggenbühl
  6. Mai 2020;What’s Your Problem – Framing Data Science Questions The Right Way;"As a data scientist, it is always tempting to focus on the newest technology, the latest release of your favorite deep learning network, or a fancy statistical test you recently heard of. While all of this is very important, and we here at STATWORX are proud to use the latest open-source machine learning tools, it is often more important to take a step back and have a closer look at the problem we want to solve. In this article, I want to show you the importance of framing your business question in a different way – the data science way. Once the problem is clearly defined, we are more than happy to apply the newest fancy algorithm. But let’s start from the beginning!The Initial ProblemManagement ViewLet’s assume for a moment that you are a data scientist here at STATWORX. Monday morning, at 10 o’clock the telephone rings, and a manager of an international bank is on the phone. After a bit of back and forth, the bank manager explains that they have a problem with defaulting loans and they need a program that predicts loans which are going to default in the future. Unfortunately, he must end the call now, but he’ll catch up with you later. In the meanwhile, you start to make sense of the problem.Data Scientist ViewWhile it’s clear for the bank manager that he provided you with all necessary information, you grab another cup of coffee, lean back in your chair and recap the problem:The bank lends money to customers todayThe customer promises the bank to pay back the loan bit by bit over the next couple of months/yearsUnfortunately, some of the customers are not able to do so and are going to default on the loanSo far everything is fine. The bank will give you data of the past and you are instructed to make a prediction. Fair enough, but what specifically was there to predict again? Do they need to know whether every single loan is going to default or not? Are they more concerned about the default trend throughout the whole bank?Data Science ExplanationFrom a data science perspective, we differentiate between two sorts of problems: Classification and Regression tasks. The way we prepare the data and the models we apply are inherently different between the two tasks. Classification problems, as the name suggested, assign data points into a specific category. For bank loans, one approach could be to construct two categories:The loan defaultedThe loan is still performingOn the other hand, the output of a Regression problem is a continuous variable. In this case, this could be:The percentage of loans which are going to default in a given monthThe total amount of money the bank will lose in a given monthFrom now on, it’s paramount to evaluate with the clients what problem they actually want to solve. While it’s a lot of fun to play around with the best tech stack, it is of the highest importance to never forget about the business needs of the client. I’ll present you two possible scenarios, one for the classification and one for the regression case.Scenario Classification ProblemManagement ViewFor the next day, you set up a phone conference with the manager and decision-makers of the bank to discuss the overall direction of the project. The management board of the bank decided that it is more important to focus on the default prediction of single loans, instead of the overall default trend. Now you know that you have to solve a classification problem. Further, you ask the board what exactly they expect from the model.Manager A: I want to have the best performing model possible!Manager B: As long as it predicts reality as accurate as possible, I’m happy ??Manager C: As long as it catches every defaulted loan for sure…Manager A: … but of course, it should not predict too many loans wrong!Data Scientist ViewYou try to match every requirement from the bank. Understandably, the bank wants to have the perfect model, which makes little to no mistakes. Unfortunately, there is always an error. You are still unsure which error is worse for the bank. To properly continue your work, it is important to define with the client which problem exactly to solve and, therefore, which error to minimize. Some options could be:Catch every loan that will defaultMake sure the model does not classify a performing loan as a defaulted loanSome kind of weighted average between both of themHave a look at the right chart above to see how it could look like.Data Science ExplanationTo generate predictions, you have to train a model on the given data. To tell the model how well it performed and to punish it for mistakes, it is necessary to define an error metric. The choice of the error metric always depends on the business case. From a technical point of view, it is possible to model nearly every business case, however, there are four metrics that are used in most classification problems.    This metric measures, as the name suggests, how accurate the model can predict the loan status. While this is the most basic metric one can think of, it’s also a dangerous one. Let’s say the bank tells us that roughly 5% of the loans on the balance sheet default. If, for some reason, our model never predicts defaults. In other words, the model classifies every loan as a non-defaulting loan. The accuracy is immediately 95/100 = 95%. For datasets where the classes are highly imbalanced, it is usually a good idea to discard accuracy.    Optimizing the machine learning algorithm for recall would ensure that the algorithm catches as many defaulted loans as possible. On the flip side, an algorithm that predicts perfectly all defaulted loans as a default is often the result that the algorithm predicts too many loans as defaulted. Many loans that are not going to default are also flagged as default.    High precision ensures that all of the loans the algorithm flags as a default are classified correctly. This is done at the expense of the overall amount of loans which are flagged as default. Therefore, it might not be possible to flag every loan which is going to default as a default, but the loans which are flagged as defaults are most likely really going to default.    Empirically speaking, an increase in recall is almost always associated with a decrease in precision and vice versa. Often, it is desired to balance precision and recall somehow. This can be done with the F-beta score.Scenario Regression ProblemManagement ViewDuring the phone conference (same one as in the classification scenario), the decision-makers from the bank announced that they want to predict the overall default trends. While that’s already important information, you evaluate with the client what exactly their business need is. At the end you’ll end up with a list of requirements:Manager A: It’s important to match the overall trend as close as possible.Manager B: During normal times, I won’t pay too much attention to the model. However, it is absolute necessary that the model performs well in extreme market situations.Manager C: To make it as easy and convenient to use as possible and to be able to explain it to the regulating agency, it has to be as explainable as possible.Data Science ViewSimilar to the last scenario, there is again a tradeoff. It is a business problem to define which error is worse. Is every deviation from the ground truth equally bad? Is a certain stability of the prediction error important? Does the client care about the volatility of the forecast? Does a baseline exists?Have a look at the left chart above to see how it could look like.Data Science ExplanationOnce again, there are several metrices one can choose from. The best metric always depends on the business need. Here are the most common ones:    The Mean Absolute Error (MAE) calculates, as the name suggests, how far the predictions are off in absolute terms. While the number is easy to interpret, it treats every deviation in the same way. On a 100-day time interval, being every day off by 1 unit is the same as predicting everything, every day right but being one day off by 100 units.    The Mean Squared Error (MSE) also calculates the difference between the actual and the predicted output. This time, the deviation is weighted. Extreme values are worse compared to many small errors.    The  compares the model to evaluate against a simple baseline model. The advantage is that the output is easy to interpret. A value of 1 describes the perfect model, while a value close to 0 (or even negative) describes a model with room for improvement. This metric is commonly used among economists and econometricians and, therefore, in some industries a metric to consider. However, it is also relatively easy to get a high , which makes it hard to compare.    The Mean Absolute Percentage Error (MAPE) measures the absolute deviation from the predicted values. On the contrary to the MAE, the MAPE displays them in relative terms, which makes it very easy to interpret and to compare. The MAPE has its own set of drawbacks and caveats. Fortunately, my colleague Jan already wrote an article about it. Check it out if you want to learn more about it hereConclusionIn either one of the cases, the classification or the regression case, the “right” answer to the problem depends on how the problem is actually defined. Before applying the latest machine learning algorithm, it is crucial that the business question is well defined. A strong collaboration with the client team is necessary and is the key to achieving the best result for the client. There is no one-size-fits-all data science solution. Even though the underlying problem is the same for every stakeholder in the bank, it might be worth it to train several models for every department. It all boils down to the business needs!We still haven’t covered several other problems, which might arise in subsequent steps. How is the default of a loan defined? What is the prediction horizon? Do we have enough data to cover all business cycles? Is the model just used internally or do we have to explain the model to a regulating agency? Should we optimize the model for some kind of internal resource constraints? To discuss this and more, feel free to reach out to me at dominique.lade@statworx.com or send me a message via LinkedIn.Über den AutorDominique LadeI am a data scientist at STATWORX. I enjoy the whole journey from defining the business problem till the final product is delivered to the client. If it is in the context of financial data – even better..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/whats-your-problem-framing-data-science-questions-the-right-way/;Statworx;  Dominique Lade
  29. April 2020;Movie Recommendation With Recommenderlab;"Because You Are Interested In Data Science, You Are Interested In This Blog PostIf you love streaming movies and tv series online as much as we do here at STATWORX, you’ve probably stumbled upon recommendations like „Customers who viewed this item also viewed…“ or „Because you have seen …, you like …“. Amazon, Netflix, HBO, Disney+, etc. all recommend their products and movies based on your previous user behavior – But how do these companies know what their customers like? The answer is collaborative filtering.In this blog post, I will first explain how collaborative filtering works. Secondly, I’m going to show you how to develop your own small movie recommender with the R package recommenderlab and provide it in a shiny application.Different ApproachesThere are several approaches to give a recommendation. In the user-based collaborative filtering (UBCF), the users are in the focus of the recommendation system. For a new proposal, the similarities between new and existing users are first calculated. Afterward, either the n most similar users or all users with a similarity above a specified threshold are consulted. The average ratings of the products are formed via these users and, if necessary, weighed according to their similarity. Then, the x highest rated products are displayed to the new user as a suggestion.For the item-based collaborative filtering IBCF, however, the focus is on the products. For every two products, the similarity between them is calculated in terms of their ratings. For each product, the k most similar products are identified, and for each user, the products that best match their previous purchases are suggested.Those and other collaborative filtering methods are implemented in the recommenderlab package:ALS_realRatingMatrix: Recommender for explicit ratings based on latent factors, calculated by alternating least squares algorithm.ALS_implicit_realRatingMatrix: Recommender for implicit data based on latent factors, calculated by alternating least squares algorithm.IBCF_realRatingMatrix: Recommender based on item-based collaborative filtering.LIBMF_realRatingMatrix: Matrix factorization with LIBMF via package recosystem.POPULAR_realRatingMatrix: Recommender based on item popularity.RANDOM_realRatingMatrix: Produce random recommendations (real ratings).RERECOMMEND_realRatingMatrix: Re-recommends highly-rated items (real ratings).SVD_realRatingMatrix: Recommender based on SVD approximation with column-mean imputation.SVDF_realRatingMatrix: Recommender based on Funk SVD with gradient descend.UBCF_realRatingMatrix: Recommender based on user-based collaborative filtering.Developing your own Movie RecommenderDatasetTo create our recommender, we use the data from movielens. These are film ratings from 0.5 (= bad) to 5 (= good) for over 9000 films from more than 600 users. The movieId is a unique mapping variable to merge the different datasets.head(movie_data)  movieId                              title                                      genres1       1                   Toy Story (1995) Adventure|Animation|Children|Comedy|Fantasy2       2                     Jumanji (1995)                  Adventure|Children|Fantasy3       3            Grumpier Old Men (1995)                              Comedy|Romance4       4           Waiting to Exhale (1995)                        Comedy|Drama|Romance5       5 Father of the Bride Part II (1995)                                      Comedy6       6                        Heat (1995)                       Action|Crime|Thrillerhead(ratings_data)  userId movieId rating timestamp1      1       1      4 9649827032      1       3      4 9649812473      1       6      4 9649822244      1      47      5 9649838155      1      50      5 9649829316      1      70      3 964982400To better understand the film ratings better, we display the number of different ranks and the average rating per film. We see that in most cases, there is no evaluation by a user. Furthermore, the average ratings contain a lot of „smooth“ ranks. These are movies that only have individual ratings, and therefore, the average score is determined by individual users.# ranting_vector0         0.5    1      1.5    2      2.5   3      3.5    4       4.5   55830804   1370   2811   1791   7551   5550  20047  13136  26818   8551  13211In order not to let individual users influence the movie ratings too much, the movies are reduced to those that have at least 50 ratings.##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. ##   2.208   3.444   3.748   3.665   3.944   4.429Under the assumption that the ratings of users who regularly give their opinion are more precise, we also only consider users who have given at least 50 ratings. For the films filtered above, we receive the following average ratings per user:You can see that the distribution of the average ratings is left-skewed, which means that many users tend to give rather good ratings. To compensate for this skewness, we normalize the data.ratings_movies_norm &lt;- normalize(ratings_movies)Model Training and EvaluationTo train our recommender and subsequently evaluate it, we carry out a 10-fold cross-validation. Also, we train both an IBCF and a UBCF recommender, which in turn calculate the similarity measure via cosine similarity and Pearson correlation. A random recommendation is used as a benchmark. To evaluate how many recommendations can be given, different numbers are tested via the vector n_recommendations.eval_sets &lt;- evaluationScheme(data = ratings_movies_norm,                              method = ""cross-validation"",                              k = 10,                              given = 5,                              goodRating = 0)models_to_evaluate &lt;- list(  `IBCF Cosinus` = list(name = ""IBCF"",                         param = list(method = ""cosine"")),  `IBCF Pearson` = list(name = ""IBCF"",                         param = list(method = ""pearson"")),  `UBCF Cosinus` = list(name = ""UBCF"",                        param = list(method = ""cosine"")),  `UBCF Pearson` = list(name = ""UBCF"",                        param = list(method = ""pearson"")),  `Zufälliger Vorschlag` = list(name = ""RANDOM"", param=NULL))n_recommendations &lt;- c(1, 5, seq(10, 100, 10))list_results &lt;- evaluate(x = eval_sets,                          method = models_to_evaluate,                          n = n_recommendations)We then have the results displayed graphically for analysis.We see that the best performing model is built by using UBCF and the Pearson correlation as a similarity measure. The model consistently achieves the highest true positive rate for the various false-positive rates and thus delivers the most relevant recommendations. Furthermore, we want to maximize the recall, which is also guaranteed at every level by the UBCF Pearson model. Since the n most similar users (parameter nn) are used to calculate the recommendations, we will examine the results of the model for different numbers of users.vector_nn &lt;- c(5, 10, 20, 30, 40)models_to_evaluate &lt;- lapply(vector_nn, function(nn){  list(name = ""UBCF"",       param = list(method = ""pearson"", nn = vector_nn))})names(models_to_evaluate) &lt;- paste0(""UBCF mit "", vector_nn, ""Nutzern"")list_results &lt;- evaluate(x = eval_sets,                          method = models_to_evaluate,                          n = n_recommendations)ConclusionOur user based collaborative filtering model with the Pearson correlation as a similarity measure and 40 users as a recommendation delivers the best results. To test the model by yourself and get movie suggestions for your own flavor, I created a small Shiny App.However, there is no guarantee that the suggested movies really meet the individual taste. Not only is the underlying data set relatively small and can still be distorted by user ratings, but the tech giants also use other data such as age, gender, user behavior, etc. for their models.But what I can say is: Data Scientists who read this blog post also read the other blog posts by STATWORX.Shiny-AppHere you can find the Shiny App. To get your own movie recommendation, select up to 10 movies from the dropdown list, rate them on a scale from 0 (= bad) to 5 (= good) and press the run button. Please note that the app is located on a free account of shinyapps.io. This makes it available for 25 hours per month. If the 25 hours are used and therefore the app is this month no longer available, you will find the code here to run it on your local RStudio.Über den AutorAndreas VoglI am a data scientist at STATWORX who likes to make smart data-driven solutions and to put ML models into production to solve real-world problems..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/movie-recommendation-with-recommenderlab/;Statworx;  Andreas Vogl
  15. April 2020;Airflow for Data Scientists;"Getting the data in the quantity, quality and format you need is often the most challenging part of data science projects. But it’s also one, if not the most important part. That’s why my colleagues and I at STATWORX tend to spend a lot of time setting up good ETL processes. Thanks to frameworks like Airflow this isn’t just a Data Engineer prerogative anymore. If you know a bit of SQL and Python, you can orchestrate your own ETL process like a pro. Read on to find out how! ETL does not stand for Extraterrestrial LifeAt least not in Data Science and Engineering.ETL stands for Extract, Transform, Load and describes a set of database operations. Extracting means we read the data from one or more data sources. Transforming means we clean, aggregate or combine the data to get it into the shape we want. Finally, we load it to a destination database.Does your ETL process consist of Karen sending you an Excel sheet that you can spend your workday just scrolling down? Or do you have to manually query a database every day, tweaking your SQL queries to the occasion? If yes, venture a peek at Airflow.How Airflow can help with ETL processesAirflow is a python based framework that allows you to programmatically create, schedule and monitor workflows. These workflows consist of tasks and dependencies that can be automated to run on a schedule. If anything fails, there are logs and error handling facilities to help you fix it.Using Airflow can make your workflows more manageable, transparent and efficient. And yes, I’m talking to you fellow Data Scientists! Getting access to up-to-date, high-quality data is far too important to leave it only to the Data Engineers ?? (we still love you). The point is, if you’re working with data, you’ll profit from knowing how to wield this powerful tool.How Airflow worksTo learn more about Airflow, check out this blog post from my colleague Marvin. It will get you up to speed quickly. Marvin explains in more detail how Airflow works and what advantages/disadvantages it has as a workflow manager. Also, he has an excellent quick-start guide with Docker. What matters to us is knowing that Airflow’s based on DAGs or Directed Acyclic Graphs that describe what tasks our workflow consists of and how these are connected.Note that in this tutorial we’re not actually going to deploy the pipeline. Otherwise, this post would be even longer. And you probably have friends and family that like to see you. So today is all about creating a DAG step by step. If you care more about the deployment side of things, stay tuned though! I plan to do a step by step guide of how to do that in the next post. As a small solace, know that you can test every task of your workflow with:airflow test your dag id your task id execution dateThere are more options, but that’s all we need for now.What you need to follow this tutorialThis tutorial shows you how you can use Airflow in combination with BigQuery and Google Cloud Storage to run a daily ETL process. So what you need is:A Google Cloud accountA python environment with AirflowA CSV file that I prepared for this tutorialIf you already have a Google Cloud account, you can hit the ground running! If not, consider opening one. You do need to provide a credit card. But don’t worry, this tutorial won’t cost you. If you sign up new, you get a free yearly trial period. But even if that one’s expired, we’re staying well within the bounds of Google’s Always Free Tier. Finally, it helps if you know some SQL. I know it’s something most Data Scientists don’t find too sexy, but the more you use it the more you like it. I guess it’s like the orthopedic shoes of Data Science. A bit ugly sure, but unbeatable in what it’s designed for. If you’re not familiar with SQL or dread it like the plague, don’t sweat it. Each query’s name says what it does.What BigQuery and Google Cloud Storage areBigQuery and Cloud Storage are some of the most popular products of the Google Cloud Platform (GCP). BigQuery is a serverless cloud data warehouse that allows you to analyze up to petabytes of data at high speeds. Cloud Storage, on the other hand, is just that: a cloud-based object storage. Grossly simplified, we use BigQuery as a database to query and Cloud Storage as a place to save the results. In more detail, our ETL process:checks for the existence of data in BigQuery and Google Cloud Storagequeries a BigQuery source table and writes the result to a tableingests the Cloud Storage data into another BigQuery tablemerges the two tables and writes the result back to Cloud Storage as a CSVConnecting Airflow to these servicesIf you set up a Google Cloud account you should have a JSON authentication file. I suggest putting this in your home directory. We use this file to connect Airflow to BigQuery and Cloud Storage. To do this, just copy and paste these lines in your terminal, substituting your project ID and JSON path. You can read more about connections here.# for bigqueryairflow connections -d --conn_id bigquery_defaultairflow connections -a --conn_id bigquery_default --conn_uri 'google-cloud-platform://:@:?extra__google_cloud_platform__project=YOUR PROJECT ID&amp;extra__google_cloud_platform__key_path=PATH TO YOUR JSON'# for google cloud storageairflow connections -d --conn_id google_cloud_defaultairflow connections -a --conn_id google_cloud_default --conn_uri 'google-cloud-platform://:@:?extra__google_cloud_platform__project=YOUR PROJECT ID&amp;extra__google_cloud_platform__key_path=PATH TO YOUR JSON'Writing our DAGTask 0: Setting the start date and the schedule intervalWe are ready to define our DAG! This DAG consists of multiple tasks or things our ETL process should do. Each task is instantiated by a so-called operator. Since we’re working with BigQuery and Cloud Storage, we take the appropriate Google Cloud Platform (GCP) operators.Before defining any tasks, we specify the start date and schedule interval. The start date is the date when the DAG first runs. In this case, I picked February 20th, 2020. The schedule interval is how often the DAG runs, i.e. on what schedule. You can use cron notation here, a timedelta object or one of airflow’s cron presets (e.g. ‚@daily‘).Tip: you normally want to keep the start date static to avoid unpredictable behavior (so no datetime.now() shenanigans even though it may seem tempting).# set start date and schedule intervalstart_date = datetime(2020, 2, 20)schedule_interval = timedelta(days=1)You find the complete DAG file on our STATWORX Github. There you also see all the config parameters I set, e.g., what our project, dataset, buckets, and tables are called, as well as all the queries. We gloss over it here, as it’s not central to understanding the DAG.Task 1: Check that there is data in BigQueryWe set up our DAG taking advantage of python’s context manager. You don’t have to do this, but it saves some typing. A little detail: I’m setting catchup=False because I don’t want Airflow to do a backfill on my data.# write dagwith DAG(dag_id='blog', default_args=default_args, schedule_interval=schedule_interval, catchup=False) as dag:    t1 = BigQueryCheckOperator(task_id='check_bq_data_exists',                               sql=queries.check_bq_data_exists,                               use_legacy_sql=False)We start by checking if the data for the date we’re interested in is available in the source table. Our source table, in this case, is the Google public dataset bigquery-public-data.austin_waste.waste_and_diversion. To perform this check we use the aptly named BigQueryCheckOperator and pass it an SQL query.If the check_bq_data_exists query returns even one non-null row of data, we consider it successful and the next task can run. Notice that we’re making use of macros and Jinja templating to dynamically insert dates into our queries that are rendered at runtime. check_bq_data_exists = """"""SELECT load_idFROM `bigquery-public-data.austin_waste.waste_and_diversion`WHERE report_date BETWEEN DATE('{{ macros.ds_add(ds, -365) }}') AND DATE('{{ ds }}')""""""Task 2: Check that there is data in Cloud StorageNext, let’s check that the CSV file is in Cloud Storage. If you’re following along, just download the file from the STATWORX Github and upload it to your bucket. We pretend that this CSV gets uploaded to Cloud Storage by another process and contains data that we need (in reality I just extracted it from the same source table).So we don’t care how the CSV got into the bucket, we just want to know: is it there? This can easily be verified in Airflow with the GoogleCloudStorageObjectSensor which checks for the existence of a file in Cloud Storage. Notice the indent because it’s still part of our DAG context. Defining the task itself is simple: just tell Airflow which object to look for in your bucket.    t2 = GoogleCloudStorageObjectSensor(task_id='check_gcs_file_exists',                                        bucket=cfg.BUCKET,                                        object=cfg.SOURCE_OBJECT)Task 3: Extract data and save it to BigQueryIf the first two tasks succeed, then all the data we need is available! Now let’s extract some data from the source table and save it to a new table of our own. For this purpose, there’s none better than the BigQueryOperator.    t3 = BigQueryOperator(task_id='write_weight_data_to_bq',                          sql=queries.write_weight_data_to_bq,                          destination_dataset_table=cfg.BQ_TABLE_WEIGHT,                          create_disposition='CREATE_IF_NEEDED',                          write_disposition='WRITE_TRUNCATE',                          use_legacy_sql=False)This operator sends a query called write_weight_data_to_bq to BigQuery and saves the result in a table specified by the config parameter cfg.BQ_TABLE_WEIGHT. We can also set a create and write disposition if we so choose. The query itself pulls the total weight of dead animals collected every day by Austin waste management services for a year. If the thought of possum pancakes makes you queasy, just substitute ‚RECYCLING – PAPER‘ for the TYPE variable in the config file.Task 4: Ingest Cloud Storage data into BigQueryOnce we’re done extracting the data above, we need to get the data that’s currently in our Cloud Storage bucket into BigQuery as well. To do this, just tell Airflow what (source) object from your Cloud Storage bucket should go to which (destination) table in your BigQuery dataset. Tip: You can also specify a schema at this step, but I didn’t bother since the autodetect option worked well.    t4 = GoogleCloudStorageToBigQueryOperator(task_id='write_route_data_to_bq',                                              bucket=cfg.BUCKET,                                              source_objects=cfg.SOURCE_OBJECT,                                              field_delimiter=';',                                              destination_project_dataset_table=cfg.BQ_TABLE_ROUTE,                                              create_disposition='CREATE_IF_NEEDED',                                              write_disposition='WRITE_TRUNCATE',                                              skip_leading_rows=1)Task 5: Merge BigQuery and Cloud Storage dataNow we have both the BigQuery source table extract and the CSV data from Cloud Storage in two separate BigQuery tables. Time to merge them! How do we do this? The BigQueryOperator is our friend here. We just pass it a SQL query that specifies how we want the tables merged. By specifying the destination_dataset argument, it’ll put the result into a table that we choose.    t5 = BigQueryOperator(task_id='prepare_and_merge_data',                          sql=queries.prepare_and_merge_data,                          use_legacy_sql=False,                          destination_dataset_table=cfg.BQ_TABLE_MERGE,                          create_disposition='CREATE_IF_NEEDED',                          write_disposition='WRITE_TRUNCATE')Click below if you want to see the query. I know it looks long and excruciating, but trust me, there are Tinder dates worse than this (‚So, uh, do you like SQL?‘ – ‚Which one?‘). If he/she follows it up with Lord of the Rings or ‚Yes‘, propose!What is it that this query is doing? Let’s recap: We have two tables at the moment. One is basically a time series on how much dead animal waste Austin public services collected over the course of a year. The second table contains information on what type of routes were driven on those days. As if this pipeline wasn’t weird enough, we now also want to know what the most common route type was on a given day. So we start by counting what route types were recorded on a given day. Next, we use a window function (... OVER (PARTITION BY ... ORDER BY ...)) to find the route type with the highest count for each day. In the end, we pull it out and using the date as a key, merge it to the table with the waste info.prepare_and_merge_data = """"""WITHsimple_route_counts AS (SELECT report_date,       route_type,       count(route_type) AS countFROM `my-first-project-238015.waste.route` GROUP BY report_date, route_type),max_route_counts AS (SELECT report_date,       FIRST_VALUE(route_type) OVER (PARTITION BY report_date ORDER BY count DESC) AS top_route,       ROW_NUMBER() OVER (PARTITION BY report_date ORDER BY count desc) AS row_numberFROM simple_route_counts),top_routes AS (SELECT report_date AS date,       top_route,FROM max_route_countsWHERE row_number = 1)SELECT a.date,       a.type,       a.weight,       b.top_routeFROM `my-first-project-238015.waste.weight` aLEFT JOIN top_routes bON a.date = b.dateORDER BY a.date DESC""""""Task 6: Export result to Google Cloud StorageLet’s finish off this process by exporting our result back to Cloud Storage. By now, you’re probably guessing what the right operator is called. If you guessed BigQueryToCloudStorageOperator, you’re spot on. How to use it though? Just specify what the source table and the path (uri) to the Cloud Storage bucket are called.    t6 = BigQueryToCloudStorageOperator(task_id='export_results_to_gcs',                                        source_project_dataset_table=cfg.BQ_TABLE_MERGE,                                        destination_cloud_storage_uris=cfg.DESTINATION_URI,                                        export_format='CSV')The only thing left to do now is to determine how the tasks relate to each other, i.e. set the dependencies. We can do this using the &gt;&gt; notation which I find more readable than set_upstream() or set_downstream(). But take your pick.    t1 &gt;&gt; t2 &gt;&gt; t3, t4 &gt;&gt; t5 &gt;&gt; t6The notation above says: if data is available in the BigQuery source table, check next if data is also available in Cloud Storage. If so, go ahead, extract the data from the source table and save it to a new BigQuery table. In addition, transfer the CSV file data from Cloud Storage into a separate BigQuery table. Once those two tasks are done, merge the two newly created tables. At last, export the merged table to Cloud Storage as a CSV.ConclusionThat’s it! Thank you very much for sticking with me to the end! Let’s wrap up what we did: we wrote a DAG file to define an automated ETL process that extracts, transforms and loads data with the help of two Google Cloud Platform services: BigQuery and Cloud Storage. Everything we needed was some Python and SQL code.What’s next? There’s much more to explore, from using the Web UI, monitoring your workflows, dynamically creating tasks, orchestrating machine learning models, and, and, and. We barely scratched the surface here.So check out Airflow’s official website to learn more. For now, I hope you got a better sense of the possibilities you have with Airflow and how you can harness its power to manage and automate your workflows. Referenceshttps://airflow.apache.org/docs/stable/https://cloud.google.com/blog/products/gcp/how-to-aggregate-data-for-bigquery-using-apache-airflowhttps://blog.godatadriven.com/zen-of-python-and-apache-airflowhttps://diogoalexandrefranco.github.io/about-airflow-date-macros-ds-and-execution-date/https://www.astronomer.io/guides/templating/https://medium.com/datareply/airflow-lesser-known-tips-tricks-and-best-practises-cf4d4a90f8fÜber den AutorManuel TilgnerI am a data scientist at STATWORX, and I enjoy making data make sense. Why? Because there's something magical about turning a jumble of numbers into insights. In my free time, I love wandering through the forest or playing in the local big band..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/airflow-for-data-scientists/;Statworx;  Manuel Tilgner
  8. April 2020;Permafrost Hackathon – Unsupervised Movement Detection;"I love riddles and all the problems that I encounter as a STATWORX data scientist I perceive as riddles. There are times, where my urge for riddles is so strong that I participate in hackathons to satisfy it. Hackathons are events, where people with different backgrounds meet and work on a specific challenge for a limited time. In this blog post, I will present my impressions and results from the Permafrost Hackathon, that took place at the ETH Zurich end of November 2019.Figure 1: Hustling at the HackathonHere is an OverviewThe hackathon organizers are part of the PermaSense project, where researchers from geospatial to engineering disciplines work together. The objective is to gather data in high quality from steep bedrock permafrost in an Alpine environment on the Hörnli ridge of the Matterhorn, an iconic Swiss Mountain. The data is obtained using a network of wireless sensors, which record among other things temperature, wind speed, and seismic activity. Below is an illustration of different sensor setups, which measure temperature and fracture resistance in rocks.Figure 2: Sensor SetupFurthermore, a picture of the mountain wall, where the sensors are installed, is taken every four minutes. For more information about the PermaSense project check out this link. All the data is open-source and can be found here.The objective of the hackathon was to find new ideas and ways to utilize this data – which is where the creativity of the hackathon participants comes into play.Let’s dive into itThe teams were build randomly over delicious finger food and few beers. My team – Team Aroma (named after a famous falafel place in Frankfurt) – consisted of Emma, Luca, Tiziano, Lisa (honorary member), and myself. All of us came from different areas. We covered the fields of Computer Science, Electrical Engineering (Signal Processing), Predictive Forecasting, and Physics.The initial idea of my team was to build some kind of data exploration engine, which incorporates the different data sources at the same time to get a better understanding and overview of the data. However, once we started with the project we focused much more on the images than the other sources and came up with two interesting approaches.Our approaches had the same goal – to detect mountaineers. But the images did not have labels, therefore, we could not turn to supervised learning methods. Alternatively, we came up with some other ways to detect, whether there are mountaineers on a certain image using unsupervised methods. One approach based on the fact that mountaineers wear colorful jackets. Hence, we created color filters for three different colors (red, yellow, and blue). The color filter worked surprisingly well but was quite prone to noise. E.g. during summer the red color filter has a hard time to distinguish rocks and mountaineers since some rocks are kind of reddish.The second approach – which is the focus of this blogpost – used the idea of subtracting sequential images. This results in a so-called difference image, where changes between the images are highlighted. Formally, this is depicted in the equation below.     This works just in special cases, e.g. where the picture depicts the same landscape.Detecting MountaineersBelow you can see some images of the mentioned mountain wall. Although, tough to spot, but there are some mountaineers in action. Now, images A and B are shown in color. Image C is the difference image of A and B (before subtraction the images are transformed into greyscale). Image A in the first row was taken at 04:32 in the morning, image B in the first row at 04:36 – in four minute intervals pictures are taken of the mountain wall. Similarly, in the second row image A is the same as image B in the first row (04:36), whereas image B in the second row was taken at 04:40.Keep in mind that greyscale images are just matrices with numbers signaling the intensity of brightness. The values are integers from 0 to 255, where zero represents complete darkness and the highest value of brightness depicts the color white. Hence, if two identical images are subtracted the difference image will be completely black – since the resulting matrix is populated just with zeros. But if the images are similar with just small differences, e.g. on one image there is a mountaineer and on the other, the mountaineer is gone, then the location of the change will be highlighted in white on the difference image.Figure 3: Example with MountaineersThat’s a nice way to spot changes, right? However, changes that we are not particularly interested in, are highlighted as well, especially, changes in light. When sunshine falls slightly different than in the previous picture it results in noise in the difference image. Noise can be identified by the white sprinkle and is also called salt noise.Changes like the sunrise (shown in figure 2) have a greater impact and will, thus, be highlighted when using our approach.Figure 4: Example with Light AlterationAfter the initial successes with the difference image approach, we tackled the issues described above. Luckily, we found a paper of Turgay Celik 1. He presents an approach to detect changes in satellite images using unsupervised learning methods, where the starting point is the difference image as well. I won’t dive into the technical details in this blog post – but I will present to you the results from our implementation. If you are curious, visit my github repo for the code or check out the paper. We implemented our movement detection in Python.Figure 5: Difference Image vs Movement Detection IIn the left panel of the figure above the difference image from the first example is depicted. The right panel shows the movement detection, which is based on the difference image. The salt noise is gone and the movement of the mountaineers is correctly identified. Note that the cloud movement in the upper left corner of the left panel is not highlighted in the right panel. These changes are not significant enough compared to the movements of the mountaineers. This is a great example where the movement detection technique works.However, the example below shows a case where the movement detection fails. Particularly, there are no significant movements. But areas, where the light changed a bit, are classified as changes.Figure 6: Difference Image vs Movement Detection IIRefinementsThe hackathon is over and we didn’t further work on our project. But there is plenty of room for improvement. As shown  the implementation works, however, it is still too sensitive to noise. To develop a better system is a research question in itself. Personally, I think comparing the difference images might be a good starting point. Clustering them into images with changes and images without might already help (e.g. using tSNE). The system would then run just on those cases, where there were changes.RoundupI am glad I attended the Permafrost hackathon and got to work on some ideas with my team. The mentioned technique is very interesting and according to the organizers, they will invest and further develop this approach. In May there will be a field trip with all interested participants to the Hörnli ridge with a presentation of new results stemming from the hackathon.Each team presented what they were working on during the hackathon. It was amazing to see all the different results. One team created a sleek looking application which the mountaineers could use to get real-time information about the weather including a one day forecast. Another team focused on predicting seismic activity and the likelihood of a rockfall. Thus, there were many great ideas for utilizing the data.That’s it from me. I hope you find the topic as interesting as I do. If there are any questions or remarks you are welcome to shoot me an email.ReferencesPermaSense ProjectDataChange Detection PaperÜber den AutorFran PericBeing confronted with challenging riddles is what I like about my job as a data scientist at STATWORX. Lets get creative!.e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/permafrost-hackathon-unsupervised-movement-detection/;Statworx;  Fran Peric
  1. April 2020;Making Of: A Free API For COVID-19 Data;"Recently, some colleagues and I attended the 2-day COVID-19 hackathon #wirvsvirus, organized by the German government. Thereby, we’ve developed a great application for simulating COVID-19 curves based on estimations of governmental measure effectiveness (FlatCurver). As there are many COVID-related dashboards and visualizations out there, I thought that gathering the underlying data from a single point of truth would be a minor issue. However, I soon realized that there are plenty of different data sources, mostly relying on the Johns Hopkins University COVID-19 case data. At first, I thought that’s great, but at a second glance, I revised my initial thought. The JHU datasets have some quirky issues to it that makes it a bit cumbersome to prepare and analyze it:weird column names including special characterscountries and states „in the mix“wide format, quite unhandy for data analysisimport problems due to line break issuesetc.For all of you, who have been or are working with COVID-19 time series data and want to step up your data-pipeline game, let me tell you: we have an API for that! The API uses official data from theEuropean Centre for Disease Prevention and Controland delivers a clear and concise data structure for further processing, analysis, etc.Overview of our COVID-19 APIOur brand new COVID-19-API brings you the latest case number time series right into your application or analysis, regardless of your development environment. For example, you can easily import the data into Python using the requests package:import requestsimport jsonimport pandas as pd# POST to APIpayload = {'country': 'Germany'} # or {'code': 'DE'}URL = 'https://api.statworx.com/covid'response = requests.post(url=URL, data=json.dumps(payload))# Convert to data framedf = pd.DataFrame.from_dict(json.loads(response.text))Or if you’re an R aficionado, use httr and jsonlite to grab the lastest data and turn it into a cool plot.library(httr)library(dplyr)library(jsonlite)library(ggplot2)# Post to APIpayload &lt;- list(code = ""ALL"")response &lt;- httr::POST(url = ""https://api.statworx.com/covid"",                       body = toJSON(payload, auto_unbox = TRUE), encode = ""json"")# Convert to data framecontent &lt;- rawToChar(response$content)df &lt;- data.frame(fromJSON(content))# Make a cool plotdf %&gt;%  mutate(date = as.Date(date)) %&gt;%  filter(cases_cum &gt; 100) %&gt;%  filter(code %in% c(""US"", ""DE"", ""IT"", ""FR"", ""ES"")) %&gt;%  group_by(code) %&gt;%  mutate(time = 1:n()) %&gt;%  ggplot(., aes(x = time, y = cases_cum, color = code)) +  xlab(""Days since 100 cases"") + ylab(""Cumulative cases"") +  geom_line() + theme_minimal()Developing the API using FlaskDeveloping a simple web app using Python is straightforward using Flask. Flask is a web framework for Python. It allows you to create websites, web applications, etc. right from Python. Flask is widely used to develop web services and APIs. A simple Flask app looks something like this.from flask import Flaskapp = Flask(__name__)@app.route('/')def handle_request():  """""" This code gets executed """"""  return 'Your first Flask app!'In the example above, app.route decorator defines at which URL our function should be triggered. You can specify multiple decorators to trigger different functions for each URL. You might want to check out our code in the Github repository to see how we build the API using Flask.Deployment using Google Cloud RunDeveloping the API using Flask is straightforward. However, building the infrastructure and auxiliary services around it can be challenging, depending on your specific needs. A couple of things you have to consider when deploying an API:AuthentificationSecurityScalabilityLatency LoggingConnectivityWe’ve decided to use Google Cloud Run, a container-based serverless computing framework on Google Cloud. Basically, GCR is a fully managed Kubernetes service, that allows you to deploy scalable web services or other serverless functions based on your container. This is how our Dockerfile looks like.# Use the official image as a parent imageFROM python:3.7# Copy the file from your host to your current locationCOPY ./main.py /app/main.pyCOPY ./requirements.txt /app/requirements.txt# Set the working directoryWORKDIR /app# Run the command inside your image filesystemRUN pip install -r requirements.txt# Inform Docker that the container is listening on the specified port at runtime.EXPOSE 80# Run the specified command within the container.CMD ""python"", ""main.py""You can develop your container locally and then push it in to the container registry of your GCP project. To do so, you have to tag your local image using docker tag according to the following scheme: HOSTNAME/PROJECT-ID/IMAGE. The hostname is one of the following: gcr.io, us.gcr.io, eu.gcr.io, asia.gcr.io. Afterward, you can push using gcloud push, followed by your image tag. From there, you can easily connect the container to the Google Cloud Run service:When deploying the service, you can define parameters for scaling, etc. However, this is not in scope for this post. Furthermore, GCR allows custom domain mapping to functions. That’s why we have the neat API endpoint https://api.statworx.com/covid.ConclusionBuilding and deploying a web service is easier than ever. We hope that you find our new API useful for your projects and analyses regarding COVID-19. If you have any questions or remarks, feel free to contact us or to open an issue on Github. Lastly, if you make use of our free API, please add a link to our website, https://www.statworx.com to your project. Thanks in advance and stay healthy!Über den AutorSebastian HeinzI am the founder and CEO of STATWORX. I enjoy writing about machine learning and AI, especially about neural networks and deep learning. In my spare time, I love to cook, eat and drink as well as traveling the world..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/making-of-a-free-api-for-covid-19-data/;Statworx;  Sebastian Heinz
  26. März 2020;How To Build A Dashboard In Python – Plotly Dash Step-by-Step Tutorial;"From my experience here at STATWORX, the best way to learn something is by trying it out yourself – with a little help from a friend! In this article, I will focus on giving you a hands-on guide on how to build a dashboard in Python. As framework, we will be using Dash, and the goal is to create a basic dashboard with a dropdown and two reactive graphs:Developed as an open-source library by Plotly, the Python framework Dash is built on top of Flask, Plotly.js, and React.js. Dash allows the building of interactive web applications in pure Python and is particularly suited for sharing insights gained from data.In case you’re interested in interactive charting with Python, I highly recommend my colleague Markus‘ blog post Plotly – An Interactive Charting Library. For a general guide about basic visualization techniques, check out this great article by my colleague Vivian on Basic rules for good looking slides and dashboards.For our purposes, a basic understanding of HTML and CSS can be helpful. Nevertheless, I will provide you with external resources and explain every step thoroughly, so you’ll be able to follow the guide.Guide structure PrerequisitesLoad the StylesheetLoad the DataGetting Started – How to start a Dash app How a .css-files changes the layout of an appThe basics of an app’s layoutCharting in Dash – How to display a Plotly-FigureCreating a Dropdown MenuWorking with CallbacksHow to add interactive functionalities to your appImplementing CallbacksVisualize Callbacks – Callback GraphConclusionThe source code can be found on GitHub.Prerequisites The project comprises a style sheet called style.css, sample stock data stockdata2.csv and the actual Dash application app.pyLoad the Stylesheet If you want your dashboard to look like the one above, please download the file style.css from our STATWORX GitHub. That is completely optional and won’t affect the functionalities of your app. Our stylesheet is a customized version of the stylesheet used by the Dash Uber Rides Demo. Dash will automatically load any .css-file placed in a folder named assets.dashapp    |--assets        |-- style.css    |--data        |-- stockdata2.csv    |-- app.pyThe documentation on external resources in dash can be found here.Load the Data Feel free to use the same data we did (stockdata2.csv), or any pick any data with the following structure:datestockvaluechange2007-01-03MSFT23.95070-0.16672007-01-03IBM80.517961.06912007-01-03SBUX16.149670.1134import pandas as pd# Load datadf = pd.read_csv('data/stockdata2.csv', index_col=0, parse_dates=True)df.index = pd.to_datetime(df'Date')Getting Started – How to start a Dash app Back to Guide StructureAfter installing Dash (instructions can be found here), we are ready to start with the application. The following statements will load the necessary packages dash and dash_html_components. Without any layout defined, the app won’t start. An empty html.Div will suffice to get the app up and running.import dashimport dash_html_components as htmlIf you have already worked with the WSGI web application framework Flask, the next step will be very familiar to you, as Dash uses Flask under the hood.# Initialise the appapp = dash.Dash(__name__)# Define the appapp.layout = html.Div()# Run the appif __name__ == '__main__':    app.run_server(debug=True)How a .css-files changes the layout of an app The module dash_html_components provides you with several html components, also check out the documentation.Worth to mention is that the nesting of components is done via the children attribute.app.layout = html.Div(children=                      html.Div(className='row',  # Define the row element                               children=                                  html.Div(className='four columns div-user-controls'),  # Define the left element                                  html.Div(className='eight columns div-for-charts bg-grey')  # Define the right element                                  )                                )The first html.Div() has one child. Another html.Div named row, which will contain all our content. The children of row are four columns div-user-controls and eight columns div-for-charts bg-grey.The style for these div components come from our style.css.Now let’s first add some more information to our app, such as a title and a description. For that, we use the Dash Components H2 to render a headline and P to generate html paragraphs.  children =     html.H2('Dash - STOCK PRICES'),    html.P('''Visualising time series with Plotly - Dash'''),    html.P('''Pick one or more stocks from the dropdown below.''')Switch to your terminal and run the app with python app.py.The basics of an app’s layout Another nice feature of Flask (and hence Dash) is hot-reloading. It makes it possible to update our app on the fly without having to restart the app every time we make a change to our code.Running our app with debug=True also adds a button to the bottom right of our app, which lets us take a look at error messages, as well a Callback Graph. We will come back to the Callback Graph in the last section of the article when we’re done implementing the functionalities of the app.Charting in Dash – How to display a Plotly-Figure Back to Guide StructureWith the building blocks for our web app in place, we can now define a plotly-graph. The function dcc.Graph() from dash_core_components uses the same figure argument as the plotly package. Dash translates every aspect of a plotly chart to a corresponding key-value pair, which will be used by the underlying JavaScript library Plotly.js.In the following section, we will need the express version of plotly.py, as well as the Package Dash Core Components. Both packages are available with the installation of Dash.import dash_core_components as dccimport plotly.express as pxDash Core Components has a collection of useful and easy-to-use components, which add interactivity and functionalities to your dashboard.Plotly Express is the express-version of plotly.py, which simplifies the creation of a plotly-graph, with the drawback of having fewer functionalities.To draw a plot on the right side of our app, add a dcc.Graph() as a child to the html.Div() named eight columns div-for-charts bg-grey. The component dcc.Graph() is used to render any plotly-powered visualization. In this case, it’s figure will be created by px.line() from the Python package plotly.express. As the express version of Plotly has limited native configurations, we are going to change the layout of our figure with the method update_layout(). Here, we use rgba(0, 0, 0, 0) to set the background transparent. Without updating the default background- and paper color, we would have a big white box in the middle of our app. As dcc.Graph() only renders the figure in the app; we can’t change its appearance once it’s created.dcc.Graph(id='timeseries',          config={'displayModeBar': False},          animate=True,          figure=px.line(df,                         x='Date',                         y='value',                         color='stock',                         template='plotly_dark').update_layout(                                   {'plot_bgcolor': 'rgba(0, 0, 0, 0)',                                    'paper_bgcolor': 'rgba(0, 0, 0, 0)'})                                    )After Dash reload the application, you will end up in something like that: A dashboard with a plotted graph:Creating a Dropdown Menu Back to Guide StructureAnother core component is dcc.dropdown(), which is used – you’ve guessed it – to create a dropdown menu. The available options in the dropdown menu are either given as arguments or supplied by a function.For our dropdown menu, we need a function that returns a list of dictionaries. The list contains dictionaries with two keys, label and value. These dictionaries provide the available options to the dropdown menu. The value of label is displayed in our app. The value of value will be exposed for other functions to use, and should not be changed.If you prefer the full name of a company to be displayed instead of the short name, you can do so by changing the value of the key label to Microsoft. For the sake of simplicity, we will use the same value for the keys label and value.Add the following function to your script, before defining the app’s layout.# Creates a list of dictionaries, which have the keys 'label' and 'value'.def get_options(list_stocks):    dict_list =     for i in list_stocks:        dict_list.append({'label': i, 'value': i})    return dict_listWith a function that returns the names of stocks in our data in key-value pairs, we can now add dcc.Dropdown() from the Dash Core Components to our app. Add a html.Div() as child to the list of children of four columns div-user-controls, with the argument className=div-for-dropdown. This html.Div() has one child, dcc.Dropdown().We want to be able to select multiple stocks at the same time and a selected default value, so our figure is not empty on startup. Set the argument multi=True and chose a default stock for value. html.Div(className='div-for-dropdown',          children=              dcc.Dropdown(id='stockselector',                           options=get_options(df'stock'.unique()),                           multi=True,                           value=df'stock'.sort_values()0,                           style={'backgroundColor': '#1E1E1E'},                           className='stockselector')                    ,          style={'color': '#1E1E1E'})The id and options arguments in dcc.Dropdown() will be important in the next section. Every other argument can be changed. If you want to try out different styles for the dropdown menu, follow the link for a list of different dropdown menus.Working with Callbacks Back to Guide StructureHow to add interactive functionalities to your app Callbacks add interactivity to your app. They can take inputs, for example, certain stocks selected via a dropdown menu, pass these inputs to a function and pass the return value of the function to another component. We will write a function that returns a figure based on provided stock names. A callback will pass the selected values from the dropdown to the function and return the figure to a dcc.Grapph() in our app.At this point, the selected values in the dropdown menu do not change the stocks displayed in our graph. For that to happen, we need to implement a callback. The callback will handle the communication between our dropdown menu 'stockselector' and our graph 'timeseries'. We can delete the figure we have previously created, as we won’t need it anymore.We want two graphs in our app, so we will add another dcc.Graph() with a different id.Remove the figure argument from dcc.Graph(id='timeseries')Add another dcc.Graph() with className='change' as child to the html.Div() named eight columns div-for-charts bg-grey.dcc.Graph(id='timeseries', config={'displayModeBar': False})dcc.Graph(id='change', config={'displayModeBar': False})Callbacks add interactivity to your app. They can take Inputs from components, for example certain stocks selected via a dropdown menu, pass these inputs to a function and pass the returned values from the function back to components.In our implementation, a callback will be triggered when a user selects a stock. The callback uses the value of the selected items in the dropdown menu (Input) and passes these values to our functions update_timeseries() and update_change(). The functions will filter the data based on the passed inputs and return a plotly figure from the filtered data. The callback then passes the figure returned from our functions back to the component specified in the output.A callback is implemented as a decorator for a function. Multiple inputs and outputs are possible, but for now, we will start with a single input and a single output. We need the class dash.dependencies.Input and dash.dependencies.Output.Add the following line to your import statements.from dash.dependencies import Input, OutputInput() and Output() take the id of a component (e.g. in dcc.Graph(id='timeseries') the components id is 'timeseries') and the property of a component as arguments.Example Callback:# Update Time Series@app.callback(Output('id of output component', 'property of output component'),              Input('id of input component', 'property of input component'))def arbitrary_function(value_of_first_input):    '''    The property of the input component is passed to the function as value_of_first_input.    The functions return value is passed to the property of the output component.    '''    return arbitrary_outputIf we want our stockselector to display a time series for one or more specific stocks, we need a function. The value of our input is a list of stocks selected from the dropdown menu stockselector.Implementing Callbacks The function draws the traces of a plotly-figure based on the stocks which were passed as arguments and returns a figure that can be used by dcc.Graph(). The inputs for our function are given in the order in which they were set in the callback. Names chosen for the function’s arguments do not impact the way values are assigned.Update the figure time series:@app.callback(Output('timeseries', 'figure'),              Input('stockselector', 'value'))def update_timeseries(selected_dropdown_value):    ''' Draw traces of the feature 'value' based one the currently selected stocks '''    # STEP 1    trace =       df_sub = df    # STEP 2    # Draw and append traces for each stock    for stock in selected_dropdown_value:           trace.append(go.Scatter(x=df_subdf_sub'stock' == stock.index,                                 y=df_subdf_sub'stock' == stock'value',                                 mode='lines',                                 opacity=0.7,                                 name=stock,                                 textposition='bottom center'))      # STEP 3    traces = trace    data = val for sublist in traces for val in sublist    # Define Figure    # STEP 4    figure = {'data': data,              'layout': go.Layout(                  colorway=""#5E0DAC"", '#FF4F00', '#375CB1', '#FF7400', '#FFF400', '#FF0056',                  template='plotly_dark',                  paper_bgcolor='rgba(0, 0, 0, 0)',                  plot_bgcolor='rgba(0, 0, 0, 0)',                  margin={'b': 15},                  hovermode='x',                  autosize=True,                  title={'text': 'Stock Prices', 'font': {'color': 'white'}, 'x': 0.5},                  xaxis={'range': df_sub.index.min(), df_sub.index.max()},              ),              }    return figureSTEP 1A trace will be drawn for each stock. Create an empty list for each trace from the plotly figure.STEP 2Within the for-loop, a trace for a plotly figure will be drawn with the function go.Scatter().Iterate over the stocks currently selected in our dropdown menu, draw a trace, and append that trace to our list from step 1.STEP 3Flatten the tracesSTEP 4Plotly figures are dictionaries with the keys data and layout. The value of data is our flattened list with the traces we have drawn. The layout is defined with the plotly class go.Layout().Add the trace to our figureDefine the layout of our figureNow we simply repeat the steps above for our second graph. Just change the data for our y-Axis to change and slightly adjust the layout.Update the figure change:@app.callback(Output('change', 'figure'),              Input('stockselector', 'value'))def update_change(selected_dropdown_value):    ''' Draw traces of the feature 'change' based one the currently selected stocks '''    trace =     df_sub = df    # Draw and append traces for each stock    for stock in selected_dropdown_value:        trace.append(go.Scatter(x=df_subdf_sub'stock' == stock.index,                                 y=df_subdf_sub'stock' == stock'change',                                 mode='lines',                                 opacity=0.7,                                 name=stock,                                 textposition='bottom center'))    traces = trace    data = val for sublist in traces for val in sublist    # Define Figure    figure = {'data': data,              'layout': go.Layout(                  colorway=""#5E0DAC"", '#FF4F00', '#375CB1', '#FF7400', '#FFF400', '#FF0056',                  template='plotly_dark',                  paper_bgcolor='rgba(0, 0, 0, 0)',                  plot_bgcolor='rgba(0, 0, 0, 0)',                  margin={'t': 50},                  height=250,                  hovermode='x',                  autosize=True,                  title={'text': 'Daily Change', 'font': {'color': 'white'}, 'x': 0.5},                  xaxis={'showticklabels': False, 'range': df_sub.index.min(), df_sub.index.max()},              ),              }    return figureRun your app again. You are now able to select one or more stocks from the dropdown. For each selected item, a line plot will be generated in the graph. By default, the dropdown menu has search functionalities, which makes the selection out of many available options an easy task.Visualize Callbacks – Callback Graph With the callbacks in place and our app completed, let’s take a quick look at our callback graph. If you are running your app with debug=True, a button will appear in the bottom right corner of the app. Here we have access to a callback graph, which is a visual representation of the callbacks which we have implemented in our code. The graph shows that our components timeseries and change display a figure based on the value of the component stockselector. If your callbacks don’t work how you expect them to, especially when working on larger and more complex apps, this tool will come in handy.ConclusionLet’s recap the most important building blocks of Dash. Getting the App up and running requires just a couple lines of code. A basic understanding of HTML and CSS is enough to create a simple Dash dashboard. You don’t have to worry about creating interactive charts, Plotly already does that for you. Making your dashboard reactive is done via Callbacks, which are functions with the users‘ interaction as the input.If you liked this blog, feel free to contact me via LinkedIn or Email. I am curious to know what you think and always happy to answer any questions about data, my journey to data science, or the exciting things we do here at STATWORX.Thank you for reading!Über den AutorAlexander BlaufussI am a data scientist at STATWORX. Fascinated by the world of data, I am interested in Deep Learning, communicating the insights gained from data and the techniques for visualizing data. If you have any questions about my Blog or Data Science in general, you are welcome to contact me via Email or LinkedIn..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/how-to-build-a-dashboard-in-python-plotly-dash-step-by-step-tutorial/;Statworx;  Alexander Blaufuss
  19. März 2020;Why Is It Called That Way?! – Origin and Meaning of R Package Names;"When I started with R, I soon discovered that, more often than not, a package name has a particular meaning. For example, the first package I ever installed was foreign. The name corresponds to its ability to read and write data from other foreign psources to R. While this and many other names are rather straightforward, others are much less intuitive. The name of a package often conveys a story, which is inspired by a general property of its functions. And sometimes I just don’t get the deeper meaning, because English is not my native language. In this blog post, I will shed light on the wonderful world of package names. After this journey, you will not only admire the creativity of R package creators; you’ll also be king at your next class reunion! Or at least at the next R-Meetup.Before we start, and I know that you are eager to continue, I have two remarks about this article.  First: Sometimes, I refer to official explanations from the authors or other sources; other times, it’s just my personal explanation of why a package is called that way. So if you know better or otherwise, do not hesitate to contact me.  Second: There are currently 15,341 packages on CRAN, and I am sure there are a lot more naming mysteries and ingenuities to discover than any curious blog reader would like to digest in one sitting. Therefore, I focussed on the most famous packages and added some of my other preferences.But enough of the talking now, let’s start!dplyr (di??pla??)You might have noticed that many packages contain the string plyr, e.g. dbplyr, implyr, dtplyr, and so on. This homophone of pliers corresponds to its refining of base R apply-functions as part of the „split-apply-combine“ strategy. Instead of doing all steps for data analysis and manipulation at once, you split the problem into manageable pieces, apply your function to each piece, and combine everything together afterward. We see this approach in perfection when we use the pipe operator.The first part of each package just refers to the object it is applied upon. So the d stands for data frames, db for databases, im for Apache Impala, dt for data tables, and so on…Sources: Hadley Wickhamlubridate (?lu?br?de?t)This wonderful package makes it so easy and smooth to work with dates and times in R. You could say it goes like a clockwork. In German, there is a proverb with the same meaning  („Das läuft wie geschmiert“), that can literally be translated to: „It works as lubricated“ggplot2 (?i??i?pl?t tu?)Leland Wilkinson wrote a book in which he defined multiple components that a comprehensive plot is made of. You have to define the data you want to show, what kind of plot it should be, e.g., points or lines, the scales of the axes, the legend, axis titles, etc. These parts, he called them layers, should be built on top of each other. The title of this influential piece of paper is Grammar of Graphics. Once you got it, it enables you to build complex yet meaningful plots with concise styling across packages. That’s because its logic has also been used by many other packages like plotly, rBokeh, visNetwork, or apexcharter.Sources: ggplot2data.table (?de?t? ?te?bl) – logoOkay, full disclosure, I am a tidyverse guy, and one of my sons shall be named Hadley. At least one. However, this does not mean that I don’t appreciate the very powerful package data.table. Occasionally, I take the liberty and exploit its functions to improve the performance of my code (hello fread() and rbindlist()). Anyway, the name itself is pretty straightforward – but did you notice how cool the logo is?! Well, there is obviously the name „data.table“ and the square brackets that are fundamental in data.table syntax. Likewise, there is the assignment by reference operator, a.k.a. the walrus operator. „Wait, stop,“ your inner marine mammal researcher says, „isn’t this a sea lion on top there?!“ Yes indeed! The sea lion is used to highlight that it is an R package since, of course, it shouts R! R!.Source: Rdatatabletibble (t?bl)Regular base R data frames are nice, but did you ever print a data frame in the console, unaware that it is 10 million rows long? Good luck with interrupting R without quitting the whole session. That might be one of the reasons why the tidyverse uses another type of data frames: tibbles.The name tibble could just stem from its similar sound to table, but I suspect there is more to it than meets the eye. Did you ever hear the story about Tibbles and Stephen Island’s Wren? NO? Then let me take you to New Zealand, AD 1894.Between the northern and southern main islands of NZ, there is a small and uninhabited island: Stephen Island. Its rocks have been the downfall of many poor souls that tried to pass the Cook Strait. Therefore, it was decided to build a lighthouse as that ships shall henceforth pass safely and undamaged. Due to its isolation, Stephen Island was the only habitat for many rare species. One of these was Lyall’s wren, a small flightless passerine. It did not know any predators and lived its life in joy and harmony, until… The arrival of the first lighthouse keeper. His name was David Lyall and he was a man interested in natural history and, facing a long and lonely time on his own at Stephen Island, the owner of a cat. This cat was not satisfied by just comforting Mr. Lyall and enjoying beach walks. Shortly after his arrival, Mr. Lyall noticed the carcasses of little birds, seemingly slaughtered and dishonored by a fierce predator. Interested in biology as he was, he found out that these small birds were a distinct species. He preserved some carcasses in alcohol and sent them to a friend. This was in October 1894. A scientific article about the wren was published in an ornithology journal, soon making the specimen a sought-after collector’s item. The summer in New Zealand goes on and in February 1895, four bird-watchers arrived at Stephen Island. They were looking for this cute little wren and found… none. Within a few months, Mr. Lyalls hungry cat made the whole species go extinct. On March 16, 1895, the Christchurch newspaper The Press wrote: „there is very good reason to believe that the bird is no longer to be found on the island, and, as it is not known to exist anywhere else, it has apparently become quite extinct. This is probably a record performance in the way of extermination.“. The name of the cat? Tibbles.Sources:  Wikipedia; All About Birds; Oddity Central Indicator: the fridge of Hadley Wickham’s parentspurrr (p????)This extension of the base R apply-functions has been one of my favorites lately. The concise usage of purrr enables powerful functional programming that, in turn, makes your code faster, more readable, and more stable. Or, as Mr. Wickham states, it makes „your pure R functions purr„. Also, note its parallelized sibling furrr.Sources: Hadley WickhamAmelia (??mi?l??)During my Master’s degree, I had a course about missing data and multiple imputations. One of the packages we used, or rather analyzed, was Amelia. It turned out that this package is named after an impressive woman: Amelia Earhart. Living in the early 20th century, she was an aviation pioneer and feminist. She has been the first woman to fly solo across the Atlantic, a remarkable achievement and an inspiration for women to start a technical career. Unfortunately, she disappeared during a flight over the central pacific at age 39 and is thus… missing. ba dum-tssSource: Gary King – Co-Authormagrittr (ma??it?)The conciseness of coding with dplyr or its siblings is not imaginable without the pipe operator %&gt;%. This allows you to write and read code from top to bottom and from left to right, just like regular text. Pipes are no special feature of R, yet I am sure René Magritte had nothing else in mind when he painted The Treachery of Images in 1929 with its slogan: „Ceci n’est pas une pipe„. The logo designers just made a slight adjustment to his painting. Or should I say: unearthed the meaning that has always been behind it?!Sources: Vignette,  revolutionanalytics.com batman (?bætm?n)Data science can be quite fun if it weren’t for the data. Especially when working with textual data, typos and inconsistent coding can be very cumbersome. For example, you’ve got questionnaire data consisting of yes/no questions. For R, this corresponds to TRUE/FALSE, but who would write this in a questionnaire? In fact, when we try to convert such data to logical values by calling as.logical(), almost every string becomes NA. Lost and doomed? NO! Cause who is more expert to determine actual NA’s than nananananana… batman!Homeric (h???m?r?k)Hey, you made it all the way down here?! You deserve a little treat! What about a soft, sweet, and special-sprinkled donut? And who would be better suitable to present it to you, than the best-known lover of donuts himself: Homer Simpson! Just help yourself: Homeric::PlotDoughnut(1, col = ""magenta"") Source: Homeric Documentationfcuk (f?k)Error in view(my_data): could not find function ""view""Are you sick and tired of this or similar error messages? Do you regularly employ your ample stock of swear words to describe the stupidity of inconsistent usage of camel or snake case function names across packages? Or do you just type faster than your shadow, causing minor typos in your, otherwise, excellent code? There is help! Just go and install the amazing fcuk package and useless error messages are a thing of the past.hellno (h?l n??)Slip into the role of a dedicated R user. I can only imagine the troubles I had to have with a specific default argument value of a base R function to write an entire package that just handles this case. I am talking about the tormentor of many beginRs when working with as.data.frame(): stringsAsFactors = TRUE. But I do not only change it to FALSE! Also, I create my own FALSE value and name it HELLNO. Honorable mentionsgremlin: package for mixed-effects model REML incorporating Generalized Inverses.harrietr: named after Charles Darwin’s pet giant tortoise. A package for phylogenetic and evolutionary biology data manipulations.beginr: it helps where we’ve all been, searching for ages until setting pch = 16.charlatan: worse than creating dubious medicine, this one makes fake data.fauxpas: explains what specific HTTP errors mean. fishualize: give your plots a fishy look.greybox: why just thinking black or white? This is a package for time series analysis.vroom: it reads data so fast to R, you almost can hear it making vroom vroom.helfRlein: some little helper functions, inspired by the German word Helferlein = little helper.Über den AutorMatthias NistlerI am a data scientist at STATWORX and passionate for wrangling data and getting the most out of it. Outside of the office, I use every second for cycling until the sun goes down..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/why-is-it-called-that-way-origin-and-meaning-of-r-package-names/;Statworx;  Matthias Nistler
  11. März 2020;Defining Your Own Shortcut in RStudio;"At STATWORX, coding is our bread and butter. Because our projects involve many different people in several organizations across multiple generations of programmers, writing clean code is essential. The main requirements for well structured and readable code are comments and sections. In RStudio, these sections are defined by comments that end with at least four dashes ---- (you can also use trailing equal signs ==== or hashes ####). In my opinion, the code is even more clear if the dashes cover the whole range of 80 characters (why you should not exceed the 80 characters limit). That’s how my code usually looks like:# loading packages -------------------------------------------------------------library(dplyr)# load data --------------------------------------------------------------------my_iris &lt;- as_tibble(iris)# prepare data -----------------------------------------------------------------my_iris_preped &lt;- my_iris %&gt;%   filter(Species == ""virginica"") %&gt;%   mutate_if(is.numeric, list(squared = sqrt))# ...Clean, huh? Well, yes, but neither of the three options available to achieve this are as neat as I want it to be:Press - for some time.Copy a certain amount of dashes and insert them sequentially.  Both options often result in too many dashes, so I have to remove the redundant ones. Use the shortcut to insert a new section (CMD/STRG + SHIFT + R). However, you cannot neatly include it after you wrote your comments. Wouldn’t it be nice to have a keyboard shortcut that included the right amount of dashes up from the cursor position? „Easy as can be,“ I thought before trying to define a custom shortcut in RStudio.Unfortunately, it turned out not to be that easy. There is a manual from RStudio that actually covers how you can create your shortcut, but it requires you to put it in a package first. Since I have not been an expert in R package development myself, I decided to go the full distance in this blogpost. By following it step by step, you should be able to define your shortcuts within a few minutes.Note: This article is not about creating a CRAN-worthy package, but covers what is necessary to define your own shortcuts. If you have already created packages before, you can skip the parts about package development and jump directly to what is new to you.Setting up an R packageFirst of all, open RStudio and create an R package directory. For this, please do the following steps:Go to „New Project…“„New Directory“„R Package“Select an awesome package name of your choice. In this example, I named my package shoRtcutIn „Create project as subdirectory of:“ select a directory of your choice. A new folder with your package name will be created in this directory.Tada, everything necessary for a powerful R Package has been set up. RStudio also automatically provides a dummy function hello(). Since we do not like to have this function in our own package, move to the „R“ folder in your project and delete the hello.R file. Do the same in the „man“ folder and delete hello.md.Creating an Addin FunctionNow we can start and define our function. For this, we need the wonderful packages usethis and devtools. These provide all the functionality we need for the next steps.Defining the Addin FunctionVia the use_r() function, we define a new R script file with the given name. That should correspond to the name of the function we are about to create. In my case, I call it set_new_chapter.# use this function to automatically create a new r script for your functionusethis::use_r(""set_new_chapter"")You are directly forwarded to the created file. Now the tricky part begins, defining a function that does what you want. When defining shortcuts that interact with an R script in RStudio, you will soon discover the package rstudioapi. With its functions, you can grab all information from RStudio and make it available within R. Let me guide you through it step by step.As per usual, I set up a regular R function and define its name as set_new_chapter. Next, I define up until which limit I want to include the dashes. You will note that I rather set nchars to 81 than 80. This is because the number corresponds to the cursor position after including the dashes. You will notice that when you write text, the cursor automatically jumps to the position right after the newly typed character. After you have written your 80th character, the cursor will be at position 81.Now we have to find out where the cursor is currently located. This information can be unearthed by the getActiveDocumentContext() function. The returned object returns quite a bit of information, but we are only interested in the cursor position regarding the column. Why the column? You can think of the script like a matrix. Hitting return brings you to a new row, typing a character to a new column. Having a font with equal space characters, which is the default setting in RStudio makes this concept easy to see.By sneaking into the nested list, we find the information we are looking for and store it in context_col. Now we check whether the cursor is already at „column“ 81. If not, there is space in which we insert the dashes. For this final step, we can use another function: insertText. As its name implies, it inserts text in an R script or console. You can either specify a specific position in the document or, by leaving it empty, insert text at the current cursor position, which is exactly what I want right now. As the final step, I need to find out the number of dashes that should be inserted. That’s the difference between the current cursor location and its target position. For example, if the cursor blinks at column 51, meaning I already have typed 50 characters, I want to insert 30 dashes.To document the function, I use the „Code“ &gt; „Insert Roxygen Skeleton“ feature and fill it out appropriately.This is what my final function looks like.#' Insert dashes from courser position to up to 80 characters#'#' @return dashes inside RStudioset_new_chapter &lt;- function(){  # set limit to which position dashes should be included  nchars &lt;- 81  # grab current document information  context &lt;- rstudioapi::getActiveDocumentContext()  # extract horizontal courser position in document  context_col &lt;- context$selection1$range$end""column""  # if a line has less than 81 characters, insert hyphens at the current line  # up to 80 characters  if (nchars &gt; context_col) {    rstudioapi::insertText(strrep(""-"", nchars - context_col))  }}Defining the Function AS and AddinNow we must somehow tell RStudio that this particular function should be used as an addin rather than a regular function. For this, go to „File“ &gt; „New File“ &gt; „Text File“ and include the following text:Name: Insert Dashes (---)Description: Inserts `---` at the cursor position up to 80 characters.Binding: set_new_chapterInteractive: falseName is a short description of what the addin does. This will be displayed when you want to set the shortcut later.Description is a longer description of its functionality.Binding sets the name of the function that should be called by the shortcut.Interactive defines whether this addin is interactive (e.g., runs a Shiny application) or not.You now must save this file as „addins.dcf“ in your project with the following path: „inst“ &gt; „rstudio“. The result should look like this:Finalize the PackageTo wrap everything up and make the shortcut available to you and your colleagues, we only have to call a few more functions. Not all these steps are necessary, yet it is good practice to create a proper package.# OPTIONAL: define the license of your packageusethis::use_mit_license(name = ""Matthias Nistler"")# define dependencies you use in your packageusethis::use_package(""rstudioapi"")# OPTIONAL: include your function description to the manualroxygen2::roxygenise()# check for errorsdevtools::check()# update/create your packagedevtools::build()&gt; ?  checking for file ‘/Users/matthiasnistler/Projekte/2020/blog_shoRtcut/DESCRIPTION’ ...&gt; ?  preparing ‘shoRtcut’:&gt; ?  checking DESCRIPTION meta-information ...&gt; ?  checking for LF line-endings in source and make files and shell scripts&gt; ?  checking for empty or unneeded directories&gt; ?  building ‘shoRtcut_0.0.0.9000.tar.gz’&gt; 1 ""/Users/matthiasnistler/Projekte/2020/shoRtcut_0.0.0.9000.tar.gz""There you go! You just created an awesome package and distributed it to your friends and colleagues.Make the shortcut availableFor the last step, you have to install your package and set a keyboard combination for your shortcut. For this, use the following specification of install.packages:install.packages(    # same path as above  ""/Users/matthiasnistler/Projekte/2020/shoRtcut_0.0.0.9000.tar.gz"",   # indicate it is a local file  repos = NULL)# check if everything worksshoRtcut:::set_new_chapter()Now go to „Tools“ &gt; „Modify Keyboard Shortcuts…“ and search for „dashes“. Here you can define the keyboard combination by clicking inside the empty „Shortcut“ field and pressing the desired key-combination on your keyboard. Click „Apply“, and that’s it! In case you are just here to use my shortcut, you can install it via remotes::install_github(""mnist91/shoRtcut"").Congratulations!You made it! Now you can use your own RStudio shortcut. Exciting, isn’t it?But that’s not all there ist – next week, I will give you an introduction to the wonderful world of R package naming. So stay tuned and happy coding!Über den AutorMatthias NistlerI am a data scientist at STATWORX and passionate for wrangling data and getting the most out of it. Outside of the office, I use every second for cycling until the sun goes down..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/defining-your-own-shortcut-in-rstudio/;Statworx;  Matthias Nistler
  4. März 2020;Community Detection with Louvain and Infomap;"In my last blog post, I wrote about networks and their visualization in R. For the coloring of the objects, I clustered the Les Miserables characters in several groups with Louvain, a Community detection algorithm. Community detection algorithms are not only useful for grouping characters in French lyrics. At STATWORX, we use these methods to give our clients insights into their product portfolio, customer, or market structure. In this blog post, I want to show you the magic behind Community detection and give you a theoretical introduction into the Louvain and Infomap algorithm.Find groups with a high density of connections within and a low density between groupsNetworks are useful constructs to schematize the organization of interactions in social and biological systems. They are just as well suited to represent interactions in the economy, especially in marketing. Such an interaction can be a joint purchase of two or more products or a joint comparison of products in online shops or on price comparison portals. Networks consist of objects and connections between the objects. The connections, also called edges, can be weighted according to certain criteria. In marketing, for example, the number or frequency of joint purchases of two products is a significant weighting of the connection between these two products.  Mostly, such real networks are so big and complex that we have to simplify their structure to get useful information from them. The methods of community detection help to find groups in the network with a high density of connections within and a low density of links between groups.We will have a look at the two methods Louvain Community Detection and Infomap because they gave the best results in the study of Lancchinetti and Fortunato (2009) when applied to different benchmarks on Community Detection methods. Louvain: Build clusters with high modularity in large networksThe Louvain Community Detection method, developed by Blondel et al. (2008), is a simple algorithm that can quickly find clusters with high modularity in large networks. ModularityThe so-called modularity measures the density of connection within clusters compared to the density of connections between clusters (Blondel 2008). It is used as an objective function to be maximized for some community detection techniques and takes on values between -1 and 1. In the case of weighted connections between the objects, we can define the modularity with the following formula:    with::    Weight of a connection between object  and :     = Sum of the weights of all connections originating from the object :    Cluster to which the object  has been assigned:    Dummy variable that takes the value 1 if both objects  and  are assigned to the same cluster:     = Sum of the weights of all connections between all existing objects, divided by 2PhasesThe algorithm is divided into two phases, which are repeated until the modularity cannot be maximized further.In the 1st phase, each object is considered as a separate cluster. For each object  , its neighbors   are checked for whether the modularity increases if  is removed from its cluster and into the cluster of an object  is assigned. The object  is then assigned to the cluster, which maximizes the increase in modularity. However, this only applies in the case of a positive increase. If no positive increase in modularity can be realized by shifting, the object  remains in its previous cluster.The process described above will be repeated and sequentially performed for all objects until no improvement in modularity can be achieved. An object is often viewed and assigned several times. The 1st phase thus stops when a local maximum has been found, i.e., if no individual displacement of an object can improve the modularity.Building on the clusters formed in the 1st phase, a new network is created in the 2nd phase whose objects are now the clusters themselves, which were formed in the 1st phase. To obtain weights for the connections between the clusters, the sum of the weights of the connections between the objects of two corresponding clusters is used. If such a new network was formed with „metacluster“, the steps of the 1st phase will be applied to the new network next, and the modularity will be further optimized. A complete run of both phases is called a pass. Such passes are repeatedly carried out until there is no more change in the cluster, and a maximum of modularity is achieved.Infomap:  Minimize the description length of a random walkThe Infomap method was first introduced by Rosvall and Bergstrom (2008). The procedure of the algorithm is in the core identical to the procedure of Blondel. The algorithm repeats the two described phases until an objective function is optimized. However, as an objective function to be optimized, Infomap does not use modularity but the so-called map equation.Map EquationThe map equation exploits the duality between finding cluster structures in networks and minimizing the description length of the motion of a so-called random walk (Bohlin 2014). This random walker randomly moves from object to object in the network. The more the connection of an object is weighted, the more likely the random walker will use that connection to reach the next object. The goal is to form clusters in which the random walker stays as long as possible, i.e., the weights of the connections within the cluster should take on greater values than the weights of the connections between objects of different clusters. The map equation code structure is designed to compress the descriptive length of the random walk when the random walker lasts for extended periods of time in certain regions of the network. Therefore, the goal is to minimize the map equation, which is defined as follows for weighted but undirected networks (Rosvall 2009):    with:: Network with  objects () and  clusters ():    relative weight of all connections of an object i, that is the sum of the weights of all connections of an object divided by the sum of the weights of all connections of the network:    : Sum of the relative weights of all connections of the objects of the cluster :    Sum of the relative weights of all connections of the objects of the cluster  leaving the cluster (connections to objects from other clusters):     = Sum of the weights of all connections between objects from different clustersThis definition of the map equation is based on the so-called entropy, the average information content, or the information density of a message. This term is based on Shannon’s Source Coding Theorem, from the field of Information Theory (Rosvall 2009).The procedure described above is hereafter referred to as the main algorithm. Objects that were assigned to the same cluster in the first phase of the main algorithm when the new network was created can only be moved together in the second phase. A previously optimal shift into a specific cluster no longer necessarily has to be optimal in a later pass (Rosvall 2009). ExtensionsThus, theoretically, there may be even better cluster divisions than the main algorithm solution. In order to improve the solution of the main algorithm, there are two extensions compared to Louvain Community Detection:Subcluster shift: The subcluster shift sees each cluster as its own network and applies the main algorithm to that network. Thus, one or more subclusters in each cluster, in the previous step, create optimal partitioning of the network. All subclusters are reassigned to their cluster and can now be moved freely between the clusters. By applying the main algorithm, it can be tested whether the shift of one subcluster into another cluster leads to a minimization of the map equation compared to the previously optimal cluster distribution (Rosvall 2009).Single object displacement: Each object is again considered as a separate cluster so that the displacement of individual objects between the optimal clusters determined in the previous step is possible. By applying the main algorithm, it can be determined whether the displacement of individual objects between the clusters can lead to further optimization of the map equation (Rosvall 2009).The two extensions are repeated sequentially until the map equation cannot be further minimized and an optimum has been achieved.How does the Louvain algorithm work in an easy example?As we can see, the core of both methods is to build clusters and reallocate objects in two phases to optimize an objective function. To get a better understanding of how these two phases work, let me illustrate the Louvain Community Detection method with an easy example, a network with six nodes:1st PhaseIn the beginning, each object is separated into its own cluster, and we have to check if the modularity gets maximized if we assign it to another cluster. Only a positive change in modularity leads to a cluster shift.For object A, for example, the calculations behind it look like the following:A ? B: A ? C:  A ? E: Similarly, we can check for  all other objects if a shift to another cluster maximizes the modularity:B ? C: C ? D:D ?F: E ? F: 2nd PhaseNow we try to combine the clusters built in the 1st phase:Orange ? Green:  Orange ? Yellow:  Green ? Yellow:  We can see that none of the assignments of a cluster to another cluster can improve the modularity. Hence we can finish Pass 1.Because we have no change in the second phase of pass 1, no further passes are required because a maximum of modularity is already achieved. In larger networks, of course, more passes are required, since there the clusters can consist of significantly more objects.In R only the package igraph is needed to apply both methodsAll we need to use these two Community detection algorithms is the package igraph, which is a collection of network analysis tools and in addition a list or a matrix with the connections between the objects in our network. In our example we use the Les Misérables Characters network to cluster the characters in several groups. Therefore, we load the dataset lesmis, which you can find in the package geomnet. We need to extract the edges out of lesmis and convert it into a data.frame. Afterwards, you have to convert this into an igraph graph. To use the weights of every connection, we need to rename the weight column so that the algorithm can identify the weights. The resulting graph can be used as the input for the two algorithms.# Libraries --------------------------------------------------------------library(igraph)library(geomnet)# Data Preparation -------------------------------------------------------#Load datasetdata(lesmis)#Edgesedges &lt;- as.data.frame(lesmis1)colnames(edges) &lt;- c(""from"", ""to"", ""weight"")#Create graph for the algorithmsg &lt;- graph_from_data_frame(edges, directed = FALSE)Now we are ready to find the communities with the functions cluster_louvain() respectively cluster_infomap()Furthermore, we can have a look to which community the characters are associated (membership()) or get a list with all communities and their members (communities()). # Community Detection ----------------------------------------------------# Louvainlc &lt;- cluster_louvain(g)membership(lc)communities(lc)plot(lc, g)# Infomapimc &lt;- cluster_infomap(g)membership(imc)communities(imc)plot(lc, g)If you want to visualize these results afterward, have a look at my last blog post or use the above-shown plot() function for a fast visualization. As you can see in the following, the fast plotting option is not as beautiful as with the package visNetwork. In addition, it is also not interactive.ConclusionBoth algorithms outperform other community detection algorithms (Lancchinetti, 2009). They have excellent performance, and Infomap delivers slightly better results in this study than Louvain. Moreover, we should consider two additional facts when choosing between these two algorithms. First, both algorithms do their job very fast. If we apply them on large networks, we can see that Louvain outperforms dramatically. Second, Louvain cannot separate outliers. This could explain why the algorithms divide people into almost identical clusters, but Infomap cuts out a few people from some clusters, and they form their own cluster.We should keep these points in mind when we have to decide between both algorithms. Another approach could be to use them both and compare their solutions.Did I spark your interest to cluster your own networks? Feel free to use my code or contact me if you have any questions.ReferencesBlondel, Vincent D. / Guillaume, Jean L. / Lambiotte, Renaud / Lefebvre, Etienne (2008), „Fast unfolding of communities in large networks“, Journal of Statistical Mechanics: Theory and Experiment, Jg.2008, Nr.10, P10008Bohlin, Ludvig / Edler, Daniel / Lancichinetti, Andrea / Rosvall, Martin (2014), „Community Detection and Visualization of Networks with the Map Equation Framework“, in: Measuring Scholary Impact: Methods and Practice (Ding, Ying / Rousseau, Ronald / Wolfram, Dietmar, Eds.), S.3-34, Springer-Verlag, BerlinLancichinetti, Andrea / Fortunato, Santo (2009), “ Community detection algorithms: a comparative analysis“, Physical review E, Jg.80, Nr.5, 056117Rosvall, Martin / Bergstrom, Carl T. (2008), „Maps of random walks on complexe net- works reveal community structure“, Proceedings of the National Academy of Sciences USA, Jg.105, Nr.4, S.1118-1123Rosvall, Martin / Axellson, Daniel / Bergstrom, Carl T. (2009), „The map equation“, The European Physical Journal Special Topics, Jg.178, Nr.1, S.13-23Über den AutorNiklas JunkerI am a data scientist at STATWORX. I love to give decision makers insights into their business they will never get without my work. Combining my business administrative background with statistical methods, machine learning and the required data is an incredibly fascinating, creative and varied  job..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/community-detection-with-louvain-and-infomap/;Statworx;  Niklas Junker
  26. Februar 2020;Testing REST APIs with Newman;"REST APIs have become a quasi-standard, be it to provide an interface to your application processes, be by setting up a flexible microservice architecture. Sooner or later, you might ask yourself what a proper testing schema would look like and which tools can support you. Some time ago, we at STATWORX asked this ourselves. A toolset that helps us with this task is Newman and Postman, which I will present to you in this blog post.Many of you, who are regularly using and developing REST, might already be quite familiar with Postman. It’s a handy and comfortable desktop tool that comes with some excellent features (see below). Newman, instead, is a command-line agent that runs the previously defined requests. Because of its lean interface, it can be used in several situations, for instance, it can be easily integrated into testing stages of Pipelines.In the following, I will explain how these siblings can be used to implement a neat testing environment. We start with Postman’s feature sets, then move on to the ability to interact with Newman. We will further have a look at a testing schema, touching some test cases, and lastly, integrate it into a Jenkins pipeline.About PostmanPostman is a convenient desktop tool handling REST request. Furthermore, Postman gives you the possibility to define test cases (in JavaScript), has a feature to switch environments, and provides you with Pre-Request steps to set up the setting before your calls. In the following, I will give you examples of some interesting features.Collection and RequestsRequests are the basic unit in Postman, and everything else spins around them. As I said previously, Postman’s GUI provides you with a comfortable way to define these: request method can be picked from a drop-down list, header information is presented clearly, there is a helper for authorization, and many more.You should have at least one collection per REST interface defined to bundle your requests. At the very end of the definition process, collections can be exported into JSON format. This result will, later on, be exploited for Newman. EnvironmentsPostman also implements the concept of environment variables. This means: Depending on where your requests are fired from, the variables adapt. The API’s hostname is a good example that should be kept variable: In the development stage, it may be just your localhost but could be different in a dockerized environment.The syntax of environment variables is double-curly brackets. If you want to use the hostname variable hostname put it like this:  {{ hostname }}Like for collections, environments can be exported into JSON files. We should keep this in mind when we move to Newman.TestsEach API request in Postman should come along with at least one test. I propose the following list as an orientation on what to test:the status code: Check the status code according to your expectation: regular GET requests are supposed to return 200 OK, POST requests 201 Created if successful. On the other hand, authorization should be tested as well as invalid client requests which are supposed to return 40x.  – See below a POST request test:pm.test(""Successful POST request"", function () {     pm.expect(pm.response.code).to.be.oneOf(201,202); });whether data is returned Test if the response has any data as a first approximationthe schema of returned data Test if the structure of the request data fits the expectations: non-nullable fields, data types, names of properties. Find below an example of a schema validation:pm.test(""Response has correct schema"", function () {    var schema = {""type"":""object"",                  ""properties"":{                      ""access_token"":{""type"":""string""},                      ""created_on"":{""type"":""string""},                      ""expires_seconds"":{""type"":""number""}                  }};    var jsonData = pm.response.json();    pm.expect(tv4.validate(jsonData,schema)).to.be.true;});values of returned data: Check if the values of the response data are sound; for non-negative values:pm.test(""Expires non negative"", function() {    pm.expect(pm.response.json().expires_seconds).to.be.above(0);})Header values Check the header of the response if useful relevant is stored there.   All tests have to be written in JavaScript. Postman ships with its own library and tv4 for schema validation.Below you find a complete running test:Introduction to NewmanAs mentioned before, Newman acts as an executor of what was defined in Postman. To generate results, Newman uses reporters. Reporters can be the command line interface itself, but also known standards as JUnit can be found. The simplest way to install newman is via NPM (Node package manager). There are ready to use docker images of NodeJS on DockerHub. Install the package via  npm install -g newman.There are two ways to call Newman: command-line interface and within JS code. We will only focus on the first.Calling the CLITo run a predefined test collections use the command newman run. Please see the example below:newman run            --reporters cli,junit            --reporter-junit-export /test/out/report.xml            -e /test/env/auth_jwt-docker.pmenv.json            /test/src/auth_jwt-test.pmc.jsonLet us take a closer look: Recall that we have previously exported the collection and the environment from Postman. The environment can be attached with the -e option. Moreover, two reporters were specified: the cli itself which prints into the terminal and junit which additional shall export a report to the file  report.xmlThe CLI reporter prints the following (Note that the first three test cases are those from the test schema proposal):? jwt-new-token  POST http://tp_auth_jwt:5000/new-token/bot123 201 CREATED, 523B, 42ms  ?  Successful POST request  ?  Response has correct schema  ?  Expires non negative? jwt-auth  POST http://tp_auth_jwt:5000/new-token/test 201 CREATED, 521B, 11ms  GET http://tp_auth_jwt:5000/auth 200 OK, 176B, 9ms  ?  Status code is 200  ?  Login name is correct? jwt-auth-no-token  GET http://tp_auth_jwt:5000/auth 401 UNAUTHORIZED, 201B, 9ms  ?  Status is 401 or 403? jwt-auth-bad-token  GET http://tp_auth_jwt:5000/auth 403 FORBIDDEN, 166B, 6ms  ?  Status is 401 or 403Integration into JenkinsNewman functionality can now be integrated into (almost?) any Pipeline tool. For Jenkins, we create a docker image based on NodeJS and with Newman installed. Next, we either pack or mount both the environment and the collection file into the docker container. When running the container, we use Newman as a command-line tool, just as we did before. To use this in a test stage of a Pipeline, we have to make sure that the REST API is actually running when Newman is executed.In the following example, the functionalities were defined as targets of a Makefile:run to run the REST API with all dependenciestest to run Newman container which itself runs the testing collectionsrm to stop and remove the REST APIAfter the API has been tested the report from JUnit is digested by Jenkins with the command junit &lt;report&gt;See below a Pipeline snippet of a test run:node{       stage('Test'){            try{                sh ""cd docker &amp;&amp; make run""                sh ""sleep 5""                sh ""cd docker &amp;&amp; make test""                junit ""source/test/out/report.xml""            } catch (Exception e){                    echo e            } finally {                    sh ""cd docker &amp;&amp; make rm""            }        }}SummaryNow it’s time to code tests for your REST API. Please also try to integrate it into your build-test cycle and into your automation pipeline because automation and defined processes are crucial to delivering reliable code and packages.I hope with this blog post, you now have a better understanding of how Postman and Newman can be used to implement a test framework for REST APIs. Postman was used as a definition tool, whereas Newman was the runner of these definitions. Because of his nature, we have also seen that Newman is the tool for your build pipeline. Happy coding!We’re hiring!Data Engineering is your jam and you’re looking for a job?  We’re currently looking for Junior Consultants and Consultants in Data Engineering. Check the requirements and benefits of working with us on our career site. We’re looking forward to your application!Über den AutorAndre MünchI am a data engineer at STATWORX. I love to have challenges to setup data structure and compose components to integrate Data Science models into productive environments..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/testing-rest-apis-with-newman/;Statworx;  Andre Münch
  19. Februar 2020;Dynamic UI Elements in Shiny – Part 2;"At STATWORX, deploying our project results with the help of Shiny has become part of our daily business. Shiny is a great way of letting users interact with their own data and the data science products that we provide. Applying the philosophy of reactivity to your app’s UI is an interesting way of bringing your apps closer in line with the spirit of the Shiny package. Shiny was designed to be reactive, so why limit this to only the server-side of your app? Introducing dynamic UI elements to your apps will help you reduce visual clutter, make for cleaner code and enhance the overall feel of your applications. I have previously discussed the advantages of using renderUI in combination with lapply and do.call in the first part of this series on dynamic UI elements in Shiny. Building onto this I would like to expand our toolbox for reactive UI design with a few more options.The objectiveIn this particular case we’re trying to build an app where one of the inputs reacts to another input dynamically. Let’s assume we’d like to present the user with multiple options to choose from in the shape of a selectInput. Let’s also assume that one of the options may call for more input from the user, let’s say a comment, to explain more clearly the previous selection. One way to do this would be to add a static textInput or similar to the app. A much more elegant solution would be to conditionally render the second input to only appear if the proper option had been selected. The image below shows how this would look in practice.There are multiple ways of going about this in Shiny. I’d like to introduce two of them to you, both of which lead to the same result but with a few key differences between them. A possible solution: reqWhat req is usually used forreq is a function from the Shiny package whose purpose is to check whether certain requirements are met before proceeding with your calculations inside a reactive environment. Usually this is used to avoid red error messages popping up in your ShinyApp UI when an element of your app depends on an input that doesn’t have a set value yet. You may have seen one of these before:These errors usually disappear once you have assigned a value to the needed inputs. req makes it so that your desired output is only calculated once its required inputs have been set, thus offering an elegant way to avoid the rather garish looking error messages in your app’s UI. How we can make use of reqIn terms of reactive UI design we can make use of req’s functionality to introduce conditional statements to our uiOutputs. This is achieved by using renderUI and req in combination as shown in the following example: output$conditional_comment &lt;- renderUI({    # specify condition    req(input$select == ""B"")    # execute only if condition is met    textAreaInput(inputId = ""comment"",                   label = ""please add a comment"",                   placeholder = ""write comment here"")   })Within req the condition to be met is specified and the rest of the code inside the reactive environment created by renderUI is only executed if that condition is met. What is nice about this solution is that if the condition has not been met there will be no red error messages or other visual clutter popping up in your app, just like what we’ve seen at the beginning of this chapter.A simple example appHere’s the complete code for a small example app:library(shiny)library(shinydashboard)ui &lt;- dashboardPage(  dashboardHeader(),  dashboardSidebar(    selectInput(inputId = ""select"",                 label = ""please select an option"",                 choices = LETTERS1:3),    uiOutput(""conditional_comment"")  ),  dashboardBody(    uiOutput(""selection_text""),    uiOutput(""comment_text"")  ))server &lt;- function(input, output) {  output$selection_text &lt;- renderUI({    paste(""The selected option is"", input$select)  })  output$conditional_comment &lt;- renderUI({    req(input$select == ""B"")    textAreaInput(inputId = ""comment"",                   label = ""please add a comment"",                   placeholder = ""write comment here"")  })  output$comment_text &lt;- renderText({    input$comment  })}shinyApp(ui = ui, server = server)If you try this out by yourself you will find that the comment box isn’t hidden or disabled when it isn’t being shown, it simply doesn’t exist unless the selectInput takes on the value of „B“. That is because the uiOutput object containing the desired textAreaInput isn’t being rendered unless the condition stated inside of req is satisfied.The popular choice: conditionalPanelOut of all the tools available for reactive UI design this is probably the most widely used. The results obtained with conditionalPanel are quite similar to what req allowed us to do in the example above, but there are a few key differences. How does this differ from req?conditionalPanel was designed to specifically enable Shiny-programmers to conditionally show or hide UI elements. Unlike the req-method, conditionalPanel is evaluated within the UI-part of the app, meaning that it doesn’t rely on renderUI to conditionally render the various inputs of the shinyverse. But wait, you might ask, how can Shiny evaluate any conditions in the UI-side of the app? Isn’t that sort of thing always done in the server-part? Well yes, that is true if the expression is written in R. To get around this, conditionalPanel relies on JavaScript to evaluate its conditions. After stating the condition in JS we can add any given UI-elements to our conditionalPanel as shown below:conditionalPanel(      # specify condition      condition = ""input.select == 'B'"",      # execute only if condition is met      textAreaInput(inputId = ""comment"",                     label = ""please add a comment"",                     placeholder = ""write comment here"")    )This code chunk displays the same behaviour as the example shown in the last chapter with one major difference: It is now part of our ShinyApp’s UI-function unlike the req-solution, which was a uiOutput calculated in the server-part of the app and later passed to our UI-function as a list-element. A simple example app:Rewriting the app to include conditionalPanel instead of req yields a script that looks something like this:library(shiny)library(shinydashboard)ui &lt;- dashboardPage(  dashboardHeader(),  dashboardSidebar(    selectInput(inputId = ""select"",                 label = ""please select an option"",                 choices = LETTERS1:3),    conditionalPanel(      condition = ""input.select == 'B'"",      textAreaInput(inputId = ""comment"",                     label = ""please add a comment"",                     placeholder = ""write comment here"")    )  ),  dashboardBody(    uiOutput(""selection_text""),    textOutput(""comment_text"")    ))server &lt;- function(input, output) {  output$selection_text &lt;- renderUI({    paste(""The selected option is"", input$select)  })  output$comment_text &lt;- renderText({    input$comment  })}shinyApp(ui = ui, server = server)With these two simple examples we have demonstrated multiple ways of letting your displayed UI elements react to how a user interacts with your app – both on the server, as well as the UI side of the application. In order to keep things simple I have used a basic textAreaInput for this demonstration, but both renderUI and conditionalPanel can hold so much more than just a simple input element.So get creative and utilize these tools, maybe even in combination with the functions from part 1 of this series, to make your apps even shinier!Über den AutorOliver GuggenbühlI am a data scientist at STATWORX and love telling stories with data - the ShinyR the better!.e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/dynamic-ui-elements-in-shiny-part-2/;Statworx;  Oliver Guggenbühl
  14. Februar 2020;Animated Plots using ggplot and gganimate;"Did you know, that you can transform plain old static ggplot graphs to animated ones? Well you can with the help of the package gganimate by RStudio’s Thomas Lin Pedersen and David Robinson and the results are amazing! My STATWORX colleagues and I are very impressed how effortless all kind of geoms are transformed to suuuper smooth animations. That’s why in this post I will provide a short overview over some of the wonderful functionalities of gganimate, I hope you’ll enjoy them as much as we do!Since Valentine’s Day is just around the corner, we’re going to explore the Speed Dating Experiment dataset compiled by Columbia Business School professors Ray Fisman and Sheena Iyengar. Hopefully, we’ll learn about gganimate as well as how to find our Valentine. If you like, you can download the data from Kaggle. Defining the basic animation: transition_*How are static plots put into motion? Essentially, gganimate creates data subsets, which are plotted individually and constitute the substantial frames, which, when played consecutively, create the basic animation. The results of gganimate are so seamless because gganimate takes care of the so-called tweening for us by calculating datapoints for transition frames displayed in-between frames with actual input data. The transition_* functions define how the data subsets are derived and thus define the general character of any animation. In this blogpost we’re going to explore three types of transitions: transition_states(), transition_reveal() and transition_filter(). But let’s start at the beginning.We’ll start with transition_states(). Here the data is split in subsets according to the categories of the variable provided to the states argument.  If several rows of a dataset pertain to the same unit of observation and should be identifiable as such, a grouping variable defining the observation units needs to be supplied. Alternatively, an identifier can be mapped to any other aesthetic. Please note, to ensure the readability of this post, all text concerining the interpretation of the speed dating data is written in italics. If you’re not interested in that part you simply can skip those paragraphs. For the data prep I’d like to refere you to my GitHub.First, we’re going to explore what the participants of the Speed Dating Experiment look for in a partner. Participants were asked to rate the importance of attributes in a potential date by allocating a budget of 100 points to several characteristics, with higher values denoting a higher importance. The participants were asked to rate the attributes according to their own views. Further, the participants were asked to rate the same attributes according to the presumed wishes of their same-sex peers, meaning they allocated the points in the way they supposed their average same-sex peer would do.We’re going to plot all of these ratings (x-axis) for all attributes (y-axis). Since we want to compare the individual wishes to the individually presumed wishes of peers, we’re going to transition between both sets of ratings. Color always indicates the personal wishes of a participant. A given bubble indicates the rating of one specific participant for a given attribute, switching between one’s own wishes and the wishes assumed for peers.## Static Plot# ...characteristic vs. (presumed) rating...# ...color&amp;size mapped to own rating, grouped by IDplot1 &lt;- ggplot(df_what_look_for,        aes(x = value,           y = variable,           color = own_rating, # bubbels are always colord according to own whishes           size = own_rating,           group = iid)) + # identifier of observations across states  geom_jitter(alpha = 0.5, # to reduce overplotting: jitttering &amp; alpha              width = 5) +   scale_color_viridis(option = ""plasma"", # use virdis' plasma scale                      begin = 0.2, # limit range of used hues                      name = ""Own Rating"") +  scale_size(guide = FALSE) + # no legend for size  labs(y = """", # no axis label       x = ""Allocation of 100 Points"",  # x-axis label       title = ""Importance of Characteristics for Potential Partner"") +  theme_minimal() +  # apply minimal theme  theme(panel.grid = element_blank(),  # remove all lines of plot raster        text = element_text(size = 16)) # increase font size## Animated Plotplot1 +   transition_states(states = rating) # animate contrast subsets acc. to variable rating  First off, if you’re a little confused wich state is wich, please be patient, we’ll explore dynamic labels in the section about ‚frame variables‘.It’s apparent that different people look for different things in a partner. Yet attractiveness is often prioritized over other qualities. But the importance of attractiveness varies most strongly of all attributes between individuals. Interestingly, people are quite aware that their peer’s ratings might differ from their own views. Further, especially the collective presumptions (= the mean values) about others are not completely off, but of higher variance than the actual ratings.So there is hope for all of us that somewhere out there somebody is looking for someone just as ambitious or just as intelligent as ourselves. However, it’s not always the inner values that count.gganimate allows us to tailor the details of the animation according to our wishes. With the argument transition_length we can define the relative length of the transition from one to the other real subsets of data takes and with state_length how long, relatively speaking, each subset of original data is displayed. Only if the wrap argument is set to TRUE, the last frame will get morphed back into the first frame of the animation, creating an endless and seamless loop. Of course, the arguments of different transition functions may vary.## Animated Plot# ...replace default argumentsplot1 +   transition_states(states = rating,                    transition_length = 3, # 3/4 of total time for transitions                    state_length = 1, # 1/4 of time to display actual data                    wrap = FALSE) # no endless loopStyling transitions: ease_aesAs mentioned before, gganimate takes care of tweening and calculates additional data points to create smooth transitions between successively displayed points of actual input data. With ease_aes we can control which so-called easing function is used to ‚morph‘ original data points into each other.  The default argument is used to declare the easing function for all aesthetics in a plot.  Alternatively, easing functions can be assigned to individual aesthetics by name. Amongst others quadric, cubic , sine and exponential easing functions are available, with the linear easing function being the default. These functions can be customized further by adding a modifier-suffix: with -in the function is applied as-is, with -out the function is reversely applied with -in-out the function is applied as-is in the first half of the transition and reversed in the second half. Here I played around with an easing function that models the bouncing of a ball. ## Animated Plot# ...add special easing functionplot1 +   transition_states(states = rating) +   ease_aes(""bounce-in"") # bouncy easing function, as-isDynamic labelling: {frame variables}To ensure that we, mesmerized by our animations, do not lose the overview gganimate provides so-called frame variables that provide metadata about the animation as a whole or the previous/current/next frame. The frame variables – when wrapped in curly brackets – are available for string literal interpretation within all plot labels. For example, we can label each frame with the value of the  states variable that defines the currently (or soon to be) displayed subset of actual data:## Animated Plot# ...add dynamic label: subtitle with current/next value of states variableplot1 +  labs(subtitle = ""{closest_state}"") + # add frame variable as subtitle  transition_states(states = rating) The set of available variables depends on the transition function. To get a list of frame variables available for any animation (per default the last one) the frame_vars() function can be called, to get both the names and values of the available variables. Indicating previous data: shadow_*To accentuate the interconnection of different frames, we can apply one of gganimates ’shadows‘. Per default shadow_null() i.e. no shadow is added to animations. In general, shadows display data points of past frames in different ways: shadow_trail() creates a trail of evenly spaced data points, while shadow_mark() displays all raw data points. We’ll use shadow_wake() to create a little ‚wake‘ of past data points which are gradually shrinking and fading away. The argument wake_length allows us to set the length of the wake, relative to the total number of frames. Since the wakes overlap, the transparency of geoms might need adjustment. Obviously, for plots with lots of data points shadows can impede the intelligibility. plot1B + # same as plot1, but with alpha = 0.1 in geom_jitter  labs(subtitle = ""{closest_state}"") +    transition_states(states = rating) +  shadow_wake(wake_length = 0.5) # adding shadowThe benefits of  transition_*While I simply love the visuals of animated plots, I think they’re also offering actual improvement. I feel transition_states compared to facetting has the advantage of making it easier to track individual observations through transitions. Further, no matter how many subplots we want to explore, we do not need lots of space and clutter our document with thousands of plots nor do we have to put up with tiny plots.  Similarly, e.g. transition_reveal holds additional value for time series by not only mapping a time variable on one of the axes but also to actual time: the transition length between the individual frames displays of actual input data corresponds to the actual relative time differences of the mapped events. To illustrate this, let’s take a quick look at the ’success‘ of all the speed dates across the different speed dating events:## Static Plot# ... date of event vs. interest in second date for women, men or couplesplot2 &lt;- ggplot(data = df_match,                aes(x = date, # date of speed dating event                    y = count, # interest in 2nd date                    color = info, # which group: women/men/reciprocal                    group = info)) +  geom_point(aes(group = seq_along(date)), # needed, otherwise transition dosen't work             size = 4, # size of points             alpha = 0.7) + # slightly transparent  geom_line(aes(lty = info), # line type according to group            alpha = 0.6) + # slightly transparent  labs(y = ""Interest After Speed Date"",       x = ""Date of Event"",       title = ""Overall Interest in Second Date"") +  scale_linetype_manual(values = c(""Men"" = ""solid"", # assign line types to groups                                   ""Women"" = ""solid"",                                   ""Reciprocal"" = ""dashed""),                        guide = FALSE) + # no legend for linetypes  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) + # y-axis in %  scale_color_manual(values = c(""Men"" = ""#2A00B6"", # assign colors to groups                                ""Women"" = ""#9B0E84"",                                ""Reciprocal"" = ""#E94657""),                     name = """") +  theme_minimal() + # apply minimal theme  theme(panel.grid = element_blank(), # remove all lines of plot raster        text = element_text(size = 16)) # increase font size## Animated Plotplot2 +  transition_reveal(along = date) Displayed are the percentages of women and men who were intrested in a second date after each of their speed dates as well as the percentage of couples in which both partners wanted to see each other again.Most of the times, women were more interested in second dates than men. Further, attraction between dating partners often didn’t go both ways: the instances in which both partners of a couple wanted a second date always were far more infrequent than the general interest of either men and women. While it’s hard to identify the most romantic time of the year, according to the data there seemed to be a slack in romance in early autumn. Maybe everybody still was heartbroken over their summer fling? Fortunately, Valentine’s Day is in February.Another very handy option is  transition_filter(), it’s a great way to present selected key insights of your data exploration. Here the animation browses through data subsets defined by a series of filter conditions. It’s up to you which data subsets you want to stage. The data is filtered according to logical statements defined in transition_filter(). All rows for which a statement holds true are included in the respective subset. We can assign names to the logical expressions, which can be accessed as frame variable. If the keep argument is set to TRUE, the data of previous frames is permanently displayed in later frames. I want to explore, whether one’s own characteristics relate to the attributes one looks for in partner. Do opposites attract? Or do birds of a feather (want to) flock together?Displayed below are the importances the speed dating participants assigned to different attributes of a potential partner. Contrasted are subsets of participants, who were rated especially funny, attractive, sincere, intelligent or ambitious by their speed dating partners. The rating scale went from 1 = low to 10 = high, thus I assume value of  &gt;7 to be rather outstanding.## Static Plot (without geom)# ...importance ratings for different attributesplot3 &lt;- ggplot(data = df_ratings,                  aes(x = variable, # different attributes                     y = own_rating, # importance regarding potential partner                     size = own_rating,                      color = variable, # different attributes                     fill = variable)) +  geom_jitter(alpha = 0.3) +  labs(x = ""Attributes of Potential Partner"", # x-axis label       y = ""Allocation of 100 Points (Importance)"",  # y-axis label       title = ""Importance of Characteristics of Potential Partner"", # title       subtitle = ""Subset of {closest_filter} Participants"") + # dynamic subtitle   scale_color_viridis_d(option = ""plasma"", # use viridis scale for color                         begin = 0.05, # limit range of used hues                        end = 0.97,                        guide = FALSE) + # don't show legend  scale_fill_viridis_d(option = ""plasma"", # use viridis scale for filling                       begin = 0.05, # limit range of used hues                       end = 0.97,                        guide = FALSE) + # don't show legend  scale_size_continuous(guide = FALSE) + # don't show legend  theme_minimal() + # apply minimal theme  theme(panel.grid = element_blank(),  # remove all lines of plot raster        text = element_text(size = 16)) # increase font size## Animated Plot # ...show ratings for different subsets of participantsplot3 +  geom_jitter(alpha = 0.3) +  transition_filter(""More Attractive"" = Attractive &gt; 7, # adding named filter expressions                    ""Less Attractive"" = Attractive &lt;= 7,                    ""More Intelligent"" = Intelligent &gt; 7,                    ""Less Intelligent"" = Intelligent &lt;= 7,                    ""More Fun"" = Fun &gt; 7,                    ""Less Fun"" = Fun &lt;= 5) Of course, the number of extraordinarily attractive, intelligent or funny participants is relatively low. Surprisingly, there seem to be little differences between what the average low vs. high scoring participants look for in a partner. Rather the lower scoring group includes more people with outlying expectations regarding certain characteristics. Individual tastes seem to vary more or less independently from individual characteristics.Styling the (dis)appearance of data: enter_* / exit_*Especially if displayed subsets of data do not or only partly overlap, it can be favorable to underscore this visually. A good way to do this are the enter_*() and exit_*() functions, which enable us to style the entry and exit of data points, which do not persist between frames. There are many combinable options: data points can simply (dis)appear (the default), fade (enter_fade()/exit_fade()), grow or shrink  (enter_grow()/exit_shrink()),  gradually change their color  (enter_recolor()/exit_recolor()), fly  (enter_fly()/exit_fly()) or drift  (enter_drift()/exit_drift()) in and out. We can use these stylistic devices to emphasize changes in the data bases of different frames. I used exit_fade() to let further not included data points gradually fade away while flying them out of the plot area on a vertical route (y_loc = 100), data points re-entering the sample fly in vertically from the bottom of the plot (y_loc = 0): ## Animated Plot # ...show ratings for different subsets of participantsplot3 +  geom_jitter(alpha = 0.3) +  transition_filter(""More Attractive"" = Attractive &gt; 7, # adding named filter expressions                    ""Less Attractive"" = Attractive &lt;= 7,                    ""More Intelligent"" = Intelligent &gt; 7,                    ""Less Intelligent"" = Intelligent &lt;= 7,                    ""More Fun"" = Fun &gt; 7,                    ""Less Fun"" = Fun &lt;= 5) +  enter_fly(y_loc = 0) + # entering data: fly in vertically from bottom  exit_fly(y_loc = 100) + # exiting data: fly out vertically to top...  exit_fade() # ...while color is fadingFinetuning and saving: animate() &amp; anim_save()Gladly, gganimate makes it very easy to finalize and save our animations. We can pass our finished gganimate object to animate() to, amongst other things, define the number of frames to be rendered (nframes) and/or the rate of frames per second (fps) and/or the number of seconds the animation should last (duration). We also have the option to define the device in which the individual frames are rendered (the default is device = “png”, but all popular devices are available). Further, we can define arguments that are passed on to the device, like e.g. width or height. Note, that simply printing an gganimateobject is equivalent to passing it to animate() with default arguments. If we plan to save our animation the argument renderer, is of importance: the function anim_save() lets us effortlessly save any gganimate object, but only so if it was rendered using one of the functions magick_renderer() or the default gifski_renderer().The function anim_save()works quite straightforward. We can define filename and path (defaults to the current working directory) as well as the animation object (defaults to the most recently created animation). # create a gganimate objectgg_animation &lt;- plot3 +  transition_filter(""More Attractive"" = Attractive &gt; 7,                    ""Less Attractive"" = Attractive &lt;= 7) # adjust the animation settings animate(gg_animation,         width = 900, # 900px wide        height = 600, # 600px high        nframes = 200, # 200 frames        fps = 10) # 10 frames per second# save the last created animation to the current directory anim_save(""my_animated_plot.gif"")Conclusion (and a Happy Valentine’s Day)I hope this blog post gave you an idea, how to use gganimate to upgrade your own ggplots to beautiful and informative animations. I only scratched the surface of gganimates functionalities, so please do not mistake this post as an exhaustive description of the presented functions or the package. There is much out there for you to explore, so don’t wait any longer and get started with gganimate! But even more important: don’t wait on love. The speed dating data shows that most likely there’s someone out there looking for someone just like you. So from everyone here at STATWORX: Happy Valentine’s Day!## 8 bit heart animationanimation2 &lt;- plot(data = df_eight_bit_heart %&gt;% # includes color and x/y position of pixels          dplyr::mutate(id = row_number()), # create row number as ID                  aes(x = x,                     y = y,                    color = color,                    group = id)) +  geom_point(size = 18, # depends on height &amp; width of animation             shape = 15) + # square  scale_color_manual(values = c(""black"" = ""black"", # map values of color to actual colors                                ""red"" = ""firebrick2"",                                ""dark red"" = ""firebrick"",                                ""white"" = ""white""),                     guide = FALSE) + # do not include legend  theme_void() + # remove everything but geom from plot  transition_states(-y, # reveal from high to low y values                     state_length = 0) +  shadow_mark() + # keep all past data points  enter_grow() + # new data grows   enter_fade() # new data starts without coloranimate(animation2,         width = 250, # depends on size defined in geom_point         height = 250, # depends on size defined in geom_point         end_pause = 15) # pause at end of animationÜber den AutorLea WaniekI am a data scientist at STATWORX, apart from machine learning, I love to play around with RMarkdown and ggplot2, making data science beautiful inside and out..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/animated-plots-using-ggplot-and-gganimate/;Statworx;  Lea Waniek
  5. Februar 2020;Machine Learning Goes Causal II: Meet the Random Forest’s Causal Brother;"At STATWORX we are excited that a new promising field of Machine Learning has evolved in recent years: Causal Machine Learning. In short, Causal Machine Learning is the scientific study of Machine Learning algorithms which allow estimating causal effects. Over the last few years, different Causal Machine Learning algorithms have been developed, combining the advances from Machine Learning with the theory of causal inference to estimate different types of causal effects. My colleague Markus has already introduced some of these algorithms in an earlier blog post. As Causal Machine Learning is a rather complex topic, I will write a series of blog posts to slowly dive into this new fascinating world of data science. In my first blog post, I gave an introduction into the topic, focusing on what Causal Machine Learning is and why it is important in practice and for the future of data science. In this second blog post, I will introduce the so-called Causal Forest, one of the most popular Causal Machine Learning algorithms to estimate heterogeneous treatment effects.Why Heterogeneous Treatment Effects?In Causal Forests, the goal is to estimate heterogeneity in treatment effects. As explained in my previous blog post, a treatment effect refers to a causal effect of a treatment or intervention on an outcome variable of scientific or political interest. For example the causal effect of a subsidised training programme on earnings. As individual treatment effects are unobservable, the practice focuses on estimating unbiased and consistent averages of the individual treatment effect. The most common parameter thereof is the average treatment effect, which is the mean of all individual treatment effects in the entire population of interest. However, sometimes treatment effects may vary widely between different subgroups in the population, bet it larger or smaller than the average treatment effect. In some cases, it might therefore be more interesting to estimate these different, i.e. heterogeneous treatment effects. In most applications it is also interesting to look beyond the average effects in order to understand how the causal effects vary with observable characteristics.(Knaus, Lechner &amp; Strittmatter, 2018)The estimation of heterogeneous treatment effects can assist in answering questions like: For whom are there big or small treatment effects? For which subgroup does a treatment generate beneficial or adverse effects? In the field of marketing, for example, the estimation of heterogeneous treatment effects can help to optimise resource allocation by answering the question of which customers respond the most to a certain marketing campaign or for which customers is the causal effect of intervention strategies on their churn behaviour the highest. Or when it comes to pricing, it might be interesting to quantify how a change in price has varying impact on sales among different age or income groups.Where Old Estimation Methods FailEstimating heterogeneous treatment effects is nothing new. Econometrics and other social sciences have long been studying which variables predict a smaller or larger than average treatment effect, which in statistical terms is also known as Moderation. One of the most traditional ways to find heterogeneous treatment effects is to use a Multiple Linear Regression with interaction terms between the variables of interest (i.e. the ones which might lead to treatment heterogeneity) and the treatment indicator. In this blog post, I will always assume that the data is from a randomised experiment, such that the assumptions to identify treatment effects are valid without further complications. We then conclude that the treatment effect depends on the variables whose interaction term is statistically significant. For example, if we have only one variable, the regression model would look as follows:    where  is the treatment indicator and  is the variable of interest. In that case, if  is significant, we know that the treatment effect depends on variable . The treatment effect for each observation can then be calculated as    which is dependent on the value of  and therefore heterogeneous among the different observations. So why is there a need for more advanced methods to estimate heterogeneous treatment effects? The example above was very simple, it only included one variable. However, usually, we have more than one variable which might influence the treatment effect. To see which variables predict heterogeneous treatment effects, we have to include many interaction terms, not only between each variable and the treatment indicator but also for all possible combinations of variables with and without the treatment indicator. If we have  variables and one treatment, this gives a total number of parameters of:    So, for example if we had 5 variables, we would have to include a total number of 64 parameters into our Linear Regression Model. This approach suffers from a lack of statistical power and could also cause computational issues. The use of a Multiple Linear Regression also imposes linear relationships unless more interactions with polynomials are included. Because Machine Learning algorithms can handle enormous numbers of variables and combining them in nonlinear and highly interactive ways, researchers have found ways to better estimate heterogeneous treatment effects by combining the field of Machine Learning with the study of Causal Inference.Generalised Random ForestsOver recent years, different Machine Learning algorithms have been developed to estimate heterogeneous treatment effects. Most of them are based on the idea of Decision Trees or Random Forests, just like the one I focus on in this blog post: Generalised Random Forests by Athey, Tibshirani and Wager (2018).Generalised Random Forests follows the idea of Random Forests and apart from heterogeneous treatment effect estimation, this algorithm can also be used for non-parametric quantile regression and instrumental variable regression. It keeps the main structure of Random Forests such as the recursive partitioning, subsampling and random split selection. However, instead of averaging over the trees Generalised Random Forests estimate a weighting function and uses the resulting weights to solve a local GMM model. To estimate heterogeneous treatment effects, this algorithm has two important additional features, which distinguish it from standard Random Forests.1. Splitting CriterionThe first important difference to Random Forests is the splitting criterion. In Random Forests, where we want to predict an outcome variable , the split at each tree node is performed by minimising the mean squared error of the outcome variable . In other words, the variable and value to split at each tree node are chosen such that the greatest reduction in the mean squared error with regard to the outcomes  is achieved. After each tree partition has been completed, the tree’s prediction for a new observation  is obtained by letting it trickle through all the way from tree’s root into a terminal node, and then taking the average of outcomes  of all the observations  that fell into the same node during training. The Random Forest prediction is then calculated as the average of the predicted tree values. In Causal Forests, we want to estimate treatment effects. As stated by the Fundamental Problem of Causal Inference however, we can never observe a treatment effect on an individual level. Therefore, the prediction of a treatment effect is given by the difference in the average outcomes  between the treated and the untreated observations in a terminal node. Without going into too much detail, to find most heterogeneous but also accurate treatment effects, the splitting criterion is adapted such that it searches for a partitioning where the treatment effects differ the most including a correction that accounts for how the splits affect the variance of the parameter estimates. 2. HonestyRandom Forests are usually evaluated by applying them to a test set and measure the accuracy of the predictions of Y using an error measure such as the mean squared error. Because we can never observe treatment effects, this form of performance measure is not possible in Causal Forests. When estimating causal effects, one, therefore, evaluates their accuracy by examining the bias, standard error and the related confidence interval of the estimates. To ensure that an estimate is as accurate as possible, the bias should asymptotically disappear and the standard error and, thus, the confidence interval, should be as small as possible. To enable this statistical inference in their Generalised Random Forest, Athey, Tibshirani and Wager introduce so-called honest trees. In order to make a tree honest, the training data is split into two subsamples: a splitting subsample and an estimating subsample. The splitting subsample is used to perform the splits and thus grow the tree. The estimating subsample is then used to make the predictions. That is, all observations in the estimating subsample are dropped down the previously-grown tree until it falls into a terminal node. The prediction of the treatment effects is then given by the difference in the average outcomes  between the treated and the untreated observations of the estimating subsample in the terminal nodes. With such honest trees, the estimates of a Causal Forest are consistent (i.e. the bias vanishes asymptotically) and asymptotically Gaussian which together with the estimator for the asymptotic variance allow valid confidence intervals.Causal Forest in ActionTo show the advantages of Causal Forests compared to old estimation methods, in the following I will compare the Generalised Random Forest to a Regression with interaction terms in a small simulation study. I use simulated data to be able to compare the estimated treatment effects with the actual treatment effects, which, as we know, would not be observable in real data. To compare the two algorithms with respect to the estimation of heterogeneous treatment effects, I test them on two different data sets, one with and one wihtout heterogeneity in the treatment effect:Data SetHeterogeneityHeterogeneity VariablesVariablesObservations1No Heterogeneity – 200002Heterogeneity and  – 20000This means that in the first data set, all observations have the same treatment effect. In this case, the average treatment effect and the heterogeneous treatment effects are the same. In the second data set, the treatment effect varies with the variables  and . Without going into too much detail here (I will probably write a separate blog post only about causal data generating processes), the relationship between those heterogeneity variables ( and ) and the treatment effect is not linear. Both simulated data sets have 20’000 observations containing an outcome variable  and 10 covariates with values between zero and one. To evaluate the two algorithms, the data sets are split in a train (75%) and a test set (25%). For the Causal Forest, I use the causal_forest() from the grf-package with tune.parameters = ""all"". I compare this to an lm() model, which includes all variables, the treatment indicator and the necessary interaction terms of the heterogeneity variables and the treatment indicator:Linear Regression Model for data set with heterogeneity:        Linear Regression Model for data set with no heterogeneity:    where  –  are the heterogeneity variables and  is the treatment indicator (i.e.  if treated and  if not treated). As already explained above, we usually do not know which variables affect the treatment effect and have therefore to include all possible interaction terms into the Linear Regression Model to see which variables lead to treatment effect heterogeneity. In the case of 10 variables as we have it here, this means we would have to include a total of 2048 parameters in our Linear Regression Model. However, since the heterogeneity variables are known in the simulated data, here, I only include the interaction terms for those variables.Data SetMetricgrflmNo HeterogeneityRMSE0.010.00HeterogeneityRMSE0.080.45Looking at the results, we can see that without heterogeneity, the treatment effect is equally well predicted by the Causal Forest (RMSE of 0.01) and the Linear Regression (RMSE of 0.00). However, as the heterogeneity level increases, the Causal Forest is far more accurate (RMSE of 0.08) than the Linear Regression (RMSE of 0.45). As expected, the Causal Forest seems to be better at detecting the underlying non-linear relationship between the heterogeneity variables and the treatment effect than the Linear Regression Model, which can also be seen in the plots below. Thus, even if we already know which variables influence the treatment effect and only need to include the necessary interaction terms, the Linear Regression Model is still less accurate than the Causal Forest due to its lack of modelling flexibility. OutlookI hope that this blog post has helped you to understand what Causal Forests are and what advantages they bring in estimating heterogeneous treatment effects compared to old estimation methods. In my upcoming blog posts on Causal Machine Learning, I will explore this new field of data science further. I will, for example, take a look at the problems of using classical Machine Learning algorithms to estimate causal effects in more detail or introduce different data generating processes to evaluate Causal Machine Learning methods in simulation studies.ReferencesAthey, S., Tibshirani, J., &amp; Wager, S. (2019). Generalised random forests. The Annals of Statistics, 47(2), 1148-1178.Knaus, M. C., Lechner, M., &amp; Strittmatter, A. (2018). Machine learning estimation of heterogeneous causal effects: Empirical monte carlo evidence. arXiv:1810.13237v2.Über den AutorLivia EichenbergerI am a data scientist at STATWORX and especially interested in Causal Machine Learning. I love the logic in data science and the beauty of neat and structured code!.e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/machine-learning-goes-causal-ii-meet-the-random-forests-causal-brother/;Statworx;  Livia Eichenberger
  29. Januar 2020;Machine Learning Goes Causal I: Why Causality Matters;"At STATWORX, we are excited that a new promising field of Machine Learning has evolved in recent years: Causal Machine Learning. In short, Causal Machine Learning is the scientific study of Machine Learning algorithms that allow estimating causal effects. Over the last few years, different Causal Machine Learning algorithms have been developed, combining the advances from Machine Learning with the theory of causal inference to estimate different types of causal effects. My colleague Markus has already introduced some of these algorithms in an earlier blog post. As Causal Machine Learning is a rather complex topic, I will write a series of blog posts to slowly dive into this new fascinating world of data science. This first blog post is an introduction into the topic, focusing on what Causal Machine Learning is and why it is important in practice and for the future of data science. The Origins of Causal Machine LearningAs Markus has already explained in his earlier blog post, analysis in economic and other social sciences revolves primarily around the estimation of causal effects, that is, the isolated effect of a feature  on the outcome variable . An example, which has been investigated by my colleague Matthias, is the causal effect of oil prices on gas prices. Actually, in most cases, the interest lies in so-called treatment effects. A treatment effect refers to a causal effect of a treatment or intervention on an outcome variable of scientific or political interest. In economics, one of the most analyzed treatment effects is the causal effect of a subsidized training program on earnings. Following the potential outcome framework introduced by Rubin (1947), the treatment effect of an individual is defined as follows:    where  indicates the potential outcome of the individual  with treatment and contrary,  denotes the potential outcome of the individual  without treatment. However, as an individual can either receive the treatment or not, and thus, we can only ever observe one of the two potential outcomes for an individual at one point in time, the individual treatment effect is unobservable. This problem is also known as the Fundamental Problem of Causal Inference. Nevertheless, under certain assumptions, the averages of the individual treatment effect may be identified. In randomized experiments, where the treatment is randomly assigned, these assumptions are naturally valid, and the identification of any aggregation level of individual treatment effects is possible without further complications. In many situations, however, randomized experiments are not possible, and the researcher has to work with observational data, where these assumptions are usually not valid. Thus, an extensive literature in economics and other fields has focused on techniques identifying causal effects in cases where these assumptions are not given. Prediction and causal inference are distinct (though closely related) problems. Athey, 2017, p. 484In contrast, (Supervised) Machine Learning literature has traditionally focused on prediction, that is, produce predictions of the outcome variable  from the feature(s) . Machine Learning models are designed to discover complex structures in given data and generalize them so that they can be used to make accurate predictions on new data. These algorithms can handle enormous numbers of predictors and combining them in nonlinear and highly interactive ways. They have been proven to be hugely successful in practice and are used in applications ranging from medicine to resource allocations in cities.Bringing Together the Best of Both WorldsAlthough economists and other social scientists prioritize precise estimates of causal effects above predictive power, they were intrigued by the advantages of Machine Learning methods, such as the precise out-of-sample prediction power or the ability to deal with large numbers of features. But as we have seen, classical Machine Learning models are not designed to estimate causal effects. Using off-the-shelf prediction methods from Machine Learning leads to biased estimates of causal effects. The existing Machine Learning techniques had to be modified to use the advantages of Machine Learning for consistently and efficiently estimating causal effects– the birth of Causal Machine Learning!Currently, Causal Machine Learning can be broadly divided into two lines of research, defined by the type of causal effect to be estimated. One line of Causal Machine Learning research focuses on modifying Machine Learning methods to estimate unbiased and consistent average treatment effects. The average treatment effect is the mean of all individual treatment effects in an entire population of interest, and probably the most common parameter analyzed in econometric causal studies. Models from this line of research try to answer questions like: How will customers react on average to a marketing campaign? What is the average effect of a price change on sales? The other line of Causal Machine Learning research focuses on modifying Machine Learning methods to uncover treatment effect heterogeneity. That is, identifying subpopulations (based on features) of individuals who have a larger or smaller than average treatment effect. These models are designed to answer questions such as: Which customers respond the most to a marketing campaign? How does the effect of a price change on sales change with the age of customers?Decision-Making Questions Need Causal AnswersAlthough the study of Causal Machine Learning has been mainly driven by economic researchers, its importance for other areas such as business should not be neglected. Companies often reach for classical Machine Learning tools to solve decision-making problems, such as where to set the price or which customers to target with a marketing campaign. However, there is a significant gap between making a prediction and making a decision. To make a data-driven decision, the understanding of causal relationships is key. Let me illustrate this problem with two examples from our daily business. Example 1: Price ElasticitiesAt the core of every company’s pricing management is the understanding of how customers will respond to a change in price. To set an optimal price, the company needs to know how much it will sell at different (hypothetical) price levels. The most practicable and meaningful metric answering this question is the price elasticity of demand. Although it might seem straightforward to estimate the price elasticity of demand using classical Machine Learning methods to predict sales as the outcome with the price level as a feature, in practice, this approach does not simply give us the causal effect of price on sales. There are a number of gaps between making a prediction and making a decision, and underlying assumptions need to be understood in order to optimise data-driven decision making.Athey, 2017, p. 483Following a similar example introduced by Athey (2017), assume we have historical data on airline prices and the respective occupancy rates. Typically, prices and occupancy rates are positively correlated as usual pricing policies of airlines specify that airlines raise their seat prices when their occupancy rate increases. In this case, a classical Machine Learning model will answer the following question: If on a particular day, airline ticket prices are high, what is the best prediction for the occupancy rate on that day? The model will correctly predict that the occupancy rate is likely to be high. However, it would be wrong to infer from this that an increase in prices leads to a higher occupancy rate. From common experience, we know that the true casual effect is quite the opposite – if an airline systematically raised its ticket prices by 10% everywhere, it would be unlikely to sell more tickets.Example 2: Customer ChurnAnother common problem, which companies like to solve with the help of Machine Learning, is the prediction of customer churn (i.e., customers abandoning the firm or service thereof). The companies are interested in identifying the customers with the highest risk of churn so that they can respond by allocating interventions in the hope of preventing these customers from leaving. Classical Machine Learning algorithms have proven to be very good at predicting customer churn. Unfortunately, these results cannot sufficiently address the company’s resource allocation problem of which customers to best target with intervention strategies. The question of the optimal allocation of resources to customers is of causal nature: For which customers are the causal effect of intervention strategies on their churn behavior the highest? A study has shown that in many cases, the overlap between customers with the highest risk of churning and customers who would respond most to interventions was much lower than 100%. Thus, treating the problem of customer churn as a prediction problem and therefore using classical Machine Learning models is not optimal, yielding lower payoffs to companies.The Wish of Every Data ScientistLooking beyond these practical examples, we can observe that there is a more profound reason why Causal Machine Learning should be of interest to any data scientist: model generalisability. A Machine Learning model that can capture causal relationships of data will be generalizable to new settings, which is still one of the biggest challenges in Machine Learning. To illustrate this, I’ll use the example of the rooster and the sun, from „The Book of Why“ by Pearl and Mackenzie (2018). A Machine Learning algorithm that is shown data about a rooster and the sun would associate the rising of the sun with the crow of the rooster and may be able to predict when the sun will rise accurately: If the rooster has just crowed, the sun will rise shortly after that. Such a model that is only capable of predicting correlations will not generalize to a situation where there is no rooster. In that case, a Machine Learning model will never predict that the sun will rise because it has never observed such a data point (i.e., without a rooster). If, however, the model captured the true causal relationship, that is, the sun being about to rise causes the rooster to crow, it would be perfectly able to predict that the sun will rise even if there is no rooster.No True Artificial Intelligence Without Causal ReasoningPearl and Mackenzie (2018) go even further, arguing that we can never reach true human-level Artificial Intelligence without teaching machines causal reasoning since cause and effect are the key mechanisms through which we humans process and understand the complex world around us. The ability to predict correlations does not make machines intelligent; it merely allows them to model a reality based on data the algorithm is provided. The algorithmisation of counterfactuals invites thinking machines to benefit from the ability to reflect on one’s past actions and to participate in this (until now) uniquely human way of thinking about the world.Pearl &amp; Mackenzie, 2018, p. 10Furthermore, Machine Learning models need the capacity to detect causal effects to ask counterfactual questions, that is, to inquire how some relationship would change given some kind of intervention. As counterfactuals are the building blocks of moral behavior and scientific thought, machines will only be able to communicate more effectively with us humans and reach the status of moral beings with free will if they learn causal and thus counterfactual reasoning. OutlookAlthough this last part has become very philosophical in the end, I hope that this blog post has helped you to understand what Causal Machine Learning is and why it is necessary not only in practice but also for the future of data science in general. In my upcoming blog posts, I will discuss various aspects of this topic in more detail. I will, for example, take a look at the problems of using classical Machine Learning algorithms to estimate causal effects in more detail or compare different Causal Machine Learning algorithms in a simulation study.ReferencesAthey, S. (2017). Beyond prediction: using big data for policy problems. Science 335, 483-485.Pearl, J., &amp; Mackenzie, D. (2018). The book of why. New York, NY: Basic Books.Rubin, D. B. (1974). Estimating causal effects of treatments in randomised and non-randomised studies. Journal of Educational Psychology, 66(5), 688-701.Über den AutorLivia EichenbergerI am a data scientist at STATWORX and especially interested in Causal Machine Learning. I love the logic in data science and the beauty of neat and structured code!.e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/machine-learning-goes-causal-i-why-causality-matters/;Statworx;  Livia Eichenberger
  23. Januar 2020;How To Create REST APIs With R Plumber;"Data operations is an increasingly important part of data science because it enables companies to feed large business data back into production effectively. We at STATWORX, therefore, operationalize our models and algorithms by translating them into Application Programming Interfaces (APIs). Representational State Transfer (REST) APIs are well suited to be implemented as part of a modern micro-services infrastructure. They are flexible, easy to deploy, scale, and maintain, and they are further accessible by multiple clients and client types at the same time. Their primary purpose is to simplify programming by abstracting the underlying implementation and only exposing objects and actions that are needed for any further development and interaction. An additional advantage of APIs is that they allow for an easy combination of code written in different programming languages or by different development teams. This is because APIs are naturally separated from each other, and communication with and between APIs is handled by IP or URL (http), typically using JSON or XML format. Imagine, e.g., an infrastructure, where an API that’s written in Python and one that’s written in R communicate with each other and serve an application written in JavaScript.In this blog post, I will show you how to translate a simple R script, which transforms tables from wide to long format, into a REST API with the R package Plumber and how to run it locally or with Docker. I have created this example API for our trainee program, and it serves our new data scientists and engineers as a starting point to familiarize themselves with the subject.Translate the R ScriptTransforming an R script into a REST API is quite easy. All you need, in addition to R and RStudio, is the package Plumber and optionally Docker. REST APIs can be interacted with by sending a REST Request, and the probably most commonly used ones are GET, PUT, POST, and DELETE. Here is the code of the example API, that transforms tables from wide to long or from long to wide format:## transform wide to long and long to wide format#' @post /widelong#' @get /widelongfunction(req) {  # library  require(tidyr)  require(dplyr)  require(magrittr)  require(httr)  require(jsonlite)  # post body  body &lt;- jsonlite::fromJSON(req$postBody)  .data &lt;- body$.data  .trans &lt;- body$.trans  .key &lt;- body$.key  .value &lt;- body$.value  .select &lt;- body$.select  # wide or long transformation  if(.trans == 'l' || .trans == 'long') {    .data %&lt;&gt;% gather(key = !!.key, value = !!.value, !!.select)    return(.data)  } else if(.trans == 'w' || .trans == 'wide') {    .data %&lt;&gt;% spread(key = !!.key, value = !!.value)    return(.data)  } else {    print('Please specify the transformation')  }}As you can see, it is a standard R function, that is extended by the special plumber comments @post and @get, which enable the API to respond to those types of requests. It is necessary to add the path, /widelong, to any incoming request. That is done because it is possible to stack several API functions, which respond to different paths. We could, e.g., add another function with the path /naremove to our API, which removes NAs from tables.The R function itself has one function argument req, which is used to receive a (POST) Request Body. In general, there are two different possibilities to send additional arguments and objects to a REST API, the header and the body. I decided to use a body only and no header at all, which makes the API cleaner, safer and allows us to send larger objects. A header could, e.g., be used to set some optional function arguments, but should be used sparsely otherwise.Using a body with the API is also the reason to allow for GET and POST Requests (@post, @get) at the same time. While some clients prefer to send a body with a GET Request, when they do not permanently post something to the server etc., many other clients do not have the option to send a body with a GET Request at all. In this case, it is mandatory to add a POST Request. Typical clients are Applications, Integrated Development Environments (IDEs), and other APIs. By accepting both request types, our API, therefore, gains greater response flexibility.For the request-response format of the API, I have decided to stick with the JavaScript Object Notation (JSON), which is probably the most common format. It would be possible to use Extensible Markup Language (XML) with R Plumber instead as well. The decision for one or the other will most likely depend on which additional R packages you want to use or on which format the API’s clients are predominantly using. The R packages that are used to handle REST Requests in my example API are jsonlite and httr. The three Tidyverse packages are used to do the table transformation to wide or long.RUN the APIThe finished REST API can be run locally with R or RStudio as follows:library(plumber)widelong_api &lt;- plumber::plumb(""./path/to/directory/widelongwide.R"")widelong_api$run(host = '127.0.0.1', port = 8000)Upon starting the API, the Plumber package provides us with an IP address, and a port and a client, e.g., another R instance, can now begin to send REST Requests. It also opens a browser tool called Swagger, which can be useful to check if your API is working as intended. Once the development of an API is finished, I would suggest to build a docker image and run it in a container. That makes the API highly portable and independent of its host system. Since we want to use most APIs in production and deploy them to, e.g., a company server or the cloud, this is especially important. Here is the Dockerfile to build the docker image of the example API: FROM trestletech/plumber# Install dependenciesRUN apt-get update --allow-releaseinfo-change &amp;&amp; apt-get install -y \    liblapack-dev \    libpq-dev# Install R packagesRUN R -e ""install.packages(c('tidyr', 'dplyr', 'magrittr', 'httr', 'jsonlite'), \repos = 'http://cran.us.r-project.org')""# Add APICOPY ./path/to/directory/widelongwide.R /widelongwide.R# Make port availableEXPOSE 8000# EntrypointENTRYPOINT ""R"", ""-e"", \""widelong &lt;- plumber::plumb('widelongwide.R'); \widelong$run(host = '0.0.0.0', port= 8000)""CMD ""/widelongwide.R""Send a REST RequestThe wide-long example API can generally respond to any client sending a POST or GET Request with a Body in JSON format, that contains a table in csv format and all needed information on how to transform it. Here is an example for a web application, which I have written for our trainee program to supplement the wide-long API:The application is written in R Shiny, which is a great R package to transform your static plots and outputs into an interactive dashboard. If you are interested in how to create dashboards in R, check out other posts on our STATWORX Blog.Last but not least here is an example on how to send a REST Request from R or RStudio:library(httr)library(jsonlite)options(stringsAsFactors = FALSE)# url for local testingurl &lt;- ""http://127.0.0.1:8000""# url for docker containerurl &lt;- ""http://0.0.0.0:8000""# read example stock data.data &lt;- read.csv('./path/to/data/stocks.csv')# create example bodybody &lt;- list(  .data = .data,  .trans = ""w"",  .key = ""stock"",  .value = ""price"",  .select = c(""X"",""Y"",""Z""))# set API pathpath &lt;- 'widelong'# send POST Request to APIraw.result &lt;- POST(url = url, path = path, body = body, encode = 'json')# check status coderaw.result$status_code# retrieve transformed example stock data.t_data &lt;- fromJSON(rawToChar(raw.result$content))As you can see, it is quite easy to make REST Requests in R. If you need some test data, you could use the stocks data example from the Tidyverse.SummaryIn this blog post, I showed you how to translate a simple R script, which transforms tables from wide to long format, into a REST API with the R package Plumber and how to run it locally or with Docker. I hope you enjoyed the read and learned something about operationalizing R scripts into REST APIs with the R package Plumber and how to run them locally and with Docker. You are of welcome to copy and use any code from this blog post to start and create your REST APIs with R.Until then, stay tuned and visit our STATWORX Blog again soon.We’re hiring!Data Engineering is your jam and you’re looking for a job?  We’re currently looking for Junior Consultants and Consultants in Data Engineering. Check the requirements and benefits of working with us on our career site. We’re looking forward to your application!Über den AutorStephan EmmerI am a data scientist at STATWORX and to work with data in a professional way on a day to day basis is just AWESOME!.e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/how-to-create-rest-apis-with-r-plumber/;Statworx;  Stephan Emmer
  15. Januar 2020;Insights From Our Latest Internal Hackathon;"A few weeks ago, several of my colleagues here at STATWORX and I participated in an exciting time series competition, hosted by a large german-based company. This was the opportunity for us to organize an internal hackathon in our office in Frankfurt. A hackathon like this has the purpose of combining knowledge to provide an excellent solution for a competition in a short time.In this blog article, I will give you an insight into how we plan and execute such hackathons at STATWORX. At first, I’m going to describe the initial situation, the specific problem we had to solve, and who participated in the hackathon. Then I’ll get to the organizational process, which tasks came up and how we distributed them. Of course, the methodological approach will also be discussed in this part. Finally, the results are then roughly discussed, and I’ll show you what the possible development looks like now.The team gathered in our headquarter in Frankfurt for the hackathon.Our preparation and first stepsQuite a few of my teammates offered to work together on this competition. Our team consisted of Sebastian (Founder &amp; CEO of STATWORX), Fabian (Head of Data Science), my Data Science colleagues Jakob, Fran, Alexander, and Andreas. We gathered in the office on the weekend, so we could really concentrate on the hackathon. Of course, everyone was roughly informed about what the competition was about. As already mentioned, it was about forecasting the daily sales quantity for the year 2019. Therefore, we got information about 10 different products (A-J). In addition, we received information about promotions until 2019. The data looked like this:After a short first glimpse, you can see that the time series are very volatile, and a clear trend cannot be identified in every time series. There are also some outliers, with the products I, J, and E particularly stand out. FYI: The data has been edited a little for this article and does correspond to the original data that we received.How did we distribute the tasks?Discussion of data and emerging tasksFirst of all, we sat down and discussed the provided data. Therefore, Fabian prepared the first descriptive analyzes. We talked about possible seasonalities, trends, effects of the given promotion information, potential problems, and external features. Starting from the discussion, we identified and distributed the corresponding tasks. Thus, three main topics have emerged: The data preparation had to be completed (e.g., outliers and external features). As a benchmark for our models, simple forecasts needed to be modeled. And of course, our final forecasts had to be modeled.Data preparation and outliersSince the explorative analysis and the discussion resulted in potential features, Sebastian and Jakob prepared external features. For this purpose, temperature data were downloaded using an API and then prepared for our dataset. Moreover, event data such as holidays were prepared accordingly, whereby there was also a feature engineering. I examined the time series for outliers and imputed them differently. For this purpose, the R package anomalize was also used, which is very helpful for quick preliminary outlier detection. The imputation was then followed by seasonality and trend. Above all, product I had many extreme outliers. But also with other products, there were some strong outliers (e.g., J and E).Modeling and evaluationAs a forecast for the full year 2019 should be prepared daily, we have therefore used the year 2018 for the evaluation. However, to be able to judge how well our models perform, we need corresponding benchmarks, as mentioned before. For this purpose, Fran and Andreas created corresponding forecasting models. Among other things, a simple auto Arima and naive forecasts were used, and the results were very different for the various products. As an evaluation metric, the MAPE was used, which also holds specific weaknesses. If you are interested in the MAPE, the accompanying weaknesses, and possible alternatives, you can read the blog post about this exact topic, written by my colleague Jan.ProductMAPE (auto arima)MAPE (naive Forecast)A0.3160.326B0.2180.306C0.2430.271D0.2430.316E3.7633.895F0.2310.272G0.1690.226H0.5640.468I1.0761.047J1.1191.672Some products have a large MAPE of up to more than 370%. Especially for product E, this can be strongly justified by the time series, in which there was a significant decline over one month in 2018. Sales averaged around 560 from the beginning of August to the middle of September, while the average for 2018 was around 12000. Other products also suffered such break-ins. This naturally leads to high errors. Unfortunately, we did not know whether this was a data error, a production problem, or something similar. Moreover, the product I had huge outliers, which on some days were more than 10 times higher than the average.What was the outcome?For our forecasting models, we used different approaches. We mainly used XGBoost, Light GBM, and Deep Learning, which was a 3-layer MLP with dropout and batch-norm. If you want to get started with Deep Learning on your own, you should read this blog post from Sebastian („Forecasting ‚Last Christmas‘ Search Volume on Google Trends using Deep Learning“). Of course, there are other models that we could have tried, even Random Forest can be used for time series forecasts. But we had relatively little information about the products and a limited amount of time. Additionally, all models were at the end a 365-days-ahead forecast, which is generally expected to result in worse results. However, let’s take a look at the results.ProductLight GBM (MAPE)Light GBM vs BenchmarkA0.276-0.04B0.206-0.012C0.209-0.034D0.215-0.028E0.477-3.286F0.180-0.051G0.141-0.028H0.367-0.101I0.600-0.476J0.902-0.217Although the MAPE is still high for some products, there is also a significant improvement compared to the benchmarks. The MAPE for product E has been reduced from over 370% to less than 50%. For products I and J, there was also a significant improvement of 20 to 40 percentage points. For other products, however, there was only a slight improvement in a few percentage points. That’s it? Of course not!As you surely already expect, there are also a lot of potential improvements. The knowledge of the exact origin process of the data, which is so far unknown to us, gives possible hints on aspects, which could be considered in the modeling. Likewise, the forecasting period itself represents considerable potential for improvement. Of course, a 365-day-ahead forecast can not provide high accuracy. However, rolling quarterly or monthly forecasting can lead to a significant improvement. Similarly, lags could be used for shorter periods, which, unfortunately, could not be used with our models. The list of possible optimizations, like information about competitors, regional data, and so forth, is exceptionally long.It should be noted that the company didn’t want a 365-day forecast, but a rolling 7-day forecast. Unfortunately, this did not emerge from the communicated task. The results would, of course, have been much better in this way, but they were still very well received. Possibly, our efforts will amount to a project in 2020, which we are really looking forward to.It was a fun experience, and I hope that I could provide you with an entertaining insight into our hackathon here at STATWORX. If you’re interested in staying updated with everything that happens at our company, join our mailing list and we’ll keep you posted! If you want to join our team and possibly take part in events like an internal hackathon, check our job offerings on our website. We’re looking forward to your application!Über den AutorMarlon SchumacherI am a data scientist at STATWORX and like working with data. Whether it's visualization or the development of machine learning models, it's always interesting..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/insights-from-our-latest-internal-hackathon/;Statworx;  Marlon Schumacher
  8. Januar 2020;STATWORX – R for Data Science Workshop;"Neben unseren Beratungsaufgaben haben wir bei STATWORX einen stetig wachsenden Academy Bereich. In der STATWORX Academy bieten wir zu verschiedenen Themengebieten und Programmiersprachen anwendungsorientierte Schulungen an. Dies umfasst Kurse in Python, R und zu speziellen Frameworks, wie zum Beispiel TensorFlow. Ein Kurs, den ich bereits mit begleiten und führen durfte, ist R for Data Science Workshop.Für wen ist der Workshop geeignet?Sowohl R Einsteiger als auch Fortgeschrittene können problemlos am Workshop teilnehmen. Zu Beginn erarbeiten wir uns erst einmal die Grundlagen der Programmiersprache R, damit alle auf dem gleichen Level lernen können. An zwei Tagen vermitteln wir die wichtigsten Kernkompetenzen eines Data Scientists und die Teilnehmenden erhalten einen Einblick in die vielfältigen Schritte des Data Science Workflows.Im folgenden Artikel gebe ich Euch eine Übersicht über den Ablauf und die Inhalte der zweitägigen Schulung. Und wer weiß, vielleicht habt auch Ihr mal Lust an einem unserer Workshops teilzunehmen.Was sind die Inhalte des Workshops R for Data Science?Einstieg in Data Science und RDer erste Tag beginnt mit einer kurzen Einführung in das Gebiet der Data Science und in die Grundlagen der Programmierung mit R. Viele der Lösungen, die wir im Rahmen der Projektarbeit bei STATWORX entwickeln, können mit dieser open-source Software effizient und elegant umgesetzt werden. R wird zunehmend als Standard für statistische Problemstellungen sowohl in der Wirtschaft als auch in der Wissenschaft benutzt. Sie eignet sich besonders gut für Datentransformation, Datenvisualisierung und das Trainieren von Machine Learning Modellen. Anschließend widmen wir uns dem tidyverse, einer Kollektion von packages die ein gemeinsames Verständnis der „design philosophy, grammar and data structures“ vereint. Mit dplyr und tidyr lassen sich Daten kinderleicht manipulieren, selektieren, bereinigen, mergen und aggregieren. Der Workshop beinhaltet sowohl eine übersichtliche Präsentation der Möglichkeiten und der Anwendungen der Funktionen aus den behandelten packages, als auch intensive Hands-on Sessions, bei denen die Teilnehmenden Ihr neu erlangtes Wissen direkt anwenden und verfestigen können.Daten visualisieren und Machine Learning mit RIn der ersten Hälfte des zweiten Tages werden die schier unendlichen Möglichkeiten der Datenvisualisierung mit ggplot2 entdeckt. Neben der Ebenenlogik dieses Paketes, zeichnet es sich besonders durch sehr ästhetische Darstellungen aus. Nach einer gemütlichen Mittagspause beim benachbartem Mexikaner im Frankfurter Westend, steht nun einer der spannendsten Abschnitte des Workshops auf dem Programm: Machine Learning. Nach einer kurzen Einführung in grundlegende Techniken wie Train-Test-Split, Kreuzvalidierung und Evaluierungsmetriken, lernen die Teilnehmenden die theoretische Fundierung und die praktische Anwendung von fortgeschrittenen Machine Learning Algorithmen wie Random Forest und Gradient Boosting Machine. Caret ermöglicht dazu den perfekten Einstieg. Random Forest und Hyperparameter Tuning mit CaretDer Random Forest ist ein Klassiker unter den Machine Learning Algorithmen. Er zeichnet sich vorallem durch eine breite Anwendbarkeit, Robustheit und sehr gute Prognosegüte aus und wird deshalb gerne als Benchmark Algorithmus benutzt. Er basiert auf dem Konzept von Entscheidungsbäumen. Diese teilen die vorhandenen Beobachtungen in verschiedene Gruppen auf, die möglichst homogen in der Target Variable sind, also eine große Erklärungskraft hinsichtlich dieser besitzen. Bei einem Random Forest werden viele einzelne Entscheidungsbäume erstellt, die dann zu einem übergeordneten Modell zusammengefasst werden. Eine Besonderheit des Algorithmus ist, dass an jedem Entscheidungsknoten die in Betracht gezogenen Features zufällig aus der Gesamtheit der im Modell definierten Features gezogen werden. Die genaue Anzahl dieser in Betracht gezogenen Features wird im Folgenden über den Parameter „mtry“ definiert.Das folgende Code Snippet zeigt, wie man mit Caret in ein paar Zeilen einen Random Forest trainieren kann. Mit Caret kann man zudem sehr schnell und einfach Hyperparameter Tuning implementieren. Im Beispiel wird eine Kreuzvalidierung mit 5 Folds benutzt um den optimalen Wert für den Parameter „mtry“ zu bestimmen. # Load caretlibrary(caret)# Define traincontrol parametertrain_opt &lt;- trainControl(method = ""cv"", # use cross-validation as resampling method                         number = 5, # use 5 folds                         search = ""grid"", # try all values defined in grid                         verboseIter = TRUE) # print training logs# Define grid for hyperparameter tuning# define values of the ""mtry"" tuning parametergrid &lt;- expand.grid(mtry = c(10, 20, 30))# Train the random forestmod_rf &lt;- train(form = target ~., # define target variable               data = df_train, # define training data               method = ""rf"", # define algorithm               trControl = train_opt, # use traincontrol parameter defined above               tuneGrid = grid) # use grid defined above# Create predictionspredict(mod_rf, newdata = df_test)# Extract variable importancevarImp(mod_rf)Interesse geweckt, aber Termin verpasst?Du möchtest mehr über die Anwendung von R für Data Science Aufgabenbereiche lernen? In unserem R for Data Science Workshop geben Dir unsere erfahrenen Trainer in nur zwei Tagen einen praxisorientierten Einstieg in die Welt von Data Science. In unserer Übersicht über offene Kurse mit R bekommst Du alle wichtigen Infos zu Terminen, Kosten und Inhalten. Es kann dabei jeder mit Interesse am Thema Data Science teilnehmen, da Programmiervorkenntnisse zwar hilfreich sein können, aber keine Voraussetzung sind!Über den AutorJan FischerI am a data scientist at STATWORX. I always enjoyed to think critically about complex problems, understand and find a solution. Fortunately, STATWORX pays me for that!.e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/statworx-r-for-data-science-workshop/;Statworx;  Jan Fischer
  20. Dezember 2019;STATWORX 2019 – A Year in Review;"Once again, an amazing year at STATWORX is coming to an end. The frequency and magnitude of positive things happening to our company are continuously increasing with every new client, employee, or partner coming to our company. That is why the very first paragraph of this post ends with a massive THANK YOU to my whole team, our customers and partners. We would not be where we are now without you. Let’s keep up the awesome work!  STATWORX is constantly growing each year, both in revenue and profit, as well as in services and products – likewise in 2019. Our growth deeply embodies a high pace of change into our company’s DNA and all the things that surround us. That is why sometimes you can lose track of all the different things happening throughout the year. To keep track of our evolving company, I’ve decided to write down some of the most important things that moved our company this year, ranging from exciting projects, new partners to great festivities and special events. Let’s begin the time travel!   Projects &amp; AcademyThe various kinds of data science, machine learning, and AI projects, which we deliver for our cross-industry customers, are the cornerstone of our company. And by cross-industry, I really mean it: automotive, finance, pharma, retail, insurance, aviation – you name it. I would assume that there isn’t a single industry we have not worked in so far. That greatly enlarges our perspective on the field of data science and AI and helps us to translate solutions from different industries or functions into our project environments across our customers. This year, there have been so many fascinating projects, both end-2-end as well as prototypes / MVPs, that it’s tough to pick the gems. My personal favorites would most likely be the ground operations optimization project for one of our airline customers, as well as our various ML and NLP projects we’ve conducted for the purchasing department of one of our automotive customers. Besides that, we’ve delivered several forecasting products, customer analytics solutions, as well as different dashboards and ML fueled applications. Furthermore, in our strategy department, we’ve supported several new customers from the financial and insurance industry, who are just getting started with data science and AI by setting up their data and analytics platform strategies. Whew! Besides our consulting projects, the “STATWORX Academy” brings a multitude of different data science, ML and AI trainings and workshops to our customers. One of the most exciting training projects this year was, for sure, a company-wide AI leadership training for one of our biggest customers. With a team from their professional development department, IT, and data science in finance, we’ve forged a manifold and interactive workshop, specifically targeted at middle to top management leaders. Besides multiple training sessions in Germany, this also led to cross-continental trainings in the USA as well as China. It was fascinating to discuss with leaders from various cultural backgrounds about AI and its implications on business, society and life in general. I am really looking forward to continue this training initiative in 2020. Another great workshop event this year was our brand-new “Deep Learning Bootcamp”. Besides our battle-proven „Data Science Bootcamp”, we’ve created a 5-day workshop all around neural networks and deep learning. Thereby, we are digging into the basic concepts, such as MLPs, CNNs, and LSTMs, but also into more advanced topics such as GANs, autoencoders, and deep reinforcement learning. The first Bootcamp was a great success, for which we received loads of positive feedback from our participants. I can’t wait for the next session in the first quarter of 2020, with many new topics and programming exercises in Python and TensorFlow. InvestmentsIn 2019 we’ve also conducted our first financial investments into companies and products we believe in: first to mention is bamboolib, a product two former STATWORKERS created in their newly founded company. bamboolib is a low-code data preparation and visualization GUI for pandas in Jupyter (yes, a GUI for Jupyter). It allows data science professionals to carry out time-consuming data prep and manipulation steps 10x faster. Furthermore, it enables Python and pandas starters to learn the concepts of data prep more easily. bamboolib is currently making huge waves on LinkedIn, so I definitely recommend that you check it out! Besides bamboolib, we’ve engaged in a joint-venture called ONEZERO-X that is bringing data science, machine learning, and AI to sports business. We’ve already had our first project for a German soccer club and created a great MVP for forecasting soccer stadium utilization by using historical sales data and many external effects. I will be presenting the new joint venture at IN-BETA AUDITIONS in Frankfurt in January 2020. PartnersSTATWORX is continually engaging with new partners to leverage synergies and to bring our services to a broader audience of customers. One of our longest and most fruitful partnerships is with Dataiku. The French software startup just reached unicorn-level company valuation and never ceases to surprise us with new powerful features on their innovative data science platform as well as with their excellent team spirit and collaborative work approach. Additionally, to support our clients on all major cloud platforms, we have further extended our cloud service provider network by entering partnerships with Microsoft and AWS, on top of our existing partnership with Google (GCP). Depending on high-quality data in everything we do, we understand how important it is for all companies to build up a clean and structured data foundation. Therefore, on multiple occasions, we joined forces with the data management and data governance focused University of St. Gallen spin-off CDQ. Last but not least, we added renowned semiconductor manufacturer STMicroelectronics to the list of our partners. We are thrilled to leverage their STM32 microcontrollers’ capabilities, to run AI directly on edge controllers, which allows the integration of state-of-the-art neural networks in highly specialized industrial applications. EventsWhen it comes to events and conferences, STATWORX is always ready to jump in. Not only to gather new knowledge but also to network, make new connections, and get new leads. One of our favorite events throughout the year is the Data Festival in Munich. This year, our Head of Data Science, Fabian Müller, gave an exciting talk about XAI (explainable AI), its current applications, and limitations. The Data Festival is getting bigger and bigger each year, which triggers the creativity of our marketing to come up with great new ideas for our STATWORX booth. This year, we launched our „Beat the AI“ campaign, where visitors of our booth could compete against a reinforcement learning agent that learned how to play Super Mario on NES. “Beat the AI” was also our campaign at EGG Germany 2019 in Stuttgart, the 1-day AI conference organized by our friends at Dataiku. Here, we gave a talk on how to bring data science and AI to organizations using Dataiku together with our friends from Mercedes Benz. Another exceptional event this year was the „KEX Forecasting Challenge“. Here, a selection of AI vendors was given a dataset of daily sales data that they were asked to forecast. The actual sales data was held back to ensure a fair competition. In an intense weekend session, the “STATWORX Black Ops” team managed to build a machine learning model for 10 different products that was able to forecast 365 days ahead. During the presentation of the results, we’ve received great feedback from the data supplying company and currently engage in a joint project. Another highlight of 2019 was our participation in the Swiss AI Hackathon, where we competed against 10 other teams in the development of a machine learning model for turnaround delay prediction. This 2-day event in Zurich certainly was one of the most challenging events this year, since we had a great variety of data sources to deal with to make the model work. After 2-days of coffee, red bull, beer, and pizza, the STATWORX team made it on to the winners‘ podium and presented our modeling approach in front of several members of the Swiss BOM. Data UniversityAs you could read up until here, the last year was filled with new opportunities and challenges for us. One of those challenges for us as a team was the Data University, our first-ever open event. We created the event with the help of our partner BARC in early 2019. Together, we developed a marketing plan, designed the curriculum, built a website, and created the workshops. In October, we held the interactive 2-day workshop-event at the Goethe University in Frankfurt, with 40 participants, 8 trainers, and a whole lot of pizzas. Furthermore, our friends from Frankfurt Data Science organized an event in the evening about “How to become a Data Scientist” with over 100 participants that perfectly matched our topics at Data University. I want to thank everyone at BARC again for the great partnership. If you’re interested in our newest events and plans, then follow us on our various social media accounts and stay tuned. Work eventsWith all the hard work we do here at STATWORX, we also like to relax and have some after-work fun. There have been multiple occasions during the past twelve months, where my team prepared parties, fun get-togethers and excursions to take our collective minds off of work. One of these occasions was the famous STATWORX summer barbecue, which took place on a very hot Friday evening in June. Everyone prepared something to eat, like a salad or tasty finger food, and we enjoyed the great weather with some fresh cocktails on our terrace. But the barbecue wasn’t the only time we had great luck with the weather. In October, part of our crew went on a day trip to the picturesque Rheingau, the famous wine region known for its world-famous Riesling wines. With sunglasses on and a glass of delicious white wine in hand, the group went hiking for about four hours in the vineyards. Last but not least, the Christmas spirit found its way into the STATWORX office. For our Christmas party in late November, the team prepared a large pot of hot wine, wrapped gifts for everyone, and showcased some very creative Ugly Sweaters. To finalize this amazing year, we also took a weekend trip to Belgium, where we time to unwind, talk, play games, laugh, and party. As you can see, we had a lot of fun days and nights together. A big shout out to everyone who helped organize these events. Cheers! Outlook on 2020What a year! By writing all of this down, I started to realize how diverse and inspiring all of the different things were that happened at our company in the last year. And there is always more to come: in the first half of 2020, we will move into the brand-new STATWORX HQ in Frankfurt on 1’400 sqm on 2 floors. This will be our biggest and boldest move so far, which will enable our company and employees to grow, prosper, and develop even further. In terms of services, we will continue to strengthen our team and offerings in data and software engineering to accord with the skyrocketing demand for ML ops and industrialization of machine learning and AI applications. In addition to that, we will be launching our first own AI software product in 2020, which will be another massive step for our company on our collective journey along the road of STATWORX. I can’t wait to see what 2020 holds for us! The only thing that remains constant in life is change. I am wishing you a happy holiday and a joyful new year. Best wishes from your friends at STATWORX. This text was written by GPT-2 (just kidding). Über den AutorSebastian HeinzI am the founder and CEO of STATWORX. I enjoy writing about machine learning and AI, especially about neural networks and deep learning. In my spare time, I love to cook, eat and drink as well as traveling the world..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/statworx-2019-a-year-in-review/;Statworx;  Sebastian Heinz
  19. Dezember 2019;Is it the most wonderful time of the year?;"Wie jedes Jahr in der Vorweihnachtszeit, dreht sich auch diesen Dezember bei den STATWORX-Mitarbeiter*innen alles um Weihnachtsmusik, Geschenke und Glühwein. Korrekterweise muss ich aber sagen, dass nicht alle Kolleg*innen so verrückt nach Weihnachten sind wie ich oder meine Kollegin Anne-Marie. Aber darf man sie gleich in die Grinch-Schubladen stecken, nur weil sie keine kompletten Weihnachtsfreaks sind?  Um unsere Kolleg*innen in puncto Weihnachtsverrücktheit besser einschätzen zu können, hatte Anne-Marie die Idee die Einstellung zu einigen Aspekten, die mit Weihnachten einhergehen, abzufragen, um sie anhand dieser Ergebnisse ihrem Weihnachtstyp zuzuordnen. Vor allem bei der Einteilung dieser Weihnachtstypen stießen wir jedoch auf einige Hürden. Wir wollten gerne direkt nach der Beantwortung aller Fragen den jeweiligen Typ angezeigt haben, wollten aber auch ein möglichst zutreffendes Ergebnis erhalten. Wir hatten die Wahl zwischen der Einteilung anhand eines Summenscores oder der Verwendung einer Clusteranalyse zur Gruppierung der Befragten. Letztendlich haben wir uns für ersteres entschieden. Aber so ganz möchte ich mich noch nicht mit dem Summenscore zufriedengeben. Daher nutze ich diesen Blogartikel, um zu erläutern, warum hier unterschiedliche Ergebnisse zu vermuten sind und möchte auch praktisch prüfen, ob sich die Ergebnisse einer Clusteranalyse grundlegend von unserer Einteilung anhand der Summenscores unterscheiden. Fragen über FragenWie kam die Weihnachtsumfrage eigentlich zustande? Nach der Entscheidung, dass wir die Umfrage erstellen möchten, entwickelten wir Fragen zu Themen wie der Nutzung von Adventskalendern, dem Essen an Heiligabend oder der Notwendigkeit von Weihnachtsdekoration. Anne-Marie, die im Marketing arbeitet und vor Kreativität strotz, ließ sich witzige Antworten auf unsere Fragen einfallen. Meine Befürchtung war aber, dass die wissenschaftliche Ernsthaftigkeit der Umfrage verloren gehen könnte und nahm ihr damit den ganzen Spaß.Ich konnte etwas damit besänftigt werden, dass wir bei jeder Frage noch eine Kategorie hinzufügten, bei der angegeben werden kann, dass keine der Antworten zutrifft. Die Antwortkategorien sind außerdem so gestaltet, dass es neben der neutralen Kategorie jeweils vier Antwortkategorien gibt. Diese sind so formuliert, dass sie eine ordinale Struktur aufweisen, eine Kategorie also den größten Weihnachtsfan kennzeichnet und die Ausprägungen dann immer schwächer in Richtung Grinch werden. Bei der Durchführung der Umfrage werden die Kategorien randomisiert abgefragt, damit die ordinale Struktur nicht gleich ersichtlich ist. Vom Summenscore zum WeihnachttypIm nächsten Schritt mussten wir anhand dieser Fragen bzw. deren Beantwortung noch die zugehörigen Weihnachtstypen identifizieren. Doch das war gar nicht so einfach, wie wir zunächst dachten. Anne-Marie wünschte sich, dass die Teilnehmenden der Umfrage gleich im Anschluss an die Beantwortung ein Ergebnis und damit Ihren Weihnachtstyp angezeigt bekämen. Ich fragte mich daher, wie sich dies zum einen einfach umsetzten lässt, und zum anderen, wie das Ergebnis den wahren Weihnachtstyp möglichst genau widerspiegeln könnte. Die erste Idee war es einen Summenscore über die Antworten, die ja eine ordinale Struktur haben und damit numerischen Werten zugeordnet werden können, zu bilden. Dieser lässt sich dann in gleich große Bereiche aufteilen, die einzelnen Typen zugeordnet werden können. Ich konnte mein Statistik-Herz noch nicht vollständig damit zufriedenstellen, etwas so Wichtiges wie den Weihnachtstyp aus einer einfachen Addition der Werte zu bestimmen. Ich überlegte also weiter und dachte daran, eine Clusteranalyse zu berechnen, um Gruppen von Befragten mit ähnlichem Antwortverhalten zu identifizieren und ihnen dann anhand dieses Antwortmusters einen Typ zuzuweisen. Eine andere Idee war es, eine Diskriminanzanalyse mit Hilfe der ersten Antworten-Sets zu berechnen und anhand der Ergebnisse die Zuteilung der Weihnachtstypen vorzunehmen. Letztendlich habe ich mich dagegen entschieden, da es sich bei dieser Umfrage um ein weihnachtlich-spaßiges Tool handelt und nicht um eine wissenschaftliche Studie. Da sich diese dritte Idee am einfachsten umsetzen ließ, haben wir uns zunächst dafür entschieden, einen Gesamtscore über alle Antworten zu ermitteln und diesen in vier Bereich aufzuteilen, die die Weihnachtstypen widerspiegeln. Wurde die Antwort „Keine der Optionen trifft auf mich zu“ oder generell keine Antwort gegeben, so wurde ein neutraler Wert hinzugefügt. Wir überlegten uns die folgenden Weihnachtstypen:Der WeihnachtsfreakDie kleine WeihnachtselfeCool wie 1 EisbärDer GrinchKleine Elfen oder cool as ice?Aber schauen wir uns doch zunächst an, welche Einteilung sich anhand des Summenscores ergeben hat. Insgesamt haben wir 48 Befragte bei unserer kleinen Weihnachtsumfrage. Zugegeben, dies ist eine recht geringe Anzahl für die meisten statistischen Verfahren, aber wir möchten bei STATWORX auch immer wieder zeigen, wie Verfahren mit realistischen Daten anzuwenden sind und wie sich diese verhalten.Mehr als die Hälfte unserer Befragten fällt in die Kategorie „Cool wie 1 Eisbär“, was eher eine neutrale Position gegenüber Weihnachten ausdrückt. Auch wenn eine befragte Person immer die Kategorie „Keine der Optionen trifft auf mich zu“ wählt, fällt sie theoretisch in diese Kategorie – dieser Fall kam bei der Umfrage nicht vor.Anne-Marie und ich hätten uns natürlich gewünscht, dass alle unsere Leidenschaft teilen und sich ausschließlich Weihnachtsfreaks und -elfen zeigen. Immerhin etwa ein Drittel der Befragten entpuppt sich nach unserer Einteilung als kleine Weihnachtselfe. Es fällt nicht schwer zu erkennen, dass es keinen einzigen Weihnachtsfreak unter unseren Kolleg*innen und Freund*innen gibt. Zuerst befürchtete ich, wir hätten die Bestimmung des Typs falsch implementiert, da niemand der Kategorie „Weihnachtsfreak“ zugeordnet wurde. Die Implementierung lief fehlerfrei. Das machte mir aber bewusst, dass wohl wirklich keine Weihnachtsfreaks teilgenommen haben. (Anne-Marie und ich haben nicht mitgemacht). Mein Missmut gegenüber dem Summenscore wuchs also zunehmend. Es wird zwar der theoretisch mögliche Wertebereich beachtet und zur Einteilung der Klassen verwendet, im Vorhinein kann jedoch noch nicht gesagt werden, welche Werte tatsächlich auftreten. Hätten wir den theoretischen Bereich also nicht einfach in vier gleichgroße Teile geteilt, sondern den Bereich für „Weihnachtsfreaks“ etwas vergrößert, dann wären auch einige Befragte in diese Kategorie gefallen.Das Ziel der Typbestimmung ist es, den wahren Typ der Personen herauszufinden. Es ist zum einen möglich, dass wir die Einteilung falsch vorgenommen haben. Es kann aber auch sein, dass keine „Weihnachtsfreaks“ an der Umfrage teilgenommen haben bzw. dass es generell keine gibt. Vor allem mit einer kleinen Stichprobe ist es schwer herauszufinden, warum es hier keine „Weihnachtsfreaks“ gibt.Wie sich der Weihnachtstyp auch anders bestimmen lässtWelche Vorteile hat eine Clusteranalyse gegenüber dem Summenscore? Der Summenscore fasst die Antworten aller Fragen zusammen. Wird bei einer Frage ein niedrigerer Wert gewählt, wirkt sich das auf den gesamten Score aus, auch wenn ansonsten hohe Werte gegeben werden. Die Clusteranalyse erlaubt eine etwas komplexere Untersuchung, da ähnliche Einheiten zusammengefasst werden und nicht nur solche, die ausschließlich hohe bzw. niedrige Werte bei allen Fragen angeben. Es ist denkbar, dass eine Einteilung in vier Kategorien, die sich nur durch die Intensität der gesamten Weihnachtsausprägung unterscheidet, nicht den eigentlichen Ausprägungen der Befragten entspricht, sondern dass der Weihnachtstyp durch einzelne Aspekten definiert wird. Einige Befragte könnten einen geringen Wert auf die vorweihnachtliche Stimmung legen, jedoch einen großen Anspruch an Geschenke haben. Andere könnte es besonders wichtig sein, an Weihnachten Zeit mit der Familie zu verbringen und gut zu Essen, jedoch könnten sie einen geringen Fokus auf Deko oder Adventskalender legen. Diese beiden beispielhaften Gruppen würden dann durch hohe Werte bei einem Aspekt und niedrige Werte bei einem anderen Aspekt als neutral gewertet, was auch die hohe Anzahl an „Cool wie 1 Eisbär“-Weihnachtsfans erklärt. Die Clusteranalyse nimmt jedoch eine detaillierte Unterscheidung vor.Zwei Varianten der ClusteranalyseGrundlegend gibt es durch verschiedene Arten zur Distanzberechnung, Anzahl an verwendeten Clustern und Algorithmen eine umfangreiche Menge an Möglichkeiten, die Clusteranalyse durchzuführen. Als ein Verfahren des unsupervised learnings besteht außerdem nicht die Möglichkeit, die Modellgüte mit den üblichen Kriterien zu prüfen. Ich beschränke mich hier auf die grundlegendsten Modelle, die sicherlich noch verbessert werden können. Es ist weder der Zweck des Artikels, die Methodik von Clusteranalysen zu erläutern, noch aufzuzeigen, wie sich die Modelle noch verbessern lassen. Vielmehr soll die grundlegende Clusterlösung mit der Typbestimmung durch den Summenscore verglichen werden.Die Antwortkategorien wurden so formuliert, dass sie eine ordinale Struktur aufweisen. Im weitesten Sinne lassen sich den Kategorien also metrische Werte geben und die Ähnlichkeit der Beobachtungen mittels euklidischer Distanz berechnen. Ich bin normalerweise sehr vorsichtig damit, ordinale Daten mit Verfahren zu behandeln, die eigentlich für metrische Daten gedacht sind. Da ich euklidischer Distanz jedoch sehr häufig bei der Clusteranalyse Anwendung finde, möchte ich die Analyse zum Vergleich so durchführen und meine strengen statistischen Ansichten zu Weihnachten etwas auflockern. Für die erste Clusterlösung werden die Antwortkategorien also als metrisch angesehen.Streng genommen sollten wir aber beachten, dass die Fragen nur vier Kategorien (bzw. fünf, wenn die neutrale Kategorie mitbeachtet wird) haben und wir auch keine standardisierte Likert-Skala für alle Fragen haben, die die Betrachtung als metrische Variablen noch eher rechtfertigen würde. Dazu kommt, dass die ordinale Struktur in den Antworten teilweise nicht so offensichtlich ist. Für die zweite Clusterlösung werden   die Antworten als nominalskaliert behandelt. Es gibt verschiedene Algorithmen zur Bestimmung der optimalen Anzahl an Gruppen bei der Clusteranalyse. Da die Ergebnisse mit denen des Summenscores verglichen werden sollen, wird die Anzahl an Clustern von Vornherein auf drei festgesetzt, damit sie der Anzahl an laut Summenscores vorkommenden Weihnachtstypen entspricht. Als Clusteralgorithmus wird für beide Ansätze „partitioning around medoids“ (PAM) aus dem cluster-Paket verwendet.Wir nehmen’s leicht: Numerische AntwortkategorienNun möchte ich damit beginnen, die erste Lösung vorzustellen, bei der die Antwortkategorien als numerisch angesehen werden. Die Antwortkategorien werden mit den Werten -2 bis 2 codiert, wobei der Wert 0 bei der Kategorie „Keine der Optionen trifft auf mich zu“ oder bei fehlenden Werten vergeben wird und ein höherer Werte eine größere Begeisterung für Weihnachten anzeigt. Die Ergebnisse zu diesem Clusteransatz mit euklidischen Distanzen, drei Clustern und PAM-Algorithmus werden im Objekt fit_num gespeichert.fit_num  &lt;- pam(x = df_num, k = 3, metric = ""euclidean"",                 diss = FALSE, stand = TRUE)Die drei Cluster sind so verteilt, dass gut die Hälfte der Befragten dem Cluster 3 zugeordnet wird und den anderen beiden etwa gleich viele Befragte. Um die Cluster näher definieren zu können, wird die Verteilung der Fragen getrennt nach Cluster betrachtet.Tendenziell lässt sich feststellen, dass Befragte in Cluster 3 eher weniger Begeisterung für Weihnachten aufbringen, Befragte in Cluster 1 dafür überdurchschnittlich viel. Wenn es aber um die finanziellen Ausgaben für Weihnachten geht oder die Gestaltung der Weihnachtsfeiertage, dann äußern sich die Befragten in den Clustern recht ähnlich. Um genau zu sein: Kategoriale AntwortkategorienMit meinem statistischen Gewissen kann ich nicht vereinbaren, nur die bisherige Lösung zu präsentieren. Ich möchte mir zusätzlich anschauen, welches Ergebnis ich erhalte, wenn die Antwortkategorien als nominalskaliert definiert sind. Damit wird zwar auch etwas Varianz durch die eigentlich ordinale Definition verloren, da diese jedoch nur schwach ausgeprägt war, ist dieser Ansatz der genaueste. Zunächst werden Gower-Distanzen mit der daisy()-Funktion berechnet, welche dann im PAM-Algorithmus verwendet werden.cat_dist &lt;- daisy(df_cat, metric = ""gower"")fit_cat  &lt;- pam(x = cat_dist, k = 3, metric = ""gower"",                 diss = TRUE, stand = FALSE)Die Verteilung der Cluster ist hier etwas einheitlicher. Das größte Cluster (Nummer 2) enthält knapp 40% der Befragten, das kleinste (Nummer 1) etwa 23%.Für diese Clusterlösung wird graphisch die Verteilung der Antwortkategorien nach Cluster dargestellt. Zwar zeigt sich nicht, dass bestimmte Antwortkategorien nur von Befragten eines Clusters gegeben werden, es zeigt sich aber eine Tendenz. Antworten, die eine größere Begeisterung für Weihnachten darstellen, wurden eher von Befragten aus dem Cluster 3 gegeben. Befragte aus Cluster 1 und 2 antworten im Wesentlichen neutral. (Zwar wird die Verteilung der einzelnen Fragen in diesem Post nicht besprochen, Interessierte können jedoch die Gesamtverteilung anhand der untenstehenden Grafik ablesen.)Clusteranalyse im Vergleich zum SummenscoreDie beiden Lösungen der Clusteranalyse teilen die Befragten nicht eindeutig in Weihnachtstypen ein, was bereits anzunehmen war. Ich bin sogar ein bisschen erstaunt, dass sich bei der ersten Clusteranalyse die Wertebereiche und bei der zweiten Clusteranalyse die Antwortkategorien doch recht klar zwischen den Clustern unterscheiden. Von anderen Clusteranalysen kenne ich weit weniger eindeutige Ergebnisse.Nun möchte ich aber noch sehen, wie sich die Clustereinteilung in Bezug auf den Summenscore verhält. Überlappen die Boxen pro Analyse nicht, dann spricht das dafür, dass die Clusteranalyse keine deutlich anderen Ergebnisse hervorbringt als die Einteilung anhand des Summenscores. Dann kann ich auch endlich Frieden mit unserer Einteilung der Weihnachtstypen finden. Als Referenz ist im folgenden Boxplot in Rot die ursprüngliche Einteilung des Weihnachtstyps anhand des Summenscores dargestellt. Die vertikalen Linien zeigen unsere selbst definierte Eingrenzung des möglichen Wertebereichs in vier gleich große Gruppen. Per Definition befinden sich die oberen Boxen also innerhalb der grauen Linien und überlappen nicht. Bei den beiden Clusteranalysen zeigen sich bei der Einteilung der Gruppen im Hinblick auf die Scores recht ähnliche Werte (wenn man Cluster K3 und N1 vergleicht).  Es fällt außerdem auf, dass Cluster T3 bei „Typ“ ziemlich exakt dem Cluster N1 bei der Analyse mit numerischen Werten und Nummer T3 bei der Analyse mit kategorialen Werten entspricht. Die extremen Weihnachtsfans, bzw. die kleinen Weihnachtselfen, wie wir sie nennen, werden also von den drei Ansätzen gleich identifiziert.Während aber bei unserer Typeinteilung Cluster T1 und T2 notwendigerweise nicht überlappen, können bei den beiden Clusteranalysen die Cluster nicht so deutlich unterschieden werden. Dies deutet darauf hin, dass die Befragten eben nicht bei allen Aspekten Weihnachtsfreaks sind, sondern nur in vereinzelten Bereichen. Positive Werte bei einigen und negative Werte bei anderen Fragen führen dann dazu, dass sich die Werte ausgleichen und der Score im neutralen Bereich liegt. Das Fazit des VergleichsIch gebe zu, im Endeffekt hat sich die Einteilung durch den Summenscore nicht deutlich von der Einteilung durch die Clusteranalyse unterschieden. Alle drei Ansätzen identifizieren eine recht separierte Gruppe von Weihnachtsfreaks.Bei den anderen Befragten zeigt sich aber, dass sie in manchen Aspekten unserer Ansicht von Weihnachtsbegeisterung entsprechen, in anderen Aspekten jedoch eher untere Antwortkategorien wählen, die für weniger Enthusiasmus gegenüber Weihnachten sprechen. Dies lässt sich jedoch nicht erkennen, wenn nur der Summenscore über die Kategorien gebildet wird. Die Clusteranalyse kann hier also helfen eine konkreteres Bild über die Befragten zu bekommen. Trotzdem bin ich froh, dass mich Anne-Marie dazu überreden konnte, den Weihnachtstyp direkt bestimmen zu lassen. Da alle gleich nach Beantwortung der Fragen ein Resultat mit dem entsprechenden Weihnachtstyp erhalten haben, hatten wir im Büro viel Spaß und interessante Diskussionen zur Einstellung zu Weihnachten.Wir möchten diesen Blogartikel auch dazu nutzen, um allen Blogbesuchern und Kunden frohe Weihnachten und einen guten Start ins Jahr 2020 zu wünschen. Wenn du mehr über die Clusteranalyse oder andere statistische Verfahren erfahren möchtest, dann wende dich gerne an uns. Über den AutorJessica AustI am part of the STATWORX statistics team and I like the whole process of analysing data, from data prep to calculating models. Each step requires to follow certain rules, but this does not mean, that it’s not fun..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/is-it-the-most-wonderful-time-of-the-year/;Statworx;  Jessica Aust
  12. Dezember 2019;A Collection of Benchmarks in R;"When you write code in R, you might face the question: „Is there a faster way to do this?“. Over the years I worked at STATWORX, I have done a lot of little benchmarks to find an answer to this kind of question. Often, I just did a quick check to see if there is any time difference between two methods, used the faster one and moved on. Of course, I forgot about my tests over time and may have wondered twice about the same problem. To break this vicious circle, I created an overview of all the benchmarks I have done so far with the possibility to add more in the future. This overview can be found on my Github.Creating an overview of all resultsThe overview needed to cater to multiple purposes:function as a quick lookup table (which is the fastest way to do a specific task)show the alternatives that were testedgive an idea of what was testedSince the tested functions are often not that complicated (e.g. range(x) vs max(x) - min(x)), the benchmarks I did so far mostly had two varying parameters (e.g., the size or the number of different values). After some feedback from two of my colleagues, I settled for this table:DATETESTCOMMENTBESTTIME_FACTORBEST_RUNSDETAILSDURATION2019-11-29 08:53:33Accsess a colum in a data frame, table or tibble.varying size of data$ tbl66.9%4/4link00:00:062019-11-29 08:53:36assign with &lt;- or =varying size of vectorequal sign27.4%6/6link00:00:01Since this is a work in progress, there is a good chance the format will change again in the future. But for now, this is shown in the table:The DATE of the last time, the benchmark run.A short description TEST of the benchmark.In the COMMENTS I tried to give a hint of what the setups looked like.The BEST option out of all tested alternatives compared by their mean time.The TIME_FACTOR presents the mean time that can be saved with the best option compared with the mean of the alternatives over all grid setups. Note: The time factor can be negative if the best option is not the best in the cases where it takes more time. For these cases, have a look at the details and dependencies of the grid parameters.BEST_RUNS is the number of cases were BEST solution was actually the best one in relation of all different varying setups that were used (e.g. sample size).DURATION is the time the whole benchmark with all setups took.Making the benchmark setup multi-usableAs I said before, I planned to make the overview extendable for new benchmarks in the future. Therefore, I created some helper functions and templates to make it easier to include new benchmarks. The main parts for this were:a template folder and script for new benchmarksa function that saves the result in my desired outputa function that creates the overview by reading in all existing resultsa script that runs all benchmarks.For adding a new benchmark, I have to copy the template folder and include the new setup I want to test. The save_benchmark() function will create the same output as for the previous benchmarks and the update_bench_overview() function will add it to the overview.The main issue is the visualization of different grid parameters and their results. The good thing is that if I get an idea on how to improve this visualization, I could add it to save_benchmark() and rerun all benchmarks with the run_all_bench.R script. At the moment, a plot for each grid parameter is created, which indicates how the change influenced the timing. Also, the summaries for each run are shown, so one can see what exactly is going on.How to set up a new benchmarkThe template for further benchmarks has different sections that can be easily adjusted. Since this is a work in progress, it might change in the future. So if you have any good ideas or think I missed something, let me know and raise an issue on my Github.It all starts with settingsThere are three libraries I need for my functions to run. If the next benchmark needs other packages, I can add them here.# these are neededlibrary(microbenchmark)library(helfRlein)library(data.table)source(""functions/save_benchmark.R"")# add more hereThe next step is to describe the benchmark. Where are the results saved? What is the benchmark all about? What parameters are changing? All this information is later used to create the plots and tables to make it more understandable.# test setup --------------------------------------------------------------# folder for resultsfolder &lt;- ""benchmarks/00_template_folder/""# test descriptiondescription &lt;- ""A short description of what is tested.""# number of repetitionsreps &lt;- 100Lcomments &lt;- ""what parameters changed""start_time &lt;- Sys.time()The more parameters, the merrierHow valid are the benchmark results? The more different settings it was tested in, the better the generalization. Is there maybe even a dependency, which is the best alternative? That can all be set up in this section, where you can define the different grid settings. I’d advise you to use variable names that can easily be understood, e.g., number_of_rows, unique_values, or sample_size. These names are also used in the plots at the end – so choose wisely!# grid setup --------------------------------------------------------------# if there are different values to testgrid &lt;- as.data.table(expand.grid(  param_1 = 10^c(2:3),  param_2 = c(5,10,20)))result_list &lt;- as.list(rep(NA, dim(grid)1))best_list &lt;- as.list(rep(NA, dim(grid)1))The benchmark coreLooping over all grid settings, creating the starting values for each run, and adding all alternatives functions –  this is the main part of the function: the benchmark itself. for (i in c(1:nrow(grid))) {  # i &lt;- 1  i_param_1 &lt;- gridi, param_1  i_param_2 &lt;- gridi, param_2  # use grid parameters to define tested setup  x &lt;- rnorm(n = i_param_1, mean = i_param_2)  tmp &lt;- microbenchmark(    ""Alternative 1"" = mean(x),    ""Alternative 2"" = sum(x) / length(x),    times = reps,    control = list(warmup = 10L),    unit = ""ms"")  #tmp &lt;- data.table(summary(tmp), i = gridi, )  result_listi &lt;- tmp  # select best by mean  tmp_sum &lt;- summary(tmp)  best_listi &lt;- as.character(tmp_sum$exprtmp_sum$mean == min(tmp_sum$mean))}All that is not saved will be lostDuring all the previous steps, the intermediate results are stored in lists, which are the input values for the save_benchmark() function. As mentioned before, it creates tables for each benchmark run and plots with an overview of the effects of each grid parameter. Lastly, it updates the main README file with the newest results.## saving all datasave_benchmark(result_list = result_list,               best_list = best_list,               folder = folder,               start_time = start_time,               description = description,               grid = grid,               reps = reps,               comments = comments)How do the results look likeAfter running the benchmark, a new README file is automatically created. This file contains an overview of the tested alternatives (as you named them), the used grid parameters, plots with the impact of these grid parameters, and the tabled summary of every single result. For example, here, you can see that the number of unique values has a positive effect (faster) on the time it takes to filter, but the number of rows has a negative impact (slower).If you are interested in not only the overview but the actual data, have a look at result_list.rds. This list contains all results of microbenckmark() for each grid combination. The last two created files are last_result.rds and log_result.txt. The first is used to create the current overall README.md, and the second is just a logfile with all previous results.Ideas for further benchmarksDo you have any thoughts on what we should benchmark next? Or did we maybe forget an alternative? Then raise an issue at my Github. If you can think of a method to better visualize the results, feel free to contact me. I welcome any feedback!Über den AutorJakob GeppNumbers were always my passion and as a data scientist and a statistician at STATWORX I can fullfill my nerdy needs. Also I am responsable for our blog. So if you have any questions or suggestions, just send me an email!.e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/a-collection-of-benchmarks-in-r/;Statworx;  Jakob Gepp
  4. Dezember 2019;Deep Learning – Überblick und Einstieg;"Deep-Fake Videos bekannter Politiker, lernende Super Marios, übersetzte Webseiten und automatisierte Landwirtschaft: Kaum ein anderes Feld hat in so kurzer Zeit einen solch rasanten Aufstieg erfahren und Einzug in alle Ausprägungen dieser Welt erreicht, wie das Thema Deep Learning. Akademisch interessant, wie auch direkt praktisch anwendbar, scheint die heutige Vermarktung und Berichterstattung sich auf die neue Allzweckwaffe eingeschossen zu haben. Aber was ist dran an dem Ding? Was ermöglicht uns dieser moderne Forschungsbereich, welcher schon Turing-Preisträger, aber noch keinen bestandenen Turing-Test hervorgebracht hat? Und wo fange ich überhaupt an? Mit diesem Blogbeitrag möchte ich einen kleinen Überblick darüber verschaffen, was verschiedene Deep Learning Modelle und Ansätze heutzutage leisten können und wie der Einstieg auch auf technischer Ebene gelingt.Schritt für Schritt zum OptimumNach dem ersten erwartungsvollen „AI Boom“ in den 1950er Jahren und dem resultierenden „AI Winter“ der 60er und 70er Jahre, sind es vor allem einige Schlüsselpublikationen, Fortschritte im Bereich GPUs (Grafikkarten) und eine schiere Flut an Daten, welche das heutige Deep Learning ermöglicht haben. Unabhängig von der Modellart und -architektur bilden diese das Fundament aller Neuronalen Netze, sei es zur Bildverarbeitung, Text- und Zeitreihenanalyse oder das selbstständige Lernen via Deep Reinforcement Learning.Sind die Grundkonzepte, welche nicht über angewandte Schulmathematik hinausgehen, einmal verstanden, so wird einem diese Arbeit auch prompt durch extrem mächtige Programmier-Frameworks wie TensorFlow und Keras abgenommen. Diese abstrahieren den Entwurf und das Training Neuronaler Netze so sehr, dass der ganze Prozess trügerisch einfach wirkt. Die Anzahl an Einstellungen und Stellschrauben, Hyperparameter genannt, ist bei Neuronalen Netzen jedoch enorm, besonders wenn man bereits in 10-12 Zeilen Code ein anwendbares Deep Learning Modell erstellen kann. Somit konzentriert sich die Hauptaufgabe eines Data Scientists auf den Entwurf der Modelle und die Suche nach den optimalen Hyperparametern; ein Vorgang, der geprägt ist von theoretischem Wissen, aber vor allem von Erfahrungswerten. Ein Workshop für alle FälleZum Thema Deep Learning gibt es online unzählige Ressourcen, wie Medium Artikel, Reddit Diskussionen und MOOCs. Obwohl hier viele Aspekte des Themas behandelt werden, sind diese Ressourcen oftmals nicht ausreichend, um für die Anwendung von Deep Learning auf reale Daten und Probleme gewappnet zu sein. Daher haben wir bei STATWORX uns dazu entschlossen, unsere Erfahrung und unser Wissen in einem umfassenden Grundlagen-Workshop zu bündeln: dem Deep Learning Bootcamp. Hierbei war es uns überaus wichtig, das komplexe Thema zugänglich zu gestalten, einen roten Faden zu haben und dabei trotzdem alles Aufregende abzudecken. Keine leichte Aufgabe, bei dem spannenden Überangebot an Deep Learning Anwendungen. Möchte ich den Inhalt von Texten schneller bestimmen als es ein Mensch könnte, einem Bild meines Lieblingspromis einen Bart wachsen lassen oder doch eher einen Zahlungsdienstleister beim Erkennen von Kreditkartenbetrug unterstützen? All das ist theoretisch mit Deep Learning möglich. Da sich viele Anwendungsfälle und Vorgehensweisen sehr ähneln ist es für das eigene Verständnis und das Identifizieren von Geschäftsfällen entscheidend, solides Grundlagenwissen sowie einen guten Überblick zu bekommen. Im Workshop beschäftigen wir uns mit einer Vielzahl an Fragen, damit unsere Teilnehmenden die verschiedenen Teilbereiche des Deep Learning kennenlernen können. Dazu gehören folgende Fragestellungen:Wie verwende ich ‚normale‘ Neuronale Netze (Multilayer Perceptrons), um den Preis eines Hauses vorherzusagen oder Anomalien in meinen Daten zu erkennen? Egal ob in der Finanzbranche, beim Bereinigen von Sensordaten oder für den Einzelhandel.Was ermöglichen mir Convolutional Neural Networks in der Bilderkennung? Wenn ich beim Klassifizieren von Bildern Hunde, Katzen, Flugzeuge usw. erkennen kann, wie übertrage ich das Ganze auf mein Geschäftsfeld. Zum Beispiel könnten Fehler in der Fertigung erkannt oder der Zustand von Feldern überwacht werden?Wie wende ich rekurrente neuronale Modelle an, um mit sequenziellen Daten umzugehen? Wie unterscheidet sich die Analyse von Finanzmarkt- oder Maschinendaten vom Umgang mit Texten, um ein gutes Bewerbungsanschreiben von einer Spamnachricht zu unterscheiden? Wie kann ich mit den gleichen Reinforcement Learning Algorithmen, die den Computer ein Videospiel meistern lassen, mittels Deep Q-Learning Prozesse in Echtzeit optimieren, wie zum Beispiel in einem chemischen Reaktor? Erfahrungswerte &amp; Best PracticesAus technischer und theoretischer Sicht sind die Unterschiede zwischen den jeweiligen Anwendungsfällen der einzelnen Modellarten teilweise erschreckend gering. Im Umkehrschluss ist der Lerneffekt und die Übertragbarkeit jedoch enorm. Der einzige Haken ist, dass es selten Faustregeln oder einen Leitfaden gibt, welche einem das beste Modell oder die besten Hyperparameter liefern. Hier offenbart sich die Komplexität der Deep Learning Modelle. Aufgrund des hohen Berechnungsaufwands ist das Ausprobieren aller oder vieler Einstellungen nicht plausibel. Somit rücken persönliche Erfahrungswerte, Best Practices und empirische Erkenntnisse in den Vordergrund. Während diese in einigen Online-Ressourcen zwar implizit enthalten sind, sind wir (und das Feedback der Teilnehmer) der Meinung, in unserem Kurs einen wesentlichen Mehrwert bieten zu können. Als Data Scientists, die jährlich an einer Vielzahl an Projekten arbeiten, übermitteln wir zu den Grundlagen auch gerne unsere eigenen Erfahrungen und (schmerzlichen) Lessons Learned. Interesse am Deep Learning Bootcamp?Falls wir Euer Interesse wecken konnten, dann schaut Euch doch mal unseren detaillierten Kurs Plan  an oder schreibt uns eine Nachricht, falls ihr in einem Workshop gemeinsam mit uns einen Prototypen entwickeln möchtet! Ansonsten wünschen wir jedem einen spannenden und guten Einstieg in die Welt des Deep Learning, man sieht sich auf Stack Overflow. Mehrmals. Täglich…Über den AutorJonas BraunBesides being a data scientist interested especially in deep learning, I enjoy the outdoors, anything culinary and also the arts. Bridging deeply rooted 'real world' elements with tech is what drives me..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/deep-learning-ueberblick-und-einstieg/;Statworx;  Jonas Braun
  21. November 2019;Tuning Random Forest on Time Series Data;"„Taxes and random forest again?“ Thomas, my colleague here at STATWORX, raised his eyebrows when I told him about this post. „Yes!“ I replied, „but not because I love taxes so much (who does?). This time it’s about tuning!“ Let’s rewind for a moment: in the previous post, we looked at how we can combine econometric techniques like differencing with machine learning (ML) algorithms like random forests to predict a time series with a high degree of accuracy. If you missed it, I encourage you to check it out here. The data is now also available as a CSV file on our STATWORX GitHub.Since we covered quite some ground in the last post, there wasn’t much room for other topics. This is where this post comes in. Today, we take a look at how we can tune the hyperparameters of a random forest when dealing with time series data.Any takers? Alright, then let’s do this! If you read the last post, feel free to skip over section 1 and move right on to 2.The setup# load packagessuppressPackageStartupMessages(require(tidyverse))suppressPackageStartupMessages(require(tsibble))suppressPackageStartupMessages(require(randomForest))suppressPackageStartupMessages(require(forecast))# specify the path to the csv file (your path here)file &lt;- ""/Users/manueltilgner/Library/Mobile Documents/com~apple~CloudDocs/Artificial Intelligence/10_Blog/RF/tax.csv""# read in the csv filetax_tbl &lt;- readr::read_delim(  file = file,  delim = "";"",  col_names = c(""Year"", ""Type"", month.abb),  skip = 1,  col_types = ""iciiiiiiiiiiii"",  na = c(""..."")) %&gt;%   select(-Type) %&gt;%   gather(Date, Value, -Year) %&gt;%   unite(""Date"", c(Date, Year), sep = "" "") %&gt;%   mutate(    Date = Date %&gt;%       lubridate::parse_date_time(""m y"") %&gt;%       yearmonth()  ) %&gt;%   drop_na() %&gt;%   as_tsibble(index = ""Date"") %&gt;%   filter(Date &lt;= ""2018-12-01"")# convert to ts formattax_ts &lt;- as.ts(tax_tbl)Again, all this is from last time, I just have it here so you can get up and running quickly (just copy paste).# pretend we're in December 2017 and have to forecast the next twelve monthstax_ts_org &lt;- window(tax_ts, end = c(2017, 12))# estimate the required order of differencingn_diffs &lt;- nsdiffs(tax_ts_org)# log transform and difference the datatax_ts_trf &lt;- tax_ts_org %&gt;%   log() %&gt;%   diff(n_diffs)# embed the matrixlag_order &lt;- 6 # the desired number of lags (six months)horizon &lt;- 12 # the forecast horizon (twelve months)tax_ts_mbd &lt;- embed(tax_ts_trf, lag_order + 1) # embedding magic!# do a train test splity_train &lt;- tax_ts_mbd, 1 # the targetX_train &lt;- tax_ts_mbd, -1 # everything but the targety_test &lt;- window(tax_ts, start = c(2018, 1), end = c(2018, 12)) # the year 2018X_test &lt;- tax_ts_mbdnrow(tax_ts_mbd), c(1:lag_order) # the test set consisting# of the six most recent values (we have six lags) of the training set. It's the# same for all models.Cross-validation and its (mis-)contentsHow do you tune the hyperparameters of an ML model when you’re dealing with time series data? Viewer discretion advised: the following material might be disturbing to some audiences.Before we answer the question above, let’s first answer the following: how do we tune the hyperparameters of an ML model on data that has no time dimension? Generally, we proceed as follows:pick a handful of hyperparameters,select a sensible range of values for each, and finally,determine which configuration is ‚best‘, e.g., in the sense of minimizing an error metric.Steps 1) and 2) are a matter of trial and error. In most cases, there’s no analytic way of deriving which hyperparameters in which configuration work best. While some practical insights have emerged over the years, generally, you just have to experiment and see what works with your data.What about step 3)? Finding the ‚best‘ configuration can be achieved with the help of different resampling schemes, a popular one of which k-fold cross-validation. Popular because k-fold cross-validation tends to provide fairly reliable model performance estimates and makes efficient use of the data. How so?Let’s refresh: k-fold cross-validation works by splitting the data into  folds of roughly equal size. Each fold constitutes a subset of the data with  observations. It then fits the model on  folds and computes its loss (e.g., RMSE) th left out fold, which serves as a validation set. This process repeats  times until each fold has served once as a validation set.From the description, it’s clear why this sounds like a bad idea for time series data. If we do k-fold cross-validation, we invariably end up fitting a model on future data to predict the past! I don’t know about you, but somehow this doesn’t feel right. No need to mention, of course, that if we shuffle the data before splitting it into folds, we completely destroy the time structure.Hold out, not so fastWhat to do instead? In time series econometrics, a traditional approach is this: reserve the last part of your time series as a holdout set. That is, fit your model on the first part of the series and evaluate it on the last part. In essence, this is nothing more than a validation set strategy. This approach is simple and intuitive. Moreover, it preserves the order of the time series. But it also has a significant downside: it does not provide robust performance estimates since we test our model only once and on a ‚test‘ set that is arbitrary. Is it a problem?It definitely can be: time series above all tend to represent phenomena that are ever-changing and evolving. As such, the test set (i.e., the ‚future‘) may be systematically different than the train set (i.e., the ‚past‘). This change in the underlying data generating process over time is known as ‚concept drift‘ and poses a serious challenge in time series forecasting.But we’re getting off track! What is it that we’re trying to achieve here? We want to get reliable performance estimates of our ML model on time series data. Only then we can decide with some degree of certitude which hyperparameters to choose for our model. Yet, the traditional approach of doing a single train/test split doesn’t really cut it for us. Much better would be multiple train/test splits. In the context of time series, this means sliding a fixed or steadily expanding window over our series, training on one part of the data, and predicting the next, then computing the loss. Next, move the window ahead an observation (or enlarge it) and repeat. Rob Hyndman has a great post on it.This approach, called time series cross-validation is effective, but also computationally expensive. Imagine this, if you have 10 hyperparameter configurations and you test each of them with 20 train/test splits, you end up calculating two hundred models. Depending on the model and the amount of data you have, this can take its sweet time.Since we’re busy people living a busy world, let’s stick with the holdout strategy for now. It sounds simple, but how do you do it? Personally, I like to use the createTimeSlices function from the caret package. This makes sense if you have a caret workflow or work with many different series. If not, simply slice your (training) series so that the last part is reserved as a validation set.To see how the createTimeSlices function works, run it on its own first.# we hold out the last 12 observations from our train set because this is also how far we want to predict into the future latercaret::createTimeSlices(  1:nrow(X_train),  initialWindow = nrow(X_train) - horizon,  horizon = horizon,  fixedWindow = TRUE)You see that it splits our train set into a train and validation set, based on the forecast horizon we specified (12 months). Ok, now let’s set it up in such a way that we can use it in a train workflow:tr_control &lt;- caret::trainControl(  method = 'timeslice',  initialWindow = nrow(X_train) - horizon,  fixedWindow = TRUE)With trainControl in place, let us next set up a tuning grid. While we can get super fancy here, for random forests, it often boils down to two hyperparameters that matter: the number of trees (ntree) and the number of predictors (mtry) that get sampled at each split in the tree. Good values for ntree are a few hundred to a thousand. More trees can give you a bump in accuracy, but usually not much. Since random forests do not run a high risk of overfitting, the question of how many trees you use really comes down to how much computing power (or time) you have. Since tuning with time series tends to be computationally expensive, let’s pick 500 trees.mtry, on the other hand, tends to be the parameter where the party’s at. It represents the number of predictors that get considered as splitting candidates at each node in the tree. James et al. (2013) recommend  or  (where  is the number of predictors) as a guideline. I’m also going to throw in , which amounts to bagging (side note: if you want to explore these concepts further, check out the posts of my colleagues on cross-validation and bagging).# caret actually only allows us to put one hyperparameter heretune_grid &lt;- expand.grid(  mtry = c(    ncol(X_train), # p    ncol(X_train) / 3, # p / 3    ceiling(sqrt(ncol(X_train))) # square root of p  ))# let's see which hyperparameter the holdout method recommendsholdout_result &lt;- caret::train(  data.frame(X_train),  y_train,  method = 'rf',  trControl = tr_control,  tuneGrid = tune_grid)This method recommends mtry = 6. So, should we stop and fit our model already? Not just yet!Cross-validation again?Remember all the bad things we said about k-fold cross-validation (CV) with time series data? All of it is still technically true. But now comes the weird part. Bergmeir et al. (2018) found that k-fold CV can actually be applied with time series models. But only if they are purely autoregressive. In other words, we can use k-fold CV on time series data, but only if the predictors in our model are lagged versions of the response.How come? The authors note that in order for k-fold CV to be valid, the errors of the model must be uncorrelated. This condition is met when the model that we train fits the data well. In other words, k-fold CV is valid if our model nests a good model, i.e., a model whose weights are a subset of the model weights we train. When this is the case, k-fold cross-validation is actually a better choice than the holdout strategy!I don’t know about you, but for me, this was a surprise. Since it’s a mighty useful insight, it’s worth repeating one more time: if the residuals of our (purely autoregressive) time series model are serially uncorrelated, then k-fold cross-validation can and should be used over the holdout strategy.Are you as curious as I am now? Then let’s put it work for us and not just once, but repeatedly!tr_control &lt;- trainControl(  method = 'repeatedcv',  number = 10,   repeats = 3)kfold_result &lt;- caret::train(  data.frame(X_train),  y_train,  method = 'rf',  trControl = tr_control,  tuneGrid = tune_grid)Using k-fold CV on this time series suggests a value of mtry = 2. Interesting! Let’s train our random forest twice now, once with mtry = 2and once with mtry = 6. Then, we compare which one gives a better prediction!mtrying it out!# set up our empty forecast tibbleforecasts_rf &lt;- tibble(  mtry_holdout = rep(NA, horizon),  mtry_kfold = rep(NA, horizon))# collect the two mtry values from the tuning stepmtrys &lt;- c(  holdout_result$bestTune1,  kfold_result$bestTune1)# train the model in a double loopfor (i in seq_len(length(mtrys))) {  # start fresh for each mtry run  y_train &lt;- tax_ts_mbd, 1     X_train &lt;- tax_ts_mbd, -1   # train the models  for (j in seq_len(horizon)) {    # set seed    set.seed(2019)    # fit the model    fit_rf &lt;- randomForest(X_train, y_train, mtry = mtrysi)    # predict using the test set    forecasts_rfj, i &lt;- predict(fit_rf, X_test)    # here is where we repeatedly reshape the training data to reflect the time                            # distance corresponding to the current forecast horizon.    y_train &lt;- y_train-1     X_train &lt;- X_train-nrow(X_train),    }}Again, we need to back-transform our forecasts to calculate the accuracy on our test set. For this, I’m going to use purrr.last_observation &lt;- as.vector(tail(tax_ts_org, 1))forecasts &lt;- forecasts_rf %&gt;%   purrr::map_df(function(x) exp(cumsum(x)) * last_observation)accuracies &lt;- forecasts %&gt;%   purrr::map(function(x) accuracy(x, as.vector(y_test))) %&gt;%  do.call(rbind, .)And what do you know! k-fold CV proved indeed better than our holdout approach. It reduced both RMSE and MAPE. We even validated our result from last time, where we also had a MAPE of 2.6. So at least here, using random forest out of the box was totally fine.MERMSEMAEMPEMAPEholdout429258.4484154.7429258.44.7097054.709705k-fold198307.5352789.9238652.62.2737852.607773Let’s visualize it as well: plot &lt;- tax_tbl %&gt;%   filter(Date &gt;= ""2018-01-01"") %&gt;%   mutate(    Ground_Truth = Value / 10000,    Forecast_Holdout = forecasts$mtry_holdout / 10000,    Forecast_Kfold = forecasts$mtry_kfold / 10000,  ) %&gt;%   ggplot(aes(x = Date)) +  geom_line(aes(y = Value / 10000, linetype = ""Truth"")) +  geom_line(aes(y = Forecast_Holdout, color = ""Holdout"")) +  geom_line(aes(y = Forecast_Kfold, color = ""k-fold CV"")) +  theme_minimal()+  labs(    title = ""Forecast of the German Wage and Income Tax for the Year 2018"",    x = ""Months"",    y = ""Euros""  ) +  scale_color_manual(    name = ""Forecasts"",    values = c(""Truth"" = ""black"", ""Holdout"" = ""darkblue"", ""k-fold CV"" = ""orange"")  ) +  scale_linetype_manual(name = ""Original"", values = c(""Truth"" = ""dashed"")) +  scale_x_date(date_labels = ""%b %Y"", date_breaks = ""2 months"")We see that the orange line, which represents the forecasts from the k-fold CV model, tends to hug the true values more snugly at several points.Conclusion (TL;DR)Tuning ML models on time series data can be expensive, but it needn’t be. If the model you’re fitting uses only endogenous predictors, i.e., lags of the response, you’re in luck! You can go ahead and use the known and beloved k-fold cross-validation strategy to tune your hyperparameters. If you want to go deeper, check out the original paper in the reference. Otherwise, go pick a time a time series of your choice and see if you can improve your model with a bit of tuning. No matter the result, it’ll always beat doing taxes!ReferencesJames, Gareth, et al. An introduction to statistical learning. Vol. 112. New York: Springer, 2013.Bergmeir, Christoph, and José M. Benítez. „On the use of cross-validation for time series predictor evaluation.“ Information Sciences 191 (2012): 192-213.Über den AutorManuel TilgnerI am a data scientist at STATWORX, and I enjoy making data make sense. Why? Because there's something magical about turning a jumble of numbers into insights. In my free time, I love wandering through the forest or playing in the local big band..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/tuning-random-forest-on-time-series-data/;Statworx;  Manuel Tilgner
  14. November 2019;5 Highlights der Data University;"Mit der Data University 2019 haben wir im letzten Monat den Start unserer ersten großen Veranstaltung gefeiert. Als Teil des Marketing-Teams habe ich die Veranstaltung medial begleitet und zwei spannende, ereignisreiche Workshoptage erleben dürfen. Als Rückblick auf die Veranstaltung habe ich mir überlegt, einen Artikel zu schreiben, in dem ich mir 5 Highlights der Data University rauspicke. Damit möchte ich euch einen Einblick hinter die Kulissen geben und möchte euch erzählen, wie die Vorbereitungen liefen, was an der Veranstaltung alles los war und wie unser Fazit aussieht. Viel Spaß beim Lesen!Der Startschuss ist gefallen!Das Workshop-Event fand vom 09.-10. Oktober 2019 an der Goethe-Uni Frankfurt statt. Unsere STATWORX Crew war bei der Data University in einer doppelten Funktion vertreten: Als Mit-Veranstalter (zusammen mit unserem Partner BARC) haben wir die Veranstaltung zum einen organisatorisch begleitet und zum anderen haben einige Kollegen aus unserem Team Data Science als Trainer die Workshops geleitet. Behind the ScenesGemeinsam mit meinen Kolleg*innen aus dem Team Marketing – Vivian, Livia, Marcel und Niklas – war ich vor allem „behind the scenes“ tätig. In unserer kleinen Kommandozentrale haben wir für die zwei Workshoptage unser Marketing-Lager aufgeschlagen. Von dort aus haben wir das Event in den sozialen Medien begleitet, haben Foto- &amp; Videomaterial gesammelt und waren Ansprechpersonen für die Teilnehmenden. Währenddessen konnten wir immer mal wieder in die Workshops reinschnuppern und uns ein eigenes Bild unseres Angebots machen. Dabei sind wir auch in direkten Austausch mit den Teilnehmenden gekommen, konnten uns direktes Feedback holen und Ideen für unsere nächsten Veranstaltungen sammeln. Nach den zwei aufregenden, aufschlussreichen Tagen bei der Data University haben wir sehr viele Eindrücke mitnehmen können. Hier sind meine Highlights!Die Highlights#1 Crew Love is True LoveOkay, zugegeben, der Spruch ist nicht mehr der Neuste. Aber er passt einfach so gut! Unsere Data University Crew hat nicht nur an den zwei Tagen vor Ort die Veranstaltung begleitet. Wir haben ein halbes Jahr zusammen an der Realisierung der Data University gearbeitet. Das hört sich nach einer langen Zeit an, aber wenn man eine Veranstaltung zum ersten Mal durchführt, gibt es viele Punkte auf der To-Do Liste. Alles fängt mit der Idee für das Workshop-Format an, dann gilt es einen Namen zu finden, das Logo zu entwickeln, eine Webseite aufzubauen, die Inhalte abzustimmen… und vieles mehr.Danke an alle Kolleg*innen von STATWORX und BARC, für die tolle Zusammenarbeit!(l.h.) Antje, Pascal, Anne-Marie, Niklas; (l.v.) Vivian, Sendi#2 Pizza &amp; Bier im SeminarhausAm ersten Abend trafen sich Teilnehmende, Trainer &amp; Crew der Data University in lockerer Atmosphäre bei Pizza &amp; Bier im Foyer des Seminarhauses. Als Crew hatten wir natürlich auch einiges zu tun, aber wir haben es uns nicht nehmen lassen, mit den fleißigen Workshopteilnehmern anzustoßen und den ersten Tag Revue passieren zu lassen. Im Anschluss an das Get-together im Seminarhaus ging es für alle zum Hörsaalzentrum. Dort fand die Abendveranstaltung statt, das Co-Meetup mit dem Frankfurt Data Science Meetup. Am Meetup nahmen auch Studierende der Goethe-Uni und weitere Data Science Interessierte teil. Mit einem kühlen Bier und einem leckeren Stück Pizza in der Hand ist so ein Hörsaal auch gleich viel gemütlicher!Besucher*innen des Co-Meetups mit Frankfurt DS Meetup#3 Kooperation mit dem Frankfurt Data Science MeetupWo wir schon beim Thema Meetup waren: Am Abend zwischen den beiden Workshoptagen fand in den Räumlichkeiten der Goethe-Uni ein Co-Meetup zwischen der Data University und dem Frankfurt Data Science Meetup statt. Wir arbeiten bereits seit einiger Zeit mit dem Frankfurt DS Meetup zusammen und haben uns daher sehr darüber gefreut, dass sich die Veranstalter des Meetups dazu bereiterklärt haben, bei der Data University mit uns zu kooperieren.Ziel war es eine Abendveranstaltung anzubieten, an der die Workshop-Teilnehmenden, Studierende der Goethe-Uni und Data Science Interessierte zusammenkommen können, sich austauschen und spannende Vorträge hören können.Den Anfang machten dort Timo Gemmecker (Daimler AG) und Fabian Müller (STATWORX GmbH) mit ihrer Keynote Speech zum Thema „How to become a Data Scientist“. In anschließenden Lightning Talks erzählten fünf Data Scientists von ihrem akademischen und professionellen Werdegang und hielten einige interessante Tipps für die Zuhörer*innen bereit.Um das Meetup abzurunden hielt Eldar Rakhmatullaev, Veranstalter des Frankfurt DS Meetups, eine Fishbowl Discussion mit Timo Gemmecker, Sebastian Derwisch (BARC) und Sebastian Heinz (STATWORX GmbH).vl.: Eldar Rakhmatullaev (Frankfurt Data Science Meetup), Sebastian Derwisch (BARC), Timo Gemmecker (Daimler AG), Sebastian Heinz (STATWORX GmbH)#4 Workshops für jeden WissensstandData Science, Deep Learning, Business Strategy, Machine Learning, Künstliche Intelligenz… Bei der Data University haben sich all diese bekannten Begriffe im Curriculum getummelt. Was genau hinter diesen Buzzwords steckt, wissen Einsteiger oftmals nur vage. Erprobte Data Scientists, Engineers oder Business Analysten haben dagegen schon ein fundierte Wissen und möchten in Workshops neue Erkenntnisse für ihren Arbeitsalltag sammeln.Bei der Data University war es also wichtig, dass wir Kurse für verschiedene Wissensstände anbieten. Deshalb gab es Workshops sowohl für Anfänger, wie z.B. „Data Science für Business Analysten“, als auch für Fortgeschrittene, mit Kursen wie „Deep Learning mit TensorFlow 2.0“. l.o. Sebastian Derwisch (BARC), r.u. David Schlepps (STATWORX GmbH)#5 Unsere TeilnehmendenRund 40 Teilnehmende aus diversen Branchen und Arbeitsfeldern waren im Oktober bei der Data University dabei. Durch diese bunte Mischung kamen in den Workshops und der Abendveranstaltungen immer wieder enthusiastische Diskussionen zustande. Dank der hohen Motivation und guten Mitarbeit der Teilnehmenden waren die Workshops dementsprechend erfolgreich. Es hat sehr viel Spaß gemacht so viele interessierte Menschen kennenzulernen und wir haben uns über die zahlreichen positiven Rückmeldungen sehr gefreut.Gruppenfoto der Teilnehmenden und des Data University TeamsOff to new adventures!Jetzt ist es schon etwas mehr als einen Monat her, dass die Data University 2019 stattgefunden hat. Genau wie unsere Teilnehmenden konnten wir einiges aus der Veranstaltung lernen und das Gelernte bereits im Arbeitsalltag umsetzen.Wenn Du dich für die Data University 2020 interessierst, dann abonniere einfach unseren STATWORX Newsletter. Dort halten wir dich mit allen Infos aus den STATWORX Büros auf dem Laufenden, unter anderem auch bezüglich Workshops und Schulungen wie der Data University. Wir freuen uns, dich bald bei einem unserer Workshops zu treffen!Wir bedanken uns bei allen Teilnehmenden &amp; Sponsoren, den Trainern von BARC &amp; STATWORX und dem Orga-Team für eine gelungene Veranstaltung 2019.Über den AutorAnne-Marie AntwerpenJournalism, Social Networks and Media have influenced my life for over a decade now. As a PR &amp; Communications Manager at STATWORX, I can finally combine my love for the many facets of the internet with my job!.e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/5-highlights-der-data-university/;Statworx;  Anne-Marie Antwerpen
  6. November 2019;Combining Price Elasticities and Sales Forecastings for Sales Improvement;"In our blog, we have talked a lot about calculating elasticities. But how do we use them in practice? A powerful way to utilize elasticities is to combine them with sales forecasts. In this blog post, we will my STATWORX colleague Daniel’s lunch place as an example to illustrate the power of combining forecasting and elasticities. As Daniel has already mentioned, this is a small chain where you can put together your salad. Using this example, Daniel explained the concept of price elasticity and used simulated prices to show different ways of calculating it. Similarly, the hypothetical optimal price was identified based on estimated price elasticities. If you want to know more about the details of price elasticities, you should read the post (see /blog/food-for-regression-using-sales-data-to-identify-price-elasticity/). So, let’s go again to the Lunch Place. But this time, it’s not about the determination of a general optimal price, but to calculate a price to achieve specific sales targets. Every sales manager is interested in how to increase sales and, of course how they might evolve in the future. For this, it is necessary to know what the sales depend on and what influence you have to increase sales. Of course, you can rely on the experience and your gut feeling. But nowadays, there are ways to support decision making for specific actions significantly. So what is the situation in the lunch place? Suppose you want to grow the number of sales as much as possible. For that, you certainly will not just buy as many ingredients as fit into your warehouse in the expectation that everything will be consumed. Therefore, one must first be able to estimate how much will be sold soon at all. Sales Forecasting can be used for this aspect. But that alone does not necessarily bring significant added value. If we assume that sales targets have been determined, which of course should not be unrealistic, we have a clear reference point which we can use. If we have a case in which the estimated sales are lower than planned, we can use the price elasticities to determine the price necessary to reach the target. If we have a case in which the estimated sales are lower than planned, we can use the price elasticities to determine the price necessary to reach the target. Of course, there may also be a case in which the estimated sales are higher than planned. If you want to increase the profit margin or because the stock situation does not allow significantly more sales, it is possible to make appropriate price adjustments.Simulation of dataTo keep it simple, we will use a single product. For this, we take prices between 4.90€ and 5.90€. At higher rates, there are also fewer sales than at low prices. In this way, it is possible to estimate price elasticities. The simulated sales at the respective prices, then look like this:Likewise, an upward trend was built into the data. At the same time, the number of sales in the summer months is higher than in the other months. Thus, we can now implement a simple form of Sales Forecasting.Calculating price elasticities and prediction of salesThe price elasticities are estimated using linear regression in which we assume a logarithmic relationship. A logarithmic relationship goes hand in hand with the assumption that demand exponentially grows as the price decreases and also that demand can not sink below zero:     In this equation,  is the intercept,  is the price elasticity and  the dummy for the summer months. In practice, such a simple model will not lead to valid results, but it should suffice for illustration:CoefficientsEstimateStd. ErrorCI.95 (lower)CI.95 (upper)Intercept8.8090.4677.8919.726log(Price)-0.9390.277-1.483-0.3840.1800.0100.1600.199The confidence intervals for the price effect are relatively large, whereas the confidence intervals for the dummy are small. However, this is primarily due to the simulation of the data. Now that we’ve estimated the price elasticity, all we need is an estimate of future sales. Future sales were forecast with a HoltWinters forecast, which only took into account the seasonality.Adjusting prices according to sales targetsNow we have all the necessary results to be able to perform the last step, in which we calculate the prices required to compensate for possible differences. For this, we take as a starting point the Cobb-Douglas-function. In addition, the effect of price and elasticity on quantity was visualized:    If we define the sales target as , we can transform the Cobb Douglas function as follows: With this formula, we can now use our results to calculate the price for a specific target.  represents the sales target,  represents the predicted sales, and  is, of course, the estimated price elasticity -0.939. The formula gives us a factor around which we would have to adjust the price to reach the sales target. Also, the confidence intervals of the price elasticities can be used to calculate a range of price adjustments. In our simple example, we will now use the predicted next month with different targets. The price used for the calculation was taken from the last month, which was 5.40€.DatePrediction PriceTarget Adj. Price Adj. Price (lower CI)Adj. Price (upper CI)2020-01-0115105.4 €20004.00 €4.47 €2.65 €2020-01-0115105.4 €18004.48 €4.80 €3.46 €2020-01-0115105.4 €13006.33 €5.97 €7.90 €2020-01-0115105.4 €10008.38 €7.13 €15.37 €The results show a wide range for optimal price adjustments, where the ranges are partly well over 1 €. Moreover, the range of the predicted sales is not even considered into these results. Of course, this is due to the data and models used here. Apart from that, the results also show the potential of such an implementation in the decision-making process. On the one hand, of course, the prices can be adjusted to achieve specific sales targets. On the other hand, margins can be optimized if the targets are achieved as predicted.ConclusionNevertheless, the example given here shows how price elasticity and sales forecasting can be combined to recommend specific price adjustments. Particularly for sales managers, such a methodology is interesting and can offer added value to the decision-making process. Especially if the modeling and the associated price adjustments are significantly more accurate. In practice, price adjustments for different products can be estimated in this way. Moreover, competitors‘ information or assumptions about them can be included in the models in addition to many other parameters. Similarly, seasonal price elasticities can be estimated, which can lead to possibly more meaningful price adjustments.Über den AutorMarlon SchumacherI am a data scientist at STATWORX and like working with data. Whether it's visualization or the development of machine learning models, it's always interesting..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/combining-price-elasticities-and-sales-forecastings-for-sales-improvement/;Statworx;  Marlon Schumacher
  30. Oktober 2019;Access your Spark Cluster from Everywhere with Apache Livy;"Livy is a REST web service for submitting Spark Jobs or accessing – and thus sharing – long-running Spark Sessions from a remote place. Instead of tedious configuration and installation of your Spark client, Livy takes over the work and provides you with a simple and convenient interface.We at STATWORX use Livy to submit Spark Jobs from Apache’s workflow tool Airflow on volatile Amazon EMR cluster. Besides, several colleagues with different scripting language skills share a running Spark cluster.  Another great aspect of Livy, namely, is that you can choose from a range of scripting languages: Java, Scala, Python, R. As it is the case for Spark, which one of them you actually should/can use, depends on your use case (and on your skills).Architecture – https://livy.incubator.apache.org/Apache Livy is still in the Incubator state, and code can be found at the Git project.When you should use itSince REST APIs are easy to integrate into your application, you should use it when:multiple clients want to share a Spark Session.the clients are lean and should not be overloaded with installation and configuration.you need a quick setup to access your Spark cluster.you want to Integrate Spark into an app on your mobile device.you have volatile clusters, and you do not want to adapt configuration every time.a remote workflow tool submits spark jobs.PreconditionsLivy is generally user-friendly, and you do not really need too much preparation. All you basically need is an HTTP client to communicate to Livy’s REST API. REST APIs are known to be easy to access (states and lists are accessible even by browsers), HTTP(s) is a familiar protocol (status codes to handle exceptions, actions like GET and POST, etc.) while providing all security measures needed. Since Livy is an agent for your Spark requests and carries your code (either as script-snippets or packages for submission) to the cluster, you actually have to write code (or have someone writing the code for you or have a package ready for submission at hand). I opted to maily use python as Spark script language in this blog post and to also interact with the Livy interface itself. Some examples were executed via curl, too.  How to use LivyThere are two modes to interact with the Livy interface: Interactive Sessions have a running session where you can send statements over. Provided that resources are available, these will be executed, and output can be obtained. It can be used to experiment with data or to have quick calculations done. Jobs/Batch submit code packages like programs. A typical use case is a regular task equipped with some arguments and workload done in the background. This could be a data preparation task, for instance, which takes input and output directories as parameters. In the following, we will have a closer look at both cases and the typical process of submission. Each case will be illustrated by examples. Interactive SessionsLet’s start with an example of an interactive Spark Session. Throughout the example, I use python and its requests package to send requests to and retrieve responses from the REST API. As mentioned before, you do not have to follow this path, and you could use your preferred HTTP client instead (provided that it also supports POST and DELETE requests).Starting with a Spark Session. There is a bunch of parameters to configure (you can look up the specifics at Livy Documentation), but for this blog post, we stick to the basics, and we will specify its name and the kind of code. If you have already submitted Spark code without Livy, parameters like executorMemory, (YARN) queue might sound familiar, and in case you run more elaborate tasks that need extra packages, you will definitely know that the jars parameter needs configuration as well.To initiate the session we have to send a POST request to the directive /sessions along with the parameters.import requestsLIVY_HOST = 'http://livy-server'directive = '/sessions'headers = {'Content-Type': 'application/json'}data = {'kind':'pyspark','name':'first-livy'}resp = requests.post(LIVY_HOST+directive, headers=headers, data=json.dumps(data))if resp.status_code == requests.codes.created:    session_id = resp.json()'id'else:    raise CustomError()Livy, in return, responds with an identifier for the session that we extract from its response. Note that the session might need some boot time until YARN (a resource manager in the Hadoop world) has allocated all the resources. Meanwhile, we check the state of the session by querying the directive: /sessions/{session_id}/state. Once the state is idle, we are able to execute commands against it.To execute spark code, statements are the way to go. The code is wrapped into the body of a POST request and sent to the right directive:  sessions/{session_id}/statements. directive = f'/sessions/{session_id}/statements'data = {'code':'...'}resp = request.post(LIVY_HOST+directive, headers=headers, data=json.dumps(data))As response message, we are provided with the following attributes:attributemeaningidto identify the statementcodethe code, once again,  that has been executedstatethe state of the executionoutputthe output of the statementThe statement passes some states (see below) and depending on your code, your interaction (statement can also be canceled) and the resources available, it will end up more or less likely in the success state. The crucial point here is that we have control over the status and can act correspondingly.  By the way, cancelling a statement is done via GET request /sessions/{session_id}/statements/{statement_id}/cancel It is time now to submit a statement: Let us imagine to be one of the classmates of Gauss and being asked to sum up the numbers from 1 to 1000. Luckily you have access to a spark cluster and – even more luckily – it has the Livy REST API running which we are connected to via our mobile app: what we just have to do is write the following spark code:  import textwrapcode = textwrap.dedent(""""""df = spark.createDataFrame(list(range(1,1000)),'int')df.groupBy().sum().collect()0'sum(value)'"""""")code_packed = {'code':code}This is all the logic we need to define. The rest is the execution against the REST API:import timedirective = f'/sessions/{session_id}/statements'resp = requests.post(LIVY_HOST+directive, headers=headers, data=json.dumps(code_packed))if resp.status_code == requests.codes.created:    stmt_id = resp.json()'id'    while True:        info_resp = requests.get(LIVY_HOST+f'/sessions/{session_id}/statements/{stmt_id}')        if info_resp.status_code == requests.codes.ok:            state = info_resp.json()'state'                if state in ('waiting','running'):                    time.sleep(2)                elif state in ('cancelling','cancelled','error'):                    raise CustomException()                else:                    break        else:            raise CustomException()    print(info_resp.json()'output')else:  #something went wrong with creation  raise CustomException()Every 2 seconds, we check the state of statement and treat the outcome accordingly: So we stop the monitoring as soon as state equals available.  Obviously, some more additions need to be made: probably error state would be treated differently to the cancel cases, and it would also be wise to set up a timeout to jump out of the loop at some point in time. Assuming the code was executed successfully, we take a look at the output attribute of the response:{'status': 'ok', 'execution_count': 2, 'data': {'text/plain': '499500'}}There we go, the answer is 499500. Finally, we kill the session again to free resources for others:directive = f'/sessions/{session_id}/statements'requests.delete(LIVY_HOST+directive)Job SubmissionWe now want to move to a more compact solution. Say we have a package ready to solve some sort of problem packed as a jar or as a python script. What only needs to be added are some parameters –  like input files, output directory, and some flags.For the sake of simplicity, we will make use of the well known Wordcount example, which Spark gladly offers an implementation of: Read a rather big file and determine how often each word appears. We again pick python as Spark language. This time curl is used as an HTTP client. As an example file, I have copied the Wikipedia entry found when typing in Livy. The text is actually about the roman historian Titus Livius. I have moved to the AWS cloud for this example because it offers a convenient way to set up a cluster equipped with Livy, and files can easily be stored in S3 by an upload handler. Let’s now see, how we should proceed:curl -X POST --data '{""file"":""s3://livy-example/wordcount.py"",""args"":s3://livy-example/livy_life.txt""}' \-H ""Content-Type: application/json"" http://livy-server:8998/batchesThe structure is quite similar to what we have seen before. By passing over the batch to Livy, we get an identifier in return along with some other information like the current state. {""id"":1,""name"":null,""state"":""running"",""appId"":""application_1567416002081_0005"",...}To monitor the progress of the job, there is also a directive to call: /batches/{batch_id}/stateMost probably, we want to guarantee at first that the job ran successfully. In all other cases, we need to find out what has happened to our job. The directive  /batches/{batchId}/log can be a help here to inspect the run. Finally, the session is removed by:curl -X DELETE http://livy-server:8998/batches/1 which returns: {""msg"":""deleted""} and we are done.TriviaAWS‘ Hadoop cluster service EMR supports Livy natively as Software Configuration option.Apache’s notebook tool Zeppelin supports Livy as an Interpreter, i.e. you write code within the notebook and execute it directly against Livy REST API without handling HTTP yourself.Be cautious not to use Livy in every case when you want to query a Spark cluster: Namely, In case you want to use Spark as Query backend and access data via Spark SQL, rather check out Thriftserver instead of building around Livy. Kerberos can be integrated into Livy for authentication purposes.Über den AutorAndre MünchI am a data engineer at STATWORX. I love to have challenges to setup data structure and compose components to integrate Data Science models into productive environments..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/access-your-spark-cluster-from-everywhere-with-apache-livy/;Statworx;  Andre Münch
  23. Oktober 2019;What’s cooking at STATWORX?;"Can AI make you a better cook?As an enthusiast hobby chef, I got quite excited after stumbling over possible applications of Word Embeddings during a Natural Language Processing course I had taken at university. For a semester project, our professor had suggested finding replacements for ingredients in cooking recipes by learning the relations between words from a large collection of recipes. The project sadly did not materialize, but the idea stuck with me ever since. So, I decided to finally find out, if AI could help me to become a better cook… With this blog post, I would like to:detail how Word Embeddings work and can be learnedtrain embeddings on a recipe datasetpublicly reveal my trusted carrot cake recipefind replacements for all non-essential ingredients in the recipe, at least such that the end result may still be considered a cakebake both recipes and submit them to my picky colleagues for a thorough inspectionimpress Nigella?Why the cake? At STATWORX, we simply love cake – and as a side effect, sugar helps us push the limits for our clients when it comes to coding! ?? Every new employee and each birthday child will prepare a cake and become everybody’s best friend in doing so – at least for the day. Since I was up, I thought I’d give the whole thing a twist.Word EmbeddingsSimply put, Word Embeddings are a collection of numbers that represent a word. To the human eye, they are entirely meaningless, and will also look different every time you re-run the algorithm. The trick is to learn them once on a huge amount of text (think all Wikipedia entries for a large scale) and save them for further use. They may serve as inputs for some Machine Learning models and purposes, or in my case, I will be comparing them to find similarities between words.So, very briefly, how are these embeddings learned? Imagine an arrow hovering over a word in a text; look at the 2 previous words to the left, and the 2 following words to the right (you can ignore all punctuation). These words make up the context for the word in the middle, as shown in the figure below. We use a neural network to learn this context and capture it in the resulting Word Embedding of the word in the middle. You then move the arrow hovering over that word one place to the right and repeat the process for the next word. You go through your entire text multiple times until some maths tells you to stop. Many words will have appeared in many different contexts, and slowly the intricacies of the relations between all words in the text will be captured. For instance, the words ‘Reagan’ and ‘Bush’ will be considered fairly similar, as they will have appeared in many similar contexts, whereas ‘sunflower’ and ‘airplane’ will seem dissimilar.For every ingredient in the following recipe, I will be looking up words the algorithm has deemed most similar. We can assume that similar words have similar contexts they appear in, and in terms of cooking, that hopefully makes them adequate substitutes for the normal ingredients.Coming to the original human-made recipe:350-400g grated carrots 250g flour250g sugar250ml of a neutral-tasting vegetable oil4 eggsCouple of squirts of vanilla aroma/essence/…A tiny bit of salt4 teaspoons of cinnamon. Taste the dough before baking and see if it can take some more, its crucial to commit here!1 pack vanilla sugar1 pack baking powder1 pack baking soda (A bit less important if you don’t have one of the two, it’ll be more dense of a cake then, but better to add I think.)200g mixed unsalted nuts,Put them in a towel, fold it up and bash with a rolling pin or the wine bottle you just emptied until it’s a variety of smaller and larger bits.200g cream cheese100g icing sugarMix everything in a bowl, no need to whisk the eggs. Grease pan and bake at 180°C circulation for 40 minutes. It’ll rise quite a bit, and when you test if the cake still sticks to the knife, you’ll probably need to bake it for another ~15min, but it’s alright if it’s still a little moist inside. Let it cool and mix 200g of cream cheese / Philadelphia with 100g icing sugar and spread on the cake once it’s cooled off.Finding replacement ingredientsThe following code excerpt shows the most similar words to the ones marked in red. The higher the score, the more similar the algorithm considers these words based on all the text that was given.You can immediately see that the results need some inspection and cleaning, as the most similar word to carrots iscarrot. This is fairly common, and a good pre-processing of the data is worth a lot, especially in removing typos or special characters and connecting words like celery root to celery_root.vec.most_similar('carrots')('carrot', 0.76), ('parsnips', 0.70), ('turnips', 0.69), ('celery', 0.64), ('rutabaga', 0.63), ('turnip', 0.63), ('parsnip', 0.61), ('green_beans', 0.59), ('baby_carrots', 0.58), ('celery_root', 0.55)##############################################vec.most_similar('vanilla_essence')('custard_powder', 0.69), ('almond_essence', 0.67), ('ground_almonds', 0.66), ('vanilla_sugar', 0.65), ('essence', 0.64), ('castor', 0.63), ('condensed_milk', 0.63), ('caster_sugar', 0.62), ('bicarbonate_of_soda', 0.62), ('golden_syrup', 0.60)##############################################vec.most_similar('cream_cheese')('cottage_cheese', 0.51), ('mascarpone_cheese', 0.48), ('sour_cream', 0.48), ('whipping_cream', 0.48), ('cream_cheeses', 0.45), ('creamy', 0.45), ('mascarpone', 0.44), ('ricotta_cheese', 0.42), ('heavy_whipping_cream', 0.42), ('marshmallow_creme', 0.42)All replacements are listed in the following table and while some might not be the highest-ranked suggestion, they are still deemed similar and I took my artistic liberties depending on the flavor of the day.OriginalReplacementcarrotshalf parsnips, half celery rootwheat flourrye floursugarbrown sugarvegetable oilolive oilvanilla essencecustard powdermixed nutswalnutscream cheesemascarponeAnd here are the results…Making the two cakes was fairly similar: while grating, the parsnips, and celery root felt a bit drier, yet the AI altered cake (pictured on the right) turned out more moist than the normal recipe (considered a plus by the test audience). The (subtle) taste of the parsnips and celery root went along nicely, adding a little gingerbread note to the cake. As cinnamon was added to both cakes, the overall taste was quite comparable. While I would’ve liked to add different spices to the second cake to make it more distinct, this does prove that the embeddings have come up with good alternative ingredients! Possibly for the next funky experiments, I should venture down the substitute rankings a little further to the midfield and give those a try.ConclusionOverall, the cakes were very well received, with the taste being described as a lot more pleasant than the initial description sounded to some hesitant colleagues. I can only recommend you give either one a try!So why this fun exercise? How do Word Embeddings help us overcome challenges for our clients? Capturing the meaning and relations between words is hugely important when identifying information in text. The embeddings provide a foundation for more complex tasks and models in Natural Language Processing and Machine Learning. Say you wish to identify only certain key information in a newspaper article. While classical text-parsing methods will correctly classify the whole text, they do not assist you in identifying the few wanted goods from the flood of information. Enter Word Embeddings and Deep Learning: the relations provided by well-trained embeddings, together with Deep Learning models that take the context of a sentence into account, reliably help us in retrieving exactly what our clients want to know.This does not stop on the word level either, extensions of this algorithm allow for entire documents or images to be assessed and compared. To end on a culinary note, recent research has used this to pair images with recipes, generating a recipe and cooking instructions for any image of a plated dish!So, pick an evening, toss the tie and don the apron!Über den AutorJonas BraunBesides being a data scientist interested especially in deep learning, I enjoy the outdoors, anything culinary and also the arts. Bridging deeply rooted 'real world' elements with tech is what drives me..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/whats-cooking-at-statworx/;Statworx;  Jonas Braun
  18. Oktober 2019;STATWORX on Tour: Wine;"Last Sunday, part of our STATWORX crew went on a day trip to the picturesque Rheingau, the famous wine region known for its Riesling. The trip was planned around the three tourist pillars of the region: wine, castles &amp; hiking.A warm welcomeFull of excitement (which we had already expressed in the days before, by sending each other funny wine and hiking related GIFs), we met at noon at Frankfurt’s central station, where we all took the train to the Rheingau. After a one-hour train journey, sweetened with some delicious self-made puff pastry snacks from our dear colleague Jessi, we arrived at the beautiful village of Oestrich-Winkel. There, our two tour guides Bernadette and Klaus Wolter from RheingauPur picked us up. After a warm welcome to the region, everyone received their own wine glass for the different tastings during the day, and off we went! Wine tasting can be a real challengeWhile walking through the beautiful vineyards, it was hard to believe that we were only a one-hour train ride away from Frankfurt. With the warm sun and the light breeze, the vast landscapes with beautiful castles could evoke feelings of holidays in Tuscany or Provence. On our way, Bernadette and Klaus taught us about the whole wine production process, starting with the right way to maintain a vineyard up to the correct wine-tasting technique. Because some of us had some starting difficulties with this tasting technique (or perhaps they just wanted to refill their glass very quickly ??), here one more time: Take a big sip of wine in your mouth, close your nose and swirl the wine around a little. Then swallow the wine and open your nose at the same time. This way, you can taste the full flavors of the wine.Gin at a vineyard?After around three hours of learning, hiking, and of course, also some wine tasting, we arrived at the beautiful Castle Johannisberg. From there, we had a beautiful view of the Rhine and the surrounding vineyards. In the castle’s shop, some of us bought a bottle of the local Gin – yes, you’ve read correctly, they produce not only wine but also some delicious fruity and smooth Gin – a real treat for our STATWORX Gin lovers. After one last glass of delightful Riesling in the warm evening sun, we headed back through the vineyards to Oestrich-Winkel, where we all enjoyed a tasty traditional dinner.Off to new adventures!On the train back, we were still talking about all the lovely experiences of the day. Seeing a grape harvesting machine in action is not something you see every day! And even the non-wine drinkers among us had to admit that the wines we had tasted that day were excellent. Although none of us had the impression that we walked that much during the day, we all felt our feet and legs as we got off the train in Frankfurt. With this feeling and the warmth of the sun still on our faces, we all went home already looking forward to the next adventure on one of our STATWORX trips!Thank you very much, Bernadette and Klaus, for the lovely tour!Über den AutorLivia EichenbergerI am a data scientist at STATWORX and especially interested in Causal Machine Learning. I love the logic in data science and the beauty of neat and structured code!.e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/statworx-on-tour-wine-castles-hiking/;Statworx;  Livia Eichenberger
  17. Oktober 2019;rBokeh – Don’t be stopped by missing arguments!;"In my last article on the STATWORX blog, I have guided you through the process of writing a complex JavaScript-callback. However, most users might be slightly frustrated by the lack of arguments to customize a standard rBokeh plot fully. Actually, rBokeh is a little bit outdated (structure() warnings all the way!) and lacks some functionalities that are available in its Python equivalent. But don’t toss in the towel right away! I created some workarounds for these, which you hopefully find helpful.In generalThis approach is my go-to solution to change a rBokeh plot for which there is an argument missing in rBokeh that is available in python. Create the plot.Inspect the structure (str(plot)) of the rBokeh object.Search for the python’s argument name.Overwrite the value with the desired option as derived from python’s bokeh.So, first of all, I set up an initial rBokeh figure that we manipulate later on.plot &lt;- figure(data = iris) %&gt;%   ly_bar(x = Species,  y = Sepal.Length,  hover = TRUE)Manipulate the hover functionalityThe first set of tricks deals with the customization of hover effects. Hover effects are essentials of interactive plots, so it makes a lot of sense to invest some time in optimizing them.AnchorUnlike in python’s bokeh, there is no anchor argument to change the position of a hover tooltip. By default, it appears in the center of the hovered element. To change it, we need to deep dive into the rBokeh object. The object is a deep and complex nested list in which all the information about the plot is stored. While some elements are always structured in the same way, different layers are named by a seemingly arbitrary string (e.g., 51dab389c6209bbf084a86b368f68724). I wrote the following code snippet to change the hover position from center to top_center.# Get the position of the anchor argument within the object-listxyz &lt;- logical()for (i in seq_along(plot$x$spec$model)) {  xyzi &lt;- !is.null(plot$x$spec$modeli$attributes$point_policy)}# Solution using for loopfor (i in which(xyz)) {  plot$x$spec$modeli$attributes$anchor &lt;- ""top_center""}In case you are not very fond of simple for loops, here are also solutions with purrr or lapply:# Solution using purrrxyz &lt;- purrr::map_lgl(plot$x$spec$model, .f = ~ !is.null(.x$attributes$anchor))plot$x$spec$modelwhich(xyz) &lt;-  purrr::map(plot$x$spec$modelwhich(xyz), ~{    .$attributes$anchor &lt;- ""top_center""    return(.)  })# Solution using the apply familyxzy &lt;- sapply(plot$x$spec$model, function(x) !is.null(x$attributes$anchor))plot$x$spec$modelwhich(xyz) &lt;-  lapply(plot$x$spec$modelxyz, function(abc) {    abc$attributes$anchor &lt;- ""top_center""    return(abc)  })All options of the tooltip position can be found here. Point policyAnother option that can be specified in the same way is whether the tooltip should appear at a specific place (snap_to_data) or should follow the courser (follow_mouse). This point_policy option is also missing in rBokeh but can be added by the same logic. Here is a solution for the purrr way but all other descriped options work as well. # Get the position of the point policy argument within the object-listxyz &lt;- purrr::map_lgl(plot$x$spec$model, .f = ~ !is.null(.x$attributes$point_policy))plot$x$spec$modelwhich(xyz) &lt;-  purrr::map(plot$x$spec$modelwhich(xyz), ~{    .$attributes$point_policy &lt;- ""follow_mouse""    return(.)  })What you see is what you wantThe last hover-related issue I want to address are the shown values. rBokeh is rather inflexible in this context. Sometimes (e.g., in ly_points) it is possible to define a specific hover information (either a variable from the data or another data frame/list of the same length as the plot data) but in other cases the hover argument is just logical (TRUE or FALSE, like in ly_bar). If you want to change its default tooltip you need to do this by hand, again. # Set up the figureplot &lt;- figure(data = iris) %&gt;%   ly_bar(x = Species,  y = Sepal.Length,  hover = T)# get the list elements where tooltips are definedhover_info &lt;- purrr::map_lgl(plot$x$spec$model, .f = ~ !is.null(.x$attributes$tooltips))# delete a specific tooltipplot$x$spec$modelwhich(hover_info)$attributes$tooltips2 &lt;- NULL# add a tooltip# list of printed name (test) and name for internal use (@hover_col_3)plot$x$spec$modelwhich(hover_info)$attributes$tooltips2 &lt;-  list(""test"",""@hover_col_3"")hover_data &lt;- purrr::map_lgl(plot$x$spec$model, .f = ~ !is.null(.x$attributes$data$hover_col_1))# manipulate a tooltipplot$x$spec$modelwhich(hover_data) &lt;-  purrr::map(plot$x$spec$modelwhich(hover_data), ~{    .$attributes$data$hover_col_1 &lt;- 1:3    # must match assigned name above    .$attributes$data$hover_col_3 &lt;- letters1:3   return(.)  })plotKeep plotting!I hope you enjoyed my blog post, and it helps you in solving or avoiding some troubles with rBokeh. And who knows, maybe a more intense use of this package might even motivate the developers to update or further develop this excellent package. So, keep plotting!All options of the tooltip position can be found here. In case you are not very fond of simple for loops, here are also solutions with purrr or lapply.Über den AutorMatthias NistlerI am a data scientist at STATWORX and passionate for wrangling data and getting the most out of it. Outside of the office, I use every second for cycling until the sun goes down..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/rbokeh-dont-be-stopped-by-missing-arguments/;Statworx;  Matthias Nistler
  9. Oktober 2019;rBokeh – Tipps and Tricks with JavaScript and beyond!;"You want to have a nice interactive visualization, e.g., in RShiny, where you can zoom in and out, subset the plot or even have hover and click effects? I guess your first shot would be plotly, and rightfully so. However, there is also a great alternative: rBokeh. Lately, I had a project where the functionality of plotly did not meet my needs. In particular, plotly does not have a function to draw rectangles. Just converting a ggplot object into plotly by ggplotly() does not allow for a properly customized callback. Thankfully, one of my STATWORX colleagues pointed me to rBokeh. Its functionality and syntax are quite similar to ggplot or plotly since it also follows the rules of grammar of graphics. In the course of this project, I learned a lot and grew to love rBokeh. Therefore, I want to share some ideas and solutions for some rather specific problems I came across. If you look for a general introduction to rBokeh, check out these links. And now, let’s start!Write complex JavaScript callbacksAn excellent quality of rBokeh is its capability to write customized callbacks with JavaScript (JS). We’re going to dive into this topic next: There are already some examples for rbokeh, but I found them rather basic and insufficiently explained. A more extensive presentation of tasks you can solve with callbacks is available for Python. I will guide you through my approach to write a fancy JS hover callback in R. ?? My goal is to trigger something in my plot by hovering over different elements. For this showcase, I use the classic iris data again. When you hover over a point, you should see the corresponding species name in a highlighted box (check the plot!).The complete code for this task is shown below but let me guide you through it step by step:Set up the information to triggerFirst, I set up the data to be triggered. This includes the names of the different iris species and where this „legend“ should appear in the plot.my_data &lt;- tibble(name = unique(iris$Species),                        x = c(2.1, 3.1, 4.1),                        y = c(7.5, 8.0, 7.5))As usual for an rBokeh plot, we need to call the figure() function first. Since we like to build a sort of legend by ourselves, I remove the legend by setting its location to NULL. The scatter plot is then initialized by ly_points(). For visual purposes, I colorize the points according to their species and increase the point size. The next line, however, is quite essential for the functionality of my callback. You must define a name for the glyph (lname) in order to refer to it in the callback. Note that it is not necessary to call it like the glyph itself. I do this just for convenience.plot &lt;-   figure(data = iris,          legend_location = NULL) %&gt;%   ly_points(x = Sepal.Width,            y = Sepal.Length,            color = Species,             size = 20,            lname = ""points"") %&gt;% The names of the species can be included in the plot by ly_text(). We have to specify the data argument and overwrite the default data that was specified before in figure(). I specify the location and text arguments and align them appropriately. Furthermore, I set the color argument to transparent. This predefined CSS color ensures that the text is „hidden“ when the plot is first rendered and before we first hover over it. Of course, we have to define the lname argument again.  ly_text(data = my_data,          x = x,          y = y,           text = name,           align = ""center"",           baseline = ""middle"",          color = ""transparent"",          lname = ""text"") %&gt;% The centered rectangles are made by ly_crect. The first special argument I set here is the alpha. Like for the ly_text, I want the glyph transparent in the initial plot. However and strangely, some glyphs do not recognize transparent as a predefined color. It is furthermore impossible to use the alpha option inside hex codes in rBokeh. The only remaining option is to set the alpha argument explicitly to zero, yet this requires an additional step later on. Nevertheless, we need to define some arbitrary color to change it later. Also, we must not forget to name the glyph!  ly_crect(data = my_data,           x = x,           y = y,            height = 0.5,           width = 1,           alpha = 0,           color = ""blue"",           lname = ""rects"")That’s it with the initial plot. Now I need to find a linkage between each point in the scatter plot and its corresponding species in my_data. This will heavily depend on the specific case you’re dealing with, but the following lapply does the job here. It returns a list in which each element refers to one line of the iris data and consequently to one point. Each list element is a list by itself and contains the position of the row in my_data with the correct species. Since we use this information in our JS code, you have to account for the zero-based property of JS. Therefore, when we want to refer to the first line in my_data, this is addressed by 0 and so on. One final note here: make sure that you store everything in lists since JS is not capable of digesting R vectors. linkages &lt;- lapply(iris$Species,                    # To keep it general, I store all values in a list.                   # This is not necessary if you link to just one value.                   # However, whenever dealing with one-to-many relationships                   # you need to store them in a list rather than in a vector                   FUN =   function(x) if(x == ""setosa"") list(0) # JS is zero-based!                   else if (x == ""versicolor"") list(1)                    else if (x == ""virginica"") list(2))Now we finally arrive at the long-anticipated callback part. Since it should be initialized by hovering, we have to call tool_hover and define which layer should trigger the callback (ref_layer). In our case, its the scatter plot layer, of course.Since we want to include JS code, we need to call custom_callback. In this, we can define which layers we want to have access to within JS (lnames) and even can make further R objects available inside of JS by the args argument. This must be a named list with the R object on the rhs and the object name you can address within JS on the lhs.plot %&gt;%  tool_hover(    ref_layer = ""points"",    callback = custom_callback(      lnames = c(""points"", ""rects"", ""text""),      args = list(links = linkages),Let’s start with the actual JS code. In the first two lines, I invoke a debugger that makes it easier to develop and debug the code but more on this topic at the end of my post.To make the code more readable and reduce overhead, I define some variables. The first variable (indices) stores the hover information (cb_data) or, more precisely, the indices of the currently hovered points. The further variables contain the underlying data of the respective glyphs.       code = paste0(""            debugger;            console.log(points_data.get('data'));      var indices = cb_data.index'1d'.indices;      var pointsdata = points_data.get('data');      var rectsdata = rects_data.get('data');      var textdata = text_data.get('data');"",This following code snippet is necessary because we had to set alpha = 0 within ly_crect. Since we do not always want to hide this glyph, we have to overwrite and increase this value here. What happens is that when the plot is first called, alpha is zero. When we first hover the plot, alpha is set to 0.2. Note that we have to change both arguments, fill_alpha, and line_alpha. Now we would see the rectangles but change its color to transparent immediately. This is achieved in the next step.      ""rects_glyph.get('fill_alpha').value = 0.2      rects_glyph.get('line_alpha').value = 0.2"",Each time the callback is triggered, that is the courser changes its position, the color for all text and rectangle elements is set to transparent. This is achieved by calling the JS rgba function (red, green, blue, alpha) and setting the alpha avalue to 0 for all glyphs. A simple for loop does this job here. For those who are not familiar with this kind of loop specification: we define an iterator variable i that’s initial value is zero. After each iteration, i is incremented by 1 (note that the ++ are behind i) as long as i is smaller than, e.g., rectsdata.fill_color.length, that is the length of the vector.      ""for (var i=0; i &lt; rectsdata.fill_color.length; i++){      rectsdata.line_colori = 'rgba(255, 255, 255, 0)';      rectsdata.fill_colori = 'rgba(255, 255, 255, 0)';      }      for (var i=0; i &lt; textdata.text_color.length; i++){      textdata.text_colori = 'rgba(255, 255, 255, 0)';      }"",We are now approaching the core of the JS callback. These two loops actually trigger the behavior we desire. The outer loop is necessary if we hover over multiple points simultaneously. For each hovered point, its index position in the data set is stored in the variable ind. In the next step, we finally code the visible callback effect. For this, the element of the links list that corresponds to the currently hovered point is detected and its value is derived. This corresponds to the position in my_data and then changes the color of the desired rectangle and text glyph. The inner loop is just included to keep my example as general as possible. In case that lists stored in links contains multiple elements by themselves.        ""for(var i=0; i &lt; indices.length; i++){      var ind = indicesi;      for (var j=0;j&lt; linksind.length; j++){      rectsdata.fill_colorlinksindj = '#0085AF';      rectsdata.line_colorlinksindj = '#0085AF';      textdata.text_colorlinksindj = '#013848';      }      }"")And that’s it! We built a fancy JS hover callback with rBokeh. And, as promised, here is the full code ?? my_data &lt;- tibble(name = unique(iris$Species),                        x = c(2.1, 3.1, 4.1),                        y = c(7.5, 8.0, 7.5))plot &lt;-   figure(data = iris,          # Remove legend         legend_location = NULL) %&gt;%   ly_points(x = Sepal.Width,            y = Sepal.Length,            # To check the correct functionality of the callback            color = Species,             # Increase size of the points            size = 20,            # IMPORTANT: define a name for the glyph!            lname = ""points"") %&gt;%   ly_text(data = my_data,          x = x,          y = y,           text = name,           # Specify the correct alignment          align = ""center"",           baseline = ""middle"",          # IMPORTANT: Make the text invisible in the initial plot!          color = ""transparent"",          # IMPORTANT: define a name for the glyph!          lname = ""text"") %&gt;%   ly_crect(data = my_data,           x = x,           y = y,            # Adapte the size of the rectangles           height = 0.5,           width = 1,           # IMPORTANT: make the rectangles transparent in the initial plot           alpha = 0,           # IMPORTANT: nevertheless, define an arbitrary color to refer to it in the           # callback           color = ""blue"",           # IMPORTANT: define a name for the glyph!           lname = ""rects"")linkages &lt;- lapply(iris$Species,                    # To keep it general, I store all values in a list.                   # This is not necessary if you link to just one value.                   # However, whenever dealing with one-to-many relationships                   # you need to store them in a list rather than in a vector                   FUN =   function(x) if(x == ""setosa"") list(0) # JS is zero-based!                   else if (x == ""versicolor"") list(1)                    else if (x == ""virginica"") list(2))plot %&gt;%  tool_hover(    custom_callback(      code = paste0(""            debugger;            console.log(points_data.get('data'));      var indices = cb_data.index'1d'.indices;      var pointsdata = points_data.get('data');      var rectsdata = rects_data.get('data');      var textdata = text_data.get('data');      rects_glyph.get('fill_alpha').value = 0.2      rects_glyph.get('line_alpha').value = 0.2      for (var i=0; i&lt; rectsdata.fill_color.length; i++){      rectsdata.line_colori = 'rgba(255, 255, 255, 0)';      rectsdata.fill_colori = 'rgba(255, 255, 255, 0)';      }      for (var i=0; i&lt; textdata.text_color.length; i++){      textdata.text_colori = 'rgba(255, 255, 255, 0)';      }      for(var i=0; i &lt; indices.length; i++){      var ind0 = indicesi;      for (var j=0;j&lt; linksind0.length; j++){      rectsdata.fill_colorlinksind0j = '#0085AF';      rectsdata.line_colorlinksind0j = '#0085AF';      textdata.text_colorlinksind0j = '#013848';      }      }      "",       lnames = c(""points"", ""rects"", ""text""),      args = list(links = linkages)),     ref_layer = ""points"")Final remarks on debuggingAs mentioned above, it is advisable to include a debugger; in your JS code. This directly triggers the debugger to start and allows you to inspect your plot properly. Just right click on your „Viewer“ window in RStudio and select „Inspect Element“.As soon as you hover over the plot, this invokes the debugger and allows you to see the complete JS element with all data and attributes.That’s basically the equivalent to calling rbokeh::debug_callback if you didn’t use your own JS code.Another nice JS function to know is console.log(). Whatever element or variable you want to have a closer look at, just put it inside this function and check what’s behind it via „Inspect Element/Console“. Very helpful to see whether something looks like or contains what you expect it would. Here is an example with console.log(points_data.get('data')).Is there more we can do to customize rBokeh..?This is all I got for you concerning callbacks in rBokeh. In my second rBokeh blog post, I explain how to manipulate a standard rBokeh plot as extensively as its counterpart in Python. Über den AutorMatthias NistlerI am a data scientist at STATWORX and passionate for wrangling data and getting the most out of it. Outside of the office, I use every second for cycling until the sun goes down..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/rbokeh-tipps-and-tricks-with-javascript-and-beyond/;Statworx;  Matthias Nistler
  2. Oktober 2019;Evaluating Model Performance by Building Cross-Validation from Scratch;"Cross-validation is a widely used technique to assess the generalization performance of a machine learning model. Here at STATWORX, we often discuss performance metrics and how to incorporate them efficiently in our data science workflow. In this blog post, I will introduce the basics of cross-validation, provide guidelines to tweak its parameters, and illustrate how to build it from scratch in an efficient way.Model evaluation and cross-validation basics Cross-validation is a model evaluation technique. The central intuition behind model evaluation is to figure out if the trained model is generalizable, that is, whether the predictive power we observe while training is also to be expected on unseen data. We could feed it directly with the data it was developed for, i.e., meant to predict. But then again, there is no way for us to know, or validate, whether the predictions are accurate. Naturally, we would want some kind of benchmark of our model’s generalization performance before launching it into production. Therefore, the idea is to split the existing training data into an actual training set and a hold-out test partition which is not used for training and serves as the „unseen“ data. Since this test partition is, in fact, part of the original training data, we have a full range of „correct“ outcomes to validate against. We can then use an appropriate error metric, such as the Root Mean Squared Error (RMSE) or the Mean Absolute Percentage Error (MAPE) to evaluate model performance. However, the applicable evaluation metric has to be chosen with caution as there are pitfalls (as described in this blog post by my colleague Jan). Many machine learning algorithms allow the user to specify hyperparameters, such as the number of neighbors in k-Nearest Neighbors or the number of trees in a Random Forest. Cross-validation can also be leveraged for „tuning“ the hyperparameters of a model by comparing the generalization error of different model specifications.Common approaches to model evaluationThere are dozens of model evaluation techniques that are always trading off between variance, bias, and computation time. It is essential to know these trade-offs when evaluating a model, since choosing the appropriate technique highly depends on the problem and the data we observe. I will cover this topic once I have introduced two of the most common model evaluation techniques: the train-test-split and k-fold cross-validation. In the former, the training data is randomly split into a train and test partition (Figure 1), commonly with a significant part of the data being retained as the training set. Proportions of 70/30 or 80/20 are the most frequently used in the literature, though the exact ratio depends on the size of your data. The drawback of this approach is that this one-time random split can end up partitioning the data into two very imbalanced parts, thus yielding biased generalization error estimates. That is especially critical if you only have limited data, as some features or patterns could end up entirely in the test part. In such a case, the model has no chance to learn them, and you will potentially underestimate its performance.A more robust alternative is the so-called k-fold cross-validation (Figure 2). Here, the data is shuffled and then randomly partitioned into  folds. The main advantage over the train-test-split approach is that each of the  partitions is iteratively used as a test (i.e., validation) set, with the remaining  parts serving as the training sets in this iteration. This process is repeated  times, such that every observation is included in both training and test sets. The appropriate error metric is then simply calculated as a mean of all of the  folds, giving the cross-validation error.This is more of an extension of the train-test split rather than a completely new method: That is, the train-test procedure is repeated  times. However, note that even if  is chosen to be as low as , i.e., you end up with only two parts. This approach is still superior to the train-test-split in that both parts are iteratively chosen for training so that the model has a chance to learn all the data rather than just a random subset of it. Therefore, this approach usually results in more robust performance estimates.Comparing the two figures above, you can see that a train-test split with a ratio of 80/20 is equivalent to one iteration of a 5-fold (that is, ) cross-validation where 4/5 of the data are retained for training, and 1/5 is held out for validation. The crucial difference is that in k-fold the validation set is shifted in each of the  iterations. Note that a k-fold cross-validation is more robust than merely repeating the train-test split  times: In k-fold CV, the partitioning is done once, and then you iterate through the folds, whereas in the repeated train-test split, you re-partition the data  times, potentially omitting some data from training.Repeated CV and LOOCVThere are many flavors of k-fold cross-validation. For instance, you can do „repeated cross-validation“ as well. The idea is that, once the data is divided into  folds, this partitioning is fixed for the whole procedure. This way, we’re not risking to exclude some portions by chance. In repeated CV, you repeat the process of shuffling and randomly partitioning the data into  folds a certain number of times. You can then average over the resulting cross-validation errors of each run to get a global performance estimate.Another special case of k-fold cross-validation is „Leave One Out Cross-Validation“ (LOOCV), where you set . That is, in each iteration, you use a single observation from your data as the validation portion and the remaining  observations as the training set. While this might sound like a hyper robust version of cross-validation, its usage is generally discouraged for two reasons:First, it’s usually very computationally expensive. For most datasets used in applied machine learning, training your model  times is neither desirable nor feasible (although it may be useful for very small datasets).Second, even if you had the computational power (and time on your hands) to endure this process, another argument advanced by critics of LOOCV from a statistical point of view is that the resulting cross-validation error can exhibit high variance. The cause of that is that your „validation set“ consists of only one observation, and depending on the distribution of your data (and potential outliers), this can vary substantially.In general, note that the performance of LOOCV is a somewhat controversial topic, both in the scientific literature and the broader machine learning community. Therefore, I encourage you to read up on this debate if you consider using LOOCV for estimating the generalization performance of your model (for example, check out this and related posts on StackExchange). As is often the case, the answer might end up being „it depends“. In any case, keep in mind the computational overhead of LOOCV, which is hard to deny (unless you have a tiny dataset).The value of  and the bias-variance trade-offIf  is not (necessarily) the best choice, then how to find an appropriate value for ? It turns out that the answer to this question boils down to the notorious bias-variance trade-off. Why is that? The value for  governs how many folds your data is partitioned into and therefore the size of (i.e., number of observations contained in) each fold. We want to choose  in a way that a sufficiently large portion of our data remains in the training set – after all, we don’t want to give too many observations away that could be used to train our model. The higher the value of , the more observations are included in our training set in each iteration. For instance, suppose we have 1,200 observations in our dataset, then with  our training set would consist of  observations, but with  it would include 1,050 observations. Naturally, with more observations used for training, you approximate your model’s actual performance (as if it were trained on the whole dataset), hence reducing the bias of your error estimate compared to a smaller fraction of the data. But with increasing , the size of your validation partition decreases, and your error estimate in each iteration is more sensitive to these few data points, potentially increasing its overall variance. Basically, it’s choosing between the „extremes“ of the train-test-split on the one hand and LOOCV on the other. The figure below schematically (!) illustrates the bias-variance performance and computational overhead of different cross-validation methods.As a rule of thumb, with higher values for , bias decreases and variance increases. By convention, values like  or  have been deemed to be a good compromise and have thus become the quasi-standard in most applied machine learning settings.„These values have been shown empirically to yield test error rate estimates that suffer neither from excessively high bias nor from very high variance.“James et al. 2013: 184If you are not particularly concerned with the process of cross-validation itself but rather want to seamlessly integrate it into your data science workflow (which I highly recommend!), you should be fine choosing either of these values for  and leave it at that.Implementing cross-validation in caretSpeaking of integrating cross-validation into your daily workflow—which possibilities are there? Luckily, cross-validation is a standard tool in popular machine learning libraries such as the caret package in R. Here you can specify the method with the trainControl function. Below is a script where we fit a random forest with 10-fold cross-validation to the iris dataset.library(caret)set.seed(12345)inTrain &lt;- createDataPartition(y = iris$Species, p = .7, list = FALSE)iris.train &lt;- irisinTrain, iris.test &lt;- iris- inTrain, fit.control &lt;- caret::trainControl(method = ""cv"", number = 10)rf.fit &lt;- caret::train(Species ~ .,                       data = iris.train,                       method = ""rf"",                       trControl = fit.control)We define our desired cross-validation method in the trainControl function, store the output in the object fit.control, and then pass this object to the trControl argument of the train function. You can specify the other methods introduced in this post in a similar fashion:# Leave-One-Out Cross-validation:fit.control &lt;- caret::trainControl(method = ""LOOCV"", number = 10)# Repeated CV (remember to specify the number of repeats!)fit.control &lt;- caret::trainControl(method = ""repeatedcv"", number = 10, repeats = 5)The old-fashioned way: Implementing k-fold cross-validation by handHowever, data science projects can quickly become so complex that the ready-made functions in machine learning packages are not suitable anymore. In such cases, you will have to implement the algorithm—including cross-validation techniques—by hand, tailored to the specific project needs. Let me walk you through a make-shift script for implementing simple k-fold cross-validation in R by hand (we will tackle the script step by step here; you can find the whole code on our GitHub).Simulating data, defining the error metric, and setting # devtools::install_github(""andrebleier/Xy"")library(tidyverse)library(Xy)sim &lt;- Xy(n = 1000,          numvars = c(2,2),          catvars = 0,          cor = c(-0.5, 0.9),          noisevars = 0)sim_data &lt;- sim$dataRMSE &lt;- function(f, o){  sqrt(mean((f - o)^2))}k &lt;- 5We start by loading the required packages and simulating some simulation data with 1,000 observations with the Xy() package developed by my colleague André (check out his blog post on simulating regression data with Xy). Because we need some kind of error metric to evaluate model performance, we define our RMSE function which is pretty straightforward: The RMSE is the root of the mean of the squared error, where error is the difference between our fitted (f) und observed (o) values—you can pretty much read the function from left to right. Lastly, we specify our , which is set to the value of 5 in the example and is stored as a simple integer.Partitioning the dataset.seed(12345)sim_data &lt;- mutate(sim_data,                   my.folds = sample(1:k,                                     size = nrow(sim_data),                                     replace = TRUE))Next up, we partition our data into  folds. For this purpose, we add a new column, my.folds, to the data: We sample (with replacement) from 1 to the value of , so 1 to 5 in our case, and randomly add one of these five numbers to each row (observation) in the data. With 1,000 observations, each number should be assigned about 200 times.Training and validating the modelcv.fun &lt;- function(this.fold, data){  train &lt;- filter(data, my.folds != this.fold)  validate &lt;- filter(data, my.folds == this.fold)  model &lt;- lm(y ~ NLIN_1 + NLIN_2 + LIN_1 + LIN_2,              data = train)  pred &lt;- predict(model, newdata = validate) %&gt;% as.vector()  this.rmse &lt;- RMSE(f = pred, o = validate$y)  return(this.rmse)}Next, we define cv.fun, which is the heart of our cross-validation procedure. This function takes two arguments: this.fold and data. I will come back to the meaning of this.fold in a minute, let’s just set it to 1 for now. Inside the function, we divide the data into a training and validation partition by subsetting according to the values of my.folds and this.fold: Every observation with a randomly assigned my.folds value other than 1 (so approximately 4/5 of the data) goes into training. Every observation with a my.folds value equal to 1 (the remaining 1/5) forms the validation set. For illustration purposes, we then fit a simple linear model with the simulated outcome and four predictors. Note that we only fit this model on the train data! We then use this model to predict() our validation data, and since we have true observed outcomes for this subset of the original overall training data (this is the whole point!), we can compute our RMSE and return it.Iterating through the folds and computing the CV errorcv.error &lt;- sapply(seq_len(k),                   FUN = cv.fun,                   data = sim_data) %&gt;%  mean()cv.errorLastly, we wrap the function call to cv.fun into a sapply() loop—this is where all the magic happens: Here we iterate over the range of , so seq_len(k) leaves us with the vector 1 1 2 3 4 5 in this case. We apply each element of this vector to cv.fun. In apply() statements, the iteration vector is always passed as the first argument of the function which is called, so in our case, each element of this vector at a time is passed to this.fold. We also pass our simulated sim_data as the data argument.Let us quickly recap what this means: In the first iteration, this.fold equals 1. This means that our train set consists of all the observations where my.folds is not 1, and observations with a value of 1 form the validation set (just as in the example above). In the next iteration of the loop, this.fold equals 2. Consequently, observations with 1, 3, 4, and 5 form the training set, and observations with a value of 2 go to validation, and so on. Iterating over all values of , this schematically provides us with the diagonal pattern seen in Figure 2 above, where each data partition at a time is used as a validation set.To wrap it all up, we calculate the mean: This is the mean of our  individual RMSE values and leaves us with our cross-validation error. And there you go: We just defined our custom cross-validation function! This is merely a template: You can insert any model and any error metric. If you’ve been following along so far, feel free to try implementing repeated CV yourself or play around with different values for .ConclusionAs you can see, implementing cross-validation yourself isn’t all that hard. It gives you great flexibility to account for project-specific needs, such as custom error metrics. If you don’t need that much flexibility, enabling cross-validation in popular machine learning packages is a breeze. I hope that I could provide you with a sufficient overview of cross-validation and how to implement it both in pre-defined functions as well as by hand. If you have questions, comments, or ideas, feel free to drop me an e-mail.ReferencesJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. An Introduction to Statistical Learning. New York: Springer.Über den AutorLukas FeickI am a data scientist at STATWORX. I have always enjoyed using data-driven approaches to tackle complex real-world problems, and to help people gain better insights..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/evaluating-model-performance-by-building-cross-validation-from-scratch/;Statworx;  Lukas Feick
  25. September 2019;Time series forecasting with random forest;" Benjamin Franklin said that only two things are certain in life: death and taxes. That explains why my colleagues at STATWORX were less than excited when they told me about their plans for the weekend a few weeks back: doing their income tax declaration. Man, I thought, that sucks, I’d rather spend this time outdoors. And then an idea was born.What could taxes and the outdoors possibly have in common? Well, I asked myself: can we predict tax revenue using random forest? (wildly creative, I know). When dealing with tax revenue, we enter the realm of time series, ruled by fantastic beasts like ARIMA, VAR, STLM, and others. These are tried and proven methods, so why use random forests?Well, you and I may both agree that random forest is one of the most awesome algorithms around: it’s simple, flexible, and powerful. So much so, that Wyner et al. (2015) call it the ‚off-the-shelf‘ tool for most data science applications. Long story short, it’s one of those algorithms that just works (if you want to know exactly how then check out this excellent post by my colleague Andre).Random forest is a hammer, but is time series data a nail?You probably used random forest for regression and classification before, but time series forecasting? Hold up you’re going to say; time series data is special! And you’re right. When it comes to data that has a time dimension, applying machine learning (ML) methods becomes a little tricky.How come? Well, random forests, like most ML methods, have no awareness of time. On the contrary, they take observations to be independent and identically distributed. This assumption is obviously violated in time series data which is characterized by serial dependence.What’s more, random forests or decision tree based methods are unable to predict a trend, i.e., they do not extrapolate. To understand why, recall that trees operate by if-then rules that recursively split the input space. Thus, they’re unable to predict values that fall outside the range of values of the target in the training set.So, should we go back to ARIMA? Not just yet! With a few tricks, we can do time series forecasting with random forests. All it takes is a little pre- and (post-)processing. This blog post will show you how you can harness random forests for forecasting!Let it be said that there are different ways to go about this. Here’s how we are going to pull it off: We’ll raid the time series econometrics toolbox for some old but gold techniques – differencing and statistical transformations. These are cornerstones of ARIMA modeling, but who says we can’t use them for random forests as well? To stick with the topic, we’ll use a time series from the German Statistical Office on the German wage and income tax revenue from 1999 – 2018 (after tax redistribution). You can download the data here. Let’s do it!Getting ready for machine learning or what’s in a time series anyway?Essentially, a (univariate) time series is a vector of values indexed by time. In order to make it ‚learnable‘ we need to do some pre-processing. This can include some or all of the following: Statistical transformations (Box-Cox transform, log transform, etc.)Detrending (differencing, STL, SEATS, etc.)Time Delay Embedding (more on this below)Feature engineering (lags, rolling statistics, Fourier terms, time dummies, etc.)For brevity and clarity, we’ll focus on steps one to three in this post.Ok, let’s structure this a bit: in order to use random forest for time series data we do TDE: transform, difference and embed. Let’s fire up R and load the required packages plus our data.# load the packagessuppressPackageStartupMessages(require(tidyverse))suppressPackageStartupMessages(require(tsibble))suppressPackageStartupMessages(require(randomForest))suppressPackageStartupMessages(require(forecast))# specify the csv file (your path here)file &lt;- "".../tax.csv""# read in the csv filetax_tbl &lt;- readr::read_delim(  file = file,  delim = "";"",  col_names = c(""Year"", ""Type"", month.abb),  skip = 1,  col_types = ""iciiiiiiiiiiii"",  na = c(""..."")) %&gt;%   select(-Type) %&gt;%   gather(Date, Value, -Year) %&gt;%   unite(""Date"", c(Date, Year), sep = "" "") %&gt;%   mutate(    Date = Date %&gt;%       lubridate::parse_date_time(""m y"") %&gt;%       yearmonth()  ) %&gt;%   drop_na() %&gt;%   as_tsibble(index = ""Date"") %&gt;%   filter(Date &lt;= ""2018-12-01"")# convert to ts formattax_ts &lt;- as.ts(tax_tbl)Before we dive into the analysis, let’s quickly check for implicit and explicit missings in the data. The tsibble package has some handy functions to do just that:# implicit missingshas_gaps(tax_tbl)# explicit missingscolSums(is.na(tax_tbl, ""Value""))Nope, looks good! So what kind of time series are we dealing with?# visualizeplot_org &lt;- tax_tbl %&gt;%   ggplot(aes(Date, Value / 1000)) + # to get the axis on a more manageable scale  geom_line() +  theme_minimal() +  labs(title = ""German Wage and Income Taxes 1999 - 2018"", x = ""Year"", y = ""Euros"")Differencing can make all the differenceIf you’ve worked with classical time series models before, you likely stumbled across the concept of differencing. The reason for this is that classical time series models require the data to be stationary. Stationarity means that the mean and variance of the series is finite and does not change over time. Thus, it implies some stability in the statistical properties of the time series. As we can see in the plot, our time series is far from it! There is an upward trend as well as a distinct seasonal pattern in the series.How do these two concepts – differencing and stationarity – relate? You probably already know or guessed it: differencing is one way to make non-stationary time series data stationary. That’s nice to know, but right now we care more about the fact that differencing removes changes in the level of a series and, with it, the trend. Just what we need for our random forest!How is it done? Here, we simply take the first differences of the data, i.e., the difference between consecutive observations . This also works with a seasonal lag , which amounts to taking the difference between an observation and a previous observation from the same season, e.g., November this year and November last year.Whereas differencing can stabilize the mean of a time series, a Box-Cox or log transformation can stabilize the variance. The family of Box-Cox transformations revolves around the parameter lambda:    When lambda is zero, the Box-Cox transformation amounts to taking logs. We choose this value to make the back-transformation of our forecasts straightforward. But don’t hesitate to experiment with different values of lambda or estimate the ‚best‘ value with the help of the forecast package.# pretend we're in December 2017 and have to forecast the next twelve monthstax_ts_org &lt;- window(tax_ts, end = c(2017, 12))# estimate the required order of differencingn_diffs &lt;- nsdiffs(tax_ts_org)# log transform and difference the datatax_ts_trf &lt;- tax_ts_org %&gt;%   log() %&gt;%   diff(n_diffs)# check out the difference! (pun)plot_trf &lt;- tax_ts_trf %&gt;%   autoplot() +  xlab(""Year"") +  ylab(""Euros"") +  ggtitle(""German Wage and Income Taxes 1999 - 2018"") +  theme_minimal()gridExtra::grid.arrange(plot_org, plot_trf)Let’s sum up what we’ve done so far: we first took logs of the data to stabilize the variance. Then, we differenced the data once to make it stationary in the mean. Together, these rather simple transformations took us from non-stationary to stationary.What’s next? Well, now we use the data thus transformed to train our random forest and to make forecasts. Once we obtain the forecasts, we reverse the transformations to get them on the original scale of the data. Just one more step before we get to the modeling part: we still only have a vector. How do we cast this data in a shape, that an ML algorithm can handle?Enter the matrix: Time Delay EmbeddingTo feed our random forest the transformed data, we need to turn what is essentially a vector into a matrix, i.e., a structure that an ML algorithm can work with. For this, we make use of a concept called time delay embedding.Time delay embedding represents a time series in a Euclidean space with the embedding dimension . To do this in R, use the base function embed(). All you have to do is plug in the time series object and set the embedding dimension as one greater than the desired number of lags.lag_order &lt;- 6 # the desired number of lags (six months)horizon &lt;- 12 # the forecast horizon (twelve months)tax_ts_mbd &lt;- embed(tax_ts_trf, lag_order + 1) # embedding magic!When you check out the tax_ts_mbd object, you’ll see that you get a matrix where the dependent variable in the first column is regressed on its lags in the remaining columns:    Time delay embedding allows us to use any linear or non-linear regression method on time series data, be it random forest, gradient boosting, support vector machines, etc. I decided to go with a lag of six months, but you can play around with other lags. Moreover, the forecast horizon is twelve as we’re forecasting the tax revenue for the year 2018.When it comes to forecasting, I’m pretty directIn this post, we make use of the direct forecasting strategy. That means that we estimate  separate models , one for each forecast horizon. In other words, we train a separate model for each time distance in the data. For an awesome tutorial on how this works check out this post.The direct forecasting strategy is less efficient than the recursive forecasting strategy, which estimates only one model  and, as the name suggests, re-uses it  times. Recursive, in this case, means that we feed back each forecast as input back to the model to get the next forecast.Despite this drawback, the direct strategy has two key advantages: First, it does not suffer from an accumulation of forecast errors, and second, it makes it straightforward to include exogenous predictors.How to implement the direct forecasting strategy is nicely demonstrated in the before-mentioned post, so I don’t want rehash it here. If you’re short on time, the tl;dr is this: we use the direct forecasting strategy to generate multi-step ahead forecasts. This entails training a model for each forecast horizon by progressively reshaping the training data to reflect the time distance between observations.y_train &lt;- tax_ts_mbd, 1 # the targetX_train &lt;- tax_ts_mbd, -1 # everything but the targety_test &lt;- window(tax_ts, start = c(2018, 1), end = c(2018, 12)) # the year 2018X_test &lt;- tax_ts_mbdnrow(tax_ts_mbd), c(1:lag_order) # the test set consisting# of the six most recent values (we have six lags) of the training set. It's the# same for all models.If you followed me here, kudos, we’re almost done! For now, we get to the fun part: letting our random forest loose on this data. We train the model in a loop, where each iteration fits one model, one for each forecast horizon .The random forest forecast: things are looking goodBelow I’m using the random forest straight out of the box, not even bothering tuning it (a topic to which I’d like to dedicate a post in the future). It may seem lazy (and probably is), but I stripped the process down to its bare bones in the hope of showing most clearly what is going on here.forecasts_rf &lt;- numeric(horizon)for (i in 1:horizon){  # set seed  set.seed(2019)  # fit the model  fit_rf &lt;- randomForest(X_train, y_train)  # predict using the test set  forecasts_rfi &lt;- predict(fit_rf, X_test)  # here is where we repeatedly reshape the training data to reflect the time distance  # corresponding to the current forecast horizon.  y_train &lt;- y_train-1   X_train &lt;- X_train-nrow(X_train),  }Alright, the loop’s done. We just trained twelve models and got twelve forecasts. Since we transformed our time series before training, we need to transform the forecasts back.Back to the former or how we get forecasts on the original scaleAs we took the log transform earlier, the back-transform is rather straightforward. We roll back the process from the inside out, i.e., we first reverse the differencing and then the log transform. We do this by exponentiating the cumulative sum of our transformed forecasts and multiplying the result with the last observation of our time series. In other words, we calculate:    # calculate the exp termexp_term &lt;- exp(cumsum(forecasts_rf))# extract the last observation from the time series (y_t)last_observation &lt;- as.vector(tail(tax_ts_org, 1))# calculate the final predictionsbacktransformed_forecasts &lt;- last_observation * exp_term# convert to ts formaty_pred &lt;- ts(  backtransformed_forecasts,  start = c(2018, 1),  frequency = 12)# add the forecasts to the original tibbletax_tbl &lt;- tax_tbl %&gt;%   mutate(Forecast = c(rep(NA, length(tax_ts_org)), y_pred))# visualize the forecastsplot_fc &lt;- tax_tbl %&gt;%   ggplot(aes(x = Date)) +  geom_line(aes(y = Value / 1000)) +  geom_line(aes(y = Forecast / 1000), color = ""blue"") +  theme_minimal() +  labs(    title = ""Forecast of the German Wage and Income Tax for the Year 2018"",    x = ""Year"",    y = ""Euros""  )accuracy(y_pred, y_test)MERMSEMAEMPEMAPEACF1Theil’s UTest set198307.5352789.9238652.62.2737852.6077730.2572560.0695694It looks like our forecast is pretty good! We achieved a MAPE of 2.6 percent. But since one should never rely on accuracy metrics alone, let’s quickly calculate a simple benchmark like the seasonal naive model. That’s a low hurdle to pass, so if our model doesn’t beat it, in the bin it goes.benchmark &lt;- forecast(snaive(tax_ts_org), h = horizon)tax_ts %&gt;%   autoplot() +  autolayer(benchmark, PI = FALSE)accuracy(benchmark, y_test)MERMSEMAEMPEMAPEMASEACF1Theil’s UTraining set216938.7470874.8388618.52.8525336.6487111.0000000.6303982NATest set485006.8495096.8485006.85.5079435.5079431.2480280.13824610.0926723The error metrics are much higher here, so it’s safe to say our random forest did a good job. Where do we go from here? Well, we haven’t tried it yet, but we may further improve our forecasts with some hyperparameter tuning. Also, just between the two of us, maybe random forest is pretty good, but not the best model for the job. We could try others or an ensemble of models.If you take away one thing from this post today, let it be this: We can do effective time series forecasting with machine learning without whipping out big guns like recurrent neural networks. All it takes is a little pre- and post-processing. So why not include random forest in your arsenal the next time you do forecasting (or procrastinate on doing your taxes)? ??UPDATE: Hi, some people have been asking me for the data, which I realize is a bit hard to come by (destatis isn’t exactly what you call user-friendly). So here’s a link to the STATWORX GitHub where you can download the csv file.ReferencesWyner, Abraham J., et al. „Explaining the success of adaboost and random forests as interpolating classifiers.“ The Journal of Machine Learning Research 18.1 (2017): 1558-1590.Über den AutorManuel TilgnerI am a data scientist at STATWORX, and I enjoy making data make sense. Why? Because there's something magical about turning a jumble of numbers into insights. In my free time, I love wandering through the forest or playing in the local big band..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/time-series-forecasting-with-random-forest/;Statworx;  Manuel Tilgner
  18. September 2019;Using emojis and .png as icons in your ggplot;"Nearly one year ago, I analyzed how we use emojis in our Slack messages. Since then, STATWORX grew, and we are a lot more people now! So, I just wanted to check if something changed.Last time, I did not show our custom emojis, since they are, of course, not available in the fonts I used. This time, I will incorporate them with geom_image(). It is part of the ggimage package from Guangchuang Yu, which you can find here on his Github. With geom_image() you can include images like .png files to your ggplot. What changed since last year?Let’s first have a look at the amount of emojis we are using. In the plot below, you can see that since my last analysis in October 2018 (red line) the amount of emojis is rising. Not as much as I thought it would, but compared to the previous period, we now have more days with a usage of over 100 emojis per day!Like last time, our top emoji is ??, followed by ?? and ??. But sneaking in at number ten is one of our custom emojis: party_hat_parrot!How to include custom images?In my previous blogpost, I hid all our custom emojis behind?since they were not part of the font. It did not occur to me to use their images, even though the package is from the same creator! So, to make up for my ignorance, I grabbed the top 30 custom emojis and downloaded their images from our Slack servers, saved them as .png and made sure they are all roughly the same size.To use geom_image() I just added the path of the images to my data (the … are just an abbreviation for the complete path).         NAME COUNT REACTION IMAGE1:      alnatura   25    63 .../custom/alnatura.png2:        blog   19    20 .../custom/blog.png3:      dataiku   15    22 .../custom/dataiku.png4: dealwithit_parrot   3    100 .../custom/dealwithit_parrot.png5:    deananddavid   31    18 .../custom/deananddavid.pngThis would have been enough to just add the images now, but since I wanted the NAME attribute as a label, I included geom_text_repel from the ggrepel library. This makes handling of non-overlapping labels much simpler! ggplot(custom_dt, aes( x = REACTION, y = COUNT, label = NAME)) +  geom_image(aes(image = IMAGE), size = 0.04) +  geom_text_repel(point.padding = 0.9, segment.alpha = 0) +  xlab(""as reaction"") +  ylab(""within message"") +  theme_minimal()Usually, if a label is „too far“ away from the marker, geom_text_repel includes a line to indicate where the labels belong. Since these lines would overlap the images, I used segment.alpha = 0 to make them invisible. With point.padding = 0.9 I gave the labels a bit more space, so it looks nicer. Depending on the size of the plot, this needs to be adjusted. In the plot, one can see our usage of emojis within a message (y-axis) and as a reaction (x-axis).To combine the emoji font and custom emojis, I used the following data and code — really… why did I not do this last time? ?? Since the UNICODE is NA when I want to use the IMAGE, there is no „double plotting“.           EMOJI REACTION COUNT  SUM PLACE   UNICODE  IMAGE 1:           :+1:   1090   0 1090   1 \U0001f44d 2:          :joy:    609  152  761   2 \U0001f602 3:         :smile:    91  496  587   3 \U0001f604 4:           :-1:    434   9  443   4 \U0001f44e 5:          :tada:    346   38  384   5 \U0001f389 6:          :fire:    274   17  291   6 \U0001f525 7: :slightly_smiling_face:     1  250  251   7 \U0001f642 8:          :wink:    27  191  218   8 \U0001f609 9:          :clap:    201   13  214   9 \U0001f44f10:    :party_hat_parrot:    192   9  201   10    &lt;NA&gt;  .../custom/party_hat_parrot.pngquartz()ggplot(plotdata2, aes(x = PLACE, y = SUM, label = UNICODE)) +  geom_bar(stat = ""identity"", fill = ""steelblue"") +  geom_text(family=""EmojiOne"") +  xlab(""Most popular emojis"") +  ylab(""Number of usage"") +  scale_fill_brewer(palette = ""Paired"") +  geom_image(aes(image = IMAGE), size = 0.04) +  theme_minimal()ps = grid.export(paste0(main_path, ""plots/top-10-used-emojis.svg""), addClass=T)dev.off()The meaning behind emojisNow we know what our top emojis are. But what is the rest of the world doing? Thanks to Emojimore for providing me with this overview! On their site, you can find meanings for a lot more emojis.Behind each of our custom emojis is a story as well. For example, all the food emojis are helping us every day to decide where to eat and provide information on what everyone is planning for lunch! And if you do not agree with the decision, just react with sadphan to let the others know about your feelings.If you want to know the whole stories behind all custom emojis or even help create new ones, then maybe you should join our team — check out our available job offers here!Über den AutorJakob GeppNumbers were always my passion and as a data scientist and a statistician at STATWORX I can fullfill my nerdy needs. Also I am responsable for our blog. So if you have any questions or suggestions, just send me an email!.e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/using-emojis-and-png-as-icons-in-your-ggplot/;Statworx;  Jakob Gepp
  11. September 2019;Dynamic UI Elements in Shiny;"At STATWORX, we regularly deploy our project results with the help of Shiny. It’s not only an easy way of letting potential users interact with your R-code, but it’s also fun to design a good-looking app. One of Shiny’s biggest strengths is its inherent reactivity after all being reactive to user input is a web-applications prime purpose. Unfortunately, many apps seem to only make use of Shiny’s responsiveness on the server side while keeping the UI completely static. This doesn’t have to be necessarily bad. Some apps wouldn’t profit from having dynamic UI elements. Adding them regardless could result in the app feeling gimmicky. But in many cases adding reactivity to the UI can not only result in less clutter on the screen but also cleaner code. And we all like that, don’t we? A toolbox for reactivity: renderUIShiny natively provides convenient tools to turn the UI of any app reactive to input. In today’s blog entry, we are namely going to look at the renderUI function in conjunction with lapply and do.call.renderUI is helpful because it frees us from the chains of having to define what kind of object we’d like to render in our render function. renderUI can render any UI element. We could, for example, let the type of the content of our uiOutput be reactive to an input instead of being set in stone.Introducing reactivity with lapplyImagine a situation where you’re tasked with building a dashboard showing the user three different KPIs for three different countries. The most obvious approach would be to specify the position of each KPI-box on the UI side of the app and creating each element on the server side with the help of shinydashboard::renderValueBox as seen in the example below. The common waylibrary(shiny)library(shinydashboard)ui &lt;- dashboardPage(  dashboardHeader(),  dashboardSidebar(),  dashboardBody(column(width = 4,                        fluidRow(valueBoxOutput(""ch_1"", width = 12)),                       fluidRow(valueBoxOutput(""jp_1"", width = 12)),                       fluidRow(valueBoxOutput(""ger_1"", width = 12))),                column(width = 4,                       fluidRow(valueBoxOutput(""ch_2"", width = 12)),                       fluidRow(valueBoxOutput(""jp_2"", width = 12)),                       fluidRow(valueBoxOutput(""ger_2"", width = 12))),                column(width = 4,                        fluidRow(valueBoxOutput(""ch_3"", width = 12)),                       fluidRow(valueBoxOutput(""jp_3"", width = 12)),                       fluidRow(valueBoxOutput(""ger_3"", width = 12)))  ))server &lt;- function(input, output) {  output$ch_1 &lt;- renderValueBox({    valueBox(value = ""CH"",             subtitle = ""Box 1"")  })  output$ch_2 &lt;- renderValueBox({    valueBox(value = ""CH"",             subtitle = ""Box 2"")  })  output$ch_3 &lt;- renderValueBox({    valueBox(value = ""CH"",             subtitle = ""Box 3"",             width = 12)  })  output$jp_1 &lt;- renderValueBox({    valueBox(value = ""JP"",             subtitle = ""Box 1"",             width = 12)  })  output$jp_2 &lt;- renderValueBox({    valueBox(value = ""JP"",             subtitle = ""Box 2"",             width = 12)  })  output$jp_3 &lt;- renderValueBox({    valueBox(value = ""JP"",             subtitle = ""Box 3"",             width = 12)  })  output$ger_1 &lt;- renderValueBox({    valueBox(value = ""GER"",             subtitle = ""Box 1"",             width = 12)  })  output$ger_2 &lt;- renderValueBox({    valueBox(value = ""GER"",             subtitle = ""Box 2"",             width = 12)  })  output$ger_3 &lt;- renderValueBox({    valueBox(value = ""GER"",             subtitle = ""Box 3"",             width = 12)  })}shinyApp(ui = ui, server = server)This might be a working solution to the task at hand, but it is hardly an elegant one. The valueboxes take up a large amount of space in our app and even though they can be resized or moved around, we always have to look at all the boxes, regardless of which ones are currently of interest. The code is also highly repetitive and largely consists of copy-pasted code chunks. A much more elegant solution would be to only show the boxes for each unit of interest (in our case countries) as chosen by the user. Here’s where renderUI comes in.renderUI not only allows us to render UI objects of any type but also integrates well with the lapply function. This means that we don’t have to render every valuebox separately, but let lapply do this repetitive job for us. The reactive wayAssuming we have any kind of input named „select“ in our app, the following code chunk will generate a valuebox for each element selected with that input. The generated boxes will show the name of each individual element as value and have their subtitle set to „Box 1“. lapply(seq_along(input$select), function(i) {      fluidRow(        valueBox(value = input$selecti,               subtitle = ""Box 1"",               width = 12)      )    })How does this work exactly? The lapply function iterates over each element of our input „select“ and executes whatever code we feed it once per element. In our case, that means lapply takes the elements of our input and creates a valuebox embedded in a fluidrow for each (technically it just spits out the corresponding HTML code that would create that). This has multiple advantages: Only boxes for chosen elements are shown, reducing visual clutter and showing what really matters.We have effectively condensed 3 renderValueBox calls into a single renderUI call, reducing copy-pasted sections in our code. If we apply this to our app our code will look something like this:library(shiny)library(shinydashboard)ui &lt;- dashboardPage(  dashboardHeader(),  dashboardSidebar(    selectizeInput(      inputId = ""select"",      label = ""Select countries:"",      choices = c(""CH"", ""JP"", ""GER""),      multiple = TRUE)  ),  dashboardBody(column(4, uiOutput(""ui1"")),                column(4, uiOutput(""ui2"")),                column(4, uiOutput(""ui3"")))  )server &lt;- function(input, output) {  output$ui1 &lt;- renderUI({    req(input$select)    lapply(seq_along(input$select), function(i) {      fluidRow(        valueBox(value = input$selecti,               subtitle = ""Box 1"",               width = 12)        )    })  })  output$ui2 &lt;- renderUI({    req(input$select)    lapply(seq_along(input$select), function(i) {      fluidRow(        valueBox(value = input$selecti,               subtitle = ""Box 2"",               width = 12)      )    })  })  output$ui3 &lt;- renderUI({    req(input$select)    lapply(seq_along(input$select), function(i) {      fluidRow(        valueBox(value = input$selecti,               subtitle = ""Box 3"",               width = 12)      )    })  })}shinyApp(ui = ui, server = server)The UI now dynamically responds to our inputs in the selectizeInput. This means that users can still show all KPI boxes if needed – but they won’t have to. In my opinion, this flexibility is what shiny was designed for – letting users interact with R-code dynamically. We have also effectively cut down on copy-pasted code by 66% already! There is still some repetition in the multiple renderUI function calls, but the server side of our app is already much more pleasing to read and make sense of than the static example of our previous app. Beyond lapply: Going further with do.callWe have just seen that with the help of lapply renderUI can dynamically generate entire UI elements. That is, however, not the full extent of what renderUI can do. Individual parts of a UI element can also be generated dynamically if we employ the help of functions that allow us to pass the dynamically generated parts of a UI element as arguments to the function call creating the element. Within the reactive context of renderUI we can call functions at will, which means that we have more tools than just lapply on our hands. Enter do.call. The do.call function enables us to execute function calls by passing a list of arguments to said function. This may sound like function-ception but bear with me. Following the do.callAssume that we’d like to create a tabsetPanel, but instead of specifying the number of tabs shown we let the users decide. The solution to this task is a two-step process.  We use lapply to iterate over a user-chosen number to create the specified amount of tabs.We use do.call to execute the shiny::tabsetPanel function with the tabs from step 1 being passed to the do.call as a simple argument. This would look something like this:# create tabs from inputmyTabs &lt;- lapply(1:input$slider, function(i) {  tabPanel(title = glue(""Tab {i}""),           h3(glue(""Content {i}""))  )})# execute tabsetPanel with tabs added as argumentsdo.call(tabsetPanel, myTabs)This creates the HTML for a tabsetPanel with a user-chosen number of tabs that all have a unique title and can be filled with content. You can try it out with this example app:library(shiny)library(shinydashboard)library(glue)ui &lt;- dashboardPage(  dashboardHeader(),  dashboardSidebar(    sliderInput(inputId = ""slider"", label = NULL, min = 1, max = 5, value = 3, step = 1)  ),  dashboardBody(    fluidRow(      box(width = 12,          p(mainPanel(width = 12,                      column(width = 6, uiOutput(""reference"")),                      column(width = 6, uiOutput(""comparison""))          )          )      )    )  ))server &lt;- function(input, output) {  output$reference &lt;- renderUI({    tabsetPanel(      tabPanel(        ""Reference"",        h3(""Reference Content""))    )  })  output$comparison &lt;- renderUI({    req(input$slider)    myTabs &lt;- lapply(1:input$slider, function(i) {      tabPanel(title = glue(""Tab {i}""),               h3(glue(""Content {i}""))      )    })    do.call(tabsetPanel, myTabs)  })}shinyApp(ui = ui, server = server)As you can see, renderUI offers a very flexible and dynamic approach to offer to UI design when being used in conjunction with lapply and the more advanced do.call. Try using these tools next time you build an app and bring the same reactivity to Shiny’s UI as your already used to utilizing in its server part. Über den AutorOliver GuggenbühlI am a data scientist at STATWORX and love telling stories with data - the ShinyR the better!.e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/dynamic-ui-elements-in-shiny/;Statworx;  Oliver Guggenbühl
  4. September 2019;R or Python;"My blog post aims at aspiring data scientists who have to decide which programming language they want to learn first. At STATWORX, we primarily use the two most popular languages, R and Python. Both languages have their strengths and weaknesses, which is why you should ideally master both. To get started, however, we recommend to learn one language and then tackle the other one. Don’t forget that both languages are just a tool; what matters is what you do with your tool at hand. Once you have understood and mastered the concepts of working with data, it should generally be easier to learn the second language. This blog post introduces both languages to beginners. I try to stay as unbiased as possible. For certain tasks, I prefer R – for others Python. Hence, the recommendations are subjective to my preferences. Nevertheless, I hope my experiences help you as a newcomer.Overview R and PythonBoth Python and R are open source programming languages, which means that the source code is publicly available and can be used for free. While Python is a general-purpose programming language, R was developed for statistical analysis. Therefore, users of these languages often have different backgrounds. In general terms, software developers use Python and statisticians R.RPythonRelease19931991DeveloperR Core TeamPython Software FoundationPackage ManagementCRANPyPIA collection of extensionsBoth languages have a basic set of functions that can be extended with packages.The Comprehensive R Archive Network (CRAN) is a platform for R packages. A set of requirements must be satisfied to include a package on CRAN. CRAN ensures that all packages available for download work. More than 10,000 packages are available on CRAN. Since R is the standard language for statisticians, CRAN has a suitable solution for almost any statistical problem. So it’s just the right place for the latest statistical methods and analyses.  Some packages depend on other packages, which can cause problems in specific scenarios. The objective of the packrat package is to ensure that all dependencies are met and everything runs smoothly.Python has two package management platforms: conda and PyPI (Python Package Index). There are also over 10,000 packages for Python, which, compared to R, cover a vast range of applications. However, only a small share of packages are relevant for data science projects. Because complications can occur when you install Python packages globally, you can use virtual environments. They ensure smooth processes for the various packages and dependencies from package to package, similar to the packrat package in R. It can be quite hard for beginners to get a good grasp of the idea behind the different environments.Although R and Python packages are used the same way, there are some fundamental differences. Usually, an R package is developed by a single person or a small group of researchers. The authors write the package based on a scientific paper and refer to it in the documentation. Whereas, often, large groups of developers are working on Python packages (numpy, pandas, scikit-learn). That has advantages and disadvantages:The namespace for functions is clear, and functions have the same structure. E.g., when setting up different models and comparing their performance, you’ll use the scikit-learn package in Python. In R you’ll use different packages depending on what model you want to implement. The function and argument names differ from package to package – which can be cumbersome. It is noteworthy that the packages caret and parsnip are trying to correct those discrepancies in R in hindsight.In some cases, the functions of scikit-learn have to be checked thoroughly. The developers are usually optimization oriented and are neglecting some statistical aspects. E.g., the scikit-learn function performing a logistic regression (sklearn.linear_model.LogisticRegression) uses per default L2 regularization. The only way to get a linear logistic model without regularization is to set the regularization parameter to a high number. That’s surprising, given the functions name. Furthermore, the developers didn’t understand why this poses an issue to the users. Even if the regularized model generalizes better and, thus, might have a better predictive performance, there are cases where I would like to obtain the non-regularized coefficients for inference. Find your IDEProgrammers often use an integrated development environment (IDE) that facilitates their work with small but subtle tools.For R users, RStudio has become the standard IDE. The IDE is distributed by the company RStudio Inc., which stands commercially behind R. RStudio provides not only a pleasant working environment but also develops packages and extensions for the R language. For example, the RStudio team contributed important packages like tidyverse, packrat, and devtools, as well as popular extensions like shiny (for dashboards) and RMarkdown (for reports).Python users can choose between numerous IDEs (PyCharm, Visual Studio Code, Spyder, and many more). However, no company stands behind Python and is comparable to RStudio Inc. Thanks to the efforts of the large community and the Python Software Foundation, new extensions for Python are continually being published.The Art of Data VisualizationThe most commonly used packages for data visualization with Python are matplotlib and seaborn. Dashboards can be created in Python with dash.However, R has an ace up its sleeve in data visualization: the ggplot2 package, which is based on Leland Wilkinson’s book The Grammar of Graphics. With this package, you can create attractive and customized graphics, which you can share via shiny dashboards with others.Both programming languages offer the possibility to create beautiful graphics easily. Nevertheless, the R package ggplot2 convinces with its flexibility, visual possibilities, and thought-out philosophy behind it. Sharing the graphics with a shiny dashboard is extremely easy. I want to mention that there are efforts to implement ggplot2 into Python.Props for readabilityPython was designed following the motto readability counts. So even people, who are not familiar with the programming language can interpret what was done in the code.R, as a programming language, has changed a lot in recent years. Mainly because of the packages developed by the RStudio team. The readability of the code has improved substantially with the dplyr package and the use of the pipe operator, where code can be read from left to right.The speed with different observation sizesNext, I compare how long it takes to create a simulated dataset in R and Python. For a fair comparison, the conditions should be approximately the same. The data is simulated with the packages Xy and XyPy in R and Python, respectively. I used microbenchmark in R and timeit in Python to measure the time. Also, I parallelized the process using eight cores (R: parallel, Python: multiprocessing) to generate the simulation as fast as possible.For the experiment, a dataset with 100 observations and 50 variables is simulated 100 times. The time it takes the computer to perform the simulation is measured individually for each simulation. That is then repeated for 1,000, 10,000, 100,000 and 1,000,000 observations.The R and Python code snippets are shown below.# R# devtools::install_github(""andrebleier/Xy"")# install.packages(""parallel"")# install.packages(""microbenchmark"")# Load packageslibrary(Xy)library(microbenchmark)library(parallel)# Extract function definition from for loopsim_this &lt;- function(n_sim) {  sim &lt;- microbenchmark(Xy(n = n_sim,                           numvars = c(50,0),                           catvars = 0),                         times = 100, unit = ""s"")  data.frame(n = n_sim,              mean = summary(sim), 4)}# Time measurement for different number of simulationsn_sim &lt;- c(1e2, 1e3, 1e4, 1e5, 1e6)sim_in_r &lt;- data.frame(n = rep(0, length(n_sim)),                       t = rep(0, length(n_sim)))for(i in 1:length(n_sim)){  out &lt;- mclapply(n_simi,                  FUN = sim_this,                  mc.cores = 8)  sim_in_ri, 1 &lt;- out11  sim_in_ri, 2 &lt;- out12}# Pythonimport multiprocessing as mpimport numpy as npimport timeitfrom XyPy import Xy# Predefine function of interestdef sim_this(n_sim):  return(timeit.timeit( lambda: Xy(n = int(n_sim),      numvars = 50, 0,      catvars = 0, 0,      weights = 5, 10,      stn = 4.0,      cor = 0, 0.1,      interactions = 1,      noisevars = 5), number = 100))# Paralleled computation pool = mp.Pool(processes = 8)n_sim = np.array(1e2, 1e3, 1e4, 1e5, 1e6)results = pool.map(sim_this, n_sim)The average duration, sorted by the number of observations, is shown in the plot below for R and Python. The x-axis is shown on a logarithmic scale with base 10, to make the plot clearer.While R is a little faster for dataset sizes of 100 and 1.000 observations, Python is significantly faster for 100.000 and 1.000.000 observations.For other speed comparisons, I recommend the following STATWORX blog posts: pandas vs. data.table and pandas vs. data.table part 2. In these posts, the focus was laid on data manipulation.The Standard in Deep LearningIf you are particularly interested in deep learning, I recommend Python to you. Most popular deep learning libraries were written or are designed to be used with Python. Deep learning is also possible with R, but the R deep learning community is much smaller. Implementations like Keras and TensorFlow can be called in R but are run in Python in the background. Furthermore, the packages do not provide full flexibility for the users, e.g., not all TensorFlow functions are available. A Survey in the CommunityAs aspiring data scientists, Kaggle is an essential platform for you. There you can participate in exciting machine learning competitions, experiment for yourself, and learn from the experiences of the community.In 2018, Kaggle conducted a Machine Learning &amp; Data Science Survey. The poll was online for two weeks and received a total of 23,859 replies. From the results of this survey, I have created different plots to get some insights regarding my blog topic. The code for the individual plots is publicly available on Github.Excursion: Python &amp; R compared to other languagesBefore we jump to R and Python, let’s see how they compare to other programming languages. Each respondent indicated which language she uses primarily. The plot below aggregates by language, and the result is: Most of the participants use Python! Followed by R in second place. In this survey, we do not distinguish between fields of work, which is why Python, the general-purpose programming language, is probably so prominent.The results between R &amp; PythonIn a direct comparison between R and Python, you can see that many R users also use Python. Whereas Python users often work exclusively with Python.Comparing the use of languages by field reveals a clear dominance of Python. In all fields, except for statisticians, the majority uses Python.Participants were also asked: What language do you recommend for beginners to learn first? The answers to the question are shown in the table below.Sprache   EmpfehlungNutzerDifferenzPython14.1818.1806.001R2.3422.046296SQL9141.211-297C++339739-400Matlab256355-99Java184903-719Scala74106-32Javascript72408-336SAS69228-159VBA38135-97Go2646-20Other16111744When the number of recommendations and the number of users are compared, you can see that R and Python are the only languages that have a positive difference.Again, Python (14.181) is well ahead of R (2.342).ConclusionBeforehand: both languages are very powerful. Therefore you can not make a wrong choice! The choice of language depends on what kind of project you want to tackle.As a universal programming language, Python is suitable for a variety of applications. Which is why I generally recommend starting with Python. But if statistical analysis or data visualization is paramount in your projects, R has an advantage over Python.As already mentioned, both languages have their advantages and disadvantages. As an advanced data scientist, you should ideally master both languages.I hope this post gives you an idea of what the differences are between R and Python and helps you make the right choice for yourself. Since I could not go into much depth with arguments for my preferences in this blog post – you are very welcome to shoot me an e-mail, if you have any further questions regarding the topic.Happy Coding!If you’re interested in training, feel free to check out our course Catalogs for R and Python at STATWORX Academy.Referenceshttps://cran.r-project.org/https://www.python.org/https://pipenv.readthedocs.io/en/latest/https://conda.io/en/latest/https://github.com/scikit-learn/scikit-learn/issues/6738https://www.rstudio.com/https://www.springer.com/us/book/9780387245447http://wiki.c2.com/?PythonPhilosophyhttps://www.kaggle.comhttps://www.statworx.com/de/academy/";https://www.statworx.com/de/blog/r-or-python/;Statworx;  Fran Peric
  28. August 2019;Machine Learning Interpretability @DataUniversity2019;"In diesem Jahr findet zum ersten mal die Data University an der Goethe-Uni in Frankfurt statt. Gemeinsam mit BARC präsentieren wir von STATWORX zwei Tage lang verschiedene Themen rund um die Themen Data Science, Data Engineering und Data Strategy.Im Data Science Track findet sich neben Advanced Machine Learning und Automated Machine Learning auch eine besonders spannende Session zum aktuell viel diskutierten Thema Machine Learning Interpretability. Erklärbarkeit für Black Box ModelleUngeachtet ihrer weit verbreiteten Anwendung werden Machine Learning Modelle nach wie vor als Black Box gesehen. Es wird häufig argumentiert, dass Machine Learning Modelle zwar eine sehr gute prädiktive Güte aufweisen, die Parameter und Einflussfaktoren jedoch nicht interpretierbar seien. Aus dieser Kritik heraus, wurde das Forschungsgebiet der Machine Learning Interpretability gegründet. Dort wird versucht, Methoden und Ansätze zu entwickeln, die die Erklärbarkeit von Machine Learning Modellen steigern. Insbesondere in den letzten Jahren, wurden in diesem Bereich deutliche Fortschritte geschaffen, wodurch die Behauptung, Machine Learning Modelle seien reine Black Box Ansätze, heute so nur noch bedingt zutrifft.Erklärbarkeit am Beispiel von PreisprognosenEin kleines Beispiel gefällig? Im folgenden Absatz findet sich der Python Code zur Erstellung eines Prognosemodells für den Kaufpreis von Häusern anhand historischer Daten im Großraum Boston, MA. Die vorliegenden Zeilen Code reichen aus, um ein Machine Learning Modell zu bauen, dass bereits eine Genauigkeit von etwa 85% erreichen kann. import pandas as pdfrom xgboost import XGBRegressor# Load the datadf = pd.read_csv('data_houses.csv')# Column names of target and featurestarget = 'price'features = 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors',           'waterfront', 'view', 'condition', 'grade', 'sqft_above',           'sqft_basement', 'yr_built', 'yr_renovated',           'lat', 'long', 'sqft_living15', 'sqft_lot15'# Subset columnsy = dftargetX = dffeatures# Train/test splitX_train, X_test, y_train, y_test = train_test_split(X, y)# Train XGBoost model mod = XGBRegressor()mod.fit(X_train, y_train)p = mod.predict(X_test)# Evaluate modelmean_absolute_error(y_test, p)Das verwendete Modell verwendet zur Schätzung der Preise ein Ensemble aus 100 Decision Trees (Gradient Boosting). Eine native Interpretation dieses Modells ist daher in der Tat kaum möglich. In den letzten Jahren wurden jedoch neue methodische Ansätze entwickelt, die von sich aus nicht-interpretierbare Modelle interpretierbar machen. Ansätze dieser Art werden allgemein als model-agnostische post hoc Ansätze bezeichnet. Einer dieser Ansätze ist der sogenannte Partial Dependency Plot. Dieser erlaubt auch für hochgradig nicht-lineare Modelle detaillierte Aussagen über den Einfluss einzelner Features auf das Target. Für das Modell aus dem vorherigen Beispiel kann somit gezeigt werden, welchen Einfluss der Breitengrad eines Hauses auf den vorhergesagten Preis hat. Model-agnostische post hoc Ansätze sind aber nicht nur global auf Modelle anwendbar, sondern können auch interessante Erkenntnisse über einzelne Observationen erzeugen. Im unteren Beispiel wurden sogenannte Shapley Values dazu genutzt, die finale Prognose von $387,092 in einzelne Bestandteile zu zerlegen. Deutlich zu sehen ist dabei, welchen Anteil die einzelnen Einflussfaktoren (z.B. auch wieder der Breitengrad) zur finalen Prognose beitragen.Join us @DataUniversity2019 !Partial Dependency Plots und Shapley Values sind nur zwei mögliche Ansätze um Machine Learning Modelle interpretierbar zu machen. Im Workshop „Machine Learning Interpretability“ der Data University werden wir diese und weitere Ansätze im Detail vorstellen, diskutieren und anhand realer Beispiele in R und Python anwenden. Du interessierst dich für die Data University und möchtest mehr über das Workshop-Event wissen? In diesem Video erzählen dir zwei unserer Trainer, Sebastian Heinz (Gründer und CEO von STATWORX) &amp; Dr. Sebastian Derwisch (Data Scientist bei BARC ), was wir bei der Data University alles vorhaben und welche die Vorteile einer Teilnahme sind. Viel Spaß beim Reinschauen!Informiere dich außerdem auf der Homepage der Data University zu allen Workshops, Trainern und Terminen. Erweitere Deinen Blickwinkel auf die Themen Data Strategy, Data Engineering, Data Science und Data Technology. Wir freuen uns auf dich!Über den AutorFabian MüllerI am the COO at STATWORX and responsible for our data science teams and key accounts. In my spare time, I'm into doing sports and fast cars..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/machine-learning-interpretability/;Statworx;  Fabian Müller
  21. August 2019;Which Factors Influence Gas Prices? Do Gas Companies Narratives Hold True?;"Like the data hunter he is, my STATWORX colleague Jakob came across a rich data source regarding gas station prices. While his focus has been on checking very common myths about gas prices (check out his blogpost!), he did a fantastic job at cleaning and preparing the raw data to get it in a usable format in the first place. Obviously, I wanted to check whether one can make use of this data and further freely accessible data sources to check gas companies narratives. Let me guide you through my work process!Opening the treasure chest of statisticsSince the raw database is remarkably huge (almost 1.5 billion rows), I first decided to aggregate the data into groups, in this case by gas stations. As a result, I’ve got one average gas price per day for each combination of fuel sorts: e5, e10, dieselbrand: nine most important brands and all othersmotorway: are the stations on a motorway yes/no2-digits postal code: regional information where the gas stations are.This aggregation makes our data frame much handier with approximately 6 million rows left. This is still a lot of information yet allows for actually run models on my local machine. Treating each group as an individual, for which I have multiple observations over time, is basically like a panel data problem. To find the correct model specifications, I performed multiple tests (Chow, Lagrange multiplier, and a Hausman test). Eventually, the choice fell onto two models:a general linear panel model with two-way fixed effects that allows detecting overall patternsa variable coefficients panel model with individual fixed effects.The latter model estimates separate coefficients for each group. This allows detecting varying impacts of factors on different regions or gas station types. In both models, each observation is weighted by its group size.Let’s bring some covariates into playAccording to the Federal Ministry of Finance, the gas prices we have to pay at the gas station are composed of three parts: (I) product purchase price, (II) costs and profit of the oil companies, (III) and taxes. The taxes, namely energy tax and value-added tax (Mehrwertsteuer), remained unchanged during our investigation period. Thus they should merely emphasize yet not influence gas price factors. The oil price, however, is often blamed for high gas prices. To control its impact, I’ve included daily raw oil prices to my model. Another potential player here is the EUR/USD exchange rate. Like all international trading goods, oil is traded in USD. Differences in the exchange rate might yield changing gas prices regardless of a stable raw oil price. So far, all factors are beyond the control of gas companies. Let’s look at a topic that got in the public focus in 2018: transportation costs. Due to the hot and dry summer in Germany, some rivers had all-time lows in their levels. That did threaten not only freshwater fish but also the usage of the streams as transportation infrastructure. Especially regions close to Rhine and Elbe receive large shares of their gas supply via these inland waterways. Both very low and very high river levels have a negative impact on inland waterway transport. Low levels force barges to reduce their cargo until transportation has to stop completely. The same is true for very high levels. There is a legal limit above which shipping has to cease. Consequent supply shortages and higher relative transportation costs have a negative impact on our gas prices eventually… or this has been the narrative of gas companies at least. I will check whether this explanation holds true considering the data or if it is instead an excuse to raise prices or keep them on a higher level afterward. Let’s turn to subjects which might actually be opportunities for gas companies to rip off drivers. One common nuisance are price jumps right at the beginning of school or public holidays. You want to fill up the tank before you drive to the sea – and bam – gas prices reach new highs. The same logic applies to weekdays. As Jakob has shown, it’s cheapest in the middle of the week, namely on Wednesdays and Thursdays, but more expensive on Mondays and Fridays when most commuters refuel. Finally, gas prices might be higher where people are wealthier and therefore less price sensitive. A convenient measurement for this is the purchasing power index (PPI).Statistics meets gut feeling – are they on the same page?To retrieve comparable coefficient estimates, I scaled all continuous independent variables beforehand. The results of the panel model are presented in the next figure. The scaling allows interpreting each variable impact in strength as well as in direction. Confidence intervals are not shown since all estimates are extremely significant, due to a large number of cases.Unsurprisingly, the raw oil price is the most crucial factor regarding gas prices. Together with the Euro-Dollar exchange rate, these two factors account for most of the variation in gas prices. This relationship can even be seen by a quick look at both normalized time series patterns. Now let us proceed to the more interesting findings. River levels actually do have a „positive“ impact. Meaning that when inland waterway transportation is hampered, gas prices increase. While this is generally true on the national level, I had a deeper look into more regional details. An interaction between river impacts and the one digit zip code regions (check out the map) reveals that this factor is a price driver mainly in the southern regions of Germany. This seems logical, considering two thoughts. Most gas and oil is transported on the sea and reaches Germany mainly in the north. Therefore, higher transportation costs add up more the longer the transportation route is. And even though I only included levels of the most important inland waterways Rhine and Elbe, they can be seen as sort of a proxy for general river level conditions in Germany. This explains why non-proximal regions are also affected by distant rivers levels. Turning back to the overall coefficients, the PPI surprisingly has a negative impact. This means where people are wealthier on average, gas prices are lower. The effect of public and school holidays are likewise unexpected. While gas prices are (negligibly) higher during the holidays, they are slightly lower at the beginning and end of the holidays. This is entirely contrary to the gut feeling of most vacationers, who regularly complain about higher prices right at these times. Finally, weekday’s impacts confirm the previous findings from Jakob’s blog. While Thursday is the cheapest day of the week, it’s most expensive on Sundays (reference category and thus not shown in the first coefficient plot above; all weekday impacts are in comparison to Sunday).TLDR;Gas prices mainly rely on raw oil prices in Euro (so far, so obvious).Extreme river levels do impact gas prices, yet rather in regions that are far off the sea.Gas prices are not linked to the relative wealth (PPI) of potential customers.Holiday traffic is not used to rip off holidaymakers.Its cheapest in the middle of the week and more expensive close or on the weekend.ReferencesGas prices: Data is used under the Creative-Commons-Lizenz (CC BY 4.0). For more information on the data check out the tankerkoenig website. Raw oil prices: Federal Reserve Bank of St. Louis EUR – USD exchange rate:  Federal Reserve Bank of St. Louis River levels: Wasserstraßen- und Schifffahrtsverwaltung des Bundes (WSV), bereitgestellt durch die Bundesanstalt für Gewässerkunde (BfG) School holidays: schulferien.org Public holidays: feiertage-api.de Zip code: excel-karte.de PPI (up until 2016): Statistische Ämter des Bundes und der Länder, Deutschland, 2019 PPI (2017 &amp; 2018): MB-ResearchÜber den AutorMatthias NistlerI am a data scientist at STATWORX and passionate for wrangling data and getting the most out of it. Outside of the office, I use every second for cycling until the sun goes down..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/which-factors-influence-gas-prices-do-gas-companies-narratives-hold-true/;Statworx;  Matthias Nistler
  16. August 2019;What the MAPE is FALSELY blamed for;"A major problem arises when comparing forecasting methods and models across different time series. This is a challenge we regularly face at STATWORX. Unit dependent measures like the MAE (Mean Absolute Error) and the RMSE (Root Mean Squared Error) turn out to be unsuitable and hardly helpful if the time series is measured in different units. However, if this is not the case, both measures provide valuable information. The MAE is perfectly interpretable as it embodies the average absolute deviation from the actual values. The RMSE, on the other hand, is not that easy to interpret, more vulnerable to extreme values but still often used in practice.One of the most commonly used measures that avoids this problem is called MAPE (Mean Absolute Percentage Error). It solves the problem of the mentioned approaches as it does not depend on the unit of the time series. Furthermore, decision-makers without a statistical background can easily interpret and understand this measure. Despite its popularity, the MAPE was and is still criticized. In this article, I evaluate these critical arguments and prove that at least some of them are highly questionable. The second part of my article concentrates on true weaknesses of the MAPE, some of them well-known but others hiding in the shadows. In the third section, I discuss various alternatives and summarize under which circumstances the use of the MAPE seems to be appropriate (and when it’s not).What the MAPE is FALSELY blamed for!It puts heavier penalties on negative errors than on positive errors.Most sources dealing with the MAPE point out this „major“ issue of the measure. The statement is primarily based on two different arguments. First, they claim that interchanging the actual value with the forecasted value proofs their point (Makridakis 1993).        Case 1:   = 150 &amp;  = 100 (positive error)        Case 2:  = 100 &amp;  = 150  (negative error) It is true that Case 1 (positive error of 50) is related to a lower APE (Absolute Percentage Error) than Case 2 (negative error of 50). However, the reason here is not that the error is positive or negative but simply that the actual value changes. If the actual value stays constant, the APE is equal for both types of errors (Goodwin &amp; Lawton 1999). That is clarified by the following example.    Case 3:  = 100 &amp;  = 50       Case 4: = 100 &amp;  = 150   The second, equally invalid argument supporting the asymmetry of the MAPE arises from the assumption about the predicted data. As the MAPE is mainly suited to be used to evaluate predictions on a ratio scale, the MAPE is bounded on the lower side by an error of 100% (Armstrong &amp; Collopy 1992). However, this does not imply that the MAPE overweights or underweights some types of errors, but that these errors are not possible.Its TRUE weaknesses!It fails if some of the actual values are equal to zero.This statement is a well-known problem of the focal measure. However, that and the latter argument were the reason for the development of a modified form of the MAPE, the SMAPE („Symmetric“ Mean Absolute Percentage). Ironically, in contrast to the original MAPE, this modified form suffers from true asymmetry (Goodwin &amp; Lawton 1999). I will clarify this argument in the last section of the article. Particular small actual values bias the MAPE.If any true values are very close to zero, the corresponding absolute percentage errors will be extremely high and therefore bias the informativity of the MAPE (Hyndman &amp; Koehler 2006). The following graph clarifies this point. Although all three forecasts have the same absolute errors, the MAPE of the time series with only one extremely small value is approximately twice as high as the MAPE of the other forecasts. This issue implies that the MAPE should be used carefully if there are extremely small observations and directly motivates the last and often ignored the weakness of the MAPE.The MAPE implies only which forecast is proportionally better.As mentioned at the beginning of this article, one advantage of using the MAPE for comparison between forecasts of different time series is its unit independency. However, it is essential to keep in mind that the MAPE only implies which forecast is proportionally better. The following graph shows three different time series and their corresponding forecasts. The only difference between them is their general level. The same absolute errors lead, therefore, to profoundly different MAPEs. This article critically questions, if it is reasonable to use such a percentage-based measure for the comparison between forecasts for different time series. If the different time series aren’t behaving in a somehow comparable level (as shown in the following graphic), using the MAPE to infer if a forecast is generally better for one time series than for another relies on the assumption that the same absolute errors are less problematic for time series on higher levels than for time series on lower levels: „If a time series fluctuates around 100, then predicting 101 is way better than predicting 2 for a time series fluctuating around 1.“That might be true in some cases. However, in general, this a questionable or at least an assumption people should always be aware of when using the MAPE to compare forecasts between different time series.Summary.In summary, the discussed findings show that the MAPE should be used with caution as an instrument for comparing forecasts across different time series. A necessary condition is that the time series only contains strictly positive values. Second only some extremely small values have the potential to bias the MAPE heavily. Last, the MAPE depends systematically on the level of the time series as it is a percentage based error. This article critically questions if it is meaningful to generalize from being a proportionally better forecast to being a generally better forecast. BETTER alternatives!The discussed implies that the MAPE alone is often not very useful when the objective is to compare accuracy between different forecasts for different time series. Although relying only on one easily understandable measure appears to be comfortable, it comes with a high risk of drawing misleading conclusions. In general, it is always recommended to use different measures combined. In addition to numerical measures, a visualization of the time series, including the actual and the forecasted values always provides valuable information. However, if one single numeric measure is the only option, there are some excellent alternatives. Scaled Measures.Scaled measures compare the measure of a forecast, for example, the MAE relative to the MAE of a benchmark method. Similar measures can be defined using RMSE, MAPE, or other measures. Common benchmark methods are the „random walk“, the „naïve“ method and the „mean“ method. These measures are easy to interpret as they show how the focal model compares to the benchmark methods. However, it is important to keep in mind that relative measures rely on the selection of the benchmark method and on how good the time series can be forecasted by the selected method.Scaled Errors.Scaled errors approaches also try to remove the scale of the data by comparing the forecasted values to those obtained by some benchmark forecast method, like the naïve method. The MASE (Mean Absolute Scaled Error), proposed by Hydnmann &amp; Koehler 2006, is defined slightly different dependent on the seasonality of the time series. In the simple case of a non-seasonal time series, the error of the focal forecast is scaled based on the in-sample MAE from the naïve forecast method. One major advantage is that it can handle actual values of zero and that it is not biased by very extreme values. Once again, it is important to keep in mind that relative measures rely on the selection of the benchmark method and on how good the time series can be forecasted by the selected method. Non-Seasonal SeasonalSDMAE.In my understanding, the basic idea of using the MAPE to compare different time series between forecasts is that the same absolute error is assumed to be less problematic for time series on higher levels than for time series on lower levels. Based on the examples shown earlier, I think that this idea is at least questionable. I argue that how good or bad a specific absolute error is evaluated should not depend on the general level of the time series but on its variation. Accordingly, the following measure the SDMAE (Standard Deviation adjusted Mean Absolute Error) is a product of the discussed issues and my imagination. It can be used for evaluating forecasts for times series containing negative values and does not suffer from actual values being equal to zero nor particularly small. Note that this measure is not defined for time series that do not fluctuate at all. Furthermore, there might be other limitations of this measure, that I am currently not aware of.Summary.I suggest using a combination of different measures to get a comprehensive understanding of the performance of the different forecasts. I also suggest complementing the MAPE with a visualization of the time series, including the actual and the forecasted values, the MAE, and a Scaled Measure or Scaled Error approach. The SDMAE should be seen as an alternative approach that was not discussed by a broader audience so far. I am thankful for your critical thoughts and comments on this idea.Worse alternatives!SMAPE.The SMAPE was created, to solve and respond to the problems of the MAPE. However, this did neither solve the problem of extreme small actual values nor the level dependency of the MAPE. The reason is that extreme small actual values are typically related to extreme small predictions (Hyndman &amp; Koehler 2006). Additional, and in contrast to the unmodified MAPE, the SMAPE raises the problem of asymmetry (Goodwin &amp; Lawton 1999). This is clarified through the following graphic, whereas the “ APE“ relates to the MAPE and the „SAPE“ relates to the SMAPE. It shows that the SAPE is higher for positive errors than for negative errors and therefore, asymmetric. The SMAPE is not recommended to be used by several scientists (Hyndman &amp; Koehler 2006). On the asymmetry of the symmetric MAPE (Goodwin &amp; Lawton 1999)ReferencesGoodwin, P., &amp; Lawton, R. (1999). On the asymmetry of the symmetric MAPE. International journal of forecasting, 15(4), 405-408.Hyndman, R. J., &amp; Koehler, A. B. (2006). Another look at measures of forecast accuracy. International journal of forecasting, 22(4), 679-688.Makridakis, S. (1993). Accuracy measures: theoretical and practical concerns. International Journal of Forecasting, 9(4), 527-529.Armstrong, J. S., &amp; Collopy, F. (1992). Error measures for generalizing about forecasting methods: Empirical comparisons. International journal of forecasting, 8(1), 69-80.Über den AutorJan FischerI am a data scientist at STATWORX. I always enjoyed to think critically about complex problems, understand and find a solution. Fortunately, STATWORX pays me for that!.e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/what-the-mape-is-falsely-blamed-for-its-true-weaknesses-and-better-alternatives/;Statworx;  Jan Fischer
  31. Juli 2019;Web Scraping 101 in Python with Requests und BeautifulSoup;"  .gist {width: 750px !important;}  .gist-file  .gist-data {max-height: 500px;max-width: 750px;}IntroInformation is everywhere online. Unfortunately, some of it is hard to access programmatically. While many websites offer an API, they are often expensive or have very strict rate limits, even if you’re working on an open-source and/or non-commercial project or product.That’s where web scraping can come into play. Wikipedia defines web scraping as follows:Web scraping,web harvesting, orweb data extraction data scraping used for extracting data from websites. Web scraping software may access the World Wide Web directly using the Hypertext Transfer ProtocolHTTP, or through a web browser.“Web scraping” wikipedia.orgIn practice, web scraping encompasses any method allowing a programmer to access the content of a website programmatically, and thus, (semi-) automatically.Here are three approaches (i.e. Python libraries) for web scraping which are among the most popular:Sending an HTTP request, ordinarily via Requests, to a webpage and then parsing the HTML (ordinarily using BeautifulSoup) which is returned to access the desired information. Typical Use Case: Standard web scraping problem, refer to the case study.Using tools ordinarily used for automated software testing, primarily Selenium, to access a websites‘ content programmatically. Typical Use Case: Websites which use Javascript or are otherwise not directly accessible through HTML.Scrapy, which can be thought of as more of a general web scraping framework, which can be used to build spiders and scrape data from various websites whilst minimizing repetition. Typical Use Case: Scraping Amazon Reviews.While you could scrape data using any other programming language as well, Python is commonly used due to its ease of syntax as well as the large variety of libraries available for scraping purposes in Python. After this short intro, this post will move on to some web scraping ethics, followed by some general information on the libraries which will be used in this post. Lastly, everything we have learned so far will be applied to a case study in which we will acquire the data of all companies in the portfolio of Sequoia Capital, one of the most well-known VC firms in the US. After checking their website and their robots.txt, scraping Sequoia’s portfolio seems to be allowed; refer to the section on robots.txt and the case study for details on how I went about determining this.In the scope of this blog post, we will only be able to have a look at one of the three methods above. Since the standard combination of Requests + BeautifulSoup is generally the most flexible and easiest to pick up, we will give it a go in this post. Note that the tools above are not mutually exclusive; you might, for example, get some HTML text with Scrapy or Selenium and then parse it with BeautifulSoup.Web Scraping Ethics One factor that is extremely relevant when conducting web scraping is ethics and legality. I’m not a lawyer, and specific laws tend to vary considerably by geography anyway, but in general web scraping tends to fall into a grey area, meaning it is usually not strictly prohibited, but also not generally legal (i.e. not legal under all circumstances). It tends to depend on the specific data you are scraping.In general, websites may ban your IP address anytime you are scraping something they don’t want you to scrape. We here at STATWORX don’t condone any illegal activity and encourage you to always check explicitly when you’re not sure if it’s okay to scrape something. For that, the following section will come in handy. Understanding robots.txtThe robot exclusion standard is a protocol which is read explicitly by web crawlers (such as the ones used by big search engines, i.e. mostly Google) and tells them which parts of a website may be indexed by the crawler and which may not. In general, crawlers or scrapers aren’t forced to follow the limitations set forth in a robots.txt, but it would be highly unethical (and potentially illegal) to not do so.The following shows an example robots.txt file taken from Hackernews, a social newsfeed run by YCombinator which is popular with many people in startups.The Hackernews robots.txt specifies that all user agents (thus the * wildcard) may access all URLs, except the URLs that are explicitly disallowed. Because only certain URLs are disallowed, this implicitly allows everything else. An alternative would be to exclude everything and then explicitly specify only certain URLs which can be accessed by crawlers or other bots.Also, notice the crawl delay of 30 seconds which means that each bot should only send one request every 30 seconds. It is good practice, in general, to let your crawler or scraper sleep in regular (rather large) intervals since too many requests can bring down sites down, even when they come from human users. When looking at the robots.txt of Hackernews, it is also quite logical why they disallowed some specific URLs: They don’t want bots to pose as users by for example submitting threads, voting or replying. Anything else (e.g. scraping threads and their contents) is fair game, as long as you respect the crawl delay. This makes sense when you consider the mission of Hackernews, which is mostly to disseminate information. Incidentally, they also offer an API that is quite easy to use, so if you really needed information from HN, you would just use their API.Refer to the Gist below for the robots.txt of Google, which is (obviously) much more restrictive than that of Hackernews. Check it out for yourself, since it is much longer than shown below, but essentially, no bots are allowed to perform a search on Google, specified on the first two lines. Only certain parts of a search are allowed, such as „about“ and „static“. If there is a general URL which is disallowed, it is overwritten if a more specific URL is allowed (e.g. the disallowing of /search is overridden by the more specific allowing of /search/about).Moving on, we will take a look at the specific Python packages which will be used in the scope of this case study, namely Requests and BeautifulSoup.RequestsRequests is a Python library used to easily make HTTP requests. Generally, Requests has two main use cases, making requests to an API and getting raw HTML content from websites (i.e., scraping).Whenever you send any type of request, you should always check the status code (especially when scraping), to make sure your request was served successfully. You can find a useful overview of status codes here. Ideally, you want your status code to be 200 (meaning your request was successful). The status code can also tell you why your request was not served, for example, that you sent too many requests (status code 429) or the infamous not found (status code 404).Use Case 1: API RequestsThe Gist above shows a basic API request directed to the NYT API. If you want to replicate this request on your own machine, you have to first create an account at the NYT Dev page and then assign the key you receive to the KEY constant. The data you receive from a REST API will be in JSON format, which is represented in Python as a dict data structure. Therefore, you will still have to „parse“ this data a bit before you actually have it in a table format which can be represented in e.g. a CSV file, i.e. you have to select which data is relevant for you.Use Case 2: Scraping The following lines request the HTML of Wikipedia’s page on web scraping. The status code attribute of the Response object contains the status code related to the request.After executing these lines, you still only have the raw HTML with all tags included. This is usually not very useful, since most of the time when scraping with Requests, we are looking for specific information and text only, as human readers are not interested in HTML tags or other markups. This is where BeautifulSoup comes in.BeautifulSoupBeautifulSoup is a Python library used for parsing documents (i.e. mostly HTML or XML files). Using Requests to obtain the HTML of a page and then parsing whichever information you are looking for with BeautifulSoup from the raw HTML is the quasi-standard web scraping „stack“ commonly used by Python programmers for easy-ish tasks. Going back to the Gist above, parsing the raw HTML returned by Wikipedia for the web scraping site would look similar to the below. In this case, BeautifulSoup extracts all headlines, i.e. all headlines in the Contents section at the top of the page. Try it out for yourself!As you can see below, you can easily find the class attribute of an HTML element using the inspector of any web browser. Figure 1: Finding HTML elements on Wikipedia using the Chrome inspector. This kind of matching is (in my opinion), one of the easiest ways to use BeautifulSoup: You simply specify the HTML tag (in this case, span) and another attribute of the content which you want to find (in this case, this other attribute is class). This allows you to match arbitrary sections of almost any webpage. For more complicated matches, you can also use regular expressions (REGEX). Once you have the elements, from which you would like to extract the text, you can access the text by scraping their text attribute.Inspector As a short interlude, it’s important to give a brief introduction to the Dev tools in Chrome (they are available in any browser, I just chose to use Chrome), which allows you to use the inspector, that gives you access to a websites HTML and also lets you copy attributes such as the XPath and CSS selector. All of these can be helpful or even necessary in the scraping process (especially when using Selenium). The workflow in the case study should give you a basic idea of how to work with the Inspector. For more detailed information on the Inspector, the official Google website linked above contains plenty of information.Figure 2 shows the basic interface of the Inspector in Chrome. Figure 2: Chrome Inspector on Wikipedia.Sequoia Capital Case StudyI actually first wanted to do this case study with the New York Times, since they have an API and thus the results received from the API could have been compared to the results from scraping. Unfortunately, most news organizations have very restrictive robots.txt, specifically not permitting searching for articles. Therefore, I decided to scrape the portfolio of one of the big VC firms in the US, Sequoia, since their robots.txt is permissive and I also think that startups and the venture capital scene are very interesting in general.Robots.txtFirst, let’s have a look at Sequoia’s robots.txt:Luckily, they permit access of various kinds – except three URLs, which is fine for our purposes. We will still build in a crawl delay of 15-30 seconds between each request.Next, let’s scope out the actual data which we are trying to scrape. We are interested in the portfolio of Sequoia, so https://www.sequoiacap.com/companies/ is the URL we are after. Figure 3: Sequoia’s portfolio.The companies are nicely laid out in a grid, making them pretty easy to scrape. Upon clicking, the page shows the details of each company. Also, notice how the URL changes in Figure 4 when clicking on a company! This is important for Requests especially. Figure 4: Detail page on one of Sequoia’s portfolio companies.Let’s aim for collecting the following basic information on each company and outputting them as a CSV file:Name of the companyURL of the companyDescriptionMilestonesTeamPartnerIf any of this information is not available for a company, we will simply append the string „NA“ instead.Time to start inspecting!Scraping ProcessCompany NameUpon inspecting the grid it looks like the information on each company is contained within a div tag with the class companies _company js-company. Thus we should just be able to look for this combination with BeautifulSoup.Figure 5: Grid structure of each company.This still leaves us with all the other information missing though, meaning we have to somehow access the detail page of each company. Notice how in Figure 5 above, each company div has an attribute called data-url. For 100 Thieves, for example, onclick has the value /companies/100-thieves/. That’s just what we need! Now, all we have to do is to append this data-URL attribute for each company to the base URL (which is just https://www.sequoiacap.com/) and now we can send yet another request to access the detail page of each company.So let’s write some code to first get the company name and then send another request to the detail page of each company: I will write code interspersed with text here. For a full script, check my Github.First of all, we take care of all the imports and set up any variables we might need. We also send our first request to the base URL which contains the grid with all companies and instantiates a BeautifulSoup parser.After we have taken care of basic bookkeeping and setup the dictionary in which we want to scrape the data, we can start working on the actual scraping, first parsing the class shown in Figure 5. As you can see in Figure 5, once we have selected the div tag with the matching class, we have to go to its first div child and then select its text, which then contains the name of the company.On the detail page, we have basically everything we wanted. Since we already have the name of the company, all we still need are URL, description, milestones, team and the respective partner from Sequoia.Company URLFor the URL, we should just be able to find elements by their class and then select the first element, since it seems like the website is always the first social link. You can see the inspector view in Figure 6. Figure 6: The first social link ordinarily contains the company URL. But wait – what if there are no social links or the company website is not provided? In the case that the website is not provided, but a social media page is, we will simply consider this social media link the company’s de facto website. If there are no social links provided at all, we have to append an NA. This is why we check explicitly for the number of objects found because we cannot access the href attribute of a tag that doesn’t exist. An example of a company without a URL is shown in Figure 7. Figure 7: Company without social links.Company descriptionAs you can see in Figure 8, the p tag containing the company description does not have any additional identifiers, therefore we are forced to first access the div tag above it and then go down to the p tag containing the description and selecting its text attribute.Figure 8: Company description.Milestones, Team &amp; Partner(s)For the last three elements, they are all located in the same structure and can thus be accessed in the same manner. We will simply match the text of their parent element and then work our way down from there.Since the specific text elements do not have good identifying characteristics, we match the text of their parent element. Once we have the text, we go up two levels, by using the parent attribute. This brings us to the div tag belonging to this specific category (e.g. Milestones or Team). Now all that is left to do is go down to the ul tag containing the actual text we are interested in and getting its text. Figure 9: Combining a text match with the parent attributes allows the acquisition of text without proper identifying characteristics.One issue with using text match is the fact that only exact matches are found. This matters in cases where the string you are looking for might differ slightly between pages. As you can see in Figure 10, this applies to our case study here for the text Partner. If a company has more than one Sequoia partner assigned to it, the string is „Partners“ instead of „Partner“. Therefore we use a REGEX when searching for the partner element to get around this. Figure 10: Exact string matching can lead to problems if there are minor differences in between HTML pages.Last but not least, it is not guaranteed that all the elements we want (i.e. Milestones, Team, and Partner) are in fact available for each company. Thus before actually selecting the element, we first find all elements matching the string and then check the length. If there are no matching elements, we append NA, otherwise, we get the requisite information.For a partner, there is always one element, thus we assume no partner information is available if there are one or fewer elements. I believe the reason that one element matching partner always shows up is the „Filter by Partner“ option shown in Figure 11. As you can see, scraping often requires some iterating to find some potential issues with your script.  Figure 11: Filter by partner option.Writing to diskTo wrap up, we append all the information corresponding to a company to the list it belongs to within our dictionary. Then we convert this dictionary into a pandas DataFrame before writing it to disk.Success! We just collected all of Seqouia’s portfolio companies and information on them.* Well, all on their website at least, I believe they have their own respective sites for e.g. India and Israel.Let’s have a look at the data we just scraped:Looks good! We collected 506 companies in total and the data quality looks really good as well. The only thing I noticed is that some companies have a social link but it does not actually go anywhere. Refer to Figure 12 for an example of this with Pixelworks. The issue is that Pixelworks has a social link but this social link does not actually contain a URL (the href target is blank) and simply links back to the Sequoia portfolio. Figure 12: Company with a social link but without a URL.I have added code in the script to replace the blanks with NAs but have left the data as is above to illustrate this point.ConclusionWith this blog post, I wanted to give you a decent introduction to web scraping in general and specifically using Requests &amp; BeautifulSoup. Now go use it in the wild by scraping some data that can be of use to you or someone you know! But always make sure to read and respect both the robots.txt and the terms and conditions of whichever page you are scraping.Furthermore, you can check out resources and tutorials on some of the other methods shown above on their official websites, such as Scrapy and Selenium. You should also challenge yourself by scraping some more dynamic sites which you can not scrape using only Requests.If you have any questions, found a bug or just feel like chatting about all things Python and scraping, feel free to contact me at david.wissel@statworx.com.Über den AutorDavid WisselI am a data scientist Intern at STATWORX and enjoy all things Python, Statistics and Computer Science. During my time off, I like running (semi-) long distance races..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/web-scraping-101-in-python-with-requests-beautifulsoup/;Statworx;  David Wissel
  24. Juli 2019;Shiny Apps skalieren @DataUniversity2019;"Vom 09.-10. Oktober präsentieren wir von STATWORX gemeinsam mit BARC die Data University an der Goethe-Uni in Frankfurt. 2 Tage lang werden wir dort unser geballtes Data Science Wissen in praxisorientierten Workshops an die Teilnehmenden weitergeben.In zwei besonders spannenden Workshops von unserem Kollegen David dreht sich am zweiten Schulungstag alles um das R Paket Shiny. Vormittags wird es eine „Einführung in Shiny Apps“ geben. Nachmittags geht es in einer zweiten Session mit dem etwas komplexeren Thema „Shiny Apps skalieren“ weiter.Eine kleine, einfache Shiny App zu bauen, ist recht leicht:require(shiny)require(argonDash)?# UI ui &lt;- argonDashPage(?   # Header -----------------------------------------------------------------  header = argonDashHeader(tags$img(src = ""https://www.data-university.de/wp-content/uploads/2019/06/logo-data-university-weiss-small.png""),               background_img = ""https://www.data-university.de/wp-content/uploads/2019/04/data-university-header-1.jpg"",               separator = TRUE,               height = 400,               mask = TRUE,               opacity = 1),      # Body --------------------------------------------------------------------  body = argonDashBody(          fluidPage(            tags$style(HTML(""a{color: #124D71; font-size: 150%;}               a:hover{color: #69D0D0;}               h1{color: #69D0D0; font-size: 250%;}"")),            fluidRow(column(width = 8,              tagList(                br(),                br(),                htmlOutput(""hi"")               )            ),                      column(width = 3,              align = ""right"",              br(),                             textInput(""text"",                    label ="""",                    value = ""Wie heißt du?""),?              actionButton(""goButton"", ""OK!""))           )       )       ))?# Simple server logicserver &lt;- function(input, output) {    observeEvent(input$goButton, {        your_name &lt;- ifelse(isolate(input$text) == ""Wie heißt du?"",               """",               paste("" "", isolate(input$text)))      output$hi &lt;- renderText({ paste0(""&lt;h1&gt; Hi"",                     your_name,                    "", werde Teil der Data University! &lt;/h1&gt; &lt;/br&gt;                    &lt;a href='https://www.data-university.de/tickets/'&gt; Aber klar doch - klick !-&lt;/a&gt;"")             })     })}?# Run the appshinyApp(ui = ui, server = server)Wie du siehst, kannst du mit Shiny in wenigen Code-Zeilen bereits ein schönes Frontend bauen. Wenn du diesen Code ausführst, sieht das Ergebnis folgendermaßen aus:Schwieriger (und mitunter kostspielig!) wird es erst, wenn eine fertige App effizient und zuverlässig einem breiteren Publikum zugänglich gemacht werden soll. Doch um das ganze Potenzial von Shiny auszuschöpfen, ist ein Deployment für eine größere Zahl von Usern, also die Skalierung einer App, unerlässlich. Warum Shiny?App-Entwicklung in RShiny ermöglicht es, interaktive Web-Applikationen direkt mit R zu entwickeln. Der umfangreiche Funktionsumfang von R kann so ganz leicht mit einer interaktiven Benutzeroberfläche verbunden werden. Beispielsweise können Data Scientists in R, mit Hilfe von Shiny, Modelle direkt in interaktive Nutzeranwendungen einbinden, ganz ohne die Hilfe von Web Developern oder die Verwendung proprietärer BI-Tools. Aber die mit Shiny umsetzbaren Funktionalitäten gehen weit über bloßes Dashboarding hinaus. Völlige Flexibilität in der Nutzung von WebtechnologienUm eine Shiny-App zu entwickeln, sind absolut keine Web Development-Skills notwendig. Allerdings können Shiny-Apps nach Belieben mit gängigen Webtechnologien wie HTML5, CSS oder JavaScript kombiniert werden. Diese Tools erlauben es, die Funktionalitäten und das Styling von Shiny-Apps beliebig zu erweitern oder zu individualisieren.  Vor allem im Enterprise-Kontext ist die Entwicklung einer Shiny-App nur die halbe Miete, denn erst Deployment und Skalierung ermöglichen einer größeren Anzahl an Usern die professionelle Nutzung einer App, ganz ohne lokale Kopien von RStudio, Code- oder Datenfiles. Skalierung von Shiny-AppsKommerziell: Shiny Server Pro &amp; Shinyapps.ioRStudio’s Shiny Server Pro ist ein kommerzielles Produkt für das komfortable On-Premises Deployment. Der Shiny Server Pro ermöglicht die einfache Skalierung von Shiny-Apps und bietet Tools für einfaches Load Balancing sowie User Management, und bietet Unterstützung für Maintenance und die Konfiguration der Server.RStudio bietet mit Shinyapps.io zudem eine allumfassende Cloud-Lösung für Deployment und Skalierung von Shiy-Apps an. Hier übernimmt RStudio das komplette Deployment inklusive Skalierung, Security und Infrastruktur. (Auch RStudio Connect )Natürlich sind diese Lösungen mitunter relativ kostenintensiv. Open-Source: ShinyProxyNeben der Nutzung  kommerzieller Produkte sind auch andere Deployment Lösungen möglich. Hierzu zählt insbesondere der ShinyProxy, der ähnlich wie der Shiny Server Pro grundlegende Enterprise-Funktionalitäten wie Authentifizierung und Skalierung bereitstellt, allerdings auf Basis von Open Source Technologien. Unter anderem die Server Konfiguration, Sicherheit, und das Serving von containerten Shiny-Apps liegen hier ganz in der Verantwortung – aber auch Kontrolle! – jedes Data Engineers selbst. Join us @DataUniversity2019 !Natürlich ist das Aufsetzen eines ShinyProxy nicht ganz trivial, aber komm doch einfach zur Data University und lerne mit uns hands-on, wie du deinen eigenen Shiny Proxy Server aufsetzen kannst! Stelle dir auf der Homepage der Data University deinen persönlichen Stundenplan aus 24 Intensivkursen zusammen und erweitere Deinen Blickwinkel auf die Themen Data Strategy, Data Engineering, Data Science und Data Technology. Wir freuen uns auf dich!Über den AutorLea WaniekI am a data scientist at STATWORX, apart from machine learning, I love to play around with RMarkdown and ggplot2, making data science beautiful inside and out..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/shiny-apps-skalieren-datauniversity2019/;Statworx;  Lea Waniek
  19. Juli 2019;Getting started with flexdashboards in R;"IntroductionHere at STATWORX, we value reader-friendly presentations of our work. For many R users, the choice is usually a Markdown file that generates a .html or .pdf document, or a Shiny application, which provides users with an easily navigable dashboard. What if you want to construct a dashboard-style presentation without much hassle? Well, look no further. R Studio’s package flexdashboard gives data scientists a Markdown-based way of easily setting up dashboards without having to resort to full-on front end development. Using Shiny may be a bit too involved when the goal is to present your work in a dashboard. Why should you learn about flexdashboards ? If you’re familiar with R Markdown and know a bit about Shiny, flexdashboards are easy to learn and give you an alternative to Shiny dashboards. In this post, you will learn the basics on how to design a flexdashboard. By the end of this article, you’ll be able to :build a simple dashboard involving multiple pagesput tabs on each page and adjust the layout integrate widgets deploy your Shiny document on ShinyApps.io.The basic rules To set up a flexdashboard, install the package from CRAN using the standard command. To get started, enter the following into the console: rmarkdown::draft(file = ""my_dashboard"",                  template = ""flex_dashboard"",                  package = ""flexdashboard"")This function creates a .Rmd file with the name associated file name, and uses the package’s flexdashboard template. Rendering your newly created dashboard, you get a column-oriented layout with a header, one page, and three boxes. By default, the page is divided into columns, and the left-hand column is made to be double the height of the two right-hand boxes. You can change the layout-orientation to rows and also select a different theme. Adding runtime: shiny to the YAML header allows you to use HTML widgets. Each row (or column) is created using the ——— header, and the panels themselves are created with a ### header followed by the title of the panel. You can introduce tabsetting for each row by adding the {.tabset} attribute after its name. To add a page, use the  (=======) header and put the page name above it. Row height can be modified by using {.data-height = } after a row name if you chose a row-oriented layout. Depending on the layout, it may make sense to use {.data-width = } instead. Here, I’ll design a dashboard which explores the famous diamonds dataset, found in the ggplot2 package. While the first page contains some exploratory plots, the second page compares the performance of a linear model and a ridge regression in predicting the price. This is the skeleton of the dashboard (minus R code and descriptive text): ---title: ""Dashing diamonds""output:  flexdashboard::flex_dashboard:  orientation: rows  vertical_layout: fill  css: bootswatch-3.3.5-4/flatly/bootstrap.css  logo: STATWORX_2.jpgruntime: shiny---?Exploratory plots =======================================================================?Sidebar {.sidebar data-width=700} -----------------------------------------------------------------------?**Exploratory plots**?&lt;br&gt;?**Scatterplots**?&lt;br&gt;?**Density plot**?&lt;br&gt;?**Summary statistics**?&lt;br&gt;??Row {.tabset}-----------------------------------------------------------------------?### Scatterplot of selected variables?### Density plot for selected variable?Row -----------------------------------------------------------------------?### Maximum carats {data-width=50}?### Most expensive color {data-width=50}?### Maximal price {data-width=50}?Row {data-height=500}-----------------------------------------------------------------------?### Summary statistics {data-width=500}?Model comparison=======================================================================?Sidebar {.sidebar data-width=700}-----------------------------------------------------------------------?**Model comparison**?&lt;br&gt;?Row{.tabset}-----------------------------------------------------------------------? **Comparison of Predictions and Target**?### Linear Model?### Ridge Regression ?Row-----------------------------------------------------------------------### Densities of predictions vs. target The sidebars were added by specifying the attribute {.sidebar} after the name, followed by a page or row header. Page headers (========) create global sidebars, whereas local sidebars are made using row headers (---------). If you choose a global sidebar, it appears on all pages whereas a local sidebar only appears on the page it is put on. In general, it’s a good idea to add the sidebar after the beginning of the page and before the first row of the page. Sidebars are also good for adding descriptions of what your dashboard/application is about. Here I also changed the width using the attribute data-width. That widens the sidebar and makes the description easier to read. You can also display outputs in your sidebar by adding code chunks below it. Adding interactive widgetsNow that the basic layout is done let’s add some interactivity. Below the description in the sidebar on the first page, I’ve added several widgets. ```{r}selectInput(""x"", ""X-Axis"", choices = names(train_df), selected = ""x"")selectInput(""y"", ""Y-Axis"", choices = names(train_df), selected = ""price"")selectInput(""z"", ""Color by:"", choices = names(train_df), selected = ""carat"")selectInput(""model_type"", ""Select model"", choices = c(""LOESS"" = ""loess"", ""Linear"" = ""lm""), selected = ""lm"")checkboxInput(""se"", ""Confidence intervals ?"")```Notice that the widgets are identical to those you typically find in a Shiny application and they’ll work because runtime: shiny is specified in the YAML.To make the plots react to changes in the date selection, you need to specify the input ID’s of your widgets within the appropriate render function. For example, the scatterplot is rendered as a plotly output: ```{r}renderPlotly({ p &lt;- train_df %&gt;%     ggplot(aes_string(x = input$x, y = input$y, col = input$z)) +   geom_point() +  theme_minimal() +   geom_smooth(method = input$model_type, position = ""identity"", se = input$se) +   labs(x = input$x, y = input$y)  p %&gt;% ggplotly()})```You can use the render functions you would also use in a Shiny application. Of course, you don’t have to use render-functions to display graphics, but they have the advantage of resizing the plots whenever the browser window is resized. Adding value boxesAside from plots and tables, one of the more stylish features of dashboards are value boxes. flexdashboard provides its own function for value boxes, with which you can nicely convey information about key indicators relevant to your work. Here, I’ll add three such boxes displaying the maximal price, the most expensive color of diamonds and the maximal amount of carats found in the dataset.  flexdashboard::valueBox(max(train_df$carat),             caption = ""maximal amount of carats"",            color = ""info"",            icon = ""fa-gem"")There are multiple sources from which icons can be drawn. In this example, I’ve used the gem icon from font awesome. This code chunk follows a header for what would otherwise be a plot or a table, i.e., a ### header. Final touches and deploymentTo finalize your dashboard, you can add a logo and chose from one of several themes, or attach a CSS file. Here, I’ve added a bootswatch theme and modified the colors slightly. Most themes require the logo to be 48×48 pixels large. ---title: ""Dashing diamonds""output:  flexdashboard::flex_dashboard:  orientation: rows  vertical_layout: fill  css: bootswatch-3.3.5-4/flatly/bootstrap.css  logo: STATWORX_2.jpgruntime: shiny---After creating your dashboard with runtime: shiny, it can be hosted on ShinyApps.io, provided you have an account. You also need to install the package rsconnect. The document can be published with the ‚publish to server‘ in RStudio or with:rsconnect::deployDoc('path')You can use this function after you’ve obtained your account and authorized it using rsconnect::setAccountInfo() with an access token and a secret provided by the website. Make sure that all of the necessary files are part of the same folder. RStudio’s publish to server has the advantage of automatically recognizing the external files your application requires. You can view the example dashboard here and the code on our GitHub page. RecapIn this post, you’ve learned how to set up a flexdashboard, customize and deploy it – all without knowing JavaScript or CSS, or even much R Shiny. However, what you’ve learned here is only the beginning! This powerful package also allows you to create storyboards, integrate them in a more modularized way with R Shiny and even set up dashboards for mobile devices. We will explore these topics together in future posts. Stay tuned!Über den AutorThomas AlcockI am a data scientist at STATWORX. The most interesting thing about data science is to find performative and explainable solutions to new problems..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/getting-started-with-flexdashboards-in-r/;Statworx;  Thomas Alcock
  10. Juli 2019;Bang Bang – How to program with dplyr;"The tidyverse is making the life of a data scientist a lot easier. That’s why we at STATWORX love to execute our analytics and data science with the tidyverse. Its user-centered approach has many advantages. Instead of the base R version df_testdf_test$x &gt; 10, we can write df_test %&gt;% filter(x&gt;10)), which is a lot more readable – especially if our data pipeline gets more complex and nested. Also, as you might have noticed, we can use the column names directly instead of referencing the Data Frame before. Because of those advantages, we want to use dplyr-verbs for writing our function. Imagine we want to write our own summary-function my_summary(), which takes a grouping variable and calculates some descriptive statistics. Let’s see what happens when we wrap a dplyr-pipeline into a function:my_summary &lt;- function(df, grouping_var){ df %&gt;%  group_by(grouping_var) %&gt;%   summarise(   avg = mean(air_time),   sum = sum(air_time),   min = min(air_time),   max = max(air_time),   obs = n()  )}my_summary(airline_df, origin)Error in grouped_df_impl(data, unname(vars), drop) :  Column `grouping_var` is unknown Our new function uses group_by(), which is searching for a grouping variable grouping_var, and not for origin, as we intended. So, what happened here? group_by() is searching within its scope for the variable grouping_var, which it does not find.  group_by() is quoting its arguments, grouping_var in our example. That’s why dplyr can implement custom ways of handling its operation. Throughout the tidyverse, tidy evaluation is used. Therefore we can use column names, as it is a variable. However, our data frame has no column grouping_var. Non-Standard EvaluationTalking about whether an argument is quoted or evaluated is a more precise way of stating whether or not a function uses non-standard evaluation (NSE).– Hadley WickhamThe quoting used by group_by() means, that it uses non-standard evaluation, like most verbs you can find in dplyr. Nonetheless, non-standard evaluation is not only found and used within dplyr and the tidyverse.Because dplyr quotes its arguments, we have to do two things to use it in our function:First, we have to quote our argument Second, we have to tell dplyr, that we already have quoted the argument, which we do with unquotingWe will see this quote-and-unquote pattern consequently through functions which are using tidy evaluation.my_summary &lt;- function(df, grouping_var){  df %&gt;%    group_by(!!grouping_var) %&gt;%     summarise(      avg = mean(air_time),      sum = sum(air_time),      min = min(air_time),      max = max(air_time),      obs = n()    )}my_summary(airline_df, quo(origin))Therefore, as input in our function, we quote the origin-variable, which means that R doesn’t search for the symbol origin in the global environment, but holds evaluation. The quotation takes place with the quo() function. In order to tell group_by(), that the variable was already quoted we need to use the !!-Operator; pronounced Bang-Bang (if you wondered about the title). If we are not using !!, group_by() at first searches for the variable within its scope, which are the columns of the given data frame. As mentioned before, throughout the tidyverse, tidy evaluation is used with its eval_tidy()-function. Whereby, it also introduces the concept of data mask, which makes data a first class object in R.Data MaskGenerally speaking, the data mask approach is much more convenient. However, on the programming site, we have to pay attention to some things, like the quote-and-unquote pattern from before. As a next step, we want the quotation to take place inside of the function, so the user of the function does not have to do it. Sadly, using quo() inside the function does not work.my_summary &lt;- function(df, grouping_var){  quo(grouping_var)  df %&gt;%    group_by(!!grouping_var) %&gt;%     summarise(      avg = mean(air_time),      sum = sum(air_time),      min = min(air_time),      max = max(air_time),      obs = n()    )}my_summary(airline_df, origin)Error in quos(...) : object 'origin' not found We are getting an error message because quo() is taking it too literal and is quoting grouping_var directly instead of substituting it with origin as desired. That’s why we use the function enquo() for enriched quotation, which creates a quosure. A quosure is an object which contains an expression and an environment. Quosures redefine the internal promise object into something that can be used for programming. Thus, the following code is working, and we see the quote-and-unquote pattern again.my_summary &lt;- function(df, grouping_var){  grouping_var &lt;- enquo(grouping_var)  df %&gt;%    group_by(!!grouping_var) %&gt;%     summarise(      avg = mean(air_time),      sum = sum(air_time),      min = min(air_time),      max = max(air_time),      obs = n()    )}my_summary(airline_df, origin)# A tibble: 2 x 6  origin   avg    sum   min   max   obs  &lt;fct&gt;  &lt;dbl&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;1 JFK     166. 587966    19   415  35392 LAX     132. 850259     1   381  6461All R code is a treeTo better understand what’s happening, it is useful to know that every R code can be represented by an Abstract Syntax Tree (AST) because the structure of the code is strictly hierarchical.  The leaves of an AST are either symbols or constants. The more complex a function call gets, the deeper an AST is getting with more and more levels. Symbols are drawn in dark-blue and have rounded corners, whereby constants have green borders and square corners. The strings are surrounded by quotes so that they won’t be confused with symbols. The branches are function calls and are depicted as orange rectangles.a(b(4, ""s""), c(3, 4, d()))To understand how an expression is represented as an AST, it helps to write it in its prefix form. y &lt;- x * 10`&lt;-`(y, `*`(x, 10))There is also the R package called lobstr, which contains the function ast() to create an AST from R Code.  The code from the first example lobstr::ast(a(b(4, ""s""), c(3, 4, d()))) results in this:It looks as expected and just like our hand-drawn AST. The concept of ASTs helps us to understand what is happening in our function.  So, if we have the following simple function, !!` introduces a placeholder (promise) for x. x &lt;- expr(-1)f(!!x, y)Due to R’s lazy evaluation, the function f() is not evaluated immediately, but once we called it. At the moment of the function call, the placeholder x is replaced by an additional AST, which can get arbitrary complex. Furthermore, it keeps the order of the operators correct, which is not the case when we use parse() and paste() with strings. So the resulting AST of our code snippet is the following:Furthermore, !! also works with symbols, functions, and constants. Perfecting our functionNow, we want to add an argument for the variable we are summarizing to refine our function. At the moment we have air_time hardcoded into it. Thus, we want to replace it with a general summary_var as an argument in our function. Additionally, we want the column names of the final output data frame to be adjusted dynamically, depending on the input variable. For adding summary_var, we follow the quote and unquote pattern from above. However, for the column-naming, we need two additional functions. Firstly, quo_name(), which converts a quoted symbol into a string. Therefore, we can use normal string operations on it and, e.g. use the base paste command for manipulating it. However, we also need to unquote it, which would be on the Left-Hand-Side, where R is not allowing any computations. Thus, we need the second function, the vestigial operator := instead of the normal =.my_summary &lt;- function(df, grouping_var, summary_var){  grouping_var &lt;- enquo(grouping_var)  summary_var &lt;- enquo(summary_var)  summary_nm &lt;- quo_name(summary_var)  summary_nm_avg &lt;- paste0(""avg_"",summary_nm)  summary_nm_sum &lt;- paste0(""sum_"",summary_nm)  summary_nm_obs &lt;- paste0(""obs_"",summary_nm)  df %&gt;%    group_by(!!grouping_var) %&gt;%     summarise(      !!summary_nm_avg := mean(!!summary_var),      !!summary_nm_sum := sum(!!summary_var),      !!summary_nm_obs := n()    )}my_summary(airline_df, origin, air_time)# A tibble: 2 x 4  origin avg_air_time sum_air_time obs_air_time  &lt;fct&gt;         &lt;dbl&gt;        &lt;int&gt;        &lt;int&gt;1 JFK            166.       587966         35392 LAX            132.       850259         6461Tidy DotsIn the next step, we want to add the possibility to summarize an arbitrary number of variables. Therefore, we need to use tidy dots (or dot-dot-dot) …. E.g. if we call the documentation for select(), we getUsage select(.data, ...)Arguments...      One or more unquoted expressions separated by commas. In select() we can use any number of variables we want to select. We will use tidy dots ... in our function. However, there are some things we have to account for. Within the function, ... is treated as a list. So we cannot use !! or enquo(), because these commands are made for single variables. However, there are counterparts for the case of .... In order to quote several arguments at once, we can use enquos(). enquos() gives back a list of quoted arguments. In order to unquote several arguments we need to use !!!, which is also called the big bang-Operator. !!! replaces arguments one-to-many, which is called unquote-splicing and respects hierarchical orders.With using purrr, we can neatly handle the computation with our list entries provided by ... (for more information ask your Purrr-Macist). So, putting everything together, we finally arrive at our final function. my_summary &lt;- function(df, grouping_var, ...) {  grouping_var &lt;- enquo(grouping_var)  smry_vars &lt;- enquos(..., .named = TRUE)  smry_avg &lt;- purrr::map(smry_vars, function(var) {    expr(mean(!!var, na.rm = TRUE))  })  names(smry_avg) &lt;- paste0(""avg_"", names(smry_avg))  smry_sum &lt;- purrr::map(smry_vars, function(var) {    expr(sum(!!var, na.rm = TRUE))  })  names(smry_sum) &lt;- paste0(""sum_"", names(smry_sum))  df %&gt;%    group_by(!!grouping_var) %&gt;%    summarise(!!!smry_avg, !!!smry_sum, obs = n())}my_summary(airline_df, origin, dep_delay, arr_delay)# A tibble: 2 x 6  origin avg_dep_delay avg_arr_delay sum_dep_delay sum_arr_delay   obs  &lt;fct&gt;          &lt;dbl&gt;         &lt;dbl&gt;         &lt;int&gt;         &lt;int&gt; &lt;int&gt;1 JFK            12.9          11.8          45792         41625  35392 LAX             8.64          5.13         55816         33117  6461And the tidy evaluation goes on and onAs mentioned in the beginning, tidy evaluation is not only used within dplyr but within most of the packages in the tidyverse. Thus, to know how tidy evaluation works is also helpful if one wants to use ggplot in order to create a function for a styled version of a grouped scatter plot. In this example, the function takes the data, the values for the x and y-axes as well as the grouping variable as inputs:scatter_by &lt;- function(.data, x, y, z=NULL) {  x &lt;- enquo(x)  y &lt;- enquo(y)  z &lt;- enquo(z)  ggplot(.data) +     geom_point(aes(!!x, !!y, color = !!z)) +    theme_minimal()}scatter_by(airline_df, distance, air_time, origin) Another example would be to use R Shiny Inputs in a sparklyr-Pipeline. input$ cannot be used directly within sparklyr, because it would try to resolve the input list object on the spark side.server.Rlibrary(shiny)library(dplyr)library(sparklyr)# Define server logic required to filter numbersshinyServer(function(input, output) {    tbl_1 &lt;- tibble(a = 1:5, b = 6:10)    sc &lt;- spark_connect(master = ""local"")    tbl_1_sp &lt;-        sparklyr::copy_to(            dest = sc,            df = tbl_1,            name = ""tbl_1_sp"",            overwrite = TRUE        )    observeEvent(input$select_a, {        number_b &lt;- tbl_1_sp %&gt;%            filter(a == !!input$select_a) %&gt;%            collect() %&gt;%            pull()        output$text_b &lt;- renderText({            paste0(""Selected number : "", number_b)        })    })})ui.Rlibrary(shiny)library(dplyr)library(sparklyr)# Define UI for application tshinyUI(fluidPage(    # Application title    titlePanel(""Select Number Example""),    # Sidebar with a slider input for number    sidebarLayout(sidebarPanel(        sliderInput(            ""select_a"",            ""Number for 1:"",            min = 1,            max = 5,            value = 1        )    ),    # Show a text as output    mainPanel(textOutput(""text_b"")))))ConclusionThere are many use cases for tidy evaluation, especially for advanced programmers. With the tidyverse getting bigger by the day, knowing tidy evaluation gets more and more useful. For getting more information about the metaprogramming in R and other advanced topics, I can recommend the book Advanced R by Hadley Wickham.“ÜberMarkus BerrothI am a data scientist at STATWORXand I love creating novel knowledge from data.In my time off,I am always open for a weekend trip..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/bang-bang-how-to-program-with-dplyr/;Statworx;  Markus Berroth
  3. Juli 2019;Monotoniebedingungen in Machine Learning Modellen mit R;"EinführungIn den letzten Jahren wurden die Rechenleistung sowie die Komplexität der Algorithmen immer höher. Dennoch war die Nutzung von menschlichen Erkenntnissen in jedem unserer Projekte von STATWORX ein wichtiger Bestandteil bei der erfolgreichen Durchführung eines Projektes. In diesem Artikel soll auf das Einbeziehen von Wissen über Monotoniebedingungen in ein Modell eingegangen werden.Zunächst soll dazu die Monotoniebedingung definiert werden.Mathematisch betrachetet bedeutet eine monoton steigende Funktion, dass bei beliebigen  bei denen ,  dann gilt:    . Gilt hingegen, dass  und  , wird von einer monoton fallenden Funktion gesprochen.  Doch wie lässt sich diese Eigenschaft auf ein konkretes Business Problem anwenden und was bedeutet dies für unser Modell?Nehmen wir zum Beispiel die Vorhersage des Preises einer gebrauchten Maschine.Versucht man diesen Preis durch ein mathematisches Modell vorherzusagen, ist der Preis der gebrauchten Maschine das Ergebnis einer Funktion , die von mehreren Variablen abhängt.  In unserem Beispiel können das der Neupreis der Maschine, der Einsatzort und die Laufzeit der Maschine sein. Aus betriebswirtschaftlicher Sicht ist dabei eine monoton fallende Funktion zwischen dem Preis und der Laufzeit zu erwarten. Bei der Verwendung nicht linearer mathematischer Modelle kann es jedoch vorkommen, dass das Modell zwar bei großen Differenzen in der Laufzeit einen negativen Zusammenhang zwischen Laufzeit und Preis einer Maschine vorhersagt, es jedoch bestimmte Regionen gibt, in denen das Modell einen positiven Zusammenhang zwischen Preis und Laufzeit ermittelt. In einem Projekt wäre das aus zwei Gründen problematisch. Ein Argument ist die Akzeptanz des Modells bei Fachexperten, die die Vorhersagen nutzen möchten. Diese werden einem Modell, das nicht monoton ist, wenig vertrauen. Der andere Grund ist, dass Overfitting vermieden werden kann und die Performance auf dem Test-Set somit verbessert wird. Dies liegt daran, dass zum Beispiel bei einem tatsächlich monoton fallenden Zusammenhang ein steigender Zusammenhang durch einige wenige Fälle verursacht worden ist, bei denen andere, nicht in dem Modell aufgenommene Variablen, das Ergebnis verzerrt haben.SimulationFür ein erstes Beispiel werden simulierte Daten genutzt. Dazu wurde eine Zielvariable  in Abhängigkeit von den Variablen  in der folgenden Form simuliert:    Dabei sind  sowie  jeweils normalverteilte Zufallsvariablen. Insgesamt wurden 150 Fälle simuliert.Im nächsten Schritt werden zwei Modelle mit dem R-Package GBM gelernt, die die Zusammenhänge der Funktion schätzen sollen. Dabei wird ein Modell ohne Beschränkungen und eines mit Beschränkungen gelernt. Hier ein Beispiel, bei dem für die erste Variable eine monoton steigende Funktion angenommen wird:# Training eines Boosted Modells ohne Monotoniebeschränkungenmodel_constrained &lt;- gbm(formula, data = df, n.trees = n_trees,                          distribution = ""gaussian"") # Training eines Boosted Modells mit Monotoniebeschränkungenmodel_constrained &lt;- gbm(formula, data = df, n.trees = n_trees,                          distribution = ""gaussian"",                          # Beschränkung der ersten Variable                         var.monotone = c(1,0,0)) Die Implementierung von Monotoniebedingungen ist teilweise auch in anderen Packages vorhanden. So können Monotoniebedingungen bei Random Forests in R mit dem Package Rborist implementiert werden. In Python ist eine Implementierung unter anderem für XGBoost vorhanden.Nehmen wir für unser Modell an, dass den Fachexperten bekannt ist, dass der Zusammenhang zwischen  und  monoton steigend ist, was der simulierten Funktion entspricht. In der Funktion gbm() lässt sich das einfach über das Funktionsargument var.monotone implementieren. Dem Funktionsargument muss ein Vektor übergeben werden, der für jede Einflussvariable angibt, ob ein positiver (1), negativer (-1) oder unspezifizierter (0) Zusammenhang vorliegt.Um den Einfluss einer Variablen zu erkennen, wird hier der Partial Dependence Plot genutzt. Dieser zeigt den Einfluss einer Variablen an, wenn alle anderen Variablen als fest angenommen werden. In der folgenden Grafik ist dabei der Einfluss von  auf die Zielvariable  dargestellt. Dabei ist zu erkennen, dass es in dem Modell ohne Beschränkungen Bereiche gibt, in denen ein negativer Zusammenhang vorliegt, was in dem Modell mit Beschränkungen nicht der Fall ist. Schauen wir uns im nächsten Schritt den MAE an, erkennen wir, dass auch dieser beim Modell mit Constraints von 749,3 auf 744,1 sinkt. Das Modell ohne Constraints hat sich also zu stark den Trainingsdaten angepasst (Overfitting), wodurch die Performance auf den Testdaten schlechter war, als beim Modell ohne Constraints. PraxisbeispielJetzt können wir die Ergebnisse auf ein konkretes Problem anwenden. Nehmen wir dazu an, dass eine Immobilienfirma den Wert von neuen Immobilien schneller ermitteln möchte, wozu ein Machine Learning Modell entworfen werden soll. In Gesprächen mit Fachexperten wurde ermittelt, dass ein positiver Zusammenhang zwischen der Größe des Grundstücks und dem Preis der Immobilie vorliegt.Um das Modell zu berechnen wird ein Datensatz zur Verfügung gestellt. Die zugehörigen Daten können hier heruntergeladen werden. Der Datensatz enthält insgesamt 81 Spalten, wovon für diesen Artikel jedoch zur Übersichtlichkeit nur wenige genutzt werden. In unserem Modell wird als Zielvariable der Preis gewählt und als Einflussvariablen werden die Anzahl an Square Feet des Grundstücks (LotArea), der Zustand der Garage (GarageCond), der Typ des Hauses (HouseStyle) sowie die Qualität des Materials (OverallQual) genutzt.Hier ein kurzer Ausschnitt aus den Daten:priceLotAreaHouseStyleGarageCondOverallQual20850084502StoryTA718150096001StoryTA6223500112502StoryTA714000095502StoryTA7250000142602StoryTA8143000141151.5FinTA5Nach der Auswahl der Variablen und dem Bilden einer Formel, wird jeweils ein Modell ohne Beschränkungen bezüglich der Monotonie und eines mit einer Beschränkung der Square Feets gerechnet. Bildet man daraufhin einen Partial Dependence Plot, erkennt man in dem Modell ohne Beschränkungen im Bereich von circa 12.000 – 18.000 Square Feet, dass bei steigender Quadratmeteranzahl der Preis sinkt. In dem Modell mit Beschränkungen ist dies nicht der Fall, was der menschlichen Intuition entsprechen würde.Vergleicht man weiterhin den Mean Average Error, erkennt man, dass das Hinzufügen von Beschränkungen in unserem Fall sogar zu einer leichten Verbesserung der Vorhersagegüte von einem MAE von 30875$ auf 30085$ geführt hat. In diesem Fall konnte durch das Hinzufügen von Fachkenntnissen das Modellergebnis also verbessert werden.Wie eben gezeigt, kann das manuelle Einfügen von menschlicher Fachkenntnis sowohl die Interpretierbarkeit als auch die Vorhersagegüte verbessern. Es lohnt sich daher als Data Scientist immer, in ständigem Austausch mit der Fachabteilung Modelle zu entwickeln.Über den AutorMartin AlbersI am a data scientist at STATWORX and I like everything that has to do with analysis and visualization..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/monotoniebedingungen-in-machine-learning-modellen-mit-r/;Statworx;  Martin Albers
  2. Juli 2019;Data Science Bootcamp;"Du interessiertst Dich für Data Science und möchtest Deine Daten sinnvoll nutzen?Im Data Science Bootcamp von der STATWORX Data Academy bekommen Einsteiger den perfekten Start in die Praxis. Gemeinsam erarbeiten wir in 5 Tagen die wichtigsten Grundlagen von Data Science, Machine Learning und Programmierung. Du entwickelst R und Python Modelle und arbeitest wie ein echter Data Scientist. ?Was erwartet dich??Wir haben die Inhalte des Bootcamps so zusammengestellt, dass den Teilnehmern Theorie und Praxis vermittelt werden. Alle relevanten Bereiche von Data Science werden in 5 Tagen kompakt abgedeckt.?? Data StrategyAm ersten Tag vermitteln wir Dir die theoretischen und praktischen Grundlagen von Data Science und führen strukturiert in das Thema ein.?? Data Exploration &amp; EngineeringDen zweiten Tag des Bootcamps nutzen wir zur intensiven Beschäftigung mit der Aufbereitung und Exploration von Daten – eine der wichtigsten Arbeiten des Data Scientists.?? Machine LearningAm dritten Tag beschäftigen wir uns mit der statistischen Modellierung von Daten, dem Testen von Hypothesen und dem Design von Experimenten.?? Statistische ModellierungAn Tag 4 des Bootcamps befassen wir uns verstärkt mit dem Design, der Programmierung und Evaluation von Machine Learning Modellen.?? Deployment, Visualisierung &amp; ReportingAbschließend vermitteln wir Dir wichtiges Wissen über das Deployment von Modellen sowie zur Visualisierung von Modellen und Daten.?Wer sollte teilnehmen??Das Bootcamp richtet sich an Business User und angehende Data Scientists, die mehr über das Thema erfahren möchten. ?Warum sollte ich teilnehmen??Kein Online Tutorial kann die persönliche Interaktion mit unseren Kursleitern und anderen Teilnehmern ersetzen. Unsere Bootcamp Trainer sind Data Science und Statistik Experten und arbeiten seit vielen Jahren erfolgreich an echten Data Science Projekten. Im Bootcamp werden maximal 8 Teilnehmer ausgebildet. So stellen wir ein optimales Lernumfeld sicher.?Feedback eines Data Science Bootcamp BesuchersIm Data Science Bootcamp von STATWORX wird in kurzer Zeit ein toller und umfassender Einblick in die Welt der Data Science, des Machine Learnings und der Programmierung gegeben. Neben den theoretischen Aspekten zu Machine Learning und Statistik, waren für mich besonders die praxisrelevanten Gesichtspunkte wie notwendige Teamstrukturen und Best Practices interessant. Die unmittelbare praktische Anwendung und Umsetzung in R oder Python hat die Thematik zudem noch greifbarer gemacht.ChristophLocations und Termine?Unsere Bootcamps werden aktuell in unseren Büros in Frankfurt und Zürich angeboten, können aber auch inhouse stattfinden. Das Data Science Bootcamp findet aktuell 1x pro Quartal statt. Hier zu den aktuellen Terminen. Mehr Termine sind aktuell in Planung.?Kosten?Die Kosten für das 5-tägige Bootcamp betragen 3.950 € pro Teilnehmer. Rabatte sind für Frühbucher erhältlich.Wir freuen uns auf deine Anfrage!Jetzt anmelden!";https://www.statworx.com/de/blog/data-science-bootcamp-blog/;Statworx;  Vivian Jeenel
  28. Juni 2019;Revisited: Forecasting Last Christmas Search Volume;"It is June and nearly half of the year is over, marking the middle between Christmas 2018 and 2019. Last year in autumn, I’ve published a blog post about predicting Wham’s „Last Christmas“ search volume using Google Trends data with different types of neural network architectures. Of course, now I want to know how good the predictions were, compared to the actual search volumes.The following table shows the predicted values by the different network architectures, the true search volume data in the relevant time region from November 2018 until January 2019, as well as the relative prediction error in brackets:monthMLPCNNLSTMactual2018-110.166 (0.21)0.194 (0.078)0.215 (0.023)0.212018-120.858 (0.057)0.882 (0.031)0.817 (0.102)0.912019-010.035 (0.153)0.034 (0.149)0.035 (0.153)0.03There’s no clear winner in this game. For the month November, the LSTM model performs best with a relative error of only 2.3%. However, in the „main“ month December, the LSTM drops in accuracy in favor of the 1-dimensional CNN with 3.1% error and the MLP with 5.7% error. Compared to November and December, January exhibits higher prediction errors &gt;10% regardless of the architecture.To bring a little more data science flavor into this post, I’ve created a short R script that presents the results in a cool „heatmap“ style.library(dplyr)library(ggplot2)# Define data frame for plottingdf_plot &lt;- data.frame(MONTH=rep(c(""2018-11"", ""2018-12"", ""2019-01""), 3),                      MODEL = c(rep(""MLP"", 3), rep(""CNN"", 3), rep(""LSTM"", 3)),                      PREDICTION = c(0.166, 0.858, 0.035, 0.194, 0.882, 0.034, 0.215, 0.817, 0.035),                      ACTUAL = rep(c(0.21, 0.91, 0.03), 3))# Do plotdf_plot %&gt;%  mutate(MAPE = round(abs(ACTUAL - PREDICTION) / ACTUAL, 3)) %&gt;%  ggplot(data = ., aes(x = MONTH, y = MODEL)) +  geom_tile(aes(fill = MAPE)) +  scale_fill_gradientn(colors = c('navyblue', 'darkmagenta', 'darkorange1')) +  geom_text(aes(label = MAPE), color = ""white"") +  theme_minimal()This year, I will (of course) redo the experiment using the newly acquired data. I am curious to find out if the prediction improves. In the meantime, you can sign up to our mailing list, bringing you the best data science, machine learning and AI reads and treats directly into your mailbox!Über den AutorSebastian HeinzI am the founder and CEO of STATWORX. I enjoy writing about machine learning and AI, especially about neural networks and deep learning. In my spare time, I love to cook, eat and drink as well as traveling the world..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/revisited-forecasting-last-christmas-search-volume/;Statworx;  Sebastian Heinz
  26. Juni 2019;Simulating the bias-variance tradeoff in R;"In my last blog post, I have elaborated on the Bagging algorithm and showed its prediction performance via simulation. Here, I want to go into the details on how to simulate the bias and variance of a nonparametric regression fitting method using R. These kinds of questions arise here at STATWORX when developing, for example, new machine learning algorithms or testing established ones which shall generalize well to new unseen data.Decomposing the mean squared errorWe consider the basic regression setup where we observe a real-valued sample  and our aim is to predict an outcome  based on some predictor variables („covariates“)  via a regression fitting method. Usually, we can measure our target only with noise ,To measure our prediction accuracy, we will use the mean squared error (MSE) which can be decomposed as follows: is our prediction obtained by the regression method at hand.From the above expression, we observe that the MSE consists of two parts:: measures the (squared) difference between the true underlying process and the mean of our predictions, i.e. : measures the variation of our predictions around its mean, i.e. In general, it will not be possible to minimize both expressions as they are competing with each other. This is what is called the bias-variance tradeoff. More complex models (e.g. higher order polynomials in polynomial regression) will result in low bias while yielding high variance as we fit characteristic features of the data that are not necessary to predict the true outcome (and vice versa, cf. Figure 1).Monte Carlo Setup &amp; Simulation CodeTo illustrate this, we consider a simple toy model.where ,  and As a fitting procedure, we will use a cubic smoothing spline (smooth.spline). For the purpose of this blog post, we only need to know that a smoothing spline divides our univariate predictor space into  intervals and fits each one to a cubic polynomial to approximate our target. The complexity of our smoothing spline can be controlled via the degrees of freedom (function argument df). If you are interested in the (impressive) mathematical details of smoothing splines, check out this script by Ryan Tibshirani.Figure 1 shows the bias-variance tradeoff from above. For relatively low degrees of freedom we obtain a model (red line) which is too simple and does not approximate the true data generating process well (black dashed line) – Note: for  we obtain the least-squares fit. Here, the bias will be relatively large while the variance will remain low. On the other hand, choosing relatively high degrees of freedoms leads to a model which overfits the data (green line). In this case, we clearly observe that the model is fitting characteristic features of the data which are not relevant for approximating the true data generating process.Figure 1: In-sample fit of a (cubic) smoothing spline with varying degrees of freedomsTo make this tradeoff more rigorous, we explicitly plot the bias and variance. For this, we conduct a Monte Carlo simulation. As a side note, to run the code snippets below, you only need the stats module which is contained in the standard R module scope.For validation purposes, we use a training and test dataset (*_test and *_train, respectively). On the training set, we construct the algorithm (obtain coefficient estimates, etc.) while on the test set, we make our predictions.# set a random seed to make the results reproducibleset.seed(123)n_sim &lt;- 200n_df &lt;- 40n_sample &lt;- 100# setup containers to store the resultsprediction_matrix &lt;- matrix(NA, nrow = n_sim, ncol = n_sample)mse_temp &lt;- matrix(NA, nrow = n_sim, ncol = n_df)results &lt;- matrix(NA, nrow = 3, ncol = n_df)# Train data -----x_train &lt;- runif(n_sample, -0.5, 0.5)f_train &lt;- 0.8*x_train+sin(6*x_train)epsilon_train &lt;- replicate(n_sim, rnorm(n_sample, 0, sqrt(2)))y_train &lt;- replicate(n_sim,f_train) + epsilon_train# Test data -----x_test &lt;- runif(n_sample, -0.5, 0.5)f_test &lt;- 0.8*x_test+sin(6*x_test)The bias-variance tradeoff can be modelled in R using two for-loops. The outer one will control the complexity of the smoothing splines (counter: df_iter). The Monte Carlo Simulation with 200 iterations (n_sim) to obtain the prediction matrix for the variance and bias is run in the inner loop. # outer for-loopfor (df_iter in seq(n_df)){  # inner for-loop  for (mc_iter in seq(n_sim)){    cspline &lt;- smooth.spline(x_train, y_train, mc_iter, df=df_iter+1)    cspline_predict &lt;- predict(cspline, x_test)    prediction_matrixmc_iter, 1:n_sample &lt;- cspline_predict$y     mse_tempmc_iter, df_iter &lt;- mean((cspline_predict$y - f_test)^2)  }  var_matrix &lt;- apply(prediction_matrix, 2, FUN = var)  bias_matrix &lt;- apply(prediction_matrix, 2, FUN = mean)  squared_bias &lt;- (bias_matrix - f_test)^2  results1, df_iter &lt;- mean(var_matrix)  results2, df_iter &lt;- mean(squared_bias)}results3,1:n_df &lt;- apply(mse_temp, 2, FUN = mean)To model  and  from the above MSE-equation, we have to approximate those theoretical (population) terms by means of a Monte Carlo simulation (inner for-loop). We run  (n_sim) Monte Carlo iterations and save the predictions obtained by the smooth.spline-object in a  prediction matrix. To approximate  at each test sample point , by , we take the average of each column.  denotes the prediction of the algorithm obtained at some sample point  in iteration . Similar considerations can be made to obtain an approximation for .Bias-variance tradeoff as a function of the degrees of freedomFigure 2 shows the simulated bias-variance tradeoff (as a function of the degrees of freedom). We clearly observe the complexity considerations of Figure 1. Here, the bias is quickly decreasing to zero while the variance exhibits linear increments with increasing degrees of freedoms. Hence, for higher degrees of freedom, the MSE is driven mostly by the variance.Comparing this Figure with Figure 1, we note that for , the bias contributes substantially more than the variance to the MSE. If we increase the degrees of freedom to  the bias tends to zero, characteristic features of the data are fitted and the MSE consists mostly of the variance.Figure 2: Bias-Variance Tradeoff of a (cubic) smoothing splineWith small modifications, you can use this code to explore the bias-variance tradeoff of other regression fitting and also Machine Learning methods such as Boosting or Random Forest. I leave it to you to find out which hyperparameters induce the bias-variance tradeoff in these algorithms.You can find the full code on my Github Account. If you spot any mistakes or if you are interested in discussing applied statistic and econometrics topics, feel free to contact me.ReferencesThe simulation setup in this blog post follows closely the one from Buhlmann.Buhlmann, P. and Yu, B. (2003). Boosting with the L2-loss: Regression and classification. J. Amer. Statist. Assoc. 98, 324–339.Über den AutorRobin KraftI am a working student at STATWORX and currently writing my Master thesis about componentwise boosting..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/simulating-the-bias-variance-tradeoff-in-r/;Statworx;  Robin Kraft
  21. Juni 2019;STATWORX summer barbecue 2019;"Summer in Frankfurt means blue skies, warm nights and happy people all around. That’s why STATWORX made it a tradition to have a company barbecue and enjoy some good food and drink while mingling with colleagues and friends. Moreover, we had a great reason to celebrate: as of late we’re the STATWORX GmbH (engl.: limited company). So, last Friday we had a party!  Work hard, party hardBut not so fast: before the first beers were opened, our CEO and CFO gave us an overview of the company’s current situation and the general plans for the future. (Side note: you know you’re in a room full of consultants if half of the audience is so intrigued by the spot-on design of the powerpoint slides that the presented content almost becomes secondary!)  After several insightful hours of presentation, motivated by past accomplishments and eager to put the presented game plan into practice, we finally began to prepare the party. I must say it was really cool to see how smoothly everyone was working together. In almost no time we had set up the tables, grills and decorations in the backyard. When the buffet table was loaded up with home-made goodies, it became evident that some of us are not only talented statisticians, marketing geniuses or brilliant data scientists but also seasoned chefs.  Soon, the first guests started to arrive, and after the first vegetarian steaks and traditional sausages were sizzling on the grill, the party was instantly in full swing. Even a short shower of rain couldn’t dampen the mood. Maybe, the exotic and simply delicious cocktails mixed by our very own barkeeper might have helped with that. Let me tell you, the non-alcoholic Mojito was a revelation.  We spend the evening connecting with new collogues, old colleagues as well as their friends and families. It also was very cool to chat with our Swiss colleagues in person again, who were kind enough to bring an exceptional swiss speciality – a whole box of Luxemburgerli! – all the way with them. The evening was topped off by several former colleagues who stopped by to catch up.To not overstrain the patience and ears of our lovely neighbours, we moved the party inside. On the makeshift dancefloor, laser-show included, we danced our heart out to the finest tunes played by our current CEO and former DJ.  Since it wouldn’t be a STATWORX production without some facts and figures, so here some stats about the barbecue: Would you like to be part of the next STATWORX summer barbecue? You can become part of the team. We’re looking forward to meeting you! Über den AutorLea WaniekI am a data scientist at STATWORX, apart from machine learning, I love to play around with RMarkdown and ggplot2, making data science beautiful inside and out..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/statworx-summer-barbecue-2019/;Statworx;  Lea Waniek
  19. Juni 2019;Optimising your R code – a guided example;"Optimising your R code is not always the priority. But when you run out of memory, or it just takes too long, you start to wonder if there are better ways to do things! In this blog post, I will show you my way of optimising my R code and the process behind it. As an example, I will use the code from my last blog post about gas prices in Germany – if you haven’t read it yet, click here! There are different steps to optimise your code. From debugging and profiling over benchmarking to rethinking the whole method. You have to repeat these steps until you are satisfied with the result.A short recap of the initial code I had around 1700 .csv files, each containing the price changes of one day for all German gas stations from 2014 up till 2019. My plan was to analyse the price patterns, but first I had to prepare and load all the data. Normally, I would just load all the files, combine them into one data.table() , do some data preparation steps and then start my analysis. This was not possible here since the raw data was around 20 GB big and I wanted to rearrange them so that I would have half-hour prices. So, I settled for the solution of reading the files one after another, while preparing and aggregating each one at a time, and combining them at the end. There were eight steps during my loop: # pseudo codefor (i.file in price_files) {    # load data    # remove wrong prices (e.g. -0.001, 0 or 8.888)    # set TIME and DATE    # build a time grid including opening times    # add last day price and save next day price    # NA handling - last observation carried forward (locf)    # adding brand, motorway and post code info    # calculating mean and quantiles}Let’s see where I can optimise my steps! Identifying the bottlenecks Since I want to optimise a loop, I can just concentrate on a few iterations instead of the whole loop. Also, I do not need to include every station for this test. Therefore, I subset my data to stations which are in the area around Frankfurt am Main. stations_dt &lt;- stations_dtpost_code &gt;= ""60000"" &amp; post_code &lt; ""66000"", Note: Yes, you can filter strings like this – they are sorted alphabetically. Also, they are characters and not numbers since there are cases like “01896”.With this small test data, I will continue my optimisation process. To get an overview of bottlenecks I use the package profvis. Since RStudio version 1.0, it is quite easy to use. I just select the lines of code with my for-loop and click Profile &gt; Profile Selected Line(s) to get a report with the time and memory consumption in a new tab.I filter only for those lines which have a non-zero value to have a nicer overview. My main task is to reduce time consumption, therefore I will focus on the longest time first. There is one major time consumer: stat_fun(). It takes around 75% of the total 30000ms. So, what is this function of mine?a deeper look at stat_fun() This function returns a data.table() with the mean, the 10% and 90% quantile and the number of observations. I use stat_fun on each group defined by this_by &lt;- c(""DATE"", ""TIME"", ""AUTOBAHN"", ""BRAND"", ""plz"").# get mean and quantiles 0.1 and 0.9stat_fun &lt;- function(x, na.rm = TRUE) {  #x &lt;- price_time_dt,.SD, .SDcol = c(""diesel"", ""e10"")  x_quant1 &lt;- x, lapply(.SD, quantile, probs = c(0.1), na.rm = na.rm),                .SDcols = names(x)  x_quant9 &lt;- x, lapply(.SD, quantile, probs = c(0.9), na.rm = na.rm),                .SDcols = names(x)  x_mean &lt;- x, lapply(.SD, mean, na.rm = na.rm), .SDcols = names(x)  x_obs &lt;- x, lapply(.SD, function(y) sum(!is.na(y))), .SDcols = names(x)  setnames(x_quant1, paste0(names(x), ""_Q10""))  setnames(x_quant9, paste0(names(x), ""_Q90""))  setnames(x_mean, paste0(names(x), ""_MEAN""))  setnames(x_obs, paste0(names(x), ""_OBS""))  out &lt;- data.table(x_quant1, x_quant9, x_mean, x_obs)  return(out)}price_time_dt2 &lt;- price_time_dt, stat_fun(.SD), .SDcols = price_vars, by = this_byWhy did I build it like this? My goal was to have one function, which returns all the values I need for my analysis later: the mean, the quantiles and the number of observations. Let’s check how we can optimise this! First, I look into the function with profvis again. Since there is not much to see but that it takes 7900ms for one iteration I profile the lines in the stat_fun. Therefore I set the inputs and then select the lines in the function for profiling.x &lt;- price_time_dt,.SD, .SDcol = c(""diesel"", ""e10"")na.rm &lt;- TRUEThe result is quite surprising, there is not much time used even though I used the whole data instead of one group.How many times does this function get called? Well, for each combination of the by key. For the first file, this is around 1500 times. I will now check if  mean() and  quantile() are taking a lot of time, if they are used by each group.   x_mean &lt;- price_time_dt, lapply(.SD, mean, na.rm = TRUE),                          .SDcols = price_vars,                          by = this_by  x_quant1 &lt;- price_time_dt, lapply(.SD, quantile,                                     probs = 0.1,                                     na.rm = TRUE),                            .SDcols = price_vars,                            by = this_by  x_quant9 &lt;- price_time_dt, lapply(.SD, quantile,                                     probs = 0.9,                                     na.rm = TRUE),                            .SDcols = price_vars,                            by = this_byThe mean() function is fast and not a problem, but quantile() takes quite long. I could combine the calculation for the 10% and 90% quantile into one call, but the resulting table I would have to reshape into my desired output – if I cannot find any other tweaks, I will come back to this approach.While reading the help pages for the quantile() function, I stumble upon the argument names: names: logical; if true, the result has a names attribute. Set to FALSE for speedup with many probs. I am not using many probs, but this still improves the time by around 700ms. This is not enough, so I will have to test the other approach!implemanting stat_fun() as part of the main code The new approach first creates a data.table() for the mean, the quantiles and the number of observations and then merges them by the groups together. With this approach, I can combine the two quantile calculations, but have to include the merging part. # calculating the meanx_mean &lt;- price_time_dt, lapply(.SD, mean, na.rm = TRUE),                                .SDcols = price_vars,                                by = this_by# calculating the quantilesx_quant &lt;- price_time_dt, lapply(.SD, quantile, probs = c(0.1, 0.9),                                  na.rm = TRUE, names = FALSE),                         .SDcols = price_vars,                         by = this_byx_quant, QUANT := c(""Q10"", ""Q90"")eq_quant &lt;- paste0(paste0(this_by, collapse = ""+""), ""~QUANT"")x_quant &lt;- dcast(data = x_quant, formula = eq_quant, value.var = price_vars)setkeyv(x_quant, NULL)# number of observationsx_obs &lt;- price_time_dt, lapply(.SD, function(y) sum(!is.na(y))),                          .SDcols = price_vars,                          by = this_bysetnames(x_mean, price_vars, paste0(price_vars, ""_MEAN""))setnames(x_obs, price_vars, paste0(price_vars, ""_OBS""))setkeyv(price_time_dt, this_by)price_time_dt2 &lt;- Reduce(merge, list(x_mean, x_quant, x_obs))As it turns out, merging is fast as well – I would not have thought so! Instead of nearly 30s it only needs 12s now! With the biggest issue out of the way, there seems to be some more potential in other lines as well. Next up are the time formatting and the missing values handling with the last observation carried forward (locf).last observations slowly carried forwardprice_time_dt &lt;- price_time_dtorder(station_uuid, DATE, TIME)price_time_dt, c(price_vars) := lapply(.SD, zoo::na.locf, na.rm = FALSE),              .SDcols = price_vars,              by = station_uuid Sometimes it is hard to reinvent the wheel – so l search the vast knowledge of the internet and this article here comes up. They describe exactly my problem: a faster locf-method with grouping. price_time_dt &lt;- price_time_dtorder(station_uuid, DATE, TIME)id_change &lt;- price_time_dt, c(TRUE, station_uuid-1 != station_uuid-.N)price_time_dt, c(price_vars) :=  lapply(.SD, function(x) xcummax(((!is.na(x)) | id_change) * .I)),  .SDcols = price_varsTo understand this method I made this table with a toy example. The last column is used as the index, which replaces missing values with the last observation. Because this method is vectorised and works without a by group, it is much faster..Iidid_changex!is.na(x)(!is.na(x)|id_change)*.Icummax(…)11TRUE4TRUE1 * 1 = 1121FALSENAFALSE0 * 2 = 0131FALSENAFALSE0 * 3 = 0142TRUE9TRUE1 * 4 = 4452FALSENAFALSE0 * 5 = 0462FALSE5TRUE1 * 6 = 6672FALSENAFALSE0 * 7 = 0683TRUENAFALSE1 * 8 = 8893FALSE17TRUE1 * 9 = 99even formatting time is relative The raw data contains a column with the timestamp of a price change. There are multiple options to start to transform this string into a date and time variable: base R with as.POSIXct()lubridate* with ymd_hms()data.table with as.IDate() and as.ITime() There are two things I want from the time transformation. First, for my aggregation over daily hours analysis, I want the date and the time in separate columns. Second, I want to round to a given minute (e.g. to half hours or hours).Since I am a data.table person and I think fewer packages in a project are a good thing, my first choices for these tasks are as.IDate() and as.ITime(). But – the go-to package when it comes to time and dates – lubridate with the round_date() function to round to a specific minute, for example, might be a good candidate as well. Time to do some benchmarking! my benchmark setup To test the different functions I will use one of the files, reduced to the date, DAY and TIME columns:                   date        DAY     TIME1: 2014-06-08 09:50:01 2014-06-08 09:50:012: 2014-06-08 09:50:01 2014-06-08 09:50:013: 2014-06-08 09:50:01 2014-06-08 09:50:014: 2014-06-08 09:50:01 2014-06-08 09:50:015: 2014-06-08 09:50:01 2014-06-08 09:50:01splitting the format into date and time microbenchmark(  # lubridate  ""lubridate::ymd()"" = DT, DAY_ymd := ymd(DAY),  ""lubridate::hms()"" = DT, TIME_hms := as.numeric(hms(TIME)),  # data.table  ""data.table::as.IDate()"" = DT, DATE_IDate := as.IDate(date),  ""data.table::as.ITime()"" = DT, DATE_ITime := as.ITime(date),  # settings  times = 100L, unit = ""ms"")Unit: milliseconds                 expr        min        lq      mean    median        uq       max neval     lubridate::ymd()   7.210735  10.18305  12.62154  11.96969  14.31220  27.62012   100     lubridate::hms()   9.611753  10.26598  11.08295  10.60343  11.38001  16.48317   100 data.table::as.IDate  74.908261  75.91852  78.98697  77.86806  80.62390  91.97302   100 data.table::as.ITime 298.047305 306.15576 314.78720 309.89135 315.41061 466.35973   100Well, lubridate is the fastest! But, the output of hms() cannot simply be saved into a data.table column because it is a nested object of the class Period from lubridate. Therefore, I will use this little workaround: price_dt, c(""DATE"", ""TIME"") := tstrsplit(date, "" "")price_dt, DATE := lubridate::ymd(DATE)price_dt, TIME := as.ITime(as.numeric(lubridate::hms(TIME)))With this, I can continue to use the as.ITime() function and its format. Also, using as.ITime() on a numeric is much faster than on the original date.rounding to the minutes For the second task, I will compare lubridate::round_date() and ply::round_any() minute &lt;- 30microbenchmark(  # rounding lubridate  ""lubridate::round_date()"" =     DT, ROUND_TIME_lb := lubridate::round_date(DATE_ymd_hms, paste0(minute, "" mins"")),  # rounding plyr  ""plyr::round_any()"" =    DT, ROUND_TIME_pl := as.ITime(plyr::round_any(as.numeric(DATE_ITime), minute * 60)),  # settings  times = 100L, unit = ""ms"")Unit: milliseconds                    expr       min        lq     mean    median        uq      max neval lubridate::round_date() 24.075546 25.582259 27.27308 26.944734 28.120435 39.62532   100       plyr::round_any()  1.351571  1.533044  1.97845  1.727221  2.249484  4.37735   100On the one hand, round_date() has an easy to understand input format, but on the other hand round_any() is much faster since it works on numeric values.With this setup, I only use lubridate and plyr in one part of my code and for the rest data.table – That makes my inner self.data.table happy and it still is quite fast. With all the changes I have made, I am curious how much the speed has improved.fixing the bottlenecks That is quite an improvement! The code now only takes 4330ms. There are probably even more enhancements possible, but I think that is enough of a speed boost. to sum it all up There are a lot of ways how you can improve and optimise your code. Some changes might be quite settled whilst others demand a change in your way of thinking about a problem. There are also a lot of packages out there which are optimised in doing a specific job (e.g. lubridate). So the question often is not „Can this be improved?“ but more like „Which package is better at this job?“. To be honest, it has taken me quite a while to finish this post. Whilst finding the bottlenecks and fixing them, I also found myself changing parts of the data preparation and adding new data error handling parts. So, the initial code might not be the same as the final one which you can inspect yourself in our github.I still hope this post can help you to improve your own coding or at least gives you some ideas on how to tackle your optimisation problems. Über den AutorJakob GeppNumbers were always my passion and as a data scientist and a statistician at STATWORX I can fullfill my nerdy needs. Also I am responsable for our blog. So if you have any questions or suggestions, just send me an email!.e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/optimising-your-r-code-a-guided-example/;Statworx;  Jakob Gepp
  18. Juni 2019;Data University 2019 – Get your Data Degree;BARC und STATWORX  präsentieren vom 09. – 10. Oktober 2019 die Data University, in Fankfurt am Main. Die Data University ist ein interaktives Workshop- und Trainings-Event mit dem Fokus auf allen wichtigen Themen rund um Daten.Was erwartet dich?Die Data University bietet an 2 Tagen 24 Workshops in 6 parallelen Tracks zu den Themen: Data Strategy, Data Engineering, Data Science und Data Technology. Als Teilnehmer kannst du pro Tag 2 Sessions belegen – jeweils eine Session am Vormittag sowie am Nachmittag. Du stellst dir selbst dein persönliches Programm zusammen!Im Gegensatz zu klassischen Data Konferenzen liegt der Fokus der Veranstaltung klar auf Wissensaufbau und Vermittlung. In kleinen Gruppen von max. 30 Personen erhalten die Teilnehmer neueste Insights von Fachexperten aus der Praxis. Zusätzlich findet in einem Hackathon in gemütlicher Arbeitsatmosphäre ein aktiver Austausch und Networking bei Pizza und Bier statt.Wer sollte teilnehmen??Die Veranstaltung richtet sich gezielt an Data Professionals, die Inspiration, Orientierung oder konkrete Hilfestellung für ihre Arbeit mit Daten suchen. Für Einsteiger und Fortgeschrittene wird es unterschiedliche Trainings geben, die individuell über 4 Tracks zusammengestellt werden können. ?Warum sollte ich teilnehmen??Unternehmen müssen durch die Nutzung ihrer Daten, Digitalisierung und Automatisierung effizienter und produktiver werden. Das gelingt nur mit Mitarbeitern, die in der Lage sind, sich diesen komplexen Themen anzunehmen. Dies wiederum setzt ein fundiertes Wissen in den Bereichen Data Strategy, Data Engineering, Data Science und Data Technology voraus. Die Data University hilft Mitarbeitern und Unternehmen gleichermaßen, das erforderliche Wissen für die Umsetzung von datengetriebenen Produkten und Dienstleistungen aufzubauen.4 Gründe für deine TeilnahmeAustausch: Die Data University vereint Anwender aus verschiedenen Branchen, die sich auf die datengetriebene Zukunft vorbereiten wollen.Best Practices: Erweitere deinen Blickwinkel und lerne von den Best Practices führender Fachexperten. Teile deine eigenen Erfahrungen und diskutiere mit!Mehrwert: Mit Hands-on Workshops und einem Hackathon leistet die Veranstaltung konkrete Hilfestellung für dein Unternehmen. Fachwissen: Gestalte dein individuelles Weiterbildungsprogramm: Vom 09.-10. Oktober bietet die Data University die Wahl aus 24 Workshops, die du dir individuell zusammenstellen kannst. ?Wer sind unsere Speaker??Experten von BARC und STATWORX behandeln praxisnah und Hands-on Probleme aus der alltäglichen Arbeit mit Daten. Außerdem werden externe Speaker von Data University Sponsoren, aus dem Bereich Data Technology, interessante Themen vorstellen. Aktuell suchen wir noch Sponsoren und Technologiepartner für die Veranstaltung im Bereich Cloud Infrastruktur sowie für die Vorstellung von Tools und Technologien in unserem Data Technology Track. Interesse geweckt? Dann sende uns eine Nachricht an: info(at)data-university.de ?Wo findest das Workshop-Event statt??Die Data University 2019 findet im Seminarhaus der Goethe Universität in Frankfurt am Main statt. Für Verpflegung wird gesorgt. Pack deinen Laptop ein und komm nach Frankfurt! Wir freuen uns auf dich!?Unser Ziel.Wir wollen, dass die Teilnehmer am Ende der Veranstaltung mit einem echten Mehrwert zurück in ihr Unternehmen gehen. Bei der Data University wird es keine langwierigen Vorträge oder die hundertste Success Story geben. Bei uns arbeiten die Teilnehmer in kleinen Gruppen mit praxiserfahrenen Trainern an realen Daten und Problemstellungen. Dadurch erweitern die Teilnehmer ihr Wissen und haben die Möglichkeit, das Gelernte in ihrem Arbeitsalltag anzuwenden.Sebastian Heinz, CEO Early Bird WartelisteBald sind die Tickets für die Data University verfügbar. Bis dahin kannst du dir dein Early Bird Ticket sichern! Trage dich jetzt in die Warteliste ein!Sichere dir dein Early Bird Ticket!;https://www.statworx.com/de/blog/data-university-2019/;Statworx;  Vivian Jeenel
  12. Juni 2019;Interactive Network Visualization with R;"Networks are everywhere. We have social networks like Facebook, competitive product networks or various networks in an organisation. Also, for STATWORX it is a common task to unveil hidden structures and clusters in a network and visualize it for our customers. In the past, we used the tool Gephi to visualize our results in network analysis. Impressed by this outstanding pretty and interactive visualization, our idea was to find a way to do visualizations in the same quality directly in R and present it to our customers in an R Shiny app.Our first intention was to visualize networks with igraph, a package that contains a collection of network analysis tools with the emphasis on efficiency, portability and ease of use. We use it in the past in our helfRlein package for the function getnetwork, described in this blog post. Unfortunately, igraph can create beautiful network visualizations, but they’re solely static. To build interactive network visualizations, you can use particular packages in R that are all using javascript libraries.Our favorite package for this visualization task is visNetwork, which uses  vis.js javascript library and is based on htmlwidgets. It’s compatible with Shiny, R Markdown documents and RStudio viewer. visNetwork has many adjustments to personalize your network, a pretty output and good performance, which is very important when using the output in Shiny. Furthermore, you can find excellent documentation here. So let us go through the steps that have to be done from your data basis up till the perfect visualization in R Shiny. To do so, we use the Les Misérables Characters network in the following as an example. This undirected network contains co-occurrences of characters in Victor Hugo’s novel ‚Les Misérables‘. A node represents a character, and an edge between two nodes shows that these two characters appeared in the same chapter of the book. The weight of each link indicates how often such a co-appearance occurred.Data PreparationFirst of all, we have to install the package with install.packages(""visNetwork"") and load the dataset lesmis. You can find the dataset in the package geomnet. To visualize the network between the Les Miserables characters, the package visNetwork needs two data frames. One for the nodes and one for the edges of the network. Fortunately, our loaded data provides both, and we only have to bring them in the right format.rm(list = ls())# Libraries ---------------------------------------------------------------library(visNetwork)library(geomnet)library(igraph)# Data Preparation --------------------------------------------------------#Load datasetdata(lesmis)#Nodesnodes &lt;- as.data.frame(lesmis2)colnames(nodes) &lt;- c(""id"", ""label"")#id has to be the same like from and to columns in edgesnodes$id &lt;- nodes$label#Edgesedges &lt;- as.data.frame(lesmis1)colnames(edges) &lt;- c(""from"", ""to"", ""width"")The following function needs specific names for the columns to detect the right column. For this purpose, edges must be a dataframe with at least one column that indicates in which node an edge starts (from) and where it ends (to). For the nodes, we require at a minimum a unique ID (id) which has to coincide to the from and to entries.Nodes:label: A column that defines how a node is labelledvalue: Defines the size of a node inside the networkgroup: Assigns a node to a group; this can be a result of a cluster analysis or a community detectionshape: Defines how a node is presented. For example as a circle, square or trianglecolor: Defines the color of a nodetitle: Sets the tooltip, which occurs when you hover over a node (this can be HTML or character)shadow: Defines if a node has a shadow or not (TRUE/FALSE)Edges:label, title, shadowlength, width: Defines the length/width of an edge inside the networkarrows: Defines where to set a possible arrow on the edgedashes: Defines if the edges should be dashed or not (TRUE/FALSE)smooth: Smooth lines (TRUE/FALSE)These are the most important settings. They were made for every single node or edge particularly. To set some configurations for all nodes or edges like the same shape or arrows you can do this later when you specify the output with visNodes and visEdges. We’ll show you this possibility later on.Additionally, we want to have a more interesting network with groups inside. We’ll highlight the groups later by adding colors to the edges in the network. Therefore we cluster the data with the community detection method Louvain and get a group column:#Create graph for Louvaingraph &lt;- graph_from_data_frame(edges, directed = FALSE)#Louvain Comunity Detectioncluster &lt;- cluster_louvain(graph)cluster_df &lt;- data.frame(as.list(membership(cluster)))cluster_df &lt;- as.data.frame(t(cluster_df))cluster_df$label &lt;- rownames(cluster_df)#Create group columnnodes &lt;- left_join(nodes, cluster_df, by = ""label"")colnames(nodes)3 &lt;- ""group""Output OptionsTo give you an impression which possibilities we have when it comes to the design and functional options  when creating our output, we will have a more in-depth look at two presentations of the Les Misérables network.We’re starting with the easiest possibility and only give the nodes and edges dataframes to the function:visNetwork(nodes, edges)Using the pipe operator we can customize our network with some other functions like visNodes, visEdges, visOptions, visLayout or visIgraphLayout:visNetwork(nodes, edges, width = ""100%"") %&gt;%  visIgraphLayout() %&gt;%  visNodes(    shape = ""dot"",    color = list(      background = ""#0085AF"",      border = ""#013848"",      highlight = ""#FF8000""    ),    shadow = list(enabled = TRUE, size = 10)  ) %&gt;%  visEdges(    shadow = FALSE,    color = list(color = ""#0085AF"", highlight = ""#C62F4B"")  ) %&gt;%  visOptions(highlightNearest = list(enabled = T, degree = 1, hover = T),             selectedBy = ""group"") %&gt;%   visLayout(randomSeed = 11)visNodes and visEdges describe the overall appearance of the nodes and edges in the network. For example, we can set the shape of all nodes or define the colors of the edges.When it comes to publication in R, rendering the network can take a long time. To deal with this issue, we use the visIgraphfunction. It decreases plotting time while computing coordinates in advance and provides all available igraph layouts.With visOptions we can adjust how the network reacts when we interact with it. For example, what happens if we click on a node.visLayout allows us to define the look of the network. Should it be a hierarchical one or do we want to improve the layout with a special algorithm? Furthermore, we can provide a seed (randomSeed), so that the network always looks the same when you load it.These are only some example functions of how we can customize our network. The package provides much more options for customization. For more details have a look at the documentation.Shiny IntegrationTo present the interactive results to our customers, we want to integrate them into a Shiny app. Therefore we prepare the data „offline“, save the nodes and edges files and create the output inside the Shiny app „online“. Here is a minimal example code of the scheme you can use for Shiny:global.R:library(shiny)library(visNetwork)server.R:shinyServer(function(input, output) {  output$network &lt;- renderVisNetwork({    load(""nodes.RData"")    load(""edges.RData"")    visNetwork(nodes, edges) %&gt;%      visIgraphLayout()  })})ui.R:shinyUI(  fluidPage(    visNetworkOutput(""network"")  ))A screenshot of our Shiny app illustrates a possible result:ConclusionBesides other available packages to visualize networks interactively in R, visNetwork is our absolute favorite. It is a powerful package to create interactive networks directly in R and publish it in Shiny.  We can integrate our networks directly into our Shiny application and run it with a stable performance when using the visIgraphLayout function. We would not need external software like Gephi anymore.Did I spark your interest to visualize your own networks? Feel free to use my code or contact me and visit the github page of the used package here.ReferencesKnuth, D. E. (1993) „Les miserables: coappearance network of characters in the novel les miserables“, The Stanford GraphBase: A Platform for Combinatorial Computing, Addison-Wesley, Reading, MAÜber den AutorNiklas JunkerI am a data scientist at STATWORX. I love to give decision makers insights into their business they will never get without my work. Combining my business administrative background with statistical methods, machine learning and the required data is an incredibly fascinating, creative and varied  job..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/interactive-network-visualization-with-r/;Statworx;  Niklas Junker
  5. Juni 2019;Coding Random Forests in 100 lines of code*;"MotivationThere are dozens of machine learning algorithms out there. It is impossible to learn all their mechanics; however, many algorithms sprout from the most established algorithms, e.g. ordinary least squares, gradient boosting, support vector machines, tree-based algorithms and neural networks. At STATWORX we discuss algorithms daily to evaluate their benefits for a specific project. In any case, understanding these core algorithms is key to most machine learning algorithms in the literature.Why bother writing from scratch?While I like reading machine learning research papers, the maths is sometimes hard to follow. That is why I like implementing the algorithms in R by myself. Of course, this means digging through the maths and the algorithms as well. However, you can challenge your understanding of the algorithm directly.In my last blog posts, I introduced two machine learning algorithms in 150 lines of R Code. You can find the other blog posts about coding gradient boosted machines and regression trees from scratch on our blog or in the readme on my GitHub. This blog post is about the random forest, which is probably the most prominent machine learning algorithm. You have probably noticed the asterisk (*) in the title. These things often suggest, that there has to be something off. Like seeing a price for a cell phone plan in a tv commercial and while reading the fine prints you learn that it only applies if you have successfully climbed Mount Everest and you got three giraffes on your yacht. Also, yes your suspicion is justified; unfortunately, the 100 lines of code only apply if we don’t add the code from the regression tree algorithm, which is essential for a random forest. That is why I would strongly recommend reading the blog about regression trees, if you are not familiar with the regression tree algorithm.Understanding ML with simple and accessible codeIn this series we try to produce very generic code, i.e. it won’t produce a state-of-the-art performance. It is instead designed to be very generic and easily readable.Admittedly, there are tons of great articles out there which explain random forests theoretically accompanied with a hands-on example. That is not the objective of this blog post. If you are interested in a hands-on tutorial with all the necessary theory, I strongly recommend this tutorial. The objective of this blog post is to establish the theory of the algorithm by writing simple R code. The only thing you need to know, besides the fundamentals of a regression tree, is our objective: We want to estimate our real-valued target (y) with a set of real-valued features (X).Reduce dimensionality with Feature ImportanceFortunately, we do not have to cover too much maths in this tutorial, since that part is already covered in the regression tree tutorial. However, there is one part, which I have added in the code since the last blog post. Regression trees and hence random forests, opposed to a standard OLS regression, can neglect unimportant features in the fitting process. This is one significant advantage of tree-based algorithms and is something which should be covered in our basic algorithm. You can find this enhancement in the new reg_tree_imp.R script on GitHub. We use this function to sprout trees in our forest later on.Before we jump into the random forest code, I would like to touch very briefly on how we can compute feature importance in a regression tree. Surely, there are tons of ways on how you can calculate feature importance, the following approach is, however, quite intuitive and straightforward.Evaluating the goodness of a splitA regression tree splits the data by choosing the feature which minimizes a certain criterion, e.g. the squared error of our prediction. Of course, it is possible, that some features will never be chosen for a split, which makes calculating their importance very easy. However, how can we compute importance with chosen features? A first shot could be to count the number of splits for each feature and relativize it by the total number of all splits. This measure is simple and intuitive, but it cannot quantify how impactful the splits were, and this is something we can accomplish with a very simple but more sophisticated metric. This metric is a weighted goodness of fit. We start by defining our goodness of fit for each node. For instance, the mean squared error, which is defined as:This metric describes the average squared error we make when we estimate our target y_i with our predictor the average value in our current node \bar(y)_{node}. Now we can measure the improvement by splitting the data with the chosen feature and compare the goodness of fit of the parent node with the performance of its children nodes. Essentially, this is more or less the exact step we performed to evaluate the best splitting feature for the current node. Weighting the impact of a splitSplits at the top of the tree are more impactful as more data reaches the nodes at this stage of the tree. That’s why it makes sense to lay more importance on earlier splits by taking into account the number of observations which reached this node. This weight describes the number of observations in the current node measured by the total number of observations. Combining the results above, we can derive a weighted improvement of a splitting feature p in a single node as:Quantifying improvements by splitting the data in a regression treeThis weighted improvement is calculated at every node which was split for the respective splitting feature p. To get better interpretability of this improvement, we sum up the improvements for each feature in our tree and normalize it by the overall improvement in our tree.Quantifying the importance of a feature in a regression treeThis is the final feature importance measure used within regression tree algorithm. Again, you can follow these steps within the code of the regression tree. I have tagged all variables, functions and column names involved in the feature importance calculation with imp_* or IMP_* , that should make it a little easier to follow.The Random forest AlgorithmAll right, enough with this regression tree and importance – we are interested in the forest in this blog post. The objective of a random forest is to combine many regression or decision trees. Such a combination of single results is referred to as ensemble techniques. The idea of this technique is very simple but yet very powerful. Building a regression tree orchestraIn a symphonic orchestra, different groups of instruments are combined to form an ensemble, which creates more powerful and diverse harmonies. Essentially, it is the same in machine learning, because every regression tree we sprout in random forest has the chance to explore the data from a different angle. Our regression tree orchestra has thus different views on the data, which makes the combination very powerful and diverse opposed to a single regression tree.Simplicity of a random forestIf you are not familiar with the mechanics algorithm, you probably think that the code gets very complicated and hard to follow. Well, to me the amazing part of this algorithm is how simple and yet effective it is. The coding part is not as challenging as you might think. Like in the other blog posts we take a look at the whole code first and then we go through it bit by bit.The algorithm at one glimpse#' reg_rf#' Fits a random forest with a continuous scaled features and target #' variable (regression)#'#' @param formula an object of class formula#' @param n_trees an integer specifying the number of trees to sprout#' @param feature_frac an numeric value defined between 0,1#'                     specifies the percentage of total features to be used in#'                     each regression tree#' @param data a data.frame or matrix#'#' @importFrom plyr raply#' @return#' @export#'#' @examples # Complete runthrough see: www.github.com/andrebleier/cheapmlreg_rf &lt;- function(formula, n_trees, feature_frac, data) {  # source the regression tree function  source(""algorithms/reg_tree_imp.R"")  # load plyr  require(plyr)  # define function to sprout a single tree  sprout_tree &lt;- function(formula, feature_frac, data) {    # extract features    features &lt;- all.vars(formula)-1    # extract target    target &lt;- all.vars(formula)1    # bag the data    # - randomly sample the data with replacement (duplicate are possible)    train &lt;-      datasample(1:nrow(data), size = nrow(data), replace = TRUE)    # randomly sample features    # - only fit the regression tree with feature_frac * 100 % of the features    features_sample &lt;- sample(features,                              size = ceiling(length(features) * feature_frac),                              replace = FALSE)    # create new formula    formula_new &lt;-      as.formula(paste0(target, "" ~ "", paste0(features_sample,                                              collapse =  "" + "")))    # fit the regression tree    tree &lt;- reg_tree_imp(formula = formula_new,                         data = train,                         minsize = ceiling(nrow(train) * 0.1))    # save the fit and the importance    return(list(tree$fit, tree$importance))  }  # apply the rf_tree function n_trees times with plyr::raply  # - track the progress with a progress bar  trees &lt;- plyr::raply(    n_trees,    sprout_tree(      formula = formula,      feature_frac = feature_frac,      data = data    ),    .progress = ""text""  )  # extract fit  fits &lt;- do.call(""cbind"", trees, 1)  # calculate the final fit as a mean of all regression trees  rf_fit &lt;- apply(fits, MARGIN = 1, mean)  # extract the feature importance  imp_full &lt;- do.call(""rbind"", trees, 2)  # build the mean feature importance between all trees  imp &lt;- aggregate(IMPORTANCE ~ FEATURES, FUN = mean, imp_full)  # build the ratio for interpretation purposes  imp$IMPORTANCE &lt;- imp$IMPORTANCE / sum(imp$IMPORTANCE)  # export  return(list(fit = rf_fit,              importance = imporder(imp$IMPORTANCE, decreasing = TRUE), ))}As you have probably noticed, our algorithm can be roughly divided into two parts. Firstly, a function sprout_tree() and afterwards some lines of code which call this function and process its output. Let us now work through all the code chunk by chunk.# source the regression tree function  source(""algorithms/reg_tree_imp.R"")  # load plyr  require(plyr)  # define function to sprout a single tree  sprout_tree &lt;- function(formula, feature_frac, data) {    # extract features    features &lt;- all.vars(formula)-1    # extract target    target &lt;- all.vars(formula)1    # bag the data    # - randomly sample the data with replacement (duplicate are possible)    train &lt;-      datasample(1:nrow(data), size = nrow(data), replace = TRUE)    # randomly sample features    # - only fit the regression tree with feature_frac * 100 % of the features    features_sample &lt;- sample(features,                              size = ceiling(length(features) * feature_frac),                              replace = FALSE)    # create new formula    formula_new &lt;-      as.formula(paste0(target, "" ~ "", paste0(features_sample,                                              collapse =  "" + "")))    # fit the regression tree    tree &lt;- reg_tree_imp(formula = formula_new,                         data = train,                         minsize = ceiling(nrow(train) * 0.1))    # save the fit and the importance    return(list(tree$fit, tree$importance))  }Sprouting a single regression tree in a forestThe first part of the code is the sprout_tree() function, which is just a wrapper for the regression tree function reg_tree_imp(), which we source as the first action of our code. Then we extract our target and the features from the formula object. Creating different angles by bagging the dataAfterwards, we bag the data, which means we are randomly sampling our data with the chance of replacement. Remember when I said every tree will look at our data from a different angle? Well, this is the part where we create the angles. Random sampling with replacement is just a synonym for generating weights on our observations. This means that a specific observation in the data set of a specific tree could be repeated 10 times. The next tree could, however, lose this observation completely. Furthermore, there is another way of creating different angles in our trees: Feature sampling. Solving collinearity issues between trees in a forestFrom our complete feature set X we are randomly sampling a feature_frac * 100 percent to reduce the dimensionality. With feature sampling we can a) compute faster and b) capture angles on our data from supposedly weaker features as we decrease the value of feature_frac . Suppose, we have some degree of multicollinearity between our features. It might occur, that the regression tree will select only a specific feature if we use every feature in every tree. However, supposedly features with less improvement could bare new valuable information for the model, but are not granted the chance. This is something you can achieve by lowering the dimensionality with the argument feature_frac. If your objective of the analysis is feature selection, e.g. feature importance, you might want to set this parameter to 80%-100% as you will get a more clear cut selection. Well, the rest of the function is fitting the regression tree and exporting the fitted values as well as the importance.Sprouting a forestIn the next code chunk we start applying the sprout_tree() function n_trees times with the help of plyr::raply(). This function repeatedly applies a function call with the same arguments and combines the results in a list. Remember, we do not need to change anything in the sprout_tree() function, since the angles are created randomly every time, we call the function.# apply the rf_tree function n_trees times with plyr::raply  # - track the progress with a progress bar  trees &lt;- plyr::raply(    n_trees,    sprout_tree(      formula = formula,      feature_frac = feature_frac,      data = data    ),    .progress = ""text""  )  # extract fit  fits &lt;- do.call(""cbind"", trees, 1)  # calculate the final fit as a mean of all regression trees  rf_fit &lt;- apply(fits, MARGIN = 1, mean)  # extract the feature importance  imp_full &lt;- do.call(""rbind"", trees, 2)  # build the mean feature importance between all trees  imp &lt;- aggregate(IMPORTANCE ~ FEATURES, FUN = mean, imp_full)  # build the ratio for interpretation purposes  imp$IMPORTANCE &lt;- imp$IMPORTANCE / sum(imp$IMPORTANCE)  # export  return(list(fit = rf_fit,              importance = imporder(imp$IMPORTANCE, decreasing = TRUE), ))Afterwards, we combine the single regression tree fits a data frame. By calculating the row mean we are taking the average fitted value of every regression tree in our forest. Our last action is to calculate the feature importance of our ensemble. That’s the mean feature importance of a feature in all trees normalized by the overall mean importance of all variables.Applying the algorithmLet us apply the function to see, whether the fit is indeed better compared to a single regression tree. Additionally, we can check out the feature importance. I have created a little example on GitHub, which you can check out. First, we simulate data with the Xy package. In this simulation, five linear variables were used to create our target y. To make it a little spicier, we add five irrelevant variables, which were created in the simulation as well. The challenge now, of course, is whether the algorithm will use any irrelevant feature or if the algorithm can perfectly identify the important features. Our formula is:eq &lt;- y ~ LIN_1 + LIN_2 + LIN_3 + LIN_4 + LIN_5 + NOISE_1 + NOISE_2 + NOISE_3 + NOISE_4 + NOISE_5The power of the forestNeither the random forest nor the regression tree has selected any unnecessary features. However, the regression tree was only split by the two most important variables. Whereas, the random forest selected all five relevant features.This does not mean that the regression tree is not able to find the right answer. It depends on the minimum observation size (minsize) of the tree. Surely the regression tree would eventually find all important features if we lower the minimum size. The random forest, however, found all five essential features with the same minimum size. Learning strengths and weaknessesOf course, this was only one example. I would strongly recommend playing around with the functions and examples by yourself because only then you can get a feel for the algorithms and where they shine or fail. Feel free to clone the GitHub repository and play around with the examples. Simulation is always nice because you get a clearcut answer; however, applying the algorithms to familiar real-world data might be beneficial to you as well.Über den AutorAndré BleierThe most exciting part of being a data scientist at STATWORX is to find this unique solution to a problem by fusing machine learning, statistics, and business knowledge..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/coding-random-forests-in-100-lines-of-code/;Statworx;  André Bleier
  17. Mai 2019;6 myths about refuelling – tackled with statistics;"On my daily commute to STATWORX, I pass the gas station down the street twice a day. Even though I’m riding a bike, I’m still looking at the gas prices. Lately, after a few weeks of observation, I thought to have found some patterns. My statistic sensors were tickling!  Of course, I am not the first one to notice patterns. Nearly everyone has their own theory about when to buy cheap gas! Some of the more famous examples are: Everything was better in the old daysIt’s cheaper in the eveningGas stations at motorways are more expensiveNo-name stations have lower prices than top brandsThe difference between diesel and gas (E10) is shrinkingIt’s cheapest on MondaysBut, are these facts or just rumours? To check all those myths, I gathered some data, explored relations and finally plotted my results!FYI:  The code I used will be presented in a future blog, when I also will take a deeper look at my coding and optimisation progress – stay tuned!The king of gas price data Since December 2013 all gas stations in Germany must report their prices to the Markttransparenzstelle für Kraftstoffe, which is part of the Bundeskartellamt. While searching for this data, I stumbled upon the site tankerkoenig.de. They had gathered the price data for nearly all gas stations since 2014. First, they only provided a data bank dump with all historical data, but since October 2018, they also offer a git repository with daily csv files. The data contain all price changes of around 15.000 gas stations, looking as follows:                  date station_uuid diesel    e5   e101: 2019-01-01 00:01:06     2ae0e4dc  1.319 1.399 1.3792: 2019-01-01 00:01:06     08a67e2e  1.339 1.409 1.3893: 2019-01-01 00:01:06     939f06e5  1.299 1.379 1.3594: 2019-01-01 00:01:06     d299da0e  1.279 1.389 1.3695: 2019-01-01 00:01:06     a1e15688  1.228 1.328 1.308In addition to the price information, tankerkoenig.de also provides additional information like the brand, the geo-coordinates and the city of each gas station. Since I was also interested in the fact whether a station is near a motorway or not, I included this data from the ADAC (the German automobile club). Of course, this did not seem to be complete, but good enough for my purpose. With the result, I was able to plot the stations, and the motorways emerged!If you want to know more about how to make plots with geo-coordinates and ggplot2, check out the blog post of my colleague Lea!Only believe the facts! To confirm or debunk the myths I chose, I went ahead and filtered, aggregated, and merged the raw data. Be prepared for some nice plots and insights!1. Everything was better in the old days Nothing shows a development better than a good old timeline! As you can see in the following graphic, I plotted the mean price per day. To get a little more insight about the price ranges, I included the 10% and 90% quantiles of prices for each day.Of course, this was just a glimpse, and there were times (many years back) when the price was below 1DM (~0.5€). But for the last five years, the price was kind of within the same range. There was a low point early in 2016, but the latest peak was still a bit lower than in mid 2014. Conclusion: For the „short-term“ old days the myth does not hold up – for the „long-run“ … probably yes! 2. It’s cheaper in the evening To check this myth, I aggregated the data over days and years. I noticed that there were different patterns over the years. From 2014 till 2017, there seemed to be a pattern where the price was high in the morning, slowly decreased over the day, with a small peak at noon, and rose up again in the evening. Since 2018, a new pattern with three peaks in a wave pattern emerged: The highest one in the morning around 7:00, one at noon and the last one around 17:00. Since the price level varied a bit over the years, I also looked at the scaled prices. Here, the difference between the patterns was even more visible.Conclusion: From 2015 to 2017, the myth seemed to be right. However, lately there were multiple „good times“ across a day. Mostly right before the peak in the afternoon or the evening. As the price did not rise that much after its evening peak, the myth is still correct. The lowest point at the moment seemed to be around 21:00.Update: While writing this post I found a recent study from the Goethe university of Frankfurt, that shows, that there is a new pattern with even more peaks! 3. Gas stations at motorways are more expensive In the plot above I already showed where the motorway stations are – the blue dots. In the next plot, the blue lines represent the prices at those motorway stations. One can clearly spot the difference! Even though there are some stations within the 10% and 90% quartile range where the prices overlapped.Conclusion: Obviously right!4. No-name stations have lower prices than top brandsTo get the top brands, I took the nine brands with the most stations in Germany and labelled the rest „other“. After that, I calculated the difference between mean price and mean price per brand. Therefore, lines above zero indicated that a brand is more expensive than the mean and vice versa.. Of the top brands, ARAL and SHELL were always above the mean, some fluctuated around the mean and some were always below the mean. The no-name stations were cheaper than the mean over the whole timespan.Conclusion: The result depends on how a no-name station is defined. Since the top five brands are mostly more expensive than the mean, I think the myth is somewhat right. At least, it is safe to say that there are differences between the brands.5. The difference between diesel and gas (E10) is shrinking To compare the differences, I took the E10 price as the baseline and plotted the other two types against it. Between E5 and E10 there was nearly no change over the last five years. However, for diesel, it was a story. Since 2014, the difference took a wild ride from 10% cheaper than E10 to 20% cheaper at the beginning of 2016 back to only 5% now. Conclusion: Yes, the difference is shrinking at the moment compared to the last five years.6. It’s cheapest on Mondays To check the last myth, I calculated the mean price per hour, type and weekday. The weekday with the lowest price at a given time can be seen in the following plot. All gas types revealed a similar pattern. Between the years, there was a subtle change mostly between Wednesday and Thursday.Conclusion: Since Monday barely ever was the cheapest day – this myth got debunked!The results are inAfter all of that data hacking and pattern finding, these are my results:Everything was better in the old days – FALSEishIt’s cheaper in the evening – TRUEGas stations at motorways are more expensive – TRUENo-name stations have lower prices than top brands – TRUEishThe difference between diesel and gas (e10) is shrinking – TRUEIt’s cheapeast on Mondays – FALSE Of course, this analysis is far from being complete and even had some shortcuts in it, as well. Still, I had a lot of fun analysing this big data set regardless of all the internal fights with my RAM. I will present some of these fights and their outcome in my next blog post. In that post, you will also get more information about my code optimisation process.In another upcoming blog post, my colleague Matthias will use this data to predict the future price. So, stay tuned! References the data is used under the Creative-Commons-Lizenz (CC BY 4.0).For more information on the data check out the tankerkoenig website.Über den AutorJakob GeppNumbers were always my passion and as a data scientist and a statistician at STATWORX I can fullfill my nerdy needs. Also I am responsable for our blog. So if you have any questions or suggestions, just send me an email!.e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/6-myths-about-refuelling-tackled-with-statistics/;Statworx;  Jakob Gepp
  15. Mai 2019;Data Science lernen im STATWORX Bootcamp;"Neben unseren Beratungsaufgaben haben wir von STATWORX einen stetig wachsenden Academy Bereich. Dieser Bereich bietet zu verschiedenen Themengebieten und Programmiersprachen Schulungen. Dies umfasst Kurse in Python oder R und auch zu speziellen Frameworks wie zum Beispiel Tensorflow. Ein Blick auf unser Angebot lohnt sich!Überblick der Bootcamp ThemenDerzeit findet bei uns im Frankfurter Office das Data Science Bootcamp statt. Dieses deckt innerhalb von fünf Tagen alle wichtigen Data Science Themen von der Planung bis zur Durchführung ab. Der Fokus liegt nicht nur auf dem Lernen von Algorithmen und statischen Methoden, sondern es werden auch die Bereiche Data Strategy, Data Preparation and Exploration und das Deployment behandelt.Data Strategy umfasst unter anderem die Themen Teamorganisation sowie die optimale Zusammensetzung eines Data Science Teams. An Hand von Case Studies veranschaulichen wir die dahinter liegenden Konzepte. Das Deployment behandelt die Kommunikation der Ergebnisse über Dashboards sowie das Integrieren von Modellen in produktive Umgebungen. Die Themen behandeln damit jegliche Bereiche eines Data Science Projektes, vom ersten Erstellen der Daten-Pipelines über die Erstellung von Modellen bis zur letztendlichen Implementierung im laufenden Betrieb ab. Alle Themen werden stets praxisrelevant behandelt.Sneak Peek – Data Preparation and ExplorationAm zweiten Tag geht es um das Thema Data Preparation and Exploration, also darum zuverstehen, woher Daten für ein Data Science Projekt kommen und wie diese Daten extrahiert, aufbereitet sowie exploriert werden können. Neben der technischen Implementierung in R und Python sowie dem Verständnis von Datenbanken, sollen die Teilnehmer vor allem diese drei Punkte mitnehmen:Die Exploration der Daten sollte ein integraler Teil eines jeden Projektes sein. Auch trotz – oder gerade wegen – komplexer Algorithmen ist es wichtig, sich eine gute Übersicht über die Verteilung und Form der Daten zu verschaffen. Die Exploration der Daten hilft in erster Linie die Daten zu verstehen, um daraus neue Features zu bilden, Ausreißer zu erkennen und fehlerhafte oder fehlende Daten zu identifizieren. Weiterhin hilft die Exploration dabei Daten gewinnbringend zu transformieren: Bei der Anwendung von Algorithmen kann zum Beispiel durch die Standardisierung der Daten, die Trainingsdauer vermindert werden. Ein großer Teil der Arbeit eines Data Scientisten ist es, Daten in einer für das Modell lesbare Form aufzubereiten. Auch wenn es einen dedizierten Data Engineer gibt, der die Datenbank verwaltet und die Daten „sauber“ sind, gibt es Schritte die vom Data Scientist durchgeführt werden. Sei es das Bilden von Lags in Zeitreihenmodellen oder das Erstellen aufwendigerer Features. Diese Schritte sind unabdingbar für das Gelingen eines Projektes. Dabei ist es wichtig zu verstehen, dass die Datenaufbereitung in realen Projekten häufig sehr viel komplexer ist, als es aus den Beispieldatensätzen im akademischen Bereich bekannt ist. Der sichere Umgang mit dem Objekttyp „Data Frame“ ist ein Muss, wenn in einem Projekt mit R und Python gearbeitet wird. In Python werden Data Frames durch pandas eingeführt und in R sind diese bereits nativ enthalten. Sie erlauben es im Gegensatz zu Matrizen verschiedene Datentypen in einem Objekt zu speichern, was diese zum perfekten Objekttyp im Data Science Bereich macht. Das schnelle selektieren bestimmter Zeilen, die Auswahl von Spalten oder die Zusammenführung verschiedener Data Frames ermöglicht es dabei schnell einen Überblick über die Daten zu bekommen und diese zum Beispiel in eine Trainings- und Testmenge zu unterteilen. Im Bootcamp wird dafür mit  dplyr für R und pandas für Python gearbeitet.Interesse geweckt, aber Termin verpasst?Das aktuelle Bootcamp ist natürlich nicht das Erste und auch nicht das Letze von STATWORX. Wessen Interesse nun geweckt wurde, kann sich in diesem Jahr finden noch für mehrere Termine in Frankfurt und Zürich anmelden. Eine Übersicht der Termine haben wir hier zusammen gestellt. Es kann dabei jeder mit Interesse am Thema Data Science teilnehmen, da Programmierkenntnisse zwar hilfreich sein können, aber keine Voraussetzung sind!Über den AutorMartin AlbersI am a data scientist at STATWORX and I like everything that has to do with analysis and visualization..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/data-science-lernen-im-statworx-bootcamp/;Statworx;  Martin Albers
  10. Mai 2019;Automated creation of Docker containers;"In the last Docker tutorial Olli presented how to build a Docker image of R-Base scripts with rocker and how to run them in a container. Based on that, I’m going to discuss how to automate the process by using a bash/shell script. Since we usually use containers to deploy our apps at STATWORX, I created a small test app with R-shiny to be saved in a test container. It is, of course, possible to store any other application with this automated script as well if you like. I also created a repository at our blog github, where you can find all files and the test app.Feel free to test and use any of its content. If you are interested in writing a setup script file yourself, note that it is possible to use alternative programming languages such as python as well.the idea behind it$ docker-machine lsNAME          ACTIVE   DRIVER       STATE     URL   SWARM   DOCKER    ERRORSDataiku       -        virtualbox   Stopped                 Unknown   default       -        virtualbox   Stopped                 Unknown   ShowCase      -        virtualbox   Stopped                 Unknown   SQLworkshop   -        virtualbox   Stopped                 Unknown   TestMachine   -        virtualbox   Stopped                 Unknown   $ docker-machine start TestMachineStarting ""TestMachine""...(TestMachine) Check network to re-create if needed...(TestMachine) Waiting for an IP...Machine ""TestMachine"" was started.Waiting for SSH to be available...Detecting the provisioner...Started machines may have new IP addresses. You may need to re-run the `docker-machine env` command.$ eval $(docker-machine env --no-proxy TestMachine)$ docker ps -aCONTAINER ID        IMAGE               COMMAND             CREATED             cfa02575ca2c        testimage           ""/sbin/my_init""     2 weeks ago         STATUS                            PORTS                    NAMESExited (255) About a minute ago   0.0.0.0:2000-&gt;3838/tcp   testcontainer$ docker start testcontainertestcontainer$ docker ps...Building and rebuilding Docker images over and over again every time you make some changes to your application can get a little tedious at times, especially if you type in the same old commands all the time. Olli discussed the advantage of creating an intermediary image for the most time-consuming processes, like installing R packages to speed things up during content creation. That’s an excellent practice, and you should try and do this for every project viable. But how about speeding up the containerisation itself? A small helper tool is needed that, once it’s written, does all the work for you.the tools to useTo create Docker images and containers, you need to install Docker on your computer. If you want to test or use all the material provided in this blog post and on our blog github, you should also install VirtualBox, R and RStudio, if you do not already have them. If you use Windows (10) as your Operating System, you also need to install the Windows Subsystem for Linux. Alternatively, you can create your own script files with PowerShell or something similar.The tool itself is a bash/shell script that builds and runs docker containers for you. All you have to do to use it, is to copy the docker_setup executable into your project directory and execute it. The only thing the tool requires from you afterwards is some naming input.If the execution for some reason fails or produces errors, try to run the tool via the terminal.source ./docker_setupTo replicate or start a new bash/shell script yourself, open your preferred text editor, create a new text file, place the preamble #!/bin/bash at the very top of it and save it. Next, open your terminal, navigate to the directory where you just saved your script and change its mode by typing chmod +x your_script_name. To test if it works correctly, you can e.g. add the line echo 'it works!' below your preamble.  #!/bin/bashecho 'it works!'If you want to check out all available mode options, visit the wiki and for a complete guide visit the Linux Shell Scripting Tutorial.the code that runs itIf you open the docker_setup executable with your preferred text editor, the code might feel a little overwhelming or confusing at first, but it is pretty straight forward.#!/bin/bash# This is a setup bash for docker containers!# activate via: chmod 0755 setup_bash or chmod +x setup_bash# navigate to wd docker_contents# excute in terminal via source ./setup_bashecho """"echo ""Welcome, You are executing a setup script bash for docker containers.""echo """"echo ""Do you want to use the Default from the global configurations?""echo """"source global_conf.shecho ""machine name = $machine_name""echo ""container = $container_name""echo ""image = $image_name""echo ""app name = $app_name""echo ""password = $password_name""echo """"docker-machine lsecho """"read -p ""What is the name of your docker-machine default? "" machine_nameecho """"if  ""$(docker-machine status $machine_name 2&gt; /dev/null)"" == """" ; then    echo ""creating machine..."" \        &amp;&amp; docker-machine create $machine_nameelse    echo ""machine already exists, starting machine..."" \        &amp;&amp; docker-machine start $machine_namefiecho """"echo ""activating machine...""eval $(docker-machine env --no-proxy $machine_name)echo """"docker ps -aecho """"read -p ""What is the name of your docker container? "" container_nameecho """"docker image lsecho """"read -p ""What is the name of your docker image? (lower case only!!) "" image_nameecho """"The main code structure rests on nested if statements. Contrary to a manual docker setup via the terminal, the script needs to account for many different possibilities and even leave some error margin. The first if statement for example – depicted in the picture above – checks if a requested docker-machine already exists. If the machine does not exist, it will be created. If it does exist, it is simply started for usage. The utilised code elements or commands are even more straightforward. The echo command returns some sort of information or a blank for better readability. The read command allows for user input to be read and stored as a variable, which in return enters all further code instances necessary. Most other code elements are docker commands and are essentially the same as the ones entered manually via the terminal. If you are interested in learning more about docker commands check the documentation and Olli’s awesome blog post.the git repositoryThe focal point of the Git Repository at our blog github is the automated docker setup, but also contains some other conveniences and hopefully will grow into an entire collection of useful scripts and bashes. I am aware that there are potentially better, faster and more convenient solutions for everything included in the repository, but if we view it as an exercise and a form of creative exchange, I think we can get some use out of it.The docker_error_logs executable allows for quick troubleshooting and storage of log files if your program or app fails to work within your docker container.The git_repair executable is not fully tested yet and should be used with care. The idea is to quickly check if your local project or repository is connected to a corresponding Git Hub repository, given an URL address, and if not to eventually ‚repair‘ the connection. It can further manage git pulls, commits and pushes for you, but again please use carefully.the next project to comeAs mentioned, I plan on further expanding the collection and usefulness of our blog github soon. In the next step I will add more convenience to the docker setup by adding a separate file that provides the option to write and store default values for repeated executions. So stay tuned and visit our STATWORX Blog again soon. Until then, happy coding.Über den AutorStephan EmmerI am a data scientist at STATWORX and to work with data in a professional way on a day to day basis is just AWESOME!.e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/automated-creation-of-docker-containers/;Statworx;  Stephan Emmer
  26. April 2019;Frankfurt Data Science Meetup: Opening the Black Box;"The place to be: Frankfurt Data Science MeetupWe at STATWORX love meetups: mingle with other data scientists, gain insights into their newest projects and approaches while having a beer… what’s not to like?That’s why STATWORX supports the Frankfurt Data Science Meetup. However, while STATWORX chips in some beer money, the real work is done by the organizers of the meetup, who are doing a terrific job. A big thanks to everyone!Yesterday’s meetup was extra special for us since our Head of Data Science Fabian Müller gave a talk on explainable machine learning. With data science growing more and more important in almost all fields of business, “cracking open the black box” is a matter of growing relevance. So, do yourself a favor and take a look at Fabian’s slides and code on github. (Even if you’re not that interested in the content, the memes and cute animal pictures are definitely worth it, believe me!) The takeaways: How and why to open the black boxAs an appetizer, here are three of my most important takeaways: It’s not only cool to understand your model but it’s also the responsible thing to do. Because our models are trained on historical data, they mirror historical circumstances. Thus, model-based decisions might perpetuate discriminatory (social) structures. On top, understanding our models helps us to make them more performant. If we use model agnostic methods, which only use the model’s predictions, we can separate learning from explaining. Such model independent methods even enable comparisons across different classes of models. Also, there are many possible angles: We can focus on a single prediction and, e.g. explore what-if scenarios, decompose predictions or identify key features. Alternatively, we can focus on the model as a whole and analyze its structure or performance. Many methods to open the black box even apply to extremely complex models, e.g. CNN’s. The best thing: no matter how complex the model, most methods are intuitively understandable. You might need the help of a neat plot or two, but even data science non-specialists will get the gist.I hope to have motivated you to check out more talks on the FFM Data Science Youtube channel. or, even better, come to the next Meetup in May. We’d love to see you! About the authorLea WaniekI am a data scientist at STATWORX, apart from machine learning, I love to play around with RMarkdown and ggplot2, making data science beautiful inside and out..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/frankfurt-data-science-meetup-opening-the-black-box/;Statworx;  Lea Waniek
  24. April 2019;R oder Python;"Mein Blogbeitrag zielt auf Data Science Einsteiger ab, die vor der Wahl stehen, welche Programmiersprache sie als Erstes lernen wollen. Wir bei STATWORX arbeiten mit den zwei beliebtesten Sprachen R und Python. Beide Sprachen haben ihre Stärken und Schwächen, weshalb man idealerweise beide beherrschen sollte. Für den Einstieg empfehlen wir eine Sprache zu erlernen und sich dann in der anderen fortzubilden. Um die Entscheidung zu erleichtern, mit welcher Programmiersprache man beginnen möchte, stelle ich Euch beide vor und vergleiche sie anschließend miteinander. Überblick R und PythonSowohl Python als auch R sind Open-Source-Programmiersprachen. Das bedeutet, dass die Quellcodes öffentlich zugänglich sind und gratis verwendet werden können. Während Python eine General Purpose Programmiersprache (Allzwecksprache) ist, wurde R für statistische Analysen entwickelt. Daher weisen die Nutzer der Sprachen oftmals unterschiedliche Hintergründe auf. Verallgemeinernd kann man sagen, dass Softwareentwickler Python nutzen und Statistiker R. RPythonVeröffentlichung19931991EntwicklerR Core TeamPython Software FoundationPackage ManagementCRANConda (empfohlen für Einsteiger)Eine Fülle an ErweiterungenBeide Sprachen verfügen über einen Grundstock an Funktionen, die mit Paketen (packages) erweitert werden können. Das Comprehensive R Archive Network (CRAN) ist eine Plattform für R Pakete. Um ein Paket auf CRAN bereit zu stellen, müssen eine ganze Reihe an Richtlinien eingehalten werden. CRAN gewährleistet dadurch, dass alle Pakete, die dort zum Download zur Verfügung stehen auch tatsächlich funktionieren. Insgesamt stehen auf CRAN 10.000 Pakete zur Verfügung. Da R die Standard-Sprache für Statistiker ist, findet man in CRAN für fast jedes Problem im Bereich Statistik eine passende Lösung. Es ist also genau die richtige Anlaufstelle für die neuesten statistischen Methoden und Analysen. Bei Python gibt es zwei Paket-Verwaltungsplattformen: conda und PyPI (Python Package Index). Auch für Python gibt es über 10.000 Pakete, die im Gegensatz zu R einen sehr breiten Anwendungsbereich abdecken. Da es zu Komplikationen kommen kann, wenn Python Pakete global installiert werden, nutzt man dafür virtuelle Umgebungen. Die sorgen für reibungslose Abläufe innerhalb der verschiedenen Pakete und bei Abhängigkeiten von Paket zu Paket. Für Anfänger ist es daher nicht so einfach, sich da zurecht zu finden.Mit Hilfe von Paketen besteht die Möglichkeit in R Python Code auszuführen sowie vice versa. Falls dich das interessiert, check den Blogbeitrag von meinem Kollegen Manuel ab. Er stellt das Paket reticulate vor.IDEs als HilfestellungProgrammierer nutzen oftmals eine integrierte Entwicklungsumgebung (IDE), die ihnen die Arbeit durch kleine aber feine Hilfsmittel erleichtert. Für R Nutzer hat sich RStudio als Standard-IDE durchgesetzt. Die IDE wird vom gleichnamigen Unternehmen vertrieben, das kommerziell hinter R steht. RStudio bietet nicht nur ein angenehmes Arbeitsumfeld, sondern entwickelt auch aktiv Pakete und Erweiterungen für die R Sprache. Vom RStudio-Team stammen beispielsweise wichtige Pakete wie tidyverse, packrat und devtools sowie beliebte Erweiterungen wie shiny (Dashboards) und RMarkdown (Berichte). Python Nutzer haben die Wahl zwischen verschiedenen IDEs (PyCharm, Visual Studio Code, Spyder, …). Allerdings gibt es kein Unternehmen, das hinter Python steht und vergleichbar mit RStudio wäre. Dennoch werden dank der Bemühungen der riesigen Community und der Python Software Foundation ständig neue Erweiterungen für Python zusammengestellt.Die Kunst der DatenvisualisierungDie meist verwendeten Pakete für Datenvisualisierung mit Python sind matplotlib und seaborn. Dashboards lassen sich in Python mit dash erstellen. Aber R hat bei der Datenvisualisierung einen Trumpf im Ärmel: Das Paket ggplot2, das auf dem Buch The Grammar of Graphics von Leland Wilkinson basiert. Mit diesem Paket kannst Du ansprechende und maßgeschneiderte Grafiken erstellen, die Du wiederum auf Dashboards mit Hilfe von shiny für Andere zugänglich machen kannst. Beide Programmiersprachen bieten die Möglichkeit, schöne Grafiken leicht zu erstellen. Trotzdem überzeugt das R Paket ggplot2 mit seiner Flexibilität und seinen visuellen Möglichkeiten.Pluspunkte für LesbarkeitPython wurde nach dem Motto Readability counts konzipiert. Somit können auch Leute, die nicht mit der Programmiersprache vertraut sind, interpretieren was im Code gemacht wird. Das ist in R Code eher nicht der Fall. Die Sprache ist weniger intuitiv aufgebaut als Python. Aufgrund der guten Lesbarkeit bietet Python daher einen leichteren Einstieg ins Programmieren. Schnelligkeit in verschiedenen ObservationsgrößenAls nächstes vergleiche ich, wie lange es dauert in R und Python einen simulierten Datensatz zu erstellen. Für eine faire Gegenüberstellung sollten die Bedingungen möglichst gleich sein. Die Daten werden mit den Paketen Xy und XyPy in R und Python respektive simuliert. Für die Zeitmessung habe ich microbenchmark in R und timeit in Python benutzt. Um die Simulation schnellstmöglich zu generieren, wird der Prozess parallelisiert auf acht Kernen (R: parallel, Python: multiprocessing). Für das Experiment wird ein Datensatz mit 100 Observationen und 50 Variablen 100 Mal simuliert. Die Zeit, die der Rechner benötigt, um die Simulation durchzuführen, wird für jede Simulation einzeln gemessen. Und das wird dann für 1.000, 10.000, 100.000 und 1.000.000 Observationen wiederholt. Die R und Python Code Snippets sind unten abgebildet. # R# devtools::install_github(""andrebleier/Xy"")# install.packages(""parallel"")# install.packages(""microbenchmark"")# Load packageslibrary(Xy)library(microbenchmark)library(parallel)# Extract function definition from for loopsim_this &lt;- function(n_sim) {  sim &lt;- microbenchmark(Xy(n = n_sim,                           numvars = c(50,0),                           catvars = 0),                         times = 100, unit = ""s"")  data.frame(n = n_sim,              mean = summary(sim), 4)}# Time measurement for different number of simulationsn_sim &lt;- c(1e2, 1e3, 1e4, 1e5, 1e6)sim_in_r &lt;- data.frame(n = rep(0, length(n_sim)),                       t = rep(0, length(n_sim)))for(i in 1:length(n_sim)){  out &lt;- mclapply(n_simi,                  FUN = sim_this,                  mc.cores = 8)  sim_in_ri, 1 &lt;- out11  sim_in_ri, 2 &lt;- out12}# Python# In terminal: pip install xypyimport multiprocessing as mpimport numpy as npimport timeitfrom xypy import Xy# Predefine function of interestdef sim_this(n_sim):  return(timeit.timeit( lambda: Xy(n = int(n_sim),      numvars = 50, 0,      catvars = 0, 0,      weights = 5, 10,      stn = 4.0,      cor = 0, 0.1,      interactions = 1,      noisevars = 5), number = 100))# Paralleled computation pool = mp.Pool(processes = 8)n_sim = np.array(1e2, 1e3, 1e4, 1e5, 1e6)results = pool.map(sim_this, n_sim)Die durchschnittliche Dauer, sortiert nach Datensatzgröße, wird für R und Python im unteren Plot dargestellt. Die X-Achse wird hier auf einer logarithmischen Skala mit Basis 10 dargestellt, um die Grafik übersichtlicher zu machen.Während R bei einer Datensatzgröße von 100 und 1.000 Observationen etwas schneller ist, hängt Python R bald darauf deutlich ab. Für weitere Vergleiche kann ich die folgenden STATWORX Blogbeiträge empfehlen: pandas vs. data.table und pandas vs. data.table part 2, dabei wird der Fokus auf Datenmanipulation gelegt.Der Standard bei Deep LearningWenn Dich vor allem Deep Learning Methoden interessieren, eignet sich Python als Sprache besser. Die meisten Deep Learning Bibliotheken wurden in Python geschrieben und implementiert. Auch in R ist Deep Learning möglich, aber die R Deep Learning Community ist deutlich kleiner. Implementationen wie Keras und TensorFlow lassen sich zwar auch in R aufrufen, dies läuft dann aber über Pakete von Drittanbietern. Die Pakete bieten daher nicht die volle Flexibilität für die Nutzer, z.B. sind nicht alle TensorFlow Funktionen erhältlich. Zu dem kommt der Aspekt der Schnelligkeit. Deep Learning mit Python ist schneller als mit R.Umfrage in der Community: Wie ticken die Anwender?Als angehende Data Scientists ist Kaggle eine wichtige Plattform für Euch. Dort kann man an spannenden Machine Learning Wettkämpfen teilnehmen, selbst experimentieren und aus den Erfahrungen der Community lernen. 2018 hat Kaggle eine Machine Learning &amp; Data Science Umfrage durchgeführt. Die Umfrage war zwei Wochen lang online und es gingen insgesamt 23.859 Antworten ein. Aus den Ergebnissen dieser Umfrage habe ich verschiedene Plots erstellt, aus denen sich einige interessante Schlüsse im Hinblick auf mein Blogthema ziehen lassen. Der Code zu den einzelnen Plots ist öffentlich zugänglich auf Github.Exkurs: Python &amp; R im Vergleich zu anderen SprachenBevor wir uns auf R und Python stürzen, schauen wir uns an, wie die beiden im Vergleich zu anderen Programmiersprachen abschneiden. Jeder Umfragenteilnehmer gab an, welche Sprache er vorrangig benutzt. Im unteren Plot wurde nach Sprache aggregiert und das Ergebnis lautet: Die große Mehrzahl der Teilnehmer benutzt vor allem Python! Gefolgt von R auf dem zweiten Platz. In dieser Umfrage unterscheiden wir nicht zwischen den Arbeitsbereichen, weshalb Python – als General Purpose Programmiersprache – vermutlich so stark hervorsticht.Die Gegenüberstellung R &amp; PythonIm direkten Vergleich zwischen R und Python sieht man, dass sehr viele R-Nutzer auch Python benutzen. Wohingegen die Python-Nutzer oftmals ausschließlich mit Python arbeiten.Wenn man die Nutzung der Sprachen nach Arbeitsbereich vergleicht, sieht man eine klare Dominanz von Python. In allen Arbeitsfeldern, bis auf Statistiker, wird mehrheitlich Python benutzt. Die Teilnehmer wurden außerdem gefragt: Welche Sprache empfiehlst Du angehenden Data Scientists zuerst zu lernen? Die Antworten auf die Frage sind in der unteren Tabelle zusammengefasst. Sprache   EmpfehlungNutzerDifferenzPython14.1818.1806.001R2.3422.046296SQL9141.211-297C++339739-400Matlab256355-99Java184903-719Scala74106-32Javascript72408-336SAS69228-159VBA38135-97Go2646-20Other16111744Wenn man die Anzahl Empfehlungen und die Anzahl Nutzer vergleicht, dann sieht man, dass R und Python die einzigen Sprachen sind, die eine positive Differenz aufweisen.Auch bei dieser Frage liegt Python (14.181) wieder weit vor R (2.342).FazitEine Sache vorweg: beide Sprachen sind sehr mächtig. Daher kann man keine falsche Wahl treffen! Die Wahl der Sprache hängt davon ab, welche Projekte man verwirklichen möchte. Als universelle Programmiersprache ist Python für diverse Anwendungsgebiete geeignet. Weshalb ich Dir grundsätzlich empfehle mit Python anzufangen. Falls aber statistische Auswertungen oder Datenvisualisierungen bei Deinen Projekten im Vordergrund stehen, hat R gegenüber Python einen Vorteil. Wie schon erwähnt haben beide Sprachen ihre Vor- und Nachteile. Als fortgeschrittener Data Scientist solltest Du idealerweise beide Sprachen beherrschen.Ich hoffe, dass Dir dieser Beitrag bei der Suche nach dem richtigen Einstieg in die Data Science Welt weiterhilft. Happy Coding!Falls Du Interesse an Schulungen hast, kannst du dir gerne unter STATWORX Academy unsere Kurs Kataloge für R und Python durchschauen.Referenzenhttps://cran.r-project.org/https://www.python.org/http://wiki.c2.com/?PythonPhilosophyhttps://plot.ly/https://www.rstudio.com/https://pipenv.readthedocs.io/en/latest/https://conda.io/en/latest/https://www.kaggle.comhttps://www.springer.com/us/book/9780387245447https://www.statworx.com/de/academy/Über den AutorFran PericBeing confronted with challenging riddles is what I like about my job as a data scientist at STATWORX. Lets get creative!.e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/r-oder-python/;Statworx;  Fran Peric
  5. April 2019;Open Workshop: Programming with Python;We at STATWORX are opening our doors for anyone who wants to learn more about data.Our open workshop is designed for Python-beginners and provides the perfect mixture between theory and practice. Participants achieve first insights into data science and programming with Python. The workshop will be held in German at our office in Frankfurt. There is a limit of 8 persons per workshop to respond to every participant in the best possible way.Register now!;https://www.statworx.com/de/blog/open-workshop-programming-with-python-may-6th-and-7th-in-frankfurt/;Statworx;  Vivian Jeenel
  5. April 2019;Open Workshop: Programming with R;We at STATWORX are opening our doors for anyone who wants to learn more about data.Our open workshop is designed for R-beginners and provides the perfect mixture between theory and practice. Participants achieve first insights into data science and programming with R. The workshop will be held in German at our office in Frankfurt. There is a limit of 8 persons per workshop to respond to every participant in the best possible way.Register now!;https://www.statworx.com/de/blog/open-workshop-programming-with-r-may-8th-and-9th-in-frankfurt/;Statworx;  Vivian Jeenel
  22. März 2019;How to Speed Up Gradient Boosting by a Factor of Two;"IntroductionAt STATWORX, we are not only helping customers find, develop, and implement a suitable data strategy but we also spend some time doing research to improve our own tool stack. This way, we can give back to the open-source community.One special focus of my research lies in tree-based ensemble methods. These algorithms are bread and butter tools in predictive learning and you can find them as a standalone model or as an ingredient to an ensemble in almost every supervised learning Kaggle challenge. Renowned models are Random Forest by Leo Breiman, Extremely Randomized Trees (Extra-Trees) by Pierre Geurts, Damien Ernst &amp; Louis Wehenkel, and Multiple Additive Regression Trees (MART; also known as Gradient Boosted Trees) by Jerome Friedman. One thing I was particularly interested in, was how much randomization techniques have helped improve prediction performance in all of the algorithms named above. In Random Forest and Extra-Trees, it is quite obvious. Here, randomization is the reason why the ensembles offer an improvement over bagging; through the de-correlation of the base learners, the variance of the ensemble and therefore its prediction error decreases. In the end, you achieve de-correlation by „shaking up“ the base trees, as it’s done in the two ensembles. However, MART also profits from randomization. In 2002, Friedman published another paper on boosting, showing that you can improve the prediction performance of boosted trees by training each tree on only a random subsample of your data. As a side-effect, your training time also decreases. Furthermore, in 2015, Rashmi and Gilad suggested adding a method known as a dropout to the boosting ensemble: a method found and used in neural nets.The Idea behind Random BoostInspired by theoretical readings on randomization techniques in boosting, I developed a new algorithm, that I called Random Boost (RB). In its essence, Random Boost sequentially grows regression trees with random depth. More precisely, the algorithm is almost identical to and has the exact same input arguments as MART. The only difference is the parameter . In MART,  determines the maximum depth of all trees in the ensemble. In Random Boost, the argument constitutes the upper bound of possible tree sizes. In each boosting iteration , a random number  between 1 and  is drawn, which then defines the maximum depth of that tree .In comparison to MART, this has two advantages:First, RB is faster than MART on average, when being equipped with the same value for the tree size. When RB and MART are trained with a value for the maximum tree depth equal to , then Random Boost will in many cases grow trees the size of  by nature. If you assume that for MART, all trees will be grown to their full size  (i.e. there is enough data left in each internal node so that tree growing doesn’t stop before the maximum size is reached), you can derive a formula showing the relative computation gain of RB over MART:     e.g. is the training time of a RB boosting ensemble with the tree size parameter being equal to .To make it a bit more practical, the formula predicts that for , , and , RB takes %, %, and % of the computation time of MART, respectively. These predictions, however, should be seen as RB’s best-case scenario, as MART is also not necessarily growing full trees. Still, the calculations suggest that efficiency gains can be expected (more on that later).Second, there are also reasons to assume that randomizing over tree depths can have a beneficial effect on the prediction performance. As already mentioned, from a variance perspective, boosting suffers from overcapacity for various reasons. One of them is choosing too rich of a base learner in terms of depth. If, for example, one assumes that the dominant interaction in the data generating process is of order three, one would pick a tree with equivalent depth in MART in order to capture this interaction depth. However, this may be overkill, as fully grown trees with a depth equal to  have eight leaves and therefore learn noise in the data, if there are only a few of such high order interactions. Perhaps, in this case, a tree with depth  but less than eight leaves would be optimal. This is not accounted for in MART, if one doesn’t want to add a pruning step to each boosting iteration at the expense of computational overhead. Random Boost may offer a more efficient remedy to this issue. With probability , a tree is grown, which is able to capture the high order effect at the cost of also learning noise. However, in all the other cases, Random Boost constructs smaller trees that do not show the over-capacity behavior and that can focus on interactions of smaller order. If over-capacity is an issue in MART due to different interactions in the data governed by a small number of high order interactions, Random Boost may perform better than MART. Furthermore, Random Boost also decorrelates trees through the extra source of randomness, which has a variance reducing the effect on the ensemble.The concept of Random Boost constitutes a slight change to MART. I used the sklearn package as a basis for my code. As a result, the algorithm is developed based on sklearn.ensemle.GradientBoostingRegressor and sklearn.ensemle.GradientBoostingClassifier and is used in exactly the same way (i.e. argument names match exactly and CV can be carried out with sklearn.model_selection.GridSearchCV). The only difference is that the RandomBoosting*-object uses max_depth to randomly draw tree depths for each iteration. As an example, you can use it like this:rb = RandomBoostingRegressor(learning_rate=0.1, max_depth=4, n_estimators=100)rb = rb.fit(X_train, y_train)rb.predict(X_test)For the full code, check out my GitHub account.Random Boost versus MART – A Simulation StudyIn order to compare the two algorithms, I ran a simulation on 25 Datasets generated by a Random Target Function Generator that was introduced by Jerome Friedman in his famous Boosting paper he wrote in 2001 (you can find the details in his paper. Python code can be found here). Each dataset (containing 20,000 observations) was randomly split into a 25% test set and a 75% training set. RB and MART were tuned via 5-fold CV on the same tuning grid.learning_rate = 0.1max_depth = (2, 3, ..., 8)n_estimators = (100, 105, 110, ..., 195)For each dataset, I tuned both models to obtain the best parameter constellation. Then, I trained each model on every point of the tuning grid again and saved the test MAE as well as the total training time in seconds. Why did I train every model again and didn’t simply store the prediction accuracy of the tuned models along with the overall tuning time? Well, I wanted to be able to see how training time varies with the tree size parameter.A Comparison of Prediction AccuraciesYou can see the distribution of MAEs of the best models on all 25 datasets below.Evidently, both algorithms perform similarly.For a better comparison, I compute the relative difference between the predictive performance of RB and MART for each dataset , i.e. . If , then RB had a larger mean absolute error than MART on dataset , and vice versa.In the majority of cases, RB did worse than MART in terms of prediction accuracy (). In the worst case, RB had a 1% higher MAE than MART. In the median, RB has a 0.19% higher MAE. I’ll leave itu p to you to decide whether that difference is practically significant. A comparison of Training timesWhen we look at training time, we get a quite clear picture. In absolute terms, it took 433 seconds to train all parameter combinations of RB on average, as opposed to 803 seconds for MART.The small black lines on top of each bar are the error bars (2 times the means standard deviation; rather small in this case).To give you a better feeling of how each model performed on each dataset, I also plotted the training times for each round.If you now compute the training time ratio between MART and RB (), you see that RB is roughly 1.8 times faster than MART on average.Another perspective on the case is to compute the relative training time  which is just 1 over the speedup. Note that this measure has to be interpreted a bit differently from the relative MAE measure above. If  then RB is as fast as MART, if , then it takes longer to train RB than MART, and if , then RB is faster than MART. On average, RB only needs roughly 54% of MART’s tuning time in the median, and it is noticeably faster in all cases. I was also wondering how the relative training time varies with  and how well the theoretically derived lower bound from above fits the actually measured relative training time. That’s why I computed the relative training time across all 25 datasets by tree size. Tree size ()Actual Training time (RB / MART)Theoretical lower bound20.7510.75030.6520.58340.5960.46950.5660.38860.5320.32870.5050.28380.4790.249The theoretical figures are optimistic, but the relative performance gain of RB increases with tree size.Results in a Nutshell and Next StepsAs part of my research on tree-based ensemble methods, I developed a new algorithm called Random Boost. Random Boost is based on Jerome Friedman’s MART, with the slight difference that it fits trees of random size. In total, this little change can reduce the problem of overfitting and noticeably speed up computation. Using a Random Target Function Generator suggested by Friedman, I found that, on average, RB is roughly twice as fast as MART with a comparable prediction accuracy in expectation.Since running the whole simulation takes quite some time (finding the optimal parameters and retraining every model takes roughly one hour for each data set on my Mac), I couldn’t run hundreds or more simulations for this blog post. That’s the objective for future research on Random Boost. Furthermore, I want to benchmark the algorithm on real-world datasets. In the meantime, feel free to look at my code and run the simulations yourself. Everything is on GitHub. Moreover, if you find something interesting and you want to share it with me, please feel free to shoot me an email.ReferencesBreiman, Leo (2001). Random Forests. Machine Learning, 45, 5–32Chang, Tianqi, and  Carlos Guestrin. 2016. XGBoost: A Scalable Tree Boosting System. Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Pages 785-794Chapelle, Olivier, and Yi Chang. 2011. “Yahoo! learning to rank challenge overview”. In Proceedings of the Learning to Rank Challenge, 1–24. Friedman, J. H. (2001). Greedy function approximation: a gradient boosting machine. Annals of statistics, 1189-1232.Friedman, J. H. (2002). “Stochastic gradient boosting”. Computational Statistics &amp; Data Analysis 38 (4): 367–378.Geurts, Pierre, Damien Ernst, and Louis Wehenkel (2006). “Extremely randomized trees”. Machine learning 63 (1): 3–42.Rashmi, K. V., and Ran Gilad-Bachrach (2015). Proceedings of the 18th International Conference on Artificial Intelligence and Statistics (AISTATS) 2015, San Diego, CA, USA. JMLR: W&amp;CP volume 38.Über den AutorTobias KrabelI am data scientist at STATWORX, with a secret passion for data and software engineering. Tocompensate for my nerdy sitting-in-the-basement side, I spend even more time in the basement writing shiny applications..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/how-to-speed-up-gradient-boosting-by-a-factor-of-two/;Statworx;  Tobias Krabel
  15. März 2019;R and Python: Using reticulate to get the best of both worlds;"It’s March 15th and that means it’s World Sleep Day (WSD). Don’t snooze off just yet! We’re about to check out a package that can make using R and Python a dream. It’s called reticulate and we’ll use it to train a Support Vector Machine for a simple classification task.What’s WSD got do with data science?I discovered WSD on the train to STATWORX HQ in Frankfurt. If you’ve travelled by train before, you know that the only thing worse than disgruntled staff is unconscious staff. It hasn’t happened to me yet, but hey let’s face it: the personnel shortage of Deutsche Bahn isn’t getting better any time soon. This brings me to the topic of today’s blog post: the sleeping habits of railroad workers. For this, I pulled a dataset from the US Federal Railroad Administration (FRA). The FRA conducted a survey on the work and sleepschedules of its train dispatchers. You can find it here. As the title suggests, we’re going to use both R and Python to predict whether a dispatcher was diagnosed with a sleeping disorder. Before we get started, awarning to all R and Python purists out there: the example below is a bit contrived. Everything we’re about to do can be done entirely in either one of the languages.So why bother using R and Python?As data scientists, we ideally have a firm grasp on both R and Python. After all, as my colleague Fran will discuss in an upcoming post, both have unique strengths that we can use to our advantage. Personally, I love R for the tidyverse and ggplot2, Python for its unified machine learning (ML) API scikit-learn.Alright, point taken you might say, but it’s a hassle to switch back and forth. It’s not worth it, and until a few years ago you would have been right! However, nowadays there are some cool ways to get the best of both worlds. If you’re coming from the R community look no further than reticulate!The reticulate package gives you a set of tools to use both R and Python interactively within an R session. Say you’re working in Python and need a specialized statistical model from an R package – or you’re working in R and want to access Python’s ML capabilities. Well, you’ve come to the right place. This package is a godsend for tasks where it’s required, or simply more convenient, to use both languages as part of your workflow.Now, there are different ways to use R and Python interactively and I encourage you to check reticulate’s github site to see which one suits you best. In this post, we’re going through a simple example of how to use Python modules within an R Notebook (i.e. Markdown document).  How to get started?If you’re working on a Mac, Python comes preinstalled on your system. Irrespective of the machine you’re on though, I recommended downloading the Anaconda distribution, so you have everything you need. One more note: you need RStudio’s newest preview version 1.2 for this to work. That’s it!Let’s open an R Notebook, insert an R chunk and (install and) load the reticulate library. Immediately after loading reticulate, use the use_python() command with the appropriate path. Placing it later in the script causes problems for some people. I specified my path as follows (note: yours may differ):library(tidyverse)library(recipes)library(reticulate)use_python(""/anaconda3/bin/python"")We’re good to go. Let’s read in the data and perform some transformations with dplyr. This is mostly recoding work. As you can see in the select command, we pick a handful of variables like sex, age, caffeine consumption, health and stress to predict whether a railroad dispatcher was diagnosed with a sleeping disorder.1) Read in the datadata &lt;- readxl::read_xls(""sleep.xls"")sleep &lt;- data %&gt;%  select(      Diagnosed_Sleep_disorder, Age_Group, Sex, Total_years_dispatcher,      Total_years_present_job, Marital_Status, Childrendependents,      Children_under_2_yrs, Caff_Beverages, Sick_Days_in_last_year,      Health_status, Avg_Work_Hrs_Week, FRA_report, Phys_Drained,      Mentally_Drained, Alert_at_Work, Job_Security  ) %&gt;%  rename_all(tolower) %&gt;%  mutate_if(is.character, as.numeric) %&gt;%  mutate_at(vars(diagnosed_sleep_disorder, sex, caff_beverages, fra_report),            ~ -(. - 2)) %&gt;%  mutate_at(vars(marital_status), ~ (. - 1)) %&gt;%  drop_na()Now that we have the variables we want, it’s time to get the data into the right shape. Here’s one more reason to love R: the recipes package. If you’re not familiar with it, check it out. You may find its workflow a bit peculiar at first, but once you get used to it, it makes data preparation a breeze.What we’re doing here is straightforward. First, we split the data into a training and test set. Next, we specify a data preparation recipe, which consists of three steps: one hot encoding factor predictors, standardising numeric predictors and down-sampling the data. One hot encoding and standardising ensure that the Support Vector Machine algorithm works properly. Down-sampling is a counter-measure against the class imbalance in our dataset.2) Prepare the datanumeric_variables &lt;- c(  ""total_years_dispatcher"", ""total_years_present_job"",  ""childrendependents"", ""children_under_2_yrs"",   ""sick_days_in_last_year"", ""avg_work_hrs_week"")factor_variables &lt;- setdiff(colnames(sleep), numeric_variables)sleep &lt;- mutate_at(sleep, vars(factor_variables), as.factor)set.seed(2019)index &lt;- sample(1:nrow(sleep), floor(nrow(sleep) * .75))sleep_train &lt;- sleepindex, sleep_test &lt;- sleep-index, recipe_formula &lt;- recipes::recipe(diagnosed_sleep_disorder ~ ., sleep_train)recipe_steps &lt;- recipe_formula %&gt;%  recipes::step_dummy(factor_variables, -all_outcomes(), one_hot = TRUE) %&gt;%  recipes::step_downsample(diagnosed_sleep_disorder) %&gt;%  recipes::step_center(numeric_variables) %&gt;%  recipes::step_scale(numeric_variables)recipe_prep &lt;- recipes::prep(recipe_steps, sleep_train, retain = TRUE)training_data &lt;- juice(recipe_prep)testing_data &lt;- bake(recipe_prep, sleep_test)Now comes the part where Python shines: its unified ML library scikit-learn. Let’s go ahead and import the Support Vector Machine (SVM) classifier as well as some other modules to tune and evaluate our model.SVM is a supervised learning algorithm. It works by finding a hyperplane in an N-dimensional space, which separates two (or more) classes as cleanly as possible. More technically, SVM maximizes the margin or the distance between the separating hyperplane and the closest data points. This is why SVM is also called a maximum margin estimator.SVM is mostly used for classification, but it can do regression too. The upside is that it works with high-dimensional data and different kernel functions, meaning it can flexibly adapt to different types of data. Its downside is that computation becomes costly with large datasets and that it reacts sensitively to hyperparameters. Still, for some applications SVM performs quite competitively.Combining SVM with kernels allows us to project our data into a higher-dimensional space. The point of this is to make the classes better separable. In our example here, we’ll use a simple linear and a radial basis function kernel. The latter can map the predictor space into infinite dimensions.3) Train the modelimport numpy as npimport pandas as pdimport seaborn as snsimport matplotlib.pyplot as pltfrom sklearn import svmfrom sklearn.model_selection import GridSearchCV, cross_val_scorefrom sklearn.metrics import classification_report, confusion_matrix, accuracy_scorey_train = r.training_data'diagnosed_sleep_disorder'X_train = r.training_data.drop('diagnosed_sleep_disorder', axis = 1)y_test = r.testing_data'diagnosed_sleep_disorder'X_test = r.testing_data.drop('diagnosed_sleep_disorder', axis = 1)clf = svm.SVC(kernel = 'linear')clf.fit(X_train, y_train)y_pred = clf.predict(X_test)print(confusion_matrix(y_test, y_pred))print(classification_report(y_test, y_pred))clf = svm.SVC(kernel = 'rbf')clf.fit(X_train, y_train)y_pred = clf.predict(X_test)print(confusion_matrix(y_test, y_pred))print(classification_report(y_test, y_pred))We can tune an SVM with the help of two parameters: C and gamma. C is a regularisation parameter that represents how much error we’re willing to tolerate. The higher C, the stricter we are, the more exactly SVM will try to fit the data by picking a hyperplane with smaller margins. As a consequence, higher C’s carry a higher risk of overfitting. For the radial basis function kernel, we can also tune gamma which determines the size of the kernel and with it the decision boundary. The higher gamma, the closer the decision boundary is to single training examples. A higher gamma can lead to a better model, yet likewise increases the risk of overfitting. 4) Tune the modelparam_grid = {'C': 0.01, 0.1, 1, 10, 100,               'gamma': 0.001, 0.01, 0.1, 1, 10,               'kernel': 'rbf', 'linear'}grid = GridSearchCV(svm.SVC(), param_grid, cv = 5, scoring = 'balanced_accuracy')grid.fit(X_train, y_train)print(grid.best_params_)From the cross-validation procedure, it appears that a gamma of 0.1 and a C of 0.01 are optimal in combination with a radial basis function kernel. So, let’s train our model with this kernel and the hyperparameter values. 5) Evaluate the accuracy of the modelclf = grid.best_estimator_y_pred = clf.predict(X_test)print('Confusion Matrix:\n\n', confusion_matrix(y_test, y_pred))print('\nClassification Report:\n\n', classification_report(y_test, y_pred))print('\nTraining Set Accuracy: {:.2f}%'.format(clf.score(X_train, y_train)))print('\nTest Set Accuracy: {:.2f}%'.format(clf.score(X_test, y_test)))conf_mat = confusion_matrix(y_test, y_pred)sns.heatmap(conf_mat, square = True, annot = True, fmt = 'g',            cbar = False, cmap = 'viridis')plt.xlabel('predicted')plt.ylabel('observed')plt.show()In this case, we achieve a training set accuracy of 93 per cent and a test set accuracy of 74 per cent. This suggests that some overfitting has occurred. To achieve a higher accuracy (or better: sensitivity/recall), we could experiment with different kernels and/or hyperparameter values. But this I’d leave up to you. With reticulate, you now have a tool to get the best of both R and Python.About the authorManuel TilgnerI am a data scientist at STATWORX, and I enjoy making data make sense. Why? Because there's something magical about turning a jumble of numbers into insights. In my free time, I love wandering through the forest or playing in the local big band..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/r-and-python-using-reticulate-to-get-the-best-of-both-worlds/;Statworx;  Manuel Tilgner
  25. Februar 2019;Fixing the most common problem with Plotly histograms;"Data exploration is a critical step in every Data Science project here at STATWORX. In order to discover every insight, we prefer interactive charts to static charts, because we have the ability to dig deeper into the data. This helps us to reveal every insight.In today’s blog post, we show you how to improve the interactivity of Plotly histograms in Python like you can see in the graphics below. You can find the code in our GitHub repo.TL;DR: Quick SummaryWe show you the default Plotly histogram and its unexpected behaviourWe improve the interactive histogram to match our human intuition and show you the codeWe explain the code line by line and give you some more context on the implementationThe problem of the default Plotly histogramThis graphic shows you the behavior of a Plotly histogram if you zoom into a specific region. What you can see is that the bars just get bigger/wider. This is not what we expect!If we zoom into a plot, we want to dig deeper and see more fine-grained information for a special area. Therefore, we expect that the histogram shows more fine-grained information. In this specific case, this means that we expect that the histogram shows bins for the selected region. So, the histogram needs to be rebinned.Improving the interactive histogramIn this graphic you can see our expected end result. If the user selects a new x-region we want to rebin the histogram based on the new x-range.In order to implement this behavior, we need to change the graphic, when the selected x-range changes. This is a new feature of plotly.py 3.0.0 which was brought to the community by the great Jon Mease. You can read more about the new Plotly.py 3.0.0 features in this announcement.Disclaimer: The implementation only works inside a Jupyter Notebook or JupyterLab because it needs an active Python kernel for the callback. It does not work in a standalone Python script and it does not work in a standalone HTML file.The implementation idea is the following: We create an interactive figure and every time the x-axis changes, we will update the underlying data of the histogram. Whoops, why didn’t we change the binning? Plotly histograms automatically handle the binning for the underlying data. Therefore, we can let the histogram do the work and just change the underlying data. This is a little bit counterintuitive but saves a lot of work.Glimpse of the full codeSo, finally here comes the relevant code without unnecessary imports etc.If you want to see the full code, please check this GitHub file.x_values = np.random.randn(5000)figure = go.FigureWidget(data=go.Histogram(x=x_values,                                            nbinsx=10),                         layout=go.Layout(xaxis={'range': -4, 4},                                          bargap=0.05))histogram = figure.data0def adjust_histogram_data(xaxis, xrange):    x_values_subset = x_valuesnp.logical_and(xrange0 &lt;= x_values,                                              x_values &lt;= xrange1)    histogram.x = x_values_subsetfigure.layout.xaxis.on_change(adjust_histogram_data, 'range')Detailed explanations for each line of codeIn the following, we will provide some detailed insights and explanations for each line of code. 1) Initializing the x_valuesx_values = np.random.randn(5000)We get 5000 new random x_values which are distributed according to a normal distribution. The values are created by the great numpy library which is abbreviated as np.2) Creating the figurefigure = go.FigureWidget(data=go.Histogram(x=x_values,                                            nbinsx=10),                         layout=go.Layout(xaxis={'range': -4, 4},                                          bargap=0.05))We generate a new FigureWidget instance. The FigureWidget object is the new „magic object“ which was introduced by Jon Mease. You can display it within Jupyter Notebook or JupyterLab like a normal Plotly figure but there are some advantages. You can manipulate the FigureWidgetin various ways from Python and you can also listen for some events and execute some more Python code which gives you a lot of options. This flexibility is the great benefit which Jon Mease envisioned. The FigureWidget receives the attributes data and layout.As data, we specify a list of all the traces (read: visualizations) that we want to show. In our case, we only want to show a single histogram. The x values for the histogram are our x_values. Also, we set the maximum number of bins with nbinsx to 10. Plotly will use this as a guideline but will not enforce the plot to exactly contain nbinsx bins. As layout, we specify a new layout object and set the range of the xaxis to -4, 4. With the bargap argument, we can enable the layout to show a gap between individual bars. This helps us to see where a bar stops and the next one begins. In our case, this value is set to 0.05.3) Saving a reference to the histogramhistogram = figure.data0We get the reference to the histogram because we want to manipulate the histogram in the last step. We don’t actually get the data but a reference to the Plotly trace object. This Plotly syntax might be a little bit misleading but it is consistent with the definition of the figure where we also specified the „traces“ as „data“.4) Overview of the callbackdef adjust_histogram_data(xaxis, xrange):    x_values_subset = x_valuesnp.logical_and(xrange0 &lt;= x_values,                                              x_values &lt;= xrange1)    hist.x = x_values_subsetfigure.layout.xaxis.on_change(adjust_histogram_data, 'range')In this chunk, we first define what we want to do at the event when the xaxis changes. Afterwards, we register the callback function adjust_histogram_data. We will break this down further but we will start with the last line because Python will execute this line first at runtime. Therefore, it makes more sense to read the code based on this reverse execution order.A little bit more background on the callback: The code within the callback method adjust_histogram_data will be called when the xaxis.on_change event actually happens because the user interacted with the chart. But first, Python needs to register the adjust_histogram_data callback method. Way later, when the callback event xaxis.on_change occurs, Python will execute the callback method adjust_histogram_data and its contents. Go back and read this section 3-4 times until you fully understand it.4a) Registering the callbackfigure.layout.xaxis.on_change(adjust_histogram_data, 'range')In this line, we tell the figure object to always call the callback function adjust_histogram_data whenever the xaxis changes. Please note, that we only specify the name of the function adjust_histogram_data without the round brackets (). This is because we only need to pass the reference to the function and do not want to call the function. This is a common error and source of confusion.Also we specify, that we are only interested in the range attribute. Therefore, the figure object will only send this information to the callback function.But how does the callback function look like and what is the task of the callback function? Those questions are explained in the next steps:4b) Defining the callback signaturedef adjust_histogram_data(xaxis, xrange):In this line, we start to define our callback function. The first argument which is passed to the function is the xaxis object which initiated the callback. This is a Plotly convention and we just need to put this placeholder here although we don’t use it.The second argument is the xrange which contains the lower and upper limit of the new xrange configuration.You might wonder: „where do the arguments xaxis and xrange come from?“ Those arguments are automatically provided by the figure when the callback gets called. When you use callbacks for the first time, this might seem like intransparent magic. But you will get used to it…4c) Updating the x_valuesx_values_subset = x_valuesnp.logical_and(xrange0 &lt;= x_values,                                          x_values &lt;= xrange1)In this line, we define our new x_values which in most cases are a subset of the original x_values. However, if the lower and upper limit are very far away from each other, we might end up selecting all the original x_values. So the subset is not always a strict subset. The lower limit of the xrange is defined by xrange0 and the  upper limit via xrange1. In order to select the subset of the x_values which is within the lower and upper limit of the xrange we use the logical_and function from numpy. There are multiple ways how we can select data subsets in Python. For example, you can also do this via pandas selectors if you use pandas dataframes/series.4d) Updating the histogram datahistogram.x = x_values_subsetIn this line, we update the underlying data of the histogram and set it to the new x_values_subset. This line will trigger the update of the Histogram and the automatic rebinning. The histogram is the reference which we created in step 3 of the code because we need it here.Wrapping upIn this blog post, we showed you how to improve the default Plotly histogram via interactive binning. We gave you the code and explained every line of the implementation. We hope that you were able to follow along and gained some good understanding of the new possibilities thanks to plotly.py 3.0.0.Maybe you sometimes have the feeling that you cannot understand the code that you find on the internet or Stackoverflow. If you want intuitive explanations for everything you need to know in the world of Data Science, consider some of our open courses which are taught by our Data Science experts here at STATWORX.ReferencesOfficial plotly.py GitHub repository https://github.com/plotly/plotly.pyOfficial overview of all new plotly.py 3.0.0 features https://medium.com/@plotlygraphs/introducing-plotly-py-3-0-0-7bb1333f69c6Über den AutorFlorian WetschoreckI am writing my Master thesis at STATWORX. The goal is to reduce the effort for an initial data analysis with a Python package..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/fixing-the-most-common-problem-with-plotly-histograms/;Statworx;  Florian Wetschoreck
  22. Februar 2019;Running your R script in Docker;"Since its release in 2014, Docker has become an essential tool for deploying applications. At STATWORX, R is part of our daily toolset. Clearly, many of us were thrilled to learn about RStudio’s Rocker Project, which makes containerizing R code easier than ever. Containerization is useful in a lot of different situations. To me, it is very helpful when I’m deploying R code in a cloud computing environment, where the coded workflow needs to be run on a regular schedule. Docker is a perfect fit for this task for two reasons: On the one hand, you can simply schedule a container to be started at your desired interval. On the other hand, you always know what behavior and what output to expect, because of the static nature of containers. So if you’re tasked with deploying a machine-learning model that should regularly make predictions, consider doing so with the help of Docker. This blog entry will guide you through the entire process of getting your R script to run in a Docker container one step at a time. For the sake of simplicity, we’ll be working with a local dataset. I’d like to start off with emphasizing that this blog entry is not a general Docker tutorial. If you don’t really know what images and containers are, I recommend that you take a look at the Docker Curriculum first. If you’re interested in running an RStudio session within a Docker container, then I suggest you pay a visit to the OpenSciLabs Docker Tutorial instead. This blog specifically focuses on containerizing an R script to eventually execute it automatically each time the container is started, without any user interaction – thus eliminating the need for the RStudio IDE. The syntax used in the Dockerfile and the command line will only be treated briefly here, so it’s best to get familiar with the basics of Docker before reading any further. What we’ll needFor the entire procedure we’ll be needing the following:An R script which we’ll build into an imageA base image on top of which we’ll build our new imageA Dockerfile which we’ll use to build our new imageYou can clone all following files and the folder structure I used from the STATWORX GitHub Repository.The R scriptWe’re working with a very simple R script that imports a dataframe, manipulates it, creates a plot based on the manipulated data and, in the end, exports both the plot and the data it is based on. The dataframe used for this example is the US 500 Records dataset provided by Brian Dunning. If you’d like to work along, I’d recommend you to copy this dataset into the 01_data folder. library(readr)library(dplyr)library(ggplot2)library(forcats)# import dataframedf &lt;- read_csv(""01_data/us-500.csv"")# manipulate dataplot_data &lt;- df %&gt;%  group_by(state) %&gt;%  count()# save manipulated data to output folderwrite_csv(plot_data, ""03_output/plot_data.csv"")# create plot based on manipulated dataplot &lt;- plot_data %&gt;%   ggplot()+  geom_col(aes(fct_reorder(state, n),                n,                fill = n))+  coord_flip()+  labs(    title = ""Number of people by state"",    subtitle = ""From US-500 dataset"",    x = ""State"",    y = ""Number of people""  )+   theme_bw()# save plot to output folderggsave(""03_output/myplot.png"", width = 10, height = 8, dpi = 100)This creates a simple bar plot based on our dataset:We use this script not only to run R code inside a Docker container, but we also want to run it on data from outside our container and afterward save our results.The base imageThe DockerHub page of the Rocker project lists all available Rocker repositories. Seeing as we’re using Tidyverse-packages in our script the rocker/tidyverse image should be an obvious choice. The problem with this repository is that it also includes RStudio, which is not something we want for this specific project. This means that we’ll have to work with the r-base repository instead and build our own Tidyverse-enabled image. We can pull the rocker/r-base image from DockerHub by executing the following command in the terminal:docker pull rocker/r-baseThis will pull the Base-R image from the Rocker DockerHub repository. We can run a container based on this image by typing the following into the terminal: docker run -it --rm rocker/r-baseCongratulations! You are now running R inside a Docker container! The terminal was turned into an R console, which we can now interact with thanks to the -it argument. The —-rm argument makes sure the container is automatically removed once we stop it. You’re free to experiment with your containerized R session, which you can exit by executing the q() function from the R console. You could, for example, start installing the packages you need for your workflow with install.packages(), but that’s usually a tedious and time-consuming task. It is better to already build your desired packages into the image, so you don’t have to bother with manually installing the packages you need every time you start a container. For that, we need a Dockerfile.The DockerfileWith a Dockerfile, we tell Docker how to build our new image. A Dockerfile is a text file that must be called „Dockerfile.txt“ and by default is assumed to be located in the build-context root directory (which in our case would be the „R-Script in Docker“ folder). First, we have to define the image on top of which we’d like to build ours. Depending on how we’d like our image to be set up, we give it a list of instructions so that running containers will be as smooth and efficient as possible. In this case, I’d like to base our new image on the previously discussed rocker/r-base image. Next, we replicate the local folder structure, so we can specify the directories we want in the Dockerfile. After that we copy the files which we want our image to have access to into said directories – this is how you get your R script into the Docker image. Furthermore, this allows us to prevent having to manually install packages after starting a container, as we can prepare a second R script that takes care of the package installation. Simply copying the R script is not enough, we also need to tell Docker to automatically run it when building the image. And that’s our first Dockerfile!# Base image https://hub.docker.com/u/rocker/FROM rocker/r-base:latest## create directoriesRUN mkdir -p /01_dataRUN mkdir -p /02_codeRUN mkdir -p /03_output## copy filesCOPY /02_code/install_packages.R /02_code/install_packages.RCOPY /02_code/myScript.R /02_code/myScript.R## install R-packagesRUN Rscript /02_code/install_packages.RDon’t forget preparing and saving your appropriate install_packages.R script, where you specify which R packages you need to be pre-installed in your image. In our case the file would look like this:install.packages(""readr"")install.packages(""dplyr"")install.packages(""ggplot2"")install.packages(""forcats"")Building and running the imageNow we have assembled all necessary parts for our new Docker image. Use the terminal to navigate to the folder where your Dockerfile is located and build the image with docker build -t myname/myimage .The process will take a while due to the package installation. Once it’s finished we can test our new image by starting a container with docker run -it --rm -v ~/""R-Script in Docker""/01_data:/01_data -v ~/""R-Script in Docker""/03_output:/03_output myname/myimageUsing the -v arguments signales Docker which local folders to map to the created folders inside the container. This is important because we want to both get our dataframe inside the container and save our output from the workflow locally so it isn’t lost once the container is stopped. This container can now interact with our dataframe in the 01_data folder and has a copy of our workflow-script inside its own 02_code folder. Telling R to source(""02_code/myScript.R"") will run the script and save the output into the 03_output folder, from where it will also be copied to our local 03_output folder. Improving on what we haveNow that we have tested and confirmed that our R script runs as expected when containerized, there’s only a few things missing. We don’t want to manually have to source the script from inside the container, but have it run automatically whenever the container is started.We can achieve this very easily by simply adding the following command to the end of our Dockerfile:## run the scriptCMD Rscript /02_code/myScript.RThis points towards the location of our script within the folder structure of our container, marks it as R code and then tells it to run whenever the container is started. Making changes to our Dockerfile, of course, means that we have to rebuild our image and that in turn means that we have to start the slow process of pre-installing our packages all over again. This is tedious, especially if chances are that there will be further revisions of any of the components of our image down the road. That’s why I suggest weCreate an intermediary Docker image where we install all important packages and dependencies so that we can then build our final, desired image on top. This way we can quickly rebuild our image within seconds, which allows us to freely experiment with our code without having to sit through Docker installing packages over and over again. Building an intermediary imageThe Dockerfile for our intermediary image looks very similar to our previous example. Because I decided to modify my install_packages() script to include the entire tidyverse for future use, I also needed to install a few debian packages the tidyverse depends upon. Not all of these are 100% necessary, but all of them should be useful in one way or another. # Base image https://hub.docker.com/u/rocker/FROM rocker/r-base:latest## install debian packagesRUN apt-get update -qq &amp;&amp; apt-get -y --no-install-recommends install \libxml2-dev \libcairo2-dev \libsqlite3-dev \libmariadbd-dev \libpq-dev \libssh2-1-dev \unixodbc-dev \libcurl4-openssl-dev \libssl-dev## copy filesCOPY 02_code/install_packages.R /install_packages.R## install R-packagesRUN Rscript /install_packages.RI build the image by navigating to the folder where my Dockerfile sits and executing the Docker build command again:docker build -t oliverstatworx/base-r-tidyverse .I have also pushed this image to my DockerHub so if you ever need a base-R image with the tidyverse pre-installed you can simply build it ontop of my image without having to go through the hassle of building it yourself.Now that the intermediary image has been built we can change our original Dockerfile to build on top of it instead of rocker/r-base and remove the package-installation because our intermediary image already takes care of that. We also add the last line that automatically starts running our script whenever the container is started. Our final Dockerfile should look something like this:# Base image https://hub.docker.com/u/oliverstatworx/FROM oliverstatworx/base-r-tidyverse:latest## create directoriesRUN mkdir -p /01_dataRUN mkdir -p /02_codeRUN mkdir -p /03_output## copy filesCOPY /02_code/myScript.R /02_code/myScript.R## run the scriptCMD Rscript /02_code/myScript.RThe final touchesSince we built our image on top of an intermediary image with all our needed packages, we can now easily modify parts of our final image to our liking. I like making my R script less verbose by suppressing warnings and messages that are not of interest anymore (since I already tested the image and know that everything works as expected) and adding messages that tell the user which part of the script is currently being executed by the running container. suppressPackageStartupMessages(library(readr))suppressPackageStartupMessages(library(dplyr))suppressPackageStartupMessages(library(ggplot2))suppressPackageStartupMessages(library(forcats))options(scipen = 999,        readr.num_columns = 0)print(""Starting Workflow"")# import dataframeprint(""Importing Dataframe"")df &lt;- read_csv(""01_data/us-500.csv"")# manipulate dataprint(""Manipulating Data"")plot_data &lt;- df %&gt;%  group_by(state) %&gt;%  count()# save manipulated data to output folderprint(""Writing manipulated Data to .csv"")write_csv(plot_data, ""03_output/plot_data.csv"")# create plot based on manipulated dataprint(""Creating Plot"")plot &lt;- plot_data %&gt;%   ggplot()+  geom_col(aes(fct_reorder(state, n),                n,                fill = n))+  coord_flip()+  labs(    title = ""Number of people by state"",    subtitle = ""From US-500 dataset"",    x = ""State"",    y = ""Number of people""  )+   theme_bw()# save plot to output folderprint(""Saving Plot"")ggsave(""03_output/myplot.png"", width = 10, height = 8, dpi = 100)print(""Worflow Finished"")After navigating to the folder where our Dockerfile is located we rebuild our image once more with: docker build -t myname/myimage . Once again we start a container based on our image and map the 01_data and 03_output folders to our local directories. This way we can import our data and save our created output locally:docker run -it --rm -v ~/""R-Script in Docker""/01_data:/01_data -v ~/""R-Script in Docker""/03_output:/03_output myname/myimageCongratulations, you now have a clean Docker image that not only automatically runs your R script whenever a container is started, but also tells you exactly which part of the code it is executing via console messages. Happy docking!Über den AutorOliver GuggenbühlI am a data scientist at STATWORX and love telling stories with data - the ShinyR the better!.e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/running-your-r-script-in-docker/;Statworx;  Oliver Guggenbühl
  14. Februar 2019;Roses Are Red;"It’s Valentine’s day, making this the most romantic time of the year. But actually, already 2018 was a year full of love here at STATWORX: many of my STATWORX colleagues got engaged. And so we began to wonder – some fearful, some hopeful – who will be next? Therefore, today we’re going to tackle this question in the only true way: with data science!Gathering the DataTo get my data, I surveyed my colleagues. I asked my (to be) married colleagues to answer my questions based on the very day they got engaged. My single colleagues answered my questions with respect to their current situation. I asked them about some factors that I’ve always suspected to influence someone’s likeliness to get married. For example, I’m sure that in comparison to Python users, R users are much more romantic. The indiscreet questions I badgered my coworkers with were: Are you married or engaged?How long have you been in your relationship?Is your employment permanent?How long have you been working at STATWORX?What’s your age?Are you living together with your partner?Are you co-owning a pet with your partner?What’s your preferred programming language: R, Python or none of both. I’m going to treat the relationship status as dichotomous variable: Married or engaged vs. single or “only” dating. To maintain some of the privacy of my colleagues I gave them all some randomly (!!) chosen pet names. (Side note: There really is a subreddit for everything.)Descriptive ExplorationSince the first step in generating data driven answer should always be a descriptive exploration of the data at hand, I made some plots.First, I took a look at the absolute frequencies of preferred programming languages in the groups of singles vs. married or engaged STATWORX employees. I fear, the romantic nature of R users is not the explanation we’re looking for:# reformatting the target variabledf1 &lt;- df %&gt;%  dplyr::mutate(engaged = ifelse(engaged == ""yes"",                                  ""Engaged or Married"",                                  ""Single"")) %&gt;%  dplyr::group_by(`primary programming language`, engaged) %&gt;%  dplyr::summarise(freq = n(),                   image = ""~/Desktop/heart_red.png"") # since in geom_image size cannot be mapped to variable# multiple layers of data subsets  ggplot() +  geom_image(data = filter(df1, freq == 1),              aes(y = `primary programming language`,                 x = engaged,                  image = image),              size = 0.1) +   geom_image(data = filter(df1, freq &gt; 1 &amp; freq &lt;= 5),              aes(y = `primary programming language`,                  x = engaged,                  image = image),             size = 0.2) +  geom_image(data = filter(df1, freq &gt;= 13),              aes(y= `primary programming language`,                  x = engaged,                  image = image),             size = 0.3) +  geom_text(data = df1,             aes(y =`primary programming language`,                 x = engaged,                 label = freq),             color = ""white"", size = 4) +  ylab(""Preferred programming language"") +  xlab(""\n Absolute frequencies"") +  theme_minimal()I also explored the association of relationship status and the more conventional factors of age and relationship duration. And indeed, those of my colleagues who are in their late twenties or older and have been partnered for a while now, are mostly married or engaged.# plotting age and relationship duration vs. relationship statusggplot() +# doing this only to get a legend:  geom_point(data = df,             aes(x = age, y = `relationship duration`,                 color = engaged), shape = 15) +   geom_image(data = filter(df, engaged == ""yes""),              aes(x = age, y = `relationship duration`,                 image = ""~/Desktop/heart_red.png"")) +  geom_image(data = filter(df, engaged == ""no""),              aes(x = age, y = `relationship duration`,                 image = ""~/Desktop/heart_black.png"")) +  ylab(""Relationship duration \n"") +  xlab(""\n Age"") +  scale_color_manual(name = ""Married or engaged?"",                     values = c(""#000000"", ""#D00B0B"")) +  scale_x_continuous(breaks = pretty_breaks()) +  theme_minimal() +  theme(legend.position = ""bottom"")Statistical ModelsI’ll employ some statistical models, but the data base is rather small. Therefore, our model options are somewhat limited (and of course only suitable for entertainment). But it’s still possible to fit a decision tree, which might help to pinpoint due to which circumstance some of us are still waiting for that special someone to put a ring on (it). # recoding target to get more understandable labelsdf &lt;- df %&gt;%  dplyr::mutate(engaged = ifelse(engaged == ""yes"",                                  ""(to be) married"",                                  ""single""))# growing a decision three with a ridiculous low minsplitfit &lt;- rpart(engaged ~ `relationship duration` + age +              `shared pet` + `permanent employment` +             cohabitating + `years at STATWORX`,             control = rpart.control(minsplit = 2), # overfitting ftw             method = ""class"", data = df)# plotting the threerpart.plot(fit, type = 3, extra = 2,            box.palette = c(""#D00B0B"", ""#fae6e6""))Our decision tree implies, that the unintentionally unmarried of us maybe should consider to move in with their partner, since cohabitating seems to be the most important factor.But that still doesn’t exactly answer the question, who of us will be next. To predict our chances to get engaged, I estimated a logistic regression. We see that cohabiting, one’s age and the time we’ve been working at STATWORX are accompanied by a higher probability to (soon to) be married. However, simply having been together for a long time or owning a pet together with our partner, does not help. (Although, I assume that this rather unintuitive interrelation is caused by a certain outlier in the data – “Honey”, I’m looking at you!)Finally, I got the logistic regression’s predicted probabilities for all of us to be married or engaged. As you can see down below, the single days of  “Teddy Bear”, “Honey”, “Sweet Pea” and “Babe” seem to be numbered. # reformatting the target variabledf &lt;- df %&gt;%  dplyr::mutate(engaged = ifelse(engaged == ""(to be) married"", 1, 0))# in-sample fitting: estimating the model log_reg &lt;- glm(engaged ~ `relationship duration` + age +               `shared pet` + `permanent employment` +                cohabitating + `years at STATWORX`,              family = binomial, data = df)df$probability &lt;- predict(log_reg, df, type = ""response"")# plotting the predicted probabilitiesggplot() +  # again, doing this only to get a legend:  geom_point(data = df,             aes(x = probability, y = nickname,                 color = as.factor(engaged)), shape = 15) +   geom_image(data = filter(df, engaged == 1),              aes(x = probability, y = nickname,                 image = ""~/Desktop/heart_red.png"")) +  geom_image(data = filter(df, engaged == 0),              aes(x = probability, y = nickname,                 image = ""~/Desktop/heart_black.png"")) +  ylab("" "") +  xlab(""Predicted Probability"") +  scale_color_manual(name = ""Married or engaged?"",                     values = c(""#000000"", ""#D00B0B""),                     labels = c(""no"", ""yes"")) +  scale_x_continuous(breaks = pretty_breaks()) +  theme_minimal() +  theme(legend.position = ""bottom"")I hope this was as insightful for you as it was for me. And to all of us, whose hopes have been shattered by cold, hard facts, let’s remember: there are tons of discounted chocolates waiting for us on February 15th. Über den AutorLea WaniekI am a data scientist at STATWORX, apart from machine learning, I love to play around with RMarkdown and ggplot2, making data science beautiful inside and out..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/roses-are-red-violets-are-blue-statistics-can-be-romantic-too/;Statworx;  Lea Waniek
  28. Januar 2019;Plotly – An interactive Charting library;"IntroductionAt STATWORX we love beautiful plots. One of my favorite plotting libraries is plotly. It’s being developed by the company of the same name since 2012. Plotly.js is a high-level javascript library for interactive graphics and offers wrappers for a diverse range of languages, like Python, R or Matlab. Furthermore, it is open source and licensed under the MIT license, therefore it can be used in a commercial context. Plotly offers more than 30 different chart types. Another reason we at STATWORX use Plotly extensively is that it can be easily integrated into web-based frameworks, like Dash or R Shiny.How does it work?A Plotly plot is based on the following three main elements: Data, Layout and Figure. DataThe Data object can contain several traces. For example, in a line chart with several lines, each line is represented by a different trace. According to that, the data object contains the data which should be plotted but also the specification of how the data should be plotted.LayoutThe Layout object defines everything, that is not related to the data. It contains elements, like the title, axis titles or background-color. However, you can also add annotations or shapes with the layout object.FigureThe Figure object includes both data and layout. It creates our final figure for plotting, which is just a simple dictionary-like object. All figures are built with plotly.js, so in the end, the Python API only interacts with the plotly.js library.ApplicationLet’s visualize some data. For that purpose we will use the LA Metro Bike Share dataset, which is hosted by the city of Los Angeles and contains anonymized Metro Bike Share trip data. In the following section we will use Plotly for Python and compare it later on with the R implementation.Creating our first line plot with PlotlyFirst, we will generate a line plot which shows the number of rented bikes over different dates differentiated over the passholder type. Thus, we first have to aggregate our data before we can plot it. As shown above, we define our different traces. Each trace contains the number of rented bikes for a specific passholder type. For line plots, we use the Scatter()– function from plotly.graph_objs. It is used for scatter and line plots, however, we can define how it is displaced by setting the mode parameter accordingly. Those traces are unified as a list in our data object. Our layout object consists of a dictionary, where we define the main title and the axis titles. At last, we put our data and layout object together as a figure-object. import pandas as pdimport plotly.graph_objs as goimport plotly.plotly as pydf = pd.read_pickle(path=""LA_bike_share.pkl"")rental_count = df.groupby(""Start_Date"", ""Passholder_Type"").size().reset_index(name =""Total_Count"")trace0 = go.Scatter(    x=rental_count.query(""Passholder_Type=='Flex Pass'"").Start_Date,    y=rental_count.query(""Passholder_Type=='Flex Pass'"").Total_Count,    name=""Flex Pass"",    mode=""lines"",    line=dict(color=""#013848""))trace1 = go.Scatter(    x=rental_count.query(""Passholder_Type=='Monthly Pass'"").Start_Date,    y=rental_count.query(""Passholder_Type=='Monthly Pass'"").Total_Count,    name=""Monthly Pass"",    mode=""lines"",    line=dict(color=""#0085AF""))trace2 = go.Scatter(    x=rental_count.query(""Passholder_Type=='Walk-up'"").Start_Date,    y=rental_count.query(""Passholder_Type=='Walk-up'"").Total_Count,    name=""Walk-up"",    mode=""lines"",    line=dict(color=""#00A378""))data = trace0,trace1,trace2layout = go.Layout(title=""Number of rented bikes over time"",                   yaxis=dict(title=""Number of rented bikes"",                               zeroline=False),                   xaxis=dict(title=""Date"",                              zeroline = False)                  )fig = go.Figure(data=data, layout=layout)Understanding the structure behind graph_objsIf we output the figure-object, we will get the following dictionary-like object. Figure({    'data': {'line': {'color': '#013848'},              'mode': 'lines',              'name': 'Flex Pass',              'type': 'scatter',              'uid': '5d8c0781-4592-4d19-acd9-a13a22431ccd',              'x': array(datetime.date(2016, 7, 7), datetime.date(2016, 7, 8),                          datetime.date(2016, 7, 9), ..., datetime.date(2017, 3, 29),                          datetime.date(2017, 3, 30), datetime.date(2017, 3, 31), dtype=object),              'y': array( 61,  93, 113, ...,  52,  36,  40)},             {'line': {'color': '#0085AF'},              'mode': 'lines',              'name': 'Monthly Pass',              'type': 'scatter',              'uid': '4c4c76b9-c909-44b7-8e8b-1b0705fa2491',              'x': array(datetime.date(2016, 7, 7), datetime.date(2016, 7, 8),                          datetime.date(2016, 7, 9), ..., datetime.date(2017, 3, 29),                          datetime.date(2017, 3, 30), datetime.date(2017, 3, 31), dtype=object),              'y': array(128, 251, 308, ..., 332, 312, 301)},             {'line': {'color': '#00A378'},              'mode': 'lines',              'name': 'Walk-up',              'type': 'scatter',              'uid': '8303cfe0-0de8-4646-a256-5f3913698bd9',              'x': array(datetime.date(2016, 7, 7), datetime.date(2016, 7, 8),                          datetime.date(2016, 7, 12), ..., datetime.date(2017, 3, 29),                          datetime.date(2017, 3, 30), datetime.date(2017, 3, 31), dtype=object),              'y': array(  1,   1,   1, ..., 122, 133, 176)},    'layout': {'title': {'text': 'Number of rented bikes over time'},               'xaxis': {'title': {'text': 'Date'}, 'zeroline': False},               'yaxis': {'title': {'text': 'Number of rented bikes'}, 'zeroline': False}}})In theory, we could build those dictionaries or change the entries by hand without using plotly.graph_objs. However, it is much more convenient to use graph_objs than to write dictionaries. In addition, we can call help on those functions and see which parameters are available for which chart type and it also raises an error with more details if something went wrong. There is also the possibility to export the fig-figure object as a JSON and import it for example in R. Displaying our plotNonetheless, we don’t want a JSON-File but rather an interactive graph. We now have two options, either we publish it online, as Plotly provides a web-service for hosting graphs including a free plan, or we create the graphs offline. This way, we can display them in a jupyter notebook or save them as a standalone HTML. In order to display our plot in a jupyter notebook, we need to execute the following codefrom plotly.offline import iplot, init_notebook_modeinit_notebook_mode(connected=True)at the beginning of each Notebook. Finally, we can display our plot with iplot(fig). Before publishing it online, we first need to set our credentials withplotly.tools.set_credentials_file(username='user.name', api_key='api.key')and use py.plot(fig, filename = 'basic-plot', auto_open=True) instead of iplot(fig). The following graph is published online on Plotly’s plattform and embedded as an inline frame. The chart above is fully interactive, which has multiple advantages:Select and deselect different linesAutomatical scaling of the y-scale in case of deselected linesHover-informations with the exact numbers and datesZoom in and out with self-adjusting date ticksDifferent chart-modes and the ability to toggle additional options, like spike lines Possibility to include a range-slider or buttonsThe graph shows a fairly clear weekly pattern, with Monthly Passholders having their high during the workweek, while Walk-ups are more active on the weekend. Apart from some unusual spikes, the number of rented bikes is higher for Monthly Passholders than for Walk-ups. Visualizing the data as a pie chartThe next question is: how does the total duration look for the different passholder types? First, we need to aggregate our data accordingly. This time we will build a pie chart in order to get the share of the total duration for each passholder type. As we previously did with the line chart, we must first generate a trace object and use Pie() from graph_objs. The arguments we use are different now: we have labels and values instead of x and y. We’re also able to determine, which hover-information we want to display and can add with hovertext custom information, or completely customize it with hovertemplate. Afterward, the trace object goes into go.Figure() in form of a list. share_duration = df.groupby(""Passholder_Type"").sum().reset_index()colors = ""#013848"", ""#0085AF"", ""#00A378""trace = go.Pie(labels=share_duration.Passholder_Type,               values=share_duration.Duration,               marker=dict(colors=colors,                           line=dict(color='white', width=1)),               hoverinfo=""label+percent""              )fig = go.Figure(data=trace)The pie chart shows us, that 59% of the total duration is caused by Walk-ups. Thus, we could assume that the average duration for Walk-ups is higher than for Monthly Passholders.There is one more thing: figure factoryNow, let’s plot the distribution of the average daily duration. For that we use the create_distplot()-function from the figure_factory. The figure factory module contains wrapper functions that create unique chart types, which are not implemented in the nativ plotly.js library, like bullet charts, dendrograms or quiver plots. Thus, they are not available for other languages, like R or Matlab. However, those functions also deviate from the structure for building a Plotly graph we discussed above and are also not consistent within figure_factory. create_distplot() creates per default a plot with a KDE-curve, histogram, and rug, respectively those plots can be removed with show_curve, show_hist and show_rug set to False. First, we create a list with our data as hist_data, in which every entry is displayed as a distribution plot on its own. Optionally, we can define group labels, colors or a rug text, which is displayed as hover information on every rug entry.import plotly.figure_factory as ffmean_duration=df.groupby(""Start_Date"", ""Passholder_Type"").mean().reset_index()hist_data = mean_duration.query(""Passholder_Type=='Flex Pass'"").Duration,             mean_duration.query(""Passholder_Type=='Monthly Pass'"").Duration,             mean_duration.query(""Passholder_Type=='Walk-up'"").Durationgroup_labels = ""Flex Pass"", ""Monthly Pass"", ""Walk-up""rug_text = mean_duration.query(""Passholder_Type=='Flex Pass'"").Start_Date,            mean_duration.query(""Passholder_Type=='Monthly Pass'"").Start_Date,            mean_duration.query(""Passholder_Type=='Walk-up'"").Start_Datecolors = ""#013848"", ""#0085AF"", ""#00A378""fig = ff.create_distplot(hist_data, group_labels, show_hist=False,                          rug_text=rug_text, colors=colors)As we assumed, Walk-ups have a higher average duration than monthly or flex passholders. The average daily duration for Walk-ups is peaking at around 0.6 hours and for Monthly and Flex Passholders already at 0.18, respectively 0.2 hours. Also, the distribution for Walk-ups is much flatter with a fat right tail. Thanks to the rug, we can see that for Flex Pass, there are some days with a very high average duration and due to the hover-information, we can immediately detect, which days have an unusually high average renting duration. The average duration on February 2, 2017, was 1.57 hours. Next, we could dig deeper and have a look on the possible reasons for such an unusual activity, for example a special event or the weather. Plotly with RAs mentioned in the beginning, Plotly is available for many languages. At STATWORX, we’re using Plotly mainly in R, especially if we’re creating a dashboard with R Shiny. However, the syntax is slightly different, as the R implementation utilizes R’s pipe-operator. Below, we create the same barplot in Python and in R. In Python, we aggregate our data with pandas, create different traces for every unique characteristic of Trip Route Category, specify that we want to create a stacked bar chart with our different traces and assemble our data and layout object with go.Figure().  total_count = df.groupby(""Passholder_Type"", ""Trip_Route_Category"").size().reset_index(name=""Total_count"") trace0 = go.Bar(   x=total_count.query(""Trip_Route_Category=='Round Trip'"").Passholder_Type,   y=total_count.query(""Trip_Route_Category=='Round Trip'"").Total_count,   name=""Round Trip"",   marker=dict(color=""#09557F""))trace1 = go.Bar(   x=total_count.query(""Trip_Route_Category=='One Way'"").Passholder_Type,   y=total_count.query(""Trip_Route_Category=='One Way'"").Total_count,   name=""One Way"",   marker=dict(color=""#FF8000""))data = trace0, trace1layout = dict(barmode=""stack"")fig = go.Figure(data=data, layout=layout)With R, we can aggregate the data with dplyr and already start our pipe there. Afterward, we pipe the plotly function to it, in the same way we already specified which data frame we want to use. Within  plot_ly(), we can directly address the column name. We don’t have to create several traces and add them with add_trace(), but can define the separation between the different Trip Route Category with the color argument. In the end, we pipe the layout()-function and define it as a stacked bar chart. Thus, with using the pipe-operator, the code looks slightly tidier. However, in comparison to the Python implementation, we are losing the neat functions of the figure factory. basic_bar_chart &lt;- df %&gt;%   group_by(Passholder_Type, Trip_Route_Category) %&gt;%   summarise( Total_count = n()) %&gt;%  plot_ly(x = ~Passholder_Type,           y = ~Total_count,          color = ~Trip_Route_Category ,           type = 'bar',           marker=list(color=c(rep(""#FF8000"",3),rep(""#09557F"",3)))) %&gt;%  layout( barmode = 'stack')The bar plot shows that Walk-ups use their rented bikes more often for Round Trips in comparison to Monthly Passholders, which could be a reason for their higher average duration. ConclusionI hope I could motivate you to have a look at interactive graphs with Plotly instead of using static seaborn or ggplot plots, especially in case of hands-on sessions or dashboards. But there is also the possibility to create an interactive Plotly chart from a ggplot or Matplotlib object with one additional line of code.With version 3.0 of plotly.py there have been many interesting new features like Jupyter Widgets, the implementation of imperative methods for creating a plot and the possibility to use datashader. Soon you’ll find a blog post on here on how to implement zoomable histograms with Plotly and Jupyter Widgets and why automatic rebinning makes sense by a colleague of mine. ""ÜberMarkus BerrothI am a data scientist at STATWORXand I love creating novel knowledge from data.In my time off,I am always open for a weekend trip..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/plotly-an-interactive-charting-library/;Statworx;  Markus Berroth
  25. Januar 2019;Ensemble Methods in Machine Learning: Bagging und Subagging;"In this blog we will explore the Bagging algorithm and a computational more efficient variant thereof, Subagging. With minor modifications these algorithms are also known as Random Forest and are widely applied here at STATWORX, in industry and academia.Almost all statistical prediction and learning problems encounter a bias-variance tradeoff. This is particularly pronounce for so-called unstable predictors. While yielding low biased estimates due to flexible adaption to the data, those kind of predictors react very sensitive to small changes in the underlying dataset and have hence high variance. A common example are Regression Tree predictors.Bagging bypasses this tradeoff by reducing the variance of the unstable predictor, while leaving its bias mostly unaffected.MethodIn particular, Bagging uses repeated bootstrap sampling to construct multiple versions of the same prediction model (e.g. Regression Trees) and averages over the resulting predictions.Let’s see how Bagging works in detail:Construct a bootstrap sample  (with replacement) of the original i.i.d. data at hand .Fit a Regression Tree to the bootstrap sample – we will denote the tree predictor by .Repeat Steps one and two  many times and calculate  .OK – so let us take a glimpse into the construction phase: We draw in total  different bootstrap samples simultaneously from the original data. Then to each of those samples a tree is fitted and the (in-sample) fitted values are averaged in Step 3 yielding the Bagged predictor.The variance-reduction happens in Step 3. To see this, consider the following toy example.Let  be i.i.d. random variables with  and  and let . Easy re-formulations show thatWe observe that indeed the variance of the mean is weakly smaller than for the individual random variables while the sample mean is unbiased.It is widely discussed in the literature why Bagging works and it remains an open research question. Bühlmann and Yu (2002) propose a subsampling variant of Bagging, called Subagging, which is more traceable from a theoretical point of view.In particular, Bühlmann and Yu (2002) replace the bootstrap procedure of Bagging by subsampling without replacement. Essentially, we are only changing Step 1 of our Bagging algorithm by randomly drawing  times without replacement from our original data with  and get hence a subset of size . With this variant at hand, it is possible to state upper bounds for the variance and mean squared error of the predictor given an appropriate choice of the subsample size .Simulation Set-UpAs the theory is a little bit cumbersome and involves knowledge in real analysis, we simulate the main findings of Bühlmann and Yu (2002).Let’s compare the mean-squared prediction error (MSPE) of the Regression Tree, Bagged and Subagged predictor and illustrate the theory part a little bit more.In order to do this, we consider the following model    where  is the regression function,  is the design matrix generated from a uniform distribution and  is the error term ().For the true data-generating process (DGP), we consider the following model which is quite frequently used in the machine learning literature and termed „Friedman #1“-model:    where  is the -th column of the design matrix  (for ).As you can see, this model is highly non-linear – Regression Tree models shall therefore be appropriate to approximate our DGP.To evaluate the prediction performance of Bagging and Subagging predictors we conduct a Monte Carlo simulation in Python.We first import the relevant packages.import numpy as npimport sklearn.model_selectionimport sklearn.ensembleimport simulation_classimport mathfrom sklearn.metrics import mean_squared_errorfrom sklearn.tree import DecisionTreeRegressorThe module simulation_class is a user-specified class that we will not discuss in this blog post but in a subsequent one.Further, we specify the simulation set-up:# Number of regressorsn_reg = 10# Observationsn_obs = 500# Simulation runsn_sim = 50# Number of trees, i.e. number of bootstrap samples (Step 1)n_tree = 50# Error Variancesigma = 1# Grid for subsample sizestart_grid = 0.1end_grid = 1n_grid = 100grid_range = np.linspace(start_grid, end_grid, num = n_grid)Below we will explain in more detail for what we need the grid specification.To store our simulation results we set up containers.# Container Set-upmse_temp_bagging = np.empty(shape = (n_obs, n_sim))mse_temp_subagging = np.empty(shape = (n_obs, n_sim))y_predict_bagging = np.empty(shape = (n_obs, n_sim))y_predict_subagging = np.empty(shape = (n_obs, n_sim))mse_decomp = np.empty(shape = (len(grid_range),2))With this initialization at hand, we generate the test and train data by the simulation_class module.#Creation of Simulation-Datatrain_setup = simulation_class.simulation(n_reg = n_reg,                                          n_obs = n_obs,                                          n_sim = n_sim,                                          sigma = sigma,                                          random_seed_design = 0,                                          random_seed_noise =  1)test_setup = simulation_class.simulation(n_reg = n_reg,                                         n_obs = n_obs,                                         n_sim = n_sim,                                         sigma = sigma,                                         random_seed_design = 2,                                         random_seed_noise = 3)f_train = train_setup.friedman_model()X_train, y_train = train_setup.error_term(f_train)f_test = test_setup.friedman_model()X_test, y_test = test_setup.error_term(f_test)As we have generated the data for our „Friedman #1“-model we are now able to simulate the mean squared error of the Bagged predictor and Subagged predictor. In Python, both algorithms are implemented via the BaggingRegressor method of the sklearn.ensemble package. Observe that for the Subagged predictor we need to specify the parameter max_samples in the BaggingRegressor. This ensures that we can draw a subsample size  with subsample fraction  from the original data. Indeed, for the subsample fraction  we have already specified the grid above by the variable grid_range .#Subagging-Simulationfor index, a in enumerate(grid_range):    for i in range(0, n_sim):        # bagged estimator        bagging = sklearn.ensemble.BaggingRegressor(            bootstrap = True,            n_estimators = 50)        y_predict_bagging:,i = bagging.fit(            X_train,            y_train:,i).predict(X_test)                mse_temp_bagging:,i = mean_squared_error(            y_test:,i,             y_predict_bagging:,i)                # subagged estimator        subagging = sklearn.ensemble.BaggingRegressor(            max_samples = math.ceil(a*n_obs),            bootstrap = False,            n_estimators = 50)                y_predict_subagging:,i = subagging.fit(            X_train,            y_train:,i).predict(X_test)                mse_temp_subagging:,i = mean_squared_error(            y_test:,i,            y_predict_subagging:,i)           mse_decompindex, 1 = np.mean(mse_temp_bagging)    mse_decompindex, 2 = np.mean(mse_temp_subagging)On my GitHub-Account you can find additional code which also calculates the simulated bias and variance for the fully grown tree and the Bagged tree.ResultsThe results of our above simulation can be found in Figure 1.Let us first compare the performance in terms of MSPE of the Regression Tree and the Bagged predictor. Table 1 shows us that Bagging drastically reduces the MSPE by decreasing the variance while almost not affecting the bias. (Recall – the mean squared prediction error is just the sum of the squared bias of the estimate, variance of the estimate and the variance of the error term (not reported).)Table 1: Performance of fully grown tree and Bagged PredictorPredictorTree (fully grown)Bagged Tree3.472.946.130.3510.614.26Figure 1 displays the MSPE as a function of the subsample fraction  for the Bagged and Subagged predictor (our above code). Together with Figure 1 and Table 1, we make several observations:We see that both the Bagged and Subagged predictor outperform a single tree (in terms of MSPE).For a subsampling fraction of approximately 0.5, Subagging achieves nearly the same prediction performance as Bagging while coming at a lower computational cost.ReferencesBreiman, L.: Bagging predictors. Machine Learning, 24, 123–140 (1996).Bühlmann, P., Yu, B.: Analyzing bagging. Annals of Statistics 30, 927–961 (2002).Über den AutorRobin KraftI am a working student at STATWORX and currently writing my Master thesis about componentwise boosting..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/ensemble-methods-in-machine-learning-bagging-subagging/;Statworx;  Robin Kraft
  16. Januar 2019;Basic rules for good looking slides and dashboards;"My everyday business is to make things beautiful. I create a lot of illustrations for STATWORX for advertisement. Like my last blog post my new post is inspired by my lovely colleagues. My colleagues are creating, among other things, dashboards to visualize the results of their analyses for our consulting projects and are working with Power Point for our data science, machine learning and artificial intelligence programming workshops. These dashboards and presentations have to be very appealing. Sometimes they ask me, if the elements on a dashboard are arranged nicely or if a slide is looking good. Of course, I do help and at the same time I try to explain, why I’m doing the things the way I do.Beauty is in the eye of the beholder – is it?How the beholder’s opinion is formed is still debated between scientist. But it is proven that symmetry, coloring or similarity are visually pleasing to everybody and you don’t have to be a designer to implement these stylistic tools successfully into your presentations. The Berlin School of “Gestaltpsychologie” wrote all this down in the “Gestaltgesetze” (English: gestalt laws). These laws are a good guideline. But how do you apply these laws in your presentations? In this blog post I will explain the most important ones with some negative and positive examples of Power Point slides and dashboards.Overview of some gestalt laws for visual presentationsLaw of Proximity | German: Gesetz der NäheElements, which are close to each other appear like they belong together. If you want to ensure a uniform appearance, make sure to group related elements.The negative example shows a dashboard with mixed up boxes. In the positive example I grouped some related elements together, harmonized the lenght and width of the boxes and sorted them by size. It is a small step with a big impact. The dashboard appears much tidier.Law of Similarity | German: Gesetz der ÄhnlichkeitYou can also group elements by using same coloration or forms for a consistent visual appearance.I colored the boxes from the positive example of the law of proximity. Now it is even easier for us to see the difference.Law of Prägnanz | German: Gesetz der PrägnanzPrägnanz is a German word and means that something is good to memorize, because of its distinct shape. Means, by using memorable shapes your slide or dashboard will look good in the beholder’s eye and the information will be remembered for a long time.As you can see in the negative example, just text and arrows are looking really boring compared to the positive example of Prägnanz using a distinct shape.Law of Continuity | German: Gesetz der guten FortsetzungIf the elements on each slide or dashboard are designed with continuous colors, shapes and positions it gives a clean and tidy look while browsing through the presentation.First you see two negative examples of continuity. The boxes and the text are on different positions, the shape of the boxes are varied and the coloration is not the same in contrast to the next positive examples.Law of Closure | German: Gesetz der GeschlossenheitClosed structures are preferred in contrast to open structures.The negative example shows a dashboard, which is looking a bit lost. The reason is that the layout structure is open without any contrast. The positive example looks much better, because I closed the structure and grouped related elements by applying the law of similarity.Some hints for the endFirst rule for a designer: Create a grid, which you can hide and show to ensure equal spacing. This is really important for a good-looking layout. It’s fun to take an old school paper and pen and scribble a bit, before implementing the grid in your layout tool!I hope these examples will help you and my colleagues ?? creating visually attractive presentations. If you don’t have a corporate style guide to follow you should think about your layout and colors before filling your slide with information.Über den AutorVivian JeenelI am marketing manager at STATWORX andI post my work, freedownloads and talk about issues that I hope you find interesting, too. In my freetime I love to travel and to attend concerts and festivals..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/basic-rules-for-good-looking-slides-and-dashboards/;Statworx;  Vivian Jeenel
  25. Dezember 2018;STATWORX on Tour: Year-End Event in Belgium;"After a year both challenging and rewarding, STATWORX has taken the opportunity of the calm before the Year-End to retreat from the office for a few days.A remote area in the Belgian countryside was chosen as the location for the weekend getaway, where the team would spend a couple of days reflecting on the events of the past year, gearing up for the challenges of the new one and engaging in various workshops and team-building activities. FridayWe kicked off our weekend retreat by meeting up with our dedicated bus driver Taki, who would accompany us on our way. After a quick team photo in front of the bus  STATWORX was ready for boarding and we were off on our merry way to Belgium.Upon arriving we were impressed by the nicely decorated common room and the expansive wellness area, which would make the retreat that much more relaxing. Hungry from travelling, everyone's faces lit up once a big delivery of pizzas from a local restaurant was brought to our doorsteps. We spent the first evening settling in, relaxing and making extensive use of the pool table over local Belgian beer.SaturdaySaturday was dedicated to workshops and group activities. Among others this included a surprisingly exhausting introduction to Yoga, extended walks and exploration of the surrounding area, baking Christmas snacks and dessert, an in-depth demonstration of Swiss 4pm-snack culture and of course constant discussion about R, its community and our very own helfRlein package of helper functions. As expected this would prompt the Python faction within the Data Science team to quickly interject and claim superiority over R and its packages, mocking the fact that we need to create our own for daily routine operations. The good mood continued over the warm hissing of Raclette-grills running overtime. The evening also brought on the first snow of the year, which made for an excellent excuse to escape the cold and watch the snowfall from the safety of the heated pool area. Others made use of the common room and played various card games and a variation of the popular Mafia role-playing game which let emotions run high. SundayThere was no time to be lost on Sunday morning. After a rich final breakfast everyone started packing their belongings and before long we were already on our way back to Frankfurt. The landscape had been covered by the previous night's snow, which made for a scenic ride back.ConclusionThis weekend getaway was a great way of ending an exciting year at STATWORX on a high note. We had the opportunity to look back on many interesting projects and give a toast on many more to come. The weekend also provided a great opportunity for members of the Team Data Science and the Team Statistics to mix it up a little and get more familiar with each other, which is not a given during the daily project routine. Furthermore, it was one of the rare occasions where members of all the three STATWORX teams, namely the teams from Frankfurt, Zurich and Vienna, got together and could share their experiences and know-how amongst each other.Strenghtened and with high spirits we return to work, looking forward to a new year hopefully equally as challenging and rewarding as the last one!Über den AutorSTATWORX.e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/statworx-on-tour-year-end-event-in-belgium/;Statworx;  STATWORX
  24. Dezember 2018;Dreaming of a white Christmas – with ggmap in R;"With the holidays approaching, one of the most discussed questions at STATWORX was whether we’ll have a white Christmas or not. And what better way to get our hopes up, than by taking a look at the DWD Climate Data Center’s historic data on the snow depth on the past ten Christmas Eves?But how to best visualize spatial data? Other than most data types, spatial data usually calls for a very particular visualization, namely data points overlaying a map. In this way, areal data is automatically contextualized by the geographic information intuitively conveyed by a map.The basic functionality of ggplot2 dosen’t offer the possibility to do so, but there is a package akin to ggplot2 that allows to do so: ggmap. ggmap was written by David Kahle and Hadley Wickham and combines the building blocks of ggplot2, the grammar of graphics as well as the static maps of Google Maps, OpenStreetMap, Stamen Maps or CloudMade Maps. And with all that, ggmap allows us to make really fancy visualizations:Above-average snow depth on Christmas Eve (2008-2017)The original functionalities of ggmap used to be somewhat more general, broad and “barrier-free”, but since those good old days – aka 2013 – some of the map suppliers changed the terms of use as well as mechanics of their APIs. At the moment, the service of Stamen Maps seems to be the most stable, while also being easily accessible – e.g. without registering for an API that requires one to provide some payment information. Therefore, we’re going to focus on Stamen Maps.First things first: the mapConveniently, ggmap employs the same theoretical framework and general syntax as ggplot2. However, ggmap requires one additional step: Before we can start plotting, we have to download a map as backdrop for our visualization. This is done with get_stamenmap(), get_cloudmademap(), get_googlemap() or get_openstreetmap() or the more general get_map(). We’re going to use get_stamenmap().To determine the depicted map cutout, the left, bottom, right and top coordinates of a bounding box, have to be supplied to the argument bbox.Conveniently, there is no need to know the exact latitudes and longitudes of each and every bounding box of interest. The function geocode_OSM() from the package tmaptools, returns whenever possible the coordinates of a search query consisting of an address, zip code and/or name of a city or country.library(scales)library(tidyverse)library(tmaptools)library(ggimage)library(ggmap)# get the bounding box geocode_OSM(""Germany"")$bbox     xmin      ymin      xmax      ymax  5.866315 47.270111 15.041932 55.099161The zoom level can be set via the zoom argument and can range between 0 (least detailed) and 18 (most detailed, quick disclaimer: this can take a very long time). The zoom level determines the resolution of the image as well as the amount of displayed annotations.Depending on whether we want to highlight roads, political or administrative boundaries or bodies of water and land different styles of maps excel. The maptype argument allows to choose from different ready-made styles: ""terrain"", ""terrain-background"", ""terrain-labels"", ""terrain-lines"", ""toner"", ""toner-2010"", ""toner-2011"", ""toner-background"", ""toner-hybrid"", ""toner-labels"", ""toner-lines"", ""toner-lite"" or ""watercolor"".Some further, very handy arguments of get_stamenmap() are crop, force and color:As implied by the name, color defines whether a map should be in black-and-white (""bw"") or when possible in color (""color"").Under the hood get_stamenmap() downloads map tiles, which are joined to the complete map. If the map tiles should be cropped so as to only depict the specified bounding box, the crop argument can be set to TRUE.Unless the force argument is set to TRUE, even when arguments changing the style of a map have been altered, once a map of a given location has been downloaded it will not be downloaded again.When we’ve obtained the map of the right location and style, we can store the “map image” in an object or simply pass it along to ggmap() to plot it. The labels, ticks etc. of axes can be controlled as usual.# getting mapplot_map_z7 &lt;- get_stamenmap(as.numeric(geocode_OSM(""Germany"")$bbox),                           zoom = 7,                           force = TRUE,                           maptype = ""terrain"")# saving plotted map aloneplot1 &lt;- ggmap(plot_map_z7) +  theme(axis.title = element_blank(),        axis.ticks = element_blank(),        axis.text = element_blank())# getting mapplot_map_z5 &lt;- get_stamenmap(as.numeric(geocode_OSM(""Germany"")$bbox),                              zoom = 5,                              force = TRUE,                              maptype = ""terrain"")# saving plotted map aloneplot2 &lt;- ggmap(plot_map_z5) +  theme(axis.title = element_blank(),        axis.ticks = element_blank(),        axis.text = element_blank())# plotting maps togetherplot &lt;- gridExtra::grid.arrange(plot1, plot2, nrow = 1)Example for maptype = „terrain“ with zoom = 7 (left) vs. zoom = 5 (right).Business as usual: layering geoms on topWe then can layer any ggplot2 geom we’d like on top of that map, with the only requirement being that the variables mapped to the axes are within the same numeric range as the latitudes and longitudes of the depicted map. We also can use many extension packages building on ggplot2. For example, we can use the very handy package ggimage by Guangchuang Yu to make our plots extra festive:# aggregating data per coordinatedf_snow_agg &lt;- df_snow %&gt;%  dplyr::mutate(LATITUDE = plyr::round_any(LATITUDE, accuracy = 1),                LONGITUDE = plyr::round_any(LONGITUDE, accuracy = 1)) %&gt;%  dplyr::group_by(LATITUDE, LONGITUDE) %&gt;%  dplyr::summarise(WERT = mean(WERT, na.rm = TRUE))# cutting into equal intervals          df_snow_agg$snow &lt;- as.numeric(cut(df_snow_agg$WERT, 12))# setting below average snow depths to 0                   df_snow_agg &lt;- df_snow_agg %&gt;%  mutate(snow = ifelse(WERT &lt;= mean(df_snow_agg$WERT), 0, snow)) # adding directory of image to usedf_snow_agg$image &lt;- ""Snowflake_white.png""# getting mapplot_map &lt;- get_stamenmap(as.numeric(geocode_OSM(""Germany"")$bbox),                          zoom = 7,                          force = TRUE,                          maptype = ""terrain"")# plotting map + aggregated snow data with ggimageplot &lt;- ggmap(plot_map) +  geom_image(data = df_snow_agg,              aes(x = LONGITUDE,                  y = LATITUDE,                  image = image,                  size = I(snow/20))) + # rescaling to get valid size values  theme(axis.title = element_blank(),        axis.ticks = element_blank(),        axis.text = element_blank())Above-average snow depth on Christmas Eve (2008-2017)We can get creative, playing with all sorts of geoms and map styles. Especially visualization strategies to prevent overplotting can be quite handy:For example, a facetted scatter plot with alpha blending:# getting mapplot_map &lt;- get_stamenmap(as.numeric(geocode_OSM(""Germany"")$bbox),                          zoom = 7,                          force = TRUE,                          maptype = ""watercolor"")# facets per year, scatter plot with alpha blending plot &lt;- ggmap(plot_map) +  geom_point(data = df_snow,              aes(x = LONGITUDE,                 y = LATITUDE,                 alpha = WERT,                 size = WERT),             color = ""white"") +  facet_wrap(. ~ DATE, nrow = 2) +  scale_alpha_continuous(range = c(0.0, 0.9)) +  theme(axis.title = element_blank(),        axis.ticks = element_blank(),        axis.text = element_blank(),         legend.position = ""none"",        strip.background = element_rect(fill = ""white""))Snow depth on Christmas Eve per year (2008-2017)Or density plots with either geom_line() or geom_polygon():# getting mapplot_map &lt;- get_stamenmap(as.numeric(geocode_OSM(""Germany"")$bbox),                          zoom = 7,                          crop = FALSE,                          force = TRUE,                          maptype = ""toner-background"")# density plot: linesplot1 &lt;- ggmap(plot_map) +  geom_density2d(data = df_snow_dens,                  aes(x = LONGITUDE,                      y = LATITUDE,                      color = ..level..),                  size = 0.7) +  scale_color_continuous(low = ""#17775A"", high = ""white"",                         name = """",                         breaks = c(0.01, 0.06),                         labels = c(""little snow"", ""much snow"")) +  theme(axis.title = element_blank(),        axis.ticks = element_blank(),        axis.text = element_blank(),        legend.position = ""left"") # density plot: polygonplot2 &lt;- ggmap(plot_map) +  stat_density_2d(data = df_snow_dens,                   aes(x = LONGITUDE, y = LATITUDE,                       alpha = ..level..),                   fill = ""#CFECFF"",                  geom = ""polygon"") +  scale_alpha(name = """",              breaks = c(0.012, 0.024, 0.036, 0.048, 0.06),              labels = c(""little snow"", """","""", """", ""much snow""))  +  theme(axis.title = element_blank(),        axis.ticks = element_blank(),        axis.text = element_blank())   plot &lt;- gridExtra::grid.arrange(plot1, plot2, nrow = 1)ggsave(plot = plot, filename = ""density.png"",  width = 5.5, height = 3)Average snow depth on Christmas Eve (2008-2017)Our plots give a first taste of what can be done with ggmap. However, based on the historic data, there won’t be a white Christmas for the most of us… Against all odds, I still hope that outside there’s a winter wonderland waiting for you just now.From everybody here at STATWORX happy holidays and a happy new year!ReferencesD. Kahle and H. Wickham. ggmap: Spatial Visualization with ggplot2. The R Journal, 5(1), 144-161. http://journal.r-project.org/archive/2013-1/kahle-wickham.pdfDWD Climate Data Center (CDC): Tägliche Stationsmessungen Schneehöhe in cm für Deutschland, Version v18.3 &amp; recent, abgerufen am 16.12.2018.Map tiles by Stamen Design, under CC BY 3.0. Data by OpenStreetMap, under CC BY SA respectively under ODbL.Über den AutorLea WaniekI am a data scientist at STATWORX, apart from machine learning, I love to play around with RMarkdown and ggplot2, making data science beautiful inside and out..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/dreaming-of-a-white-christmas-with-ggmap-in-r/;Statworx;  Lea Waniek
  24. Dezember 2018;Day 24 – big helper helfRlein;"In the last 23 days I presented one function each day from the helfRlein package we created here at STATWORX. I hope you found some of the functions useful and had some fun discovering new ways of doing things with R! Since today is Christmas, only one thing remains to say:To see all functions you can either check out our GitHub or you can read about them here. If you have any suggestions, please feel free to send me an email or open an issue at GitHub!Have a Merry Christmas and a Happy New Year!Über den AutorJakob GeppNumbers were always my passion and as a data scientist and a statistician at STATWORX I can fullfill my nerdy needs. Also I am responsable for our blog. So if you have any questions or suggestions, just send me an email!.e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/day-24-big-helper-helfrlein/;Statworx;  Jakob Gepp
  13. August 2018;Various Versions of R and Python Logos;"Since a couple of months, I am working for STATWORX as a marketing manager (and I love it). My colleagues asked me many times, if it is possible to do a logo of R or Python, in different colors with transparent background, so the logo fits perfectly in their presentations. Here, I proudly present the result of my work (Download link at the bottom of the post)!Besides Markdown and related frameworks, our great team of employees is working a lot with good old PowerPoint for our consulting and training projects. To visualize their topics, they frequently work with software logos like those of the open source programming languages R and Python. To create a useful, informative and great looking presentation it is often necessary to adapt the design of these logos. Sometimes you need the logo to be grayscale, sometimes black and/or white or fully colored to get the look you want. Since my colleagues rather spend their time developing awesome machine learning models than researching logos on Google image search, I decided to build vectorized versions of R and Python logos using Adobe Illustrator in different color schemes and file formats for them. To do so, I first searched online for vectorized R and Python logos I can build upon. You will find them for example on Wikipedia, however not in different colors or different file formats like PNG or SVG.Rasterized and vectorized imagesWhat are the differences between PNG and SVG? The main difference between the two file formats is that PNG is rasterized and SVG is a vectorized image format. If you zoom or compress the image, rasterized formats will lose quality, sometimes you can even see the pixels. SVG is a vector format, which means that rather the image pixels a path of vectors of the outlines of the image is saved. Other advantages of SVG are, that they can be easily imported, scaled and (re)colored in PowerPoint (actually SVG import im PowerPoint via drag and drop is a pretty recent feature). Furthermore, they are ideal for usage on websites, because SVG can be opened with any browser on any mobile device and it is freely scalable. SVG can be exported as code, so you can edit the logo using a code editor (if you really want this). Given that most logos have paths (except they are saved as a raster format like PNG or JPEG), it is smart to save them as SVG to keep all advantages of a vectorized graphic. Note: Since SVG files are actually XML code, they allow for injecting malicious code. That is why some website editors (like WordPress) disallowed the usage of SVG by default. However, since my colleagues use the SVG logos in their PowerPoint presentations, I do not care much about web security issues. Below, you’ll see the difference between a rasterized and vectorized image.DownloadYou can download the R logos here on our Github page. You can use them, as it is stated here on their site. The archive includes the R logo colored, black, white, light grey, dark grey as SVG and (if it is enough for your purposes) as PNG. The Python logo is a trademark of the Python Software Foundation. That’s why I do not upload them for you. I hope it will be as useful for you as for my colleagues!Über den AutorVivian JeenelI am marketing manager at STATWORX andI post my work, freedownloads and talk about issues that I hope you find interesting, too. In my freetime I love to travel and to attend concerts and festivals..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/various-versions-of-r-and-python-logos/;Statworx;  Vivian Jeenel
  13. Juli 2018;Using Machine Learning for Causal Inference;"Machine Learning (ML) is still an underdog in the field of economics. However, it gets more and more recognition in the recent years. One reason for being an underdog is, that in economics and other social sciences one is not only interested in predicting but also in making causal inference. Thus many ""off-the-shelf"" ML algorithms are solving a fundamentally different problem. We here at STATWORX are also facing a variety of problems e.g. dynamic pricing optimization. ""Prediction by itself is only occasionally sufficient. The post office is happy with any method that predicts correct addresses from hand-written scrawls…But most statistical surveys have the identification of causal factors as their ultimate goal."" – Bradley EfronIntroductionHowever, the literature of combining ML and casual inferencing is growing by the day. One common problem of causal inference is the estimation of heterogeneous treatment effects. So, we will take a look at three interesting and different approaches for it and focus on a very recent paper by Athey et al. which is forthcoming in ""The Annals of Statistics""1. Model-based Recursive PartitioningOne of the earlier papers about causal trees is by Zeileis et al., 20082. They describe an algorithm for Model-based Recursive Partitioning (MOB), which looks at recursive partitioning for more complex models. They fit at first a parametric model to the data set, while using Maximum-Likelihood, then test for parameter instability for a set of predefined variables and lastly split the model with the variable regarding the highest parameter instability. Those steps are repeated in each of the daughter nodes till a stopping criterion is reached. However, they do not provide statistical properties for the mob and the partitions are still quite large.Bayesian Additive Regression TreeAnother paper uses Bayesian Additive Regression Tree (BART) for the estimation of heterogeneous treatment effects3. Hereby, one advantage of this approach is, that BART can detect and handle interactions and non-linearity in the response surface. It uses a Sum-of-Tree Model. First, a weak-learning tree is grown, whereby the residuals are calculated and the next tree is fitted according to these residuals. Similar to Boosting Algorithms, BART wants do avoid overfitting. This is achieved by using a regularization prior, which restricts overfitting and the contribution of each tree to the final result.Generalized Random ForestHowever, this and the next blog post will be mainly focused on the Generalized Random Forest (GRF) by Athey et al., who have already been exploring the possibilities of ML in economics before. It is a method for non-parametric statistical estimation, which uses the basic ideas of the Random Forest. Therefore, it keeps the recursive partitioning, subsampling and random split selection. Nevertheless, the final outcome is not estimated via simple averaging over the trees. The Forest is used to estimate an adaptive weighting function. So, we grow a set of trees and each observation gets weighted equalling how often it falls into the same leaf as the target observation. Those weights are used to solve a ""local GMM"" model. Another important piece of the GRF is the split selection algorithm, which emphasizes maximizing heterogeneity. With this framework, a wide variety of applications is possible like quantile regressions but also the estimation of heterogeneous treatment effects. Therefore, the split selection must be suitable for a lot of different purposes. As in Breiman's Random Forest, splits are selected greedily. However, in the case of general moment estimation, we don't have a direct loss criterion to minimize. So instead we want to maximize a criterion ? , which favors splits that are increasing the heterogeneity of our in-sample estimation. Maximizing ? directly on the other side would be computationally costly, therefore Athey et al. are using a gradient-based approximation for it. This results in a computational performance, similar to standard CART- approaches. Comparing the regression forest of GRF to standard random forestAthey et al.  are claiming in their paper that in the special case of a regression forest, the GRF gets the same results as the standard random forest by Breiman (2001). So, one already implemented estimation method in the grf-package4 is a regression forest. Therefore, I will compare those results, with the random forest implementations of the randomForest-package as well as the implementation of the ranger-packages. For tuning porpuses, I will use a random search with 50 iterations for the randomForest and ranger-package and for the grf the implemented tune_regression_forest()-function. The Algorithms will be benchmarked on 3 data sets, which have been already used in another blog post, while using the RMSE to compare the results. For easy handling, I implemented the regression_forest() into the caret framework, which can be found on my GitHub.Data SetMetricgrfrangerrandomForestairRMSE0.250.240.24bikeRMSE2.902.412.67gasRMSE36.032.634.4The GRF performs a little bit worse in comparison with the other implementations. However, this could be also due to the tuning of the parameters, because there are more parameters to tune. According to their GitHub, they are planning on improving the tune_regression_forest()-Function.One advantage of the GRF is, that it produces unbiased confidence intervals for each estimation point. In order to do so, they are performing honest tree splitting, which was first described in their paper about causal trees5. With honest stree splitting, one sample is used to make the splits and another distinct sample is used to estimate the coefficients. However, standard regression is not the exciting part of the Generalized Random Forest. Therefore, I will take a look at how the GRF performs in estimating heterogeneous treatment effects with simulated data and compare it to the estimation results of the MOB and the BART in my next blog post.ReferencesAthey, Tibshirani, Wager. Forthcoming.""Generalized Random Forests""Zeileis, Hothorn, Hornik. 2008.""Model-based Recursive Partitioning"" Hill. 2011.""Bayesian Nonparametric Modeling for Causal Inference""https://github.com/swager/grfAthey and Imbens. 2016.""Recursive partitioning for heterogeneous causal effects.""Über den AutorMarkus BerrothI am a data scientist at STATWORXand I love creating novel knowledge from data.In my time off,I am always open for a weekend trip..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/using-machine-learning-for-causal-inference/;Statworx;  Markus Berroth
  9. Juli 2018;Food for Regression: Using Sales Data to Identify Price Elasticity;"A few hundred meters from our office, there is a little lunch place. It is part of a small chain that specializes in assemble-yourself, ready-to-eat salads. When we moved into our new office a few years ago, this salad vendor quickly became a daily fixture. However, overtime, this changed. We still eat there regularly, but I am certain, if one were to look at their STATWORX – related turnover the trend would not delight management and the question is why?The answer has a lot to do with the arrival of new competitors, improved cooking skills, elaborate promotions and certainly also pricing. It is the latter – pricing – that will be at the center of this series.When analyzing pricing related issues, it is often of essential interest to have a measure of how some change in price affects demand. The measure generally agreed upon by economists to describe this relationship is that of price elasticity of demand, . As a relative measure, it is unit independent, which turns it into a winner. Elasticity is defined as the percent change in quantity divided by percentage change in price:    Conceptually, three conditions are commonly distinguished: elasticity scores of  indicate an ‚elastic demand.‘ This means that if one increases the price by one percent, the quantity of demand decreases by more than one percent. The other two conditions are elasticities of demand , in which case we speak of ‚inelastic demand‘ and the case when elasticity equals . This is called ‚unit elastic demand.‘Being able to deduce the actual price elasticity of demand for our salad bar would be of great help. With a reliable elastic score at hand, we can answer questions like: How many salads can we expect to sell at a given price? How does a price change of 10% affect demand? With this knowledge, it would be possible to utilize one’s pricing as a tool in order to target different salad-business KPIs. Eventually, the salad bar can adjust its price in order to maximize profit or to increase sales – depending on their strategic objectives.It is the intricacies of deriving this price elasticities of demand with regressions that will be the subject of this short blog. The situation of our salad vendor makes for an illustrative case on how we can calculate price elasticity and how they can be used to adjust one’s pricing strategy.SetupTo be upfront – although this salad vendor exists, and it is in fact an integral part in the STATWORX food chain – all the data we work with is made up. It describes, how I imagine this little salad vendor’s market situation to be. With each new post, as we examine more complex issues, we will delve deeper into the intricacies of the salad vendor’s world.The question of this blog post is simple: How can we use linear regression to derive price elasticities? To explore this, we need historic prices and sales information. To begin, there will be no consideration of competition, no in-store alternatives, no new promotional activates, no seasonal-effects, or anything else.Daily sales prices of the past two years were simulated for our little salad bar by randomly selecting prices between 5.59€ and 9.99€ – clearly not a great pricing strategy, but it suffices for this post’s purposes. A multiplicative demand function was used to derive sales with some randomness added. And with that we are done simulating the data. For more details, check out the code at our Githubpage.Calculating Elasticity of DemandWe want to know how a linear regression function relates to elasticity. It turns out that this depends on how the variables have been transformed. It is possible to deduce elasticity – a factor of relative of change – in almost any situation. Here you find the four most common transformations.TransformationFunctionElasticityLevel-LevelLog-LevelLevel-LogLog-LogDependent on the pre-regression variable transformation, different post-regression transformations are necessary in order derive the elasticity scores. The table above shows that in the case of a log-log model, the elasticity is a constant value across the entire demand curve; while in all other cases, it is dependent on the specific current price and/or demand. This means that the choice of the model is indicative of the assumed demand curve. Choosing wrongly results in a misspecified model.This is great to know, but which model should one use? To evaluate this, I simply ran each of these four models. The results you can find in the table below, but they are nothing like you will ever find in the real world, in that all effects are highly significant and the  is ridiculously high for any social or economic analysis. This is by design as hardly any randomness was added. In addition, the data was setup up in a way that the log-log model was predestined to generate the winning model.ModelInterceptPrice Variable ElasticityLevel-Level439.58 (3.2)-38.57 (0.42)-2.500.84Log-Level6.59 (0.01)-0.23 (0.01)-1.630.95Level-Log671.22 (3.53)-265.66 (1.83)-2.110.93Log-Log7.86 (0.01)-1.52 (0.01)-1.510.97The argument is not that a log-log model is the best model to derive elasticities. Although, there are strong microeconomics arguments to be made about why the log-log model is the most reasonable model to describe demand elasticity. The underlying demand curve describes demand most like economists assume it to behave. It ensures that demand cannot sink below zero as the price increases and on the other side demand exponentially grows as the price decreases. Yet, the deductibility of a constant elasticity value, as aforementioned, is its most desirable feature. This fact makes it much easier to apply elasticity to optimize pricing.Still, empirical analysis might guide us to assume other price-demand relationships. The graphic below shows this in an illustrative way. The legend of the graph orders the models in increasing order of fit. Looking at each graph, it becomes clear why the level-log model fares better than the level-level model, and why the log-level model outperforms the level-log model and so on. The non-linear relationship between price and demand that we introduced by relying a multiplicative demand curve is best described by the log-log model. Had I used an additive demand curve the ranking would have been the other way around. Thus, the argument is that under certain circumstances the model choice can have a significant impact.For the application in practice we have to be very aware of the functional form that is indicated by the regression we chose. The effects can have severe consequences. The elasticity with which the data was generated was -1.5. In order to illustrate the effect that model choice can have on the estimated elasticity, I calculated average elasticities for level-level, log-levvel and level-level models and compared it with the price coefficient of the log-log model. This is a bit of an oversimplification, but the point still stands: The results are substantially different, which has consequences when one tries to utilize the deducted elasticities. Yet, based on the graphical analysis and the model information, we would come to the conclusion that the log-log fares best, so we can proceed as theory would want us to.Price OptimalizationBefore we finish, let’s quickly look at how we can use elasticity to improve the little salad vendor’s erratic pricing strategy (my random daily price change). For this we need to know the salad bar’s cost function. Luckily, we do: it has fix costs of about 300€ for every day it is open. The preparation of a single salad costs about 2.50€ per salad. The cost function is thus:    Microeconomic theory teaches us that fix costs do not matter when calculating elasticity based margin optimized prices. I’ll spare you the details, but the function to calculate the optimized price eventually states:    Applying the elasticity derived from the log-log model, this results in a proposed optimized price that lies somewhere between 7.21€ and 7.47€. The estimation is 7.34€. Instead of daily changing its price randomly, it is best to stick to prices in this range. The salad vendor can expect to sell around 125 Salads each day, ensuring a daily profit of between 287€ and 320€.KPIsLowerbound ElasticityExp. ElasticityUpperbound ElasticityElasticity-1.51-1.52-1.54Opt. Price7.43€7.30€7.17€Quantity126125125Profit319€302€285€This is a daring statement. With the actual example data the conclusion would be fine. But in practice such perfect regression results, with so little uncertainty, are unrealistic. And this is where it tends to get tricky. To illustrate this point, I adjusted the standard error from almost nonexistent to 0.15. The results should still be highly significant, but looking at the table below one would be surprised about the consequences of such small changes.KPIsLowerbound ElasticityExp. ElasticityUpperbound ElasticityElasticity-1.23-1.52-1.82Opt. Price13.51€7.30€5.57€Quantity106125114Profit864€302€51€The certainty with which we proposed the optimal price was very much unfounded. In this example, the range for elasticity still is relatively small despite the increased uncertainty. Yet, the resulting price range for the ideal price is between 5.58€ and 13.73€, which is not a very precise proposal. The price range actually exceeds the highest price that the little salad vendor ever dared to set. The consequences are severe: the resulting profit varies almost sixteen-fold between the highest and lowest prices.To state the obvious, the illustrated approach to elasticity calculations is just the tip of the iceberg. Meaning that we need to invest time into improving the current approach. The next posts will focus on intervening factors like promotional activities and similar products.Über den AutorDaniel LüttgauI am a data science consultant at STATWORX.Exploring business concepts and thinking up ways to utilize data for our customers is what I enjoy most about my job. My freetime is commited to my dog, travelling and my wife and friends.";https://www.statworx.com/de/blog/food-for-regression-using-sales-data-to-identify-price-elasticity/;Statworx;  Daniel Lüttgau
  6. Juli 2018;A framework to automate your work: Why Aiflow?;"Recently we at STATWORX faced the usual situation where we needed to transform a proof of concept (POC) into something that could be used in production. The ""new"" aspect of this transformation was that the POC was loaded with a tiny amount (a few hundred megabytes) but had to make ready for a waste amount of data (terabytes). The focus was to build pipelines of data which connect all the single pieces and automate the whole workflow from database, ETL (Extract-Transform-Load) and calculations, till the actual application. Thus, the simple master-script which calls one script after another was not an option anymore. It was relatively clear that a program or a framework which uses DAG's was necessary. So, within this post, I will swiftly go over what a DAG is in this context, what alternatives we considered and which one we have chosen in the end. Further, there will be a second Part explaining more detailed how the workflow with Airflow looks like, e.g., some hello-world program and the whole setup. So what's a DAG?DAG is the acronym for Directed Acyclic Graph and is a mathematical concept to represent points/knots in relation to each other visually without any cycles and a precise order. In other words, it is just a bunch of knots which are connected to each other (left part of the image below). Next, we add relationships between each of them (middle part of the picture below) which point in a particular direction, and lastly, we restrict the connections do not form any cycles in between the knots (right part of the images below). In programming, we can use this model and define every single task as a knot in the graph. Every job that can be done independently will be an initial knot with no predecessors and as such will have no relation point towards him. From there on we will link those tasks, which are directly dependent on it. Continuing this process and connect all task to the graph we can manifest a whole project into a visual guide. Even though this might be trivial for simple projects like 'First do A then B and finally C', once our workflow reaches a certain size or needs to be scalable, this won't be the case anymore. Following it is advisable to express it in a DAG such that all the direct and indirect dependencies are expressed. This representation isn't just a way to show the context visually such that also non-technical people could grasp what is happening, but also gives a useful overview of all processes which could run concurrently, and which sequentially. Just imagine if you have a workflow made up out of several dozen tasks (like the one above) which consists of some that need to run sequentially and some that could run in parallel. Just imagine if one of these tasks failed, without the DAG it wouldn't be clear what should happen next. Which one needs to wait until the failed one finally succeed? Which can keep running since they do not depend on it? With a DAG this question can be answered quickly and doesn't even come up if a program is keeping track of it. Due to this convenience, a lot of software and packages adopted this representation to automate workflows.What were we looking for?As mentioned we were looking for a piece of software, a framework or at least a library as aorchestrator which works base on a DAG. As we would need to keep track of the whole jobs manually – when does a task start, what if a task fails – what does it mean to the whole workflow. This is especially necessary as the flow needed to be executed every week and therefore monitoring it by hand would be tedious. Moreover, due to the weekly schedule and inbuilt or advanced scheduler would be a huge plus. Why Advanced? – There are simple schedulers like cron which are great for starting a specific job and a specific time but would not integrate with the workflow. So, one that also keeps track of the DAG would be great. Finally, it would also be required that we could extend the workflow easily. As it needed to be scalable it would be helpful if we could call a script – e.g., to clean data-  several times just with a different argument – for different batches of data-  as different nodes in the workflow without much overhead and code.What were our options?Once we settled the decision, that we need to implement some orchestrator who is based on a DAG, a wide variety of software, framework and packages popped up in Google Search. It was necessary to narrow down the amount of possibility so only a few were left which we could examine in deep. We needed something that was neither to heavy based on a GUI since it limited the flexibility and scalability. Nor should it be too code-intensive or in an inconvenient programming language since it could take long to pick it up and get everybody on board. So, options like Jenkins or WAF were thrown out right away. Nevertheless, we could narrow it down to three options:Option 1 – Native Solution: Cloud-OrchestratorSince the POC was deployed on a cloud, the first option was also rather obvious – we could use one of the native orchestrators. They offered us a simple GUI to define our DAG's, a scheduler and were designed to pipe data like in our case. Even though this sounds good, the inevitable problem was that such GUI's are restricted, one would need to pay for the transactions, and there would be no fun at all without coding. Nevertheless, we kept the solution as a backup plan.Option 2 – Apaches Hadoop Solutions: Oozie or AzkabanOozie and Azkaban are both open-source workflow-manager written in Java and designed to integrate with Hadoop Systems. Therefore, they are both designed to execute DAG's, are scalable and have an integrated scheduler. While Oozie tries to offer high flexibility in exchange for usability Azkaban has the trade in the other way around. As such, orchestration is only possible through the WebUI in the case of Azkaban. Oozie, on the other hand, relays on XML-Files or the bash to manage and schedule your work.Option 3 – Pythonic Solution: Luigi or AirflowLuigi and Airflow are both workflow managers written in python and available as open source frameworks. Luigi was developed in 2011 by Spotify and was designed to be as general as possible – in contrast to Oozie or Azkaban which were intended for Hadoop. The main difference compared to the other two is that Luigi is code based rather than GUI-based. The executable workflows are all defined by python-code rather than in a user interface. Luigi's WebUI offers high usability, like searching, filtering or monitoring the graphs and tasks. Similar to this is Airflow which was developed by Airbnb and opened up in 2015. Moreover, it was accepted in the Apache Incubator since 2016. Like Luigi, it is also code-based with an interface to increase usability. Furthermore, it comes with an integrated scheduler so that one doesn't need to rely on cron.Our decisionOur first criteria for further filtering was that we wanted a code based orchestrator. Even though interfaces are relatively straightforward to pick up and easy to get used to, it would come at the cost of slower development. Moreover, editing and extending would also be time-consuming, if every single adjustment would require clicking instead of reusing function or code snippets. Therefore, we turned option number one – the local Cloud-Orchestrator. The loss of flexibility shouldn't be underestimated. Any learning or takeaways with an independent orchestrator could likely apply to any other project. This wouldn't be the case for a native one, as it is bound to the specific environment. The most significant difference between the other two options where the languages in which they operate. Luigi and Airflow are Python-based while Oozie and Azkaban are based on Java, and bash scripts. Also, this decision was easy to determine, as python is an excellent scripting language, easy to read, fast to learn and simple to write. With the aspect of flexibility and scalability in mind, python was offering us a better utility compared the (compiled) programming language Java. Moreover, the workflow definition needed to be either down in a GUI (again) or with XML. This way we could also exclude option two.The last thing to settle was either to use Spotify's Luigi or Airbnbs Airflow. It was a decision between taking the mature and stable or go with the young star of the workflow managers. Both projects are still maintained and highly active on GitHub with over several thousand commits, several hundred times being stared and several hundred of contributors. Nevertheless, there was one aspect which was mainly driving our decision – cron. Luigi can only schedule jobs with the usage of cron, unlike Airflow which has an integrated scheduler. So, what is the problem with cron?Cron works fine if you want one job done at a specified time. However, once you want to schedule several jobs which depend on each other, it gets tricky. Cron does not regard these dependencies whether one task scheduled job needs to wait until the predecessor is finished. Let's say we want a job to run every five minutes and transport some real-time data from a database. In case everything goes fine, we will not run into trouble. A job will start, it will finish, the next one starts and so on. However, what if the connection to the database isn't working? Job one will start but never finish. Five minutes later the second one will do the same while job one will still be active. This might continue until the whole machine is blocked by unfinished jobs or crashes. With Airflow, such a scenario could be easily avoided as it automatically stops starting new jobs when requirements are not meet.Summarizing our decisionWe chose apache Airflow over the other alternatives base on: No cron – With Airflows included scheduler we don't need to rely on cron to schedule our DAG and only use one framework (not like Luigi) Code Bases – In Airflow all the workflows, dependencies, and scheduling are done in python code. Therefore, it is rather easy to build complex structures and extend the flows.Language – Python is a language somewhat natural to pick up and was available on our team. Thus, Airflow fulfills all our needs. With it, we have an orchestrator which keeps track of the workflow we define by code using python. Therefore we could also easily extend the workflow in any direction – more data, more batches,  more steps in the process or even on multiple machines concurrently won’t be a problem anymore. Further Airflow also includes a nice visual interface of the workflows such that one could also easily monitor it. Finally, Airflow allows us to renounce crone as it comes with an advanced onboard scheduler. One that can not only start a task, but also keeps track of each and is highly customizable in its execution properties. In the second Part of this blog we will look deeper into Airflow, how to use it and how to configure it for multiple scenarios of usage.Über den AutorMarvin TaschenbergerI am data scientist at STATWORX,I love Python, Postgres Scala (&amp; Akka) and Docker. Doesn't matter what problem you have, with those four, I'll solve it..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/a-framework-to-automate-your-work-why-aiflow/;Statworx;  Marvin Taschenberger
  19. März 2018;Dive – the debugging function you deserve;"A programmer’s miseryWriting R functions is always fun up until the point where you have to debug them. Especially if you are layering functions. In the best case, you can just use the native debug functionalities of R. However, in my opinion this is only feasible when you code in RStudio, because you can use the interface to maneuver around. If you like to code in another editor, e.g. Sublime or Notepad++ or just don’t want to use the debug functionalities of R, then you are probably familiar with the following scenario:Right, this is painful to watch. Executing line by line, trying not to execute the pesky comma? Ugh. Honestly, if you have two to three arguments it is not that bad, but having large functions with 20+ arguments this always becomes a programmer’s misery. By now you probably know where this is headed. Yupp, we are going to use a function to help us out of that misery. You can get it on the STATWORX as well as my own GitHub account. Introducing diveThe function really is as simple as it gets. It has two arguments, a string and the other is a switch for the output format. The string is the function call you want to debug. So, after seeing the long and stony path of debugging without dive, here’s what you must do using the function:Essentially, you are just pasting the whole junk into the function and as a result get the function’s arguments printed into the console. From there you can copy them conveniently into the editor of your choice. The only thing you must look out for are quotation marks in your function call. You have to escape all quotation marks in your function call before executing the dive call. If you are not a fan of all these copy and paste shenanigans, there is an option to return the function arguments as a list. The advantage of this option in my opinion is that you do not have to paste the whole stuff somewhere in your code, but you have one list with all arguments. Of course, you can adjust this list as you please, like you would change it in the console option. When all your options are correctly specified you can just use the function list2env(your_dive_list_return) to evaluate all function’s arguments. Since, sometimes this might not be desirable, for instance, if you are 100% sure you don’t need the arguments a second time, dive can evaluate the arguments directly into your environment by setting return = “env”.  But wait, there’s more…If you are not sold by now, then I might catch you with the following feature. You can also use dive with any apply function. This feature was the main motivation for me to code a debug supporting function. In my projects, I try to circumvent using any kind of loops whenever it is feasible. Thus, I code many functions and use them with an apply function instead. However, this is very painful to debug with the native debug functionalities of R and RStudio. With dive you can stop at every function level and change the inputs conveniently. So, how can you use the function with applies? Well, just like you would use it without apply functions. Just paste the stuff into dive. Of course, you are limited with the output option when you want to debug apply functions, since the iterator cannot be evaluated in the environment. However, the output formats list and console are still valid.If you have any ideas how to improve the concept or if you are missing some features, just drop me an e-mail.Über den AutorAndré BleierThe most exciting part of being a data scientist at STATWORX is to find this unique solution to a problem by fusing machine learning, statistics, and business knowledge..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/dive-the-debugging-function-you-deserve/;Statworx;  André Bleier
  16. März 2018;Empirische Bestimmung von Elastizität – Teil 2;"Wie kann man Preiselastizität bestimmen? Die Antwort auf diese Frage ist nicht eindeutig, sondern fallspezifisch. Es gibt viele Verfahren, um Preiselastizität empirischzu bestimmen. DirekteExpertenbefragungen, Kundenbefragungen, indirekte Kundenbefragungen durch Conjoint Analysen und vielfältige experimentelle Testmethoden.Wenn die Datenlage es erlaubt, wenden wir Methoden an, die auf historischen Marktdaten basieren. Unterfüttert mit Daten zu Faktoren, wie Wettbewerbspreise, Werbeinformationen etc., lassen sich hier durch am erfolgreichsten die entscheidenden Wirkungsmechanismen für eine dynamische und automatisierte Preisgestaltung identifizieren.In drei Blogposts sollen die verschiedenen Verfahren beleuchtet und ihre Vor- und Nachteile aufbereitet werden. Im vergangen ersten Teil wurden befragungsbasierten Verfahrenbesprochen. In diesem zweiten Teil wird der Fokus auf experimentellen Methoden und Conjoint Analysen liegen. Im darauf folgenden und abschließenden Post sollen gängige regressionsbasierte Verfahren Thema sein.Die im vorherigen Blogbeitrag beschrieben Expertenbefragungen und direkte Kundenbefragungen sind in vielerlei Hinsicht schwierig. Experteninterviews vernachlässigen systematisch die Kundenperspektive. Kundenbefragungen sind inhaltlich problematisch, da sie fast zwingend eine isolierte Fokussierung auf die Preissetzung bewirken. Dies steht im Widerspruch zum eigentlichen Kaufprozess, der viel wahrscheinlicher einer Kosten-Nutzen-Abwägung entspricht. Indirekte VerfahrenUm diesen Problemen auszuweichen, wird daher häufig auf indirekte Befragungsmethoden zurückgegriffen. Durch indirekte Befragungsverfahren sollen die Kaufsituation und die Kundenentscheidung realistischer abgebildet werden, als dies durch Befragungen möglich ist. Ein zentrales Konzept sind dabei Conjoint Analyse Verfahren. Conjoint Analysen sind ein zentrales und etabliertes Marketinginstrument mit vielfältigen auf die jeweilige Problemstellung zugeschnittene Varianten.Grundsätzlich sollen Conjoint Analysen dabei helfen, die tatsächlichen Abwägungen und Präferenzen von Kunden zwischen verschiedenen Bedürfnissen besser nachzuvollziehen. Durch ein umfassenderes Verständnis der Kundenpräferenzen sollen dann Rückschlüsse auf die konkrete Fragestellung abgeleitet werden. Prinzipiell können so durch Conjoint Analysen vielfältige und komplexe Fragestellungen beantwortet werden.Um die Zahlungsbereitschaft von Kunden abzufragen, werden Kunden z.B. verschiedene sogenannte Produkt-Preis-Profile zur Auswahl gestellt. Die verschiedenen Auswahlmöglichkeiten unterscheiden sich hinsichtlich im Voraus definierter Merkmale und bzgl. des Preises. Die Teilnehmer sind aufgefordert, zwischen diesen Alternativen zu wählen und Ihre Präferenzen darzulegen. Basierend auf diesen Präferenzaussagen werden die Wirkungen des Preises und der verschiedenen Produktmerkmale herausgearbeitet.Der Erfolg von Conjoint Verfahren hängt wesentlich von den vorbereitenden Schritten bei der Produkt-Preis-Profi-Gestaltung ab. Eine Vielzahl von Entscheidungen müssen getroffen werden, z.B.:Wie viele verschiedene Merkmale sollen verglichen werden? Dies ist nicht trivial, denn es kann schnell unübersichtlich werden. Entscheidet man sich für nur 3 Merkmale mit jeweils 3 Ausprägungen, so konfrontiert man den Konsumenten bereits mit bis zu 27 Alternativen. Wollte man hier mit Paarvergleichen arbeiten, dann müsste sich der Teilnehmer theoretisch durch 351 Paarvergleich arbeiten. In der Praxis werden natürlich Wie viele Ausprägungen sollen je Merkmal auswählbar sein? Studien zu Folge werden Produkte mit mehr Auswahlmöglichkeiten von Teilnehmern als wichtiger erachtet. Es gibt viele ähnliche Beispiele dafür wie das Testdesign das Ergebnis beeinflusst. Problematisch wird dies, wenn die Wirkung des Testdesigns den tatsächlichen Kaufprozess entstellt. Wie sollen den Teilnehmern die verschiedenen Alternativen präsentiert werden? Sollen alle Alternativen mit ihren unterschiedlichen Preisen gleichzeitig präsentiert werden (Vollprofile) oder sind Paarvergleiche besser? Vollprofile sind realitätsnäher, aber konfrontieren den Teilnehmer mit einer komplexeren Problemstellung, wodurch evtl. keine klaren Ergebnisse abgeleitet werden können. Der Anwender von Conjoint Verfahren wird mit der komplexen Anforderungspalette konfrontiert, der man bei Experimenten oder bei der Fragebogenausgestaltung ebenfalls begegnet. Dennoch sind Conjoint Analysen ein beliebtes und häufig angewandtes Verfahren. Insbesondere Computergestützte Methoden helfen inzwischen die Komplexität zu reduzieren.Ähnlich wie die anderen bereits beschriebenen Verfahren sind Conjoint-Analysen kein Instrument, das für ein fortlaufendes systematisches Elastizitäts-Monitoring geeignet ist. Conjoint Verfahren können für explorative Analysen, z.B. im Fall von Produktneueinführungen, ein sehr nützliches Instrument darstellen. Gilt es aber ein fortlaufendes, reaktives Preisgestaltungsinstrument zu entwickeln, so wird das Verfahren an seine Grenzen stoßen, insbesondere dann, wenn man ein breites Produktsortiment hat.ExperimenteBei Experimenten werden Kunden(-Gruppen) in realen oder nachgestellten Kaufsituationen (Feld-bzw. Laborexperimente) verschiedene Preisalternativen vorgegeben. Aus dem Kaufverhalten der Testteilnehmer werden Rückschlüsse bzgl. des Effektes des Preises auf den Absatz abgeleitet. Ähnlich wie bei Conjoint Analysen gibt es bei der Ausgestaltung solcher Tests viele Vorgehensmöglichkeiten. Insbesondere der E-Commerce bietet vielfältige Anwendungsmöglichkeiten, da viele zusätzliche transaktionsspezifische Informationen bei der statistischen Auswertung des Experiments hinzugezogen werden können. Doch auch im Brick-And-Mortar-Bereich können durch klassische Store-Tests wichtige Rückschlüsse auf produktspezifische Zahlungsbereitschaften gewonnen werden.Letztlich gibt es fast keine Grenzen bei der Ausgestaltung von preisbezogenen Experimenten, insbesondere dann, wenn auch Laborexperimente in Frage kommen. Hier stellt sich immer die Frage nach der externen Validität dieser Experimente. Künstliche Situationen können atypisches Verhalten verursachen. Ebenso kann es schwierig sein, aus den Erkenntnissen des Experiments praxisrelevante Schlüsse zu ziehen.Ebenfalls problematisch sind die hohen Kosten, die mit Experimenten einhergehen. Insbesondere mit Feldexperimenten greift man direkt in das aktive Geschäft ein. Jede Preissenkung kann mit Profitverlust einhergehen, jede Preiserhöhung kann Absatzverluste bedeuten. In unseren Projekten mit Kunden nutzen wir Feldexperimente als Validierungswerkzeug für unsere auf Marktdaten basierten Analyseverfahren. Modellgesteuerte Feldexperimente können genutzt werden, um die Prognosegüte eines Modells zu evaluieren und dessen Schwächen aufzudecken.Alle bisher beschriebenen Verfahren haben ihre Funktion und können bei entsprechenden Rahmenbedingungen sinnvoll und richtig sein. Wenn die Datenlage es aber erlaubt, wenden wir Methoden an, die auf historischen Marktdaten basieren. Im abschließenden Post sollen gängige regressionsbasierte Verfahren Thema sein.Über den AutorDaniel LüttgauI am a data science consultant at STATWORX.Exploring business concepts and thinking up ways to utilize data for our customers is what I enjoy most about my job. My freetime is commited to my dog, travelling and my wife and friends..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/empirische-bestimmung-von-elastizitaet-teil-2/;Statworx;  Daniel Lüttgau
  13. März 2018;Empirische Bestimmung von Elastizität – Teil 1;"Wie kann man Preiselastizität bestimmen? Die Antwort auf diese Frage ist nicht eindeutig, sondern fallspezifisch. Es gibt viele Verfahren, um Preiselastizität empirisch zu bestimmen. Direkte Expertenbefragungen, Kundenbefragungen, indirekte Kundenbefragungen durch Conjoint Analysen und vielfältige experimentelle Testmethoden.Wenn die Datenlage es erlaubt, wenden wir Methoden an, die auf historischen Marktdaten basieren. Unterfüttert mit Daten zu Faktoren, wie Wettbewerbspreise, Werbeinformationen etc., lassen sich hierdurch am erfolgreichsten die entscheidenden Wirkungsmechanismen für eine dynamische und automatisierte Preisgestaltung identifizieren.In drei Blogposts sollen die verschiedenen Verfahren beleuchtet und ihre Vor- und Nachteile aufbereitet werden. Der heutige erste Teil beginnt mit befragungsbasierten Verfahren. Der zweite wird sich mit experimentellen Methoden und Conjoint Analysen beschäftigen. Im darauf folgenden und abschließenden Post sollen gängige regressionsbasierte Verfahren Thema sein.ExpertenbefragungenDer auf Befragungen basierende Methodenbereich gliedert sich in Expertenbefragungen und verschiedene Verfahren der Kundenbefragung. Wir beginnen mit Expertenbefragungen. Dabei werden Fachleute mit umfassendem Wissen über den Markt oder ein spezielles Marktsegment, in dem das Unternehmen agiert, zur Preiswirkung befragt. Naheliegende Ansprechpartner sind vertriebsnahe Manager, Mitarbeitern aus dem Vertrieb oder Marketing, aber auch externe Fachleute und Berater. Methodisch kann zwischen unstrukturierten, freien Interviews und strukturierten, häufig Workshop basierten, in klare Vorgehensschritte untergliederte Verfahren gewählt werden. Expertenbefragungen sind in der Regel günstiger und weniger zeitintensiv als Kundenbefragungen. In der Hoffnung, dass Fachleute markttransformierende Ereignisse früher und besser erkennen als andere Verfahren, werden Expertenbefragungen gelegentlich auch unterstützend zu anderen Methoden durchgeführt. Ein Vorteil von Expertenbefragungen ist, dass sie auch in Momenten hoher Ungewissheit zu Ergebnissen führen. Vollständig unerprobter Preisstrategien oder neue Marktsituationen durch neue Wettbewerber oder veränderte Kundenbedürfnisse sind in der Regel nur mit Hilfe von Expertenurteilen abzubilden. Unklar ist aber, welchen Wert diese Ergebnisse haben, denn der mit unbekannten Situationen einhergehenden Unsicherheit sind auch die Experten ausgesetzt. Darüber hinaus kämpfen Expertenbefragungen mit einer Reihe von Problemen: Es gelingt durch sie nicht, die Kundenperspektive mit einzubeziehen. Außerdem können Experten als Folge von systematisch falschen Annahmen oder lange gültigen, aber veralteten,Paradigmen falsche Schlüsse ziehen. Experten kommen außerdem häufig zu stark divergierenden Prognosen, die dann schwer vereinbar sind. Insgesamt gilt, dass Expertenbefragungen für ein fortlaufendes Elastizitäten-Monitoring eher ungeeignet sind. Andere analytische Verfahren sind in höherer Frequenz und größerem Umfang durchführbar. In schnelllebigen Marktumfeldern oder für Akteure mit großen Produktsortimenten sind dies entscheidende Nachteile von Expertenbefragungen.KundenbefragungenBei direkten Befragungen werden Kunden durch Fragen wie „Zu welchem Preis würden Sie dieses Produkt gerade noch kaufen?“ zu ihrer Zahlungsbereitschaft befragt. Die Angaben werden genutzt, um Elastizitätswerte abzuleiten. Durch eine bewusste Fragekonstruktion oder eine gezielte Abfolge können zusätzliche Sachverhalte abgefragt werden. Die ergänzende Frage: „Ab welchem Preisunterschied würden Sie von Produkt X zu Produkt Y wechseln?” erlaubt beispielsweise die gezielte Abfrage von Kreuzelastizitäten zwischen spezifischen Substituten.Direkte Befragungen sind in vielerlei Hinsicht schwierig. Insbesondere im Kontext von Konsumgütern mit einer Vielzahl an Produkten und Alternativen wird das Abfragen von Zahlungsbereitschaften aufwendig. Inhaltlich problematisch ist die fast zwingend isolierte Fokussierung auf die Preissetzung. Dies steht im Widerspruch zum eigentlichen Kaufprozess, der viel wahrscheinlicher einer Kosten-Nutzen-Abwägung entspricht. Studien weisen auch auf eine Diskrepanz zwischen tatsächlichem und in Umfragen bekundeten Verhalten hin.Um diesen Problemen auszuweichen, wird daher häufig auf indirekte Befragungsmethoden in Form von Conjoint Analyse oder experimentelle Testdesigns zurückgegriffen. Durch diese Verfahren sollen die Kaufsituation und die Kundenentscheidung realistischer abgebildet werden, als dies durch Befragungen möglich ist. Diese alternativen Verfahren sind Thema des nächsten Beitrags.Über den AutorDaniel LüttgauI am a data science consultant at STATWORX.Exploring business concepts and thinking up ways to utilize data for our customers is what I enjoy most about my job. My freetime is commited to my dog, travelling and my wife and friends..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/empirische-bestimmung-von-elastizitaet-teil-1/;Statworx;  Daniel Lüttgau
  8. März 2018;Als Data Science Praktikant bei STATWORX;"Neben dem Einstieg als Trainee oder Data Science Consultant bei STATWORX gibt es ebenso die Möglichkeit, ein Praktikum im Bereich Data Science zu absolvieren. Unsere aktuellen Stellenausschreibungen findet ihr übrigens hier.Bewerbung bei STATWORXDas Berufsbild des Data Scientists ist durch seine vielfältigen Aufgaben und die bunte Durchmischung der Kompetenzen vor allem in den letzten Jahren sehr attraktiv geworden. Dies spiegelt sich auch in den Suchanfragen bei Google zum Begriff Data Scientist. Da ich mich während meiner Bachelorarbeit bereits mit den Themen des maschinellen Lernens und der Datenanalyse ausführlich beschäftigt hatte, wollte ich sehr gerne mehr über diese Themen im Rahmen eines Praktikums lernen. Mit STATWORX habe ich das passende Unternehmen sehr schnell gefunden und nicht lange gezögert mich als Data Science Praktikant zu bewerben. Der gesamte Bewerbungsprozess von Telefoninterview über ein Vorstellungsgespräch lief innerhalb einer Woche wirklich sehr schnell ab. Wenn ihr gerne mehr über den Hintergrund von uns wissen wollt, so beschreibt Fabian Müller (Head of Data Science) in einem Blog-Beitrag, wie er von den Sozialwissenschaften zu STATWORX gekommen ist. Die ersten Tage bei STATWORXAn meinem ersten Tag als Praktikant begrüßten mich die Mitarbeiter sehr herzlich. Nach einem kurzen Rundgang durch das Büro konnte ich direkt mit Fabian das neue MacBook Pro einrichten. Gleichzeitig gab es eine ausführliche Einführung in das gesamte Software-Ökosystem von STATWORX. Nachdem auch die Arbeitsstandards erklärt und Kommunikationskanäle eingerichtet waren, habe ich schon meine erste Aufgabe bekommen.  Zunächst ein kurzer Ausflug in die Arbeitsweise von uns. Grundsätzlich teilen sich die Arbeitsfelder von STATWORX in drei Bereiche auf:Data Science ProjekteSchulungen in Python/RStatistik-BeratungFür die technische Implementierung der Projekte verwenden wir bei STATWORX ausschließlich OpenSource-Lösungen. Durch die florierende Community in diesem Bereich werden Pakte bzw. Bibliotheken für Python und R ständig weiterentwickelt sowie mit neuen Funktionen ausgestattet. Damit wir unsere Erfahrungen und den Status Quo stets an unsere Kunden weitergeben, ist es meine Aufgabe, eine Schulung im Bereich Deep Learning für R dahingehend zu aktualisieren und das Schulungskonzept zu erweitern. Durch die schnelle Einrichtung des MacBooks und die Einführung konnte ich direkt nach einer Stunde die ersten Deep Learning Modelle in R entwickeln und trainieren. Im Laufe des Tages gab es immer wieder kleine Sitzungen mit meinen Kollegen, die mir ihre verschiedenen Projekte und aktuellen Herausforderungen vorgestellt sowie über ihre bisherigen Erfahrungen erzählt haben. Zum Abschluss des ersten Tages haben wir noch das Data Science Meet-Up in Frankfurt besucht. Bei ein paar kühlen Drinks gab es Präsentationen zu verschiedenen AI Themen. Ein Vortrag von 904Labs aus Amsterdam hat die Relevanz von Suchfunktionen auf e-Commerce Website unterstrichen und ihre Lösung, wie sie die Suche mithilfe von AI verbessern, vorgestellt. Eine weitere Präsentation hatte eher einen geschichtswissenschaftlichen Charakter, sie beantwortete die Frage, wie sich die Menschheit im Laufe der Zeit Wissen angeeignet und untereinander ausgetauscht hat. So ging ein erster spannender Tag als Data Science Praktikant bei STATWORX zu Ende. Wissenstransfer wird bei uns groß geschrieben!Mein zweiter Tag begann mit einem Team Meeting des Data Science Teams. Hier haben alle Kollegen den Status aller Projekte und die nächsten Schritte besprochen. Die Bandbreite an verschiedenen Projekten und die jeweiligen Lösungsansätze haben mich beeindruckt und unterstreichen die Möglichkeiten während eines Praktikums bei STATWORX viel dazuzulernen. Durch das Projektgeschäft sind viele Kollegen nicht immer im Büro und so habe ich am Freitag noch einige neue Gesichter kennen lernen können. Nachmittags haben wir uns noch in einer kleineren Runde zusammengesetzt, um über verschiedene Möglichkeiten von Bootstrapping für Zeitreihen zu reden und hier mögliche Best Practices abzuleiten. Wie an jedem Freitag hat sich dann noch sowohl das Statistik Team als auch das Data Sience Team für einen Wissenstransfer zusammengesetzt. In dieser Sitzung hat Alexander Darrall aus dem Statistik-Team verschiedene parametrische sowie nicht-parametrische Testverfahren vorgestellt und diskutiert, welcher Anwendungsfall am besten für sie geeignet ist. Während seines Vortrags kam es zu interessanten Diskussionen innerhalb des gesamten Teams. Abends haben wir die Woche noch bei ein paar Burgern und kühlen Getränken ausklingen lassen. FazitNach meinen ersten Tagen bei STATWORX kann ich nur den positiven Eindruck aus meinem Bewerbungsgespräch bestätigen. Typische Praktikanten Aufgaben gibt es nicht. Man ist von Anfang an Teil des kompletten Data Science Teams und arbeitet ohne Einschränkungen an Projekten sowie Schulungen mit. Die Benefits für ein Praktikum bei STATWORX sind:ein super dynamisches Teamflache Hierarchienspannende Projektesteile Lernkurvecooles OfficeTop EquipmentReferenzen:https://trends.google.de/trends/Über den AutorMoritz GnisiaI'man intern in the STATWORX data science team for half a year in 2018 and enjoyed writing the Python for data science Introduction. Beside this I am passioned about aviation and especially like gliding in the air..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/als-data-science-praktikant-bei-statworx/;Statworx;  Moritz Gnisia
  13. Februar 2018;pandas vs. data.table – A study of data-frames;"Overview and SettingPython and R have become the most important languages in analytics and data science. Usually a data scientist can at least navigate one language with relative ease and at STATWORX we luckily have both expertises available. While, with enough will and effort, any coding project can be completed in either language, perhaps they differ in some perfomance aspects. To assess this question, we chose a task no data scientist can escape – the inevitable data preparation (80% data preparation, 20% analysis as the proverb goes). While R has a native object type for tabular data – it was made by statisticians after all – Python lacks this feature in the standard library. Relational data can be crunched with the pandas library, which has become the de-facto standard package for tabular data manipulation in Python. It is based on the even more popular framework of numPy, a linear algebra and matrix operation framework. Even though R uses the so called dataframe by default, there has been put some effort into upgrading and enhancing the built-in type. One of the most famous and widely used packages in this regard is data.table. The class data.table excels at combining operations in a performant way, since most of its codebase relies on C functions (for other ways to speed up R code with C, check out out the blog post of our colleague Andre Bleier about Rcpp – German only!).We will compare the performance of pandas with that of data.table in a structured and systematic experiment split up to four different cases:Data retrieval with a select-like operationData filtering with a conditional select operationData sort operationsData aggregation operationsFor each of these tasks we will use the same simulated dataset which contains the following data types:Integer, in three different distributions (Normal, zero inflated Poisson, Uniform)Double, in the same distributionsString, random lengths within parametersBool, Bernoulli distributedThe computations were performed on a machine with an Intel i7 2.2GHz with 4 physical cores, 16GB RAM and a SSD harddrive. Software Versions were OS X 10.13.3, Python 3.6.4 and R 3.4.2. The respective library versions used were 0.22 for pandas and 1.10.4-3 for data.table.SelectIn the first step, we checked the speed of accessing several columns in a dataset. We have timed this operation for a single column as well as a group of three columns. The setup is further divided by the position of the columns. For both, the single and multiple column scenarios, we used columns from the beginning, middle, end and for the multiple column case additionally a random location in the dataset.The left side shows the results of the single table queries, while the right grid presents the three column case. The measured value is the median execution time of pandas relative to the median execution time of data.table. The results are mixed. In the majority of the tested scenarios pandas selection takes roughly 1.5 to 2 times longer than the equivalent data.table query in R. There are also a lot of nearly equal execution speeds across all settings, especially in the scenarios with 50k/100k observations. In absolute terms execution speed for both data.table and pandas was nearly instantanous. The maximum median value across all queries lies around 2ms for both packages. See the Github project repo of this simulation for additional plots and the source code.FilterWe have tested seven different scenarios using filter operations. Scenario 1 through 4 perform a filter operation on a single column of varying data type (with the aforementioned order – int, double, string, bool). Scenarios 5 – 7 are combinations of the first four with scenario 5 consisting of 1 and 2 and scenario 6 of 1,2 and 3. Scenario 7 includes all single scenarios together in a combined filter query.For the given settings pandas yields lower medians for most of the executed scenarios. These relative timings range from nearly identical median timings down to a tenth of the data.table results. Scenario 3 is an exception to this observation. The biggest computation gap between pandas and data.table in absolute terms is the median timing scenario 2 with 100k rows and 1200 columns. The pandas library completed the query in roughly 1.4 seconds, whereas data.table took around 12 seconds. It is unclear where this spike in execution time stems from, as this run seems to be a clear outlier. Other execution times range from 80-600 ms (Again, if you are interested in the detailed numbers check the project repo).Conclusion, Critique and OutlookWe have seen roughly equal performance for select operations with data.table performing slightly faster in relative terms with very low absolute execution speeds. Pandas, however, copes better with filter operations, since the computation time is overall lower as opposed to the R counterpart. This experiment does not claim absolute authority but wants to provide a glimpse at general performance of tabular data processing in both languages. There are of course modifications to this experimental setting which would improve the general design. The most important point would possibly be to alternate between different queries per scenario instead of running the same job 100 times all over. If you have any ideas to improve this study, please share your thoughts in an email. We have learned some interesting facts – both languages, R and Python possess competitive tabular data libraries, which have coped fairly even amongst our two tests. Another observation we were able to make is the reduced query time for more complex filter operations in both packages. This indicates that there seems to be some parallel search in place across all filter conditions which reduces the search-space for additional conditions. Stay tuned for the second part of this series, where we will be examining the more computational intense group-by and arrange operations.ReferencesDowle, Matt, et al. data. table: Extension of data. frame. (2014).McKinney, Wes, Data Structures for Statistical Computing in Python, Proceedings of the 9th Python in Science Conference, 51-56. (2010).Über den AutorChristian MoreauChristian ist im Data Science Team und interessiert sich stark für Python und IT Themen. In seiner Freizeit interessiert er sich für Politik und programmiert gerne..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/pandas-vs-data-table-a-study-of-data-frames/;Statworx;  Christian Moreau
  13. Februar 2018;Rats! Where are my R-Packages?;"It happened to many of us. Somehow, we managed to get our hands on a neatly prepared script of a colleague. However, instead of getting away with just scrounging beautiful code off our fellow human beings, we realize that something is missing to seamlessly steal their awesomeness: Their packages.  A thing that we have to get our heads around when starting in Data Science teams (or pretty much in any position that requires coding) is how to make our work reproducible for others. While version control systems such as git can help us to keep our code aligned and to manage changes in our scripts and files in a clear and tidy fashion, we might still run into the problem that our colleagues' code and loaded packages will refuse to bow to us and just work as they did for them.There is a rat among us.One solution is to invite a rat among us to ""wiretap"" our use of packages. Simply starting a new R-project in packrat mode (or using packrat::init()) will open a new private and empty library. This library is saved within the folder of our new project and is private in the sense that it operates independently of your usual library. Long story short, any package installation, update or removal that happens in a packrat project, stays in the packrat project.It sounds like a bit of work to start with a fresh library? Yes, in the short term, probably. Nevertheless, packrat is able to bring more convenience to managing your packages in the long run. First, the use of snapshots makes it much easier to restore an earlier version of your (project) library. In case you want to use a development version of a package (e.g. using devtools::install()), it is very easy to restore a previous package version. Taking snapshotsOn the other hand, take the case of the two hypothetical, yet pretty awesome Jedi-packages ""the_force"" and ""jedi_mind_tricks"". You install both packages and take a packrat::snapshot(), for later replication. Despite experiencing new and unlimited power, you suddenly run into name space issues since the_force::mind_trick() and jedi_mind_trick::mind_trick() are named equivalently. After removing one of the packages, however, you realize that you do not care about the name space issue and rather want to go back to an earlier stage of your library – a case for packrat::restore():Second, packrat makes it much easier to replicate the work of others without changing your own library – and potentially risking to run into version problems with previously working projects. In practice, you are suddenly able to consider packrat when maintaining your git repository and keep your team in the loop about the most recent changes in terms of packages. Lets imagine a rather simple case – a project that depends on the package purrr of version 0.2.4, which is available on CRAN and can therefore be restored from there using packrat.This implies that we do not need to commit the contents of our local library to our git repository, but that packrat uses the packrat.lock file (a detailed list of a used packages, their versions and their source) generated on our computer to reproduce the listed version using CRAN or any other specified source. You can also commit source tarballs, e.g. when you have created packages in your local repository, however. Similarly, it is possible to use packrat::bundle() (as well as packrat::unbundle() on the target system) to bundle your project into a gzipped tarball (including the private library if specified) for easy sharing.There is more, though: Probably one of the neatest features is packrat::status(). This command helps you to analyse your code and to check whether all dependecies that are necessary to execute the code are available in the private library. In case dependencies are missing, packrat specifies the packages and versions that not present. The most awesome feature, however, is using this function in the other direction: Tidying up. The packrat::status() function can also identify packages in the project that are not necessary – packrat::clean() allows us get rid of those.Referenceshttps://media.giphy.com/media/3o6Mb9mSDkIbzSWY4E/giphy.gifÜber den AutorDavid SchleppsI am a data scientist at STATWORX and while I am working on machine learning problems during the daytime, I dream about the beauty of Shiny and the Tidyverse at night..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/rats-where-are-my-r-packages/;Statworx;  David Schlepps
  8. Dezember 2017;Bilder lernen mit Neuronalen Netzen; „Bilder lernen“ mit Neuronalen NetzwerkenConvolutional Neural Networks (CNN) sind ein beliebter Architekturtyp neuronaler Netzwerke, die hauptsächlich zur Klassifikation von Bildern und Videos eingesetzt werden. Der Aufbau von Convolutional Networks unterscheidet sich deutlich von dem des Multilayer Perceptron (MLP), das bereits in vorherigen Posts zur Einführung und Programmierung neuronaler Netze besprochen wurde. Convolutional und Pooling LayersCNNs verwenden eine spezielle Architektur, die sogenannten Convolutional und Pooling Layers. Der Zweck dieser Schicht ist die Betrachtung eines Inputs aus verschiedenen Perspektiven, bei dem die räumliche Anordnung der Features von Bedeutung ist. Jedes Neuron im Convolutional Layer überprüft einen bestimmten Bereich des Input Feldes mithilfe eines sogenannten Filters. Einen Filter untersucht das Bild auf eine bestimmte Eigenschaft, wie z.B. Farbzusammensetzung oder Kanten und ist prinzipiell nichts Anderes als eine eigens für diese Aufgabe trainierte und zuständige Schicht.Das Ergebnis eines Filters ist der gewichtete Input für einen Bereich und wird im Convolutional Layer gespeichert. Die Anzahl der Convolutional Layers ist gleich die Anzahl der verwendeten Filter, da jeweils das gesamte Input Bild von jedem Filter geprüft wird. Diese Information wird zwischen den einzelnen Convolutional Layern mit sogenannten Pooling Layern komprimiert. Die Pooling Layer laufen die durch die Filter erstellte Feature Map ab und komprimieren diese. Komprimieren bedeutet in diesem Fall die Reduktion der Pixelanzahl nach einer gegebenen Logik. Diese kann beispielsweise eine Summe oder der Maximalwert eines kleinen Bereichs der Feature Map sein. Dies ähnelt den Convolutional Layern, mit dem Unterschied, dass die Pooling Schichten nicht trainiert werden und ihre Logik somit bestehen bleibt. Danach können noch weitere Convolutional Layer und/oder Pooling Schichten folgen. Anschließend werden die abstrahierten Features in ein voll vernetztes MLP übergeben (siehe Abb). Eine höhere Anzahl von Schichten führt meist zu einem höheren Grad der Abstraktion des Ursprungsbildes. In diesem MLP wird z.B. bei der Bildklassifikation eine abschließende Multiclass Klassifikation durchgeführt und die wahrscheinlichste Klasse zurückgegeben. Multiclass bezeichnet dabei ein Problem bei dem die Zielvariable zu einer von mehreren Klassen gehören kann. Das Gegenstück hierzu ist die binäre Klassifikation, bei der die Zielvariable nur zwei Zustände annehmen kann. Benchmarking und EvaluationBildklassifikation ist einer der Benchmark Cases für die Evaluation aktueller Deep Learning Forschung. Bekannte Datensätze die hierfür verwendet werden sind bspw. MNIST oder CIFAR10 bzw. CIFAR100.  Ersterer enthält die Bilder handgeschriebener Ziffern auf Postumschlägen des US Postal Service und ist somit ein Multi-Class Klassifikationsproblem mit 10 Ausprägungen (Ziffern 0-9). Die CIFAR Datensätze beinhalten Bilder von Tieren und Fahrzeugen –in der Version mit 10 Ausprägungen– oder auch von Pflanzen, Menschen oder sogar Elektro-Geräten in dem Datensatz mit 100 Klassen. 10 und 100 stehen dabei für die Anzahl an unterschiedlichen Klassen, die durch Bilder im Datensatz repräsentiert sind. Eine höhere Anzahl an Klassen, stellt in der Regel ein schwierigeres Problem für den Algorithmus dar. Aktuelle state-of-the-art Modelle zur Bildklassifikation erreichen eine Genauigkeit von bis zu 99.8% für MNIST, 96.5% für CIFAR-10 bzw. 75.7% für den CIFAR-100 Datensatz2.Natürlich bieten die Deep Learning Frameworks Tensorflow und Keras eine Reihe von vorkonfigurierten Bausteinen zur Erstellung von Convolutional Neural Networks. Im kommenden Blogbeitrag veranschaulichen wir die Erstellung eines CNN, mit dem wir versuchen werden, die Kategorien des CIFAR-10 Datensatzes zu erlernen.ReferenzenBildnachweis CNN: Beitrag auf StatsexchangeSammlung von CNN-Benchmark ErgebnissenÜber den AutorChristian MoreauChristian ist im Data Science Team und interessiert sich stark für Python und IT Themen. In seiner Freizeit interessiert er sich für Politik und programmiert gerne.;https://www.statworx.com/de/blog/bilder-lernen-mit-neuronalen-netzen/;Statworx;  Christian Moreau
  8. Dezember 2017;Fehlerbehandlung in R – Handhabung von Ausnahmen mit trycatch;"Der vorherige Teil der Reihe drehte sich um die Handhabung von unerwarteten Fehlern und Bugs. Doch manchmal erwartet man das Auftreten von Fehlern, beispielsweiße falls man das gleiche Modell für mehrere Datensätze anwenden möchte. Dabei kann unter anderem der Fehler auftreten, dass das Modell aufgrund von fehlender Varianz nicht geschätzt werden kann. In diesem Fall möchte man nicht, dass durch diesen einen Fehler die komplette Schätzung abbricht, sondern, dass mit der nächsten Schätzung fortgefahren wird.In R gibt es drei unterschiedliche Methoden um dies zu erreichen:try() ignoriert den Fehler und führt die Berechnung fort.trycatch()?lässt eine zusätzlich Fehlermeldung/Aktion zuweisen.withCallingHandlers()?ist eine besondere Variante von trycatch()?, welches die Ausnahme lokal handhabt. Es wird nur selten benötigt, daher fokussieren wir uns auf die beiden erstgenannten.tryMit try() wird der Code weiterhin ausgeführt unbeachtet von auftretenden Fehlern.Im folgenden Beispiel tritt ein Fehler auf, wodurch der Prozess abgebrochen wird und es wird kein Wert zurückgegeben.Fügen wir die fehlererzeugende Funktion in ein try() ein, wird die Fehlermeldung weiterhin angezeigt, jedoch wird der restliche Code ausgeführt und wir bekommen weiterhin einen Return. Aktivieren wir die Option silent = TRUE innerhalb der try()-Funktion, wird nicht einmal mehr die Fehlermeldung angezeigt.  Code-Blöcke werden innerhalb von try() in der geschweiften Klammer {}  zusammengefasst.  Zusätzlich ist es möglich die Klasse von try() abzufragen. Taucht kein Fehler auf, ist es die Klasse des letzten Returns, taucht jedoch einer auf, ist es eine eigene „try-error”-Klasse.  Dadurch kann im Nachhinein überprüft werden, ob die Funktion erfolgreich ausgeführt wurde, dies ist besonders hilfreich, wenn man eine Funktion auf mehrere Objekte anwendet. trycatch Im Unterschied zu try() können mit trycatch() nicht nur Fehler gehandhabt werden, sondern auch Warnungen, Messages und Abbrüche. Eine weitere Besonderheit ist, dass je nach auftretender Ausnahme unterschiedliche Funktion aufgerufen werden können. In der Regel werden hierbei Default-Werte übergeben oder bedeutsamere Meldungen erzeugt. Im folgenden Beispiel wollen wir die Funktion über einen Vektor mit den Ausprägungen Vektor = data.frame(4, 2, -3, 10, ""hallo"") loopen. Wobei die log-Funktion bei negativen Werten eine Warnung ausgibt und für Factors und Strings einen Fehler. Das letzte, wichtige Argument von trycatch() ist finally. Die dort angegebene Funktion wird als letztes ausgeführt, ungeachtet ob der vorherige Code erfolgreich durchlief oder abgebrochen ist. Dies ist nützlich um nicht mehr benötigte Objekte zu löschen oder Verbindungen zu schließen. Der nächste Artikel der Reihe wird sich um Unit-Testing drehen. ReferenzenAdvanced R by Hadley WickhamÜber den AutorMarkus BerrothI am a data scientist at STATWORXand I love creating novel knowledge from data.In my time off,I am always open for a weekend trip.";https://www.statworx.com/de/blog/fehlerbehandlung-in-r-handhabung-von-ausnahmen-mit-trycatch/;Statworx;  Markus Berroth
  7. Dezember 2017;Einflussfaktoren der Elastizitätskalkulation;Preiselastizität ist  die praktikabelste und aussagekräftigste Metrik, um die im Preismanagement entscheidende Frage zu beantworten: Wie reagieren Kunden auf eine Preiserhöhung/Preissenkung um x Prozent? Auf der Basis dieser Kennzahl ist es möglich ein differenziertes Preismanagement aufzubauen. Immer im Blick: der Kunde.Leider ist der Weg hinzu belastbaren Elastizitätswerten steinig. Es gibt in der Praxis viele Fallstricke und Besonderheiten, die, wenn nicht angemessen berücksichtigt, große Ungenauigkeiten bei der Bestimmung von Preiselastizität verursachen. Die Folge können folgenschwere Fehleinschätzungen sein. Diese Fallstricke aufzuzeigen ist das Ziel dieser Blog-Beitragsserie. Der Schwerpunkt dieses Beitrags liegt auf der Frage: Welche Faktoren beeinflussen auf welche Weise die Preiselastizität?Viele intervenierende EinflussfaktorenAuf die Beziehung zwischen Preis und Absatz wirken alle Aspekte des Verkaufsprozesses und des Marktes hinein. Die Zahl möglicher Einflussfaktoren und Wirkungszusammenhänge ist so groß und vielfältig, dass die Suche nach Vollständigkeit nicht zielführend sein kann. Es gibt keine Checkliste, die, wenn gewissenhaft abgearbeitet, garantiert, dass nun alle wichtigen Faktoren berücksichtigt wurden und dadurch robuste Elastizitätswerte abgeleitet werden können.Um die Vielfältigkeit der Einflussfaktoren aufzuzeigen, habe ich hier eine Auswahl von häufig genannten Einflüssen bereitgestellt: Informationen zu der Zahl der Wettbewerber, ihrer Relevanz, ihrer Preise und ihre Werbeaktionen  Informationen zu den eigenen Werbemaßnahmen: Welche Werbemaßnahmen gibt es? Was war Ihre Laufzeit? Wie wirken sich die Maßnahmen auf die Preisgestaltung aus (Couponaktionen etc.)? Sind die Maßnahmen individualisiert oder gemeingültig?  Informationen zum Standort: Online oder Offline? Alleinstehen oder in einem Einkaufszentrum? Stadt oder Land? Informationen zu Konkurrenz- und Komplementärprodukten  Markeninformationen  Feiertage, Saisonalität und ggf. Informationen zu den Wochentagen, zu dem Zeitpunkt des Verkaufs oder gar zum Wetter bzw. zu Wettervorhersagen die zum Verkaufszeitpunkt galten Es ist weder praktikabel noch möglich alle diese Faktoren in eine Modellierung zu integrieren. Hinzukommt, dass viele diese Faktoren in komplexen Wirkungszusammenhänge mit Preis und Absatz stehen. Zwei Beispiele sollen veranschaulichen, welche Komplexität schon mit der Modellierung einer einzelner Faktoren einhergehen kann:MarkenMarken sind eines der zentralsten Marketinginstrumente. Erfolgreich umgesetzt, steigern Marken die Kundenloyalität und dienen Kunden als Qualitätsstandards. Diese Aspekte wirken in die Preissetzung hinein. Für etablierte Marken/Markenprodukte ist zu erwarten, dass die Kundereaktionen auf Preisveränderungen weniger stark ausfallen als beispielsweise bei Eigenmarken von Handelsketten. Unsere Praxiserfahrung bestätigt diese Annahme nur bedingt. Zwar haben Markenprodukte in der Regel höhere Preise als andere Produkte, doch insbesondere für Retailer leitet sich dadurch noch keine erhöhte Preisgestaltungsfreiheit ab. Markenprodukte werden von Kunden aktiver und bewusster recherchiert. In Märkten mit hoher Markttransparenz sind daher für Markenprodukte erhebliche Preisveränderungsreaktionen zu beobachten.ProduktlebenszyklusDie wissenschaftliche Literatur kommt bei der Frage wie sich der Produktlebenszyklus auf Preiselastizität auswirkt zu ambivalente Aussagen. Die Annahmen bezüglich der Wirkungsweise von Produktlebenszyklen sind vielfältige. Ein neues Produkt mag relevante Vorteile mit sich bringen, so dass Kunden erst dann ein Preisbewusstsein entwickeln, wenn es entsprechende Konkurrenzprodukte gibt. Zur Zeit der Produkteinführung ist also eine geringe Preiselastizität vorzufinden, die mit der Zeit steigt. Andererseits lassen sich ebenso Produktkategorien finden die eine starke Produktdifferenzierung erfahren haben. Die Folge sind eine gesteigerte Kundenbindung und damit fast zwingend eine geringere Preiselastizität.Insbesondere für Handelsunternehmen mit einem diversen Produktportfolio kann der Aspekt Produktlebenszyklus ein schwer zu überblickender Aspekt bei der Elastizitäten-Bestimmung sein. Dies bedeutet aber nicht, dass der Produktlebenszyklus unwichtig ist. Es bedeutet aus unserer Sicht nur, dass Produktlebenszyklen auf Grund ihrer vielfältigen potentiellen Wirkungsdynamiken ein analytisch anspruchsvoll zu erfassendes Konstrukt sind.Welche Faktoren sollte man berücksichtigen? Eine vollständige Liste aller möglichen Faktoren, die relevant sein können kenne ich nicht. Es kann sie auf Grund der Komplexität der Aufgabe und der vielfältigen Märkte auch nicht geben. Es gibt aber dennoch ein paar wichtige Orientierungspunkte, die hilfreich sein können. Abschließend daher ein paar Richtlinien: Zielsetzung der Analyse: Wichtig ist es, im Voraus Klarheit über die Zielsetzungen zu schaffen mit der Elastizität bestimmt werden sollen. Geht es darum Elastizitäten für einzelne Produkt zu bestimmen oder für eine Marke? Wird eine Basis-Elastizität für ein Produkt gesucht oder geht es um Elastizitäten in speziellen Verkaufssituation wie etwa im Rahmen von Werbemaßnahme? Je nach Wunsch leiten sich unterschiedliche Minimalbedarfe bezüglich der zu berücksichtigenden Faktoren ab. Datenverfügbarkeit: Ein offensichtliches, manchmal etwas ernüchterndes, aber sehr wichtiges Auswahlkriterium ist die Datenverfügbarkeit. Es ist ein hilfreicher Schritt sich am Anfang einen zumindest oberflächlichen Überblick über die verfügbaren Daten zu verschaffen. Häufig grenzt sich allein dadurch schon die Zahl der verwertbaren Variablen ein. Gleichzeitig kann dieser Schritt eine Quelle für viele neue und relevante Features sein. Dieser Punkt muss keinesfalls ernüchternd sein, sondern kann Anstoßpunkt für viele kreative Ideen werden.  Geschäftsfeld: In welchem Geschäftsfeld bewegt sich der Kunde und welche Charaktermerkmale machen seine Produkte aus? Handelt es sich um Business-to-Business oder Business-To-Consumer Transaktionen? Handelt es sich um kurzlebige Verbrauchsgüter oder langlebige Gebrauchsgüter? Handelt es sich bei dem Verbrauschgut um ein leicht lagerbares Gut oder eines das schnell verfällt? Der Austausch mit dem jeweiligen Projektpartner ist hier immer einer der ersten Arbeitsschritte. Hier liegt die Expertise. Über den AutorDaniel LüttgauI am a data science consultant at STATWORX.Exploring business concepts and thinking up ways to utilize data for our customers is what I enjoy most about my job. My freetime is commited to my dog, travelling and my wife and friends.;https://www.statworx.com/de/blog/einflussfaktoren-der-elastizitaetskalkulation/;Statworx;  Daniel Lüttgau
  23. Oktober 2017;Rcpp Doping für den R Code;Wie viel Zeit ich schon damit verbracht habe rote Stoppschilder zu beobachten? Sehr, sehr viel!Effizientes Programmieren kann helfen diese Zeit zu minimieren, aber oft hat man gar keinen Einfluss auf die Laufzeit seines Programms, da Spezifikationen im Unterbau von  das eigentliche Bottleneck sind.R ist eine sehr flexible Programmiersprache. Als Benutzer muss man weder Speicherplatz auf dem Arbeitsspeicher allokieren noch Datentypen bzw. Objekttypen von Variablen deklarieren. Diese Eigenschaft nimmt dem Nutzer sehr viel Arbeit ab, allerdings verliert man durch hinzugewonnene Flexibilität an Leistung. Mithilfe des R-Pakets  kann man das Zepter selbst in die Hand nehmen und weniger dynamischen, aber dafür leistungsfähigen Code schreiben. Allerdings gibt es dabei für eingefleischte R Hacker einen Haken: Man muss  Code schreiben!Rcpp ist ein R Front-End für C++ Programme bzw. Code. Warum C++? R besteht im Kern aus  und  Code, allerdings ist dieser Kern optimiert maximale Flexibilität zu gewährleisten. C++ bietet sich als Vehikel für leistungsfähigen Code an, da beide Programmiersprachen objektorientiert sind.Als geschulter C++ Hacker kann man durch dieses Front-End ganz leicht seine Programme in R Code einbetten. Wenn man mit C++ keine bis wenig Erfahrung hat, kann man durch die für R gewohnte Nutzer umgesetzte Rcpp Bibliothek durch geringen Aufwand selbst C++ Programme schreiben. Als Starthilfe hierzu ist das Buch zum Paket Rcpp von Dirk Eddelbuettel sehr zu empfehlen, da man in kürzester Zeit erste Programme bzw. Funktionen schreiben kann.Um die Performance von R Core Funktionen und einer C++ Version zu vergleichen, nutzen wir die Matrixmultiplikation. In R multipliziert man Matrizen mithilfe des  Operators. Die selbstgeschriebene Rcpp Version der Matrixmultiplikation benutzt das für lineare Algebra optimierte C++ Paket . Mithilfe von Rcpp kann man auch C++ eigene Bibliotheken verwenden, hierzu muss man nötige Plug-Ins laden. Wie viel schneller können diese selbstgeschriebenen Funktionen nun sein? Wir testen die Performanz im Folgenden in einer kleinen Simulation.SimulationSetup der Evaluation besteht aus der Multiplikation zweier quadratischer Matrizen, die jeweils mit gleichverteilten Zufallszahlen aus dem Intervall  befüllt werden. Iterativ wird die Dimension  der Matrix um eine Einheit erhöht um die Steigung des Rechenaufwands zu untersuchen. Zusätzlich wird die Multiplikation mit den verschiedenen Versionen pro Iteration 100-mal durchgeführt um zufällige Schwankungen zu glätten.Man sieht deutlich, dass der Rechenaufwand im Falle der R Core Multiplikation mit zunehmender Komplexität quadratisch ansteigt. Im Gegensatz dazu steigt der Aufwand bei Rcpp nahezu linear. Die selbsterstellte Rcpp Funktion ist etwa um den Faktor sechs schneller als das vorimplementierte R Pendant.Natürlich ist es nicht immer sinnvoll eine Rcpp Version zu schreiben, da sich durch die Kompilierungszeit (in C++) auch schnell ein Bottleneck in das Programm einschleicht. Hierbei ist es wichtig zu erkennen bei welchen Funktionen die Verwendung von Rcpp einen Mehrwert im Kontext der Rechenzeit oder Speicherbelegung generiert. Dafür gibt es leider keine Faustregel, aber jegliche Art von Schleifen sind in C++ meist effizienter. Durch die Ausnutzung der Eigen Bibliothek kann man leistungs- und speichereffiziente Operationen der linearen Algebra in R verwenden.Vergesst nicht die Rcpp Funktionen gegen ihr R Pendant in Bezug auf Laufzeit und Speicherbelegung zu benchmarken!ReferenzenEddelbuettel, Dirk. Seamless R and C++ integration with Rcpp. New York: Springer, 2013.Über den AutorAndré BleierThe most exciting part of being a data scientist at STATWORX is to find this unique solution to a problem by fusing machine learning, statistics, and business knowledge.;https://www.statworx.com/de/blog/rcpp-doping-fuer-den-r-code/;Statworx;  André Bleier
  23. Oktober 2017;Stolperfalle logistische Regressionskoeffizienten und Odds Ratios;"Logistische Regressionsmodelle, sind mit gängiger Statistiksoftware meist genauso leicht zu schätzen wie lineare Regressionen. Doch die Interpretation solcher Modelle, also der Part der statistischen Analyse der nicht von der Software übernommen wird, birgt eine Tücke: die Bezugsgröße der Regressionskoeffizienten. Ausgehend von den unabhängigen Merkmalen der Beobachtungen, modellieren logistische Regressionsmodelle die Wahrscheinlichkeit mit der eine bestimmte Ausprägung eines kategorialen abhängigen Merkmals auftritt. Zur Schätzung dieser Wahrscheinlichkeiten ist die Transformation der Regressionsgewichte der unabhängigen Variablen notwendig, so dass logistische Regressionskoeffizienten den Zusammenhang zwischen den Ausprägungen der unabhängigen Variablen und den Logits für die betrachtete Merkmalsausprägung der abhängigen Variablen spiegeln. Parallel zur linearen Regression kann geschlossen werden, dass eine Erhöhung einer gegebenen unabhängigen Variable um eine Einheit, mit der Veränderung des Logits für das Auftreten der betrachteten Merkmalsausprägung der abhängigen Variable um ? Einheiten einhergeht. Zwar ist diese Interpretation formal korrekt, offenkundig jedoch wenig aufschlussreich. Logits, Odds Ratios und WahrscheinlichkeitenEs drängt sich die Frage auf, was genau Logits sind. Die Antwort ist augenscheinlich recht einfach: Logits sind logarithmierte Odds Ratios. Wir halten fest: Logit = ln(Odds Ratio). Aber natürlich stellt sich nun die Frage, was wiederrum Odds Ratios sind. Im Deutschen werden Odds Ratios als Chancenverhätnisse (oder auch Quotenverhältnisse) bezeichnet. Tatsächlich sind Odds Ratios nicht mehr als simple Verhältnisse von Chancen (beziehungsweise Quoten oder eben Odds). Im gegebenen Kontext bezeichnen Odds Ratios das Verhältnis der Chancen für das Auftreten der betrachteten Merkmalsausprägung der abhängigen Variable, zwischen zwei Gruppen welche sich in der Ausprägungen eines unabhängigen Merkmals unterscheiden. Wir halten fest: Odds Ratio = Chance für Merkmalsausprägung in Gruppe 1 : Chance für Merkmalsausprägung in Gruppe 2. Natürlich wird damit die Frage aufgeworfen, was genau Chancen sind. Chancen sind das jeweilige Verhältnis der Wahrscheinlichkeit für das Auftreten einer Merkmalsausprägung relativ zu der Wahrscheinlichkeit für das Nicht-Auftreten der Merkmalsausprägung innerhalb einer, zum Beispiel durch ein unabhängiges Merkmal definierten, Gruppe. Wir halten fest: Chance für Merkmalsausprägung = Wahrscheinlichkeit von Merkmalsausprägung : Gegenwahrscheinlichkeit von Merkmalsausprägung. Die Wahrscheinlichkeit für eine Merkmalsausprägung entspricht dabei dem Anteil von Beobachtungseinheiten einer Gruppe, welche die jeweilige Ausprägung aufweisen. Wir halten fest: Wahrscheinlichkeit von Merkmalsausprägung = Anteil der Gruppenmitglieder mit Merkmalsausprägung.  Ein Beispiel: Nerds, Normalos und Star WarsZur Veranschaulichung werden nachstehend Logit und Odds Ratio dafür ein Star-Wars-Fan zu sein, für eine Gruppe von 10 „Statistik-Nerds“ relativ zu einer Gruppe von 10 „Normalos“ berechnet. Berechnung von Hand 7 der 10 Nerds sind Star Wars Fans 4 der 10 Normalos sind Star Wars Fans. Daraus folgt:  Berechnung via logistischer Regression in RZu dem gleichen Ergebnis kommt man, wenn man in R eine logistische Regression für die gegebenen Daten schätzt und den standartmäßig ausgegebenen Logit-Koeffizienten exponenziert.   Die Gruppenzugehörigkeit wird über eine Dummy-Variablen mit der Ausprägung 1 für alle Nerds und der Ausprägung 0 für alle Normalos erfasst, daher entspricht hier die Erhöhung der UV um eine Einheit hier dem Wechsel der Gruppenzugehörigkeit.  (Logarithmierte) Verhältnisse von Verhältnissen Die Berechnung von Odds Ratios ist zwar einfach, jedoch sind Odds Ratios zur Interpretation logistischer Modelle nur auf den ersten Blick geeigneter als die logistischen Regressionskoeffizienten. Es handelt sich bei Odds Ratios um Verhältnisse von Wahrscheinlichkeitsverhältnissen. Genau wie in ihrer logarithmierten Form als Logits, entziehen Odds Ratios sich daher wohl dem intuitiven Verständnis der allermeisten Menschen. Formal korrekt kann ausgesagt werden, dass eine Erhöhung einer gegebenen unabhängigen Variable um eine Einheit, mit einer Veränderung der Odds für das Auftreten der betrachteten Merkmalsausprägung der abhängigen Variable um den Faktor e? einhergeht. Jedoch lässt sich von Odds Ratios, genauso wenig wie von logistischen Regressionskoeffizienten, nicht direkt auf die Wahrscheinlichkeiten in Gruppen oder die Wahrscheinlichkeitsverhältnisse zwischen kontrastierten Gruppen schließen. Daher sind bei der Interpretation logistischer Regressionsmodelle Aussagen wie „…die Erhöhung einer der unabhängigen Variable um eine Einheit ist verbunden mit einer um e? / ? veränderten Wahrscheinlichkeit…“, nicht zulässig. Wie fehlgeleitet solche Behauptungen sind, wird deutlich, wenn man sich vor Augen führt, dass ganz unterschiedliche Ausgangswahrscheinlichkeiten in gleichen Odds Ratios beziehungsweise Logits resultieren können. So kann beispielsweise das Odds Ratio aus dem vorangegangenen Beispiel auch durch ganz andere Wahrscheinlichkeiten in zwei kontrastierten Gruppen entstehen:  .tg .tg-baqh{text-align:center;vertical-align:top} P1P2Verhältnis P1 / P2Odds 1Odds 2Odds RatioLöst man die Formel zur Berechnung des Odds Ratio nach der Eintrittswahrscheinlichkeit einer der Gruppen auf, erhält man die Funktionsgleichung der Kurve auf der alle Wahrscheinlichkeitskombinationen mit dem selben Odds Ratio liegen. Nachstehend ist diese Kurve für ein Odds Ratio von 3,5 abgebildet.  FazitDa selbst formal korrekte Interpretationen der absoluten Werten von Logits (?), genauso wie von Odds Ratios (e?) uninformativ und potentiell irreführend sind, wird an dieser Stelle empfohlen lediglich die durch Logits und Odds Ratios implizierte Richtung von Zusammenhängen zu interpretieren. Eine Erhöhung einer unabhängigen Variable (um eine Einheit), geht bei Odds Ratios &gt; 1 mit einer erhöhten, bei Odds Ratios &lt; 1 mit einer verringerten Wahrscheinlichkeit für das Auftreten der betrachteten Ausprägung der abhängigen Variable einher. Für ?-Koeffizienten, also logarithmierte Odds Ratios, gilt aufgrund der ln-Transformation, dass die Erhöhung einer unabhängigen Variable (um eine Einheit), bei ? &gt; 0 mit einer erhöhten, bei ? &lt; 0 mit einer verringerten Wahrscheinlichkeit für das Auftreten der betrachteten Ausprägung der abhängigen Variable einher geht. Referenzen Best, H., &amp; Wolf, C. (2012). Modellvergleich und Ergebnisinterpretation in Logit-und Probit-Regressionen. KZfSS Kölner Zeitschrift für Soziologie und Sozialpsychologie, 64(2), 377-395.Über den AutorLea WaniekI am a data scientist at STATWORX, apart from machine learning, I love to play around with RMarkdown and ggplot2, making data science beautiful inside and out.";https://www.statworx.com/de/blog/stolperfalle-logistische-regressionskoeffizienten-und-odds-ratios/;Statworx;  Lea Waniek
  18. Oktober 2017;Fehlerbehandlung in R: Debugging mit RStudio;"In der Blog-Reihe „Fehlerbehandlung in R“ geht es um effizientes und systematisches Überprüfen von R-Code. Den Beginn macht das Finden von Fehlern durch Debugging, weiter geht es mit der Handhabung von Fehlern und endet mit Unit-Testing, das zum Überprüfen von korrekter Funktionalität von R-Code dient.Die Reihe startet mit Debugging in R, wobei Debugging ein breitgefächertes Thema ist. Dieser Artikel fokussiert sich daher auf die Möglichkeiten, die RStudio bietet.  Debugging hilft dabei herauszufinden an welcher Stelle im Code sich ein Fehler befindet oder an welcher Stelle der Code sich anders verhält als erwartet. Dies beinhaltet im Generellen drei Schritte:  den Code laufen lassen  Code an der Stelle stoppen an welcher vermutet wird, dass sich dort der Fehler befindet  Schritt für Schritt durch den Code gehen und diesen dabei überprüfen.  In den Debugging-Modus eintreten Um in den Debugging-Modus zu kommen, muss RStudio mitgeteilt werden, wann es die Berechnungen stoppen soll. Es gibt keinen „Pause-Button“ mit welchem man eine laufende Berechnung stoppen kann um in den Debugging-Modus einzutreten. Dies ist in so gut wie allen anderen Programmiersprachen der Fall, da die Berechnungen in der Regel zu schnell von statten gehen als dass es möglich wäre an der richtigen Stelle zu stoppen. Stattdessen bestimmt man zuvor an welcher Stelle der Code angehalten werden soll. Dies ist nicht zu verwechseln mit dem „Stop“-Button über der der Konsole, welcher die Berechnung komplett abbricht.   Vor einer Zeile stoppenDer einfachste und meist genutzte Weg um in den Debugging-Modus zu gelangen, ist es einen Breakpoint im Code-Editor zu setzen. Dies kann auf einfache Weise in RStudio gemacht werden indem man links neben die Zeilennummer klickt oder durch das Drücken von Shift+F9 auf der Tastatur und zeitgleiches Klicken mit der Maus in der gewünschten Zeile.  Hierbei wird eine Tracing-Funktion innerhalb der eigentlichen Funktion eingefügt. Der Breakpoint wird durch einen ausgefüllten roten Kreis im Editor gekennzeichnet. Außerdem kann man einen schnellen Überblick erlangen in welcher Funktion sich ein Breakpoint befindet, indem man in das Environment-Fenster schaut, welche Funktion ebenfalls durch einen roten Kreis gekennzeichnet wird.  Falls die Funktion noch nicht existiert, zu einem weil man das File noch nicht gesourced hat oder zu anderem weil sich die Funktion im Environment und im Editor unterscheiden, kann der Breakpoint noch nicht aktiviert werden. Dies wird durch einen nicht-ausgefüllten Kreis kenntlich gemacht.  In der Regel hilft es das File einmal zu sourcen, wodurch die Tracking-Funktion eingefügt wird und der Breakpoint startbereit ist.  Setzt man den Breakpoint per RStudio-Editor ist es nicht notwendig die Funktion zu bearbeiten und zusätzlichen Code per Hand einzufügen. Allerdings gibt es bestimmte Situationen in denen diese Breakpoints nicht funktionieren wie beispielsweise komplexere Funktionssyntaxen. Außerdem wird konditionelles Debugging bisher nicht von RStudio unterstützt. Hier schafft die browser()-Funktion Abhilfe. Da es sich hierbei um eine tatsächliche Funktion handelt, muss sie in den Code geschrieben werden, kann aber an so gut wie jeder Stelle hinzugefügt werden. Sind sie erstmal aktiv und aufgerufen verhalten sich die Editor-Breakpoins und browser() sehr ähnlich.  Stoppen bevor eine Funktion ausgeführt wirdDer Editor-Breakpoint oder browser() eigenen sich optimal für Funktionen für welche der Source-Code vorliegt. Hat man jedoch nicht das .R File zur Hand kann alternativ eine ganze Funktion mit debug() bzw. debugonce() gedebugged werden. Hierfür wird die jeweilige Funktion innerhalb von debug() bzw. debugonce() geschrieben , beispielsweise debugonce(mean) . Dies ändert nicht die Funktion an sich, aber es startet den Debugger direkt nach dem Funktionsaufruf, man kann es sich vorstellen als würde man einen Breakpoint direkt zu Beginn der Funktion setzen würde.  debugonce() aktiviert den Debug-Modus nur ein einziges Mal für die jeweilige Funktion zum nächstmöglichen Zeitpunkt zu welchem diese aufgerufen wird. debug() hingegen aktiviert jedes Mal den Debugger, wenn die Funktion aufgerufen wird, was schlimmsten Falls in einer endlosen Schleife resultieren kann. Daher ist es in der Regel zu empfehlen debugonce() zu benutzen. Das Gegenstück zu debug() ist undebug(), welches benutzt wird wenn man nicht mehr jedes Mal die Funktion bei Aufruf debuggen möchte. Bei Fehler stoppenDie dritte Möglichkeit in den Debugging-Modus zu gelangen ist die Einstellung, dass der Debugger jedes Mal automatisch aktiviert wird, wenn ein Fehler auftaucht. Dadurch stoppt die Funktion automatisch und der Debugging-Modus startet direkt von selbst. Diese Funktionalität wird über die RStudio Oberfläche aktiviert in dem man Debug -&gt; On Error von „Error Inspector“ zu „Break in Code“ ändert.  Allerdings wird der Debugger per Default nur aktiviert, wenn ein Fehler im eigenen Code auftaucht. Falls man einen Fehler finden möchte, welcher ebenfalls Code von Dritten beinhaltet, kann diese Einstellung unter Tools -&gt; Global Options und dem abwählen von „Use debug error handler only when my code contains errors“ abgeändert werden. Alternativ kann die Option dauerhaft mit options(error = browser()) überschrieben werden. Es kann jedoch schnell störend werden, dass jedes Mal der Debugger aktiviert wird. Daher sollte nicht vergessen werden diese Option wieder mit options(error = NULL) rückgängig zu machen sobald das Debugging beendet ist. Der nächste Teil der Reihe „Fehlerbehandlung in R“ dreht sich um effektives Debugging in R nachdem der Debugger aktiviert wurde.Über den AutorMarkus BerrothI am a data scientist at STATWORXand I love creating novel knowledge from data.In my time off,I am always open for a weekend trip.";https://www.statworx.com/de/blog/fehlerbehandlung-in-r-debugging-mit-rstudio/;Statworx;  Markus Berroth
  16. Oktober 2017;Methoden Einführung: der t-Test;Einer der am häufigsten verwendeten statistischen Tests ist der t-Test. Er kann unter anderem dazu genutzt werden, zu prüfen, ob der Mittelwert einer Zufallsvariable einem bestimmten Wert entspricht. Auch kann er für den Vergleich zweier Mittelwerte herangezogen werden. Wie bei jedem anderen statistischen Test auch, müssen gewisse Voraussetzungen erfüllt sein, damit der t-Test sicher eingesetzt werden kann: Normalverteilung der ZufallsvariablenUnabhängigkeit der BeobachtungenVarianzhomogenität (im Zwei-Gruppenfall)Wenn diese Voraussetzungen nicht erfüllt sind, ist dennoch nicht aller Tage Abend! Die Robustheit des t-Testes lässt es zu, dass bei größeren Stichprobenumfängen die Normalverteilung weniger wichtig wird(1) . Bei kleinen Stichproben gibt es nicht-parametrische Alternativen, wie den Wilcoxon-Rangsummen-Test. Eine Beschreibung zu diesem Test gibt es hier. Formeln für unterschiedliche FälleWo genau spielt die Unabhängigkeit eine Rolle? Hierfür ist es wichtig zwischen verschiedenen Fällen zu unterscheiden. Hat man nur eine Variable, so bezieht sich die Unabhängigkeit auf die einzelnen Beobachtungen untereinander. Wenn zwei Zufallsvariablen verglichen werden, dann können folgende Fälle vorliegen, die zu verschiedenen Berechnungen der Teststatistik führen:  StichprobeGepaart / verbunden Ungepaart / unabhängig Varianzenbekanntunbekanntgleich  ;https://www.statworx.com/de/blog/methoden-einfuehrung-der-t-test/;Statworx;  Jakob Gepp
  16. Oktober 2017;Deep Learning – Teil 1: Einführung;"Deep Learning ist aktuell einer der spannendsten Forschungsbereiche im?Machine?Learning. Für eine Vielzahl von?Fragestellungen?liefern Deep Learning Modelle State-of-the-Art Ergebnisse, vor allem im Bereich der Bild-, Sequenz- und Spracherkennung. Weiterhin findet Deep Learning erfolgreich Anwendung in der Fahrzeugkonstruktion (selbstfahrende Autos), in der Finanzwelt (Aktienkursvorhersage, Risikoprognose, automatische Handelssysteme), in der Medizin (maschinelle Bilderkennung von Karzinomen) und Biologie (Genomik), im e-Commerce (Recommendation Systeme) und im Web Umfeld (Anomalieerkennung).? Wie unterscheidet sich Deep Learning von klassischen?Machine?Learning Algorithmen und warum funktioniert es bei vielen Fragestellungen so gut? Dieser Artikel soll Ihnen eine Einführung in die Grundkonzepte von Deep Learning geben und Unterschiede zu?klassischen?Verfahren aus dem?Machine?Learning herausarbeiten. Einführung in neuronale NetzeDeep Learning erscheint dem Anwender auf den ersten Blick als eine relative neue Methodik. Dies ist hauptsächlich darin begründet, dass die generelle Aufmerksamkeit rund um das Thema durch die vielen methodischen Durchbrüche in den letzten Jahren nicht abzureißen scheint. Fast täglich erscheinen neue wissenschaftliche Publikationen zum Thema Deep?Learning?bzw. zu angrenzenden Forschungsbereichen. Fakt ist jedoch, dass die theoretischen und methodischen Grundlagen für Deep Learning durch die wissenschaftliche Entwicklung von neuronalen Netzen bereits vor vielen Jahrzehnten in der 1950er Jahren gelegt wurden. Aufgrund verschiedener technischer und methodischer Limitationen waren damals wirkliche „Netze“, die aus hunderten oder tausenden von Elementen bestehen noch in weiter Ferne. Zunächst beschränkte sich die Forschung auf einzelne Einheiten neuronaler Netze, das Perzeptron bzw. Neuron. Als Vorreiter auf diesem Gebiet ist Frank Rosenblatt zu nennen, der während seiner Zeit am Cornell Aeronautical Laboratory 1957 bis 1959 ein einzelnes Perzeptron bestehend aus 400 photosensitiven Einheiten (Inputs), die über eine „Zuordnungsschicht“ (Association Layer) mit einer Ausgabeschicht von 8 Einheiten verbunden waren. Heute gilt das Rosenblatt Perzeptron als die Geburtsstunde von neuronalen Netzen und Deep Learning Modellen.Frank Rosenblatt (links) bei der Arbeit am Mark I Perceptron.Selbst Jahrzehnte später in den 1980er Jahren, als neuronale Netze ein erstes gesteigertes Interesse erfuhren, blieben tatsächliche praktische Erfolge eher überschaubar. Andere Algorithmen, bspw. Kernel Methoden (Support Vector Machines) oder Decision Trees verdrängten?neuronale Netze in den 80er und 90er Jahren auf die hinteren Plätze der?Machine?Learning Rangliste.?Zum damaligen Zeitpunkt konnten neuronale Netze ihr volles Potenzial noch nicht entfalten. Zum einen fehlte es noch immer an Rechenpower, zum anderen litten komplexere Netzarchitekturen, so wie sie heute verwendet werden, am sog. ""Vanishing?Gradient Problem"", welches das ohnehin schon schwierige Training der Netze weiter erschwerte oder gänzlich unmöglich machte. Bis in die Anfänge des neuen Jahrtausends fristeten die heute umjubelten neuronale Netze ein weitestgehend tristes Dasein im stillen Kämmerlein. Erst als Geoffrey?Hinton, einer der renommiertesten Forscher im Bereich neuronaler Netze, im Jahr 2006 ein revolutionäres Paper veröffentlichte in dem erstmalig das erfolgreiche Training eines mehrschichtigen neuronalen Netzes gelang, erfuhr das Thema neue Beachtung in Forschung und Praxis. Neue methodische?Fortschritte, bspw. die Spezifikation von alternativen Aktivierungsfunktionen, die das?Vanishing?Gradient Problem verhindern sollten oder die Entwicklung von verteilten Rechensystemen beflügelten die Forschung und Anwendung im Bereich neuronaler Netze. Seit nun mehr als 10 Jahren etablieren sich neuronale Netze und Deep Learning zusehends als Synonym für maschinelles Lernen und künstliche Intelligenz. Sie sind ein boomendes wissenschaftliches Forschungsfeld und finden in vielen Praxisszenarien Anwendung. Regelmäßig unterbieten sich Deep Learning Modelle bei der Lösung komplexer?Machine?Learning Aufgaben im Bereich Sprach- und Bilderkennung. Getrieben durch die rasanten technischen Fortschritte im Bereich verteiltes Rechnen und GPU Computing scheint es nun so, als wäre Deep Learning schlussendlich zur richtigen Zeit am richtigen Ort, um sein volles Potenzial zu entfalten. Neuronale Netze – ein Abbild des ?Gehirns?Entgegen der weit verbreiteten Meinung, dass neuronale Netze auf der Funktionsweise des Gehirns basieren, kann lediglich bestätigt werden, dass sich moderne Deep Learning Modelle nur zu einem gewissen Teil aus Erkenntnissen der Neurowissenschaft entwickelten. Zudem weiß man heute, dass die tatsächlichen Abläufe und Funktionen im Gehirn, die zur Verarbeitung von Informationen berechnet werden, wesentlich komplexer sind als in neuronalen Netzen abgebildet. Grundsätzlich kann jedoch die Idee, dass viele einzelne ""Recheneinheiten"" (Neuronen) durch eine Vernetzung untereinander Informationen intelligent verarbeiten als Grundprinzip anerkannt werden.Im Gehirn sind Neuronen über Synapsen miteinander verbunden, die sich zwischen Neuronen neu bilden bzw. allgemein verändern können. Die unten stehende Abbildung zeigt eine Pyramidenzelle, eine der am häufigsten anzutreffenden Neuronenstrukturen im menschlichen Gehirn.Darstellung einer Pyramidenzelle (Abbildung aus Haykin, Simon (2008), Seite 8)Wie auch andere Neuronenarten empfängt die Pyramidenzelle die meisten Ihre Eingangssignale über die Dendriten, die mittels Axon und dessen Dendriten zehntausend oder mehr Verbindungen zu anderen Neuronen eingehen können. Gemäß dem sog. Konvergenz-Divergenzprinzip erfolgt die Erregung einzelner Neuronen auf Basis vieler anderer Zellen wobei das Neuron ebenfalls gleichzeitig Signale an viele andere Zellen aussendet. Somit entsteht ein hochkomplexes Netzwerk von Verbindungen im Gehirn, die zur Informationsverbeitung genutzt werden.Einfach gesprochen, verarbeiten im Gehirn somit einzelne Neuronen die Signale anderer Neuronen (Inputs) und geben ein darauf basierendes, neues Signal an die nächste Gruppe von Neuronen weiter. Ein einzelnes Neuron ist somit durch eine bestimmte biologische (bzw. mathematische) Funktion repräsentiert, die seine Eingangssignale bewertet, ein entsprechendes Reaktionssignal erzeugt und dieses im Netzwerk an weitere Neuronen weitergibt. Der Begriff Netzwerk entsteht dadurch, dass viele dieser Neuronen in Schichten zusammengefasst werden und Ihre Signale die jeweils folgenden Knoten bzw. Schichten weitergeben und sich somit ein Netz zwischen den Neuronen spannt.Was versteht man unter Deep Learning?Unter Deep Learning versteht man heute Architekturen von neuronalen Netzen, die über mehr als einen Hidden Layer verfügen. Der Nutzen mehrerer Neuronen-Schichten macht sich dadurch bemerkbar, dass zwischen den Schichten ""neue"" Informationen gebildet werden können, die eine Repräsentation der ursprünglichen Informationen darstellen. Wichtig hierbei ist zu verstehen, dass diese Repräsentationen eine Abwandlung bzw. Abstaktion der eigentlichen Eingangssignale sind.„Unter Deep Learning versteht man heute neuronalen Netze, die über mehr als einen Hidden Layer verfügen.“Dieser Mechanismus, der unter dem Begriff ""Representation?Learning"" zusammengefasst werden kann sorgt dafür, dass Deep Learning Modelle in der Regel sehr gut auf neue Datenpunkte abstrahieren?können. Der Grund dafür ist, dass die geschaffenen Abstraktionen der Daten wesentlich generellerer Natur als es die ursprünglichen Eingangsdaten sind. Lernen durch Repräsentation ist bei Deep Learning eines der Hauptunterscheidungsmerkmale gegenüber klassischen Machine Learning Modellen, die i.d.R. ohne Repräsentation der Daten lernen. Die folgende Abbildung illustriert den Unterschied zwischen statistischen Computerprogrammen (Rule-based Systems), klassischen Machine Learning Modellen und Deep Learning. Schematische Darstellung Representation Learning (Abbildung aus Goodfellow (2016), Seite 10)Da Deep Learning Modelle theoretisch über sehr viele Schichten verfügen, ist die Abstraktionskapazität besonders hoch. Somit kann ein Deep Learning Modell auf verschiedenen Ebenen Abstraktionen der Eingangsdaten bilden und zur Lösung des Machine Learning Problems verwenden. Ein Beispiel für eine solche komplexe Architektur ist das GoogleNet zur Bildklassifikation (siehe Abbildung). Übersicht Architektur GoogleNet.Das GoogleNet verfügt über mehrere Architekturblöcke, die speziell auf die Anforderungen im Bereich Objekt- bzw. Bilderkennung ausgelegt sind. Somit ist das Netz in der Lage durch Representation Learning mehrere tausend Objekte auf Bildern mit einer hohen Genauigkeit zu erkennen. Aufbau und Bestandteile von neuronalen NetzenZurück zu den Basics. Der Grundbaustein jedes neuronalen Netzes ist das Neuron. Ein Neuron ist ein Knotenpunkt im neuronalen Netzwerk an dem ein oder mehrere Eingangssignale (numerische Daten) zusammentreffen und von der sog. Aktivierungsfunktion des Neurons weiterverarbeitet werden. Der Output?des Neurons ist somit eine Funktion des?Inputs. Es existieren verschiedene Aktivierungsfunktionen, auf die wir in späteren Teilen unserer Deep Learning Reihe genauer eingehen werden.Der elementare Grundbaustein jedes neuronalen Netzes ist das Neuron. Ein Neuron ist ein Knotenpunkt im neuronalen Netzwerk an dem ein oder mehrere Eingangssignale (Inputs) zusammentreffen und verarbeitet werden. Dabei kann es sich, je nachdem an welcher Stelle sich das Neuron im Netzwerk befindet, sowohl um Signale der Eimngangsschicht als auch Signale vorhergehender Neuronen handeln. Nach der Verarbeitung der Eingangssignale werden diese als Output an die nachfolgenden Neuronen weitergegeben. Formal gesprochen, ist der Output eines Neurons eine Funktion der Inputs. Die folgende Abbildung soll den grundsätzlichen Aufbau darstellen:Die?obenstehende?Abbildung stellt schematisch den Aufbau eines einzelnen Neurons dar. Auf der linken Seite der Abbildung treffen die Inputs  am Neuron ein. Input steht dabei für einen beliebigen numerischen Wert (z.B. vorhandene Daten oder Signale vorhergehender Neuronen). Das Neuron bewertet (gewichtet) den Input, berechnet den Output  und gibt diesen an die darauffolgenden, verbundenen Neuronen weiter. Der Input eines Neurons ist somit die gewichtete Summe aller mit ihm verknüpften vorher liegenden Neuronen.In der Regel werden in neuronalen Netzen keine einzelnen Neuronen, sondern ganze Schichten mit beliebig vielen dieser Knoten, modelliert. Diese Schichten werden „Hidden Layer“ genannt und sind der Kern der Architektur von Deep Learning Modellen. Analog zum Hidden Layer existieren der Input- und Output Layer. Ersterer ist der Eintrittspunkt der Daten in das Modell, während letzterer das Ergebnis des Modells repräsentiert. In einer einfachen Architektur sind alle Neuronen aller benachbarten Layer miteinander verbunden. Die folgende Abbildung stellt den Aufbau eines einfachen neuronalen Netzes mit einem Input Layer, einem Hidden Layer und einem Output Layer dar.Die Verbindungen (Gewichte / Weights) zwischen den Neuronen sind derjenige Teil des neuronalen Netzes, der auf die vorliegenden Daten angepasst wird. Die Architektur des Netzes bleibt (zumindest in einfachen Architekturen) konstant während des Trainings. Die Gewichte werden von Iteration zu Iteration so angepasst, dass der Fehler, den das neuronale Netz während dem Trainings macht immer weiter reduziert wird. Während des Trainings werden Abweichungen der Schätzung des Netzwerks von den tatsächlich beobachteten Datenpunkten berechnet. Nach der Berechnung des Gesamtfehlers werden die Gewichte des Netzes rekursiv aktualisiert. Die Richtung dieser Aktualisierung wird so gewählt, dass der Fehler im nächsten Durchlauf kleiner wird. Diese Methodik nennt man ""Gradient?Descent"" und beschreibt die iterative Anpassung der Modellparameter in entgegengesetzter Richtung zum Modellfehler. Auch heutzutage werden Deep Learning Modelle und neuronale Netze mittels Gradient?Descent?bzw. aktuelleren Abwandlungen davon trainiert. Architekturen von neuronalen NetzenDie Architektur von neuronalen Netzen kann durch den Anwender nahezu frei spezifiziert werden. Tools wie ?TensorFlow ?oder Theano ermöglichen es dem Anwender, Netzarchitekturen beliebiger Komplexität zu modellieren und auf den vorliegenden?Daten zu?schätzen. Zu den Parametern der Netzarchitektur zählen im einfachsten Falle die Anzahl der Hidden Layer, die Anzahl der Neuronen pro Layer sowie deren Aktivierungsfunktion. Im Rahmen der Forschung zu neuronalen Netzen und Deep Learning haben sich unterschiedlichste Architekturen von Netzwerken für spezifische Anwendungen entwickelt. Jede dieser Architekturen weist spezifische Vorteile und Eigenschaften auf, die die Verarbeitung von speziellen Informationen erleichtern sollen. So wird beispielsweise bei?Convolutional?Neural?Networks (CNNs), die primär zur Verarbeitung von Bildinformationen eingesetzt werden, die?räumliche?Anordnung von Informationen berücksichtigt, bei?Recurrent?Neural?Nets (RNNs) die?zeitliche?Anordnung von Datenpunkten. Im Folgenden sollen die wichtigsten Typen von Architekturen kurz skizziert werden. Feedforward ?NetzeUnter einem?Feedforward ?Netz versteht man ein neuronales Netz mit einer Inputschicht, einem  oder mehreren Hidden Layers sowie einer?Outputschicht. In der Regel handelt es sich bei den Hidden?Layers?um sogenannte ""Dense?Layers"", d.h. voll vernetzte Neuronen mit Gewichtungen zu allen Neuronen der vorherigen und folgenden Schicht. Die folgende Abbildung zeigt ein Deep Learning Modell, das mit vier Hidden Layers konstruiert wurde.Während Multi Layer Perceptrons (MLP) als universale Architekturen für eine Vielzahl von Fragestellungen geeignet sind, weisen Sie einige Schwächen in bestimmte Einsatzgebieten auf. So steigt die für eine Bildklassifikation notwendige Anzahl der Neuronen für eine Bildklassifikation mit jedem Pixel des Bildes immer weiter an. Convolutional?Neural?Networks (CNNs)Convolutional?Neural?Networks (CNN) sind ein spezieller Typ von neuronalen Netzwerken zur Verarbeitung von räumlich angeordneten Daten. Hierzu zählen bspw. Bildinformationen (2 Dimensionen), Videos (3 Dimensionen) oder Audiospuren (1-2 Dimensionen). Die Architektur von CNNs unterscheidet sich deutlich von der eines klassischen?Feedforward?Netzes. CNNs werden mit einer speziellen Architektur gestaltet, den sogenannten?Convolutional?und Pooling?Layers. Der Zweck dieser Schichten ist die Untersuchung des Inputs aus verschiedenen Perspektiven. Jedes Neuron im?Convolutional?Layer überprüft einen bestimmten Bereich des Input Feldes mithilfe eines Filters, dem sog. Kernel. Ein Filter untersucht das Bild auf eine bestimmte Eigenschaft, wie z.B. Farbzusammensetzung oder Helligkeit. Das Ergebnis eines Filters ist der gewichtete Input eines Bereichs und wird im?Convolutional?Layer gespeichert.Die Größe (oder vielmehr Tiefe) des?Convolutional?Layers?definiert sich über die Anzahl der Filter, da jeweils das gesamte Input Bild von jedem Filter geprüft wird. Diese Information wird zwischen den einzelnen?Convolutional?Layern?mit sogenannten Pooling?Layern?komprimiert. Die Pooling Layer laufen die, durch die Filter erstellten, Feature Maps ab und komprimieren diese, d.h. sie reduzieren die Anzahl der Pixel nach einer gegebenen Logik weiter. Beispielsweise können hierbei Maximalwerte oder Mittelwerte der Filter verwendet werden. Anschließend können noch weitere?Convolutional?Layer und/oder Pooling Schichten folgen, bis die abstrahierten Features in ein voll vernetztes MLP übergeben werden, das wiederum im Output Layer mündet und die Schätzungen des Modells berechnet. Convolutional und Pooling Layer komprimieren somit die räumlich angeordneten Informationen und reduzieren die Anzahl der geschätzten Gewichtungen im Netzwerk. Somit können auch hochdimensionale Bilder (hohe Auflösung) als Inputs verwendet werden. Recurrent?Neural?Networks (RNNs)Recurrent?Neural?Networks (RNNs) sind ein Oberbegriff für eine Gruppe von Netzwerkarchitekturen bei denen die Neuronen ihre Signale in einem geschlossenen Kreis weitergeben. Dies bedeutet, dass der Output einer Schicht auch an die gleiche Schicht als Input zurückgegeben wird. Es ist dem Netzwerk dadurch möglich Informationen bzw. Daten aus zeitlich weit auseinanderliegende Observationen im Training mit zu berücksichtigen. Dadurch eignet sich diese Architekturform primär für die Analyse sequentieller Daten, wie etwa Sprache, Text oder Zeitreihendaten.Eine der bekanntesten RNN Architekturen ist das Long Short Term Memory (LSTM) Netzwerk. Hierbei werden zusätzliche Parameter darauf trainiert, den Input und Output des Netzes für die nächste Iteration zu speichern oder zu verwerfen, um auf diese Weise zusätzliche Informationen zur Vorhersage für den nächsten Sequenzabschnitt zur Verfügung zu stellen. So können zuvor aufgetretene Signale über die zeitliche Dimension der Daten gespeichert und später verwendet werden. LSTMs werden aktuell sehr erfolgreich im NLP (Natural Language Processing) angewendet, um Übersetzungen von Texten anzufertigen oder Chat-Bots zu trainieren. Weiterhin eignen sich RNNs für die Modellierung von Sequenzen im Allgemeinen, bspw. bei der Zeitreihenprognose oder aber auch für Next Best Action Empfehlungen. Weitere NetzarchitekturenNeben den drei oben dargestellten Architekturen für Deep Learning Modelle existieren zahlreiche Varianten und Abwandlungen von CNNs und RNNs. Weiterhin erfahren aktuell sog. GANs (Generative?Adversarial?Networks) große Aufmerksamkeit in der Deep Learning Forschung. GANs werden verwendet, um Inputs des Modells zu synthetisieren, um somit neue Datenpunkte aus der gleichen Wahrscheinlichkeitsverteilung der Inputs zu generieren. So lassen sich zum Beispiel Datensätze aber auch Bild- und Toninformationen erzeugen, die dem gleichen ""Stil"" der Inputs entsprechen. Somit können z.B. neue Texte, die in ihrer Komposition denen bekannter Schriftsteller entsprechen oder neue Kunstwerke, die dem Malstil großer Künstler nachempfunden sind erzeugt werden. Die Anwendungsbereiche von GANs sind extrem spannend und zukunftsträchtig, jedoch ist aktuell das Training solcher Netze noch experimentell und noch nicht ausreichend gut erforscht. Erste Modelle und Ergebnisse aus der aktuellen Forschung sind jedoch äußerst vielversprechend. Spannende Ergebnisse werden insbesondere im Bereich der künstlichen Bilderzeugung generiert. So werden heute bereits GANs trainiert, die es ermöglichen anhand eines Fotos das Aussehen im hohen Alter zu simulieren oder anhand einer Frontalaufnahme eine 360-Grad-Ansicht des Motivs zu generieren. Weitere Anwendung finden GANs im Bereich Text-t0-Image Synthese, d.h. anhand einer textualen Bildbeschreibung erzeugt das GAN eine fotorealistische Abbildung.Zusammenfassung und AusblickDeep Learning und neuronale Netze sind spannende Machine Learning Methoden, die auf eine Vielzahl von Fragestellungen angewendet werden können. Durch Representation?Learning, also der Fähigkeit abstrakte Konzepte aus Daten zu extrahieren und diese zur Lösung eines Problems zu verwenden, zeigen Deep Learning Modelle für viele komplexe Fragestellungen eine hohe Genauigkeit und Generalisierbarkeit auf neue Daten. Die Entwicklung von Deep Learning Modellen, Algorithmen und Architekturen schreitet extrem schnell voran, sodass davon ausgegangen werden kann, dass sich Deep Learning weiter als Benchmark in vielen?Machine?Learning Disziplinen festigen wird. Durch die immer weitere voranschreitende technische Entwicklung kann damit gerechnet werden, dass immer komplexere Architekturen modelliert werden können. Bereits heute gibt es spezielle Deep Learning Hardware, wie Google TPUs (Tensor Processing Units), die speziell auf die numerischen Anforderungen von Deep Learning Modellen abgestimmt sind.ReferenzenRosenblatt, Frank (1958). The perceptron. A probabilistic model for information storage and organization in the brain. Psychological Reviews, 65: S. 386–408.Goodfellow, Ian; Bengio Yoshua; Courville, Cohan (2016). Deep Learning. MIT Press.Haykin, Simon (2008). Neural Networks and Machine Learning. Pearson.Bishop, Christopher M (1996). Neural Networks for Pattern Recognition. Oxford Press.""ÜberSebastian HeinzI am the founder and CEO of STATWORX. I enjoy writing about machine learning and AI, especially about neural networks and deep learning. In my spare time, I love to cook, eat and drink as well as traveling the world.";https://www.statworx.com/de/blog/deep-learning-teil-1-einfuehrung/;Statworx;  Sebastian Heinz
  8. Oktober 2017;Einführung in die nicht-parametrische Statistik: der Wilcoxon-Rangsummen-Test;"Niederländer lieben Käse, Holzschuhe und am liebsten sitzen sie hinter dem Steuer ihres Wohnwagens! Oder? Will man solche Vorurteile (seriös) untersuchen, muss man dazu Daten erheben. So könnte man die jährliche Fahrzeit im Wohnwagen bei Niederländern und beispielsweise Deutschen erfassen und im Anschluss überprüfen, ob sich beide Nationen im Mittel bei der Fahrzeit unterscheiden. Dafür bietet sich als erstes der t-Test für unabhängige Stichproben an, der testet, ob sich die Mittelwerte beider Gruppen signifikant voneinander unterscheiden. Wendet man diesen Test an, befindet man sich sofort im parametrischen Universum: einem Raum voller Möglichkeiten, aber auch ein Raum voller statistischer Voraussetzungen.Die bekannteste davon ist sicherlich, dass die abhängige Variable (die Zeit hinter dem Steuer) normalverteilt sein muss. Zudem sollten die Gruppen ungefähr gleich groß und jeweils mindestens 30 Personen vorhanden sein. Ist eine dieser Voraussetzungen nicht erfüllt, ist das Ergebnis des t-Tests nicht mehr verlässlich. In einem solchen Fall ist es Zeit für eine Reise in das nicht-parametrische Universum. Bei dieser Reise entdeckt man schnell die Welt des Wilcoxon-Rangsummen-Tests. Dieser ist die nicht-parametrische Alternative zum t-Test für unabhängige Stichproben und macht weniger strenge Annahmen über die Verteilung der Daten. Insbesondere die Voraussetzung einer Normalverteilung der abhängigen Variable muss nicht erfüllt sein. Was ihn mit dem t-Test allerdings verbindet ist, dass beide Tests überprüfen, ob sich zwei Gruppen im Mittel voneinander unterscheiden. Der Wilcoxon-Rangsummen-Test überprüft jedoch den Median zwischen zwei Gruppen und nicht den Mittelwert. Lasst die Entdeckungsreise beginnen Bevor wir die Welt des Wilcoxon-Rangsummen-Tests erkunden, noch ein kurzer Reisehinweis: Sie wurde 1945 von Frank Wilcoxon entdeckt und beschrieben, doch auch Mann und Whitney haben 1947 darüber berichtet. Dadurch ist der Test unter verschiedenen Namen bekannt: Wilcoxon-Rangsummen-Test, Mann-Whitney- oder auch U-Test. Dies ist jedoch kein Grund reisekrank zu werden, denn die Beschreibungen von Wilcoxon (1945) und Mann/Whitney (1947) unterscheiden sich zwar leicht (geringfügig andere Berechnungen), aber die Welt ist immer die Gleiche (gleiches Ergebnis beider Tests). Jetzt aber Rucksack auf und lasst uns die Reise beginnen. Die erste Entdeckung, die wir dabei machen, kommt jedoch unerwartet: Auch Niederländer und Deutsche haben diese Welt bereits entdeckt und erkunden sie mit ihren Wohnwägen. Trotz unseres Erstaunens nutzen wir diesen Umstand sofort, um das Vorurteil über Niederländer und ihren Wohnwagen zu erforschen. Wir konnten letztlich 26 Personen nach ihrer Zeit hinter dem Steuer befragen, haben die Werte notiert und zusätzlich für die Fahrzeiten Rangplätze vergeben, um zu sehen welche Person die meiste und welche die kürzeste Zeit gefahren ist (siehe fiktive Daten in Tabelle). Rang 1 bekommt jene Person mit der kürzesten und Rang 26 jene mit der längsten Fahrzeit und zwar unabhängig davon, aus welchem Land die Person stammt.NiederlandeDeutschlandFahrzeit mit dem Wohnwagen minRangplatzFahrzeit mit dem Wohnwagen minRangplatz86031444781110532567051293451555726948516452221072423728213115852648383109412452655965317601510104372251684102832163001198491859789101431958888101572057987In den Fahrzeiten werden Unterschiede zwischen den Nationen deutlich, aber sind sie auch signifikant? Aufgrund der kleinen Stichprobe in beiden Gruppen kann der t-Test nicht für diese Frage angewendet werden, der Wilcoxon-Rangsummen-Test ist hier passender. Eine perfekte Möglichkeit die Welt dieses Tests kennenzulernen! Der Test macht erstmal genau das Gleiche wie wir: Er ordnet den einzelnen Fahrzeiten der Reihe nach Ränge zu (siehe Tabelle). Im Anschluss werden diese aufsummiert. Aber Achtung: Die Vergabe der Ränge erfolgte ohne Gruppenzugehörigkeit, die Aufsummierung erfolgt getrennt nach den Nationen. Dadurch ergibt sich bei den Niederländern eine Rangsumme von 260 und bei den Deutschen von 91. Bei ungleich großen Gruppen verwendet derTest die Rangsumme in der kleineren Gruppe und bei gleich großen Gruppen, wie in diesem Fall, die kleinere Rangsumme von 91. Diese wird dann an der Anzahl der Personen in der Gruppe relativiert, da sonst größere Gruppen auch immer größere Rangsummen aufweisen würden. Durch die Relativierung ergibt sich die Teststatistik W, für die der dazugehörige p-Wert wie folgt berechnet werden muss. Entweder durch eine exakte Berechnung des p-Werts mit Hilfe einer Simulation oder indem die Teststatistik W durch ihren Standardfehler geteilt wird. Da in den Daten jeder Wert nur einmal vorkommt und die Stichprobe insgesamt kleiner als 40 ist, muss die exakte Methode angewendet werden. Der p-Wert für unsere Daten ist kleiner als 0,001 und das Ergebnis daher signifikant. Man kann also davon ausgehen, dass sich beide Nationen nicht nur in unserer Stichprobe, sondern auch in der gesamten Population bei der mittleren Fahrzeit im Wohnwagen unterscheiden. Wieso die Welt des Wilcoxon-Rangsummen-Test ist, wie sie ist An dieser Stelle ist unsere Reise fast schon beendet, lediglich eine Frage drängt sich noch auf: Wieso kann der Wilcoxon-Rangsummen-Test mit einer Rangsumme überprüfen, ob sich die Mediane zweier Gruppen voneinander unterscheiden? Ganz einfach: weil sich unterschiedliche Mediane der beiden Gruppen auch in unterschiedlichen Rangsummen widerspiegeln. In den von uns erhobenen Daten zeigt sich dies besonders deutlich. Der Median bei den Niederländern ist 10157 und bei den Deutschen 5798 Minuten. Dieser große Unterschied zwischen den Gruppen zeigt sich auch in den Rangsummen: 260 bei den Niederländern und 91 bei den Deutschen. Der Grund dafür, dass sich sowohl Mediane als auch Rangsummen zwischen den beiden Gruppen stark unterscheiden ist, dass aufgrund der unterschiedlichen Mediane alle Fahrzeiten der Deutschen geringer sind als jene der Niederländer. Dadurch bekommen die Deutschen nur die niedrigen und die Niederländer nur die hohen Ränge, was letztlich zu den unterschiedlichen Rangsummen führt. Dadurch kann der Test indirekt untersuchen, ob sich die Mediane zweier unabhängiger Gruppen signifikant voneinander unterscheiden. Referenzen:  Mann, H. B. &amp; Whitney, D. R. (1947). On a test of whether one of two random variables is stochastically larger than the other. Annals of Mathematical Statistics, 18, 50-60. Wilcoxon, F. (1945). Individual comparisons by ranking methods. Biometrics, 1, 80-83. Über den AutorAlexander NiltopI am a statistician at STATWORX and don't just want to understand the fascinating world of statistics but explain it easily, too. I hope, it worked!";https://www.statworx.com/de/blog/einfuehrung-in-die-nicht-parametrische-statistik-der-wilcoxon-rangsummen-test/;Statworx;  Alexander Niltop
  8. Oktober 2017;Schlüsselkennzahl: Preiselastizität der Nachfrage;"Preismanagement ist in den vergangenen Jahren zunehmend in den Fokus von Geschäftsführungen gerückt(1). Verbunden ist diese Entwicklung mit der Hoffnung, mit gezielter Preisgestaltung einen zentralen, aber vernachlässigten Profittreiber identifiziert zu haben. Tatsächlich bestätigen Experten, dass Preismanagement in der Vergangenheit häufig unter unzureichender Aufmerksamkeit gelitten hat (2,4). Unterentwickelte Preissetzungsexpertise und ausbaufähige Preisstrategien sind die Folge. Die Bedeutung vom richtigen Preismanagement Preise haben einen stärkeren Einfluss auf den Absatz als die meisten anderen Faktoren. Glaubt man entsprechenden Studien, haben Preissetzungsmaßnahmen eine erheblich stärkere Wirkungskraft als etwaige Steigerungen des Werbebudgets oder als Investitionen ins Vertriebspersonal(3). Preismanagement kann eine solche Wirkungskraft deshalb entfalten, weil Preisänderungen einen direkten und beinahe sofortigen Einfluss auf Umsatz, Absatz und Gewinn haben – dies ohne dabei zwingenderweise Veränderungen der Prozesse und Strukturen eines Unternehmens zu erfordern.Unstrittig ist, dass Preismanagement das wohl sensibelste Instrument im Marketing-Werkzeugkasten eines Unternehmens ist. Anders als es durch eine neue Werbekampagne, Maßnahmen zur Kosteneinsparung oder gar innovative Veränderungen am Produkt möglich ist, erlauben z.B. Preissetzungsmaßnahmen eine beinahe unmittelbare Reaktion auf veränderte Marktsituationen. Und – auch das zeigen Studien – beim Kaufverhalten des Kunden stellt sich die Wirkung einer Preisveränderung schneller ein als es bei Werbemaßnahmen oder Produktneueinführungen der Fall ist (4). Darin liegen Risiko und Chance gleichermaßen.Multi-Channel-Marketing und Vertrieb haben die Bedeutung des Preisemanagements in den vergangenen Jahren noch einmal potenziert. Kunden, Produzenten und Retailer befinden sich in einem merkwürdigen Limbo: Zum einen bietet das Internet Kunden nie zuvor dagewesene Möglichkeiten zum Preisvergleich und damit verbunden eine hohe Preistransparenz. Auf der anderen Seite bietet es Produzenten und Retailern durch differenzierte Marktinformationen und dynamische Preissetzungsstrategien bisher ungeahnte Möglichkeiten der Preisdifferenzierung. Die Vielschichtigkeit des Themas, die Fülle an ungenutzten Daten und Informationen und die dadurch wachsende Bedeutung von professionellem Preismanagements sind es, die so viele Kunden motivieren, an uns heranzutreten. Gemeinsam mit Ihnen entwickeln wir Analytics-Ansätze und bestimmen Metriken, mit denen Sie ein informationsgesteuertes Preismanagement entwickeln können. Preiselastizität der Nachfrage Regelmäßig fällt dabei der Fokus auf das Thema Preiselastizität der Nachfrage. Dieses Interesse an Preiselastizität hat, wie die Abb. 1(5) zeigt, seinen guten Grund: Preis und Marksituation bedingen zusammen mit den jeweiligen Produktmerkmalen das Verkaufsvolumen und damit alle anderen wesentlichen Geschäftskennzahlen. Dabei beschreibt Preiselastizität die praktikabelste und aussagekräftigste Metrik, um die im Preismanagement entscheidende Frage zu beantworten: Wie reagieren Kunden auf eine Preisehöhung/Preissenkung um x Prozent? Damit ist die entscheidende Komponente, die es beim Preismanagement zu verstehen gilt, der Kunde.Das Maß der Preiselastizität reduziert die Frage bzgl. der Kundenreaktion auf eine einfach interpretierbare Kennzahl. Konkret beschreibt Preiselastizität der Nachfrage die relative Veränderung des Absatzes in Folge einer relativen Veränderung des Preises. Zum Beispiel: Ein Unternehmen erhöht seinen Preis um 10%. Sinkt der Absatz in der Folge ebenfalls um 10%, spricht man von einer Preiselastizität von -1. Sinkt der Absatz um 20%, so liegt die Elastizität bei -2. Dies ist intuitive und klar. Auf der Basis dieser Kennzahl ist es möglich ein differenziertes Preismanagement aufzubauen. Preiselastizität erlaubt es Entscheidungsträgern, Produkte miteinander zu vergleichen. In Kombination mit belastbaren Kosteninformationen bildet Preiselastizität die Grundlage für profit-optimale Preisbestimmung. Diese herausragende Stellung verdankt Preiselastizität u. a. ihrer „Dimensionsfreiheit“ (5). Preiselastizitätskennzahlen können über Marken, Produktgruppen, Kunden und Märkte hinweg verglichen werden. Dies legt die Grundlage für eine detaillierte Marktsegmentierung.Wir haben in den vergangenen Jahren vielfältige Erfahrung mit dem Bestimmen von Preiselastizitäten anhand von Marktdaten gesammelt. Im Rahmen dieser Projekte galt es, das Informationspotential der Daten unserer Kunden so zu nutzen, dass es als Grundlage für die Verbesserung der Preisstrategie diente. Bei der Aufarbeitung der Daten rücken wir zwei Fragen in den Mittelpunkt: Welche Faktoren beeinflussen auf welche Weise Preiselastizität? Wie können wir Preiselastizität so bestimmen, dass sie um störende Einflussfaktoren und potentielle Messfehler bereinigt ist? Diskussionsanstoß Die Lösung ist keinesfalls trivial. Es wurden in der Empirie viele Fallstricke und Besonderheiten identifiziert, die, wenn nicht angemessen berücksichtigt, große Ungenauigkeiten bei der Bestimmung von Preiselastizität und daraus resultierende Fehleinschätzungen verursachen. Auch wenn man zu dem Thema eine umfassende, wenn auch kontroverse Literatur, vorfindet, so ist sie für die Lösung vieler Fragen unabdingbar und hilft beim Entdecken des richtigen Vorgehens. Doch die Praxis zeigt, dass die theoretischen Erkenntnisse die konkreten Bedürfnisse des Kunden nicht ausreichend und spezifisch genug erfüllen können. Die Daten, mit denen wir arbeiten, sind kundenspezifisch und jeder Auftrag stellt neue, individuelle Herausforderungen, für die es in der Wissenschaft keine fertigen Lösungen gibt.In den kommenden Monaten soll es daher an dieser Stelle um die verschiedenen Aspekte und Herausforderungen gehen, die mit der Bestimmung und Anwendung der Preiselastizität zur Steuerung der Nachfrage einhergehen. Wir werden verschiedene Methoden und ihre Vor- und Nachteile diskutieren. Wie werden besprechen, wie man welche Einflussfaktoren berücksichtigen kann und welchen Einfluss sie haben. Und wir werden diskutieren, wie man die Ergebnisse nutzen kann, um daraus Business-Tools zu entwickeln, die ein besseres Preismanagement ermöglichen. An dieser Stelle wird es um die oben beschriebenen grundsätzlichen Ansätze und Konzepte gehen. Vielleicht werden auch Sie dazu motiviert, mehr aus Ihren Daten herauszuholen. Referenzen Simon-Kucher &amp; Partner (2016). Global Pricing Study 2016. Bonn.Rekettye, G. (2011). The increasing importance of pricing. In: Hetsi, E., Kürtösi, Zs. (eds), The diversity of research at the Szeged Institute of Business Studies. Szeged: JATEPress, 13-22Albers, S., Mantrala, M.K., Sridar, S. (2010). Personal Selling Elasticites: A Meta Analysis. Journal of Marketing Research, 47(5), 840-853.Simon, H., Fassnacht M. (2016). Preismanagement: Strategie, Analyse, Entscheidung, Umsetzung. (Auf. 4). Wiesbaden: Springer GablerFriedel. E. (2014). Price Elasticity: Research on Magnitude and Determinants. Frankfurt am Main: Peter Lang Über den AutorDaniel LüttgauI am a data science consultant at STATWORX.Exploring business concepts and thinking up ways to utilize data for our customers is what I enjoy most about my job. My freetime is commited to my dog, travelling and my wife and friends..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/schluesselkennzahl-preiselastizitaet-der-nachfrage/;Statworx;  Daniel Lüttgau
  20. September 2017;Data Science mit R;"Data Science, Big Data, Machine Learning – die rasante Entwicklung von Computern, Massenspeichern und korrespondierenden Technologien wirft seit geraumer Zeit ein neues Licht auf die Speicherung und Auswertung von Daten strukturierter und unstrukturierter Art. Insbesondere der Begriff „Big Data“, der mittlerweileeher inflationär verwendet wird, war ein Buzzword ganzer Industrien. Im gleichen Atemzug zu Begriffen wie Data Science, Machine Learning und Big Data fällt der Begriff R – neben Python die Open-Source Software für Statistik, Machine Learning und Data Science.Die Evolution von Data ScienceStrukturiert gespeichert werden Daten in Unternehmen schon seit vielen Jahren. Oft zwar noch dezentralisiert aber grundsätzlich zugänglich. Tools für IT-Reporting, BI und Dashboards gibt es wie Sand am Meer, vertrieben von den Großen der Branche (z.B. IBM, SAS, SAP usw.) zu entsprechenden Preisen. Bei klassischen BI- oder Reporting-Systemen ist der Fokus klar retrospektiv ausgerichtet: vergangene Daten werden visualisiert und deskribiert. Zusammenhänge in Daten können, aufgrund der Dimensionalität und Komplexität der verschiedenen Datenpunkte und Quellen, in der Regel kaum bzw. nur schwer erkannt werden. Prospektive, also in die Zukunft gerichtete Analysen sind nicht möglich. Die „Ressource Daten“ liegt somit in vielen Unternehmen brach.Der wahre Wert, der in Daten steckt wird in der Regel erst ersichtlich, wenn es gelingt, anhand vorhandener (vergangener) Daten Informationen zu extrahieren, die prospektiv anwendbar sind und einen Mehrwert erzeugen. Data Science ist der Überbegriff für die hierzu benötigte Fusion aus Methoden und Technologien, die im Datenumfeld angesiedelt sind: Data Warehousing, Data Mining, Machine Learning, Statistik, Programmierung, Computing und Visualisierung spielen hierbei zentrale Rollen. Der sogenannte „Data Scientist“ vereint idealerweise alle Kenntnisse aus den zuvor genannten Feldern und ist somit in der Lage, in Unternehmen Daten zu konsolidieren, aufzubereiten, auszuwerten und fachliche, prospektiv ausgerichtete Handlungsempfehlungen abzuleiten.Anwendungsgebiete von Data ScienceDurch die Omnipräsenz von Daten und Datenquellen in Unternehmen ergeben sich unendliche Anwendungsfälle. Die Frage, die sich Entscheider heute stellen sollten ist eher „was kann ich nicht mit Daten beantworten?“. Klassische Use Cases stellen sich häufig wie folgt dar:was sind die Einflussfaktoren meiner Kunden für einen Kauf/Abschluss?wie viele Einheiten von Produkt x werde ich im nächsten Quartal absetzen?wie gestalte ich Lagerkapazitäten optimal um keinen Platz zu verschwenden?welche Kunden verhalten sich ähnlich und woran kann ich dies erkennen?welche Bauteile meines Produktes fallen wann wie häufig aus?soll ich meinem Kunden einen Kredit gewähren, mit welcher Wahrscheinlichkeit zahlt er zurück?Die Liste ist beliebig lang fortführbar. Häufig existieren im Vorfeld nur wenige oder keine Hypothesen oder Ideen bzgl. möglicher Zusammenhänge in den Daten, diese werden im Laufe des Prozesses kontinuierlich „erzeugt“ und geprüft.Data Science mit RR hat sich in den vergangen Jahren zu einer wahren Allzweckwaffe im Bereich Data Science entwickelt: statistische Modellierungsverfahren, Verfahren aus Data Mining und Machine Learning, Hadoop-Integration, Deployment von interaktiven Web-Applikationen, Web-Scraping und Crawling, Schnittstellen zu fast allen relationalen Datenbanksystemen und Integration in Software wie SPSS, SAS, RapidMiner, Mathematica, Excel, Tableau und QlikView sind nur einige der Features, aus denen R Nutzer und Data Scientists schöpfen können. Mit aktuell über 10.000 Paketen (Zusatzfunktionen zum normalen Funktionsumfang), die von Mitgliedern der R Community entwickelt werden, verfügt R über eine konkurrenzlose Vielfalt an Modellen, Algorithmen und schlussendlich Use Cases im Bereich Data Science.Als wichtiges Kernelement verfügt R über eine massive Auswahl an statistischen und maschinellen Lernverfahren, mit denen Information in Form von Zusammenhängen, Strukturen und Vorhersagen aus bestehenden internen und externen Datenquellen extrahiert werden können. Somit bilden diese Methoden häufig den Kern eines Data Science Projektes. Das Toolset ist vielfältig und beinhaltet beispielweise Modelle wie Random Forests, neuronale Netze, Support Vector Machines, Collaborative Filtering, Bagging und Boosting. Mit Hilfe dieser Verfahren (und einer gesunden Portion Fachwissen und Verstand) lassen sich aus den vorhandenen Daten Modelle erzeugen, die in der Lage sind zukunftsorientierte Aussagen über Kunden, Produkte und Märkte zu treffen.Data Science KurseWir bieten im Rahmen der STATWORX Academy verschiedene Kursezum Thema Data Sciencemit R an, bei denenzentrale Konzepte, Modelle und Best-Practices aus den Bereichen Data Science, Statistik und Machine Learning vermittelt werden. In Hands-on Sessions werden die erlernten Kenntnisse auf praxisnahe (gerne auch Ihre eigenen) Datensätze angewendet und somit vertieft und gefestigt.Data Science ConsultingIn unserem Geschäftsbereich Data Science Consulting bieten wir unseren Kunden eine kompetente Beratung und Umsetzung in den Bereichen Beratung, Modellentwicklung und Programmierung sowie verschrienen Workshops zum Thema Data Science an.Über den AutorSebastian HeinzI am the founder and CEO of STATWORX. I enjoy writing about machine learning and AI, especially about neural networks and deep learning. In my spare time, I love to cook, eat and drink as well as traveling the world..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/data-science-mit-r/;Statworx;  Sebastian Heinz
  4. September 2017;Web-Applikationen mit R und R Shiny;"Seit einiger Zeit ist mit Shiny ein Paket für die Statistiksoftware R verfügbar, mit dessen Hilfe man ansprechende, interaktive Webapplikationen erstellen und hierbei auf den kompletten Funktionsumfang von R zugreifen kann.Mit Hilfe des R Paketes Shiny lassen sich schnell HTML/JavaScript-basierte, interaktive Webapplikationen erstellen. Die möglichen Anwendungsszenarien sind vielfältig: Reporting, Deployment von statistischen Analysen, interaktive Visualisierungen von Datenbeständen. Bei der Erstellung einer Shiny Applikation sind grundsätzlich keine HTML- oder JavaScript-Kenntnisse notwendig, da die komplette Programmierung der App direkt in R stattfindet. Lediglich beim Customizen der Applikation, sprich, bei der Anpassung von Farben, Logos, Fonts und Layouts sind Grundkenntnisse in HTML/CSS/JavaScript nötig.Besonders vorteilhaft an Shiny ist, dass die interaktive Web-App auf den kompletten Funktionsumfang von R zurückgreifen kann. R bietet zur Zeit über 10000 Packages für statistische Auswertungen, Data Mining und Predictive Analytics an und zählt somit zu der wichtigsten Statistiksoftware auf dem Markt. Zudem ist R eine Open-Source-Software und somit, ebenso wie das Zusatzpaket Shiny, kostenlos erhältlich.Shiny ermöglicht es dem Anwender, in einer optisch ansprechenden und übersichtlichen Web-Oberfläche, verschiedene Input-Parameter auszuwählen bzw. zu variieren, auf Basis derer eine Berechnung und Analyse in R gestartet wird. Hierfür werden zwei Shiny-Programme geschrieben. Eines zur Definition der Benutzeroberfläche (ui.r) und ein zweites in dem die Analyse mit R stattfindet (server.r). Als Eingabemöglichkeiten auf Benutzerseite stehen z. B. Slider, Drop-Downs, Text- und Zahleneingabe-, aber auch Optionsfelder zur Verfügung. Auch der Import von lokalen Excel-Files und eine entsprechende Verarbeitung der Daten in der Shiny Applikation ist möglich. Auf der Output-Seite stehen dem Benutzer Tabellen, Summaries, Grafiken, Landkarten und Texte zur Verfügung, die reaktiv, je nach Selektion der Inputparameter, on-the-fly angepasst werden. Grundsätzlich lassen sich alle R Packages in eine Shiny-App implementieren, was nahezu unendliche Anwendungsmöglichkeiten offenbart.Die fertige Applikation kann bspw. via GitHub gist, als R-Paket oder als zip-Ordner versendet oder auf allen Rechnern, die R und Shiny installiert haben, aufgerufen werden. Außerdem kann die Web-App auf einem Linux-basierten Shiny Server online zur Verfügung gestellt werden. Hier existieren eine kostenfreie Open-Source-Edition sowie eine kostenpflichtige Enterprise-Variante, die weitere, für Unternehmen wichtige, Features enthält.Fazit: Dank Shiny ist es möglich, auf eine einfache und schnelle Art ansprechende, interaktive Applikationen zu erstellen, die auf den kompletten statistischen Funktionsumfang von R zurückgreifen können.Über den AutorTobias KrabelI am data scientist at STATWORX, with a secret passion for data and software engineering. Tocompensate for my nerdy sitting-in-the-basement side, I spend even more time in the basement writing shiny applications..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/web-applikationen-mit-r-und-r-shiny/;Statworx;  Tobias Krabel
  4. September 2017;Reporting mit R und R Markdown: A Whisky Guide;"Wir bei STATWORX lieben Daten. Und wir genießen von Zeit zu Zeit gerne ein Gläschen guten schottischen Single Malt Whisky unter Kollegen. Vor einiger Zeit hatten die Kollegen von Revolution Analytics einen interessanten Blog-Beitrag zum Thema k-Means Clustering von Single Malt Whiskies veröffentlicht, den wir aufmerksam studiert haben.Ebenfalls wurde kürzlich eine aktualisierte Version von RStudio veröffentlicht, die ein umfangreiches Update im Bereich Reporting mit knitr und shiny enthielt. Da wir ohnehin geplant hatten, etwas zum Thema Reporting mit R für den STATWORX Blog zu verfassen, haben wir die Chance genutzt, dass Thema „Whisky Daten“ weiter zu verfolgen.Das Ergebnis kann sich sehen lassen: ein vollständig mit knitr erstellter HTML-Report zum Thema Reporting mit R knitr auf Daten-Basis von 86 schottischen Single Malt Whiskies (oder besser gesagt Destillerien). Wir wünschen viel Spaß beim Lesen!Über den AutorFabian MüllerI am the COO at STATWORX and responsible for our data science teams and key accounts. In my spare time, I'm into doing sports and fast cars..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/reporting-mit-r-und-r-markdown-a-whisky-guide/;Statworx;  Fabian Müller
  15. Mai 2017;Hypothesen richtig formulieren;"Eine Hypothese ist eine empirisch überprüfbare Annahme über die Wirkungsbeziehung von zwei oder mehr Faktoren der realen und erfahrbaren Welt. Am Anfang eines jeden Forschungsprojektes stehen, außer einer klaren Fragestellung, die das Forschungsvorhaben leitet, stets auch eine oder mehrere Hypothesen. Erst dadurch kann eine systematische Forschungsarbeit gewährleistet werden.Übersicht zu HypothesenIm Verlauf des Forschungsprojektes gilt es, die Richtigkeit der Hypothesen zu überprüfen. Die Hypothesen müssen also so formuliert sein, dass sie durch eine methodische Sammlung von geeigneten Daten und durch die anschließende systematische Auswertung der Daten auch tatsächlich falsifizierbar sind. Das heißt, die vermuteten Wirkungszusammenhänge der Hypothese sollen sich am Ende als reale Wirkungszusammenhänge erweisen und als plausible Erklärung für beobachtete Phänomene dienen.Es ist also von großer Bedeutung, richtige Hypothesen zu erstellen und dieser wichtigen und komplexen „Vorarbeit“ der Forschung große Aufmerksamkeit und genügend Zeit zu widmen. Hypothesenbildung ist alles andere als eine lästige oder anspruchslose Pflichterfüllung. Gute und qualitativ hochwertige Hypothesenbildung ist eine echte Herausforderung und stellt eine wichtige Voraussetzung für am Ende wirksam nutzbare Ergebnisse dar.In unseren Workshops und Projektberatungen stellen wir immer wieder fest, dass vielversprechende Studien und Forschungsprojekte bereits in diesem frühen Stadium der Hypothesenbildung ins Schleudern geraten. Auch wenn es uns vielfach durch Improvisationsgeschick und Erfahrung gelingt, Studienvorhaben unserer Kunden trotz mangelhafter Hypothesenbildung vor dem Entgleisen zu bewahren, so bleibt der gelegentliche „Totalschaden“ leider nicht aus.Falsche Hypothesen können gravierende Folgefehler verursachen. Unbedachte Formulierungen können Hypothesen zu nicht überprüfbaren Theoriesackgassen werden lassen. Aufwendig gesammelte Daten können nichtig werden, weil bei der Hypothesenerstellung unbedacht vorgegangen wurde. Spannende und vielversprechende Forschungsprojekte dürfen nicht an ihren ungenauen oder falschen Hypothesen scheitern. Dies lässt sich recht leicht vermeiden!In einer kleinen 5-schrittigen Anleitung wollen wir dir deshalb helfen, die größten Hürden bei der eigenständigen Hypothesenbildung zu nehmen. Ziel dieses Guides ist es, dich mit zentralen und realistischen Standards vertraut zu machen, dir pragmatische Lösungswege anzubieten, und dir ein grundlegendes Handwerkszeug für die effiziente Hypothesenbildung zur Verfügung zu stellen:Schritt 1 – Bestimme die relevanten Variablen frühzeitigKläre, für welche Faktoren du dich interessierst. Überlegen dir sorgfältig, wie diese Faktoren am besten mit Daten dargestellt werden können. Lege für die zukünftige statistische Auswertung fest, welcher Faktor durch welche Variable im Datensatz repräsentiert werden soll. Es werden nur diese Faktoren in der statistischen Analyse berücksichtigt. Je besser die Variablen das zu erforschende Phänomen abbilden, desto aussagekräftigere Ergebnisse lassen sich durch die nachfolgende, statistische Analyse erzielen.Tipp: Lass dich von der Arbeit anderer inspirieren und entdecke, wie deine Faktoren in früheren Studien gemessen wurden. Vielfach lassen sich früher erfasste Daten auch für andere (deine) Forschungsfragen verwenden. Dies spielt insbesondere dann eine Rolle, wenn du mit Skalen und Konstrukten arbeitest. Diese werden in der Regel sehr aufwändig entwickelt, getestet und validiert. Ein Prozess, den du im Rahmen deiner Arbeit wahrscheinlich nur schwer nachbilden kannst.Schritt 2 – Formuliere eine allgemeine HypotheseEine Hypothese drückt stets eine vermutete Wirkungsbeziehung zwischen i.d.R. zwei ausgewählten Variablen dar. Dabei handelt es bei einer Variablen um eine unabhängige und bei der anderen um eine abhängige Variable. Zur Erinnerung: Die unabhängige Variable ist die Variable, von der angenommen wird, dass sie Auswirkungen auf die abhängige Variable hat.Es ist für diesen Schritt nicht wichtig, besonders präzise oder detaillierte Formulierungen für die Variable zu finden. Wichtig ist aber, dass klar definiert ist, welche der beiden Variablen die abhängige und welche die unabhängige Variable ist!Zwei Aspekte sind für die Hypothesenbildungen wichtig:Pro Hypothese darf es nur eine abhängige Variable geben. Solltest du mehr abhängige Variable haben, gilt es nachzubessern. In den häufigsten Fällen kannst du das Problem dadurch lösen, dass du für jede der abhängigen Variablen eine eigene Hypothese formulierst.Jede Hypothese sollte auch nur eine unabhängige Variable enthalten. Es gibt zwar Fälle, in denen auch zwei oder mehr Variablen sinnvoll sind, aber dies ist nur dann sinnvoll, wenn du ausdrücklich nach bestimmten Wirkungszusammenhängen zwischen mehreren unabhängigen Variablen suchen. Überlege dir gut, ob dies auf deinen Fall wirklich zutrifft. Viel öfter wird es fälschlich nur als bequemer angesehen, mehrere Hypothesen in einer zu bündeln. Tatsächlich führt dies aber zur Ergebnisunklarheit.Schritt 3 – Bestimme die Wirkungsrichtung deiner HypotheseEs ist üblich zwischen ungerichteten Hypothesen und gerichteten Hypothesen zu unterscheiden. Die allgemeinen Hypothesen, die du in Schritt 2 entwickelt hast, sind ungerichtete Hypothesen. Zur Erinnerung: Eine ungerichtete Hypothese besagt, dass eine Variable die andere Variable auf irgendeine Weise beeinflusst, sie besagt aber nicht, auf welche Weise dies geschieht. In diesem Schritt gilt es nun, die genaue Wirkungsrichtung deiner Hypothese zu bestimmen, d.h. deine Hypothese soll genau beschreiben, wie die unabhängige Variable die abhängige Variable beeinflusst.Auch wenn es Forschungsprojekte gibt, in denen ungerichtete Hypothesen ausreichen, so sind gerichtete Hypothese fast immer die bessere Wahl. Falls deine Theorie und /oder die zugrundlegende Literatur eine Grundlage dafür bieten, nutze die Gelegenheit, denn gerichtete Hypothesen haben eine deutlich höhere Informationskraft als ungerichtete. Dein Forschungsprojekt gewinnt deshalb an Gewicht, wenn es dir gelingt, gerichtete Hypothesen aufzustellen.Schritt 4 – Stelle die Testbarkeit deiner Hypothesen sicherDie in deiner Hypothese dargestellte Wirkungsbeziehung muss in der realen Welt getestet werden, sie muss also beobachtbar und zumindest indirekt messbar sein. Falls dies nicht gewährleistet ist, muss die Hypothese unbedingt noch einmal überarbeitet werden.Folgende beiden Indikatoren eignen sich gut, die Testbarkeit deiner Hypothesen zu überprüfen:Lege pro Hypothese genau die Grundgesamtheit (der Personen oder Dinge) fest, über die du neue Erkenntnisse erlangen willst.Stelle für jede deiner Hypothesen eine sogenannte Nullhypothese auf. Deine bisherigen Hypothesen beinhalteten immer genau die Annahme, die dich eigentlich interessiert. Die Nullhypothese hingegen steht für die Annahme, die du widerlegen willst.Formuliere also die Annahme, die du wiederlegen möchtest. Gehe dabei genauso sorgfältig vor, wie bei deinen ursprünglichen Hypothesen. Wenn du die Grundgesamtheit und Nullhypothese genau benennen kannst, dann bist du auf einem guten Weg.Schritt 5 – Formuliere und bezeichne deine Hypothesen professionellAlle Formulierungen der Hypothesen werden zunächst noch einmal überarbeitet und weiter konkretisiert. Als Faustregel gilt dabei: Formuliere den Wirkungszusammenhang zwischen abhängiger und unabhängiger Variable so detailliert wie nötig und so einfach wie möglich. Abschließend nummerierst du deine Hypothesen systematisch durch. Jede Hypothese wird mit dem Buchstaben „H“, einer Zahl sowie einem Doppelpunkt gekennzeichnet (z.B. „H1:, H2:, H3:, …“).ZusammenfassungIn diesem Guide haben wir dir einige wichtige Tipps zur Formulierung von Hypothesen gegeben. Diese sind als „Best-Practice“ zu verstehen und können natürlich in besonderen Situationen an dein Forschungsvorhaben angepasst werden. Falls du Hilfe bei der Erstellung deiner Hypothesen benötigst, steht dir unser Statistik Team gerne zur Verfügung.Über den AutorDaniel LüttgauI am a data science consultant at STATWORX.Exploring business concepts and thinking up ways to utilize data for our customers is what I enjoy most about my job. My freetime is commited to my dog, travelling and my wife and friends..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/hypothesen-richtig-formulieren/;Statworx;  Daniel Lüttgau
  15. Mai 2017;Fehlerquellen in Fragebogen;"Die Forschungsfrage ist formuliert, die Thesen sind aufgestellt und die zu messenden Variablen sind klar definiert. Jetzt nur noch schnell den Fragebogen runterschreiben, bevor es endlich mit der Auswertung losgehen kann! Oder? In der Praxis steckt mehr dahinter.Fehlerquellen in FragebogenGanz so trivial wie man oft glaubt ist das Aufsetzen des Fragebogens leider nicht. Wer noch keine Erfahrung damit hat, sollte sich an dieser Stelle lieber etwas Zeit nehmen, um sich bewusst zu werden, welche Fehlerquellen dabei lauern und wie diese nach Möglichkeit umgangen werden können. Du wirst merken, diese Zeit ist gut investiert, denn Fehler in der Datenerhebung lassen sich später in der statistischen Analyse nur noch schwer oder schlimmstenfalls nicht mehr bereinigen.Grundsätzlich können drei Typen von Fehlerquellen unterschieden werden:Merkmale der Frage als FehlerquelleMerkmale des Befragten als FehlerquelleMerkmale der Antwort als FehlerquelleHier geht es vor allem um zwei Dinge: die Formulierung und die Reihenfolge der Fragen.Sogenannte Reihenfolgeeffekte können einen starken Einfluss auf die erhobenen Daten haben, wie das folgende Beispiel verdeutlichen soll. In einer US-amerikanischen Studie aus dem Jahre 1950 von Hyman und Sheatsley wurden die Teilnehmer nach ihrer Zustimmung zu folgenden Aussagen befragt:Frage (A): Freie Berichterstattung für US-Reporter in kommunistischen LändernFrage (B): Freie Berichterstattung für kommunistische Reporter in den USAZustimmung bei Reihenfolge A, B:(A): 89,8%, (B): 73,1%Zustimmung bei Reihenfolge B, A:(A): 65,6%, (B): 36,5%Das Urteil der Befragten kann also durch die Position im Fragebogen deutlich verzerrt werden. Der einfachste Weg dem entgegenzuwirken ist bei der Erstellung des Fragebogens eine Rotation der Fragen festzulegen. Damit ist die Position zufällig und unterscheidet sich von Teilnehmer zu Teilnehmer. Alternativ können Füllfragen verwendet werden und den direkten Bezug der Fragen entzerren.Fragen als FehlerquelleBei der Formulierung deiner Fragen gilt: Vermeide relativierende Füllwörter, wie „eigentlich“ oder „ganz gut“. Denn durch ihre abmildernde Wirkung auf die gemachte Aussage wird die Zustimmung zu selbiger künstlich gesteigert und damit verfälscht. Treffe stattdessen klare Formulierungen.Sei außerdem vorsichtig bei der Verwendung von Antonymen. Willst Du zum Beispiel herausfinden, wie groß die Zustimmung in der Bevölkerung für Paketdrohnen ist, könntest du sowohl fragen, ob diese erlaubt oder aber ob sie verboten werden sollen. Weil die Menschen generell zur Beibehaltung des Status Quo tendieren, würdest du vermutlich feststellen, dass mehr Leute ein Verbot ablehnen, als eine Erlaubnis zu befürworten. Versuche deshalb auch solche Formulierungen zu vermeiden. Außerdem: Stelle deine Fragen kurz, präzise und neutral. Vermeide Negationen. Verwende eine klare Sprache und vermeide unnötige Fachbegriffe.Befragte als FehlerquelleMessfehler entstehen regelmäßig auch dadurch, dass es dem Befragten schlicht an Wissen oder Erfahrungen fehlt, er trotzdem aber eine Antwort abgeben muss. Überlege deshalb genau wann du auf eine Antwort nicht verzichten und wann du eine Outside-Option einfügen kannst (z.B. „Ich weiß nicht“). Wenn du persönliche Angaben abfragst, wie das Alter oder den aktuellen Erwerbsstatus, lasse den Befragten immer die Möglichkeit nicht zu antworten.Zu einem weiteren Problem kann die häufig zu beobachtende Tendenz der Befragten führen, ihre Antworten der sozialen Erwünschtheit anzupassen. Besonders stark tritt dies in persönlichen Interview-Situationen auf, sollte aber auch in anonymen Fragebögen nicht vernachlässigt werden. Ein Hinweis zu Beginn der Studie, der Anonymität garantiert und um ehrliche Antworten bittet, sollte deshalb nicht fehlen. Zusätzlich können auch „Lügen-Skalen“ verwendet werden, welche unehrliche Teilnehmer aufdecken sollen (Beispiel: „Manchmal bin ich neidisch, wenn andere Glück haben.“).Antworten als FehlerquelleWelches Antwortformat gebe ich den Befragten vor? Ja/nein? Eine abstufende Skala von 1 bis 10? Von -3 bis 3? Es gibt unzählige Möglichkeiten und welche die richtige ist hängt ganz von deinen Fragen ab. Gemeinhin ist man der Auffassung, dass eine 7er-Skala bei Abstufungen oftmals die beste Lösung sei. Manchmal ist es jedoch sinnvoll die neutrale Mitte auszuschließen, um eine Tendenz zu erzwingen – dann wäre also eine 6er-Skala besser geeignet. Unter statistischen Gesichtspunkten kann man 5er, 6er oder 7er-Skalen als quasimetrisch ansehen und viele verschiedene Analysen damit durchführen. Bei 3er oder 2er-Skalen liegt ein nominales oder ordinales Skalenniveau vor, was die Auswertung manchmal erschweren kann.Natürlich solltest du deine Skalenendpunkte benennen. Sei dabei aber nicht zu extrem, um die Tendenz zur Mitte nicht zu verstärken. Eine Skala von „Absolut schrecklich!“ bis „Besser geht es nicht!“ wird aufgrund ihrer Breite ein wesentlich undifferenzierteres Ergebnis generieren als eine enger gesteckte Skala, die von den Befragten vollständig ausgeschöpft wird. Wenn du alle Skalenpunkte beziffern willst, überlege dir, ob die vorliegende Skala ein- oder zweidimensional ist und spiegele dies in den Skalenwerten wider. Für eine zweidimensionale 7er-Skala beispielsweise erweist es sich als sinnvoll, diese nicht von eins bis sieben durchzuzählen, sondern die neutrale Mitte mit einer Null zu versehen und in beide Richtungen hoch zu zählen.ZusammenfassungIn diesem Beitrag haben wir dir einen kurzen Überblick über mögliche Fehlerquellen in Fragebogen gegeben. Wie du sicher gemerkt hast, ist die Konstruktion von Fragebögen eine komplexe Aufgabe. Je besser dein Fragebogen konstruiert und erhoben ist, desto aussagekräftiger wird die statistische Auswertung. Falls du Hilfe bei der Konstruktion deines Fragebogens haben solltest, steht dir unser Statistik Team gerne zur Verfügung.LiteraturhinweiseStrack, F. (1994): Zur Psychologie der standardisierten Befragung, Berlin/Heidelberg/New YorkÜber den AutorJessica AustI am part of the STATWORX statistics team and I like the whole process of analysing data, from data prep to calculating models. Each step requires to follow certain rules, but this does not mean, that it’s not fun..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/fehlerquellen-in-fragebogen/;Statworx;  Jessica Aust
  15. Mai 2017;Korrelation und Kausalität;"Ein bekanntes Beispiel aus der Statistik: Je mehr Leute in Kentucky heiraten, desto mehr Menschen ertrinken nachdem sie aus einem Fischerboot gefallen sind. Mit einem Korrelationskoeffizienten von r = 0.952 ist dieser Zusammenhang, statistisch gesehen, fast perfekt. Aber sollte man deswegen in Kentucky lieber auf das Heiraten verzichten? Oder ist etwa der Pro-Kopf-Verbrauch von Käse verantwortlich für ein unglückliches Ableben durch Verheddern im Bettlaken? Immerhin kann auch hier eine starke Korrelation beobachtet werden (r = 0.947).Für beide Fälle lautet die Antwort wohl eher „nein“. Stattdessen sollen diese Beispiele klarmachen, dass eine Korrelation noch lange keine Kausalität nachweist. Wozu dienen also Korrelationsanalysen und worauf muss man bei deren Interpretation achten?Korrelative ZusammenhängeEine Analyse der Korrelation zweier Variablen ist immer dann interessant, wenn wir wissen wollen, ob ein statistischer Zusammenhang zwischen diesen Variablen besteht und in welche Richtung dieser verläuft. Dabei unterscheiden wir vier grundlegende Szenarien, die das folgende Beispiel verdeutlichen soll: „Gibt es einen Zusammenhang zwischen der Anzahl an wöchentlichen Arbeitsstunden und der Häufigkeit an Restaurantbesuchen einer Person?“Kein Zusammenhang: Durch Kenntnis der wöchentlichen Arbeitsstunden kann keinerlei Aussage über die Häufigkeit an Restaurantbesuchen gemacht werden.Positiver Zusammenhang: Je mehr eine Person pro Woche arbeitet, desto häufiger besucht sie ein Restaurant.Negativer Zusammenhang: Je mehr eine Person pro Woche arbeitet, desto seltener besucht sie ein Restaurant.Nichtlinearer Zusammenhang: Sowohl eine unter- als auch überdurchschnittliche Anzahl an wöchentlichen Arbeitsstunden erhöht die Häufigkeit von Restaurantbesuchen.Ob der beobachtete Zusammenhang auch eine kausale Verknüpfung hat, welche Variable Ursache und welche Wirkung ist – diese Fragen bleiben von der Korrelationsanalyse unbeantwortet. Nehmen wir an, wir würden für unser Beispiel einen positiven Zusammenhang beobachten. Dann könnte eine Erklärung lauten, dass Personen die länger arbeiten, weniger Zeit zum Kochen haben und deshalb häufiger auf Restaurants ausweichen. Alternativ denkbar wäre auch, dass Personen die gerne essen gehen mehr arbeiten müssen, um sich ihre häufigen Restaurantbesuche leisten zu können. Auch ein rein zufälliges Entstehen der Korrelation ist nicht auszuschließen, wie die zwei Eingangsbeispiele klarmachen sollten.Keine Kausalität in KorrelationWir wissen also nicht, ob ein kausaler, ursächlicher Zusammenhang vorliegt, was genau Ursache und was Wirkung ist. Trotzdem kann es natürlich wünschenswert sein durch (gründlich recherchierte) inhaltliche Interpretation aus einem korrelativen Zusammenhang eine Kausalität abzuleiten. Ganz wichtig ist es aber sich bewusst zu machen, dass diese Interpretationen, so schlüssig sie erscheinen mögen, nie von der Korrelation statistisch belegt sind.Kausalität nachweisenTatsächlich lässt sich ein kausaler Zusammenhang nie mit statistischen Methoden vollständig nachweisen (wobei es hier in der Statistik neue Stoßrichtungen gibt, z.B. zum Thema Causal Inference). Die beste Approximation erhalten wir durch ein kontrolliertes Experiment, d.h. durch Manipulation der unabhängigen Variable X (angenommen als Ursache, z.B. wöchentliche Arbeitsstunden) bei gleichzeitiger Beobachtung der abhängigen Variable Y (angenommen als Wirkung, z.B. Anzahl Restaurantbesuche). Verändert sich nun Y infolge der Manipulation von X, kann, zumindest statistisch von Zusammenhang der beiden Faktoren ausgegangen werden.KorrelationskoeffizientenZur Berechnung von Korrelationen stehen dem Wissenschaftler verschiedene Korrelationskoeffizienten zur Verfügung. Diese werden, je nach Skalenniveau der Daten und vermutetem Zusammenhang, ausgewählt. Die beiden wichtigsten Korrelationskoeffizienten sind der Pearson-Korrelationskoeffizient sowie der Spearman-Korrelationskoeffizient. Ersterer wird verwendet, wenn beide zu korrelierenden Variablen metrisch bzw. intervallskaliert und normalverteilt sind. Die Spearman Korrelation hingegen wird basierend auf Rangdaten berechnet und ist auch für ordinale und nicht-normalverteilte Daten geeignet. Beide Koeffizienten sind im Intervall zwischen r = -1 und r = 1 definiert, wobei r = -1 einen perfekten negativen und r = 1 einen perfekten positiven Zusammenhang beschreiben.Praktischer Einsatz von KorrelationenIn der statistischen Praxis werden Korrelationen häufig im Rahmen der explorativen Datenanalyse verwendet, d.h. als erste Indikation für etwaige statistische Effekte, die mit komplexeren Methoden, wie z.B. der Regressionsanalyse, weiter untersucht werden. Dies wird auch vor dem Hintergrund klarer, dass bei einfachen Korrelationsanalysen keine weiteren Variablen zur Kontrolle der Wirkung verwendet werden können. Man geht also davon aus, dass lediglich eine Wirkung von X auf Y vorliegt und keine anderen Faktoren Y beeinflussen. Dies ist für die meisten Experimente eine extrem unplausible Annahme.ZusammenfassungWichtig ist zu verstehen, dass mit statistischen Korrelationen keine Aussagen über kausale Wirkzusammenhänge getroffen werden können. Alle statistischen Modelle sind lediglich einfache Abstraktionen der Wirklichkeit und werden in den meisten Fällen nie den tatsächlichen Kausalzusammenhang zwischen Variablen abbilden können. Aber, um es mit den Worten des berühmten Statistikers George Box zu sagen: „All models are wrong…but some of them are useful.“. Falls du Unterstützung bei der Auswahl oder Berechnung von Korrelationen benötigst, hilft dir unser Statistik Team gerne weiter.Causal Inference: http://egap.org/methods-guides/10-things-you-need-know-about-causal-inferenceAll models are wrong: https://en.wikipedia.org/wiki/All_models_are_wrongÜber den AutorJakob GeppNumbers were always my passion and as a data scientist and a statistician at STATWORX I can fullfill my nerdy needs. Also I am responsable for our blog. So if you have any questions or suggestions, just send me an email!.e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/korrelation-und-kausalitaet/;Statworx;  Jakob Gepp
  4. September 2016;Titanic-Survival-Calculator mit R Shiny;"Mit R Shiny lassen sich in R schnell interaktive Web-Applikationen erstellen. Immer wieder gibt es umfangreiche Updates, die dem R User neue Funktionen zur Verfügung stellen, um die Funktionen seiner Dashboards zu erweitern. Grund genug für uns eine kleine App zu entwerfen: der Titanic-Survival-Calculator!Auf Basis der historischen Passagieraufzeichnungen der Titanic ist es möglich, statistisches Modell zu entwickeln, das basierend auf Alter, Geschlecht sowie Reise-Klasse, die Wahrscheinlichkeit berechnet das Titanic-Unglück überlebt zu haben.Testen Sie, ob auch Sie es geschafft hätten. Viel Spaß beim Experimentieren!Zum SimulatorÜber den AutorTobias KrabelI am data scientist at STATWORX, with a secret passion for data and software engineering. Tocompensate for my nerdy sitting-in-the-basement side, I spend even more time in the basement writing shiny applications..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/titanic-survival-calculator-mit-r-shiny/;Statworx;  Tobias Krabel
  23. Dezember 2020;2020 – A Year in Review for Me and GPT-3;"Recently, I got access to the beta program of GPT-3, by far one of the most fascinating pieces of technology I came across in the last years. GPT-3 is an AI system for NLU (Natural Language Understanding), developed by OpenAI. In particular, GPT-3 excels at tasks such as text generation, question answering, text summarization and many, many more. That is why I’ve decided to co-write my yearly review together with GPT-3. This was my first man-machine collaboration and the work together with the AI completely blew my mind. Yellow marked parts of this post have been written by GPT-3. Check it out!2020 is a year, that will make history. I mean, the type of history that gets printed in a schoolbook.  It’s a year that will be remembered for centuries to come. Books will be written; movies and documentaries will be made. But what will be remembered the most?  For STATWORX it was a year full of ups and downs. We started bold in January and February with great new ideas, clients and projects. In our management offsite in Berlin in January, we were plotting how successful 2020 will be. Inspired by the vibrant energy (and nightlife) of the city, our heads were full of ideas. Also, in January, we were traveling to the beautiful city of San Francisco to check out the latest developments in data science and machine learning at R Conf 2020. On our way back, a quick (mandatory) detour to Las Vegas. Things felt great, and energy levels were insane. 2020 was to be our year!Image 1 – Impressions from our trip to San Francisco. I don’t know what it is, but California and SFO in particular, have a special kind of vibe. It’s as if it’s home to all the creative people in the world. The people are cool, the weather is cool, the food is cool, the night life is cool, the whole atmosphere is cool. It’s a place that makes you want to live there. There must be something in the water.But, when COVID hit Germany late February and March, the situation immediately changed. The German economy shut down for most parts, including many of our clients. Lockdown time. Projects were stopped or postponed; sales stuck. People had to stay at home to stay safe. This changed everything. We had to re-think and re-strategize for the future.  We had to think of new ways to build our business; in other words, to be smart with our money. We had to go back to the drawing board to evaluate what to invest in and what to stop.Speaking of investing: At that time, our new 1500sqm head office in Frankfurt was already signed, sealed and delivered. Two floors, one specifically designed for face-to-face workshops, trainings and our legendary STATWORX parties. Of course, none of that really happened in 2020. Furthermore, we’ve opened our new branch office in Zurich this year, accommodating space for our growing Swiss team. We were moving in both locations, in the middle of the crisis.Image 2 – After nearly two years of planning and execution, the new STATWORX Headquarter in Frankfurt opens in June 2020. Simply an amazing piece of work!  With the possibility of a recession lingering over our heads, I had to ask myself if I had chosen the right line of work. The answer was simple: „Yes“.   Both, crisis and lockdown significantly pushed digital working and digitalization in general. Companies quickly had to adapt to large-scale remote working, digital processes and new work in general. Also, for STATWORX, where we embrace office culture, team spirit and a cold beer after work, the new situation posed many challenges.  More and more employees had to work remotely. The work of our clients and our own work was increasingly digital. We had to find solutions to avoid a digital silo effect and to ensure that our team members are not just working digitally but also maintain high-quality teamwork.Image 3 – During lockdown, our team stayed at home to stay safe. Working environments showed a high variance. ??Realizing, that the lockdown would have a severe impact on our clients and the worldwide economy in general, we swiftly had to identify opportunities for digital offerings. First, since in-person sales wasn’t an option, we’ve forged digital content such as our webinar series on different data science and AI related topics. Based in this series we’ve generated more than 500 new contacts in our CRM that our sales team could then reach out to.  The second digital offering came in the form of a new digital training courses that were immediately booked and rolled out by our existing customers. The course program covers the basics as well as business implications of machine learning and AI and will soon be also available as a webinar, an on-demand video and as an e-book.  We have also added more digital courses to our existing STATWORX Academy portfolio due to the high remote training demand in 2020, which turned out to be one of the great successes in 2020. Learning for the future doesn’t stop!Image 4 – During the ongoing crisis, we had to adapt in terms of digital content and offerings. Here, you see some impressions from one of our video productions. We’ve learned a lot!Around June, when the number of COVID cases was decreasing in Germany, it was time to restart the engine and boost marketing. We’ve invested in content creation, marketing and automation. This led to a series of around 10 whitepapers and 15 blog posts that were published in the second half of the year. Out of this, we were able boost our online exposure and generated new leads and project engagements. Also, finally, after a long and exhausting home office period, the team got together again. It felt great to see all of our team members being well and safe. This also made clear, how important social interaction is for our personal wellbeing and of course also for a team.Image 5 – After a long period of home office and remote work, the STATWORX team finally met again at our office opening event. That felt great and showed how important our team spirit is.Many of our AI consulting and development projects that were laid off in Q1 came back one after another which led to one of the most exciting end-of-year-rallies in the history of the company. We’ve accomplished a success for 2020, even though things looked sinister in the first half of the year. Our team stood together, with great courage and understanding for the situation.Looking back at 2020, I have to say that the crisis heavily pushed us out of our comfort zone and sparked many great initiatives and successes in our company. It was a challenging but rewarding year full of learnings. As a small Christmas gift from us, we have decided to give all our customers (ranging from startups to large enterprises) the chance to order a free AI Potential Workshop for your company! I wish you all a fantastic 2021!  Über den AutorSebastian HeinzI am the founder and CEO of STATWORX. I enjoy writing about machine learning and AI, especially about neural networks and deep learning. In my spare time, I love to cook, eat and drink as well as traveling the world..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/2020-a-year-in-review-for-me-and-gpt-3/;Statworx;  Sebastian Heinz
  15. Dezember 2020;Praktische Deep Learning Anwendung – Mit künstlicher Intelligenz Plätzchen-Rezepte und Weihnachtslieder generieren;"Der Anfang der Adventszeit brachte dieses Jahr nicht nur den ersten Schnee in der Rhein-Main Region (Abb. 1), sondern leitete auch bei vielen nach einem Jahr voller turbulenter Ereignisse und endlosen Stunden im Homeoffice die alljährliche Zeit der Besinnlichkeit ein. Obgleich sich die angesprochene Besinnung bei vielen eher euphemistisch äußert, in Form von Geschenk- und Planungsstress oder der alljährlichen Jahresendrallye bei STATWORX, dürfen gerne auch Erinnerungen an gutes Essen, kitschige Musik und gemütliche Abende heraufbeschworen werden.Abb. 1: Für einen kurzen Moment scheint der Weltenbrand durch unsere Lieblingsniederschlagsform erloschen, und die Welt wirkt restlos in unschuldiges und reines Weiß getaucht.Anm. der Redaktion: Der Autor hatte eine schwierige Woche. Nichtsdestotrotz gestehen wir ein, dass seine Worte ein positives Stimmungsbild inspirieren könnten.Mit dem Advent bricht bekanntlich auch die Zeit der Weihnachtsplätzchen an. Eine exzellente Gelegenheit, bei meinen STATWORX-Kolleg*innen unter dem Vorwand der Nächstenliebe Sympathie und professionelles Ansehen durch unschuldig wirkende, zuckerhaltige Kalorienbomben inklusive Deep-Learning-Kniff zu erlangen. In Ergänzung zu meinem vorangegangenen Artikel zum Thema Kochen mit Deep Learning, möchte ich in diesem Beitrag eine kleine Web-App vorstellen, mit der sich alternative Zutaten für Rezepte vorschlagen lassen, untermalt von KI-generierten Weihnachtsliedern. Ziel dieser Artikel (und anderer, wie zum Beispiel unsere Super Mario spielende KI) ist es, die Grundlagen hinter den oftmals etwas mystisch wirkenden Deep Learning Methoden anschaulich zu vermitteln. In vielen Gesprächen entstand bei uns der Eindruck, dass durch solch einen Überblick spannende Anwendungs- und Geschäftsfälle wesentlich besser erschlossen werden können, indem man sich zunächst bewusst macht, welche grundsätzlichen Anwendungen mittels Deep Learning erschlossen werden können.Alternative Zutaten für Rezepte finden mittels Word EmbeddingsSchon länger dominieren Anwendungen und Publikationen zum Thema Natural Language Processing die Deep Learning Forschung. Regelmäßig ermöglichen bahnbrechende Ergebnisse neue Anwendungsfälle. Eines der Schlagworte war hier zunächst Word Embeddings mittels Word2Vec und nun brandaktuell Transformer-Modelle und ihr Attention-Mechanismus, wie beispielsweise BERT oder GPT-3.Zur genaueren Veranschaulichung dieser Methode werden im Folgenden zunächst die Grundlagen von Word Embeddings erläutert. Anschließend wird aufgezeigt, wie diese Methode genutzt wurde, um Zutaten in Rezepten durch andere, möglichst ähnlich-verwendete Lebensmittel auszutauschen. Dabei werden nicht einfach „blind“ Vorschläge gemacht sondern aus ähnlichen, kontextuell verwandten Rezepten Vorschläge erkannt und abgeleitet. Somit kann (hoffentlich) sichergestellt werden, dass das alternative Rezept zur kulinarischen Freude wird und nicht unmittelbar im Mülleimer landet.Grundlagen von Word EmbeddingsWord2Vec ist ein ausgezeichnetes Beispiel, um eine der zentralen Konzepte des Deep Learnings zu veranschaulichen: das Representation Learning. Sehr vereinfacht dem Gehirn nachempfunden, durchlaufen Daten auch in künstlichen neuronalen Netzen mehrere Schichten. Jede Schicht verändert und kombiniert die Daten vorhergehender Schichten dahingehend, dass eine dichtere und abstraktere Repräsentation der Daten entsteht. Diese Ansammlungen an Zahlen, die aus menschlicher Sicht keine intuitive Bedeutung mehr haben, enthalten nicht mehr alle Informationen der Ursprungsdaten sondern nur noch jene, die für das Modell zur Lösung der jeweiligen Aufgabe als relevant eingestuft werden. Beim Training der Modelle wird die Repräsentationen also dahingehend optimiert, die Fehler der Modell-Vorhersage zu minimieren. Möchte man also Word Embeddings nutzen, um Worte hinsichtlich ihrer Bedeutung zu vergleichen oder Ähnlichkeiten zu erkennen, wird ein Deep Learning Modell zum allgemeinen Sprachverständnis trainiert. Dies beinhaltet meistens Aufgaben, wie die Vorhersage benachbarter Worte oder des nächsten Satzes. Um diese Aufgaben lösen zu können, muss das Modell also die grundsätzliche Verwendung von Worten, insb. im Kontext von anderen Worten erlernen. Da man im Falle unserer Word Embeddings aber nur an den internen Repräsentierung der einzelnen Worte interessiert ist, ist dieses Vorgehen (Abb. 2) eigentlich nur Mittel zum Zweck.Abb.2: Word2Vec Verfahren zur Vorhersage von benachbarten WortenBeim Word2Vec Verfahren wird im Training versucht, ausgehend vom mittleren Wort die links und rechts benachbarten Worte vorherzusagen. Wichtig ist, dass die Repräsentierung des mittleren Wortes durch das Umfeld bzw. den Kontext geprägt wird. Treten Worte in ähnlichen Kontexten auf, ähnelt sich ihre numerische Repräsentierung. Daraus erwarten wir beispielsweise, dass die Embeddings von Feldsalat und Radicchio sich wesentlich näher sind (häufiger Kontext: Öl, Zwiebeln, Essig, Schüssel…), als die von Feldsalat und Dosenananas (seltener Kontext: trauriger verkaterter Sonntag im Studium).Anwendung auf ein konkretes Beispiel: Pätzchen-Rezepte mit Bake2VecWie auch im vorangegangenen Artikel zu diesem Beitrag wurden die Embeddings auf einer großen Ansammlung an Texten, genannt Corpus, trainiert. In diesem Fall ca. eine Million Rezepte aus Büchern und vor allem bekannten (englischen) Rezept-Webseiten, wie von der BBC oder Epicurious.Hierbei prägen zwei verschiedene Kontexte die Embeddings:Die Auflistung aller Zutaten eines GerichtsDie Fließtext-Anleitung zur Zubereitung, also die angeleiteten Schritte im Kochrezept selbstNach dem Training entsteht eine Datenbank an Wort- und Embedding-Paaren, die mittels bestimmter Metriken verglichen werden können. Für gewöhnlich wird die Kosinus-Ähnlichkeit zwischen den Embeddings berechnet, womit man einen normierten Wert zwischen 0 und 1 enthält, der sozusagen die Güte einer Zutat als Ersatz einer anderen abbildet. (Abb. 3)Abb. 3: Screenshot aus der Web-App zur Recherche von AlternativzutatenFür jede Zutat lassen sich Alternativen anzeigen. Beurteilen Sie die Vorschläge im Vergleich zu Ihren eigenen Ideen! Auch können Sie die Zutaten im Textfeld ändern/überschreiben, und sich Vorschläge für diese anzeigen lassen. Hier gehts zur Web-App.Mit Generativen Modellen Weihnachtslieder generierenGenerative Modelle beweisen schon länger, zu welchen eindrucksvollen Resultaten Deep Learning abseits klassischer Machine Learning Anwendungen, wie Klassifikation und Regression, fähig sind. Anwendungsfälle wie das Generieren von fehlenden Datenpunkten oder das Ausfüllen von verdeckten Flächen in Bildern finden in der Geschäftswelt Anwendung. Künstler*innen und Enthusiasten erstellen dagegen Kunstwerke, Gedichte oder Fotos von nicht-existierenden Gesichtern. Vor 2 Jahren erzielte sogar das erste KI-generierte Porträtgemälde über $400.000 bei einer Auktion.Neben der bildenden Kunst haben generative Modelle auch in der Musikszene ersten Anklang gefunden. Im folgenden Abschnitt betrachten wir die Generierung von Weihnachtsliedern am Beispiel einfacher Jingles und Ausschnitte wie „Oh Tannenbaum“ oder „Alle Jahre wieder“. Damit schaffen wir zum Wohle der Lesenden und Hörenden einen Mariah Carey- und Michael Bublé-freien Safe Space, dem aber leider auch die George Michaels, Crosbys und Sinatras zum Opfer fallen.Tensorflow MagentaDas Magenta Projekt möchte „die Rolle von Machine Learning in kreativen Prozessen“ erforschen und bietet dabei eine Vielzahl an Veröffentlichungen, Programmier-Bibliotheken und Tools zur Generierung von Musik. Die Dance-Punk Band YACHT arbeitete daraufhin mit Google zusammen und erstellte einen Katalog aus den MIDI Tonspuren ihrer bisherigen Lieder, die dann einzeln oder gepaart (z.B. Gitarren- und Bass-Riff zusammen) verwendet wurden, um neue Musik im Stil des bisherigen Songs der Band zu generieren. Ähnlich wurde auch mit den Liedtexten verfahren. Selbst das Album Cover und die Videos im Hintergrund der Konzerte wurden mit Deep Learning Modellen generiert und verfeinert.Die im nächsten Abschnitt beschriebenen Rekurrenten Neuronale Netze bieten eine etablierte Methode im Umgang mit einspurigen Riffs oder Jingles. Verschiedene Ausführungen und Varianten davon sind im „Magenta-Werkzeugkasten“ als MelodyRNN enthalten.Rekurrente ModelleVor der Einführung von Transformer-Modellen wurden im Bereich NLP größtenteils Rekurrente Neuronale Netze (RNNs) verwendet, die von Natur aus besonders geeignet sind, sequenziellen Daten zu verarbeiten. Dies bedeutet, dass Rekurrente Netze es ermöglichen, Entscheidungen oder Vorhersagen bezüglich aufeinander folgender, abhängiger Datenpunkte zu treffen. Hierzu zählen bspw. aufeinanderfolgende Worte in einem Satz, Werte aus Sensoren im Zeitverlauf, oder eben Noten einer Partitur, die im Zeitverlauf betrachtet eine Melodie ergeben. Am Beispiel einer Wortsequenz ist es also wichtig, dass ein Machine Learning Modell einordnen kann, welche Worte in welcher Abfolge im Text erscheinen.Um diese Informationen aus den Daten ableiten zu können, pflegen RNNs einen internen Zustand, der zusätzlich zur oben beschriebenen Repräsentation nicht nur vom aktuell betrachteten Wort, sondern auch vom internen Zustand des RNNs bei vorherigen Worten abhängt. (Abb. 4)Abb. 4: RNN BeispielDas RNN pflegt einen internen Zustand vorangegangener Inputs. Der neue Zustand ergibt sich aus einer Gewichtung des alten Zustands und des aktuellen Inputs.Als Output wird in diesem Fall versucht, das folgende Wort vorherzusagen. Das RNN sollte erkennen, dass im Kontext des Wortes Weihnachten eher das Wort „backen“ vorhergesagt werden muss, und nicht etwa „kaufen“.Um ein generatives Modell zu entwickeln, trainiert man das RNN darauf, für eine gegebene Sequenz an Worten stets das nächste, passende Wort vorherzusagen. Wendet man das Modell anschließend an, präsentiert man eben eine solche Sequenz an Worten als vorgegebenen Input, nimmt den vorhergesagten Output des Modells hinzu, füttert diesen als folgenden Input wieder in das Modell und wiederholt diesen Vorgang beliebig oft. So macht das Modell Vorhersagen auf dessen eigene Vorhersagen und generiert somit einen Text.Musik generierenDa es sich bei Noten von Volks- und Weihnachtsliedern auch um eine sequenzielle Abfolge von Daten handelt, lässt sich die zuvor beschriebene Methodik anschaulich auf musikalische Daten übertragen.Möchte man ein Lied in einem bestimmten Stil oder Genre eines anderen Liedes weiterführen oder generieren, kann man im einfachsten Falle ein von Magenta publiziertes (und vortrainiertes) MelodyRNN verwenden. Dort gibt man die gewünschte Anfangsmelodie als MIDI Datei vor und definiert ggf. noch weitere Parameter wie Länge oder Freiheitsgrade in der Generierung. Anschließend wird das Lied, analog zum Text-Beispiel, Ton für Ton synthetisiert.Abb.5: Kein Vivaldi, dafür aber eine Flasche Primitivo; der musikalisch gänzlich untalentierte Autor schafft es eines Nachts, Noten in einspurige MIDI Dateien ohne Taktwechsel zu exportieren, um sie mit Magenta verarbeiten zu können.Für den interessierten Lesenden gibt es an dieser Stelle noch einiges zu entdecken! Um den Bogen zum Thema Representation Learning zu schlagen, empfehlen wir einen Blick auf MusicVAE https://magenta.tensorflow.org/music-vae. Mit einem Variational Autoencoder lassen sich Embeddings (merke: nicht nur auf Worte beschränkt, sondern auch ganze Texte, Bilder, Lieder,…) erlernen, welche einfach und strukturiert manipulierbar sind. Somit lässt sich beispielsweise eine Komponente eines Embeddings ändern, die das Musikgenre darstellt, aber nicht die Melodie. (Re)konstruiert man anschließend das Embedding, erhält man ein Lied mit der gleichen Melodie, jedoch im Stil einer anderen Musikrichtung.Beispiele für State-of-the-Art Musikgenerierung finden sich auch in der OpenAI Jukebox, welche sich auch eines VAE-Modells bedient.FazitBei den in diesem Artikel beschriebenen Methoden muss natürlich meist etwas Auslese oder Feinabstimmung betrieben werden, um die gewünschten Ergebnissen zu erzielen. Im Falle der kreativen Anwendung, wie bei der Band YACHT, wurden tausende, wenn nicht zehntausend Melodien generiert und durchsucht, um zufriedenstellende Resultate zu erhalten. Bei den Plätzchenzutaten hingegen ging es eher um die Exploration von Modell-Varianten und -Parametern, welche dann zu beständig guten Ergebnissen führen.Der Autor hofft, dass Ihnen der kleine Einblick in Methoden des Natural Language Processing gefallen hat und möchte Sie animieren, die Plätzchen Web-App auszuprobieren, oder diese zu missbrauchen, um sich Alternativen zu deftigen Zutaten oder Gemüse auszugeben.Bleiben Sie so artig, wie nötig! Frohe Weihnachten! J.Anmerkungen &amp; QuellenA) Hallo und danke Mama!Abb. 6: Eine unvollständige Auswahl der Plätzchen der Familie des Autors. Während mütterlicherseits jährlich über 20 Sorten gebacken werden, schafft es die Tante nur auf mickrige acht. Die Begeisterung und das Interesse am Kochen und Backen ist im Kindesalter bereits auf den Autor übergesprungen. Im Gegenzug jedoch wundert sich dessen Familie des Öfteren, „was dieses Deep Learning denn genau ist“ und was der Junge auf der Arbeit eigentlich macht. Anwendungsbeispiele wie diese Web-App sind dabei wunderbare Gelegenheiten, Methoden des Deep Learning anschaulich zu erklären, Alltagsbezug zu schaffen und sich für die vielen kommenden fachlichen Herausforderungen zu motivieren!B) Die Rezepte dieser kleinen Exkursion wurden Chefkoch.de entnommen, zu den Originalen gelangen Sie über die folgenden Links:Vanillekipferl ZimtsterneSpitzbubenÜber den AutorJonas BraunBesides being a data scientist interested especially in deep learning, I enjoy the outdoors, anything culinary and also the arts. Bridging deeply rooted 'real world' elements with tech is what drives me..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/praktische-deep-learning-anwendung-mit-kunstlicher-intelligenz-platzchen-rezepte-und-weihnachtslieder-generieren/;Statworx;  Jonas Braun
  8. Dezember 2020;Whitepaper: Wie sich Scrum für Data Science Projekte anwenden lässt;"Management SummaryWarum Scrum?In den letzten Jahren haben sich Data Science Projekte einem starken Wandel unterzogen. Von der Entwicklung einfacher Proof of Concepts (PoC) verschiebt sich der Schwerpunkt immer stärker in Richtung vollumfänglicher Softwareapplikationen mit dem Fokus auf Operationalisierung und Industrialisierung der Lösung. Vor allem das Thema „agiles Arbeiten“ rückt im Projektkontext von Data Science und KI immer stärker in den Vordergrund. Hierbei ist insbesondere ein Projektmanagement nach der Scrum-Methode vielerorts zur Best Practice geworden.Um Scrum erfolgreich auf Data Science Projekten anzuwenden, sollten neben der Umsetzung der zentralen Konzepte von Scrum – regelmäßige Kommunikation, Aufgaben in kleinen Paketen erledigen, das Gesamtziel nicht aus den Augen verlieren, ständiges Anpassen und Verbessern der Arbeitssituation – einige Anpassungen der Abläufe vorgenommen werden.4 Herausforderungen bei der Anwendung von Scrum für Data Science ProjekteDiese Anpassungen beziehen sich allerdings nicht nur auf die Scrum Prozesse, sondern auch auf die Arbeitsweise eines Data Scientists. Im diesem Beitrag werden dabei folgende Bereiche näher betrachtet und Handlungsempfehlungen gegeben:Zusammensetzung des TeamsEin Entwicklerteam, in dem jeder Einzelne fähig ist, den Gesamtprozess zu verstehen und zu bearbeiten, ist in der Lage, Aufgaben schneller und effizienter zu erledigen.Festlegung des SprintzielsUm der in den Daten steckenden Ungewissheit gerecht zu werden, sollten Sprintziele auch Erkenntnisse berücksichtigen, die keinen Modellfortschritt liefern, sondern Wissen aufbauen.Angepasste AkzeptanzTrotz des explorativen und iterativen Charakters von Data Science, sollte eine Spirale der ewigen Optimierung vermieden werden.Güte des CodesDie Entwicklung neuer Features sollte immer auf Basis von Code stattfinden, der mit wenig Aufwand in Produktion gebracht werden kann.Wenn Scrum neu auf einem Projekt eingeführt wird, muss bedacht werden, dass es nicht sofort zu einer Verbesserung des Projektfortschritts bzw. der Qualität des Endprodukts kommt. Die Umstellungen, die Scrum in den Projektalltag bringt, werden Zeit brauchen, um ihre Wirkung zu entfalten. Wie können diese Probleme behoben und unterschiedlichen Ansichten angeglichen werden? Kann Scrum ein richtiger Ansatz für Data Science Projekte sein? Wenn ja, müssen die Prozesse angepasst werden oder muss sich die Einstellung der Data Scientists ändern? Mit dem richtigen Vorgehen bei der Einführung von Scrum kann eine Umgebung geschaffen werden, die zum Erfolg des Projektes beitragen wird. Im Whitepaper „Wie sich Scrum für Data Science Projekte anwenden lässt“ erhalten Sie Lösungsansätze und Best Practices zu diesen und weiteren Fragen zum Thema Scrum für Data Science Projekte.Laden Sie sich das komplette Whitepaper jetzt kostenfrei herunter!Sie haben Interesse am Thema Data Strategy?Hier können Sie sich das gesamte Whitepaper „Wie sich Scrum für Data Science Projekte anwenden lässt“ kostenfrei herunterladen.Über den AutorJakob GeppNumbers were always my passion and as a data scientist and a statistician at STATWORX I can fullfill my nerdy needs. Also I am responsable for our blog. So if you have any questions or suggestions, just send me an email!.e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/whitepaper-wie-sich-scrum-fur-data-science-projekte-anwenden-lasst/;Statworx;  Jakob Gepp
  26. November 2020;Case Study: Automatisierte Marketing-Interaktion im Retail;"Management SummaryIm Kundenlebenszyklus gilt es, den Kunden zum richtigen Zeitpunkt mit dem richtigen Angebot zu kontaktieren. Dabei sollen aktive Kunden zu einem weiteren Einkauf motiviert oder inaktive Kunden reaktiviert werden. Der Retailer kann seinen Kunden hierzu vielfältige Angebot machen, bspw. durch individuelle Produktempfehlungen, kundenspezifische Rabattaktionen oder ereignisbasierte Marketingaktionen. Häufig ist das Ziel dabei, möglichst die Kunden auszuwählen, mit denen Umsatz bei möglichst geringen Kosten generiert werden kann.Zur Lösung dieser Herausforderung haben wir bei STATWORX hierzu für einen Kunden aus dem Einzelhandel einen ganzheitlichen Ansatz entwickelt. Auf Basis von Kundenstamm- und Transaktionsdaten haben wir verschiedene state-of-the-art Methoden des Machine Learnings und der künstlichen Intelligenz genutzt, um Kundengruppen individuell und automatisiert auf verschiedenen Kanälen ansprechen zu können. Unser Kunde hatte zwei zentrale Herausforderungen im Direktmarketing identifiziert, die durch die bisher verwendeten Methoden nicht zufriedenstellend gelöst werden konnten:Customer Churn &amp; Retention: Wann und wie sollten inaktive Kunden gezielt angesprochen werden, um eine Abwanderung möglichst effizient zu verhindern? (Outbound Marketing)Next Basket Prediction: Welche Produkte sollten aktiven Kunden empfohlen werden, die sie zu einem Folgekauf anregen können? (Inbound Marketing)Für die Kundenreaktivierung wird zunächst die Kaufwahrscheinlichkeit aller Kunden für einen definierten Zeitraum in der Zukunft ermittelt. Diese Prognose dient dazu, Kunden, die eine sehr hohe Wahrscheinlichkeit für einen Kauf haben, für das weitere Prozedere auszuschließen. Denn diese Kunden müssen ohnehin nicht im Rahmen einer Kampagne reaktiviert werden. Genauso können Kunden ausgeschlossen werden, die eine sehr niedrige Kaufwahrscheinlichkeit aufweisen, da eine Reaktivierung dieser Kunden grundsätzlich wenig erfolgversprechend ist.Im Anschluss an diesen ersten Schritt wird ein zweites Machine Learning Modell angewendet, das für jeden Kunden individuell aus einer im Vorhinein definierten Auswahl an Rabattgutscheinen den jeweils optimalen Gutschein selektiert.Die Newsletter, inklusive der entsprechenden Rabattgutscheine, werden automatisiert an die Kunden versendet. Daraufhin wird die Aktivität der Kunden im Aktionszeitraum beobachtet und als Trainingsmenge in das zweite Modell eingespeist sowie die Kampagne anhand der Aktivierungs- und Reaktionsquoten evaluiert.Zur Empfehlung weiterer Produkte an aktive Kunden wird ein state-of-the-art Modell aus der aktuellen Forschung genutzt. Das Modell nutzt rekurrente neuronale Netze (RNN), um die gesamte Kaufhistorie des Kunden berücksichtigen zu können. Es erlernt nicht nur die dynamische Repräsentation der Kunden, sondern erfasst auch globale sequentielle Merkmale über alle Warenkörbe hinweg. Basierend auf diesem Modell können Newsletter mit kundenindividuellen Produktempfehlungen versendet werden.Mithilfe dieser beiden Ansätze können die Retail-Kunden in allen Phasen des Kundenlebenszyklus‘ optimal angesprochen, der manuelle Aufwand bei der Kundenauswahl und Versendung des Contents signifikant reduziert und der zur Aktion zugeordnete Umsatz gesteigert werden.InhaltsverzeichnisMotivationChallengeSolutionData Warehouse als BasisCustomer Churn &amp; Retention ModellNext Basket Prediction ModellAutomatisierter VersandEvaluationImpactFazit &amp; AusblickQuellenMotivationFür jedes Retail-Unternehmen ist es ein wichtiges Ziel, Kosten zu reduzieren und Umsätze zu erhöhen, um schlussendlich den Gewinn zu maximieren. Dies fängt beim Einkauf an, geht weiter über eine margenoptimierte Preissetzung und endet mit einer gezielten Kundenansprache.Im Kundenlebenszyklus gilt es, den Kunden zum richtigen Zeitpunkt mit dem richtigen Angebot zu kontaktieren. Dabei sollen aktive Kunden zu einem weiteren Einkauf motiviert oder inaktive Kunden reaktiviert bzw. deren Abwanderung zu einem Konkurrenten verhindert werden. Der Retailer kann seinen Kunden hierzu vielfältige Angebot machen, bspw. durch individuelle Produktempfehlungen, kundenspezifische Rabattaktionen oder ereignisbasierte Newsletter.Ziel ist dabei, möglichst solche Kunden auszuwählen, mit denen Umsatz bei möglichst geringen Kosten generiert werden kann. Zu den Kosten zählen dabei nicht nur reine Werbekosten, sondern auch indirekte Kosten, die z.B. dann entstehen, wenn der Retailer aktive Kunden mit einem Rabattgutschein anspricht. Dies führt zu keiner Umsatzsteigerung, da diese Kunden auch ohne Rabattgutschein eingekauft hätten. Weiterhin entstehen bei dem Retailer Kosten, wenn Personen für Werbeaktionen selektiert werden, deren tatsächliche Kaufwahrscheinlichkeit gegen Null tendiert.„I know that half of marketing is wasted – my problem is that I just don’t know which half.”John WanamakerZur Lösung dieser Herausforderung benötigt der Retailer somit einen Ansatz, um die richtigen Kunden zum richtigen Zeitpunkt mit dem richtigen Content anzusprechen. STATWORX hat hierzu für einen Kunden aus dem Einzelhandel einen ganzheitlichen Ansatz entwickelt. Auf Basis von Kundenstamm- und Transaktionsdaten haben wir verschiedene state-of-the-art Methoden des Machine Learnings und der künstlichen Intelligenz nutzt, um Kundengruppen individuell und automatisiert auf verschiedenen Kanälen ansprechen zu können. Dadurch konnte das Unternehmen seinen manuellen Aufwand bei der Kundenauswahl und Versendung des Contents signifikant reduzieren und gleichzeitig den der Aktion zugeordneten Umsatz steigern.ChallengeAufgrund bisheriger Erfahrungen im Direktmarketing hatte unser Kunde zwei zentrale Herausforderungen identifiziert, die durch die bisherigen verwendeten Methoden nicht zufriedenstellend gelöst werden konnten:Customer Churn &amp; Retention: Wann und wie sollten inaktive Kunden gezielt angesprochen werden, um eine Abwanderung möglichst effizient zu verhindern? Genauer gesagt, welche Kunden müssen zu einem gegebenen Zeitpunkt kontaktiert und welche Anreize sollte man ihnen für den nächsten Einkauf bieten? (Outbound Marketing)Next Basket Prediction: Welche Produkte sollten aktiven Kunden empfohlen werden, die sie zu einem Folgekauf anregen können? (Inbound Marketing)Um diese Fragestellungen modellgetrieben beantworten zu können, bedarf es einer umfangreichen Datenbasis. Für diese müssen alle relevanten Informationen aus den vorliegenden Datenquellen extrahiert, miteinander verknüpft und in geeigneter Form aggregiert werden. So soll eine umfassende zentrale Datenbank auf Kundenebene entstehen, die für die oben genannten Fragestellungen sowie auch weitere Problemstellungen verwendet werden kann. Zu dieser Datenbasis gehören in diesem Fall die Artikel- und Kundenstammdaten, historischen Transaktionsdaten, Kundenaktionsdaten, Standortdaten sowie Informationen aus externen Datenquellen.Für unseren Kunden war es außerdem von besonderer Relevanz, die sich aus den Fragestellungen ergebenden Schritte möglichst automatisiert ablaufen zu lassen und dementsprechend in die eigene IT-Infrastruktur zu integrieren. Somit müssen alle Schritte von der Datenextraktion &amp; -aufbereitung hin zum Versand der individuellen Newsletter regelmäßig automatisiert ablaufen, bzw. ereignisbasiert angestoßen werden.Zusätzlich sollte auch die Wartbarkeit und eine manuelle Nutzung der Data Pipeline und der Modelle durch die interne Data Science Abteilung gewährleistet sein. Insbesondere das auf Kundenebene zu aggregierende Data Warehouse soll der Abteilung, über die beiden Problemstellungen hinaus, als Datengrundlage für Ad-hoc-Analysen oder für weitere eigene Modelle und Analysen dienen.SolutionDie eingangs beschriebenen Fragestellungen unterscheiden sich vor allem in der Art ihrer Komplexität. Bei der Kundenreaktivierung liegt die Herausforderung vor allem in der Entwicklung der Data Pipeline und der Datenaufbereitung. Beim Produkt-Recommender stellt hingegen die Entwicklung der Methodik die größte Herausforderung dar.Im Bereich der Kundenreaktivierung wird der auf Kundenebene aggregierte Datensatz zunächst dazu verwendet, die Kaufwahrscheinlichkeit aller Kunden für einen definierten Zeitraum in der Zukunft zu ermitteln. Diese Prognose dient dazu, Kunden, die eine sehr hohe Wahrscheinlichkeit für einen Kauf haben, für das weitere Prozedere auszuschließen. Der Grund hierfür ist, dass diese Kunden ohnehin nicht im Rahmen einer Kampagne reaktiviert werden müssen. Genauso können Kunden ausgeschlossen werden, die eine sehr niedrige Kaufwahrscheinlichkeit aufweisen, da eine Reaktivierung dieser Kunden grundsätzlich wenig erfolgsversprechend ist.Im Anschluss an diesen ersten Schritt wird ein zweites Machine Learning Modell angewendet, das für jeden Kunden individuell aus einer im Vorhinein definierten Auswahl an Rabattgutscheinen den jeweils optimalen Gutschein selektiert. Ferner kann die Menge der zu reaktivierenden Kunden anhand verschiedener Kennzahlen eingeschränkt werden. Die zu reaktivierenden Kunden werden nach Vertriebslinie und Versandart unterteilt und die Mailings bzw. Newsletter inklusive der entsprechenden Rabattgutscheine automatisiert an die Kunden versendet. Daraufhin wird die Aktivität der Kunden im Aktionszeitraum beobachtet und als Trainingsmenge in das zweite Modell eingespeist. Außerdem wird die Kampagne anhand der Aktivierungs- und Reaktionsquoten evaluiert.Zur Empfehlung weiterer Produkte an aktive Kunden wird ein state-of-the-art Modell aus der aktuellen Forschung genutzt. Das DREAM Modell 2 nutzt rekurrente neuronale Netze (RNN), um die gesamte Kaufhistorie des Kunden berücksichtigen zu können. DREAM erlernt nicht nur die dynamische Repräsentation der Kunden, sondern erfasst auch globale sequentielle Merkmale über alle Warenkörbe hinweg. Diese Darstellung kann die Interessen der Kunden dynamisch, zu verschiedenen Zeitpunkten repräsentieren und mit den globalen sequentiellen Merkmalen aller Warenkörbe des Benutzers im Laufe der Zeit in Verbindung setzen. Hierdurch kann ein deutlich realistischeres Modell zur Produktempfehlung angewendet werden, was sich auch in signifikant besseren Trefferquoten zwischen den vorhergesagten und tatsächlichen Warenkörben widerspiegelt.Data Warehouse als BasisDas Data Warehouse bildet die Datenbasis für alle verwendeten Modelle. Es enthält alle Datenpunkte, mit denen die Kaufwahrscheinlichkeiten prognostiziert, Produktempfehlungen erzeugt und verschiedenste Analysen und Visualisierungen erstellt werden können. Im Rahmen der Integration der verschiedenen Datenquellen werden zunächst alle gesperrten und gelöschten Kunden sowie Personen, die der Direktwerbung nicht zugestimmt haben, aus dem Kundenstamm entfernt.Der Kundenstamm wird durch Kundenkartendaten, die Kundenadressen und die geographischen Informationen der Postleitzahlen angereichert. Darüber hinaus werden Postrückläufer ohne E-Mail-Adresse und/oder ohne E-Mail Opt-in aus dem Datensatz entfernt.Abschließend werden die Filialinformationen der Stammfiliale der Kunden angefügt. Die Filialinformationen bestehen aus geographischen Informationen, Daten zu den Filialflächen und externen Konsumdaten. Ergänzt werden diese Daten durch die in diesem Projekt berechneten Distanzen zum nächstgelegenen Konkurrenten. Neben den Stammdaten der Kunden werden die Transaktionsdaten zusammengefasst. Jeder verkaufte Artikel wird durch weitere Informationen aus dem Artikelstamm ergänzt und auf Bon-Ebene aggregiert.Diesem Datensatz können nun die ebenfalls auf Bon-Ebene vorliegenden Informationen aus dem Kundenbonusprogramm hinzugefügt werden. Der Datensatz wird daraufhin auf Kundenebene aggregiert. Von besonderer Bedeutung ist dabei, dass bei den beiden Aggregationsschritten die Kaufhistorie bis auf Artikelebene für die Produktempfehlung erhalten bleibt. Dazu wird eine Spalte erstellt, in der in einer verschachtelten Liste alle Warenkörbe und die darin enthaltenen Artikel eines jeden Kunden aufgelistet sind.Ergänzend wird die Kauffrequenz pro Kunde berechnet, repräsentiert als die durchschnittliche Anzahl an Tagen zwischen den einzelnen Einkäufen.Customer Churn &amp; Retention ModellBasierend auf dem historischen Kaufverhalten wird ein XGBoost-Modell 1 trainiert, um die Wahrscheinlichkeit der Kunden mindestens einen Kauf in den nächsten 3 Monaten zu tätigen, vorhersagen zu können. Das trainierte Modell wird auf alle im Data Warehouse enthaltenen Kunden angewendet. Anschließend können bspw. die Top 5% der Kunden mit den höchsten Kaufwahrscheinlichkeiten aus dem Datensatz ausgeschlossen werden. So wird vermieden, dass ohnehin aktive Kunden, die keine Reaktivierung benötigen, angeschrieben werden und einen Rabattgutschein erhalten. Ebenso werden auch alle Kunden mit einer sehr niedrigen Kaufwahrscheinlichkeit herausgefiltert. Darüber hinaus werden alle Kunden ausgeschlossen, die schon in der vorherigen Mailing Aktion kontaktiert wurden.Abbildung 1: Prozessdarstellung des Customer Churn &amp; Retention ProgrammesBasierend auf der Aktivierung der Kunden, die im Aktionszeitraum der vergangenen Kampagnen kontaktierten wurden, wird ein weiteres XGBoost-Modell trainiert. Dieses Modell sagt die Wahrscheinlichkeit mindestens einen Kauf zu tätigen für verschiedene Rabattgutscheine voraus, für jeden von Modell 1 nicht ausgeschlossenem Kunden, bzw. für eine zufällige Auswahl der nicht ausgeschlossenen Kunden. Die folgende Auswahl der anzuschreibenden Kunden sowie der optimalen Rabatthöhe erfolgt auf Basis des gewünschten Mailing Volumens und des Erwartungswertes des Warenkorbes je Rabattgutschein und Kunde.Die durch das Modell 1 priorisierte Auswahl an Kunden kann durch die Vorgabe einer Mindestkaufwahrscheinlichkeit und/oder eines Mindesterwartungswertes des Warenkorbes weiter eingeschränkt werden.Bei der Auswahl der optimalen Rabattkombination, basierend auf dem Erwartungswert, wird gleichzeitig auf die Kaufwahrscheinlichkeit und auf die zu erwartenden Kosten des Gutscheins optimiert. Hierfür wird das durchschnittliche Volumen der historischen Warenkörbe für jeden Kunden individuell berechnet und die vorhergesagte Kaufwahrscheinlichkeit des Kunden und der Rabattkombination dem Modell 2 entnommen. Für Kunden mit weniger als vier Einkäufen im Beobachtungszeitraum wird der durchschnittliche Warenkorb aller betrachteten Kunden eingesetzt.Next Basket Prediction ModellDas Modell zur Generierung von Produktempfehlungen nutzt eine ähnliche Data Pipeline wie das Customer Churn &amp; Retention Modell. Zunächst wird die im Data Warehouse vorhandene Kaufhistorie dazu genutzt, das Modell zu trainieren. Anschließend kann das Modell zu jedem beliebigen Zeitpunkt für alle Kunden individuelle Kaufempfehlungen bzw. Vorhersagen über den nächsten Warenkorb ausgeben. Hierbei kann definiert werden wie viele Produkte als Empfehlung ausgegeben werden sollen. Diese Empfehlungen sind nach der Kaufwahrscheinlichkeit absteigend sortiert, sodass auch im Nachhinein noch eine weitere Selektion möglich ist.Anschließend können nach verschiedenen Regeln diejenigen Kunden ausgewählt werden, die einen Newsletter mit ihrer persönlichen Kaufempfehlung erhalten sollen. Welche Kunden das sind wird mit der Marketingabteilung individuell abgestimmt und laufend angepasst. Auch der Einbezug des ersten Modells zur Berechnung der Kaufwahrscheinlichkeiten aus dem Customer Churn &amp; Retention Modell ist hierbei eine Option.Abbildung 2: Prozessdarstellung des EmpfehlungssystemsAutomatisierter VersandNachdem die anzuschreibenden Kunden durch das Modell identifiziert wurden, können diese mit dem individuellen Rabattgutschein bzw. der Produktempfehlungen über eine API-Schnittstelle an den Dienstleister übergeben werden, der den E-Mail- und Post-Versand übernimmt.Im Rahmen des Customer Churn &amp; Retention Modells wird zusätzlich noch einer zufällig ausgewählten Anzahl an Kunden ein ebenso zufälliger Rabattgutschein zugesendet, um einen Vergleich zu dem trainierten Modell zu haben. Beim Produktempfehlungsmodell besteht diese Möglichkeit ebenfalls.Sobald der Versand abgeschlossen ist, stellt der Dienstleister über die API-Schnittstelle eine Datei zur Verfügung, aus der der Versanderfolg ersichtlich wird. Dadurch kann bei der Evaluation sichergestellt werden, dass auch nur solche Kunden betrachtet werden, die tatsächlich einen Rabattgutschein bzw. eine Kaufempfehlung bekommen haben.EvaluationUm den Erfolg des Kundenreaktivierungsprogramms überprüfen zu können und neue Trainingsdaten für Modell 2 zu erhalten, werden die auf Kundenebene aggregierten Daten aus dem Data Warehouse auf die in der letzten Aktion kontaktierten Kunden selektiert. Anschließend wird überprüft, ob die Kunden aktiv waren oder sogar den Rabattgutschein genutzt haben.Da die Aktivität des Kunden im Beobachtungszeitraum die Zielgröße beider Modelle ist, kann zur Evaluation eine zusätzliche Kontrollgruppe genutzt werden. Für diese wird die Aktivität im Beobachtungszeitraum ebenfalls beobachtet. Somit kann festgestellt werden, ob durch die kundenindividuellen Rabattgutscheine generell die Aktivität der Kunden erhöht werden kann. Genauso können bei der Evaluation auch die durch das Modell ausgewählten Kunden und Rabattgutscheine mit den zufällig ausgewählten Kunden und Rabattgutscheinen verglichen werden, um die Wirksamkeit der Kunden- und Rabattauswahl zu überprüfen.Für das Empfehlungsmodell muss der Erfolg auf eine andere Weise gemessen werden. Es reicht nicht mehr aus, dass ein Kunde überhaupt einen Kauf im Beobachtungszeitraum tätigt. Vielmehr liegt der Fokus darauf, zu messen, ob der Kunde mindestens ein Produkt der ihm zuvor durch das Modell empfohlenen Produkte bei seinem nächsten Einkauf im Warenkorb liegen hat.Um dies zu bestimmen, haben wir die sogenannte Hit Rate definiert. Ein Hit liegt vor, wenn der Kunde mindestens ein Produkt aus dem empfohlenen Warenkorb kauft. Die Hit Rate beschreibt demzufolge den Anteil erfolgreicher Empfehlungen (Hits) an der Gesamtzahl der Empfehlungen.Um auch hier die modellbasierten Hits in das Verhältnis zu zufälligen Hits setzen zu können, wird ebenfalls eine Kontrollgruppe betrachtet. Für diese wurden zwar auch Kaufempfehlungen berechnet, allerdings kein Newsletter dazu versendet. So kann die Hit Rate des Modells mit der Hit Rate in der Kontrollgruppe verglichen und der Erfolg des Modells gemessen werden.ImpactMit unserem modellgetriebenen und automatisierten Ansatz konnten im Unternehmen sowohl Prozesse als auch Ergebnisse im Direktmarketing verbessert werden.Angefangen mit der Integration einer Vielzahl an Datenquellen zu einem Data Warehouse auf Kundenbasis, steht dem Unternehmen nun eine täglich aktualisierte Datenbasis zur Verfügung, die nicht nur für die Customer Churn &amp; Retention und Next Basket Prediction Modelle genutzt wird. Auch für weitere Modelle, Ad-hoc-Analysen und Business Intelligence Anwendungen wird dieses Data Warehouse im Unternehmen eingesetzt.Durch unseren Ansatz konnte der manuelle Aufwand bei der Ansprache von Kunden auf vielen Ebenen reduziert und oftmals sogar komplett automatisiert werden. Beispiele hierfür sind die automatisierte Identifikation und Auswahl der geeigneten Kunden für eine Rabattaktion oder einen Newsletter mit Produktempfehlungen, die automatisierte Überprüfung der Kunden bezüglich konsistenter Kontaktdaten, Sperrvermerken oder Löschungen, und die automatisierte Versandabwicklung mit einem externen Dienstleister. All diese nun automatisierten Schritte mussten zuvor durch Mitarbeitende manuell und mit erhöhtem Zeitaufwand erledigt werden.Darüber hinaus gibt es auch Aufgaben, die manuell überhaupt nicht durchführbar sind. Ein Beispiel hierfür ist die individuell auf den Kunden abgestimmte Auswahl von Produktempfehlungen. Hier können nun Newsletter mit einer standardisierten Produktauswahl für alle Kunden durch Newsletter mit einer individuellen Produktauswahl ersetzt werden.Nicht nur der Aufwand im Direktmarketing konnte reduziert werden, sondern auch die Ergebnisse der verschiedenen Maßnahmen haben sich verbessert. Durch die gezielte Ansprache inaktiver oder selten aktiver Kunden, konnten solche Kunden zurückgewonnen werden, die den Retailer aus den Augen verloren hatten, zur Konkurrenz gewechselt waren oder einen Anreiz benötigten, um wieder beim Retailer einzukaufen. Mithilfe unserer modellgetriebenen Identifikation der zur Ansprache geeigneten Kunden und der Auswahl individuell passender Rabattgutscheine konnten Streuverluste minimiert werden. Einerseits wurden Versandkosten für Kunden eingespart, die auf die Rabattgutscheine gar nicht reagieren und von unserem Modell aussortiert werden. Andererseits wurden auch Kosten für Rabattgutscheine eingespart, die von regelmäßig einkaufenden Kunden eingelöst werden, die auch ohne einen Rabattgutschein Einkäufe getätigt hätten und nun ebenfalls nicht mehr in die Rabattaktionen miteinbezogen werden.Ferner konnten mit der Bestimmung des gewinnoptimalen Rabattgutscheins individuell je Kunde die Kosten und Gewinne aus den Rabattaktionen selbst weiter optimiert werden.Nachdem es uns gelungen war mithilfe des Customer Churn &amp; Retention Ansatzes kostenoptimal Kunden zurückzugewinnen, galt es nun diese Kunden auch zu weiteren Käufen anzuregen. Hierbei konnte die Next Basket Prediction dem Unternehmen helfen, automatisiert und individuell die zurückgewonnenen Kunden, aber auch regelmäßig aktive Kunden, interessante Produkte zu präsentieren und so die Kunden weiter regelmäßig zum Besuch des Retailers zu motivieren, zu weiteren Ankäufen anregen und eine tiefergehende Bindung zum Kunden aufzubauen.Fazit &amp; AusblickMit den beiden modellgetriebenen Ansätzen konnten wir dem Einzelhändler dabei helfen, seine Kundenansprache in den verschiedenen Phasen des Kundenlebenszyklus zu optimieren und automatisieren, bei einer gleichzeitigen Kostensenkung und Umsatzsteigerung.Durch unseren ganzheitlichen Ansatz steht dem Einzelhändler ein Data Warehouse zur Verfügung, das er einerseits für weitergehende Analysen oder Business Intelligence Anwendungen nutzen kann, mit dem er andererseits aber auch weitere Aufgaben im Marketingbereich durch modellgetriebene Ansätze optimieren und automatisieren kann.Mit der fortschreitenden Digitalisierung haben die Kunden mittlerweile den Anspruch an die Unternehmen, mit individuellem Content angesprochen zu werden. Dieser Trend wird sich auch in Zukunft fortsetzen, sodass es gilt die im Unternehmen vorhandenen Datenquellen zu nutzen, um diesen Wunsch des Kunden zu erfüllen und ihn seinen Bedürfnissen entsprechend zu kontaktieren.Quellen1 Chen, Tianqi / Guestrin, Carlos (2016) „Xgboost: A scalable tree boosting system“, In: Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, S. 785-794";https://www.statworx.com/de/blog/case-study-automatisierte-marketing-interaktion-im-retail/;Statworx;  Niklas Junker
  24. November 2020;Whitepaper: 35 KI und Machine Learning Use Cases für die Retail- und Konsumgüterindustrie;"Management SummaryKünstliche Intelligenz (KI) und Machine Learning (ML) gehören in den meisten Branchen zu den wichtigsten Technologietrends. Die Art, wie Unternehmen ihr Geschäft betreiben, wird durch KI grundlegend verändert. Der Wert einer KI entsteht in vielerlei Form, sei es durch verbesserte datenbasierte Entscheidungsfindung, optimierte oder automatisierte Prozesse, weniger manuelle Arbeit, höhere Produktivität, niedrigere Kosten oder höhere Einnahmen. KI und Machine Learning können nicht länger als ein Privileg von Spitzentechnologie-Giganten, wie Amazon, Google oder Facebook, verstanden werden. Jedes Unternehmen, unabhängig von Größe, Erfahrung oder anderen Merkmalen, kann die Chancen nutzen, die sich aus der Weiterentwicklung dieser Technologien ergeben.Warum KI &amp; ML Use Cases für die Retail- und Kosumgüterindustrie besonders relevant sindInsbesondere für Unternehmen in der Retail- und Konsumgüterindustrie gibt es viele nützliche Anwendungen für Technologien, die unter KI &amp; ML zusammengefasst werden können. Ein Hauptgrund dafür ist die enorme Datenmenge, die von Unternehmen dieser Branche (insb. im E-Commerce) gesammelt wird.In diesem Whitepaper geben wir Ihnen einen Überblick über mögliche Use Cases von KI in dieser speziellen Branche. Sie erhalten eine übersichtliche, umfassende Sammlung von 35 Use Cases, die Sie dazu anregen soll, über mögliche Anwendungen in Ihrem Unternehmen nachzudenken. Dabei konzentrieren wir uns auf die Business Perspektive und gehen nicht auf technische Details ein. Wir erklären, welche Anwendungsmöglichkeiten sich Ihnen bieten und warum Ihr Unternehmen von der Integration von künstlicher Intelligenz und Machine Learning profitieren kann. Wenn Sie an technischen Details oder Folgefragen interessiert sind, können Sie uns jederzeit kontaktieren.Mit KI &amp; ML Use Cases funktionsübgreifend Mehrwert schaffenDie Use Cases sind in die folgenden vier Kategorien unterteilt:Marketing, Sales &amp; After SalesPeople &amp; OrganizationFinance, Accounting &amp; ControllingSupply Chain, Manufacturing &amp; QualityObwohl die Use Cases nach Unternehmensfunktionen gegliedert sind, möchten wir betonen, dass einige davon über verschiedene Funktionen hinweg interessant und wertvoll sind. Wir sind jedoch der Meinung, dass diese Struktur dazu beiträgt, den Überblick zu behalten und den Lesenden nicht zu überfordern. Laden Sie sich das komplette Whitepaper jetzt kostenfrei herunter!Sie haben Interesse am Thema künstliche Intelligenz &amp; Machine Learning Use Cases?Hier können Sie sich das gesamte Whitepaper „35 KI und Machine Learning Use Cases für die Retail- und Konsumgüterindustrie“ kostenfrei herunterladen.Über den AutorJan FischerI am a data scientist at STATWORX. I always enjoyed to think critically about complex problems, understand and find a solution. Fortunately, STATWORX pays me for that!.e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/whitepaper-35-ki-und-machine-learning-use-cases-fur-die-retail-und-konsumguterindustrie/;Statworx;  Jan Fischer
  17. November 2020;Whitepaper: Wie man durch Explainable AI Vertrauen im Fachbereich aufbaut;"Management SummaryKünstliche Intelligenz in UnternehmenKünstliche Intelligenz (KI) in Unternehmen ist längst kein neuer Trend mehr. Trotzdem ist das Potenzial für KI noch lange nicht ausgeschöpft und bietet nach wie vor enorme Möglichkeiten. Eine PWC Studie kommt zu dem Ergebnis, dass KI bis 2030 mehr als 15 Billionen Dollar zur Weltwirtschaft beitragen könnte. Einer Analyse der Boston Consulting Group zufolge wird die Bedeutung von KI für Unternehmen durch die Covid-19-Krise sogar noch zunehmen. KI-Systeme nutzen riesige Datenmengen, um zugrundeliegende Datenmuster zu erkennen und ermöglichen damit Computersystemen komplexe datenbasierte Entscheidungen zu treffen, Bilder zu analysieren, menschliche Sprache zu verstehen, Verkaufszahlen vorherzusagen und vieles mehr. Diese Möglichkeiten, die KI Unternehmen bietet, sind gerade in der Krise besonders wertvoll, aber auch, um sich an die Welt nach der Krise anzupassen. BCG zufolge werden hier Use Cases, die KI mit menschlichem Urteilsvermögen und Erfahrung verbinden, besonders erfolgreich sein.Warum Explainable AI?Damit Menschen und KI erfolgreich zusammenarbeiten können, ist jedoch ausreichendes Vertrauen in KI-Systeme ausschlaggebend. Um vertrauenswürdig zu sein, müssen ihre Entscheidungen nachvollziehbar, erklärbar und verlässlich (reproduzierbar) sein. Die steigende Komplexität fortgeschrittener KI-Systeme macht es aber gleichzeitig immer schwieriger nachzuvollziehen, wie diese im Detail funktionieren. Für Nutzer sind KI-Systeme daher in der Regel sogenannte Black Boxes. Das Training und die Architektur moderner KI-Systeme kann technisch so komplex sein, dass sogar Experten die Entscheidungen des Systems auf einer inhaltlichen Ebene nicht mehr erklären können.Hier setzt Explainable Artificial Intelligence (XAI) an. Mit XAI sollen die Entscheidungen und Aktivitäten einer KI für den Menschen transparent und nachvollziehbar gemacht werden. Es wird die Grundlage geschaffen, damit Nutzer und Entscheidungsträger verstehen, wie KI-Systeme funktionieren und zu bestimmten Ergebnissen kommen. Diese Transparenz bildet die Basis für das Vertrauen und die Akzeptanz von künstlicher Intelligenz – die Voraussetzungen für ihren erfolgreichen Einsatz.Eine Vielzahl von Methoden ermöglichen es heute auch komplexe KI-Systeme erklärbar zu machen. Auch wenn es dabei eine Reihe von Herausforderungen zu beachten gilt, sind die Vorteile von XAI im Unternehmen immens. Das Whitepaper „Wie man durch Explainable AI Vertrauen im Fachbereich aufbaut“ gibt einen Überblick über Einsatzmöglichkeiten, Vorteile, Methoden und Herausforderungen beim Einsatz von XAI im Unternehmen und dient damit als Wegweiser für dieses wichtige Zukunftsthema.Sie haben Interesse am Thema Explainable AI?Hier können Sie sich das gesamte Whitepaper „Wie man durch Explainable AI Vertrauen im Fachbereich aufbaut“ kostenfrei herunterladen.Über den AutorVerena EikmeierI am a data scientist at STATWORX and especially interested in natural language processing and on how data science applications can enrich human decision making..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/whitepaper-wie-man-durch-explainable-ai-vertrauen-im-fachbereich-aufbaut/;Statworx;  Verena Eikmeier
  12. November 2020;5 praxisnahe Beispiele für NLP Use Cases;"Management SummaryIn modernen Unternehmen fallen im Tagesgeschäft an vielen Stellen Informationen in Textform an: Je nach Businesskontext können dies Rechnungen sein, oder auch Emails, Kundeneingaben (wie Rezensionen oder Anfragen), Produktbeschreibungen, Erklärungen, FAQs sowie Bewerbungen. Diese Informationsquellen blieben bis vor kurzem weitestgehend dem Menschen vorbehalten, da das maschinelle, inhaltliche Verstehen von Text ein technologisch herausforderndes Problem darstellt.Aufgrund jüngster Errungenschaften im Bereich Deep Learning können nun eine Reihe unterschiedlicher NLP („Natural Language Processing“) Tasks mit erstaunlicher Güte gelöst werden.Erfahren Sie in diesem Beitrag anhand von fünf praxisnahen Beispielen, wie NLP Use Cases diverse Businessprobleme lösen und so für Effizienz und Innovation sorgen.InhaltsverzeichnisEinführungWas ist NLP? – Ein kurzer ÜberblickDie 5 NLP Use CasesTextklassifizierung im RekrutierungsprozessNamed Entity Recognition zur Verbesserung der Usability einer NachrichtenseiteEin Chatbot für ein FernbusunternehmenEin Question-Answering-System als Voice Assistant für technische Fragen zum AutomobilAutomatische Textzusammenfassungen (Textgenerierung) von Schadensbeschreibungen für eine SachversicherungAusblickEinführungNatural Language Processing (NLP) ist zweifelslos ein Gebiet, dem in jüngster Vergangenheit besondere Aufmerksamkeit im Big Data-Umfeld zugekommen ist. So hat sich das von Google gemessene Interesse an dem Thema in den letzten drei Jahren mehr als verdoppelt. Daran ist erkennbar, dass innovative NLP-Technologien längst nicht mehr nur ein Thema für die Big Player, wie Apple, Google oder Amazon, ist. Vielmehr ist eine generelle Demokratisierung der Technologie zu beobachten. Einer der Gründe dafür ist sicherlich, dass nach einer Schätzung von IBM etwa 80% der „weltweiten Informationen“ nicht in strukturierten Datenbanken vorliegen, sondern in unstrukturierter, natürlicher Sprache. NLP wird zukünftig eine Schlüsselrolle einnehmen, wenn es darum geht, diese Informationen nutzbar zu machen. Damit wird der erfolgreiche Einsatz von NLP-Technologien zu einem der Erfolgsfaktoren für die Digitalisierung in Unternehmen werden.Damit Sie sich ein Bild davon machen können, welche Möglichkeiten NLP heutzutage im Businesskontext öffnet, werden Ihnen im Folgenden fünf praxisnahe Anwendungsfälle vorgestellt und die dahinterstehende Lösungen erklärt.Was ist NLP? – Ein kurzer ÜberblickAls ein Forschungsthema, das bereits in den 50er Jahren Linguisten und Informatiker beschäftigte, fristete NLP im 20sten Jahrhundert auf der Anwendungsseite ein kaum sichtbares Dasein.Der zentrale Grund dafür lag in der Verfügbarkeit der notwendigen Trainingsdaten. Zwar ist generell die Verfügbarkeit von unstrukturierten Daten, in Form von Texten, insbesondere mit dem Aufstieg des Internets exponentiell gestiegen, jedoch fehlte es weiterhin an geeigneten Daten für das Modelltraining. Dies lässt sich damit begründen, dass die frühen NLP Modelle zumeist überwacht trainiert werden mussten (sogenanntes Supervised Learning). Das Supervised Learning setzt jedoch voraus, dass Trainingsdaten mit einer dedizierten Zielvariable versehen werden müssen. Dies bedeutet, dass z.B. bei einer Textklassifikation der Textkorpus vor dem Modelltraining manuell durch Menschen annotiert werden muss.Dies änderte sich Ende der 2010er Jahre, als eine neue Modellgeneration künstlicher neuronaler Netzwerke zu einem Paradigmenwechsel führte. Diese sogenannten „Language Models“ werden auf Grundlage riesiger Textkorpora von Facebook, Google und Co. (vor-)trainiert, indem einzelne Wörter in den Texten zufällig maskiert und im Verlauf des Trainings vorhergesagt werden. Es handelt sich dabei um das sogenannte selbstüberwachte Lernen (Self-Supervised Learning), das nicht länger eine separate Zielvariable voraussetzt. Im Zuge des Trainings erlernen diese Modelle ein kontextuelles Verständnis von Texten.Vorteil dieses Vorgehens ist, dass ein- und dasselbe Modell mit Hilfe des erlernten kontextuellen Verständnisses für eine Vielzahl unterschiedlicher Downstream-Tasks (z.B. Textklassifizierung, Sentiment Analysis, Named Entity Recognition) nachjustiert werden kann. Dieser Vorgang wird als Transfer Learning bezeichnet. In der Praxis lassen sich diese vortrainierten Modelle herunterladen, sodass nur die Feinjustierung für die spezifische Anwendung durch zusätzliche Daten selbst gemacht werden muss. Folglich lassen sich mittlerweile mit wenig Entwicklungsaufwand performante NLP-Anwendungen entwickeln.Um mehr über Language Models (insbesondere die sogenannten Transformer Modelle wie „BERT“, bzw. „roBERTa“, u.ä.) sowie Trends und Hemmnisse im Bereich NLP zu erfahren, lesen Sie hier den Beitrag zum Thema NLP-Trends von unserem Kollegen Dominique Lade. Die 5 Use CasesTextklassifizierung im RekrutierungsprozessEin medizinisches Forschungsinstitut möchte seinen Rekrutierungsprozess von Studienteilnehmer*innen effizienter gestalten.Für das Testen eines neuen Medikaments werden unterschiedliche, untereinander abhängige Anforderungen an die infrage kommende Personen gestellt (z.B. Alter, allg. Gesundheitszustand, Vorhandensein/Abwesenheit von Vorerkrankungen, Medikationen, genetische Dispositionen etc.). Das Prüfen all dieser Anforderungen ist mit einem großen Zeitaufwand verbunden. Üblicherweise dauert das Sichten und Beurteilen relevanter Informationen etwa eine Stunde pro potenziellen Studienteilnehmenden. Hauptgrund dafür ist, dass die klinischen Notizen Informationen über Patienten enthalten, die über strukturierte Daten wie Laborwerte und Medikamente hinausgehen: Auch unstrukturierter Informationen sind in den medizinischen Berichten, Arztbriefen, und Entlassungsberichten o.ä. in Textform zu finden. Insbesondere das Auswerten letzterer Daten bedarf viel Lesezeit und ist daher mit großem Aufwand verbunden. Um den Prozess zu beschleunigen, entwickelt das Forschungsinstitut ein Machine Learning Modell, das eine Vorauswahl von vielversprechenden Kandidaten trifft, sodass die Experten*innen lediglich die vorgeschlagene Personengruppe validieren müssen.Die NLP LösungAus methodischer Sicht handelt es sich bei diesem Problem um eine sogenannte Textklassifikation. Dabei wird basierend auf einem Text, eine Prognose für eine zuvor definierte Zielvariable erstellt. Um das Modell zu trainieren, ist es – wie im Supervised Learning üblich – notwendig, die Daten, in diesem Fall also die Arztdokumente, mit der Zielvariable zu annotieren. Da es hier ein Klassifikationsproblem zu lösen gilt (geeignete oder ungeeignete Studienteilnehmer*in), beurteilen die Experten*innen für einige Personen im Pool die Eignung für die Studie manuell. Ist eine Person geeignet, wird sie mit einer Eins gekennzeichnet (=positiver Fall), ansonsten mit einer Null (=negativer Fall). Anhand dieser Trainingsbeispiele kann das Modell nun Zusammenhänge zwischen den medizinischen Dokumenten der Personen und ihrer Eignung lernen.Um der Komplexität des Problems Herr zu werden, wird ein entsprechend komplexes Modell namens ClinicalBERT verwendet. Dabei handelt es sich um ein Language Modell, das auf BERT (Bidirectional Encoder Representations from Transformers) basiert, und zusätzlich auf einem Datensatz von klinischen Texten trainiert wurde. Somit ist ClinicalBERT in der Lage, sogenannte Repräsentationen von sämtlichen medizinischen Dokumentationen für jede Person zu generieren. In einem letzten Schritt wird das neuronale Netzwerk von ClinicalBERT durch eine taskspezifische Komponente ergänzt. In diesem Fall handelt es sich um eine binäre Klassifikation: Zu jeder Person soll eine Eignungswahrscheinlichkeit ausgegeben werden. Durch einen entsprechenden linearen Layer wird die hochdimensionale Textdokumentation schlussendlich in eine einzige Zahl, die Eignungswahrscheinlichkeit, überführt. In einem Gradientenverfahren lernt das Modell nun anhand der Trainingsbeispiele die Eignungswahrscheinlichkeiten.Weitere Anwendungsszenarien von TextklassifikationTextklassifikation findet häufig in der Form von Sentiment Analysis statt. Dabei geht es darum, Texte in vordefinierte Gefühlskategorien (z.B. negativ/positiv) einzuordnen. Diese Informationen sind insbesondere in der Finanzwelt oder beim Social Media Monitoring wichtig. Darüber hinaus kann Textklassifikation in verschiedenen Kontexten verwendet werden, in denen es darum geht, Dokumente nach ihrem Typ zu sortieren (z.B. Rechnungen, Briefe, Mahnungen…).Named Entity Recognition zur Verbesserung der Usability einer NachrichtenseiteEin Verlagshaus bietet seinen Leser*innen auf einer Nachrichtenseite eine Vielzahl von Artikeln über diverse Themen an. Im Zuge von Optimierungsmaßnahmen möchte man ein besseres Recommender-System implementieren, sodass zu jedem Artikel weitere passende (ergänzende oder ähnliche) Artikel vorgeschlagen werden. Außerdem möchte man die Suchfunktion auf der Landingpage verbessern, damit der Kunde oder die Kundin schnell den Artikel findet, der gesucht ist.Um für diese Zwecke eine gute Datengrundlage zu schaffen, entscheidet sich der Verlag dazu, mit Named Entity Recognition den Texten automatisierte Tags zuzuordnen, anhand derer sowohl das Recommender-System als auch die Suchfunktion verbessert werden können. Nach erfolgreicher Implementierung wird auf deutlich mehr vorgeschlagene Artikel geklickt und die Suchfunktion ist wesentlich komfortabler geworden. Im Resultat verbringen die Leser*innen signifikant mehr Zeit auf der Seite.Die NLP LösungUm das Problem zu lösen, ist es wichtig, die Funktionsweise von NER zu verstehen:Bei NER geht es darum, Worte oder ganze Satzglieder inhaltlichen Kategorien zuzuordnen. So kann man „Peter“ beispielsweise als Person identifizieren, „Frankfurt am Main“ ist ein Ort und „24.12.2020“ ist eine Zeitangabe. Offensichtlich gibt es aber auch deutlich kompliziertere Fälle. Dazu vergleichen Sie die folgenden Satzpaare:„Früher spazierte Emma im Park immer an der schönen Bank aus Holz vorbei.“ (Bank = Sitzbank)„Gestern eilte sie noch zur Bank, um das nötige Bargeld abzuheben.“(Bank = Geldinstitut)Für den Menschen ist vollkommen offensichtlich, dass das Wort „Bank“ in den beiden Sätzen eine jeweils andere Bedeutungen hat. Diese scheinbar einfache Unterscheidung ist für den Computer allerdings alles andere als trivial. Ein Entity Recognition Modell könnte die beiden Sätze wie folgt kennzeichnen:„Früher (Zeitangabe) spazierte Emma (Person) im Park immer an der schönen Bank (Sitzgelegenheit) aus Holz vorbei.“„Gestern (Zeitangabe) eilte sie (Person/Pronomen) noch zur Bank (Geldinstitut), um das nötige Bargeld abzuheben.“In der Vergangenheit hätte man zur Lösung des obigen NER-Problems zu regelbasierten Algorithmen gegriffen, doch auch hier setzt sich der Machine Learning Ansatz durch:Das vorliegende Multiclass-Klassifizierungsproblem der Entitätsbestimmung wird erneut mithilfe des BERT-Modells angegangen. Zusätzlich wird das Modell auf einem annotierten Datensatz trainiert, in dem die Entitäten manuell identifiziert sind. Die umfangreichste öffentlich zugängliche Datenbank in englischer Sprache ist die Groningen Meaning Bank (GMB). Nach erfolgreichem Training ist das Modell in der Lage, aus dem Kontext, der sich aus dem Satz ergibt, auch bisher unbekannte Wörter korrekt zu bestimmen. So erkennt das Modell, dass nach Präpositionen wie „in, bei, nach…“ ein Ort folgt, aber auch komplexere Kontexte werden in Bezug auf die Entitätsbestimmung herangezogen.Weitere Anwendungsszenarien von NER:NER ist als klassische Information Retrieval-Task für viele andere NER-Tasks, wie zum Beispiel Chatbots und Frage-Antwort Systeme, zentral. Darüber hinaus wird NER häufig zur Textkatalogisierung verwendet, bei der der Typ des Textes anhand von stichhaltigen, erkannten Entitäten bestimmt wird.Ein Chatbot für ein FernbusunternehmenEin Fernbusunternehmen möchte seine Erreichbarkeit erhöhen und darum die Kommunikationswege mit dem Kunden ausbauen. Neben seiner Homepage und seiner App möchte das Unternehmen einen dritten Weg zum Kunden, nämlich einen Whatsapp-Chatbot, anbieten. Die Zielvorstellung ist, dass man in der Konversation mit dem Chatbot gewisse Aktionen wie das Suchen, Buchen und Stornieren von Fahrten ausführen kann. Außerdem soll mit dem Chatbot ein zuverlässiger Weg geschaffen werden, die Fahrgäste über Verspätungen zu informieren.Mit der Einführung des Chatbots können nicht nur bestehende Fahrgäste leichter erreicht werden, sondern auch Kontakt zu neuen Kunden*innen aufgebaut werden, die noch keine App installiert haben.Die NLP LösungAbhängig von den Anforderungen, die an den Chatbot gestellten werden, wählt man zwischen verschiedenen Chatbot Architekturen aus.Über die Jahre sind im Wesentlichen vier Chatbot-Paradigmen erprobt worden: In einer ersten Generation wurde die Anfrage auf bekannte Muster geprüft und entsprechend angepasste vorgefertigte Antworten ausgegeben („pattern matching“). Etwas ausgefeilter ist das sogenannte „grounding“, bei der durch Named Entity Recognition (s.o.) aus Wissensbibliotheken (z.B. Wikipedia) extrahierte Informationen in einem Netzwerk organisiert werden. Ein solches Netzwerk hat den Vorteil, dass nicht nur eingetragenes Wissen abgerufen werden kann, sondern, dass auch nicht registriertes Wissen durch die Netzwerkstruktur inferiert werden kann. Beim „searching“ werden direkt Fragen-Antwortpaare aus dem Konversationsverlauf (oder aus davor registrierten Logs) zum Suchen einer passenden Antwort herangezogen. Die Anwendung von Machine Learning Modellen ist der bewährteste Ansatz, um dynamisch passende Antworten zu generieren („generative models“).Um einen modernen Chatbot mit klar eingrenzbaren Kompetenzen für das Fernbusunternehmen zu implementieren, empfiehlt es sich, auf bestehende Frameworks wie Google Dialogflow zurückzugreifen. Hierbei handelt es sich um eine Plattform, mit der sich Chatbots konfigurieren lassen, die die Elemente aller zuvor gennannten Chatbot-Paradigmen besitzen. Dazu übergibt man Parameter wie Intends, Entitäten und Actions.Ein Intend („Benutzerabsicht“) ist beispielsweise die Fahrplanauskunft. Indem man verschiedene Beispielphrasen („Wie komme ich am … von … nach … “, „Wann fährt der nächste Bus von … nach …“) an ein Language Model übergibt, gelingt es dem Chatbot auch ungesehene Inputsätze dem richtigen Intend zuzuordnen (vgl. Textklassifikation).Weiterhin werden die verschiedenen Reiseorte und Zeitangaben als Entitäten definiert. Wird nun vom Chatbot ein Intend mit passenden Entitäten erfasst (vgl. NER), dann kann eine Action, in diesem Fall eine Datenbankabfrage, ausgelöst werden. Schlussendlich wird eine Intend-Answer mit den relevanten Informationen ausgegeben, die an sämtliche vom Benutzer angegebene Informationen im Chatverlauf angepasst ist („stateful“).Weitere Anwendungsszenarien von Chatbots:Es gibt vielfältige Einsatzmöglichkeiten im Kundenservice – je nach Komplexität des Szenarios von der automatischen Vorbereitung (z.B. Sortierung) eines Kundenauftrags hin zur kompletten Abwicklung einer Kundenerfahrung.Ein Question-Answering-System als Voice Assistant für technische Fragen zum AutomobilEin Automobilhersteller stellt fest, dass viele seiner Kunden*innen nicht gut mit den Handbüchern, die den Autos beiliegt, zurechtkommt. Häufig wird zu lange nach der relevanten Information gesucht oder sie wird gar nicht gefunden. Daher wird beschlossen, ergänzend zum statischen Handbuch auch einen Voice Assistant anzubieten, der auf technische Fragen präzise Antworten gibt. Zukünftig können die Fahrer*innen bequem mit ihrer Mittelkonsole sprechen, wenn sie ihr Fahrzeug warten wollen oder technische Auskunft wünschen.Die NLP LösungMit Frage-Antwort-Systemen wird sich schon seit Jahrzehnten auseinandergesetzt wird, stehen sie doch in gewisser Hinsicht an der Vorfront der künstlichen Intelligenz. Ein Frage-Antwort-System, das unter Berücksichtigung aller vorliegenden Daten immer eine korrekte Antwort fände, könnte man auch als „General AI“ bezeichnen. Eine Hauptschwierigkeit auf dem Weg zur General AI ist, dass das Gebiet, über das das System informiert sein muss, unbegrenzt ist. Demgegenüber liefern Frage-Antwort-Systeme gute Ergebnisse, wenn das Gebiet klar eingegrenzt ist, wie es beim Automobilassistenten der Fall ist. Grundsätzlich gilt: Je spezifischer das Gebiet, desto bessere Ergebnisse können erwartet werden.Für die Implementierung des Frage-Antwort-Systems werden strukturierte Daten, wie technische Spezifikationen der Komponenten und Kennzahlen des Modells, aber auch unstrukturierte Daten, wie Handlungsanweisungen, aus dem Handbuch herangezogen. Sämtliche Daten werden in einem Vorbereitungsschritt mithilfe anderer NLP-Techniken (Klassifikation, NER) in Frage-Antwort-Form gebracht. Diese Daten werden einer Version von BERT übergeben, die bereits auf einem großen Frage-Antwort-Datensatz („SQuAD“) vortrainiert wurde. Das Modell ist damit in der Lage, souverän bereits eingespeiste Fragen zu beantworten, aber auch „educated guesses“ für ungesehene Fragen abzugeben.Weitere Anwendungsszenarien von Frage-Antwort-Systemen:Mithilfe von Frage-Antwort-Systemen können unternehmensinterne Suchmaschinen um Funktionalitäten erweitert werden. Im E-Commerce können auf Basis von Artikelbeschreibungen und Rezensionen automatisiert Antworten auf Sachfragen gegeben werden.Automatische Textzusammenfassungen (Textgenerierung) von Schadensbeschreibungen für eine SachversicherungEine Versicherung möchte die Effizienz ihrer Schadensregulierungsabteilung erhöhen. Es wurde festgestellt, dass es bei einigen Schadensreklamationen vom Kunden zu internen Zuständigkeitskonflikten kommt. Grund dafür ist, dass diese Schäden von Kund*innen zumeist über mehrere Seiten beschrieben werden und so eine erhöhte Einarbeitungszeit benötigt wird, um beurteilen zu können, ob man den Fall bearbeiten soll. So passiert es häufig, dass eine Schadensbeschreibung komplett gelesen werden muss, um zu verstehen, dass man den Schaden selbst nicht zu bearbeiten hat. Nun soll ein System Abhilfe schaffen, das automatisierte Zusammenfassungen generiert. Die Sachbearbeiter*innen können in Folge der Implementierung nun deutlich schneller über die Zuständigkeit entscheiden.Die NLP LösungGrundsätzlich kann man beim Probelm der Textzusammenfassung zwischen zwei verschiedenen Ansätzen differenzieren: Bei der Extraction werden aus dem Inputtext die wichtigsten Sätze identifiziert, die dann im einfachsten Fall als Zusammenfassung verwendet werden. Dem gegenüber steht die Abstraction, bei der ein Text durch ein Modell in einen neu generierten Zusammenfassungstext überführt wird. Der zweite Ansatz ist deutlich komplexer, da hier Paraphrasierung, Generalisierung oder das Einbeziehen von weiterführendem Wissen möglich ist. Daher birgt dieser Ansatz auch ein größeres Potenzial, sinnvolle Zusammenfassungen generieren zu können, ist allerdings auch fehleranfälliger. Moderne Algorithmen zur Textzusammenfassung verfolgen den zweiten Ansatz, oder aber eine Kombination aus beiden Ansätzen.Zur Lösung des Versicherungs-Use-Cases wird ein sogenanntes Sequence-to-Sequence-Modell verwendet, welches einer Wortsequenz (der Schadensbeschreibung) einer anderen Wortsequenz (der Zusammenfassung) zuordnet. Hierbei handelt es sich üblicherweise um ein rekurrentes neuronales Netzwerk (RNN), das auf Grundlage von Textzusammenfassungs-Paaren trainiert wird. Der Trainingsprozess ist so gestaltet, dass die Wahrscheinlichkeit für das nächste Wort abhängig von den letzten Worten (und zusätzlich einem „inner state“ des Modells), modelliert wird. Gleichsam schreibt das Modell effektiv die Zusammenfassung „von links nach rechts“, indem sukzessiv das nächste Wort vorhergesagt wird. Ein alternativer Ansatz sieht vor, den Input vom Language Model BERT numerisch encodieren zu lassen und auf Basis dieser Zahlenrepräsentation einen GPT-Decoder den Text autoregressiv zusammenfassen zu lassen. Mithilfe von Modellparametern kann in beiden Fällen angepasst werden, wie lang die Zusammenfassung etwa sein soll.Weitere Anwendungsszenarien von Sprachgenerierung:Ein solches Szenario ist an vielen Stellen denkbar: Das automatisierte Schreiben von Berichten, die Generierung von Texten auf der Grundlage der Analyse von Einzelhandelsverkaufsdaten, die Zusammenfassung von elektronischen Krankenakten oder die Erstellung von textlichen Wettervorhersagen aus Wetterdaten sind denkbare Anwendungen. Darüber hinaus kommt es auch bei anderen NLP Anwendungsfällen wie Chatbots und Q&amp;A-Systemen zur Sprachgenerierung.AusblickVielleicht haben Sie beim Durchlesen dieser Anwendungsbeispiele von Textklassifikation, Chatbots, Frage-Antwort-Systemen, NER und Textzusammenfassungen den Eindruck gewonnen, dass es auch in Ihrem Unternehmen viele Prozesse gibt, die sich mit NLP-Lösungen beschleunigen ließen.Tatsächlich ist NLP nicht nur ein spannendes Forschungsfeld, sondern auch eine Technologie, deren Anwendbarkeit im Businessumfeld stetig wächst.NLP wird in Zukunft nicht nur ein Fundament einer datengetriebenen Unternehmenskultur werden, sondern birgt schon jetzt durch direkte Anwendung ein riesiges Innovationspotenzial, in das es sich zu investieren lohnt.Bei STATWORX haben wir bereits jahrelange Erfahrung in der Entwicklung von maßgeschneiderten NLP-Lösungen. Hier finden die zwei unserer Case Studies zum Thema NLP: Social Media Recruitingmit NLP &amp; SupplierRecommendation Tool. Wir stehen Ihnen gerne für eine individuelle Beratung zu diesem und vielen weiteren Themen zur Verfügung.Über den AutorFelix PlaggeAs a data scientist at STATWORX I am always keen on finding relevant structure in data. Also, I'm passionate about the mathematical fundament of machine learning algorithms..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/5-praxisnahe-beispiele-fuer-nlp-use-cases/;Statworx;  Felix Plagge
  10. November 2020;Whitepaper: Machine Learning in der Cloud – AWS;"Management SummaryDie digitale Transformation ist für Unternehmen aller Branchen und Größen eine Herausforderung. Basierend auf aktuellen Umfragen, werden als größte Hindernisse einer erfolgreichen Digitalisierung die Anforderungen an Datenschutz und -sicherheit sowie fehlende IT-Kompetenzen in Unternehmen identifiziert. Damit Unternehmen in einem digitalen, software- und datengetriebenen Zeitalter weiter erfolgreich sein können, müssen die notwendigen technischen Voraussetzungen etabliert werden. Als wichtiger Baustein hierbei wird die Anwendung von Cloud Technologie gesehen. Eine Migration von IT-Infrastrukturen oder Anwendungen in die Cloud ist ein wichtiger Schritt, um die digitale Wettbewerbsfähigkeit von Organisationen weiter sicherzustellen.Machine Learning in der CloudWelche Möglichkeiten sich für Unternehmen nach einer erfolgreichen Migration in die Cloud eröffnen können, wird beispielsweise deutlich durch das breite Angebot von Machine-Learning-as-a-Service Lösungen (MLaaS). Standardisierte Services, wie bspw. automatisierte Video- und Bilderkennung oder Übersetzung von Sprache zu Text (und vice versa), reduzieren erheblich das erforderliche Fachwissen zur Anwendung von Machine Learning oder KI. Die zuvor hohe Einstiegsbarriere, um mittels Machine Learning eine Hebelwirkung für zentrale Geschäftsbereiche zu bewirken, wird durch das Angebot aller Cloud Service Provider in diesem Bereich in den kommenden Jahren weiter rapide sinken. Dies ermöglicht es auch Unternehmen, die bisher keine dedizierten Ressourcen im Bereich KI aufgebaut haben, die Chancen zu nutzen, die sich durch die Anwendung von KI für eigene Produkte, Services und Prozesse ergeben.Abbildung 1: Use Cases vs. Know-HowProvider für Cloud-ComputingDie drei größten Provider für Cloud-Computing, Amazon Web Services (AWS), Microsoft Azure (Azure) und Google Cloud Platform (GCP), unterscheiden sich hinsichtlich ihrer Machine Learning Services nur in Teilbereichen. AWS setzt, nicht zuletzt aufgrund der Marktposition, auf ein sehr breites Angebot von eigenen ML-Services. Diese können durch verschiedenste Module von Drittanbietern ergänzt werden. Microsoft Azure gilt als sicherer Partner für Unternehmen, die auch schon in der Vergangenheit auf Produkte von Microsoft gesetzt haben. GCP kann sich durch ein Angebot an spezialisierten Produkten im Bereich Deep Learning von der Konkurrenz abgrenzen.Was erwartet Sie in unserem Whitepaper „Machine Learning in der Cloud“?Für die Anwendung von Machine Learning Verfahren ist die Verfügbarkeit von lokalen Rechenzentren und der dazugehörigen Infrastruktur nicht mehr zwangsläufig erforderlich. In diesem Whitepaper beleuchten wir, mit welchen Angeboten die drei größten Provider für Cloud-Computing, AWS, Azure und GCP, die Hürden für erfolgreiche ML-Projekte herabsetzen. Mit einem Fokus auf die erforderlichen ML- und IT-Kompetenzen, werden wir uns einen Überblick über die Angebote verschaffen und abschließend die Unterschiede zwischen den Cloud-Providern hervorheben.Sie haben Interesse am Thema Machine Learning in der Cloud?Hier können Sie sich das gesamte Whitepaper „Machine Learning in der Cloud“ kostenfrei herunterladen.Über den AutorAlexander BlaufussI am a data scientist at STATWORX. Fascinated by the world of data, I am interested in Deep Learning, communicating the insights gained from data and the techniques for visualizing data. If you have any questions about my Blog or Data Science in general, you are welcome to contact me via Email or LinkedIn..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/whitepaper-machine-learning-in-der-cloud-aws-azure-und-gcp-im-vergleich/;Statworx;  Alexander Blaufuss
  5. November 2020;Die 5 wichtigsten Use Cases für Computer Vision;"Computer sehen zu lassen, dies mag für viele nach Science-Fiction klingen. Denn mit «sehen» ist nicht das Filmen mit einer Webcam, sondern das Verständnis von Bildmaterial gemeint. Tatsächlich sind derartige Technologien hinter den Kulissen vieler alltäglicher Services schon lange im Einsatz. Soziale Netzwerke erkennen seit Jahren Freunde und Bekannte auf Fotos und moderne Smartphones lassen sich mit dem Gesicht anstatt einem PIN-Code entsperren. Neben diesen kleinen Alltagserleichterungen birgt das rasant wachsende Feld der «Computer Vision» weitaus größeres Potenzial für den industriellen Einsatz. Die spezialisierte Verarbeitung von Bildmaterial verspricht sowohl viele repetitive Prozesse zu erleichtern und automatisieren. Zudem sollen Experten und Fachpersonal entlastet und in ihren Entscheidungen unterstützt werden.Die Grundlagen für Bilderkennung und Computer Vision wurden bereits in den 1970er Jahren geschaffen. Allerdings hat das Feld erst in den letzten Jahren vermehrt Anwendung außerhalb der Forschung gefunden. In unserer Tätigkeit als Data Science &amp; AI Beratung hier bei STATWORX haben wir bereits einige interessante Anwendungsfälle von Computer Vision kennengelernt. Dieser Beitrag stellt fünf ausgewählte und besonders vielversprechende Use Cases verschiedener Industrien vor, die entweder bereits in Produktion anzutreffen sind, oder in den kommenden Jahren große Veränderungen in ihren jeweiligen Feldern versprechen.InhaltsverzeichnisUse Cases Computer VisionEinzelhandel: Customer Behavior TrackingLandwirtschaft: Erkennung von Weizenrost mittels Computer VisionGesundheitswesen: Bildsegmentierung von ScansAutomobil Industrie: Objekterkennung und -klassifizierung im VerkehrFitness: Human Pose EstimationZusammenfassungUse Cases Computer Vision1. Einzelhandel: Customer Behavior TrackingOnlineshops wie Amazon können sich die Analysefähigkeit ihrer digitalen Plattform schon lange zunutze machen. Das Verhalten der Kundschaft kann detailliert analysiert und die User Experience dadurch optimiert werden. Auch die Retailbranche versucht die Erfahrung ihrer Kundschaft zu optimieren und ideal zu gestalten. Allerdings haben bisher die Tools gefehlt, um Interaktion von Personen mit ausgestellten Gegenständen automatisch zu erfassen. Computer Vision vermag diese Lücke für den Einzelhandel nun ein Stück weit zu schließen.In Kombination mit bestehenden Sicherheitskameras können Algorithmen Videomaterial automatisch auswerten und somit das Kundschaftsverhalten innerhalb des Ladens studieren. Beispielsweise kann die aktuelle Anzahl an Personen im Laden jederzeit gezählt werden, was sich zu Zeiten der COVID-19 Pandemie mit den Auflagen zur maximal erlaubten Anzahl an Besuchern in Geschäften als Anwendungsgebiet anbietet. Interessanter dürften aber Analysen auf der Individualebene sein, wie die gewählte Route durch das Geschäft und einzelne Abteilungen. Damit lassen sich das Design, der Aufbau und die Platzierung von Produkten optimieren, Staus in gut besuchten Abteilungen vermeiden und insgesamt die User Experience der Kundschaft verbessern. Revolutionär ist die Möglichkeit zum Tracking der Aufmerksamkeit, welche einzelne Regale und Produkte von der Kundschaft erhalten. Spezialisierte Algorithmen sind dazu in der Lage, die Blickrichtung von Menschen zu erfassen und somit zu messen, wie lange ein beliebiges Objekt von Passanten betrachtet wird.Mithilfe dieser Technologie hat der Einzelhandel nun die Möglichkeit zum Onlinehandel aufzuschließen und das Kundschaftsverhalten innerhalb ihrer Geschäfte detailliert auszuwerten. Dies ermöglicht nicht nur die Steigerung von Absätzen, sondern auch die Minimierung der Aufenthaltszeit und optimierte Verteilung von Kunden innerhalb der Ladenfläche.Abbildung 1: Customer Behavior Tracking mit Computer Vision(https://www.youtube.com/watch?v=jiaNA1hln5I)2. Landwirtschaft: Erkennung von Weizenrost mittels Computer VisionModerne Technologien ermöglichen Landwirtschaftsbetrieben die effiziente Bestellung immer größerer Felder. Dies hat gleichzeitig zur Folge, dass diese Flächen auf Schädlinge und Pflanzenkrankheiten überprüfen müssen, denn falls übersehen, können Pflanzenkrankheiten zu schmerzhaften Ernteeinbrüchen und Verlusten führen.Machine Learning verschafft hier Abhilfe, denn mittels des Einsatzes von Drohnen, Satellitenbildern und Remote-Sensoren können große Datenmengen generiert werden. Moderne Technologie erleichtert die Erhebung unterschiedlicher Messwerte, Parameter und Statistiken, welche automatisiert überwacht werden können. Landwirtschaftsbetriebe haben somit rund um die Uhr einen Überblick über die Bodenbedingungen, Bewässerungsgrad, Pflanzengesundheit und lokalen Temperaturen, trotz der großflächigen Bepflanzung von stetig größeren Feldern. Machine Learning Algorithmen werten diese Daten aus. So kann der Landwirtschaftbetrieb frühzeitig anhand dieser Informationen auf potenzielle Problemherde reagieren und vorhandene Ressourcen effizient verteilen kann.Computer Vision ist für die Landwirtschaft besonders interessant, denn durch die Analyse von Bildmaterial lassen sich Pflanzenkrankheiten bereits im Anfangsstadium erkennen. Vor wenigen Jahren wurden Pflanzenkrankheiten häufig erst dann erkannt wurden, wenn sie sich bereits ausbreiten konnten. Basierend auf Computer Vision lässt sich die großflächige Ausbreitung mittels Frühwarnsysteme nun frühzeitig erkennen und stoppen. Landwirtschaftsbetriebe verlieren dadurch nicht nur weniger Ernte, sie sparen auch beim Einsatz von Gegenmaßnahmen wie Pestiziden, da vergleichsweise kleinere Flächen behandelt werden müssen.Besonders die automatisierte Erkennung von Weizenrost hat innerhalb der Computer Vision Community viel Aufmerksamkeit erhalten. Verschiedene Vertreter dieses aggressiven Pilzes befallen Getreide in Ostafrika, rund ums Mittelmeer, wie auch in Zentraleuropa und führen zu großen Ernteausfällen von Weizen. Da der Schädling an Stängeln und Blättern von Getreide gut sichtbar ist, lässt er sich von trainierten Bilderkennungsalgorithmen schon früh erkennen und an der weiteren Ausbreitung hindern.Abbildung 2: Erkennung von Weizenrost mit Computer Vision(https://www.kdnuggets.com/2020/06/crop-disease-detection-computer-vision.html)3. Gesundheitswesen: Bildsegmentierung von ScansDas Potenzial von Computer Vision im Gesundheitswesen ist riesig, die möglichen Anwendungen zahllos. Die medizinische Diagnostik verlässt sich stark auf das Studium von Bildern, Scans und Fotografien. Die Analyse von Ultraschallbildern, MRI- und CT-Scans gehören zum Standardrepertoire der modernen Medizin. Computer Vision Technologien versprechen diesen Prozess nicht nur zu vereinfachen, sondern auch Fehldiagnosen vorzubeugen und entstehende Behandlungskosten zu senken. Computer Vision soll dabei medizinisches Fachpersonal nicht ersetzen, sondern deren Arbeit erleichtern und bei Entscheidungen unterstützen. Bildsegmentierung hilft bei der Diagnostik, indem relevante Bereiche auf 2D- oder 3D Scans erkannt und eingefärbt werden können, um das Studium von Schwarz-Weiß-Bildern zu erleichtern.Abbildung 3: Bildsegmentierung von Lungen CT-Scans(https://syncedreview.com/2020/03/18/ai-ct-scan-analysis-for-covid-19-detection-and-patient-monitoring/)Der neuste Use Case für diese Technologie liefert die COVID-19 Pandemie. Bildsegmentierung kann Ärzt*innen und Wissenschaftler*innen bei der Identifikation von COVID-19 und der Analyse und Quantifizierung der Ansteckung und des Krankheitsverlaufs unterstützen. Der trainierte Bilderkennungsalgorithmus identifiziert verdächtige Stellen auf CT-Scans der Lunge. Anschließend ermittelt er deren Größe und Volumen, sodass der Krankheitsverlauf betroffener Patienten klar verfolgt werden kann.Abbildung 4: COVID-19 Tracking mittels CT-Scans(https://i0.wp.com/syncedreview.com/wp-content/uploads/2020/03/image-67.png?w=1276&amp;ssl=1)Der Nutzen für das Monitoring einer neuen Krankheit ist riesig. Computer Vision erleichtert Ärzt*innen nicht nur die Diagnose der Krankheit und Überwachung während der Therapie. Die Technologie generiert auch wertvolle Daten zum Studium der Krankheit und ihrem Verlauf. Dabei profitiert auch die Forschung von den erhobenen Daten und dem erstellten Bildmaterial, sodass mehr Zeit für Experimente und Teste anstatt der Datenerhebung verwendet werden kann.4. Automobil Industrie: Objekterkennung und -klassifizierung im VerkehrSelbstfahrende Autos gehören definitiv zu den Use Cases aus dem Bereich der künstlichen Intelligenz, denen in letzten Jahren medial am meisten Aufmerksamkeit gewidmet wurde. Zu erklären ist dies wohl eher mit dem futuristischen Anstrich der Idee von autonomem Fahren als den tatsächlichen Konsequenzen der Technologie. Im Grunde genommen sind darin mehrere Machine Learning Probleme verpackt, Computer Vision bildet aber ein wichtiges Kernstück bei deren Lösung. So muss der Algorithmus (der sogenannte «Agent»), von dem das Auto gesteuert wird, jederzeit über die Umgebung des Autos aufgeklärt sein. Der Agent muss wissen wie die Straße verläuft, wo sich andere Autos in der Nähe befinden, wie groß der Abstand zu potenziellen Hindernissen und Objekten ist und wie schnell sich diese Objekte auf der Straße bewegen, um sich konstant der sich stets ändernden Umwelt anpassen zu können. Dazu sind autonome Fahrzeuge mit umfangreichen Kameras ausgestattet, welche ihre Umgebung flächendeckend filmen. Das erstellte Filmmaterial wird anschließend in Echtzeit von einem Bilderkennungsalgorithmus überwacht. Ähnlich wie beim Customer Behavior Tracking setzt dies voraus, dass der Algorithmus nicht nur statische Bilder, sondern einen konstanten Fluss an Bildern nach relevanten Objekten absuchen und diese klassifizieren kann.Abbildung 5: Objekterkennung und Klassifizierung im Straßenverkehr (https://miro.medium.com/max/1000/1*Ivhk4q4u8gCvsX7sFy3FsQ.png)Diese Technologie existiert bereits und kommt auch industriell zum Einsatz. Die Problematik im Straßenverkehr stammt von dessen Komplexität, Volatilität und der Schwierigkeit, einen Algorithmus so zu trainieren, dass auch etwaiges Versagen des Agenten in komplexen Ausnahmesituationen ausgeschlossen werden kann. Dabei entblößt sich die Achillessehne von Computer Vision: Der Bedarf nach großen Mengen an Trainigsdaten, deren Generierung im Straßenverkehr mit hohen Kosten verbunden ist.5. Fitness: Human Pose EstimationDie Fitnessbranche befindet sich seit Jahren im Prozess der digitalen Transformation. Neue Trainingsprogramme und Trends werden via YouTube einem Millionenpublikum vorgestellt, Trainingsfortschritte werden mit Apps verfolgt und ausgewertet und spätestens seit dem Beginn der Coronakrise erfreuen sich virtuelle Trainings und Home Workouts massiver Beliebtheit. Gerade beim Kraftsport lassen sich Fitnesstrainer*innen aufgrund der hohen Verletzungsgefahr nicht aus dem Studio wegdenken – noch nicht. Denn während heute das Überprüfen der eigenen Haltung und Position beim Training via Video bereits gängig ist, ermöglicht es Computer Vision auch in diesem Feld Videomaterial genauer als das menschliche Auge auszuwerten und zu beurteilen.Zum Einsatz kommt dabei eine Technologie, die dem bereits vorgestellten Attention Tracking der Einzelhandelsbranche ähnelt. Human Pose Estimation ermöglicht einem Algorithmus das Erkennen und Schätzen der Haltung und Pose von Menschen auf Video. Dazu wird die Position der Gelenke und deren Stellung im Bezug zueinander ermittelt. Da der Algorithmus gelernt hat, wie die ideale und sichere Ausführung einer Fitnessübung aussehen soll, lassen sich Abweichungen davon automatisiert erkennen und hervorheben. Implementiert in einer Smartphone App kann dies in Echtzeit und mit unmittelbarem Warnsignal geschehen. Somit kann rechtzeitig vor gefährlichen Fehlern gewarnt werden, anstatt Bewegungen erst im Nachhinein zu analysieren. Dies verspricht das Verletzungsrisiko beim Krafttraining maßgeblich zu reduzieren. Training ohne Fitnesstrainer*innen wird dadurch sicherer und die Kosten für sicheres Krafttraining werden gesenkt.Human Pose Estimation ist ein weiterer Schritt in Richtung digitalem Fitnesstraining. Smartphones sind im Fitnesstraining bereits weitgehend etabliert. Apps, die das Training sicherer machen, dürften bei der breiten Nutzerbasis großen Anklang finden.Abbildung 6: Analyse von Bewegungsabläufen in Echtzeit mit Computer Vision(https://mobidev.biz/wp-content/uploads/2020/07/detect-mistakes-knee-cave.gif)ZusammenfassungComputer Vision ist ein vielseitiges und vielversprechendes Feld von Machine Learning. Es verspricht die Lösung einer breiten Palette von Problemen in verschiedensten Branchen und Industrien. Das Verarbeiten von Bild- und Videomaterial in Echtzeit ermöglicht die Lösung von Problemstellungen weit komplexer als mit herkömmlichen Datenformaten. Das bringt den Stand von Machine Learning den «intelligenten» Systemen immer näher. Bereits heute bieten sich immer häufiger alltägliche Schnittstellen zu Computer Vision an – ein Trend, der sich in den kommenden Jahren nur zu beschleunigen scheint.Die hier vorgestellten Beispiele sind nur die Spitze des Eisbergs. Tatsächlich gibt es in jeder der genannten Branchen große Bestrebungen mithilfe von Computer Vision Technologie bestehende Prozesse effizienter zu gestalten. Aktuell gibt es viele Bestrebungen Computer Vision in die dritte Dimension zu heben und anstelle von Fotos und Scans auch 3D-Modelle verarbeiten zu lassen. Die Nachfrage nach industrieller Bildverarbeitung in 3D wächst, sowohl in der Vermessung, der Medizin, wie auch der Robotik. Die Verarbeitung von 3D-Bildmaterial wird in den kommenden Jahren noch Beachtung erhalten, denn viele Problemstellungen lassen sich erst in 3D effizient lösen.Über den AutorOliver GuggenbühlI am a data scientist at STATWORX and love telling stories with data - the ShinyR the better!.e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/die-5-wichtigsten-use-cases-fuer-computer-vision/;Statworx;  Oliver Guggenbühl
  3. November 2020;Whitepaper: KI-Weiterbildung für Führungskräfte;"Management SummaryInnerhalb der vergangenen Jahre ist für viele Unternehmen klar geworden, dass die erfolgreiche Anwendung von künstlicher Intelligenz ein entscheidender Erfolgsfaktor in der Zukunft sein wird. Verbunden mit dieser Erkenntnis stehen Unternehmen heute vor der Herausforderung künstliche Intelligenz für sich nutzbar und skalierbar zu machen. Eine elementare Rolle bei der Meisterung dieser Herausforderung kommt den aktuellen und zukünftigen Führungskräften der Organisationen zu. Sie sind dafür verantwortlich, die durch künstliche Intelligenz einsetzende Transformation zu steuern. Es ist daher unumgänglich, Führungskräfte systematisch auf diese neue Rolle vorzubereiten. Die Aufgaben und Anforderungen, die Führungskräften dabei zukommen unterscheiden sich dabei teils erheblich. Während die obere Führungsebene die KI-Vision des Unternehmens definiert, müssen Mitglieder der mittleren Führungsebene die organisatorischen Rahmenbedingungen für eine erfolgreiche Operationalisierung und Skalierung der Technologie schaffen. Schlussendlich ist die untere Führungsebene für den Erfolg der operativen Anwendung der Technologie verantwortlich.In puncto Weiterbildung müssen somit in jedem Management-Level dedizierte inhaltliche Schwerpunkte gesetzt werden. Zuvor müssen jedoch einheitliche Grundlagen über alle Führungsebenen hinweg geschaffen werden. Hierbei konnten wir in der Vergangenheit die folgenden Themen als besonders relevant identifizieren:Potenziale, die durch die Anwendung von KI heute schon entstehen, aufzeigen und diskutierenEin grundlegendes Verständnis für die Funktionsweise von KI schaffen und aktuelle sowie zukünftigen Trends in diesem Bereich aufzeigenWie sich Anwendungsfälle für den Einsatz von KI imeigenen Unternehmen identifizieren und bewerten lassenÜberblick zu aktuellen Methoden sowie technologischen Lösungen im Bereich KI(Ethische) Auswirkungen und Risiken, die durch den Einsatz von KI entstehen besser einschätzen könnenIm Whitepaper „KI-Weiterbildung für Führungskräfte“ zeigen wir Ihnen anhand der Case Study „AI Basics for Executives“ beispielhaft, wie eine erfolgreiche Umsetzung eines ausdifferenzierten Weiterbildungskonzepts aussehen kann. Hierbei wurde, speziell für Führungskräfte eines international agierenden Konzerns, ein Weiterbildungsformat entwickelt, das die Zielgruppe des mittleren bis gehobenen Managements auf die anstehende KI-Transformation vorbereitet. Das Training wurde sowohl in Deutschland als auch international mehrfach durchgeführt.Sie haben Interesse am Thema KI-Weiterbildung?Hier können Sie sich das gesamte Whitepaper „KI- Weiterbildung für Führungskräfte“ kostenfrei herunterladen.Über den AutorFabian MüllerI am the COO at STATWORX and responsible for our data science teams and key accounts. In my spare time, I'm into doing sports and fast cars..e15819-3.x-column {text-align:left;background-color:transparent;z-index:1;}.e15819-7.x-anchor {width:100%;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchor {width:200px;height:2.75em;margin:20px 0px 0px 0px;border-radius:2px;font-size:1em;background-color:#0085af;box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-8.x-anchor .x-anchor-content {flex-direction:row;justify-content:center;align-items:center;}.e15819-7.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-8.x-anchorclass*=""active"" {background-color:rgba(255,255,255,1);box-shadow:0em 0.15em 0.65em 0em rgba(0,0,0,0.25);}.e15819-7.x-anchor .x-anchor-text {margin:5px;}.e15819-8.x-anchor .x-anchor-text {margin:5px;}.e15819-7.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-8.x-anchor .x-anchor-text-primary {font-family:inherit;font-size:1em;font-style:normal;font-weight:400;line-height:1;color:rgb(255,255,255);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-primary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-8.x-anchor .x-anchor-text-secondary {margin-top:0.35em;font-family:inherit;font-size:0.75em;font-style:normal;font-weight:400;line-height:1;color:rgba(0,0,0,1);}.e15819-7.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-8.x-anchorclass*=""active"" .x-anchor-text-secondary {color:rgba(0,0,0,0.5);}.e15819-7.x-anchor .x-graphic {margin:5px;}.e15819-8.x-anchor .x-graphic {margin:5px;}.e15819-7.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-8.x-anchor .x-graphic-icon {width:1em;height:1em;line-height:1em;font-size:1.25em;color:rgb(255,255,255);background-color:transparent;}.e15819-7.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-8.x-anchor .x-graphic-iconclass*=""active"" {color:rgba(0,0,0,0.5);background-color:transparent;}.e15819-7.x-modal {font-size:16px;}.e15819-8.x-modal {font-size:16px;}.e15819-7.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-8.x-modal .x-modal-bg {background-color:rgba(0,0,0,0.75);}.e15819-7.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-8.x-modal .x-modal-close {width:calc(1em * 1);height:calc(1em * 1);font-size:1.5em;color:rgba(255,255,255,0.5);}.e15819-7.x-modal .x-modal-close:hover,.e15819-7.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-8.x-modal .x-modal-close:hover,.e15819-8.x-modal .x-modal-close:focus {color:#ffffff;}.e15819-7.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-8.x-modal .x-modal-content-inner {padding:calc(1.5em * 1);}.e15819-7.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-8.x-modal .x-modal-content {max-width:28em;padding:2em;background-color:#ffffff;box-shadow:0em 0.15em 2em 0em rgba(0,0,0,0.25);}.e15819-4.x-text {margin:0em 0em 0em 0em;padding:0em;font-size:13px;background-color:transparent;}.e15819-4.x-text .x-text-content-text-primary {margin:0 calc(0em * -1) 0 0;font-family:inherit;font-size:1em;font-style:normal;font-weight:100;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(217,217,217);}.e15819-5.x-line {width:100%;max-width:none;margin:5px 0px 12px 0px;border-width:1px 0 0 0;border-style:solid;border-color:rgba(0,0,0,0.07);font-size:10px;}.e15819-2.x-container {margin:0em auto 0em auto;background-color:transparent;box-shadow:-00em 0em 0em 0em transparent;z-index:1;}.e15819-1.x-section {margin:0em 0em 0em 0em;padding:0px 0px 0px 0px;text-align:left;background-color:transparent;z-index:1;}.e15819-6.x-text {margin:0em 0em 0em 0em;padding:0em;font-family:inherit;font-size:1em;font-style:normal;font-weight:200;line-height:1.4;letter-spacing:0em;text-align:left;text-transform:none;color:rgb(96,96,96);background-color:transparent;}.e15819-6.x-text > :first-child {margin-top:0;}.e15819-6.x-text > :last-child {margin-bottom:0;}ABOUT USSTATWORXis a consulting company for data science, statistics, machine learning and artificial intelligencelocated in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to  blog(at)statworx.com. Sign Up Now! Sign Up Now! .button {  background-color: #0085af;}.x-container.width {    width: 100% !important;}.x-section {    padding-top: 00px !important;    padding-bottom: 80px !important;}";https://www.statworx.com/de/blog/whitepaper-ki-weiterbildung-fuer-fuehrungskraefte/;Statworx;  Fabian Müller
