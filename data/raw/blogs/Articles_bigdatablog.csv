Datum;Titel;Text;Link;Quelle;Autor
;"     ""Was ist Künstliche Intelligenz und was kann sie leisten?""";Um die großen Datenmassen beherrschbar zu machen und Nutzen aus ihnen zu gewinnen, sind mehr als herkömmliche Analysewerkzeuge nötig. Obwohl das Phänomen schon seit vielen Jahrzehnten bekannt ist, gewinnt es erst im Big-Data-Zeitalter seine eigentliche Relevanz. Was ist neu an der jüngsten Euphorie rund um Künstliche Intelligenz, Machine Learning und Deep Neutral Networks? Um das zu verstehen, soll im Folgenden sowohl ein genauerer Blick auf KI und die einzelnen Methoden geworfen werden.Was ist überhaupt Künstliche Intelligenz?Künstliche Intelligenz (KI) wird im Allgemeinen als die Fähigkeit von Maschinen definiert, nicht nur mechanische Vorgänge zu beherrschen, sondern auch komplexe mentale Prozesse vollziehen zu können. Die Entwicklung im Bereich der Künstlichen Intelligenz ist bereits einige Jahrzehnte alt. Einer der in diesem Zusammenhang in der Regel genannten Ursprünge ist der Aufsatz von Alan Turing (1912-1954) „Computing Machinery and Intelligence“ aus dem Jahr 1950. Auch der nach ihm benannte „Turing-Test“ wird immer wieder als Kriterium angeführt, anhand dem beurteil werden kann, ob eine Maschine als wahrhaft „intelligent“ bezeichnet werden kann – doch dazu gleich noch mehr.Warum gerade heute KI so interessant istDass gerade in den letzten Jahren verstärkt über Künstliche Intelligenz gesprochen und sogar ihr Durchbruch gefeiert wird, hängt nicht mit einem vertieften wissenschaftlichen Verständnis von Künstlicher Intelligenz oder von der Entwicklung einer neuen Programmiersprache ab. Vielmehr hängt das hauptsächlich damit zusammen, dass die Rechenleistung von Hochleistungsprozessoren so gut geworden ist, dass viele Billionen Rechenoperationen pro Sekunde durchgeführt werden können. Zur Verdeutlichung: Der „konventionelle“ Prozessor im iPhone X schafft mehr als 600 Milliarden Rechenoperationen pro Sekunde. Gleichzeitig sind die Speichertechnologien so günstig geworden, dass große Mengen von Daten sehr schnell verarbeitet werden können. Diese Kombination aus Rechenleistung und Speicherplatz macht es möglich, Maschinen mit einem hohen Grad an Intelligenz auszustatten.Zur Unterscheidung von „starker“ und „schwacher“ Künstlicher IntelligenzZu Beginn einer Auseinandersetzung mit Künstlicher Intelligenz muss aber zunächst eine weitere wichtige generelle Unterscheidung vorgenommen werden, die vor allem begrifflicher Natur ist. Oft wird zwar der Begriff „Künstliche Intelligenz“ verwendet, aber im Grunde genommen wird über zwei sehr unterschiedliche Dinge gesprochen. Es gibt zwei verschiedene Weisen, „Künstliche Intelligenz“ zu definieren: Eine „starke“ und eine „schwache“. Mit „starker KI“, die manchmal auch als „Superintelligenz“ bezeichnet wird, ist jene Form von Künstlicher Intelligenz gemeint, die den Turing Test besteht. Sie könnte also einen Menschen bei einer Versuchsanordnung – dem Turing-Test –, bei dem weder ein Sicht- noch eine Hörkontakt zu den Gesprächspartner vorhanden ist, davon überzeugen, dass sie eine menschliche Person ist, die ein bestimmtes Geschlecht imitiert.Alternativen zum Turing-TestDer Turing-Test ist nicht unumstritten und so gibt es daneben auch andere Versuche, eine starke KI zu definieren. Dabei wird vor allem betont, dass eine starke KI mindestens den Grad an Intelligenz von Menschen aufweisen müsse oder diese übertreffen müsse. Dabei wird angenommen, dass das menschliche Gehirn mit den unterschiedlichsten Aufgaben sehr gut zurecht kommt. Ähnliches muss für eine KI gelten. Diese kann dann als generell intelligent bezeichnet werden, wenn sie viele Aufgaben gleichermaßen gut beherrscht. Als „schwache KI“ wird hingegen das Vermögen definiert, einzelne kognitive Aufgaben mindestens ebensogut oder besser wie Menschen durchführen zu können.Bislang hat keine KI den Turing-Test zweifelsfrei bestanden und die zahlreichen KI-Lösungen, dienen in der Regel einem ganz bestimmten Zweck. Zwar kann IBMs KI mit dem Namen Watson sowohl so programmiert werden, dass er bei einer Quiz-Show teilnehmen und gewinnen kann. Es ist allerdings ebenso möglich, ihn so zu trainieren, dass er bei der medizinischen Diagnose von MRT-Aufnahmen und der Interpretation von Röntgenbildern helfen kann. Beides kann „er“ aber nicht zur gleichen Zeit.Mensch vs. Maschine: Was ist Intelligenz?Auch die Frage „Was ist Intelligenz“ wird nicht oft genug in Zusammenhängen wie diesen gestellt. Dabei ist es durchaus relevant, sich vor Augen zu führen, welche Aspekte von dem, was als Intelligenz verstanden wird, durch Künstliche Intelligenz abgedeckt werden kann. Intelligenz lässt sich unter zahlreichen Gesichtspunkten betrachten. Als emotionale, kognitive oder als soziale Intelligenz. Sie lässt sich aber auch unter dem Aspekt des Bewusstseins diskutieren. Unabhängig vom Stellenwert der einzelnen Komponenten dürfte evident sein, dass menschliche Intelligenz sehr viel mehr Facetten hat. Die bloße Fähigkeit zur Bewältigung von intellektuellen und kognitiven Prozessen macht uns nicht zu intelligenten Wesen. Der Vergleich Mensch-Maschine in Bezug auf den Begriff „Intelligenz“ hinkt auch noch in einer anderen Hinsicht. Der Lösungsweg einer KI ist ein grundlegend anderer als der eines intelligenten Menschen.Eine KI nutzt eine enorme Rechenpower, um in kürzester Zeit gigantische Datenmengen zu durchforsten. Beispielsweise, um Muster in medizinischen Daten zu erkennen, die auf eine Krebserkrankung hinweisen. Ein Arzt, der dieselbe Aufgabe lösen muss, geht nicht im Geiste alle Fälle und alle Referenzen durch, um schließlich zu einem Ergebnis zu kommen. Vielmehr nutzt er Heuristiken, Erfahrung, Abstraktion und regelhaftes Wissen, um schließlich zu einer Einschätzung zu kommen. Es soll hier nicht gesagt werden, dass das eine besser oder schlechter als das andere ist. Es ist aber wichtig festzustellen, dass es sich um grundlegend verschiedene Herangehensweisen handelt. Und auch etwas anderes gehört zur Wahrheit. Ist ein intelligenter Algorithmus erst einmal trainiert, kann er bestimmte Aufgaben oft sehr viel besser bewältigen als Menschen.Künstlich neuronale Netzwerke, Deep Learning und Machine LearningDer eben im Nebensatz erwähnte Aspekte, dass intelligente Algorithmen „trainiert“ werden müssen, ist zentral. Ganz im Gegensatz zu als anderen Computerprogrammen handelt es sich bei KI-Software nicht um „Out-of-the-box-Lösungen“. Sie werden nicht einmal erstellt und können dann in der Folge wie andere Software immer wieder angewendet werden. Vielmehr handelt es sich um lernfähige Algorithmen, die zunächst trainiert werden müssen und im Laufe ihrer Anwendung immer besser werden. Das macht auch den Charme von bestimmten Geräten und Maschinen aus. Dank Machine Learning werden beispielsweise die Kameras in Smartphones von Google permanent verbessert. Sie lernen mit der Zeit zu verstehen, was auf Bildern zu sehen ist und nehmen entsprechend Optimierungen vor.Dabei gibt es unterschiedliche Lernmethoden wie „künstlich neuronale Netzwerke“, „Deep Learning“ oder „Machine Learning“, um nur die derzeit wichtigsten zu nennen. Auch Machine Learning lässt sich wiederum nochmal nach Lernmethoden unterscheiden. Einerseits in solche Algorithmen auf Basis eines Trainings-Sets selbst optimieren können. Andererseits in solche, bei denen Programmierer beziehungsweise Anwender dem Programm ein Feedback geben, ob bestimmte Ergebnisse richtig sind oder nicht. Je nachdem wie das Feedback ausfällt, kann ein intelligentes Programm Anpassungen für die Zukunft vornehmen. So kann es im Lauf der Zeit immer bessere Antworten geben. Ein entscheidender Aspekt, wenn es darum geht, die Anwendbarkeit und das Potenzial von KI einzuschätzen. ;https://bigdatablog.de/2017/11/13/kuenstliche-intelligenz/;BigDataBlog;Christian Schön
;" Retweets,      ""Smartes Bier ohne Reinheitsgebot? Mit Big Data und Predictive Analytics dem perfekten Bier auf der Spur.""";In Deutschland und insbesondere in der Hauptstadt des Bieres in München ist die Sache mit dem Bier eigentlich klar: Nichts geht hier über das Reinheitsgebot. In Belgien, Schweden oder den USA sind die Brauereien sehr viel experimentierfreudiger und zumindest in Wettbewerben um das beste Bier des Jahres gibt ihnen der Erfolg recht. Die meisten der ausgezeichneten Biere kommen aus diesen Ländern und oft handelt es sich um kleine Mikro-Brauereien, die sich auf eine Marktnische spezialisiert haben.Der Brauprozess ist sehr komplex und durch die Auswertung von Sensordaten und mit der Hilfe von Algorithmen kann der Biergenuss heute verbessert werden. Um das perfekte Bier zu brauen, setzt beispielsweise eine Brauerei aus Portland, Deschutes Brewery, auf Big Data. Ihr Business-Plan sieht vor, pro Jahr verschiedene neue Sorten auf den Mark zu bringen und gleichzeitig die Qualität auf einem konstant hohen Niveau zu halten – und das zum Teil auch bei kleineren Produktionsmengen. Insbesondere für eine kleine Brauerei ist dies ein schwierig zu erreichendes Ziel, das aber dank Predictive Analytics erreicht werden kann.“Mit Big Data zum perfekten Bier: Wie Predictive Analytics hilft, den Brauprozess zu optimieren“ Twittern WhatsApp.Mit Sensordaten und Datenmodellen zum perfekten BierDer Brauprozess wird in etliche unterschiedliche Phasen eingeteilt: Mälzen, Schroten, Maischen, Läutern, Würzekochen, Hopfengabe, Würzeklärung, Würzekühlung, Gärung, Reifung, Filtrieren und schließlich die Abfüllung. Jede dieser einzelnen Phasen hat Auswirkungen auf das Endergebnis – darüber hinaus können auch verschiedene Hopfen- oder Gerstesorten eine charakteristische Note hinterlassen. Jedoch haben insbesondere die Vorgänge, bei denen der angesetzte Sud erhitzt und abgekühlt wird, einen entscheidenden Einfluss auf den Geschmack und die Qualität des Bieres.Mit Sensoren lassen sich die Übergänge zwischen den Vorgängen des Befüllen des Tanks und der Fermentation, der Zugabe der Hefe sowie zwischen dem Abkühlen und dem Reifungungsprozess überwachen. Sie zeichnen dabei zahlreiche Werte wie Temperatur, Druck oder PH-Werte auf. Zunächst werden die Algorithmen mit Testdaten gefüttert, damit sie nach und nach genaue Vorhersagen über die Entwicklung des Brauprozesses machen können. Wenn erst mal ein Modell für den Übergang zwischen zwei Phasen erstellt ist, lassen sich die Erkenntnisse auf andere Prozesse übertragen. Das Ziel ist es, den gesamten Brauprozess einerseits so effizient wie möglich zu machen, um die Kapazitäten der Anlagen voll auszuschöpfen. Andererseits können die Predictive-Analytics-Algorithmen die Qualität des Bieres immer gleich hoch und das Geschmackserlebnis konstant gehalten werden. Da chemische Prozesse nicht immer exakt gleich ablaufen, ist diese Form des Smarten Bierbrauens, das auf Echtzeitanalysen basiert, gegenüber herkömmlichen Methoden im Vorteil.Der Braumeister Brian Faivre von Deschutes Brewery setzt auf Big Data: Neben chemischen Prozessen sind nun auch technische Prozesse fehleranfällig.AI bringt Bierbrauer und Kunden zusammen: Gelebte KundenzentrierungNeben dem Fokus auf den Brauprozess mit Big Data, lassen sich Algorithmen auch dazu verwenden, um Bier dem Geschmack der Kunden anzupassen. Kunden können heute via Smartphone den Unternehmen eine direktes Feedback geben, ob sie mit dem gekauften Produkt zufrieden sind. Ein britischer Bierbrauer macht sich diese Informationen zunutze und passt mithilfe Künstlicher Intelligenz (AI) den Geschmack seines Biers den Wünschen der Kunden an. Gleichzeitig versucht er durch das Feedback die Qualität seines Erzeugnisses permanent zu verbessern:Durch das Einscannen eines individuellen Codes auf den Etiketten kann ein AI-Bot die Bewertungen der Kunden exakt zuordnen und auswerten.“AI und Mashine Learning lassen Kunden direkt mit Brauereien kommunizieren mit einem gemeinsamen Ziel: Besseres Bier“ Twittern WhatsAppNicht nur der Brauprozess kann mit Hilfe von Datenauswertungen verbessert werdenDer Genuss eines Bieres hängt auch vom richtigen Zeitpunkt des Öffnens der Flasche und der perfekten Trinktemperatur ab. Auch hier können Algorithmen helfen: Ab welchem Zeitpunkt verliert ein abgefülltes Bier den charakteristischen Geschmack, für den man es gekauft hat? Dieser Zeitpunkt lässt sich exakt messen und beispielsweise auf dem Etikett angeben. Auch die perfekte Lager- und Trinktemperatur lassen sich so ermitteln und kommunizieren. Vorreiter ist auch hier die Deschutes Brewerey, die nicht mehr das Haltbarkeitsdatum ihrer Biere auf dem Etikett angibt, sondern auch exakt den Zeitpunkt bestimmen kann, ab dem bestimmte Geschmackskomponenten nicht mehr zu schmecken sind.Solche Vorteile lassen sich nicht nur bei Getränken wie Bier nutzen, sondern auch bei Whiskey. Jede Flasche der Marke Johnnie Walker ist heute ein Teil des Internet der Dinge. Für das Unternehmen ergeben sich daraus eine ganze Reihe von Vorteilen – so kann beispielsweise die gesamte Lieferkette überwacht und auf diese Weise der Diebstahl von Waren minimiert werden. Aber auch für den Kunden selbst ergeben sich Vorteile: Er erhält einen genauen Überblick über die Historie der Lagerung jeder einzelnen Flasche. So kann sichergestellt werden, dass beispielsweise die Lagertemperatur immer optimal war.Mit jedem Smartphone lassen sich zahlreiche Daten über den Lebenszyklus der individuellen Flasche und zusätzlicher Content für die Kunden abrufen.Die Digitalisierung aller Lebensbereiche macht auch vor dem Reinheitsgebot nicht haltDer Kontrast könnte größer kaum sein: Auf der einen Seite das Reinheitsgebot und alte Kellergewölbe, in denen Fässer mit Bier, Wein oder Whisky lagern und auf der anderen Seite Sensoren, Algorithmen und Big Data. Die Digitalisierung aller Lebensbereiche kennt kaum Grenzen und bringt zahlreiche Vorteile für die Kunden und für Unternehmen mit sich. Mit dem Wissen über den Brauprozess können beispielsweise kleinere Brauereien wie die Deschutes Brewerey ihr Geschäftsmodell kontinuierlich erweitern: Neue Geschmacksrichtungen lassen sich schneller entwickeln und entsprechend vermarkten. Die Kunst besteht heute darin, geeignete Datenquellen zu finden oder zu schließen, um einerseits Daten zu sammeln und andererseits aus diesen Datenmengen konkretes Wissen zu ziehen. Die Digitalisierung schafft damit etwas, das besonders hierzulande als heilig gilt: Die Verbesserung des Reinheitsgebotes.Wenn wir das aber anders denken wollen, zitiere ich gerne Antje Neubauer, die sagt: „Wir haben als Unternehmen auch eine gesellschaftliche Verantwortung, die wir ernst nehmen, indem wir Haltung zeigen und auf Werte verweisen, die in unserer Demokratie selbstverständlich sein sollten.“ Big Data hat Vorteile, aber die Verantwortung über die Daten und damit verbunden die Werte die ein Unternehmen hat, ist immer ethisch zu durchdenken.;https://bigdatablog.de/2017/02/10/smartes-bier-ohne-reinheitsgebot-mit-big-data-und-algorithmen-dem-perfekten-bier-auf-der-spur/;BigDataBlog;Ibrahim Evsan
09. Jan 17;" Retweets,      ""Big Data und Deep Learning: Wie Maschinen Bilder verstehen können und visuelle Navigation die Welt der Bilder erschließen.""";Bilder sind Zeichen. Diese wichtige Einsicht ist nicht selbstverständlich. Da Bilder auch die Realität abbilden, können wir „verstehen“, was wir auf Bildern sehen, weil es diese Ähnlichkeitsbeziehung gibt. Sehen wir ein Bild von einem mit Gras bewachsenen Hügel, auf dem ein Mensch steht, der den Sonnenuntergang betrachtet, verstehen wir dieses Bild, weil wir Hügel, Mensch und Sonnenuntergängen aus unserer Erfahrung heraus kennen. Aber wie bringt man einem Computer bei, was auf Bildern zu sehen ist, wo ihnen doch sowohl diese Form der Seh-Erfahrung und das Weltwissen als auch unser Bild-Verständnis fehlen? Diese Frage ist alles andere als trivial, weil sie doch darüber entscheidet, ob und wie Unternehmen, die über gigantische Archive verfügen, Bilder verwalten und die darin steckenden Informationen nutzen können.Die Frage ist also, wie es gelingen kann, Computern beziehungsweise intelligenten Programmen beizubringen, was überhaupt auf Bildern zu sehen ist und wie dieser Bildinhalt zu verstehen ist. Es gibt eine ganze Reihe von Bildtypen, die wir problemlos lesen können, und das obwohl es in einigen Fällen den eingangs erwähnten Realitätsbezug nicht gibt oder obwohl die Bedeutung dessen, was ein Bild darstellt, komplexer ist als der reine Ähnlichkeitsbezug. Ein Symbol ist beispielsweise mehr als ein bloßes Abbild eines Gegenstandes. Mit einer Rose oder einem Herz können wir starke Affekte wie Liebe zum Ausdruck bringen und wollen dem Gegenüber nicht nur das Bild einer bestimmten Pflanze oder eines Organs zeigen. Auch Gemälde oder mit Photoshop veränderte Fotografien zeigen nicht mehr einfach nur die Realität, sondern erzählen Geschichten, machen vielleicht Anspielungen auf andere Bilder oder verwenden einen speziellen Code.In Anlehnung an René Magrittes Gemälde „Ceci n’est pas une pipe“ (© Katja Gerasimova @ Shutterstock.com). Können Computer mit Hilfe von Deep Learning den Unterschied zwischen einer Pfeife und dem Bild von einer Pfeife verstehen?Warum Google Bilder findet… und warum nichtDie Bildsuche von Google gibt es zwar seit vielen Jahren. Diese funktionierte aber lange Jahre nur deshalb, weil der Suchalgorithmus von Google die Texte durchsuchte, in denen auch Bilder vorkamen. Die Begleittexte lieferten die gesuchten Stichworte und so kam das Ergebnis zustande. Die Annahme, dass Texte und Bilder zusammengehören und sich gegenseitig erklären, musste als Annahme gegeben sein, damit die Google Bilder-Suche erfolgreich war. Während Google inzwischen auf Deep-Learning-Algorithmen umgestellt hat, basieren Bildsuchen in anderen Bildarchiven großen Teils noch auf der Verschlagwortung von Bildern, sprich: Metadaten. Bildagenturen wie Shutterstock oder 500px lassen ihre Kunden beim Upload die entsprechenden Keywords hinzufügen. Eine solche Kombination aus Text und Bild stellt eine effektive Möglichkeit dar, um mit Bildern umzugehen. Aber diese Methode hat genau an diesem Punkt auch ihre Grenzen: Beispielsweise aufgrund von Homonymen, also Wörtern die etwas unterschiedliches bezeichnen, obwohl sie genau gleich klingen oder geschrieben werden. Sie können zu falschen Suchergebnissen führen. Eine Bildsuche nach „Golf“ kann sowohl zu Ergebnissen beinhalten, die ein Auto der Marke VW enthalten oder die eine bestimmten Sportart darstellen, aber sogar eine Meeresformation zeigen (z.B. „Golf von Mexiko“).Auch Falschschreibungen von den Suchenden selbst können das Suchergebnis verfälschen oder zumindest die Suche erschweren. Nicht nur, dass solche Methoden per se keine adäquate Lösung des Problems darstellt, es stellt sich darüber hinaus die Frage: Was machen wir mit Bildarchiven, die aus vielen Millionen von Bildern bestehen? Diese von Hand durchzugehen und mit Stichworten zu versehen, ist ein Ding der Unmöglichkeit oder zumindest hochgradig ineffizient. Deep Learning, ein Teilaspekt von Machine Learning, basiert auf der Idee von neuronalen Netzwerken. Algorithmen dieser Art können, sind sie erst einmal trainiert, erstaunliche Dinge leisten: “Mit der Hilfe von #DeepLearningAlgorithmen lernen Computer, Bilder zu verstehen. #BigData“ Twittern WhatsAppWie klassifiziert Facebook alle Bilder, die auf die Plattform hochgeladen werden?Auch Facebook arbeitet seit längerem an einem auf künstlicher Intelligenz beruhenden Algorithmus, der dazu in der Lage ist, Bilder zu verstehen. Das Ziel ist es unter anderem die nur visuell wahrnehmbaren Inhalte auch all den Menschen zugänglich zu machen, die blind oder sehbehindert sind. Allein in Deutschland, für das bezeichnender Weise exakte Zahlen fehlen, handelt es sehr wahrscheinlich um weit mehr als 1,2 Mio. Menschen (Stand: 2002).Wie genau und wie umfassend Facebook dabei ist, Bilder erkennen zu lassen, lässt sich mit der frei erhältlichen Extension für Google Chrome nachvollziehen. Mit„Show Facebook Computer Vision Tags“ lassen sich alle Labels sichtbar machen, die Facebook einem Bild automatisch zuordnet:Auch mit dem kostenlosen Online-Tool „akiwi“ ist es möglich, für seine eigenen Bilder solche Keywords zur Verschlagwortung automatisch finden zu lassen. Akiwi entstand als Studentenprojekt im Kontext der Forschung der Hochschule für Technik und Wirtschaft Berlin (HTW), wo man sich seit längerem mit Deep Learning und visueller Navigation sowie Bildanalyse beschäftig. All diese Versuche stehen aber an der Schnittstelle zwischen Bilderkennung und Verfahren zur textbasierten Bildsuche.Bilder als wichtige Datenquelle erschließenInzwischen verfügen nicht nur soziale Netzwerke, Bild- und Medienagenturen sowie Foto-Communities wie Flickr und E-Commerce-Händler über unvorstellbare Mengen an Bilddaten. Nahezu jedes größere Unternehmen hat in seinen Daten-Archiven neben Textdokumenten, Tabellen oder Maschinendaten auch Bilder gespeichert. Angesichts der Unzulänglichkeiten der textbasierten Bildsuche stellt sich die dringliche Frage, wie es möglich ist erstens Algorithmen zu entwickeln, die selbständig Bilder erkennen und zweitens ob es neue Wege gibt, Bilder zu durchsuchen. Darum beschäftigen sich die Forscher an der HTW mit neuen visuellen Verfahren, zur Darstellung von großen Bilddatenbanken und zur Bildsuche. Das Prinzip von Google-Maps aufgreifend stellt Picsbuffet Bilder in Form einer Bilder-Landkarte dar, über die die Nutzer in Vogelperspektive fliegen.Dieses neue Prinzip, Bilder darzustellen, hat gegenüber herkömmlichen Darstellungsformen, bei denen Listen von oben nach unten durchsucht werden müssen, viele Vorteile: Duplikate werden sofort erkannt und ähnliche Bilder in Gruppen dargestellt. Nimmt man das praktische Beispiel E-Commerce, so wird es durch die alternative Darstellungs- und Suchform möglich, ähnliche Produkte auf einen Blick zu finden. Aber nicht nur bei der Produktsuche können mit Deep-Learning-Algorithmen neue Präsentationsformen entwickelt werden. Auch bei der Suche nach urheberrechtswidrig gebrauchten Bildern wäre eine auf den Prinzipien von Deep Learning basierte Bildsuche überlegen.Deep Learning: Maschinen lernen, Bilder zu sehen und zu verstehenEntwicklungen in diesem Bereich sind darum so wichtig, weil sie einen wichtigen Bestandteil beim Autonomen Fahren spielen. Bilderkennung ist insofern wichtig, als dass autonom fahrende Autos Bilder wie Verkehrszeichen sehen und richtig kategorisieren können müssen. Sobald die Deep-Learning-Algorithmen einmal trainiert sind, leisten sie erstaunliches. Das „German Traffic Sign Recognition Benchmark“ des Instituts für Neuroinformatik der Universität Bochum übertraf schon vor vielen Jahren die Leistung von Menschen bei der selben Aufgabe bei weitem. Doch nicht nur weil Deep-Learning-Algorithmen exakter und schneller arbeiten, sind sie in Zukunft unverzichtbar. Allein durch die schiere Menge der Datenmassen ist es notwendig, Maschinen das Sehen und Verstehen von visuellen Informationen beizubringen. Deep-Learning-Netzwerke schaffen die Grundlage zur Verwaltung von Big Data, erschließen das wirtschaftliche Potenzial, das in Bilddaten steckt, und stellen einen wichtigen Baustein für weitere künftige Entwicklungen dar. Während heute daran gearbeitet wird, die Mensch-Maschine-Kommunikation mittels natürlicher Sprache zu optimieren, wird es in Zukunft möglich sein, dass Maschinen ihre Umwelt sehen und interpretieren können.;https://bigdatablog.de/2017/01/09/big-data-und-deep-learning-wie-maschinen-bilder-verstehen-koennen-und-visuelle-navigation-die-welt-der-bilder-erschliessen/;BigDataBlog;Christian Schön
24. Okt 16;" Retweets,      ""Rückblick auf das HPE Event „Reimagine 2016“ in Stuttgart.""";"Am 29.09. fand das HPE Event „Reimagine 2016“ statt und die Veranstaltung war ein großer Erfolg. Ich habe viele neue Eindrücke über den aktuellen Stand der Diskussion zum Thema „Digitale Zukunft in Deutschland“ gewinnen und interessante Gespräche dazu führen können. Es war eine perfekt ausbalancierte Mischung aus Praxisnähe und Zukunftsvision. Jeder Besucher konnte einen klaren Blick auf die Zukunft bekommen und bekam vorgeführt, wie der Weg dorthin konkret zu beschreiten ist. Denn eines ist mir nach der Reimagine 2016 klarer denn je: An der umfassenden Digitalisierung der Wirtschaft und der Gesellschaft führt in Zukunft kein Weg mehr vorbei.Gerade deswegen müssen wir heute damit anfangen, uns den neuen Herausforderungen zu stellen. Dass diese Zukunft schon zum Greifen nahe ist, wurde mir bei der Veranstaltung einmal mehr deutlich. Denn viele der nötigen Lösungen und Produkte liegen bereits bereit und müssen nur noch angewendet werden. Insbesondere einige der einfachen und offenen HPE-Systeme haben mich hier nachhaltig beeindruckt. Die digitale Zukunft ist leicht, drahtlos und cloud-basiert. Der Fokus auf Cloud-Technologien zeigt, dass zwei der wichtigsten Trends der vergangenen Jahre ein fester Bestandteil der Geschäftswelt von morgen sind: Das Thema „Mobile“ und das Internet of Things.Meg Whitmans Opening Talk bei der HPE Reimagine 2016.Digitalisierung jetzt! Alle Unternehmen sind betroffen#Reimagine2016 Meg Whitman: Jedes Unternehmen (egal ob Gesundheit o. Finanzen) sind Technologiefirmen. Das muss verstanden werden. – Stimmt.— Ibrahim Evsan (@Ibo) September 28, 2016Die Opening Keynote von HPE Präsidentin und Chief Executive Officer von HPE Meg Whitman, eine echte Persönlichkeit, war wie zu erwarten fulminant und inspirierend. Sind fasste die aktuelle Lage in deutliche Worte: Alle Unternehmen müssen digital werden. Dabei ist es ganz gleich, in welchem Bereich sie verortet sind, welche Produkte oder Dienstleistungen sie verkaufen und wie sehr die einzelnen Prozesse analog oder physikalisch sind. Kein Unternehmen, dass sich dieser Herausforderung verweigert, hat realistische Überlebenschancen auf dem Markt. Ich denke, dass sich die ersten Konsequenzen dieser Einschätzung bereits heute zeigen.„Wir müssen uns fokussieren, um die Produkte zu verbessern, Geld zu sparen &amp; und genau im Fokus zu investieren.“Meg WhitmanDas Interview mit Meg Whitman vermittelt einen guten Eindruck von ihrer Persönlichkeit.“Rückblick auf die „HPE Reimagine 2016“: Alle Unternehmen müssen digital werden. #Reimagine2016“ Twittern WhatsAppWer nicht innoviert, verschwindetEine der größten Schwierigkeiten der Firmen besteht heute darin, mit dem Takt der Innovation und der Entwicklung der Gesellschaft Schritt zu halten. Seit es Smartphones gibt, sieht man kaum noch jemanden mit einer Digitalkamera – wohlgemerkt einem bereits digitalisierten Produkt. Aber: Es gibt auch keine Digitalkamera, mit der ich meine Fotos direkt auf Instagram oder meine Videos auf Snapchat posten kann. Und während traditionelle Autohersteller alle paar Jahre ein neues Modell auf den Markt bringen, integriert Tesla Innovationen permanent in die aktuelle Produktion. Das Thema „Digitaler Darwinismus“, das hier anklingt, wurde von Karl-Heinz Land überzeugend vorgetragen. Wer nicht innoviert, sprich: wer es nicht schafft, sich den aktuellen Umweltbedingungen anzupassen, wird auf kurz oder lang vom Markt verschwinden.Hier ein anderer Vortrag, in dem Karl-Heinz Land seine Thesen ausführlich erklärt.Über die Notwendigkeit eines einheitlichen Cloud-Systems#Reimagine16 Patel: es gibt 10.000 Clouds. Deswegen heißt die Zukunft: Cloud for all. Einfache Nutzung  aller Clouds. #OrchestrationLayer— Ibrahim Evsan (@Ibo) September 28, 2016Sehr spannend waren für mich auch die Einschätzung bezüglich der aktuellen Entwicklungen im Bereich Cloud-Technologie von Ferhad Patel, EMEA Sales Director der Global Datacenter Group Intel. Im Moment lagern die Daten von Unternehmen in zu vielen unterschiedlichen Systemen, die nur zum Teil miteinander kompatibel sind.Fahed Patel bei der HPE Reimagine 2016: Wir brauchen ein einheitliches Cloudsystem.Das wird dann zu einem Problem, wenn es zu Kollaborationen kommt und genau das wird in Zukunft immer öfter passieren (müssen). Die Entwicklung einer einheitlichen Cloud und von gemeinsamen Standards ist erstrebenswert. Nicht nur, weil dies durch die Globalisierung erforderlich wird, sondern weil durch die digitale Vernetzung offene Netzwerkstrukturen entstehen. Das iPhone wurde erst dann ein Mega-Erfolg, als Apple anderen Entwicklern erlaubte, Apps für das iOS-System zu entwickeln. Eines der absoluten Highlights der Veranstaltung für mich war darum das Projekt Cloud28+: Eine offene Cloud-Lösung für Europa – im Grunde genommen bereits die Realisierung der Idee einer „Cloud der Clouds“. Cloud28+ garantiert Datensicherheit, fördert die Zusammenarbeit und beschleunigt die digitale Transformation.Der Weg in die digitale Zukunft führt über Apps und APIsAngesichts der großen Herausforderungen stellt sich die Frage, was die konkreten Schritte sind, um erfolgreich den Weg in die digitale Zukunft zu beschreiten. Einer der zentralen Schlüssel dazu, liegt in der Gestaltung der Schnittstellen – sprich Apps und APIs (application programming interface, wörtlich ‚Anwendungs­programmier­schnittstelle‘). Der Grund dafür liegt auf der Hand: Die Menschen, die mit den neuen Technologien umgehen, kommen aus allen nur denkbaren, unterschiedlichen Berufsgruppen. Ein entscheidendes Kriterium, das über Erfolg und Misserfolg entscheidet, ist die Anwendbarkeit. Je leichter Apps und APIs zu bedienen sind, desto leichter lässt sich die digitale Transformation in einem Unternehmen umsetzen. Eine bei der Veranstaltung in Stuttgart immer wieder erwähnte Größe in diesem Bereich ist Aruba und deren Geschäfts-Apps, die Grundbedingungen für den Arbeitsplatz der Zukunft schaffen. Die vielen Beispiele zeigten ganz eindeutig, dass die einfache Bedienbarkeit und die Vielfalt der verfügbaren Apps nicht einfach nur Mittel zum Zweck ist: Sie sind ein Ausdruck für die Vielfalt der Anwendungsmöglichkeiten und Chancen, die die digitale Zukunft bereithält.Von Start-ups lernen: Innovations- und Erfolgsmodelle für eine erfolgreiche digitale Transformation.";https://bigdatablog.de/2016/10/24/rueckblick-auf-das-hpe-event-reimagine-2016-in-stuttgart/;BigDataBlog;Ibrahim Evsan
16. Sep 16;" Retweets,      ""Big Data, das IoT und die digitale Transformation: Die Top-Themen der Zukunft auf dem HPE Event Reimagine 2016 in Stuttgart.""";"Daten gelten als das neue Öl des kommenden Jahrhunderts. Wie beim Öl ist es jedoch auch bei den Daten so, dass der eigentliche Wert dann entsteht, wenn der Rohstoff richtig verarbeitet wird. Erst in seiner raffinierten Form wurde das Öl zum Treibstoff und Wachstumsmotor unserer modernen, industrialisierten Welt. Big Data, Advanced Data Analytics und digitale Technologien wie Cloud Computing werden diese Funktion bei dem Umgang mit Daten übernehmen. Die Algorithmen werden in allen Geschäftsbereichen bis hin in die Führungsetage Einzug halten und zu einer neuen Form des Wirtschaftens führen.Big Data ist eines der Expertengebiete, die von HPE – Hewlett Packard Enterprise bei der HPE Reimagine 2016 präsentiert werden.Neue Technologien sind nichts ohne Expertise und StrategieImmer wieder erfahre ich, dass der richtige und gewinnbringende Umgang mit Big Data eine große Herausforderungen für Unternehmen darstellt. Daten werden in allen unternehmerischen Bereichen erhoben, was dazu führt, dass unter Umständen bestehende Strukturen infrage gestellt werden oder völlig neu organisiert werden. In den meisten Fällen können Unternehmen dies nicht aus sich selbst heraus leisten und brauchen entsprechend Partner und Experten an ihrer Seite. Am 28.09.2016 treffen in Stuttgart Experten aus unterschiedlichen Branchen aufeinander, um gemeinsam im Rahmen der HPE Reimagine 2016 unter dem Motto „Accelerating next“ über die Chancen der Zukunft zu sprechen. Eine ideale Gelegenheit, um mit IT-Experten, Visionären und Strategen über neue Ideen, Herausforderungen und konkrete Lösungswege ins Gespräch zu kommen.“#BigData Event-Tipp: Die #HPE Reimagine 2016 am 28.09.2016 in Stuttgart.“ Twittern WhatsAppBig Data und das Internet der Dinge als ZukunftstechnologienEine Zeit lange wurde Big Data als ein kurzlebiges Buzzword der Nullerjahre beschrieben. Ich denke, dass spätestens seitdem das Internet der Dinge seinen Siegeszug angetreten hat, klar ist, dass auch Big Data eine der Schlüsseltechnologien der kommenden Jahrzehnte sein und bleiben wird. Unter anderem darum lohnt es sich, das HPE Reimagine 2016 Event zu besuchen, weil genau dort Vordenker wie Karl-Heinz Land, Meg Whitman – die Präsidentin von Hewlett Packard Enterprise, einem der führenden Technologieunternehmen – oder IoT-Experten Volkhardt Bregulla zusammentreffen. Eine hochspannende Mischung, von der ich mir viele neue Einsichten verspreche. Ich denke, dass jedes Unternehmen, das innovativ sein will und sich an der Spitze des Feldes positionieren möchte, von einem Austausch mit Experten wie diesen profitieren kann.Die vernetzten Dinge des Internet of Things wie der Connectecd Car werden uns eine wahre Flut von Daten bescheren.Big Data, IoT und das Potenzial neuer GeschäftsmodelleDurch die Explosion der Datenmenge verändert sich bereits heute die Unternehmens- und Führungskultur in vielen innovativen Firmen und Betrieben. Ich bin immer wieder fasziniert von der Innovationskraft, der Beschleunigung und Agilität, die mit Big Data und Advanced Data Analytics zu erreichen ist. Selbst weltumspannende Großkonzerne wie Bosch oder Technologieriesen wie Google können mit den Innovationen kleiner Start-ups mithalten. Nicht selten liefern die Erkenntnisse aus Big Data Analysen die Grundlage für Entscheidungen im Management und den Mitarbeitern auf verschiedenen Ebenen. Auch HPE hat die Themen Big Data und Internet of Things fest im Programm der Reimagine 2016 verankert: Connected Car, Smart Bike &amp; Connected Factory mit dem Fraunhofer IPA (Institut für Produktionstechnik und Automatisierung) wird sicher eines der Highlights des Events.“#BigData und #IoT sind die Top-Zukunftsthemen auf der HPE Reimagine 2016 in Stuttgart.“ Twittern WhatsAppPersönlicher Kontakt und individuelle Beratung machen den UnterschiedNeben der Möglichkeit persönlich mit namhaften Experten in Kontakt zu treten, halte ich das Transformationsprogramm der HPE Reimagine 2016 für eine ganz besondere Gelegenheit für Gründer, Unternehmer und Manager. Es besteht aus “Transformation Workshops“ und einem „Transformation Advisor Programm“. Im Rahmen dieses Programms bietet sich die Möglichkeit, 25 Minuten lang exklusiv und 1-to-1 mit Technologen und Strategen über die eigene Unternehmensstrategie auszutauschen und beraten zu lassen. Der Kontakt zu erfahrenen IT-Experten und Beratern kann letztlich den entscheidenden Unterschied ausmachen. Ich selbst werde auch die Gelegenheit nutzen, am 27.09. die HPE Reimagine 2016 zu besuchen und würde mich sehr freuen, dort viele bekannte und viele neue Gesichter zu sehen.Bis zum 27.09.2016 haben Sie hier die Möglichkeit sich zu registrieren.Zum HPE Reimagine 2016 Event.";https://bigdatablog.de/2016/09/16/3248/;BigDataBlog;Ibrahim Evsan
18. Aug 16;" Retweets,      ""Size matters: Big Data, kommt es wirklich auf die Größe an?""";Der Wert von großen Datenmengen ergibt sich aus ihrer Nützlichkeit. Unternehmen und Organisationen stellen immer häufiger fest, dass sie ihre erfassten und gespeicherten Daten in wertvolles Wissen verwandeln können. Wenn Daten richtig ausgewertet werden, dann erhält man Informationen über die Produktion, die Mitarbeiter, den Vertrieb oder das Verhalten der Kunden. Um betriebliche Abläufe besser zu verstehen und in Zukunft bessere Entscheidungen zu treffen, begannen Unternehmen große Datenmengen zu sammeln und auszuwerten. Die Supermarktkette Wal-Mart verfügte eine gewisse Zeit lang über die zweitgrößte Datensammlung der Welt. Um den schwammigen Begriff “Big Data” genauer zu definieren, bestimmte IBM vier Faktoren, die auch die „vier Vs“ genannt werden: Volume (Datenvolumen),Velocity (Verarbeitungsgeschwindigkeit), Variety (Datenvarietät) und Veracity (Richtigkeit der Daten) Drei der vier Vs sind Faktoren, die mit der tatsächlichen Menge von Daten korrelieren. Insofern ist die Frage naheliegend: Wie groß sind diese Datenmengen, um von Big Data sprechen zu können?“3 von 4 Faktoren der Def. von #BigData hat mit der Datenmenge zu tun. Wie wichtig ist Größe wirklich?“ Twittern WhatsAppWie groß ist wirklich groß?Wie groß muss ein Set an Daten sein, um als Big Data zu gelten? Mehrere hundert Gigabyte? Ein Terabyte? Ein Petabyte? Diese Frage lässt sich auf diese Weise nicht konkret und ein für alle Mal beantworten. Egal wo eine Grenze festgesetzt werden würde, ab der Big Data “Big” ist, sie würde nur für eine kurze Zeit gelten. Was wir noch vor kurzer Zeit als sehr groß empfunden haben, erscheint wenige Geräte-Generationen als klein.Die Gründe dafür sind vielfältig: Speichermedien waren nicht von Anfang an schnell, billig und leicht herzustellen. Erst in den letzten 10 Jahren wurden hier riesige Fortschritte erzielt und entsprechend vermehrt Daten gespeichert. Auch die Performance von Prozessoren spielt eine entscheidende Rolle. Wenn ein Superrechner sehr große Datenmengen in kürzester Zeit verarbeiten kann, erscheint die Datenmengen nicht mehr als besonders „groß“. Größe ist in diesem Bereich also sehr relativ und nicht unbedingt das entscheidende Kriterium für „Big Data“.Big Data bezeichnet Datenmengen, diezu groß oderzu komplex sind odersich zu schnell ändern oderzu schwach strukturiert sind.Big Data als MethodeDer Begriff Big Data wird mit vielen verschiedenen Aspekten assoziiert, so dass in der Diskussion über Big Data manchmal unklar ist, worum es eigentlich geht. Nimmt man zunächst die wörtliche Bedeutung von Big Data, so handelt es sich um sehr große, komplexe Datenmengen. Sein negatives Image erhielt der Begriff im Zusammenhang mit Datenschutzproblemen und dem Aspekt der Kontrolle, die Unternehmen und staatliche Organisationen ausüben können. Big Data ist aber auch eine bestimmte Methode, um mit Daten umzugehen. Unabhängig von der tatsächlichen Datenmenge lassen sich diese Methoden auch auf verhältnismäßig kleinere Datenmengen anwenden.“Nur eine genaue Vorstellung von #BigData erlaubt auch einen sinnvollen Diskurs darüber.“ Twittern WhatsAppNimmt man beispielsweise eine bestimmte Menge von Textdaten. Im Rahmen von Big-Data-Analysen geht es in erster Linie gerade nicht darum, den konkreten Inhalt dieser Textdokumente auszuwerten. Die Fragestellungen, mit der Big-Data-Analysten an Daten herangehen, verfolgen im Regelfall die Absicht, ein bestimmtes Muster zu erkennen. Bei der Sentimentanalyse, geht es etwa um die Auswertung von Millionen von Tweets. Die Absicht könnte beispielsweise sein herauszufinden, wie bei einer kommenden Wahl ein Kandidat wahrscheinlich abschneidet. Gibt es Themen, die besonders gut oder besonders schlecht ankommen? Um solche Fragen zu beantworten wird nun nicht jeder einzelne Tweet gelesen. Die Gesamtheit der Tweets wird nach der Anzahl von bestimmten Textmustern durchsucht, um sagen zu können, welche Stimmung bei den Menschen die vorherrschende ist.Das Internet der Dinge wird Big Data in den Schatten stellenPredictive Maintenance: Ein Kernstück der Industrie 4.0. | Big Data BlogPredictive Maintenance sagt wahrscheinliche Ausfälle vorher: Sensoren registrieren kleinste Veränderungen in Getrieben, Leitungen oder Werkteilen.bigdatablog.de Ganz egal wie große das Volumen von Daten im Moment auch wird und wo die Grenze zwischen Big Data und „Small Data“ gezogen wird. Die Größenordnungen werden sich in den kommenden Jahren durch das Internet der Dinge massiv verschieben. Unter dem Begriff des Internets der Dinge, manchmal auch Internet of Everything genannt, wird die vollständige Vernetzung von wirklich allem möglichen. Angefangen von der Kuh, die einen Chip im Ohr trägt, sämtliche Werkteile in einer Produktionsstraße, Verbindungsdaten, Kühlschränke in der Küche, Rasierklingen, Sensoren im Fußboden, die Temperatur und Feuchtigkeit messen, bis hin zu Verkehrsampeln und Messsystemen im Abwassersystem in der Smart City. Sieht man für einen Moment von der Diskussion über Datensicherheit und Kontrolle kurz ab und betrachtet nur den technischen Aspekt, so ist jetzt schon klar: Es wird eine unvorstellbare Menge an Daten anfallen. Wir verfügen im Moment nicht einmal über die passenden Vergleichsmaßstäbe, um die Datenmenge, über die wir verfügen werden, zu bemessen. Entsprechend drängend ist die Fragen nach der Entwicklung der Rechenleistung und der Speichermedien. Vielleicht war es verfrüht, den Begriff „Big Data“ einzuführen, wenn man in naher Zukunft auf die heutigen Datenmengen zurückblicken wird. Denn soviel steht heute schon fest: Die eigentliche Ära von Big Data steht uns erst noch bevor. Big Data ist auch Thema auf deiner Veranstaltung? Wir können dir noch einige Speaker empfehlen.Mehr erfahren;https://bigdatablog.de/2016/08/18/size-matters-wie-gross-ist-big-data-und-kommt-es-wirklich-auf-die-groesse-an/;BigDataBlog;Ibrahim Evsan
21. Jul 16;" Retweets,      ""Predictive Maintenance: Ein Kernstück der Industrie 4.0.""";Um die Instandhaltung aller Geräte, Maschinen, Bauelemente oder allgemein gesagt technischen Geräten sicherzustellen, wurde bislang auf regelmäßige Wartung und Inspektionen gesetzt. Im Zeitalter der Industrie 4.0 werden diese Vorkehrungsmaßnahmen ergänzt und wesentlich erweitert durch: Predictive Maintenance.Das Versprechen, das sich mit Predictive Maintenance – der vorausschauenden Wartung – verbindet, gleicht einem Paradigmenwechsel: Anstatt auf einen Ausfall oder Störungen nur im Nachhinein reagieren zu können, befähigt Predictive Maintenance Unternehmen mögliche Defekte zu erkennen, bevor sie tatsächlich eintreten. Dazu werden immer mehr Produktionsanlagen, Maschinenparks und andere Betriebsmittel mit Sensoren ausgestattet. Mit unterschiedlichen Technologien wie etwa Ultraschall wird den Maschinen zu Leibe gerückt.“Heute repariert man Dinge nicht mehr wenn, sondern bevor sie kaputt gehen. #PredictiveMaintenance“ Twittern WhatsAppPredictive Maintenance ist mehr als nur eine neue Form der WartungEiner der einfachsten Anwendungsfälle von Predictive Maintenance wäre die Überwachung einer Produktionsanlage zum Beispiel mit einem Vibrations- oder Temperatursensor. Sobald die Vibrationen unregelmäßig bzw. zu schnell, oder die Temperaturen im inneren der Maschine so hoch werden, dass sie möglicherweise bald ausfällt, wird dies frühzeitig erkannt. Wenn eine Produktionsmaschine ausfällt, verursacht dies zum Teil hohe Folgekosten, da es dadurch zu Engpässen und Ausfällen an anderen Stellen kommt.Hier ein Beispiel für einen Vibrationssensor: Predictive Maintenance kann aber mehr. Entscheidet sich ein Unternehmen wie beispielsweise der Hersteller einer Fußbodenheizung für das Monitoring von Geräten und Werkteilen, so wird nicht jedes einzelne mit Sensoren ausgestattete Element isoliert betrachtet. Ein Hersteller erhält vielmehr ein Gesamtbild vom Gebrauch seines Produktes. Liefert ein Sensor auffällige Daten, etwa einen dauerhaft zu niedrigen Druck in einem Heizungsrohr, muss das nicht zwangsläufig bedeuten, dass es sich um einen Defekt an dieser Stelle handelt. Die Ursachen können ebenso vielfältig sein, wie die Umwelteinflüsse und der Gebrauch durch die Nutzer. Rohre rosten, falsche Benutzung beansprucht ein Element zu stark, etc. Die Vernetzung und Verknüpfung aller Sensordaten macht Predictive Maintenance erst zu einem machtvollen Tool.Umgebungsdaten: Wenn das Wetterdaten zum Wirtschaftsfaktor wirdPredictive Maintenance macht jedoch nicht bei den Maschinen und Bauteilen halt. Ebenso wie die Wirtschaft als Ganzes ein komplexes Gefüge ist, hängen im einzelnen Unternehmen die Dinge zusammen. Ein Windpark ist unter einer technischen Perspektive betrachtet eine Ansammlung von Rotorenblättern, Generatoren, Bremsen und Getriebe, die allesamt mit Sensoren überwacht werden können. Der Ausfall von Windkraftanlagen hängt jedoch auch sehr stark mit Einflüssen von Außen zusammen. Vogel- und Fledermausschläge verursachen ebenso Störfälle wie starke Windböen, Vereisung oder Blitzschläge. Ein umfassend durchgeführtes Predictive Maintenance darf auch Wetterdaten und Umweltforschung nicht außen vor lassen.“Wirtschaft als Teil eines Ganzen. #PredictiveMaintenance macht das Wetter zum Wirtschaftsfaktor“ Twittern WhatsAppPredictive Maintenance folgt dem Dreiklang: Erfassen, Bewerten und VorhersagenTeure Reparaturen vermeiden, Ausfälle vorhersehen und vorbeugende Maßnahmen ergreifen – all das leistet die vorausschauende Instandhaltung. Der Dreiklang:Digitales Erfassen der DatenBewertung und Analyse der DatenVorhersagen der wahrscheinlichen Ereignissestellt den passiven, traditionellen Hergang der Abläufe auf den Kopf. Predictive Maintenance ist damit ein Herzstück der Industrie 4.0 und damit eine große Chance für die Wirtschaft. Im Zeitalter der Digitalisierung und Vernetzung hat der Blick in die Zukunft nichts mehr mit dem Blick in eine Kristallkugel zu tun. Predictive Analytics schafft handfestes Wissen von der Zukunft.;https://bigdatablog.de/2016/07/21/predictive-maintenance-ein-kernstueck-der-industrie-4-0/;BigDataBlog;Ibrahim Evsan
21. Jun 16;"Zukunft und Trends der digitalen Welt,  Retweets,      ""Big Data in der Medizin""";Nehmen wir eine Gruppe von 100 Menschen, die an Diabetes (meist Diabetes mellitus Typ II) erkrankt sind. Wieviele Gemeinsamkeiten hätte diese Gruppe von Erkrankten wohl? Sie würde sich sehr wahrscheinlich so stark unterscheiden, dass sich kein stimmiges Gesamtbild ergibt. Sie könnten sich hinsichtlich ihres Ernährungs- und Lebensstils, ihres sozialen Umfeldes, ihres Alters oder ihrer genetischen Disposition so stark unterscheiden, dass ihre einzige Gemeinsamkeit die Diabetes-Erkrankung wäre. Die Ursachen für die Erkrankung sind vielfältig und ihre weltweite Ausbreitung ist enorm. In Europa sind etwa 10% der Bevölkerung an Diabetes erkrankt und Schätzungen gehen von etwa 500 Mio Erkrankten weltweit in 2030 aus. Was hat Big Data mit dieser Form von Erkrankung und der Medizin im allgemeinen zu tun?Die Bekämpfung einer Volkskrankheit mit Big-Data-AnalyticsDiabetes wurde als eine der Volkskrankheiten des 21. Jahrhunderts genannt. Der Handlungsbedarf ist mehr als gegeben. Um diese Stoffwechsel-Erkrankung erfolgreich bekämpfen zu können, müssen zunächst ihre genauen Ursachen genau erkannt und beschrieben werden. Auch Erkenntnisse über den Behandlungsverlauf können zur Bekämpfung der Krankheit enorm hilfreich sein. Bei beiden Aufgaben können die Analysemethoden von Big Data helfen. Eine der Stärken von Big Data ist es, Muster und Zusammenhänge in großen, unübersichtlichen Mengen von Daten zu verschaffen. Diese Datengrundlage muss jedoch zunächst erst einmal geschaffen werden. Kosten und Nutzen einer datenbasierten Forschung Einer Schätzung des Robert-Koch-Instituts zur Folge, leben in Deutschland im Moment 1.3 Mio. Menschen, die mit einer unerkannten und unbehandelten Diabetes-Erkrankung leben. Angesichts der ohnehin rasant steigenden Zahl von Neuerkrankungen, ist dies einer der wichtigsten Anlässe zur besseren Erforschung der Krankheit. Das DDZ (Deutsches Diabetes Zentrum) in Düsseldorf ist das derzeit einzige überregional vernetzte Forschungszentrum, das sich um die systematische Erforschung der Stoffwechselkrankheit kümmert. Untersucht werden hier die genetischen, biochemischen und biometrischen Ursachen sowie die individuellen Verläufe von Diabetes-Erkrankungen. Einen Plan für eine systematische, datenbasierte Analyse der Krankheit fehlt jedoch im Moment. Dabei lägen Daten bereits in unterschiedlichsten Registern und Formen vor – etwa bei Krankenkassen oder in amtlichen Statistiken. Würden diese Daten zu einem Data Lake zusammengeführt, könnten diese Daten mit Patientendaten angereichert werden. Angesichts der Kosten, die Diabetes im Moment verursacht, stünde dieses Vorhaben dazu in keinem Verhältnis: 20% der Gesamtausgaben aller gesetzlichen Krankenkassen werden für die Behandlung von Diabetes und damit verbundenen Folgeerkrankungen ausgegeben. Nimmt man alle direkten Kosten der Krankheit zusammen, ergibt sich die stolze Summe von 35 Milliarden Euro pro Jahr.“#BigData in der #Medizin – Patientendaten sind das Schlüsselelement in einem #BigDataLake.“ Twittern WhatsAppDie Rolle der Daten in der MedizinDie Datenerhebung und Datenverarbeitung gehören zu einem der zentralen Bestandteil der Medizin. Seit jeher bestimmt die Medizin der Dreiklang, der aus Anamnese, Diagnose und Prognose besteht. Bei der Anamnese geht es darum, alle relevanten Informationen über einen Patienten zu erkennen und aufzunehmen (Datenerhebung). Bei der Diagnose geht es darum, in diese Informationen ein bekanntes Muster zu erkennen, das einem der bekannten Krankheitsbild entspricht (Datenverarbeitung). Die Prognose wiederum trifft die Aussagen über den zukünftigen Verlauf einer Krankheit (Datenauswertung).Prognostik früher und heuteDigital Humanities: Big Data in den GeisteswissenschaftenBig Data und computerbasierte Analysemethoden finden längst in den Geistes- und Kulturwissenschaften Anwendung. Über die Zukunft der Digital Humanities.bigdatablog.deIn vielen Anwendungsfällen der Medizin gehören bereits heute datenbasierte Methoden zum festen Bestandteil der täglichen Praxis in der Klinik – zum Beispiel bei der 3D-Mammographie oder bei Haut-Scans zur Krebsvorsorge. Daten spielen an drei verschiedenen Stellen des Prozesses von Anamnese, Diagnose und Prognose eine Rolle: Erstens bei der Messung, zweitens bei der Visualisierung und Animation und drittens beim Vergleich mit anderen Patientendaten. Patientendaten sind der zentrale Bestandteil eines Big-Data-Lakes und können bei der Diagnose und Prognose einen unschätzbaren Dienst erweisen. Insbesondere bei Krankheiten wie dem Diabetes, dessen Ursachen, Verläufe und Folgeerkrankungen so vielfältig sind, könnten die Methoden von Big-Data-Analytics ihre Stärke voll ausspielen. Am Ende einer Strategie, die zur datenbasierten Erforschung führen, können praktische Anwendungen im Rahmen von Gesundheits-Tracking stehen. Jeder Besitzer einer SmartWatch oder eines Tracking-Armbands hätte die Möglichkeit, seine Vitaldaten mit einer großen Diabetes-Datenbank abzugleichen. Mit einer aussagekräftigen Datengrundlage würde auch eine Grundlage für diejenigen geschaffen, deren Erkrankung bislang unerkannt blieb. Jeder Einzelne könnte über eine frei zugängliche Seite im Netz seine eigenen Vitaldaten abklären und so zur Eindämmung der Volkskrankheit Diabetes beitragen.Zukunft und Trends der digitalen WeltBig Data ist auch Thema auf deiner Veranstaltung? Wir können dir noch einige Speaker empfehlen.Mehr erfahren;https://bigdatablog.de/2016/06/21/big-data-der-medizin/;BigDataBlog;Christian Schön
29. Apr 16;"Mehr zum Thema Pentaho findest du auch hier:                        ,  Retweets,      ""Mit Pentaho und Jedox zur integrierten Business Intelligence-Plattform""";"Anwender, die eigene Datenanalysen und Reports erstellen wollen, benötigen für unterschiedliche Zwecke das jeweils passende BI-Tool. Ob Open Source, proprietäre Lösung oder beides – die Auswahl sollte funktionsabhängig getroffen werden und sich flexibel kombinieren lassen.Wann nehme ich welche BI-Suite?Pentaho und Jedox sind zwei leistungsfähige Business Intelligence (BI)-Systeme auf Open Source-Basis. Beide BI-Suiten bieten einen vollständigen Funktionsumfang für Data Warehousing, Reporting, Analyse und Planung, verfolgen jedoch unterschiedliche Ansätze und eignen sich daher für unterschiedliche Anwendungsszenarien. Die Stärken von Pentaho und Jedox sind:Datenintegration bzw. Data WarehousePentaho Data Integration (PDI) ist ein Urgestein im BI-Bereich. Für die Datenverarbeitung zapft PDI alle gängigen Datenquellen sowie die neuen Big Data Stores an. Die Funktionen und die möglichen Anwendungsfelder sind sehr umfangreich. Bei Jedox ist dagegen hervorzuheben, dass ETL-Prozesse aus der Oberfläche heraus schnell und einfach angestoßen werden können. Die Anwender können dadurch auch Batch-Verarbeitungen vornehmen.ReportingPentaho bietet bessere Möglichkeiten bei der Erstellung von Berichten. Das liegt unter anderem an den Frontends, die sich an unterschiedliche Anwendergruppen von Entwicklern bis Endanwendern richten. Jedox bietet dagegen mit seinem Excel-Frontend die perfekte Arbeitsumgebung für den Controlling- und Finanzbereich. Die User können schnell und unkompliziert Berichte erstellen und über verschiedene Kanäle anderen Mitarbeitern zur Verfügung stellen.Online Analytical Processing (OLAP) und AnalysenJedox ist eine leistungsstarke Analyseplattform mit der Multidimensional Online Analytical Processing (MOLAP)-Datenbank im Hintergrund und Excel-Tabellenkalkulationen als Frontend. Pentaho bietet mit Mondrian dagegen einen ROLAP-Ansatz, der etwas technischer orientiert ist und weniger Freiheitsgrade bietet als die Jedox-Tools für Excel oder das Web. ROLAP-Systeme nutzen relationale Datenbanksysteme, aus denen die Daten abgefragt werden. Pentaho verwendet außerdem analytische Datenbanken, wodurch OLAP-Anwendungen auch für sehr große Datenmengen bereitgestellt werden können.DashboardsVon Self-Service Dashboards bis hin zu maßgeschneiderten CTools-Dashboards bietet Pentaho zahlreiche Möglichkeiten für die Gestaltung von Daten-Cockpits. Durch die Software-Sammlung CTools lassen sich die Möglichkeiten zur Datenvisualisierung vielfältig erweitern. Jedox baut auch in Sachen Dashboards auf einen Excel-Ansatz: die Features zur Erstellung von Dashboards sind daher eher eingeschränkt.PlanungJedox punktet klar im Bereich Unternehmensplanung. Durch eine Write-back-Funktion in Jedox lassen sich Datenwürfel beschreiben. Dadurch können umfangreiche Planungsapplikationen entwickelt werden, welche die bestehenden Prozesse vereinheitlichen und beschleunigen. Pentaho bietet hier keinerlei Funktionen.Pentaho oder Jedox?Der vorangegangene Vergleich kann eine Entscheidungshilfe bieten, welche Software die für den jeweiligen Verwendungszweck passende ist. Aber auch eine Kombination beider BI-Suiten sollte in Betracht gezogen werden: Aufgrund ihrer modulbasierten Architektur können Pentaho und Jedox auch in Teilen implementiert werden, sodass nur das installiert werden muss, was wirklich gebraucht wird. Zwei Kombinationslösungen seien hier vorgestellt:Kombination I: Datenintegration mit Pentaho und intuitive Analysen mit JedoxTweets von @BigDataDE!function(d,s,id){var js,fjs=d.getElementsByTagName(s)0,p=/^http:/.test(d.location)?""http"":""https"";if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+""://platform.twitter.com/widgets.js"";fjs.parentNode.insertBefore(js,fjs);}}(document,""script"",""twitter-wjs"");Sollen unterschiedliche Datenquellen in einem Data Warehouse zusammengeführt werden, ist Pentaho die richtige Lösung. Es bietet mit Pentaho Data Integration für ETL-Aufgaben eines der leistungsstärksten Werkzeuge, die momentan auf dem Markt sind. Für das Frontend für die Datenauswertungen kommt Jedox zum Einsatz. Sind die Daten einmal in der MOLAP-Datenbank, können die Anwender wie gewohnt mit Tabellenkalkulationen in Excel arbeiten. Selbstverständlich lassen sich auch in das Jedox-Frontend Daten eingeben. Sollen diese Werte aber ebenfalls im Data Warehouse liegen, müssen sie per ETL in die relationale Datenbank eingepflegt werden.In der GPU-Variante kann Jedox auch als hochperformante Analysedatenbank für große Datenmengen aus Hadoop oder anderen Big Data Stores dienen. Hierfür greift Jedox über ein optionales GPU-Modul auf die Rechenleistung und den Arbeitsspeicher eines Grafikprozessors zu, wodurch OLAP-Analysen um ein Vielfaches beschleunigt werden. Der Jedox GPU Accelerator ist die weltweite erste OLAP-Engine, die die Leistung von Grafikkarten für komplexe mehrdimensionale Berechnungen nutzbar macht.Kombination II: Erweiterung von Pentaho um Planungsfunktionalitäten von JedoxDie zweite Variante geht von einer bestehenden Pentaho-Installation aus, die um die Planungsfunktionalitäten von Jedox erweitert wird. Dabei wird die komplette Architektur, vom Data Warehouse bis zu den Reports und Analysen, mit Pentaho bewirtschaftet. Dafür spricht, dass mit dieser Architektur auch Big Data-Technologien nutzbar sind, Berichte an große Empfängerkreise verteilt werden können und sich OLAP auf sehr großen Faktentabellen anwenden lässt. Die Frontends setzen dabei auf dieselbe Datenbasis auf wie das Data Warehouse.In Jedox lässt sich eine eigene „Planungswelt“ schaffen, in der die Anwender mit den Daten verschiedene Szenarien durchspielen. Ist eine finale Planversion erreicht, werden die Daten per ETL in das Data Warehouse zurückgespielt. Von diesem Zeitpunkt an stehen die Planwerte allen weiteren Pentaho-Frontends für Plan-Ist-Vergleiche und Ähnliches zur Verfügung. Soll das Ganze weiter ausgebaut werden, können in einer nächsten Stufe die Web-Frontends integriert werden, sodass sich Anwender nur noch an einer einzigen Oberfläche anmelden müssen.Mehr zum Thema Pentaho findest du auch hier:                        Weiterlesen                                        MongoDB und Pentaho: BI-Analysen der nächsten Generation                                                                                    Weiterlesen                                        Superschnelle Datenintegration: Pentaho integriert Apache Spark                                                                                    Weiterlesen                                        MongoDB und Pentaho: Real-Time Analytics mit NoSQL                                                                                    Die kombinierte AlternativePentaho und Jedox für sich alleine bieten leistungsstarke Lösungen für Business Intelligence. Beide Plattformen haben individuelle Stärken und Schwächen, bedingt durch die unterschiedlichen Herangehensweisen an die Herausforderung Business Intelligence. Werden sie aber in einer gemeinsamen Lösung integriert, entsteht eine umfassende Plattform für Informationsaufbereitungen, die mit kurzen Abfragezeiten und hohen Verarbeitungsgeschwindigkeiten selbst bei sehr großen Datenmengen überzeugt.Die Pentaho-Jedox-Kombination ist auch deshalb so interessant, weil sie über offene Schnittstellen verfügt und in Anschaffung und Betrieb relativ günstig ist. Für Unternehmen stellt eine solche integrierte BI-Plattform darum eine attraktive Alternative zu den mächtigen BI-Suiten der großen Hersteller dar. Wer mehr über die beiden Lösungen erfahren möchte, dem sei das Buch „Pentaho und Jedox. Business Intelligence-Lösungen: Data Warehousing, Reporting, Analyse, Planung“ empfohlen. Es ist im Hanser Verlag erschienen und kostet 59,99 Euro.";https://bigdatablog.de/2016/04/29/self-service-business-analytics-mit-pentaho-und-jedox-zur-integrierten-business-intelligence-plattform/;BigDataBlog;Stefan Müller
02. Mrz 16;"Mehr zum Thema Big-Data-Frameworks:                        , Zukunft und Trends der digitalen Welt,  Retweets,      ""Warum Big Data auf dem Weg zu künstlicher Intelligenz entscheidend sein wird""";"In einem gewissen Sinne können Maschinen wie Computer keine Intelligenz erlangen. Zumindest keine Intelligenz wie die, über die Menschen verfügen. Das hat der Regisseur Stanley Kubrick bereits 1968 erkannt, als die Erforschung von künstlicher Intelligenz (KI) gerade ihre erste Blüte erlebte. In seinem Film „2001 – Odyssee im Weltraum“ dreht sich ein Teil der Handlung um die bemannte Raumfahrt. Auf einem Raumschiff steuern jedoch nicht Menschen, sondern der Supercomputer HAL das Schiff, dessen menschliche Besatzung sich im Tiefschlaf befindet.HAL gibt vor, verstehen, sprechen, handeln und sogar fühlen zu können wie ein Mensch. Ganz ähnlich simulieren auch digitale Assistenten wie Siri und Cortana die Fähigkeiten, zu verstehen und antworten zu können. Zu tatsächlich menschenähnlichen Empfindungen und Verhaltensweisen wird HAL im Film bezeichnenderweise aber erst in dem Moment befähigt, in dem er abgeschaltet wird. Dass die Frage nach dem Überleben bzw. das Bewusstsein der eigenen Sterblichkeit entscheidend für die Entwicklung von Intelligenz ist, ist auch eine zentrale These der Intelligenzforschung.Die Intelligenz der RechenmaschinenDennoch können schon heute Computer bestimmte Aufgaben sehr viel schneller und sehr viel besser erledigen als Menschen. Es stellt sich also die Frage, wie ihre Intelligenz zu erklären ist. Als Marvin Minsky am MIT in Boston in den 1950er Jahren an der Erforschung der künstlichen Intelligenz arbeitete, versuchte er mit technischen Mitteln, neuronale Netzwerke zu simulieren. Denkt man an Big-Data-Frameworks wie Hadoop, ist dieser Ansatz auch heute in gewisser Weise wieder die Grundlage für die Weiterentwicklung der künstlichen Intelligenz.Mehr zum Thema Big-Data-Frameworks:                        Weiterlesen                                        Die neue Realität: Erweiterung des Data Warehouse um Hadoop, NoSQL &amp; Co.                                                                                    Weiterlesen                                        Hadoop bekommt Konkurrenz: Apache Spark ist 100 mal schneller                                                                                    Weiterlesen                                        Big Data und Hadoop: Apache macht das Unmögliche möglich                                                                                    Allerdings werden die Systeme nicht intelligenter, sondern die Rechenleistung der einzelnen Rechner in Netzwerkverbünden schneller und ihre Speicherkapazität größer. Dies sind die entscheidenden Zutaten für die neue Form der künstlichen Intelligenz, die in den nächsten Jahren in immer mehr Bereichen Anwendung finden werden. Das lässt sich am Erfolg des Computerprogramms Watson zeigen. Im Falle von Watson beruht das beeindruckende Ergebnis seiner Leistung auf einem Rechnerverbund von 90 Rechnern mit 19 Terabyte Arbeitsspeicher, die durch Apache UIMA und Hadoop parallel betrieben werden.“Computer erledigen bestimmte Aufgaben besser als Menschen. Wie ist ihre #Intelligenz zu erklären?“ Twittern WhatsAppWie unterscheidet sich Denken von Deep Learning?Ein menschliches Gehirn vollzieht nicht dieselben Berechnungen wie Watson und braucht einfach länger dafür. Die Kognition schafft das Lösen von Denkaufgaben gewissermaßen durch Tricks wie durch Abstraktion. Denkprozesse sind andererseits auch hochgradig von Gefühlen und Intuition geprägt. Und schließlich spielt das Unbewusste eine nicht zu unterschätzende Rolle in der menschlichen Psyche. All diese Dinge werden nicht von Algorithmen abgebildet oder nachgeahmt. Maschinen gehen eigene, mit menschlichem Denken nur bedingt vergleichbaren Lösungswege.www.ibrahimevsan.deBei der Nachahmung von neuronalen Netzwerken mit technischen Mitteln spricht man von „Deep Learning“. Auch diese Algorithmen nutzen Abkürzungen und wiederkehrende Muster (Pattern Recognition), um die Rechenprozesse abzukürzen. Das macht die scheinbar intelligenten Programme so effektiv und dem menschlichen Gehirn zum Teil sogar überlegen. Ein essenzieller Bestandteil auf dem Weg zur Erzeugung von intelligenten Programmen sind Trainingsphasen, in denen die Algorithmen wiederkehrende Muster zu erkennen lernen.Wenn Watson beispielsweise trainiert wird, bei medizinischen Diagnosen zu helfen – etwa bei der Erkennung von Krebs – werden zunächst unzählige Scans ausgewertet, um zu lernen, die Wahrscheinlichkeit einer Erkrankung zu erkennen. Die Trefferquote von Watson ist überwältigend. Dennoch gilt in der Praxis, dass am Ende eines Diagnoseprozesses nach wie vor das Urteil des Arztes steht. Nur dieser hat den Gesamteindruck von einem Patienten. Der Blickwinkel auch von noch so intelligenten Algorithmen wird immer beschränkt bleiben. Sie basieren auf der Logik der ihrer Programme und sind darauf trainiert, spezielle Aufgaben hervorragend zu lösen. Sobald die Fragestellungen zu komplex werden, wächst die Anzahl der zu berechnenden Möglichkeiten in einem neuronalen Netz exponentiell und erfordert eine enorme Rechenleistung.KI und der Faktor MenschIntelligente Programme müssen trainiert werden, um intelligent zu sein. Sie müssen wissen, ob das errechnete Ergebnis das richtige ist. Das Training ist einer der großen Kostenfaktoren bei der Entwicklung der KI. Eines der wichtigsten Anwendungsbiete von intelligenten Programmen ist derzeit die Entwicklung des autonomen Autos. Die Herausforderung an die Algorithmen: das Erkennen und richtige Verstehen von Straßenschildern. Google nutzte schon in Vergangenheit seine eigenen Nutzer, um die Intelligenz neuer Anwendungen zu verbessern. Kostenlose Services wie die lange angebotene kostenlose Telefonhotline, über die Nutzer Google-Suchanfragen mit ihrer natürlichen Sprache stellen konnten, verbesserten die Spracherkennungsalgorithmen. Heute verfügt Google dank dieser Erfahrung über eine der besten Spracherkennungen und nutzt sie für den Dienst „Ok Google“.“#DeepLearning ist eine Schlüsseltechnologie, um intelligente Programme zu trainieren.“ Twittern WhatsAppWie der Medien- und Technologiekritikers Evgeny Morozov kürzlich beobachtet hat, passiert dies höchstwahrscheinlich auf ähnliche Weise mit den Systemen, die für das selbstfahrende Auto nötig sind. Nutzer von Google Scholar müssen einen Turing-Test bestehen, um bestimmte Funktionen zu nutzen. Dieser besteht aus Captchas, die Bilder von Straßenschildern darstellen, die ausgewertet werden müssen. Die Vermutung liegt nahe, dass die Nutzer damit die intelligenten Algorithmen für Googles selbstfahrendes Auto trainieren.Was passiert, wenn Maschinen die Arbeit von Menschen übernehmen?In einer aktuellen FORSA-Umfrage äußerten sich viele Menschen positiv zum Thema KI. Dennoch herrscht nach wie vor die Meinung, dass nur Spezialgebiete wie Wissenschaft, Technik und Militär von den Entwicklungen im Bereich KI profitieren werden. Nur acht Prozent der befragten Erwerbstätigen sehen ihre Arbeitsstelle durch KI bedroht. Aber immerhin sehen etwa 50 Prozent durchaus die Möglichkeit, dass ihr beruflicher Alltag in den kommenden fünf Jahren durch den Einsatz von intelligenten Maschinen verändern wird.Neben der Akzeptanz durch die Menschen selbst, wird ein weiterer Aspekt eine entscheidende Rolle spielen, ob und in welchem Umfang in Zukunft intelligente Software und smarte Maschinen immer stärker in den Alltag eindringen. Die Frage: Wer übernimmt die Verantwortung – sowohl in juristischer wie in moralischer Hinsicht – für Handlungen und Entscheidungen von Programmen und Maschinen?Zukunft und Trends der digitalen WeltBig Data ist auch Thema auf deiner Veranstaltung? Wir können dir noch einige Speaker empfehlen.Mehr erfahren";https://bigdatablog.de/2016/03/02/big-data-fuer-kuenstliche-intelligenz-entscheidend/;BigDataBlog;Christian Schön
16. Feb 16;"Mehr zum Thema Self-Service Business Analytics:                        , Weitere lesenswerte Artikel:                        ,  Retweets,      ""Business Intelligence-Tools für die Wohlfühlzone Excel""";Ständig wachsende und heterogene Datenmengen machen die manuelle Datenverarbeitung mit Excel zusehends beschwerlich und fehleranfällig. Jedox vereinfacht die Berichterstattung durch ein Reporting-Werkzeug mit webbasierter Excel-Oberfläche.Warum Jedox?Mit der Softwaresuite von Jedox lassen sich Unternehmensdaten systematisch analysieren. Die Business Intelligence-Anwendung dient der Auswertung und Visualisierung von Daten. Gegenüber anderen BI-Suiten wartet Jedox zudem mit einem Excel-Frontend auf. Damit können auch Fachanwender ohne tieferes IT-Know-how – entweder über Microsoft Excel oder den Webbrowser –Daten auswerten oder Datenmodelle erstellen.Controller oder Business-Analysten, die bislang mit Excel, PowerPoint und Co. arbeiteten, um Berichte und Analysen zu erstellen, bietet Jedox die Möglichkeit, die Vorteile eines Online-Analytical-Processing (OLAP)-Systems zu nutzen und dabei weiterhin mit einer vertrauten Arbeitsoberfläche zu arbeiten. So lassen sich bspw. Controlling-Lösungen für Prozesskostenabrechnungen erstellen.Mehr zum Thema Self-Service Business Analytics:                        Weiterlesen                                        Self-Service Business Analytics: Open Source erobert Business Intelligence                                                                                    Weiterlesen                                        Self-Service Business Analytics: Reporting- und Analysetools kombinieren                                                                                    Weiterlesen                                        MongoDB und Pentaho: Real-Time Analytics mit NoSQL                                                                                    Planung, Analyse und Reporting stehen im VordergrundEin weiteres Unterscheidungskriterium zu anderen gängigen BI-Suiten: mit Jedox lassen sich Anwendungen für die Unternehmensplanung erstellen. Bei Jedox stehen daher insbesondere die Einsatzbereiche Planung, Analyse und Reporting im Vordergrund.Mit den Web- und Excel-basierten Planungstools von Jedox können Fachanwender Berichte erstellen und komplexe Planungsanwendungen selbstständig berechnen und analysieren. Sämtliche Eingabedaten und qualitativen Ergebnisse hinterlegt Jedox zentral auf der In-Memory OLAP-Datenbank, sodass auch mehrere Fachbereiche koordiniert und ressourcensparend zusammenarbeiten können.“So arbeitet #Jedox. Warum sich der Einsatz der BI-Suite mit Excel-Frontend lohnt. #Analytics #BigData“ Twittern WhatsAppFür Datenauswertungen arbeitet die Ad-hoc-Analyse von Jedox mit einer Excel-Umgebung. Der Nutzer navigiert direkt in den Daten und kann auf Informationen bis zur untersten Belegebene zugreifen. Beispielsweise lassen sich Abweichungen unterschiedlicher Kostenarten pro Kostenstelle betrachten und bei Bedarf bis auf einzelne Buchungsbelege zurückverfolgen. Mit dem Jedox Web Browser können außerdem funktionale und visuell ansprechende Analyseanwendungen realisiert werden. Neben der Web-Oberfläche bietet Jedox das für mobile Endgeräte optimierte Jedox Mobile, über das Nutzer auch von unterwegs auf zentral hinterlegte Berichte zugreifen können.Die Jedox-Anwendungen im Firmen-Video:Die Funktionsmodule von JedoxDaten aus SAP ERP und SAP BW ziehenJedox ETL extrahiert, liest und transformiert Daten aus unterschiedlichsten Quellen und Formaten. Die Bedienung und Modellierung der ETL-Prozesse erfolgt ebenfalls über die Weboberfläche. So existieren beispielsweise auch Schnittstellen für SAP ERP und SAP BW (Business Warehouse). Verarbeitete Daten können in die Würfel der OLAP-Datenbank oder auch in relationale Datenbanken oder Dateien geladen werden. Nach der Definition der ETL-Strecken werden diese zeitgesteuert und automatisiert auf dem Server ausgeführt.Excel als Frontend der BI-SuiteExcel ist das am weitesten verbreitete Business Intelligence-Werkzeug und aus dem Büroalltag der meisten IT-Nutzer nicht mehr wegzudenken. Das hat Jedox frühzeitig erkannt und von Anfang an auf die Tabellenkalkulation als Frontend für seine BI-Suite gesetzt. Die Software-Suite hat sich vor allem in fachanwendergeprägten Einsatzszenarien durchgesetzt. Über das Excel-Frontend lassen sich Datenwürfel beliebig kombinieren und auch ohne tiefgehende IT-Kenntnisse Ad-hoc-Berichte und Analysen erstellen. Die Daten kommen dabei aus der OLAP-Datenbank, Formatierungen und Grafiken aus Excel. Der Vorteil ist klar: der Anwender kann mit seinem gewohnten Werkzeug arbeiten und profitiert gleichzeitig von den Analysemöglichkeiten eines OLAP-Servers.Multidimensionale OLAP-Würfel und „Splashing“Der OLAP-Server von Jedox ist eine multidimensionale Datenbank und speichert die Daten in Würfeln und Dimensionen. Die In-Memory-Technologie kann auch Werte in die OLAP-Würfel zurückschreiben. Das bedeutet, dass sich Eingaben nicht nur auf der Detailebene des Würfels, sondern auch in Aggregation vornehmen lassen. Durch das sogenannte „Splashing“ werden die erfassten Werte nach vorgegebenen Schlüsseln auf die zugrunde liegenden Hierarchieebenen verteilt. Erfasst ein Anwender neue Werte, sind diese in Echtzeit für Berechnungen in der Datenbank verwendbar.Jedox Web als webbasierte BenutzeroberflächeNutzern, die nicht auf Excel als Arbeitsumgebung bestehen, bietet Jedox die Webanwendung Jedox Web. Das User Interface ist an Excel angelehnt und bietet vollen Funktionszugriff auf alle BI-Tools der Jedox Software Suite – von Datenextraktionen und -transformationen, über die Erstellung von OLAP-Würfeln, bis hin zum Aufbau von Berichten, Datenanalysen und Dashboards. Alle Funktionen sind auch in einer mobilen Version für Tablets und Smartphones verfügbar.Gemacht für FachanwenderDurch das Analyzer-Tool ist Jedox außerdem stark im Bereich Unternehmensplanung und eignet sich insbesondere für die Planungsanwendung. Der Jedox Analyzer ist eine weitere Oberfläche, mit der sich Ad-hoc-Abfragen ähnlich von Pivot-Tabellen erstellen lassen. Ein integrierter ETL-Server und ein Task-Manager bieten die Möglichkeit, zeitgesteuerte Abläufe einzustellen, über den OLAP-Manager wird die Datenbank verwaltet. Die Benutzer- und Lizenzverwaltung erfolgt im Systemmanager.Jedox eignet sich besonders für Excel-affine Fachanwender. Über das Excel Add-on können Controller und andere Anwender aus IT-fremden Bereichen in ihrer gewohnten Arbeitsumgebung Daten auswerten.Weitere lesenswerte Artikel:                        Weiterlesen                                        Top 10 Big-Data-Visualisierungs-Tools für Privatanwender                                                                                    Weiterlesen                                        Datenvisualisierung bei Big Data: Wenn Daten Schönheit erlangen                                                                                    Weiterlesen                                        Top 10 Big-Data-Visualisierungs-Tools für Entwickler                                                                                    ;https://bigdatablog.de/2016/02/16/jedox-business-intelligence-tool-fuer-excel/;BigDataBlog;Stefan Müller
03. Feb 16;"Das könnte dich zum Thema Daten außerdem interessieren:                        ,  Retweets,      ""Big Data und das Ende der Speicherkapazität""";3,5 Zoll Disketten oder ZIP-Laufwerke wirken heute wie Relikte aus einem vergangenen Jahrhundert. Dabei ist es vergleichsweise wenige Jahre her, als Sony 2009 die Produktion von Disketten komplett einstellte. Mit der Entwicklung von optischen Speichermedien wie DVDs und Blu-Rays bzw. der enormen Steigerung von Kapazitäten von Festplatten wurden große Speicher zu günstigen Preisen realisiert. Zur Herausforderung wurden Speicherkapazitäten erst wieder, seit der Bedarf durch Big Data rasant gestiegen ist.Zur Speicherung von sehr großen Datenmengen, wie sie etwa beim Teilchenbeschleuniger Large Hadron Collider in Genf anfallen, wird nach wie vor auf Magnetbänder zurückgegriffen. Diese sind vor allem in der Frühphase der Computerentwicklung verwendet worden. Die Menge an Daten, die wir weltweit erzeugen, wächst so schnell, dass die Speicherkapazitäten, die mit den heutigen Speichermedien möglich sind, bald überschritten werden. Alle derzeit verfügbaren digitalen Speichertechnolgien weisen zudem gravierende Mängel auf, so dass sich die Frage stellt, worauf in Zukunft alle Daten gespeichert werden.Die folgende Dokumentation, die von arte France produziert wurde, zeigt die Geschichte und die Zukunft einiger Speichermedien:Der Status quoIm Bereich Big Data Analytics zeichnet sich in den letzten Jahren eine bevorzugte Methode ab, um vor allem Echtzeit-Analysen durchführen zu können. In einem Rechnerverbund, werden Daten in den Arbeitsspeicher (SSD) geladen, und dort in Echtzeit bearbeitet. Die Verfügbarkeit in Echtzeit wird in den meisten Anwendungsfällen immer wichtiger, da so Daten zur Entscheidungsgrundlage oder Echtzeit-Beratung (wie bei Einkaufstipps) genutzt werden können. Der größte Nachteil von Flash-Speichern: Sie können nicht beliebig oft beschrieben werden. Ihre Lebensdauer ist daher begrenzt und sie können nicht als sicherer dauerhafter Speicher verwendet werden. Ebenso schlecht sieht es bei der Haltbarkeit von Daten auf optischen Speichermedien wie DVDs oder Blu-Rays aus. Auch dort kann ein Datenverlust nach wenigen Jahren eintreten. Die momentan kostengünstigsten und praktischsten Datenspeicher sind daher nach wie vor magnetische Festspeicher (HDD).“#BigData wächst exponentiell. Welche Strategien gibt es, wenn die Speicher voll sind?“ Twittern WhatsAppDie Hoffnungs(daten)träger: Holographic Versatile Disc und RacetrackIn zwei Bereichen bahnen sich Weiterentwicklungen von bestehenden Systemen an, die in einigen Jahren marktreif sein könnten. Allerdings würden in jedem Fall Brückentechnologien dringend benötigt werden, da die Neuerungen nach heutiger Einschätzung noch einige Zeit bis zur serienreife brauchen. Während sich im Bereich des heute sehr verbreiteten Flash-Speichers keine Neuerungen abzeichnen, könnte es insbesondere durch eine Weiterentwicklung zu einem Revival der optischen Speichermedien kommen. Bislang werden Daten nur auf der Oberfläche der Dateträgerschicht von optischen Datenträgern verwandt. Die Holographic Versatile Disc könnte durch die Ausnutzung der dritten Dimension Daten in mehreren Schichten speichern. Dadurch würde die Kapazität auf 4 Terabyte pro Medium und eine Zugriffsgeschwindigkeit von 1 Gigabit pro Sekunde steigern.Das könnte dich zum Thema Daten außerdem interessieren:                        Weiterlesen                                        Big Data Analytics: Datenanalysen mit Geschichten                                                                                    Weiterlesen                                        Erst durch Metadaten wird Big Data beherrschbar                                                                                    Weiterlesen                                        Big Data Risiken: Worauf bei der Datenauswertung geachtet werden muss                                                                                    Im Bereich der Festplatten gibt es neben kleineren Neuerungen vor allem das Projekt des Racetracks. Dabei werden Daten in magnetischen Nanodrähten gespeichert, die in Bahnen angeordnet werden und an Rennbahnen erinnern. Durch ihren geringen Raumbedarf erreicht der Racetrack eine 100-mal höhere Speicherdichte im Vergleich zu SSDs. Die Zugriffsgeschwindigkeiten liegen ebenfalls weit jenseits der heutigen Standards. Obwohl es bereits 2011 gelungen ist, einen Prototypen herzustellen, ist unklar, wann ein Modell marktreif ist, das den Alltagsbedingungen stand hält.Futuristische Speichertechnologien: DNA und Protein-coated-DiscsDie menschliche DNA ist im Grunde ebenfalls ein Informationsspeicher, der alle notwendigen Daten speichert, die zur Entwicklung eines Menschen benötigt werden. In Daten übersetzt entspricht der Bauplan des Lebens etwa der Größe von zwei CD-Roms (ca. 1500 MB). 2012 ist es einer Gruppe von Genetikern der Harvard Universität gelungen, Informationen in DNA zu erzeugen, zu codieren und zu decodieren. Wenn es gelingen sollte, DNA als Speichermedium in Zukunft nutzbar zu machen, hätte dies große Vorteile. Informationen, die auf biologische Weise gespeichert sind, werden:platzsparend gespeichert,die Speicherung benötigt keine Energie,auch auf lange Zeit kommt es zu keinem Datenverlust.Der größte Nachteil von DNA-Datenspeicher ist bislang: die extrem langsame Lese- und Schreibgeschwindigkeit.“Wird #BigData künftig in Form von DNA gespeichert? Ja, biologische Speicher haben einige Vorteile.“ Twittern WhatsAppEine andere Entwicklung aus dem Bereich der Biochemie ist die Entwicklung von DVD-ähnlichen Speichermedien, die mit einem Protein beschichtet sind. Diese Proteine, die auf Licht reagieren, werden von Bakterien gewonnen. Eine mit solchen Proteinen beschichtete DVD würde eine Kapazität von 50 Terabyte erlangen. Zum Vergleich: Unter Laborbedingungen speichert eine Blu-ray-Disc maximal 500 GB Daten. Doch auch die proteinbeschichteten Discs haben einen Makel: Sie sind bislang nur einige Monate lesbar.Die Zukunft von Big Data: Gute Daten, schlechte DatenDie Anforderungen von Big Data an Speichermedien werden sich in den nächsten Jahren dramatisch steigern. Die Sensoren des Internet der Dinge werden zu einer Explosion der Datenmenge beitragen. Da die Entwicklung der Speichermedien mit diesem Wachstum nicht Schritt halten wird, stellt sich in Zukunft immer mehr die Frage:Welche Daten müssen archiviert werden, und welche sind nur für den kurz- und mittelfristigen Bedarf wichtig?Den Wert der Daten einzuschätzen, ist, um zu entscheiden, was dauerhaft gespeichert wird und was nicht, nicht ganz einfach. Der Wert von historischen Daten kann sich erst im Rückblick herausstellen. Die Einteilung von Daten in gute Daten und „schlechte“, unwichtige Daten ebenfalls. Einer der Erfolgsfaktoren von Big Data hängt künftig demnach auch an der Entwicklung des Speichermediums der Zukunft.;https://bigdatablog.de/2016/02/03/big-data-und-das-ende-der-speicherkapazitaet/;BigDataBlog;Ibrahim Evsan
21. Jan 16;" Retweets,      ""Mangelnde Akzeptanz von Big Data""";Unterschiedlichste Unternehmen sammeln große Datenmengen, die Menschen durch die Benutzung ihrer Angebote freiwillig und unfreiwillig erzeugen und hinterlassen. Diese Daten haben nicht nur für die Wirtschaft, sondern auch für die Politik, die Forschung oder gemeinnützige Organisationen einen enormen Wert. Das Problem, das die meisten Menschen (51 Prozent der Europäer) mit Big Data haben: diese Daten repräsentieren einen Teil ihrer Persönlichkeit und lassen Rückschlüsse auf sie zu. Jedes Unternehmen und jede Organisation, die Daten speichert, hat die Möglichkeit, das Wissen über Menschen zu nutzen.Europaweit erkennen allerdings nur 32 Prozent mehr Vor- als Nachteile in Big Data. Das hat eine Studie ergeben, die nun veröffentlicht wurde. Das Vodafone Institut für Gesellschaft und Kommunikation führte eine repräsentative Untersuchung in 8 Ländern der Europäischen Union durch. Jeweils 1.000 Menschen aus Deutschland, Frankreich, Großbritannien, Italien, Irland, Niederlande, Spanien und der Tschechischen Republik wurden dazu telefonisch befragt. Das Ziel war es, herauszufinden, wie die Menschen zur Erhebung, Speicherung und Weiterverwendung ihrer Daten stehen.“Bei 51% der Europäer überwiegen die Nachteile von #BigData – Akzeptanz hängt vom Verwendungszweck ab.“ Twittern WhatsAppDatenschutz steht für die Mehrheit an erster StelleDer Schutz der persönlichen Daten steht bei den Menschen in Europa an erster Stelle. Dabei sieht die Situation im Vergleich zu anderen Teilen der Welt hier besonders gut aus. Big Data und der rechtliche Schutz der PersonBei Daten, die in den Big Data Pools lagern und ausgewertet werden, kann es sich um personenbezogene Daten handeln. Sind Datenschutz und Big Data vereinbar?bigdatablog.deDie Rechte der EU-Bürger hinsichtlich ihrer Daten werden in den kommenden Jahren im Rahmen der neuen, längst überfälligen EU-Datenschutzverordnung gestärkt werden. Auch die jüngsten Urteile des Europäischen Gerichtshofes wie das bezüglich des „Safe Harbour Abkommens“ sprechen eine eindeutige Sprache. Doch scheint es um das Vertrauen in die Unternehmen und die öffentliche Hand schlechter denn je bestellt. Insbesondere Menschen aus Deutschland reagieren bezüglich des Datenschutzes sensibel. Spanier hingegen sind das Big-Data-freundlichste Volk innerhalb Europas. Betrachtet man die Ergebnisse der Studie länderunabhängig, fällt auf, dass ältere Menschen und Menschen mit niedrigem Bildungsniveau skeptischer sind als junge oder gut gebildete Menschen.“Obwohl der #Datenschutz für Menschen in der EU immer besser wird, schwindet das Vertrauen.“ Twittern WhatsAppDer Zweck heiligt die MittelDas Bild, das die Vodafone-Studie zeichnet, ist aber bei weitem nicht einheitlich düster. Sie zeigen vielmehr einen klaren Auftrag, den die Menschen an Entscheider in Wirtschaft und Politik stellen. Denn, wenn es um einen guten Verwendungszweck geht, habe viele Menschen eine positive Haltung: Dann sind sie durchaus bereit, ihre Daten zur Verfügung zu stellen. Geht es etwa um die medizinische Forschung, so sind immerhin 42 Prozent der befragten Deutschen mit der Verarbeitung ihrer Daten einverstanden. Dem eigenen Arbeitgeber würden 36 Prozent ihre Daten anvertrauen und Banken 33 Prozent. Wichtig ist also ein hoher Nutzwert und eine Weiterverwendung, die zu den eigenen Werten passt. Bei Suchmaschinen und sozialen Netzwerke ist dieser Nutzwert für die Menschen offenbar besonders gering, denn denen trauen nur 16 bzw. 11 Prozent einen vertrauensvollen Umgang mit ihren Daten zu.Die digitale Gretchenfrage: Wie hältst du es mit deinen Daten?Meine Daten gehören mir. Es fehlt nur noch der technische und der rechtliche Rahmen, bis mein Eigentum auch von mir selbst verwaltet wird.Ich habe nichts zu verbergen. Meine Daten können sowohl der Forschung als auch dem Wohl der Unternehmen zugutekommen.Kommt darauf an. Ich würde gerne gefragt werden. Ohne mich durch hunderte Paragrafen lesen zu müssen, würde ich gerne im Einzelfall entscheiden.Alarmierend ist die Einsicht, dass nur 12 Prozent aller Befragten angeben, sich in den Allgemeinen Geschäftsbedingungen darüber informieren, was mit ihren Daten geschieht. Im Umkehrschluss wären sogar 55 Prozent aller befragten bereit, eher Geld für Online-Dienste wie Suchmaschinen zu bezahlen, als mit ihren Daten.Hier geht es zur Studie (PDF) des Vodafone-Instituts, die viele weitere interessante Einsichten bereit hält: Big Data – wann Menschen bereit sind, ihre Daten zu teilen.;https://bigdatablog.de/2016/01/21/mangelnde-akzeptanz-von-big-data/;BigDataBlog;Antje Neubauer
12. Jan 16;"Mehr zu den Themen Self-Service Business Analytics und Pentaho                        ,  Retweets,      ""Self-Service Business Analytics: Reporting- und Analysetools kombinieren""";Business Intelligence-Werkzeuge verwandeln unstrukturierte Datenberge in wertvolle Informationen. So lässt sich der in den Unternehmensdaten verborgene Wissensschatz überhaupt heben. Im Mittelpunkt dieses Beitrags steht die Open Source Business Intelligence-Suite Pentaho.Die modulare Plattform lässt sich schnell und einfach in bestehende IT-Strukturen eingliedern. Mit Pentaho sind Anwender in der Lage, gesammelte Daten in genau die Informationen zu verwandeln, die von der jeweiligen Fachabteilung benötigt werden.Warum Pentaho?Bei der Suche nach der richtigen Business Intelligence-Anwendung wird man schnell mit einem äußerst vielfältigen Angebot an proprietären und Open Source-Lösungen konfrontiert. Für den Einsatz von Pentaho als Business Intelligence-Software im Unternehmen sprechen vier gute Gründe:Business und Big Data Analytics in einer Plattform – Mit Pentaho können Daten aus Big Data Stores innerhalb einer Plattform extrahiert, verarbeitet, ausgewertet und visualisiert werden.Einfache Erweiterbarkeit – Pentaho ist die führende Open Source-Software für Business Intelligence, flexibel und modular aufgebaut, und bietet Schnittstellen zu allen gängigen Hadoop-Distributionen, NoSQL- und analytischen Datenbanken.Breites Spektrum an Analysemöglichkeiten – Pentaho integriert viele verschiedene Frontends für die unterschiedlichen Anwendergruppen im Unternehmen.Hohe Innovationskraft – Die weltweite aktive Community testet und entwickelt die Software kontinuierlich weiter.So arbeitet die Pentaho-PlattformDaten konsolidieren und nutzbar machenDas Data Warehouse als integrierte Datenbasis eines Unternehmens ist das Herzstück einer jeden Business Intelligence-Architektur. Doch ohne sauber aufbereitete und strukturierte Daten machen die schönsten Analysen und Berichte keinen Sinn. Das Modul Pentaho Data Integration (PDI) übernimmt genau diese Aufgabe: mittels ETL-Prozessen bereinigt es Daten aus mehreren (und gegebenenfalls auch unterschiedlich strukturierten) Quellen und führt sie in einem Data Warehouse zusammen. Wie das geht, beschreibt dieses Whitepaper.Berichte entwickeln und verteilenPentaho bietet unterschiedliche Funktionen im Bereich Reporting, die von interaktiven Self-Service-Berichten bis zu formatierten Standardberichten reichen. Anwendern bietet das Pentaho Interactive Reporting eine webbasierte Drag-and-Drop-Oberfläche zum Erstellen von Ad-hoc-Berichten. Mit dem Pentaho Report Designer können dagegen pixelgenaue Berichte aufgesetzt und in unterschiedlichen Formaten an die Empfänger im Unternehmen verteilt werden.Relationale OLAP-Analysen mit dem Mondrian-ServerMondrian ist der Name der OLAP-Engine innerhalb der Pentaho-Plattform. Die Anwendung ist in Java geschrieben und ermöglicht die interaktive Analyse von multidimensionalen Datenbeständen. Der Server bietet dem Anwender performante Auswertungen, selbst bei einer sehr großen Datenbasis. Mondrian und der Pentaho Analyzer versetzen auch nicht-technisch versierte Fachanwender in die Lage, eigenständig Berichte und Auswertungen zu erstellen. Im Gegensatz zum Report Designer sind keine besonderen Kenntnisse der Datenbanken und Abfragesprachen notwendig.Pentaho DashboardsMit dem Pentaho Dashboard Designer erstellen und verwalten Anwender eigene Dashboards. Damit soll der Betrachter intuitiv und schnell über einen bestimmten Sachverhalt informiert werden. Je nach der genutzten Pentaho-Edition variiert der Funktionsumfang. Anwender der Enterprise Edition haben mit dem Dashboard Designer die Möglichkeit, Cockpits eigenständig in einer Oberfläche zusammenzustellen.Integrierte BI-PlattformDie Pentaho Business Analytics Suite in der aktuellen Version 5.1 bietet einen vollständigen Ansatz für Business Intelligence und Big Data Analytics. Pentaho vereint in einer Plattform umfangreiche Funktionen zur Integration von Daten aus den unterschiedlichsten Quellen, ebenso wie Werkzeuge für Berichtwesen, Analysen und Visualisierungen. Anwender müssen keine Drittsoftware bedienen, um weitere Funktionsbereiche, z.B. für direkte Auswertungen von Big Data, abzudecken. Pentaho ist offen und erweiterbar und bietet somit eine ideale Ausgangslage zur Gestaltung passgenauer Business Intelligence-Anwendungen, die Daten zu wertvollen Informationen veredeln.“So arbeitet #Pentaho – und warum sich der Einsatz der #OpenSource BI-Suite lohnt. #Analytics #BigData“ Twittern WhatsAppMehr zu den Themen Self-Service Business Analytics und Pentaho                        Weiterlesen                                        Self-Service Business Analytics: Open Source erobert Business Intelligence                                                                                    Weiterlesen                                        Superschnelle Datenintegration: Pentaho integriert Apache Spark                                                                                    Weiterlesen                                        MongoDB und Pentaho: Real-Time Analytics mit NoSQL                                                                                    ;https://bigdatablog.de/2016/01/12/self-service-business-analytics-reporting-und-analysetools-kombinieren/;BigDataBlog;Stefan Müller
05. Jan 16;" Retweets,      ""Digital Humanities: Über das Potenzial von Big Data in den Geistes- und Kulturwissenschaften""";Digital Humanities: Big Data in den Geisteswissenschaften | © Johan Swanepoel @ Shutterstock.comIn Deutschland erscheinen pro Jahr ca. 100.000 neue Buch-Publikationen. Oft erfassen die Statistiker E-Books noch nicht, insofern dürfte die tatsächliche Zahl deutlich höher liegen. Weltweit erscheinen seit dem Jahr 2000 jährlich mehr als 1.000.000 Bücher. Hinzu kommen alle Online-Veröffentlichungen. Lesen kann man die gesamte Menge an Texten schon lange nicht mehr. Wer heute noch Aussagen über Trends in der Literatur oder der Wissenschaft machen möchte, kommt an Big Data nicht mehr vorbei. Aber handelt es sich bei den Datenanalysen noch um Textanalysen in einem geisteswissenschaftlichen Sinne?Seit einigen Jahren befassen sich Wissenschaftler unter dem Sammelbegriff „Digital Humanities“ mit Fragestellungen wie diesen.Das Fach Digital Humanities (deutsch: „digitale Geisteswissenschaften“) umfasst die Anwendung von computergestützten Verfahren und die systematische Verwendung von digitalen Ressourcen in den Geistes- und Kulturwissenschaften.Quelle: https://de.wikipedia.org/wiki/Digital_HumanitiesOb die Digital Humanities ein eigenes Fach oder nur eine bestimmte Methode in den Geistes- und Kulturwissenschaften ist, ist noch nicht abschließend geklärt und auch weniger wichtig als ihre Funktion: Die Digital Humanities sind das Bindeglied zwischen digitaler bzw. computergestützter Informationsverarbeitung und den Geistes- und Kulturwissenschaften.“Die #DigitalHumanities sind das Bindeglied zwischen Geistes- und Kulturwissenschaften und Informatik.“ Twittern WhatsAppDie Geisteswissenschaften als HilfswissenschaftEiner der prominentesten Berührungspunkte von Geisteswissenschaften und Data Science findet sich an den Schnittstellen zu praktischen Anwendungsfällen. Dort dienen die geisteswissenschaftlichen Einzeldisziplinen als Hilfswissenschaften, mit deren Unterstützung bestimmte Fragestellung gelöst werden können. So werden die quantitative Textanalyse (Text Mining), die Sentimentanalyse (zur Erkennung von Stimmungen in sozialen Netzwerken, Foren und Blogs) und Bildanalysen verwendet, um Vorhersagen über Finanztrends und Wirtschaftsentwicklungen, Wahlentscheidungen, politische Einstellungen und Protestbewegungen, oder in der Marktforschung und Trendanalyse zu treffen. Über diesen Umweg kamen neue Methoden zur Text- und Bildauswertung wieder zurück in die Geistes- und Kulturwissenschaften, wo sie wiederum zu neuen Einsichten, aber auch zu ganz neuen Fragestellungen führten.Der praktische Nutzen der ComputerlinguistikEin wichtiges Beispiel für so eine Schnittstelle zwischen Informatik und Geisteswissenschaften ist die Computerlinguistik, die digitale Sprachwissenschaft. Inzwischen gehen die Entwicklungen in der Computerlinguistik so weit, dass sie kaum mehr nur als Hilfswissenschaft bezeichnet werden darf. Der erste Schritt war eine Übersetzung der natürlichen Sprache in logisch-abstrakte Regeln, um sie für Algorithmen und Programme verständlich zu machen. Inzwischen ist die Computerlinguistik im Herzen der Erforschung von künstlicher Intelligenz, Text Mining, Sprachsteuerung, Verbesserung von Suchmaschinen und von Schreib- und Übersetzungsprogrammen.Der Computer als blinder Fleck der Data SciencesEine der Fragestellungen der Digital Humanities richtet sich auf die neuen computergestützten Untersuchungsmethoden selbst. Sie fragen danach, wie die datenbasierte Forschung die Wissenschaften und ihr Erkenntnisinteresse verändert haben. In historischer und theoretischer Hinsicht untersuchen sie Bedingungen der Datenerzeugung und Datenverarbeitung. Sie zeigen, wie datengetriebene Wissenschaften ihre Daten erzeugen und hinterfragen diese Bedingungen ihrer Herstellung.Beispielsweise wird in der Kognitionsforschung versucht zu verstehen, wie das menschliche Gehirn funktioniert. Die dazu verwendeten Modelle basieren aber auf Gesetzmäßigkeiten, die der Logik von Computern und Programmiersprachen folgen – alles, was jenseits dieser Gesetzmäßigkeiten liegt, wird automatisch von der Erkenntnis ausgeschlossen. Selbst wenn es gelingen sollte, ein menschliches Gehirn zu simulieren, wie es das Human Brain Projekt (HBP) versucht, so lassen sich am Ende nur Aussagen über die Simulation und nicht über den mit ihr abgebildeten Gegenstand, das Gehirn, treffen.Die Europäische Union glaubt jedoch daran, dass das Gehirn in einem Computer abgebildet und untersucht werden kann und unterstützt das HBP mit einer Milliarde Euro. Im menschlichen Auge ist der Blinde Fleck, der Ort, an dem alle Sehnerven zusammenlaufen, gerade der Bereich, der selbst nichts wahrnehmen kann. Auf ähnliche Weise ist der Computer der Blinde Fleck vieler datengetriebener Forschungsprojekte.Die Grundoperationen: Sammeln, Ordnen und InterpretierenDaten zu sammeln, zu ordnen und zu analysieren ist in vielen geistes- und kulturwissenschaftlichen Fächern zunächst kein neuer Ansatz, sondern eine der Grundoperationen. Allerdings verändern sich die Fragestellungen einzelner wissenschaftlicher Arbeit erheblich, wenn die Datenmenge steigt. In den letzten Jahren entstanden enorme Textarchive wovon Google Books sicher das bekannteste ist. Aber auch in Deutschland gibt es ambitionierte Unternehmungen wie das Deutsche Textarchiv oder die Deutsche Digitale Bibliothek. Solche großen Datenbanken ermöglichen es Forschungsmaterial in ganz anderen Größenordnungen zu untersuchen.Nehmen wir zunächst eine traditionell angefertigte Doktorarbeit, die sich mit der Entwicklung der Gattung des Romans im 19. Jahrhundert beschäftigt. Big Data als Voraussetzung für das Semantic WebDas Semantic Web ist die nächste Entwicklungsstufe intelligenter Suchmaschinen. Gleichzeitig entsteht damit die Voraussetzung für künstliche Intelligenz.bigdatablog.deWie viel Forschungsmaterial könnte ein geübter Leser in mehreren Jahren sichten, ordnen und interpretieren? Vielleicht zwischen 250 und 350 Bücher – das wäre schon eine riesige Menge, bei der allein aufgrund des Lektüreaufwands keine umfangreiche Einzelanalyse jedes Werkes mehr möglich wäre. Mithilfe von digitalen Textdatenbanken wird es mit einem Mal möglich, 6.000 oder wenn nötig auch 60.000 Werke auszuwerten. Auch hier würde am Ende keine detaillierte Einzelanalyse aller Werke stehen. Aber mit semantischen Analysemethoden ließen sich über einzelne Begriffe, Konzepte oder Erzählmuster Aussagen treffen. Philologische Fragestellungen und DatenanalysenDie Analyse von Mustern, die Texten zugrunde liegen, kann sowohl digital als auch „von Hand“ erfolgen. Der deutsch-russische Strukturalist und Literaturwissenschaftler Vldadimir Propp untersuchte beispielsweise 1928 in seiner berühmten Studie „Die Morphologie des Märchens“ die Grundstrukturen und Erzählmuster, die allen Märchen zugrunde liegen. Dazu verglich er viele „Datensätze“, also verschiedenste Märchen miteinander, um Gemeinsamkeiten zu erkennen und zu abstrahieren. Fragestellungen wie diese, die typisch für den Strukturalismus sind, gehen den Big Data Sciences zwar historisch voraus, ähneln sich aber in ihren Methoden.Die Formalisten und Strukturalisten des 20. Jahrhunderts wären mit Sicherheit glühende Anhänger datenbasierter Geisteswissenschaften gewesen. Darüber hinaus finden sich viele andere philologische und kulturwissenschaftliche Fragestellungen und Arbeitsbereiche, die eine hohe Kompatibilität und Anschlussfähigkeit zu Big Data Analytics aufweisen: überall dort, wo wiederkehrende Muster untersucht werden wie in der der Motivgeschichte, bei der Analyse von Metaphern oder bei der Begriffsgeschichte.“Die #DigitalHumanities suchen nach Schnittstellen von Text- bzw. Bildanalyse und Datenanalyse.“ Twittern WhatsAppÜber das Potenzial von Big Data in den GeisteswissenschaftenDer Volkswirtschaftler Steffen Roth nutzte in einer viel beachteten Studie (PDF) den Google Ngram Viewer, der mit dem weltweit größten digitalen Bucharchivs arbeitet. Mit Ngram lässt sich die Häufigkeit darstellen, mit der ein Begriff in den Publikationen zu einer bestimmten Zeit auftaucht. Roth kam zu sehr überraschenden Einsichten, was die Karriere von bestimmten ökonomischen Begriffen betrifft, die als typisch für moderne Gesellschaften gelten. Im Gegensatz zur verbreiteten Forschungsmeinung tauchten ökonomische Begriffe aber nicht besonders häufig in modernen Texten auf.Solche zum Teil überraschenden Ergebnisse sind im Moment noch mit Vorsicht zu genießen und auch keine Seltenheit. In der Google-Books-Datenbank, der weltweit größten Sammlung digitalisierter Texte, finden sich im Moment (Stand Dezember 2015) erst ca. 13-15% aller jemals gedruckten Bücher. Das sind immerhin beachtliche 15 Millionen Bücher, die insgesamt etwa 4,5 Milliarden Seiten umfassen. Doch die entscheidende Frage ist: welche Bücher befinden sich in dem Archiv und welche nicht? Die Häufung eines bestimmten Begriffs zu einer bestimmten Zeit kann immer noch an wenigen Autoren oder Einzelwerken hängen. Für auf den ersten Blick überraschende Treffer, finden bei der Überprüfung der einzelnen Fundstellen einfache Erklärungen. Doch trotz solcher „Kinderkrankheiten“ und blinder Flecken ist das enorme Potenzial, das in Big-Data-Analysen für die Geistes- und Kulturwissenschaften offensichtlich.;https://bigdatablog.de/2016/01/05/digital-humanities-ueber-das-potenzial-von-big-data-in-den-geistes-und-kulturwissenschaften/;BigDataBlog;Christian Schön
18. Dez 15;"Mehr zum Thema Datenvisualisierung                        ,  Retweets,      ""Self-Service Business Analytics: Open Source erobert Business Intelligence""";"Immer mehr Anwender wollen ihre Daten selbstständig analysieren und visualisieren, um ihr unternehmerisches Handeln besser zu planen. Flexible Business Intelligence-Tools auf Basis von Open Source versetzen Endanwender in die Lage, nutzerfreundliche Reportings zu erstellen, ohne dafür ganze Systemlandschaften ersetzen zu müssen oder auf die Unterstützung eines IT-Spezialisten angewiesen zu sein.Mehr zum Thema Datenvisualisierung                        Weiterlesen                                        Top 10 Big-Data-Visualisierungs-Tools für Privatanwender                                                                                    Weiterlesen                                        Datenvisualisierung bei Big Data: Wenn Daten Schönheit erlangen                                                                                    Weiterlesen                                        Top 10 Big-Data-Visualisierungs-Tools für Entwickler                                                                                    Data Warehouse als zentrales DatenlagerIn vielen Unternehmen ist die Ausgangssituation folgende: Steuerungsrelevante Informationen über Lieferanten, Prozesse, Produkte, Kunden und das sonstige Unternehmensumfeld sind über die unterschiedlichen operativen Systeme verstreut, vorgehalten in diversen Datenbanken und -formaten. Sollen diese Daten aber für Berichte und Auswertungen zur Verfügung stehen, müssen sie in einem zentralen Datenlager – dem Data Warehouse – zuerst konsolidiert und dann aufbereitet werden.Als zentrales Datenlager im Unternehmen stellt das Data Warehouse den Dreh- und Angelpunkt einer Business Intelligence-Architektur dar. Ein flexibles und leistungsfähiges Data Warehouse ist:„subject-oriented“ und kann Daten nach Themen organisieren„integrated“ und konsolidiert alle Daten, etwa aus ERP- und CRM-Systemen oder aus externen Quellen„time-variant“ und speichert langfristig, für historische Analysen„non-volatile“ und speichert Daten immer persistentDie konkreten Vorteile von Dashboards, OLAP-Würfeln und Data Mining5 Regeln für die Erstellung von DashboardsDashboards? helfen dabei, Unternehmensinformationen und Daten richtig darzustellen und zu lesen. Wir geben 5 Tipps, wie sie richtig erstellt werden können.bigdatablog.deManager und Entscheider erwarten einen schnellen und intuitiven Zugriff auf geschäftsrelevante Informationen. Mit Business Intelligence-Tools (BI-Tools) lassen sich grafische Sichten auf Daten, sogenannte Dashboards, erstellen, die Zusammenhänge, Trends oder Ausreißer auf einen Blick sichtbar machen. Dashboards fassen den Ist-Zustand in Kennzahlen zusammen, bieten grafische und tabellarische Datenauswertungen mit hohem Verdichtungsgrad und ermöglichen Ad-hoc-Anfragen zur aktuellen wirtschaftlichen Situation.Analysten oder Controller wollen oft auch eigene Auswertungen erstellen können, um Probleme auszuwerten. Diesen Anspruch befriedigt das Online Analytical Processing (OLAP), auch als Cubes oder Datenwürfel bekannt. OLAP-Systeme sind Analysetools, die Power-Usern einen multidimensionalen Zugang zu den unterschiedlichsten Datenquellen eröffnen. Diese kann man, gleich einem Würfel, aus mehreren Perspektiven betrachten und kritische Geschäftsprozesse durch „Was-wäre-wenn“-Szenarien identifizieren.Anders als bei definierten OLAP-Analysen kann Data Mining sogar völlig neue Muster im Datenbestand sichtbar machen. Data Mining verwendet dazu verschiedene Methoden aus Statistik, Mathematik und künstlicher Intelligenz, wie etwa Entscheidungsbäume, künstliche neuronale Netze, Clusterverfahren oder Assoziationsanalysen. Data Mining „schürft“ nach bisher unbekannten Zusammenhängen und stellt eigene Hypothesen auf.Open Source macht klügerViele der modernen Big Data-Technologien sind Open Source, beispielsweise Hadoop und MongoDB. Open Source Business Intelligence-Software (OSBI) setzt sich auch deshalb durch, weil es herstellerunabhängig ist – und Kunden somit allzu intensive Abhängigkeiten zu einzelnen Softwareanbietern vermeiden. Ein Vergleich einiger OSBI-Lösungen findet sich beispielsweise hier.Ein vielversprechendes Konzept bietet der Best Source-Ansatz, dessen Ziel es ist, bewährte kommerzielle Software mit Open Source-Lösungen zu ergänzen. Beispielsweise lässt sich, auf Basis des bestehenden SAP-Systems, mit quelloffener Software eine Business Intelligence-Lösung entwickeln. Eine derart aufgesetzte Business Intelligence-Lösung bietet den Vorteil, dass das recht starre ERP-System um flexible Auswertungsfunktionen erweitert wird, die auch Daten aus Drittsystemen berücksichtigen. Anwender müssen sich nicht auf horrende Vorabinvestitionen in proprietäre Systeme einlassen. Sie können ihr bestehendes Unternehmenssystem kostengünstig um Open Source-Software erweitern und ihre BI-Tools flexibel an der Unternehmensstrategie ausrichten.Transparenz macht sicherOSBI-Systeme versprechen eine hohe Transparenz. Die kommt dem Sicherheitsbedürfnis entgegen, das viele Unternehmen gegenüber Systemen mit dem Rang „mission-critical“ haben. Schließlich sind Business Intelligence-Systeme nicht selten das Nervensystem des Unternehmens. Der Vorteil von Open Source: Weil jeder den Quellcode einsehen kann, sind potenzielle Sicherheitsrisiken viel leichter zu identifizieren und zu beheben.Offenheit macht flexibelOpen Source garantiert Interoperabilität dank offener Standards. Bei kollaborativer Entwicklung fließen Neuerungen sofort wieder in die Community zurück – das sorgt für schnelle Innovationen und macht unabhängig vom Hersteller.“Setze bei Business Intelligence-Tools auf #OpenSource &amp; ermögliche eine höhere Nutzerfreundlichkeit“ Twittern WhatsAppGenerell überzeugt OSBI-Software durch weitreichende Customizing-Möglichkeiten. Während das Angebot der großen Anbieter oft recht starr ist, lässt sich eine OSBI-Software passgenau implementieren. So werden die Anforderungen der Nutzer besser erfüllt und die Fachbereiche bestmöglich unterstützt.";https://bigdatablog.de/2015/12/18/self-service-business-analytics-open-source-erobert-business-intelligence/;BigDataBlog;Stefan Müller
18. Nov 15;" Retweets,      ""Predictive Maintanence und Predictive Analytics: Interview mit Matthias Mierisch von arvato Systems""";"Interview mit dem Matthias Mierisch, Vorsitzender der Geschäftsleitung von arvato Systems | © Tsyhun @ Shutterstock.comDie Arvato AG mit ihrem Hauptsitz im nordrhein-westfälischen Gütersloh, ist ein international tätiges Unternehmen, das zur Berteslmann SE &amp; Co. KGaA gehört. Mit über 70.000 Mitarbeitern weltweit gehört arvarto zu einem der führenden Dienstleister. Das Kunstwort arvato ist zusammengesetzt aus ar – für “ars”, also Kunst – va – für Variation – t – für Technik – und schließlich o – für Organisation.Arvato ist in sieben unterschiedlichen Bereichen, sogenannte Solution Groups, untergliedert:Customer-Relationship-Solutions,Digital Marketing,Financial Solutions,IT Solutions, Print Solutions,Replikation undSupply-Chain-Management.Big Data und Predictive Analytics wurde in den letzten Jahren in all diesen Bereichen relevant. Matthias Mierisch, Vorsitzender der Geschäftsleitung von arvato Systems, hat sich Zeit für unsere Fragen genommen.Big Data Blog: Herr Mierisch, arvato Systems bietet seit mehr als 30 Jahren Lösungen im IT-Bereich an. Seit wann konnten Sie den rasanten Anwuchs an Daten wahrnehmen und mussten sich darauf einstellen?Matthias Mierisch: Intensiv befassen wir uns mit dem Thema Big Data seit gut fünf Jahren. In der Zeit sind auch einige Kunden mit den ersten Problemstellungen zur Beherrschung und Analyse von stark wachsenden Datenmengen auf uns zugekommen. Grundsätzlich ist ein großes Wachstum der Datenbanken über die letzten Jahre zu verzeichnen, das auch eine Bereitstellung von neuen Ressourcen erfordert, um den erweiterten Anforderungen der Kunden, z. B. nach Echtzeitauswertungen, nachzukommen.Wir haben aber keinen abrupten Wandel wahrgenommen, es war eher eine schleichende Entwicklung, in der wir Stück für Stück Expertise aufgebaut und entsprechende Projekte anhand der Kundenbedarfe realisiert haben. Auf diesem Weg haben sich dann auch verschiedenste Technologien etabliert, die wir für unsere Kunden prüfen und in individuelle und teils komplett neue Services integrieren.Wie stark hat sich Ihr Technologie-Stack in den letzten Jahren verändert und wie schnell verändern sich die Anforderungen, so dass neue technologische Lösungen erforderlich sind?Im Umfeld Big Data hat sich die Bandbreite an technischen Lösungen in den letzten Jahren bis heute stark vergrößert und auch verändert. In Bezug auf strukturierte Daten sind Technologien wie Flashstorage und In-Memory-Technologien sowie Engineered Systems dazu gekommen.Bei semi-strukturierten Daten sind teilweise komplett neue Technologien entstanden. Hier bieten große Hersteller oft zusammengestellte Technologien in Appliances an, es gibt aber auch viele Anbieter mit Ergänzungsprodukten und neuen Technologien. Die Anforderungen ändern sich dabei stetig, sodass oft Implementierungsprojekte durchgeführt werden müssen, jeder Datenbestand ist anders.Wir befassen uns generell mit vielen Technologien verschiedenster Anbieter, und prüfen, welche davon für unsere Kunden nützlich sind. Mit SAP HANA haben wir uns zum Beispiel auch sehr stark befasst und den Nutzen für unsere Kunden erkannt. Daher sind wir seit diesem Jahr auch „SAP-Certified Provider of SAP HANA Operations Services“. Themen, wie Virtualisierung, Echtzeitanalysen oder die Einbindung von Mobilgeräten zum Abrufen von Daten von überall zu jeder Zeit sind aktuell interessant.Ergänzt wird Big Data immer um das Thema Datensicherheit. Der Kern der Sache hat sich dabei aber nicht stark verändert, es kommen lediglich neue Ausprägungen hinzu, für die es wiederum neue Technologien gibt. Für uns ist es wichtig, dass unsere Kunden sich bei all den Veränderungen und verschiedensten Technologie nicht mit den einzelnen Bausteinen befassen müssen. Sie formulieren lediglich ihre Anforderungen, zu denen wir eine passende Referenzarchitektur für genau die Fragestellung aufbauen – mit den aktuellen Technologien.Welche Rolle spielen die Angebote aus dem Apache-Park wie beispielsweise Hadoop, Spark oder Hive?Big Data und Hadoop: Apache macht das Unmögliche möglichBig Data lebt von großer Datenvielfalt, enormen Datenmengen und guter Verfügbarkeit. Hadoop hilft, diese zu bewerkstelligen. Wie geht das?bigdatablog.deAls technologieunabhängiger Systemintegrator befassen wir uns grundsätzlich mit vielen aktuellen Technologien, auch natürlich mit den Apache-Lösungen. Hadoop ist die zentrale Komponente einer Big Data Lösung für semi-strukturierte Daten, hier werden die Datenbestände abgelegt.Ein weiterer wichtiger Baustein ist für uns aber auch eine Implementierung von System R, mit dem, neben Map-Reduce Technologien, Auswertungen implementiert werden können. In diesem Umfeld spielt dann wieder HIVE eine Rolle, bei dem die Hersteller auch eine Integration in SQL-Technologien anbieten. Aber auch neue Speichertechnologien wie NoSQL Datenbanken oder andere Werkzeuge kommen bei uns zum Einsatz. Das macht das Feld aktuell ein wenig unübersichtlich, hier wird sich in den kommenden Jahren aber die „Spreu vom Weizen“ trennen.Im Moment geht der Trend in Richtung SaaS und Cloud-Computing. Gehören On-Premise-Lösungen der Vergangenheit an, oder gibt es noch Szenarien, in denen diese Variante weiterhin erforderlich ist?Natürlich gehen wir gerade auch verstärkt in die Richtung Cloud, das ist für uns das große Wachstumsfeld der nächsten Jahre. Wir sind der festen Überzeugung, dass das Thema Hybrid Cloud ein zukünftig bewährtes Betriebsmodell für unsere Kunden ist, mit all seinen Vorteilen aus reduzierter Komplexität, erhöhter Skalierbarkeit oder Kostensenkungen. Dafür bieten wir schon heute Lösungen und dafür erarbeiten wir kontinuierlich weitere Lösungen.Dennoch wird es nach wie vor auch Anforderungen für On-Premise Lösungen geben, etwa für große Enterprise Applikationen z. B. über Oracle/Exadata, oder Lösungen für semi-strukturierte Daten. Vor allem Unternehmen mit besonders sensiblen Daten fragen weiterhin On-Premise Lösungen bei uns an, das wird sich meiner Einschätzung nach mittelfristig auch nicht ändern.Was sind aus Ihrer Perspektive die kommenden Trends: in welchen Geschäftsbereichen wird Big Data eingesetzt werden, wo es bislang noch keine große Rolle spielt? Welche Möglichkeiten bieten sich dadurch?Ein großes Potential sehe ich aktuell darin, präzise Voraussagen zu zukünftigen Entwicklungen auf Basis von Datenmodellen treffen zu können. Diesen Trend kann man unter dem Begriff Predictive Analytics zusammenfassen. Wo das Reporting aufhört, kann ein Unternehmen mit Predictive Analytics weitergehen, um Entwicklungen besser einschätzen und letztendlich fundierte Entscheidungen für die nächsten Jahre treffen zu können.Das kann ein großer Wettbewerbsvorteil sein, quer durch fast alle Branchen. Ich denke da z. B. an Energieversorger, die präzisere Lastprognosen erstellen, oder Industrieunternehmen, die mit „Predictive Maintenance“ eine vorausschauende Wartung vornehmen können. Gerade der Handel wird davon profitieren, Konsumentenverhalten besser voraussagen zu können.“Handel, Energieversorger &amp; die Industrie werden von #PredictiveMaintanence #Analytics profitieren.“ Twittern WhatsAppEndkundenorientierte Unternehmen werden die Verbraucher in Zukunft immer besser verstehen und können so z. B. auch gezieltere Marketingkampagnen mit einem größeren ROI durchführen. Insgesamt wird sich aber das „Ökosystem“ zu Big Data noch weiter wandeln und in nächster Zeit immer wieder neue Lösungen, Standards auch natürlich daraus entwickelte Geschäftsmodelle hervorbringen.Big Data ist sehr schnell in allen Geschäftsbereichen relevant geworden, so dass sich viele Herausforderungen erst nach und nach herauskristallisiert haben. Was ist die größte Herausforderung im Zusammenhang mit Big Data? Die rechtlichen Rahmenbedingungen, die Datensicherheit oder die technische Machbarkeit?Für uns selbst fangen die Herausforderungen oft schon mit dem berühmten „Henne-Ei-Problem“ an: Kunden erwarten von uns für ihre Anforderungen fertige Use Cases und Vorschläge für mögliche Big-Data-Projekte. Wir müssen aber vor einer Lösung erst einmal beleuchten, aus welchen Datenbeständen des Kunden welche Informationen die gewünschten Ergebnisse und Vorteile bringen.Es gibt also kaum fertige Services von der Stange, jeder Kunde ist in der Lösungsfindung individuell zu betrachten. Wenn man das Thema Big Data insgesamt betrachtet, ist Datensicherheit aktuell sicher eine der größten Herausforderungen. Daher beteiligen wir uns gerade u. a. auch an dem Projekt „Trusted Data Port“, in dem es genau um die Lösung für sichere und effektive Datennutzung geht.Arvato ist weltweit mit über 70. 000 Mitarbeitern tätig. Sie haben also einen guten Überblick über die internationale Wettbewerbsituation. Ist der Standort Deutschland international konkurrenzfähig? Sind noch mehr oder andere Ausbildungs- und Studienmöglichkeiten notwendig?Amazon: 450 Big-Data-Experten für Berlin | Big Data BlogIm neuen Machine-Learning-Zentrum in Berlin will Amazon hunderte Big-Data-Experten beschäftigen, um digitale Innovationen zu entwicklen und voranzutreiben.bigdatablog.deEin klares Ja zum Standort Deutschland. Die Unternehmen erfahren nach wie vor große internationale Wertschätzung und sind konkurrenzfähig. In vielen Branchen sind wir da im Vergleich gut aufgestellt und ein Wandel innerhalb der digitalen Transformation ist schon deutlich sichtbar. Dieser Wandel spiegelt sich teilweise auch schon in den Ausbildungs- und Studienmöglichkeiten wider, sodass die nachfolgenden Generationen besser gerüstet sind.Gerade im universitären Umfeld steht z. B. „System R“ auf dem Lehrplan, die gesuchten Spezialisten werden bereits als „Data Scientists“ ausgebildet. Aber auch auf Konferenzen spüre ich die Entwicklung – hat man sich dort in der Vergangenheit „nur“ mit strukturierten Daten befasst, wird der Bogen heute deutlich mehr in Richtung semi-strukturierter Daten gespannt oder die Verknüpfung der beiden Bereiche beleuchtet.Das „Ar“ in Arvato steht für „Kunst“ („Art“) – was ist die Kunst oder der künstlerische Aspekt bei der Datenauswertung? Die Kunst der Daten-Interpretation?Die Kunst besteht hier in der richtigen Mischung der verschiedenen Aspekte. Als IT-Systemintegrator müssen wir unseren Kunden nicht nur aus den verschiedensten Big-Data-Technologien am Markt eine optimale und sichere Lösung für seinen Unternehmensbedarf aufstellen. Wir müssen hier schon vor der eigentlichen Auswertung ansetzen und mit dem Kunden gemeinsam erörtern, welche Daten aus welchen Quellen mit welchen Regeln auszuwerten sind, um dann die wertvollen Schlüsse aus deren Interpretation zu ziehen – optimal in Echtzeit.“Was erwarten Kunden von IT-Dienstleistern? Matthias Mierisch von @arvatoag im Interview. #BigData“ Twittern WhatsAppEs sind also nicht Einzelaspekte, die Kunst liegt im Zusammenspiel für ein optimales Daten-Gesamtbild, das das Unternehmen wirklich in der täglichen Arbeit unterstützt und Wettbewerbsvorteile schafft. Ein Beispiel aus der Fertigungsindustrie: Wir befassen uns gerade unter anderem mit der Auswertung von Massendaten aus der Produktionssteuerung oder Betriebs- und Maschinendatenerfassung. Die Kunst besteht hier darin, präzise Vorhersagen aus den Daten für das Unternehmen zum genau passenden Zeitpunkt zu ermöglichen, sodass man bei Bedarf in den laufenden Produktionsprozess eingreifen und die Maschinen effektiver nutzen kann. Damit bietet man Kunden einen echten Mehrwert.Herzlichen Dank für das Interview!";https://bigdatablog.de/2015/11/18/predictive-maintanence-und-predictive-analytics-interview-mit-matthias-mierisch-von-arvato-systems/;BigDataBlog;Christian Schön
12. Nov 15;" Retweets,      ""Der vernetzte Hof: Big Data in der Landwirtschaft""";"Laut Branchenverband Bitkom wendet heute jeder fünfte Landwirt digitale Anwendungen an und erwartet ein Wachstumspotential von 3 Milliarden Euro durch Digitalisierung in diesem Bereich. Die vernetzte Agrarwirtschaft kann an vielen Stellen von Datenbanken und Echtzeitüberwachung profitieren: Der Abgleich von Wetterdaten mit Daten zum Wachstum von Pflanzen, dem prognostizierten Bedarf und dem aktuellen Marktpreis ist ein komplexes Gefüge. Die Effekte von Automatisierung soweit wie möglich zu nutzen und die verfügbaren Daten zu verwerten, liegt deswegen mehr als nahe.Big Data in der Landwirtschaft: Die Vernetzung von Feldern, Kühen und MaschinenDas Internet der Dinge ergreift die gesamte Wertschöpfungskette der Landwirtschaft. Sensoren auf den Feldern können exakt den Feuchtigkeitsgehalt und die Zusammensetzung der Böden angeben. Das spart Zeit, Treibstoff sowie Wasser und lässt den logistischen Ablauf besser planen. Da es immer häufiger zu Trockenphasen kommt, stellt dies keinen unwesentlichen ökonomischen Faktor dar.Auch Kühe, Kälber, Rinder oder Schweine werden in der digitalisierten Landwirtschaft via Sensoren überwacht. Anstatt einer Marke im Ohr werden Nutztiere künftig mit Chips und Sensoren am Halsband ausgestattet, die wertvolle Daten liefern. Um das Ziel, mehr Abläufe zu automatisieren, zu erreichen, sind zunächst Informationen nötig. Die Sensoren bringen nicht nur das, sondern fördern noch weit mehr Wissen über die Tiere zutage. Ob eine Kuh besamungsbereit ist, verrät ihre Körpertemperatur, die über integrierte Thermometer permanent gemessen wird. Automatisch erhält der Landwirt eine Benachrichtigung per SMS, welche Kuh als nächstes dran ist. Die Erfassung von Vitaldaten in Echtzeit erlaubt darüber hinaus, Krankheiten frühzeitig zu erkennen und zu behandeln.“Wenn das Feld twittert &amp; die Kuh simst: #Digitalisierung bringt #BigData in die #Landwirtschaft.“ Twittern WhatsAppSchließlich garantiert das Monitoring des gesamten Maschinenparks eines landwirtschaftlichen Betriebes so wenig Ausfallzeiten wie möglich. Bereits wenn es Anzeichen von Störungen im Ablauf gibt, könnten Sensoren in Maschinen auf Probleme hinweisen. Hier spielt Big Data seine Stärken aus: Die Analyse von Mustern und Abweichungen davon werden identifiziert. Standzeiten von Geräten und deren kompletter Ausfall wird damit verhindert.Mit Datenbanken und Prognosen zu mehr ÖkologieDie Menge an Saatgut, Düngemittel und der Einsatz von Mitteln zur Schädlingsbekämpfung können durch den Einsatz von Datenbanken besser reguliert und reglementiert werden. Der Zugriff auf wissenschaftlich exakte Wetter- und Klimadatenbanken unterstützen bei Jahresplanung und dem täglichen Einsatz. Durch regionale und überregionale Zusammenschlüsse könnten Datenbanken zur Verbreitung bestimmter Schädlinge und auch Nützlinge entstehen. Dadurch würde ein intelligenterer Einsatz von Insektiziden oder Herbiziden möglich und die Landwirtschaft dadurch ökologischer werden. Doch auch die Kosten für Dünger und Saatgut sind ein entscheidender Faktor.Ökonomische Vorteile durch die Digitalisierung der LandwirtschaftOb tagesaktuelle Getreidepreise oder bei der Einhaltung der Auflagen, die nötig sind, um Agrarsubventionen zu erhalten – auch hier verhilft die konsequente Vernetzung von landwirtschaftlichen Betrieben dabei, schnell handeln zu können. Das Berufsbild von Landwirten ist bereits heute dem von modernen Unternehmern ähnlich. Sie stehen technischen Innovationen landwirtschaftlicher Geräte ebenso aufgeschlossen gegenüber wie neuen Geschäftsmodellen.“Der vernetzte Hof ermöglicht ökonomischeres &amp; ökologischeres Wirtschaften. #Digitalisierung #BigData“ Twittern WhatsAppDas enorme Wertschöpfungspotential und die Vorteile, die Big Data in der Landwirtschaft bringen, sprechen für sich. Oft sind es vielmehr technische Beschränkungen wie die fehlende Versorgung mit Breitbandinternet in den ländlichen Regionen, die der Digitalisierung entgegenstehen. Auch Vorbehalte gegenüber der Datensicherheit bremsen die Landwirtschaft 4.0 manchmal noch aus. Handlungsbedarf besteht also nicht unbedingt und ausschließlich auf der Seite der Landwirte, sondern bei Software-, Cloud- und Hardware-Anbietern.";https://bigdatablog.de/2015/11/12/big-data-in-der-landwirtschaft/;BigDataBlog;Ibrahim Evsan
06. Nov 15;" Retweets,      ""Big Data und Industrie 4.0: Interview mit Christoph Gabath von DATANOMIQ""";Das Projekt Industrie 4.0 hat das Potenzial, die Wirtschaft vollständig zu verändern – mit Auswirkungen auf die gesamte Gesellschaft. Wir stehen erst am Anfang einer langen Entwicklung, die viele Vorteile verspricht, viele Risiken birgt und deren Ende nicht absehbar ist. Orientierung und Wissen über die eigenen Handlungsmöglichkeiten sind wichtiger denn je zuvor. Christoph Gabath von der DATANOMIQ GmbH spricht mit dem Big Data Blog über die internationalen Entwicklungen und die Chancen für Deutschland.Christoph Gabath von der DATANOMIQ GmbH;https://bigdatablog.de/2015/11/06/big-data-und-industrie-4-0-interview-mit-christoph-gabath-von-datanomiq/;BigDataBlog;Christian Schön
29. Okt 15;" Retweets,      ""MongoDB und Pentaho: BI-Analysen der nächsten Generation""";"MongoDB und Pentaho: BI-Analysen der nächsten Generation | © kentoh @ Shutterstock.comBig Data bietet Unternehmen jede Menge neuer Möglichkeiten, stellt aber auch neue Anforderungen an Datenhaltung und Analyse. Im Beitrag „Real-Time Analytics mit NoSQL“ werden wesentliche Eigenschaften beleuchtet, die eine NoSQL-Datenbank wie MongoDB hinsichtlich der horizontalen Skalierbarkeit und Performance großer Datenmengen bietet. Mit der Business Analytics Suite von Pentaho lassen sich diese Daten verständlich aufbereiten und in geschäftsrelevante Informationen verwandeln – was die Wertschöpfung von Geschäftsprozessen deutlich erhöht.Das Bedürfnis der Anwender nach Verständlichkeit und ansprechender Darstellung steigt parallel zur Datenflut. MongoDB und Pentaho im Unternehmenseinsatz zusammenzubringen, ist deshalb ein äußerst vielversprechender Ansatz. Die Kombination beider Tools ermöglicht Anwendern, operative Unternehmensdaten durch echtzeitnahe Analyse und grafische Aufbereitung zu strategisch relevanten Kennzahlen zu veredeln. Der native Datenzugriff verknüpft die Vorteile eines Document-DBs mit den grafischen und benutzerfreundlichen Verwaltungs- und Analysewerkzeugen der Pentaho Business Analytics Suite. Das reduziert den Schulungs- und Wartungsaufwand, erlaubt neue Formen der Datenanalyse und -modellierung und ermöglicht auch nicht-technischen Nutzergruppen, direkt mit der Datenbank zu interagieren.MongoDB und Pentaho: Nahtlose Integration und leistungsstarke AnalysenMongoDB kann auf Grund des Shardings mehrerer Server auch eine sehr große Anzahl strukturiert und unstrukturiert vorliegender Daten sehr schnell bereitstellen. Um diese Daten in kurzen Intervallen analysieren und bewerten zu können, bedarf es einer geeigneten Software. Pentaho Business Analytics ist solch eine Datenanalysesoftware. Sie besteht aus verschiedenen Modulen und deckt unterschiedliche Bereiche der Business Intelligence (BI) ab. Dazu zählen u.a. der ETL-Bereich, Reporting, OLAP und Data Mining, die das gesamte Spektrum von Operational Intelligence bis hin zu Self-Service-BI bedienen.Pentaho nutzt zur Interaktion grafische Drag&amp;Drop-Oberflächen, die das Arbeiten mit einer technischen Anwendung wie MongoDB erleichtern. Pentaho integriert MongoDB nativ. Das heißt: Pentaho greift direkt auf die von MongoDB bereitgestellten Funktionen und Inhalte wie das Aggregation Framework, die Tag Sets und die Replikationen zu. Das beschleunigt die Verfügbarkeit von Unternehmensdaten für alle Nutzergruppen und hilft, die Produktivität zu verbessern.Integration der Datenbank MongoDB in die Pentaho BA SuiteDirekte Datenverarbeitung aus NoSQL-SpeichernEine zentrale Aufgabe im BI-Umfeld sind Extraktions-, Transformations- und Ladeprozesse (ETL). Um die von MongoDB bereitgestellten Daten direkt zu integrieren, nutzt Pentaho seine umfassende Datenintegrationsplattform Pentaho Data Integration (PDI). PDI ist ein grafisches ETL-Tool zum Laden und Verarbeiten der in MongoDB hinterlegten Daten. Mittels Data Blending und interaktiven Visualisierungen lassen sich Kodierung und Komplexität deutlich reduzieren. Das ermöglicht beispielsweise auch dem Marketing, Big Data-Analyselösungen auf den Features von MongoDB aufzubauen. Weil kein aufwändiger MapReduce-Code zu schreiben ist, sparen IT-Experten wegen der wegfallenden manuellen Programmierarbeiten ebenfalls viel Zeit.Verknüpfung von Pentaho Data Integration (PDI) und MongoDBOLAP-Auswertung mit dem Analyzer-ModulNachdem alle relevanten Daten und Kennzahlen in einem Mondrian-Schema hinterlegt und zu OLAP-Würfeln zusammengefasst sind, geht es an die Analyse. Hierfür verwendet Pentaho den Analyzer – ein Tool zur Auswertung und Visualisierung von Daten. Bis vor kurzem war der Pentaho Analyzer nur in Verbindung mit relationalen Datenbanksystemen einsetzbar. Doch mit Version 5.1 der Pentaho BA-Suite lassen sich Ad-Hoc-Analysen, Dashboards und Reports nun auch aus MongoBD erzeugen. Damit liefern Pentaho und MongoDB eine Rund-um-Unternehmensansicht und bieten kleinen wie großen Unternehmen die Möglichkeit, die Wertschöpfungspotentiale von Big Data voll zu nutzen.Visualisierung schafft WissenMongoDB bietet eine effiziente und leistungsstarke Möglichkeit, besonders große und stark variierende Daten zu verwalten. Gerade durch die Techniken der In-Memory-Verarbeitung, der horizontalen Skalierung und dynamischer Schemata ist MongoDB sehr gut auf die Anforderungen und Einsatzszenarien im Big-Data-Bereich abgestimmt. Aber auch traditionelle Anwendungen lassen sich zuverlässig umsetzen, weshalb MongoDB im professionellen Unternehmenseinsatz verstärkt als Alternative zu relationalen Datenbanken genutzt wird.“#BI-Analysen der nächsten Generation – wir zeigen, was #MongoDB &amp; #Pentaho leisten. #Data“ Twittern WhatsAppMit der Pentaho BA-Suite lassen sich Berichte, Dashboards und Analysen direkt aus MongoDB erstellen und mit einer Vielzahl anderer Datenquellen zu interaktiven Visualisierungen kombinieren. Im Unternehmenseinsatz bringen beide Tools ihre individuellen Stärken in die Business Intelligence ein, sodass sich eine Vielzahl von Use Cases und Anwendungen abdecken lässt.";https://bigdatablog.de/2015/10/29/mongodb-und-pentaho-bi-analysen-der-naechsten-generation/;BigDataBlog;Stefan Müller
21. Okt 15;" Retweets,      ""MongoDB und Pentaho: Real-Time Analytics mit NoSQL""";MongoDB und Pentaho: Real-Time Analytics mit NoSQL | © kentoh @ Shutterstock.comSeit Pentaho die Version 5.1 seiner Business Intelligence Suite veröffentlicht hat, lassen sich native Business Intelligence-Analysen auch direkt auf nicht-relationalen MongoDB-Speichern durchführen. Der Pentaho Analyzer ist damit die erste echte End-to-End-Analyselösung für MongoDB. Das Tool ermöglicht es dem Anwender, die Vorteile der dokumentenorientierten Datenbank in puncto Flexibilität und Geschwindigkeit voll auszuschöpfen. Zeit, einen Blick auf MongoDB, den Marktführer im Bereich NoSQL-Datenbanken, zu werfen.MongoDB ist zwar erst seit 2009 am Markt, doch bereits eine der führenden NoSQL-Datenbanken. Es wurde mit dem Ziel entwickelt, die strukturbedingten Begrenzungen traditioneller Datenbankensysteme zu durchbrechen, um den steigenden Anforderungen an die Geschwindigkeit von Datenverarbeitungen und -auswertungen gewachsen zu sein. Konkret beinhalten NoSQL-Umgebungen die Möglichkeit, unstrukturiert vorliegende Daten zu erfassen. Der entscheidende Unterschied zu relationalen Datenbanksystemen besteht darin, dass MongoDB auf feste Tabellenschemata verzichtet und Dokument-orientiert arbeitet.Dokumentenbasierte Speicherung und dynamische SchemataEine der Kerntechniken von MongoDB ist die dokumentenbasierte Speicherung der Daten. MongoDB ist auf ein dynamisches Schema ausgelegt, dass sich neuen Daten flexibel anpassen kann. Auf MySQL-typische Relationstabellen und Joins, die mit dem Ausbau der Datenbank immer komplexer werden, wird verzichtet. Um Dokumente mit ähnlicher Struktur zusammenzufassen, nutzt das Datenbankmanagementsystem sogenannte Collections. Die einzelnen Dokumente können eine beliebige Anzahl frei definierbarer Felder besitzen, und auch verschachtelte Array-Strukturen sind möglich.Daten werden in Form von Dokumenten im BSON-Format (Binary JSON) abgespeichert. Durch die Java-nahe Notation eignet sich MongoDB insbesondere für Web-Anwendungen, da die Daten ohne Transformation direkt vom Datenbanksystem abgespeichert werden können. Bei MongoDB gibt es keine Abfragesprache wie SQL. CRUD-Operationen (create, read, update, delete) werden durch objektspezifische Methoden der jeweiligen Programmiersprache realisiert, die in verschiedenen Client-Drivers zusammengefasst sind. MongoDB unterstützt alle im Web gängigen Programmiersprachen wie JavaScript, Perl, Python oder Ruby.Performance, Skalierbarkeit und Ausfallsicherheit von MongoDBMongoDB eignet sich vor allem für Anwendungen mit einem hohen Datenvolumen und Datendurchsatz, also im Bereich Real-Time-Analytics. Ausschlaggebend sind dafür die horizontale Skalierung über Sharding und das Memory-Mapping.Beim Sharding werden die Daten anhand des Shardingkeys auf mehrere virtuelle Maschinen aufgeteilt (in sogenannte Chunks). Der Config-Server gibt Auskunft darüber, wo welche Shards hinterlegt sind. Synchronisiert werden sie von mongos, eine Art Router. Vorteil des Shardings: der Arbeitsaufwand, wie er zum Beispiel beim Abfragen von Daten entsteht, wird parallel durch mehrere Rechner erledigt. Das liefert eine hohe Performance und minimiert die Gefahr von möglichen Engpässen (Bottlenecks). Durch Verarbeitungsgeschwindigkeiten, die mit einem In-Memory-System vergleichbar sind, ermöglicht MongoDB die Auswertung von Daten in Echtzeit.Die Ausfallsicherheit wird bei MongoDB durch Replica Sets sichergestellt. Ein Replica Set besteht aus mindestens einem Primär- und einem Sekundärknoten. Fällt das primäre Replikat aus, beispielsweise aufgrund von Hardware- oder Softwareproblemen, wählt MongoDB ein sekundäres Replikat und konfiguriert es als neues Primärreplikat. Das ausgefallene Replikat wird nach der Wiederherstellung automatisch als Sekundärreplikat konfiguriert.MongoDB: Anwendungsfälle und AnwenderMongoDB wird von namhaften Anwendern für die gängigen BI-Bereiche ETL, OLAP und Data Mining angewendet.Case 1: High Volume Data FeedsBig Data, also riesige Datenströme, zeichnen sich dadurch aus, dass sie extrem schnell erzeugt werden und in sehr unterschiedlicher Form vorliegen. Häufig werden dabei semi- oder unstrukturierte Datenquellen verwendet. Auch das Format und die Struktur der Daten kann stark variieren. Für die schnelle Verarbeitung dieser Daten sind daher besonders skalierbare und performante Datenbankmanagementsysteme notwendig, die flexibel bezüglich der Struktur und der Datenformate sein müssen. MongoDB bietet diese Möglichkeiten und wird deshalb im Big-Data-Kontext gerne eingesetzt, zum Beispiel von dem Online-Netzwerk Craigslist und dem Sicherheitssoftware-Hersteller McAfee.Case 2: Operational IntelligenceHinter dem Begriff Operational Intelligence (OI) verbirgt sich eine neue, dynamische und in Echtzeit agierende Form der Datenauswertung. Bei OI werden maschinengenerierte Daten in Echtzeit geladen und verarbeitet. Möglich ist das durch ein direktes Ablegen und Indexieren der Daten. Eine spezielle Transformation der Maschinendaten, wie z.B. bei Log- und Sensordateien, ist nicht notwendig. So können ungewöhnliche Ereignisse schneller identifiziert und analysiert und eventuelle Fehler rascher behoben werden.Operational Intelligence wird gerne für Real-Time-Dashboards verwendet. In diesen Dashboards lassen sich Ereignisse ohne große Verzögerung graphisch darstellen. Neue Möglichkeiten bietet OI auch im Marketing-Bereich. Hier wird MongoDB als Grundlage für AD-Targeting und Social Media Monitoring verwendet. Bekannte Nutzer des Datenbankmanagementsystems im OI-Sektor sind z.B. der CRM-Entwickler Salesforce und die Kreativagentur Razorfish.Case 3: Content ManagementContent-Management-Systeme dienen zur Speicherung und Verwaltung von Informationen und Metadaten für eine Reihe von Anwendungen wie Webseiten und Datenarchive. Inhalte sind heute viel komplexer und bestehen neben Texten auch aus Bildern, Tönen, Videos und integrierten sozialen Medien. Die Mehrheit der Datenbankmanagementsysteme (DBM) stößt dabei schnell an ihre Grenzen, weil sie die heterogenen Daten nur unter großem Aufwand bzw. durch Kombination verschiedener DBMs verwalten können.“Ihr wollt wissen, wie ihr #MongoDB anwenden könnt? – 3 Beispiele #NoSQL #Data“ Twittern WhatsAppEine Alternative ist MongoDB. Die dokumentenorientierte Speicherform und das dynamische Schemadesign ermöglichen die effiziente Verwaltung der heterogenen Daten unabhängig von Herkunft und Struktur. Alle Daten können durch einen einzigen Datenspeicher repräsentiert werden. Dadurch ist es vergleichsweise einfach möglich, inhaltsreiche Anwendungen zum Beispiel mit personalisiertem Inhalt und Layout aufzubauen. Im Umfeld Content Management setzen das Reiseportal Expedia und der Softwareentwickler SAP MongoDB ein.Die Beispiele zeigen, dass NoSQL im Business-Intelligence-Bereich eine immer größere Rolle spielt. Im nächsten Beitrag „BI-Analysen der nächsten Generation“ wenden wir uns der BI-Suite von Pentaho zu und zeigen, was das quelloffene Open Source-BI zusammen mit MongoDB im Unternehmenseinsatz leisten kann.;https://bigdatablog.de/2015/10/21/mongodb-und-pentaho-real-time-analytics-mit-nosql/;BigDataBlog;Stefan Müller
15. Okt 15;" Retweets,      ""Das Amazon Development Center in Berlin: Ein Gespräch über die Zukunft von Big Data""";Amazon ergänzt seit einigen Jahren seine Geschäftsfelder im Bereich Big Data Dienstleistungen. In Berlin wurde dafür das Amazon Development Center für Mashine Learning eröffnet. Dort sollen nicht nur neue Services für externe Kunden weiterentwickelt werden, sondern auch ein Ort für Zusammenarbeit entstehen. E-Book-Autoren treffen auf Entwickler, Entwickler auf die Startup-Szene. Der Dircetor of Machine Learning Science, Dr. Ralf Herbrich, hat sich für unsere Fragen Zeit genommen.This Interview is also available in EnglishHerr Herbrich, in Berlin gibt es bereits seit 2013 Amazon Web Services, worüber Big-Data-Dienstleistungen wie die Stimmungsanalyse durch die Auswertung von Twitter-Mitteilungen genutzt werden können. Wie unterscheiden sich davon die Dienstleistungen, die Sie im neuen Amazon Entwicklungszentrum anbieten?Amazon arbeitet beständig an Innovationen für seinen Kunden. Das Berlin Development Center ist dafür ein großartiges Beispiel. Im Berliner Development Center arbeiten wir an der Entwicklung der nächsten Gerneration von Amazons Technologien mit dem Fokus auf Machine Learning und Verteilten Systemen. Das Development Center ist eigentlich auch schon seit 2013 in Berlin und war zunächst in einem temporären Büro in Berlin Charlottenburg, bis wir im Juni 2015 in die Krausenhöfe in unser neues Büro eingezogen sind.Neben anderen Teams, bietet das Büro inzwischen auch Amazon Web Services (AWS), dem AWS OpsWork Team und dem Amazon Machine Learning Team eine Heimat. Einige der Technologien, die von diesen Teams entwickelt werden, sind bereits live und für die Kunden verfügbar wie beispielsweise Amazon Machine Learning und AWS Ops Works. All diese Technologien ermöglichen es, unseren Kunden neue Möglichkeiten zu bieten – seien das die neuen Features für AWS oder die internen Möglichkeiten, die es uns erlauben, die Amazon Pakete noch schneller an die Amazon Einzelhandelskunden zu liefern.;https://bigdatablog.de/2015/10/15/amazon-development-center-fuer-machine-learning/;BigDataBlog;Christian Schön
03. Sep 15;" Retweets,      ""Superschnelle Datenintegration: Pentaho integriert Apache Spark""";Gute Neuigkeiten für die Anwender der Big Data-Tools Pentaho und Apache Spark: Pentaho Data Integration (PDI) verfügt zukünftig über eine native Integration von Spark. Das beschleunigt Datenintegrationsprozesse für Big Data-Analysen enorm.Der Big Data-Hersteller Pentaho Corporation hat kürzlich die native Integration von Pentaho Data Integration (PDI) mit Apache Spark bekannt gegeben. Damit können Datenintegrations-Jobs mit Spark direkt aus Pentaho gesteuert werden. Die Integration hat die Big Data-Szene aufhorchen lassen, denn dadurch lässt sich die Produktivität von Daten-Projekten stark steigern, Wartungskosten reduzieren und die für Spark erforderlichen Mitarbeiterqualifikationen deutlich senken.Spark-Framework ergänzt PDIApache Spark ist ein Cluster-Computing-Framework, das entwickelt wurde, um anstelle von Hadoop MapReduce auf der Oberseite des Hadoop Distributed File-Systems (HDFS) zu laufen. Bei der Entwicklung wurde besonderer Wert auf Schnelligkeit, Benutzerfreundlichkeit und maschinelles Lernen geachtet. PDI ist im Wesentlichen eine tragbare Datenaufbereitungsmaschine für ETL, die als Stand-Alone-Pentaho-Cluster oder in einem Hadoop-Cluster über MapReduce oder Yarn eingesetzt werden kann. Mit Spark wird PDI um ein neues Framework ergänzt, durch das Big Data ETL-Prozesse noch schneller durchgeführt werden können. Anwender können ETL-Jobs mit Hilfe der grafischen Design-Umgebung im PDI entwickeln, testen und optimieren und später auf Spark laufen lassen. Da beide Lösungen auf Open Source basieren, war es für Pentaho einfach, das ebenfalls quelloffene Spark-Projekt zu integrieren.Was bringt die Integration Big Data-Usern?Durch die Unterstützung für In Memory Cluster-Computing kann mit Spark eine Leistung erreicht werden, welche die von Hadoop MapReduce deutlich übersteigt. Dadurch sind Analysen in Echtzeit möglich. Das bedeutet, dass Anwender keine separate Technologie mehr benötigen, um Realtime Analytics durchzuführen. Darüber hinaus umfassen die großen Arbeitslasten solcher Auswertungen typischerweise unterschiedliche Datentypen, die jeweils auf bestimmte Art und Weise analysiert werden müssen. Das ist eine weitere Stärke von Spark.Einsatzbereiche für Spark und PentahoPentaho arbeitet gerade daran, Beispiel-Szenarien für den Einsatz von Spark zu entwickeln. Damit möchte das Unternehmen die Anwender dabei unterstützen, die richtigen Einsatzbereiche für Spark und Pentaho zu finden und Big Data-Projekte einfacher umzusetzen. Dazu gehören unter anderem direkt auf Spark SQL ausgeführte Abfragen und parallel durchgeführte Spark-Abfragen (Spark Parallel Execution).“#Pentaho integriert #ApacheSpark – das beschleunigt Prozesse für #BigData-Analysen enorm.“ Twittern WhatsAppDer Big Data-Markt befindet sich in einem kontinuierlichen Wandel. Spark gehört zu den Projekten in diesem Bereich, an denen am aktivsten gearbeitet und mitentwickelt wurde. Die Lösung besitzt daher strategische Bedeutung für einen Big Data -Anbieter wie Pentaho. Durch den Open Source-Ansatz können neue Technologien einfach integriert werden und bietet Kunden aktuelle Innovationen – ein enormer Vorteil in einem so dynamischen Markt wie dem für Big Data-Anwendungen.;https://bigdatablog.de/2015/09/03/pentaho-integriert-apache-spark/;BigDataBlog;Stefan Müller
24. Aug 15;" Retweets,      ""Big Data als Voraussetzung für das Semantic Web""";Einer der Pioniere der Computertechnologie, Alan Turing, träumte bereits davon, dass es einmal möglich sein wird, sich mit einer Maschine zu unterhalten. Mit Big Data und dem Semantic Web könnte Turings Traum bald Wirklichkeit werden. Der nach Alan Turing benannte “Turing-Test” ist eine Versuchsanordnung, bei der eine Person einem Gegenüber Fragen stellt. Allerdings sieht der Fragensteller sein Gegenüber nicht, weiß damit auch nicht, ob ihm eine Maschine oder ein Mensch antwortet. Die Antworten selbst sollen Aufschluss darüber geben, ob ein Mensch sie gegeben hat oder nicht. Eine künstliche Intelligenz, der es möglich ist, Sprache und Bedeutung zu verstehen, muss sehr viele Informationen auf einmal auswerten.Wie das Semantic Web Sprache verstehtDas Wort “Golf” kann ganz verschiedenes bedeuten. Es ist eine Sportart, ein Autotyp, aber auch der Teil des Meers wie der Golf von Mexiko. Wann welche Bedeutung des Wortes gemeint ist, hängt stark vom Kontext ab. In einem Gespräch ist uns sehr schnell klar, was jeweils mit Golf gemeint ist. Wenn jemand allerdings in den Google-Suchschlitz das Wort Golf eintippt, weiß der Suchalgorithmus im Regelfall nicht automatisch, wonach wir suchen. Inzwischen besitzt Google eine ganze Menge Informationen über seine Nutzer, sodass die Suchmaschine Annahmen machen kann, nach was eine Person sucht. Deswegen sehen Suchergebnisse, bei identischen Suchbegriffen, nicht immer gleich aus.Diese Unterschiede zu verstehen ist nur der erste winzige Schritt in die Richtung intelligente Mensch-Maschine-Kommunikation. Die manchmal witzigen und verschrobenen Antworten von Siri zeigen, wie weit der Weg noch ist, bis eine künstliche Intelligenz wirklich versteht, was wir meinen. Zudem sind die Antworten im Moment mehr oder weniger stark formalisiert und vorgegeben. Siri kann also nicht wirklich aus einem Wortschatz auswählen und Sätze bilden. Oder neue Worte dazulernen, auch wenn wir versuchen würden, ihr die Worte zu erklären.“Mit dem #SemanticWeb beginnt die Technik uns Menschen zu verstehen, wenn wir sprechen. #BigData“ Twittern WhatsAppDie Chance aber besteht, dass ein Computer sich selbständig die Bedeutung von Worten erschließt. Dazu ist es “nur” nötig, eine genügen große Menge von Texten nach dem entsprechenden Wort und seiner jeweiligen Verwendung zu analysieren. Denn die Semantik, also die Bedeutung der Worte, entstehen aus ihrer Verwendung. Der Satz “Ich gehe Golf spielen” kann nur auf eine Sportart verweisen. Das heißt, wenn das Wort “Golf” im Zusammenhang mit dem Wort “spielen” auftaucht, ist die Sportart Golf gemeint. Manchmal ist es aber nicht so eindeutig. Nur der Satz “Ich gehe zum Golf” genügt beispielsweise nicht aus, um eindeutig zu identifizieren, was gemeint ist. In solchen Fällen sind mehr Informationen notwendig. Um so komplexe Zusammenhänge zu analysieren sind enorme Datenmengen in Echtzeit auszuwerten.Was steht in den Sätzen davor und danach?Wer sagt den Satz?In welcher Situation fällt der Satz? etc.Semantic Web: Das Web 3.0 wird uns verstehenDank Big Data wird das Semantic Web, auch Web 3.0 genannt, die Fähigkeit erwerben, uns zu verstehen. Dank vieler Informationen, die zu jeder Zeit verfügbar sind und einbezogen werden können. Ein Smartphone beispielsweise weiß, ob sich sein Besitzer gerade zuhause aufhält, im Urlaub ist, auf dem Weg zu einem Termin ist, welche Uhrzeit gerade ist etc.Solche Umgebungsvariablen helfen, um bestimmte Bedeutungen auszuschließen oder wahrscheinlich zu machen. Den Status quo dessen, was derzeit möglich ist, stellte das Computerprogramm Watson unter Beweis, als es in der Fernsehsendung Jeopardy gegen die menschlichen Herausforderer gewann. Dazu wertete er eine Textdatenbank mit 100 Gigabyte in Echtzeit aus. Allerdings nutzte er dazu nicht nur einen einzigen intelligenten Algorithmus, sondern eine große Menge von verschiedener Algorithmen, die alle gleichzeitig dieselbe Aufgabe bearbeiteten.“Mit #BigData wird aus dem Web 2.0 das #Web 3.0 – auch #SemanticWeb genannt.“ Twittern WhatsAppJe mehr Algorithmen auf das gleiche Ergebnis kamen, desto wahrscheinlicher war die Antwort richtig. Das Prinzip des Semantic Web, auf dem auch Watson beruht, hat den Vorteil in größeren Zusammenhängen zu “denken”. Damit Watson zu dieser Leistung fähig war, benötigte er 90 Server und insgesamt 16 Terabyte Ram. Da es nur eine Frage der Rechenleistung ist, um die Bedeutung der Sprache zu verstehen, ist es also nur eine Frage der Zeit, bis das Semantic Web Wirklichkeit wird.;https://bigdatablog.de/2015/08/24/big-data-als-voraussetzung-fuer-das-semantic-web/;BigDataBlog;Christian Schön
23. Aug 15;"     ""The Amazon Development Center in Berlin: An Interview about the Future of Big Data""";A few years ago Amazon has startet to replenish its business with Big Data Services – and for that opened its Amazon Development Center for machine learning in Berlin, where authors of e-books meet engineers and engineers meet startups. Dr. Ralf Herbrich, Director of Machine Learning Science, has answered our questions.Dieses Interview auf Deutsch lesenSince 2013 Amazon Web Services is located in Berlin, offering Big Data Services like Sentiment Analysis using data from Twitter. In which way are the services of the new Amazon Development Centre different?Amazon is constantly innovating on behalf of our customers and the Berlin Development Center is a great example of this. At the Berlin Development Center we are working to develop next-generation technologies for Amazon with a particular focus on the area of machine learning and distributed systems. The Berlin Development Center started in a temporary office in Berlin Charlottenburg from September 2013 until June 2015 when we opened our new Berlin office in Krausenhöfe.Among other teams, the office is home to the Amazon Web Services (AWS) team, the AWS OpsWorks team as well as the Amazon Machine Learning team. Some of the technologies developed by these teams have already been launched and made available for customers, such as Amazon Machine Learning and AWS OpsWorks. All of these technologies enable both Amazon and AWS to deliver new capabilities to our customers, whether that be new features for AWS or internal capabilities that allow us to get packages in the hands of Amazon retail customers quicker.;https://bigdatablog.de/2015/08/23/machine-learning-an-interview-about-the-future-of-big-data/;BigDataBlog;Christian Schön
21. Aug 15;" Retweets,      ""Big-Data-Trend: Telematiktarife in der Versicherungsbranche""";"Die Einführung von sogenannten Telematiktarifen spaltet die Versicherungsbranche in zwei Lager. Während die ersten Krankenkassen die Anschaffung von Smart Watches oder Fintessarmbändern bezuschussen und Tarifvergünstigungen gewähren, lehnen andere Kassen dies strikt ab. Gründe, die gegen Telematik in der Versicherungsbranche sprechen, sind das Solidaritätsprinzip, auf dem die Kassen beruhen und datenschutzrechtliche Bedenken. Viele Kassen können und wollen im Fall von Fitnessbändern nicht kontrollieren, wer sie besitzt oder nutzt. Zudem befürchten sie, dass eine Art Zwei-Klassen-System entsteht, in dem die Kosten ungleich verteilt sind.Pro Telematik: Diese Vorteile können entstehenDiesen Einwänden steht eine ganze Reihe von positiven Effekten gegenüber, die durch die Telematik erreicht werden können. Die nachgewiesene Auswirkungen von E-Health auf die Gesundheit ist dabei der Aspekt, auf dem sie beruhen. Weniger Arztkosten, weniger Kosten für Medikamente, weniger Krankheitsausfälle führen zu weniger Kosten für die Versicherungspolicen. Darüber hinaus profitiert die medizinische Forschung von den Daten der Fitnsesstracker, Smartphones und Smart Watches.Telematiktarife bei AutoversicherungenDie Versicherungen Allianz, HUK Coburg, VHV und Itzehoer Versicherung sind die ersten Anbieter auf dem Versicherungsmarkt, die ankündigten, Telematiktarife einzuführen. Die Allianz kann bereits auf eine erfolgreiche Umsetzung dieses neuen Modells in Italien blicken. Dort verändert Big Data bereits die Versicherungsbrache: zwei Millionen Italiener ließen Blackboxes in ihre Autos installieren, die nun ihren Fahrstil aufzeichnen. Wer sich an die Straßenverkehrsordnung hält, profitiert von Vergünstigungen.“Wer eine #Blackbox installiert, kann von neuen #Telematiktarifen einiger Versicherer profitieren.“ Twittern WhatsAppDie Anzahl der Sensoren in Autos und das Datenaufkommen ist enorm hoch, so dass an der Sammlung und (Echtzeit-)Auswertung nicht nur die Versicherungen interessiert sind.Perspektiven: Telematik in anderen BranchenNeben der Versicherungsbranche verspricht sich vor allem die Automobilindustrie viel von dem Einsatz von Telematik-Systemen. Die drei großen Autohersteller Audi, BMW und Mercedes schmiedeten eine Allianz, um gegen die Internetgiganten Google und Apple anzukommen. Gemeinsam kauften sie den Kartendienst Nokia Here, den sie als Joint Venture weiterentwickeln. Das Ziel der Allianz ist es, umfassendes 3-D-Kartenmaterial zu sammeln und in Zukunft für das automatisierte Fahren zu nutzen.Hierzu sind äußerst präzise Karten notwendig, die nicht mit der heute zulässigen Toleranz von bis zu 2 Metern arbeiten können. Bei der Erstellung der Karten ist es nötig 700.000 Messpunkte pro Sekunde zu erfassen. Der Weg zu einem so detaillierten Kartenmaterial geht über telematische Systeme in herkömmlichen Fahrzeugen. Autos werden zukünftig dauerhaft zu fahrenden Messsystemen werden. Dies ist notwendig, um in Echtzeit das Kartenmaterial für Navigationssysteme und selbstfahrende Autos auf dem aktuellen Stand zu halten. Dadurch kann die genaue Position von Baustellen, Pannenfahrzeugen oder anderen Gegenständen erfasst und weitergegeben werden.“#Krankenkassen unterstützen #Smartwatches &amp; Co: So verändert die #Telematik die Versicherungsbranche.“ Twittern WhatsAppVon Vorteilen wie diesen werden immer mehr Branchen ihren Nutzen haben. Den Automobilherstellern werden die Logistikunternehmen und Reiseunternehmen folgen. Bei der Bewertung der Möglichkeiten durch die neuen Technologien, wird entscheidend sein, wie sie eingesetzt werden. Die Versicherungsbranche ist die erste, die unter Beweis stellen kann, dass der Nutzen von Big Data nicht auf Kosten der Allgemeinheit geht.";https://bigdatablog.de/2015/08/21/big-data-trend-telematiktarife-in-der-versicherungsbranche/;BigDataBlog;Ibrahim Evsan
20. Aug 15;" Retweets,      ""Big Data-as-a-Service: Wenn Big Data und Cloud Computing eins werden""";"Wir kennen diese Zahlen: Experten erwarten 44 Zettabytes an Daten bis 2020, rechnerisch rund 5.200 Gigabyte für jeden Erdenbürger. Speicherhersteller heben bereits warnend den Finger vor einem drohenden Speicherengpass. Einen Großteil der Daten werden dann allerdings Geräte beisteuern: 50 Milliarden davon sind im Internet of Things bis 2020 verbunden. Diese „Dinge“ sondern unentwegt Informationen ab, die Antworten auf Fragen bergen, die wir noch gar nicht kennen – soweit die Geschichte von Big Data, die Ihnen vertraut sein dürfte.Was wir brauchen sind logische Schlussfolgerungen – das Beispiel Predictive PolicingDie große Kunst besteht in der logischen Verbindung vieler Daten aus unterschiedlichen Quellen und deren schnelle Auswertung. Beeindruckende Ergebnisse liefert bereits die vorausschauende Polizeiarbeit. Entwickelt wurden die Analysewerkzeuge für „Predictive Policing“ in den USA auf Basis von Programmen, die bei Erdbeben die Nachbeben vorhersagen. Die Polizei von Los Angeles setzt sie erfolgreich ein. Inzwischen ist diese Art der Polizeiarbeit in Zürich im Dauerbetrieb und auch Bayern testet solche Systeme seit 2014. Bei Einbrüchen wertet der Computer Daten aus wie: Tageszeit, Gebäude, wie kamen die Täter ins Haus, was haben sie mitgenommen? Und der Computer schaut in die Zukunft und prognostiziert weitere Einbrüche in der näheren Umgebung in den nächsten sieben Tagen. Von immerhin bis zu 30 Prozent weniger Einbrüchen soll aufgrund des Verfahrens die Rede sein.Services wandern ins Internet und verwalten sich selbstTechnische Basis für solche Algorithmen sind viel Speicherplatz, schnelle Rechner und Analyseprogramme. Nur werden die wenigsten Unternehmen diese Infrastruktur für eine kleine Anzahl von Analysen anschaffen. Sogar die Autoindustrie mietet solche Ressourcen für digitale Crashtests. Das bedeutet: Die Bausteine für Big Data werden Stück für Stück ins Internet wandern, wie wir es schon bei vielen anderen Prozessen gesehen haben.Beispiel Software-defined data center: Erst haben wir Speicherplatz virtualisiert, dann Rechenkapazitäten, dann Programme. Inzwischen arbeiten wir an Security-as-as-Service und Network-as-a-Service. Am Schluss steht im Grunde genommen IT-as-a-Service. Alle Komponenten sprechen die selbe Sprache, sind aus der Ferne ansteuerbar und können Ressourcen teilen. Sie verwalten sich weitgehend selbst.Damit die Technik nicht zum Flaschenhals wird, wie es die Speicherindustrie fürchtet, müssen die Daten in die Wolke umziehen. Massendaten brauchen zwar Sicherheitskopien, müssen aber nicht 1.000 Mal parallel gespeichert werden. Dies ist eines der Grundprinzipien der Digitalisierung: Es reicht ein Original für viele zeitlich beschränkte Kopien – bei Musik und Film heißt das Streaming. Bei Big Data beginnt diese Entwicklung etwa mit einem Marktplatz für Massendaten: Jedes Unternehmen, dass viele Daten produziert, könnte diese auf dem Marktplatz anbieten.Big Data-as-a-Service: Wie Cloud Computing und Big Data voneinander profitierenVoraussetzung ist ein gemeinsames Datenformat für Big Data. So könnte die Bahn viele Daten von Temperaturfühlern aus tausenden von Weichen veredeln. Die Bahn braucht diese Sensoren, um im Winter drohende Vereisung zu erfassen. Drei Viertel des Jahres aber liefern diese Geräte Informationen, die die Bahn gar nicht braucht. Irgend ein kluger Kopf auf der Welt wird jedoch auf eine Idee kommen, diese zu nutzen, weiterzuverarbeiten und zu verkaufen. So funktionieren Plattformen als Geschäftsmodell: Sie sind erfolgreich, wenn andere damit mehr Geld verdienen.“#Cloud &amp; #BigData werden eins – wie Wolken und Daten künftig voneinander profitieren können.“ Twittern WhatsAppDie Massendaten sind bereits im Internet: Sei es als Open Source oder über erste Marktplätze. Es gibt auch schon Ansätze für den zweiten Schritt: Die Analysetools werden virtualisiert und stehen in der Cloud bereit. Das ist der richtige Weg: Aus Big Data wird Big Data-as-a-Service. Die technischen Hürden nimmt uns die Cloud. Als letzte Herausforderung bleibt nur noch unsere Kreativität.";https://bigdatablog.de/2015/08/20/big-data-as-a-service-wenn-big-data-und-cloud-computing-eins-werden/;BigDataBlog;Dr. Ferri Abolhassan
18. Aug 15;" Retweets,      ""Security Intelligence: Mehr Sicherheit durch Big Data""";Big Data und das Zeitalter der Security Intelligence | © adike @ Shutterstock.comAttacken auf IT-Systeme von Firmen, Regierungsorganisationen und Privatpersonen werden immer intelligenter. Der Preis, den Unternehmen und Organisationen dabei zahlen, ist hoch.Rund 50% aller Unternehmen haben bereits Cyper-Attacken mit dem Ziel des Datendiebstahls, der Wirtschaftsspionage oder der Sabotage zu verzeichnen. Die Sicherheitsarchitekturen, die rund um Firmengeheimnisse und sensible Daten gebaut ist, lassen in den meisten Fällen zu wünschen übrig. Darüber hinaus gebe es oft für den Fall eines Angriffs aus dem Netz keinen Notfallplan, da kein Bewusstsein für die abstrakte Bedrohung vorhanden sei.IT-Sicherheit dank Prävention mit Big Data Security IntelligenceBig Data Security Intelligence (BDSI) lautet die präventive Gegenmaßnahme auf diese Bedrohungen. Sie stellt eine evolutionäre Weiterentwicklung von normalen IT-Sicherheitssystemen dar. Bislang funktionieren diese auf Basis von Archiven, in denen bekannte Viren, Trojaner oder Angriffsmuster gespeichert sind. Trotz Echtzeitüberwachung gelingt es mit neuen Taktiken und neuen Programmen immer wieder, in überwachte Systeme einzudringen.“#BigData Security Intelligence (#BDSI) stellt die nächste Stufe von sicheren Systemen dar.“ Twittern WhatsAppDie Unternehmen machen es den Angreifern aber auch leicht: Gerade mal 23% der Unternehmen verfügen laut Bitkom überhaupt über ein Angriffserkennungssystem (Intrusion Detection). Der Schlüssel zu mehr Sicherheit liegt in den individuellen Verhaltensmustern der “normalen” Nutzer, die aus Maschinen- und Logdaten gewonnen werden können.Angriffe sind Abweichungen von der NormDamit big-data-gestützte Sicherheitssysteme arbeiten können, benötigen sie einen gewissen Vorlauf, beziehungsweise müssen sie auf historische Daten zurückgreifen können. Aus hinreichend großen Datensätzen lassen sich dann Muster ableiten und Personen zuordnen, die über eine Zugangsberechtigung zu einem System verfügen. Dabei gilt: Je größer und vielfältiger die zur Verfügung stehenden Daten sind, desto exakter können Verhaltensweisen identifiziert werden. Ein Angriff nimmt sich im Verhältnis zu diesem als Norm erkannten Muster dann als Abweichung aus.Ein Sicherheitssystem, das auf Big Data beruht, hat demnach in einen strategischen Vorteil, da es Angriffe auch von bislang unbekannten Programmen zielsicher erkennt. Gleichzeitig steigen die technischen Anforderungen an das Sicherheitssystem. Die Überwachung in Echtzeit umfasst dann nicht mehr nur die Auswertung des aktuellen Geschehens, sondern immer zugleich den Abgleich mit den NoSQL-Datenbanken.Next Generation Firewalls (NGFW) vs. Big Data Security Intelligence (BDSI)Neben der BDSI gibt es noch weitere technologische Weiterentwicklungen, die versuchen, mit der neuen Bedrohungslage zurechtzukommen. Diese leiden jedoch alle unter demselben Makel: Es handelt sich um statische Systeme, die die Ränder der Systeme definieren und versuchen zu sichern. Auch die Next Generation Firewall, kurz NGFW, begrenzt auf Hardware- und Software-Ebene Systeme und Applikationen und versucht so, Sicherheit herzustellen. Dies wird den komplexen, offenen Systemen, die meist über mobile Geräte permanent erweitert werden, nicht gerecht. Der nächste evolutionäre Schritt beim Thema Sicherheit kann daher nur über ein intelligenteres, auf Big Data gestütztes Sicherheitssystem erreicht werden.“#BDSI erkennt u.a. Angriffe von Hackern, weil sie von bekannten Mustern abweichen. #BigData“ Twittern WhatsApp;https://bigdatablog.de/2015/08/18/security-intelligence-mehr-sicherheit-durch-big-data/;BigDataBlog;Ibrahim Evsan
13. Aug 15;" Retweets,      ""Amazon: Big-Data-Services erweitern das Geschäftsmodell""";"Amazon: Big Data Services erweitern das GeschäftsmodellAmazon, längst mehr als ein E-Commerce-Gigant, erweitert permanent seine Produktpalette. Amazon nutze viele Jahre selbst die Vorteile von Big Data für Produktempfehlungen, die Verbesserung ihres Kundenservices und die Optimierung von Lagerung und Logistik. Die daraus resultierende Erfahrungen sowie die Infrastrukturen, die Amazon in diesem Bereich aufbaute, bilden seit 2006 ein eigenständiges Geschäftsmodell.Inzwischen ist Amazon Web Services (AWS) sogar der Marktführer im Bereich Cloud-Services und zwar weit vor Microsoft, IBM und Google. Auf ihrer Übersicht „Big Data as a Flyer“ zeigt AWS, dass sie über ein umfassendes Angebot in allen Bereichen verfügen; viele davon basieren auf Hadoop–  und anderen Apache-Framework-Lösungen. Derzeit plant Amazon, seine Standorte in Deutschland im großen Maßstab auszubauen. Unter anderem soll laut Martin Geier, Geschäftsführer von AWS-Deutschland, Berlin das „Epizentrum für Innovationen“ werden.Das Amazon Development CenterIn diesem Zuge ist die Anwerbung von 450 Big-Data-Experten für den Berliner Standort zu sehen. Hier eröffnete im neuen Verlagszentrum in Mitte das Amazon Development Center, das mehrere Ziele gleichzeitig verfolgt. Zum einen steht durch die neuen Räumlichkeiten ein praktischer, repräsentativer Ort für Veranstaltung wie das AWS-Summit 2015 zur Verfügung. Zugleich sendet Amazon den Mitbewerbern – nicht zuletzt dem direkt benachbarten Axel-Springer-Verlag – ein politisches Signal und zeigt Präsenz.“#Amazon sendet mit dem #DevelopmentCenter ein wichtiges Signal für den Standort Deutschland. #BigData“ Twittern WhatsAppDas Development Center soll weiter auch Anziehungspunkt für die Startup-Szene sein, die dort einen Raum und Mittel findet, um ihre innovativen Ideen voranzutreiben. Davon verspricht sich nicht zuletzt Amazon selbst auch einen eigenen Nutzen. Zu guter Letzt entsteht hier ein Big-Data- bzw. Machine-Learning-Zentrum, das neue Services nach Deutschland und Europa bringt.Machine-Learning-Science für adaptive SystemeMachine Learning ist ein zentraler Baustein innerhalb des Softwaregefüges von Amazon und den Software-Services, die sich an externe Kunden richten. Weltweit nutzen inzwischen mehr als eine Millionen Firmen die Dienste von AWS. Die intelligenten Machine-Learning-Algorithmen erkennen automatisch Trends, beispielsweise in der Mode, und helfen so, geeignete Produktvorschläge zu machen.Auch der Einkauf, die Lagerung und der Vertrieb werden durch Absatzprognosen genauer und effektiver. Adaptive, lernende Systeme werden dank Big-Data-Machine-Learning immer intelligenter und passen sich automatisch neuen Situationen an. Die Analyse von größten Datenmengen erlaubt es, zuverlässig Muster zu erkennen, die dann zur Produktempfehlung und Produktprognose genutzt werden.Tech-Hub mit Perspektiven für den Standort DeutschlandDas Potenzial, das durch Big-Data-Analysen beziehungsweise Machine-Learning-Algorithmen entsteht, ist dadurch jedoch bei weitem nicht ausgeschöpft. Auf dem größten seiner Märkte, in den USA, befinden sich noch weit mehr Services im Angebot von AWS. Der Ausbau des Berliner Standorts zum Tech-Hub hat deswegen Signalwirkung für den Standort Deutschland.“#BigData und #MachineLearning: @awscloud will seinen Markt ausbauen und Innovationen fördern. #Amazon“ Twittern WhatsAppSchon bald könnten weitere interessante Anwendungsgebiete mit Hilfe von Machine-Learning-Algorithmen erschlossen werden. Mit Hadoop lassen sich Twittermeldungen in Echtzeit auswerten und aktuelle Stimmungsbilder („Sentiment Analysis“) erstellen. Auch im Bereich Betrugserkennung („Fraud Detection“), für Klickprognosen oder zur Personalisierung von Inhalten lassen sich die Muster, die aus Big-Data-Analysen gewonnen werden, nutzen.In diesem Video wird erklärt, wie man AWS für Big Data nutzen kann:";https://bigdatablog.de/2015/08/13/amazon-big-data-services-erweitern-das-geschaeftsmodell/;BigDataBlog;Ibrahim Evsan
03. Aug 15;"     ""Predictive Policing: Big Data in der Polizeiarbeit""";Big Data könnte bald bei der Polizeiarbeit und in der Kriminologie bei der Verbrechensbekämpfung helfen. Mit Predictive Policing werden Verbrechen vorhergesehen und präventive Maßnahmen möglich gemacht. Hollywood nahm sich dieses Themas längst an: In “Minority Report” werden Menschen verhaftet, die nur geplant haben, ein Verbrechen zu begehen. Die Fähigkeit, die Zukunft vorherzusehen, wird im Film natürlich kinowirksam inszeniert. Drei weibliche Geschöpfe werden mit Medikamenten in einen Zustand versetzt, der es ihnen ermöglicht Gewaltverbrechen vorherzusehen.Im Prinzip wird hier aber eine bildliche Form für das gefunden, was sich mit Big Data erreichen lässt. Was im Film “Präkognition” heißt, entspricht in unserer Wirklichkeit der Methode der “Predictive Analytics”. Strukturen und Muster in Big Data werden dabei genutzt, um die Wahrscheinlichkeit von zukünftigen Ereignissen zu berechnen. Auch wenn die Vorhersagemethoden sich unterscheiden, so zeichnet der Film doch die Auswirkungen auf die Gesellschaft nach, wenn Verbrechen in Zukunft vorhergesagt werden können.Daten statt Drogen: Big Data als Grundlage von Predictive AnalyticsAnders als in “Minoritiy Report” bringen in der echten Welt nicht Drogen, sondern Daten beziehungsweise Muster in Daten die Einsichten über künftige Ereignisse. Ein erstes Pilotprojekt, das in Bayern gestartet wurde, brachte bereits erste Erfolge. Ganz unumstritten ist der Einsatz des Programms PRECOPS (= Pre Crime Observation Systems), obwohl immer mehr Bundesländer mit dem Einsatz der Software Liebäugeln. Doch nicht nur von Seiten des Datenschutzes kommt die Kritik, sondern auch staatsrechtliche und psychologische Gründe sprechen gegen einen Einsatz.Profiling als Rasterfahndung 2.0 mit NebenwirkungenDaniel Guagnin von der TU Berlin erklärt im Video, warum das sogenannte Profiling, also die Nutzung von Data-Mining und Big Data in der Polizeiarbeit, auch Nebenwirkungen hat. Im großen Stil wird das Profiling in Deutschland zwar noch nicht angewendet, dennoch werden Daten erhoben und überregional geteilt. Gleichzeitig ist es für den Einzelnen schwer herauszubekommen, ob und welche Daten über ihn erhoben und gespeichert ist. Für Aktivisten und einfach nur politisch aktive Bürger, die sich öffentlich für ihre Belange einsetzen, könnte es immer schwerer werden, sich zu engagieren. Wessen Daten einmal erfasst seien, rücke vielleicht auch bei anderer Gelegenheit in Verdacht. Auch normale Vorgänge, wie die Anmeldung einer Demonstration, könnte unter gewissen Umständen schwierig werden oder ganz unterbleiben. Letzteres würde zu einer Schwächung von Bürgerengagement führen und damit der Demokratie schaden.Die Tatsache, dass Daten von Menschen erhoben worden sein könnten, hat konkreten Einfluss auf ihr Verhalten. Ist sich jemand nicht sicher, ob möglicherweise verfängliche Informationen über ihn gespeichert sind, könnte im Extremfall lieber nichts tun, als sich eventuell gesetzeswidrig zu verhalten. Dabei zeigen Beispiele, dass es Fälle gibt, in denen der Bruch von Gesetzen sinnvoll ist. Das Verhalten von Rosa Parks, das es in die Schul- und Geschichtsbücher geschafft hat, ist eines davon. Indem sie sich weigerte, ihren Sitzplatz in einem öffentlichen Bus einem Weißen zu überlassen, führte letztlich dazu, dass die so genannten Jim-Crow-Gesetze abgeschafft wurden.Die Vorteile von Predictive Policing in der PolizeiarbeitNeben diesen Gegenargumenten bringt das Predictive Policing auch Vorteile mit sich. In den USA, Großbritannien und Teilen Italiens wird die neue Form der Polizeiarbeit bereits praktiziert. Die Auswertung von Statistiken und vielfältigen anderen Parametern wird dazu genutzt, Streifen gezielt einzusetzen. Diese müssen nicht mehr wie bislang mehr oder weniger ‘blind’ losfahren, sondern können genau in den Regionen unterwegs sein, wo Einbrüche oder Überfälle wahrscheinlich sind. Das spart nicht nur Zeit und Geld, sondern hilft auch dabei Täter schneller zu überführen.“#BigData erobert die Polizeiarbeit: Mit #Predictive #Analytics lassen sich Verbrechen vorhersehen.“ Twittern WhatsAppEiner der Erfolge von Predictive Policing: Einbrüche können um bis zu 30% reduziert werden. Erfolg oder Misserfolg des Einsatzes von Big Data in der Polizeiarbeit wird sich jedoch nicht nur an solchen Zahlen messen lassen. Vielmehr wird eine Wertedebatte darüber notwendig sein, wie eine Gesellschaft beschaffen sein soll, in der wir leben möchten. Das Beispiel von Rosa Parks zeigt, dass es durchaus Sinn macht, bestimmte Freiräume zu erhalten, die es im Einzelfall ermöglichen auch gegen Gesetze zu verstoßen und den Verstoß nicht vorab zu vereiteln.;https://bigdatablog.de/2015/08/03/predictive-policing-big-data-in-der-polizeiarbeit/;BigDataBlog;Christian Schön
29. Jul 15;" Retweets,      ""Daten richtig darstellen: 5 Regeln für die Erstellung von Dashboards""";Richtig und sinnvoll mit Informationen umzugehen, ist nicht leicht. Gemeinhin herrscht in vielen Firmen der Glaube, dass mehr Daten auch mehr Informationsgehalt bedeute. Dabei ist das Gegenteil der Fall: Wer seine Unternehmensinformationen richtig nutzen möchte, muss sie so einfach wie möglich darstellen. Eine gute Möglichkeit dafür sind Dashboards. Wie man sie richtig gestaltet und was dabei zu beachten ist, beschreibt dieser Beitrag.Hinter einem Dashboard steht normalerweise ein Business-Intelligence-System. Es fasst die Daten, die dargestellt werden sollen, zusammen und stellt sie für die weitere Bearbeitung zur Verfügung. Außerdem verteilen BI-Systeme die Daten auf eine möglichst effektive und effiziente Weise. Die Anforderungen an die Aufbereitung und Darstellung der Informationen können sehr unterschiedlich sein und hängen meistens von den jeweiligen Adressaten ab: Die Geschäftsführung oder der Vorstand brauchen Informationen anders aufbereitet als Mitarbeiter aus der Fachabteilung. Die Anforderungen an die Datenaufbereitung betreffen daher die Flexibilität oder Interaktivität der Daten, ihren Detaillierungsgrad sowie ihre Visualisierung oder Mobilität.Dashboards bieten einen zentralen Blick auf wichtige InformationenDashboards stellen eine Möglichkeit dar, Unternehmensinformationen aufzubereiten und Daten zu visualisieren. Auch komplexe Informationen lassen sich in Dashboards komprimiert darstellen. Sie führen Daten aus verschiedenen Datenquellen zusammen und visualisieren sie durch grafische Mittel. Dashboards sollten intuitiv bedienbar sein, um den Konsum der Informationen einfach zu gestalten. Im Normalfall erstellt man Dashboards nur für Mitglieder der Geschäftsführung, um ihnen dabei zu helfen, sich auf die wirklich wichtigen Themen zu konzentrieren und richtungsweisende Entscheidungen schneller zu treffen.Achtung: falscher DiagrammtypEin wesentlicher Bestandteil von Dashboards sind Diagramme. Diagramme sind sehr nützliche Werkzeuge, um Daten zu visualisieren, werden aber häufig falsch verwendet. Dadurch können sie Fehlinterpretationen fördern. Der jeweilige Diagrammtyp kann große Auswirkungen auf die Aussagekraft der dargestellten Informationen haben – so liest man einen nach links zeigenden Diagrammbalken anderes als einen, der nach rechts zeigt. Neben dem richtigen Diagrammtyp gibt es noch weitere Dinge, die man bei der Erstellung eines Dashboards beachten sollte. Sie sind im Folgenden in fünf Regeln zusammengefasst. Das in den Beispielen verwendete BI-System ist Pentaho, die eingesetzte Technik zur Erstellung des Dashboard basiert auf Ctools, einer Sammlung von Open Source-basierten Werkzeugen für die Aufbereitung von Daten.Regel 1: Diagramme sind der Hauptbestandteil des Dashboards„In 2013 war unser Umsatz 4987739 Euro, im Vorjahr verzeichneten wir 3677384 Euro“. Nur ein Rechenkünstler kann hier auf Anhieb erkennen, wie stark der Umsatz gestiegen ist – die zentrale Aussage des Satzes. Derartige Formulierungen gehören daher nicht in ein Dashboard. Den Inhalt eines Dashboards sollte man auf einen Blick verstehen. Komplizierte Tabellen und lange Texte verwirren den Leser und sollten daher in Diagrammen visualisiert werden. Diagramme zeigen den Inhalt wesentlich schneller und effizienter an, weil der Betrachter rasch die wichtigen Informationen erfassen kann.Daneben helfen sie, Platz im Dashboard zu sparen. Vorsicht aber bei der Auswahl des Diagrammtyps: wenn ein Diagramm zu viel Platz braucht, um seine Daten zu visualisieren, besser ein anderes wählen.Regel 1: Diagramme sind der Hauptbestandteil des Dashboards | © PentahoRegel 2: Ein Dashboard besitzt eine hohe InformationsdichteUm ausreichend Platz für die wichtigsten Daten zu haben, soll ein Dashboard immer den gesamten Bildschirm ausfüllen. Auf Bilder (z.B. Hintergrundgrafiken oder Fotos) sollte man ganz verzichten, weil sie nur vom Wesentlichen ablenken. Sobald alle wichtigen Daten im Dashboard platziert sind, ist es besser, keine weiteren Informationen dazu zu fügen, auch wenn noch Platz frei ist. Kuchendiagramme („Pie charts“) sind zu vermeiden, weil sie entgegen der allgemeinen Meinung nicht besser lesbar sind als Balkendiagramme, aber fast doppelt so viel Platz brauchen.Regel 2: Sorgen Sie für eine hohe Informationsdichte im Dashboard | © PentahoRegel 3: Einfache Darstellung von Inhalten3D-Diagramme sehen zwar beeindruckend aus, sind aber schlecht zu lesen. Man sollte daher auf ihren Einsatz verzichten, um seine Daten in einer einfach lesbaren Form darzustellen. Bei einem Dashboard geht es darum, Inhalte verständlich darzustellen, und nicht, den ersten Preis für die ausgefallenste Datenvisualisierung zu gewinnen. Wenn sie keine Aussage besitzen, sollte man daher generell auf Gestaltungselemente verzichten: Animationen gehören genauso wenig in ein Dashboard wie viele bunte Farben. Auch, wenn es langweilig klingt, sind Graphen am besten lesbar, wenn sie in unterschiedlichen Grautönen dargestellt werden. Vermeiden Sie helle Farben, sie heben Daten nur unnötig hervor und verfälschen das Interpretationsergebnis. Setzen Sie Signalfarben sparsam ein und nur dann, wenn der Inhalt explizit hervorgehoben werden soll.Verschiedene Arten von Datenvisualisierungen werden unterschiedlich gelesen. Man weiß inzwischen, dass die Anordnung und Darstellung der Daten großen Einfluss darauf hat, wie sie interpretiert werden. Außerdem gibt es bestimmte Verhaltensmuster beim Lesen von Diagrammen: Zeiteinheiten werden normalerweise von links nach rechts erfasst. Deshalb sollte man die zeitliche Entwicklung (Tage, Monate, Jahre) immer auf der X-Achse ansiedeln. Strukturen wie beispielsweise Produkte gehören dagegen auf die Y-Achse.Regel 3: Inhalte einfach darstellen | © PentahoRegel 4: Vergleichsdaten erheben und darstellenDiagramme sind dazu da, Vergleiche zu zeigen. So logisch das klingt, so schwierig scheint manchmal die Umsetzung. Diagramme, die viele Vergleiche enthalten, sind verständlicher und aussagekräftiger als welche, die wenige Vergleiche darstellen. Ein Diagramm, für dessen Verständnis erst der dahinterliegende Bericht konsultiert werden muss, ist fehlkonzipiert. Man sollte daher mindestens drei der folgenden (exemplarischen) Werte verwenden:Zielwerte,Vorjahreswerte,Zeitserien (die letzten Wochen, Monate oder Jahre),Werte von Konkurrenten etc.Bei Balkendiagrammen sollten die Balken immer sortiert sein, weil sie leichter zu lesen sind.Regel 4: Vergleichsdaten erheben und darstellen | © PentahoRegel 5: Ein Dashboard besitzt ein einheitliches DesignDashboards sollten ein einheitliches Aussehen besitzen. Dazu gehört, alle Elemente möglichst standardisiert darzustellen. Verwenden Sie beispielsweise das gleiche Symbol für alle Zielwerte. Gleiches betrifft die Farbwahl. Vor der Erstellung eines Dashboards sollte man über ein Farb- und Schriftenkonzept verfügen. Dadurch bekommt es ein aufgeräumtes und ruhiges Aussehen. Betrachter reagieren verwirrt darauf, wenn gleiche Inhalte mit verschiedenen Diagrammen dargestellt werden. Sie sollten daher darauf verzichten.Auch bei der Wahl der Dimensionen eines Diagramms ist Zurückhaltung geboten. Empfehlenswert ist es, in einem Diagramm maximal sechs Dimensionen zu verwenden, z.B. Zeit, Kategorisierung (gut, mittel, schlecht), Ist-Wert, Zielwert, Produkt.Regel 5: Einheitliches Dashboard-Design | © PentahoDashboards haben den Zweck, Daten komprimiert und in einer ansprechenden Form aufzubereiten, sodass sie bei der Entscheidungsfindung helfen. Hält man sich an die beschriebenen fünf Regeln, ist das nicht weiter schwierig. Ein Beispiel für ein gelungenes Dashboard ist hier dargestellt:Übersichtlich, klar strukturiert und farblich zurückhaltend: So sieht ein gutes Dashboard aus | © Pentaho“#Daten darstellen und lesen: 5 Regeln für die Erstellung von #Dashboards.“ Twittern WhatsAppHier gibt es die 5 Dashboards-Tipps als PDF zum Herunterladen.;https://bigdatablog.de/2015/07/29/5-regeln-fuer-die-erstellung-von-dashboards/;BigDataBlog;Stefan Müller
24. Jul 15;"     ""Zur aktuellen Nutzung von Big Data in Großunternehmen""";"Zur aktuellen Nutzung von Big Data in Großunternehmen | © prochasson frederic @ Shutterstock.comEine aktuelle Studie zeigt, dass Big Data bei großen Unternehmen und Regierungsorganisationen angekommen ist. Das SANS-Institut befragte gemeinsam mit dem Dienstleister Cloudera 206 Vertreter von Konzernen und anderen Institituionen. Über zwei Drittel der Befragten der Studie gaben an, entweder bereits eine Big-Data-Strategie zu haben, gerade zu erproben oder ihren Einsatz planen. Nur weniger als 5% der Befragten gaben an, gar kein Interesse an Big Data zu haben. Desweiteren kommt die Studie zu einer sehr positiven Einschätzung bezüglich der Nutzung von Big Data insgesamt: bei mehr als einem Dutzend verschiedener Anwendungen bleibt der Anteil derer, die mit der Effizienz nicht zufrieden sind, im einstelligen Prozentbereich.Die Studie leistet in puncto Big Data wahre PionierarbeitDas Hadoop-Ökosystem stellt weltweit eine weitverbreitete Grundlage für viele Big-Data-Lösungen dar. Bislang sei aber die Frage nach der Sicherheit dieser Anwendungen noch nie systematisch gestellt worden. In diese Lücke springt die SANS-Studie nun ein. Die Frage nach der Informationssicherheit stellt sich umso dringender, als dass Datenarchive in 72% der Fälle sensible Geschäftsdaten enthalten und in 73% der Fälle auf konkrete Personen zurückführen können. Hadoop speichert Daten in einem Cluster von Netzwerkrechnern. Die Frage nach der Sicherheit betrifft also nicht einen, sondern eine große Zahl von Einzelrechnern.Um die Sicherheit der Daten, so die Studie, ist es im Moment nicht optimal bestellt. Obwohl vielen die Neuartigkeit von Big Data und die damit verbundene, gestiegene Anforderung an Datensicherheit bewusst ist, wird auf konventionelle Methoden zur Sicherung von sensiblen Daten zurückgegriffen. Die verbreitetste Maßnahme, um Informationen zu sichern, ist, den Zugang zu dem Rechner per Passwort zu sichern, über den auf die Daten bzw. das Analyseprogramm zugegriffen wird (über 50%).Die Verschlüsselung einzelner Datensätze ist mit unter 20% eher verpönt. Zwar geht der Trend unter den Befragten in der kommenden Zeit eindeutig weg von Hadoop und seinen Komponenten hin zu anderen Lösungen. Angesichts des Wertes, der in den Daten steckt, lohnt es sich, Sicherheitskonzepte verstärkt anzugehen.Ein Vergleich hinsichtlich Unternehmergeist: USA vs. DeutschlandDie Situation in Deutschland ist im Vergleich zu den USA, in denen die SANS-Studie durchgeführt wurde, nur unwesentlich schlechter. Laut der BARC-Studie sind hierzulande immerhin nur 17% der Unternehmen der Meinung, dass Big Data ihren Geschäften nicht nutzen würde. Gleichzeitig haben nur 40% bereits konkrete Erfahrung mit Big-Data-Projekten gesammelt, während die Bereitschaft in den Vereinigten Staaten mit über 70% deutlich höher ist. Dieser doch merkliche Unterschied überrascht allerdings nicht besonders, da die Unternehmenskultur in den USA traditionell risikofreudiger und aufgeschlossener gegenüber Neuem ist.Ein Appell der Autorin der StudieDie Autorin der Studie, Barbara Filkins, sieht trotz der positiven Gesamteinschätzung Handlungsbedarf. Denn gerade weil Big Data nun nicht mehr nur ein modisches Schlagwort, sondern Realität sei, gilt es nun konkret zu handeln. Die Herausforderungen, die von den großen Datenmengen ausgehen, sind ebenso neuartig wie es die Lösungsstrategien sein müssen.“Studie zur Nutzung von #BigData: Big-Data-Strategien arbeiten effizient &amp; machen sich bezahlt.“ Twittern WhatsAppVor allem der Bereich der Datensicherheit sollte im Zentrum der Aufmerksamkeit stehen. Daten liefern nicht nur wertvolle Einsichten, sondern sind ein machtvolles Instrument geworden, das zu schützen ist.";https://bigdatablog.de/2015/07/24/zur-aktuellen-nutzung-von-big-data-in-grossunternehmen/;BigDataBlog;Ibrahim Evsan
13. Jul 15;"     ""Internet der Dinge und Big Data bringen eHealth in Medizin und Forschung""";"Leben und Labor werden mit dem Internet of Things (IoT) gleichgesetzt. Die medizinische Forschung untersuchte Krankheiten bislang in aufwändigen Versuchen im Labor unterm Mikroskop, im Tierversuch oder unternahmen umfangreiche Studien mit Freiwilligen. Das kann sich dank des Internets der Dinge sehr bald ändern. Die vielen Sensoren in Smartphones, Smart Watches und Fitnessbändern oder Küchengeräten werden viele neue, aufschlussreiche Erkenntnisse bringen.Wie hängen Bewegung, Ernährung und Gesundheit zusammen?Welche Auswirkungen haben diese Faktoren beim Verlauf einer Krankheit?Die Antworten, die das Internet der Dinge liefern kann, kommen direkt aus dem Leben und sind um ein Vielfaches umfangreicher, genauer und individueller als die Antworten aus dem Labor.Wie Big Data bei der Diagnose unterstütztBei ärztlichen Diagnosen könnten Erkenntnisse aus den Big-Data-Datenbanken, die aus dem Internet of Things gespeist werden, großen Nutzen bringen. Ärzte können so ihre eigenen Diagnosen absichern, indem sie Patientendaten und Krankheitsverläufe mit den Datenbanken abgleichen und verifizieren. Denn die Erfahrung von Ärzten nach einer langen Karriere können einige Tausenden Patienten sein. In Datenbanken können jedoch Krankheitsverläufe von Millionen Patienten gespeichert und abgerufen werden.“Durch Patientendaten werden Diagnosen bald sicherer &amp; die Forschung besser. #eHealth #BigData“ Twittern WhatsAppErste Modellprojekte zeigen bereits, dass Big Data Leben retten kann. Vor allem betrifft dies Hochrisikopatienten, die der Gefahr des plötzlichen Herztodes ausgesetzt sind.Das Internet der Dinge und die ForschungMit dem Research Kit stellt Apple Entwicklern ein Open-Source-Software-Framework für das iPhone und die Apple Watch zur freien Verfügung. Damit lassen sich Apps programmieren, die gesundheitsrelevante Daten erheben. Die Sensoren in immer mehr Geräten liefern Informationen zu Puls, Ernährung, Bewegungs-, Sitz- oder Schlafverhalten sowie Daten aus der Umwelt. Diese Daten werden – selbstverständlich anonymisiert – direkt zu den Servern eines großen Verbunds von renommierten Universitätskliniken geleitet und analysiert. Dort dienen sie der Forschung, indem sie Erkenntnisse über die Auswirkung des Verhaltens ihrer Träger auf ihren Gesundheitszustand liefern.Intelligente, vernetzte Medizintechnik rettet LebenDas Internet der Dinge betrifft aber nicht nur die Dinge des alltäglichen Lebens. Auch im Krankenhaus und anderen medizinischen Einrichtungen werden RFID-Chips, Beacons oder NFC mehr Informationen liefern, Standards verbessern und letztlich Leben retten. Die Hygiene kann gesteigert werden, wenn durch drahtlose oder automatische Übertragung weniger Gegenstände berührt werden müssen.“Das #InternetderDinge revolutioniert #Forschung und #Medizin – und rettet Leben. #BigData #IOT“ Twittern WhatsAppBesonders die Intensivmedizin profitiert davon. Mehr Daten über interne Abläufe führen aber auch zu Optimierung und Verbesserung von medizinischen Produkten. Mit den großen Datenmengen, die das Internet of Things liefert, lassen sich Abläufe verbessern, die Sicherheit steigern, die Vermehrung von Keimen verhindern und ein genaueres Verständnissen von Krankheit und Gesundheit erlangen.Lese auch: Wie das Internet der Dinge in der Pflege helfen kann";https://bigdatablog.de/2015/07/13/internet-der-dinge-in-medizin-und-forschung/;BigDataBlog;Ibrahim Evsan
08. Jul 15;"     ""Wie das Internet der Dinge in der Pflege helfen kann""";"Das Internet der Dinge bringt eine Vielzahl von Sensoren auch in den Bereich der Pflege von Menschen. Pflegeroboter und technische Assistenzsysteme helfen bereits in manchen Ländern bei der täglichen Grundversorgung von Patienten. Die Vernetzung von vielen weiteren, ganz alltäglichen Dingen sorgen für eine zusätzliche Erleichterung des Arbeitsalltags von Pflegepersonal und Angehörigen. Die Vernetzung schafft mehr Sicherheit und kann Erkenntnisse bringen, die ohne sie nicht möglich wären. Damit stellt das Internet der Dinge eine große Chance für die Pflege dar.Eine schnelle Verbesserung im Bereich Pflege ist dringend nötig. Immer weniger Menschen ergreifen einen Beruf in diesem Bereich. Gleichzeitig wird die Bevölkerung immer älter, und die Anforderungen an die Pflege steigen. Bereits jetzt gibt es gravierende personelle Engpässe. Das Internet der Dinge ist eine Chance für die Pflege, die dadurch wieder menschlicher werden kann.Das Internet der Dinge in der Pflege: Konzentration auf das WesentlicheViel Zeit wird in der Pflege heutzutage auf die Dokumentation verwandt. Oft wird noch von Hand in Patienten-Akte eingetragen, welche Leistungen erbracht und welche Medikamente verabreicht wurden. Die Zeit, die dafür benötigt wird, könnte viel sinnvoller verwendet werden; vor allem auch für Zuwendungen, die nicht unbedingt bei den Krankenkassen abgerechnet werden können.Laut ICN (International Council of Nurcing) ist die Achtung des Lebens und der Würde des Menschen eines der Hauptziele der Pflege. Wenn Gegenstände, die zur Pflege genutzt werden, mit dem Internet verbunden sind und Sensoren wertvolle Daten liefern, werden Routineaufgaben erheblich erleichtert oder gar ganz übernommen.Dokumentation und Monitoring via SensortechnikDokumentation und Monitoring sind zwei Kernaufgaben von Sensoren, die vollautomatisiert übernommen werden. Arbeitsabläufe – beispielsweise auf der Intensivstation – können mit der Hilfe von Sensoren effektiver gestaltet, verbessert und automatisiert werden. So konnte bei der Frühchenüberwachung Erfolge erzielt werden, die weit über die Übername von Aufgaben hinaus geht. Über die Auswertung von den Big Data aus einer Vielzahl von Brutkästen konnten Muster erkannt werden, die auf Komplikationen hinweisen; und zwar lange bevor ein gefährliches Ereignis wirklich eintritt.Das Online-KrankenbettEin Krankenbett, das online ist, unterstützt ebenfalls effektiv bei Pflegeaufgaben. Die Dekubitusprophylaxe, also Maßnahmen, die das Wundliegen verhindern, werden mit einem Bett, das online ist, einfacher. Die Sensoren unter einer Matratze erkennen, wie lange ein Patient in einer Position lag und gibt ein Signal zum Umbetten an das Pflegepersonal. Dazu ist eine Echtzeitüberwachung der Liegeposition notwendig, die gleichzeitig auch Daten über die Schlafqualität liefern. In der Altenpflege werden viele kleinere, alltägliche Notwendigkeiten durch vernetzte Gegenstände erleichtert. Das regelmäßige Einnehmen von Tabletten wird durch die Pillendose registriert und gegebenenfalls erinnert sie mit einem akustischen Signal an die Vergabe.Das Internet der Dinge macht Pflege wieder zum Dienst am MenschenDas Internet der Dinge in der Pflege ist beispielhaft dafür, wie der Einsatz von neuen Technologien im Dienst der Menschlichkeit stehen. Durch die Errungenschaften in der Medizin wird die Weltbevölkerung immer älter und der steigende Bedarf an Pflegenden kann ohne neue Technologien kaum mehr abgefedert werden. Das Internet der Dinge ist die Schlüsseltechnolgie, um die hinzugewonnene Lebenszeit in Würde zu verbringen sowie Krankheiten und Gebrechen besser zu bewältigen.Wenn zeitfressende Routineaufgaben durch die digitale Vernetzung und Auswertung von Big Data besser erledigt oder ganz übernommen werden können, wird Zeit frei für den eigentlichen Dienst am Menschen.“Das #InternetderDinge liefert Daten für die Pflege &amp; legt den Fokus wieder auf den Menschen.“ Twittern WhatsAppZwischenmenschliche Nähe und Zuwendung, für die so die Weichen gestellt werden, ist nicht nur der schnellen Förderung der Gesundheit dienlich. Es werden dadurch auch die Werte des Pflegeberufs gestärkt, die ein würdevolles Dasein ermöglichen.";https://bigdatablog.de/2015/07/08/wie-das-internet-der-dinge-in-der-pflege-helfen-kann/;BigDataBlog;Christian Schön
25. Jun 15;"     ""Top 10 Big-Data-Visualisierungs-Tools für Privatanwender""";"Daten beherrschen nicht nur zusehends Industrie und Wirtschaft, sondern werden für private Anwender, Künstler oder Journalisten wichtiger. Um Daten hier in eine anschauliche Form zu bringen, stehen eine Reihe von Tools zur Verfügung, für die kein Informatikstudium notwendig ist. Die Visualisierung von Daten ist ein zentraler Schritt bei der Analyse und Auswertung. Da auch im Alltag an immer mehr Stellen Daten anfallen, schlummert hier das Potenzial an ungeahnten Einsichten.EHealth und Selbstoptimierung sind zwei Anwendungsbereiche, in denen die Datenerhebung im Privaten bereits angekommen ist. Kurven zum Trainingsverlauf lassen Rückschlüsse für die nächste Saison zu; Korrelationen zwischen Ernährungsgewohnheiten, Aktivitäten und Gesundheitsdaten sind bei der Gestaltung des Lebensstils interessant. Bestimmte Zusammenhänge werden erst in der Darstellung ersichtlich. Aber auch im bestimmten beruflichen Zweigen werden Datenvisualisierungen immer wichtiger, ohne dass zwangsläufig Entwickler-Kenntnisse vorhanden sind.Für die folgenden 10 Tools sind keine Spezialkenntnisse nötig und dennoch können sich die Ergebnisse sehen lassen.ChartsBinScreenshot: http://chartsbin.com/graphChartsBin ist spezialisiert auf Geo-Daten. In wenigen, selbsterklärenden Schritten können CVS-Daten eingelesen, mit Meta-Daten versehen und ausgewertet werden. Die Möglichkeiten, statistische Daten, GPS-Informationen oder andere Geo-Daten darzustellen, sind recht umfangreich und grafisch sehr ansprechend. Die Visualisierungen können direkt online geteilt oder auf die eigene Webpage eingebunden werden.ChartBlocksScreenshot: http://www.chartblocks.com/en/ChartBlocks ist ein sehr einfach gehaltenes Online-Tool zum schnellen erstellen von Charts. Der größte Vorteil dabei: Daten können aus den unterschiedlichsten Quellen eingespeist werden. Es gibt umfangreichere Tools als ChartBlocks, jedoch ist diese Schwäche zugleich eine Stärke. Je kleiner die Auswahl an Chart-Typen, desto schneller gelangt man zum Ziel. Nachdem ein Chart erstellt wurde, kann er auf der eigenen Homepage eingebettet, in den Social Medie geteilt, oder als SVG- oder PNG-Datei exportiert werden. In einem Mini-Tutorial werden die einfache Funktionsweise kurz erklärt:DatawrapperScreenshot: https://datawrapper.de/Datawrapper ist ein sehr einfach zu benutzendes Tool. In Windeseile lassen sich interaktive Charts erstellen. In wenigen Schritten entstehen Diagramme und Karten. Datawrapper begegnet den großen Datenmengen von Big Data Einfachheit und Schnelligkeit. Viele Datenjournalisten, Reporter und Nachrichtenseiten nutzen Datawrapper, um einfach und schnell Grafiken für ihre Artikel zu erstellen. Allerdings eignet sich das Online-Tool vor allem für einfach strukturierte Daten.Easel.lyScreenshot: http://www.easel.ly/Die nett anzusehenden Charts sind ein Mega-Trend in den sozialen Netzwerken. Allen voran Pinterest hat diese Art der Datenvisualisierung zum Durchbruch verholfen. Es gibt zahlreiche kostenlose Online-Tools, mit denen Charts dieser Art erstellt werden können. Easelly ist (neben Piktochart) eine der am weitesten ausgereiften und sehr leicht zu bedienenden Web-Apps in diesem Feld. Der kleine Image-Film zeigt wie schnell ein solcher Chart entstehen kann:infographics from easel.ly on Vimeo.InfogramScreenshot: https://infogr.am/Mit Infogram ist es möglich, Charts und Infografiken online zu erstellen. Den Webdienst gibt es in einer kostenlosen Variante und einer Pro-Version. Infogram richtet sich besonders an Privatanwender: Angefangen von der einfachen Bedienbarkeit bis hin zur Möglichkeit, sich mit dem Facebook-Account einzuloggen – die Schwellen sind niedrig gehalten. Die Ergebnisse lassen sich aber sehen und muten sehr professionell an.PlotlyScreenshot: https://plot.ly/Mit Plotly ist ein Online-Tool, das mit einer ganzen Reihe von Chart-Typen aufwarten kann. Für die meisten Anwendungsfälle sollte etwas dabei sein. Die Besonderheit von Plotly: Die Social-Sharing-Features, die gleich integriert sind. Auch wenn Plotly für Einsteiger sehr gut geeignet ist, richtet sich der Funktionsumfang ebenso an Fortgeschrittene. Das Tool kann nicht nur einfach Graphen aus Daten erstellen, sondern erlaubt auch statistische Analysen, für die mehr Know-how nötig ist. Für ein Online-Tool ist der Umfang der Möglichkeiten überdurchschnittlich gut.TableauScreenshot: http://www.tableau.com/de-deTableau ist eines der meistverbreiteten Visualisierungs-Tools. Es bietet eine einfache, intuitive Bedienung und erlaubt gleichzeitig komplexe Sachverhalte zu visualisieren. Es unterstützt zudem eine große Bandbreite von Charts: Graphen, Karten, Diagramme etc. Das Dashboard bietet einen guten Überblick über alle Funktionen und Daten. Sogar Livedaten können eingespielt werden und auch mit großen Datenmengen kommt Tableau bestens klar. Wie leicht das Tool zu bedienen ist, zeigt das Werbevideo von Tableau:Timeline JSScreenshot: http://timeline.knightlab.com/Seit Facebook vor wenigen Jahren die “Timeline” eingeführt hat, ist diese Art der Darstellung zu einem gängigen Standard geworden. Für alle Daten-Analysen, bei denen ein zeitlicher Ablauf eine tragende Rolle spielt, bietet sich diese Art der Visualisierung an. Mit Timeline JS ist ein freies, Opensource-Tool verfügbar, mit dem es ohne Programmierkenntnisse möglich ist, tolle Timelines zu erstellen. In nur vier Schritten lassen sich die Timelines gestalten. Mit nur wenigen Klicks lässt sich Material von allen nur denkbaren Quellen einfügen: Youtube, Google Maps, Vimeo, Soundcloud, Twitter, Wikipedia u.s.w.RawScreenshot: http://raw.densitydesign.org/Raw ist eine Opensource-Web-App, die selbst damit Werbung macht, die Lücke zwischen Tabellenkalkulation und Vektorgrafiken zu schließen. Was sich so technisch anhört, ist perfekt designt und so einfach funktioniert es:Raw 1.0 – Basic Tutorial from DensityDesign on Vimeo.Die so entstehenden Vektorgrafiken erinnern nicht zufällig sehr stark an die von D3.js, das zu den Profi-Tools zählt. Bei RAW stehen 16 verschiedene Chart-Typen zur Auswahl. Da die Verarbeitung der Daten über den Browser, sind zudem auch sensible Daten sicher.VenngageScreenshot: https://venngage.com/Mit Venngage bewegt man sich an der Grenze zwischen privater und geschäftlicher Anwendung. Die umfangreichen Zusatzfeatures, die in der Bezahlversion verfügbar sind, sprechen eine eindeutige Sprache. Doch bereits in der kostenlosen Version gibt es vieles, was für professionelle Belange genügt. Mit ein paar Klicks lassen sich Reportings erstellen, die so gut aussehen, dass sie Präsentationszwecken mehr als genügen.“Top-Visualisierungs-#Tools für Nicht-#Entwickler. So werden #Daten leicht zum Leben erweckt. #BigData“ Twittern WhatsAppLese auch:Top 10 Big-Data-Visualisierungs-Tools für Entwickler";https://bigdatablog.de/2015/06/25/top-10-big-data-visualisierungs-tools-fuer-privatanwender/;BigDataBlog;Ibrahim Evsan
22. Jun 15;" Retweets,      ""Klaas Bollhoefer von “The unbelievable Machine Company” spricht über Big Data, KI und Data Science""";"Das Zeitalter der unglaublich intelligenten Maschinen steht vor uns. 2008 leistete das Berliner Big-Data-StartUp “The unbelievable Mashine Company” – kurz “*um” – noch Pionierarbeit auf dem Gebiet der Data Science und Web Operations. *um war zudem einer der ersten Anbieter von Cloud-Computing in Deutschland. Seither bietet die Firma maßgeschneiderte Speziallösungen rund um das Hadoop-Ökosystem an. Maschinen sollen unter anderem mitfilfe von Machine-Learning-Algorithmen wortwörtlich unglaubliches vollbringen. Wir haben uns mit Klaas Bollhöfer, dem Chief Data Scientist, unterhalten.Herr Bollhoefer, was genau ist so unglaublich an der “unbelievable Machine” und woher kommt Ihr ungewöhnlicher Firmenname?The unbelievable Machine Company aus Berlin ist ein Full-Service Anbieter für Cloud Computing und Big Data. Wir sind herstellerunabhängig und realisieren maßgeschneiderte Lösungen „von der Idee bis zum Kabel“. Das gesagt – unsere Experten sind nicht nur in der Beratung oder der Analyse von Daten, aka Data Science, stark, sondern auch Experten für Software-Entwicklung („on scale“), DevOps und Data Center Operations. Der Legende nach wurde unser Firmenname durch einen Bandnamen-Generator im Internet generiert. Und wie immer bei Legenden – es ist etwas Wahres daran…Was zeichnet die Lösungen von “The unbelievable Machine Company” besonders aus?Wir haben stets den Kunden mit seinen individuellen Anforderungen im Fokus und realisieren keine Lösungen „von der Stange“. Egal ob High-Performance-Anforderungen in Richtung Infrastruktur, knifflige Machine-Learning-Aufgaben oder perfekt sitzende digitale Business Cases – herzlich willkommen! Darüber hinaus ticken wir anders als der Wald- und Wiesen-IT-Dienstleister. Wir leben was wir tun, denken als Community und sehen das gesamte Internet als Betriebssystem, für das wir unsere Lösungen kreieren.Können Sie uns anhand eines Beispiels den Vorzug dieser Art der Herangehensweise verdeutlichen?Der Erfolg stellt sich immer da ein, wo etablierte, paketierte und zu strikt gedachte Lösungen an Grenzen stoßen und wo diese mit neuen Ansätzen, Expertenwissen und der Unterstützung von Algorithmen sowie Technologie verschoben werden.Wir haben beispielsweise mal einen Recommender entwickelt, der nicht aus dem Portfolio des Onlineshops und dessen Nutzung selbst gespeist wird, sondern aus Assoziationsnetzen aus dem Web/Social Web. Der Shop war noch jung am Markt, die bestehende Datenbasis zu schwach für relevante Empfehlungsmuster. Bonuspunkt: die aus dem Web extrahierten „non-trivialen“ Beziehungsmuster rund um Produkte waren auch im weiteren Verlauf modell-prägende sogenannte Features für eine neuartige Form Empfehlungslogik.Wie kann Big Data, Machine Learning und Cloud-Computing uns als Gesellschaft helfen? Wo sehen Sie das derzeit größte und noch ungenutzte Potenzial von Big Data?Technik kann immer da helfen, wo intelligente Menschen intelligente Fragen stellen und den Status quo herausfordern. Da unterscheidet sich Big Data nicht von anderen Entwicklungen und Technologien der letzten 20, 30, 50 Jahre. Großes Potenzial sehe ich dort, wo Technologie die reine Technikperspektive verlässt und multidisziplinär und/oder fachfremd angewandt wird. Neue Analyseverfahren auf Datenbeständen der Soziologie, Medizin, Geowissenschaften oder auch in beliebiger Kombination und Korrelation dieser birgt noch ungenutzte, bis dato unbekannte Potenziale.Was sind für Sie die bislang faszinierendsten Beispiele, was durch KI oder Machine Learning erreichbar ist?Sicherlich die aktuellen Entwicklungen und Ergebnisse im Bereich Deep Learning. Skype wird in die Lage versetzt, in Echtzeit synchron zwischen zwei Sprachen zu übersetzen, Google kann automatisiert Bilder erkennen und annotieren, Algorithmen sind in der Lage, selbstlernend Computerspiele zu beherrschen. Und wir stehen noch am Anfang der durch die massive Rechenpower und skalierbare Architekturen befeuerten Entwicklung im Bereich der neuronalen Netze. Das hat nicht im Entferntesten was mit menschlicher Intelligenz oder der Funktionsweise unseres Gehirns zu tun, zeigt aber in beeindruckender Weise, was Algorithmen, Automatisierung und Moores Law in Kombination für unschätzbare (im doppelten Sinn des Wortes) Möglichkeiten schaffen.Wir selbst sind aktuell in erste Projekte im Kontext von Industrie 4.0 eingebunden, die Deep Learning und automatisierte Bilderkennung in neuartige Lösungen verpacken.Wie weit entfernt sind wir aus Ihrer Perspektive von der Smart Factory und der Industrie 4.0?Aus dem Blickwinkel von Big Data stehen wir gerade am Anfang und es herrscht so etwas wie Aufbruchstimmung in der Industrie. Die Verarbeitung, Analyse und Visualisierung maschinengenerierter Daten und die Nutzung in Bereichen wie Prozessoptimierung, Energie-Effizienz oder Condition Monitoring sind naheliegende erste Cases und werden sicherlich eine Starthilfe von Industrie 4.0 darstellen. Betrachtet man das Thema aber in all seinen Facetten und der damit verbundenen Größenordnung des Unterfangens sind wir sicherlich noch weit davon entfernt, von so etwas wie Distributed Smart Production oder ähnlichem zu sprechen. Auch die letzte industrielle Revolution ist nicht über Nacht gestartet und vollendet worden.Wie intelligent können Maschinen werden und bedrohen sie in absehbarer Zukunft viele Arbeitsplätze?Das ist das Schreckensszenario, das derzeit entworfen wird und selbst von (so sagt man) Experten wie Stephen Hawking, Elon Musk und anderen stilsicher heraufbeschworen wird. Ganz ehrlich – es liegt nicht zuletzt an uns, wie, wofür und in welcher Form wir „intelligente“ Maschinen und automatisierte Systeme nutzen und auch nutzen wollen. Daher sind für mich auch die Entwicklungen, die ein wenig jenseits der Debatte um künstliche Intelligenz massiv fortschreiten, wesentlich bezeichnender und kritischer.Ich spreche davon, dass immer mehr Automatismen, Logiken und Programme die Prozesse regeln, in denen menschliche/manuelle Tätigkeiten involviert sind, beispielsweise in den Warenlagern großer Online-Retailer. Diese Jobs und perspektivisch zahlreiche weitere „regulier- und steuerbare“ Tätigkeiten verschieben sich „below the API“ und sind damit de facto möglicher Bestandteil zukünftiger Software-Lösungen. Auch bedrohlich: diese Softwarelösungen sind eben nicht intelligent, sondern de facto dumm, regelbasiert und auf bestimmte Kenngrößen starr optimiert.Werden im Fahrwasser der neuen Technologien nicht auch viele neue Berufe entstehen – insbesondere im Zusammenhang mit Big Data?Ja, es werden nicht nur Berufe verschwinden, sondern de facto auch neue entstehen. Spezifisch im Bereich Data Science, Engineering und Operations sind wir im Augenblick noch in der Phase, wo mehrheitlich Generalisten in den jeweiligen Rollen agieren. Hier ist perspektivisch eine weitere Spezialisierung, wie es allgemein in der IT- und Digitalbranche zu beobachten ist, zu erwarten. Es ist an der Zeit in Schul- und Hochschulbildung die dafür notwendigen Vorbereitungen zu treffen.In welchen Unternehmensbereichen steckten schon jetzt Möglichkeiten, die noch nicht genutzt werden?Das ist so konkret schwer zu beantworten, da de facto alle Bereiche mehr und mehr „data-driven“ werden beziehungsweise werden können. Vorreiter sind sicherlich Bereiche wie Marketing und Vertrieb, Logistik und Supply Chain Management oder Produktmanagement/-entwicklung und R&amp;D.Welche Rolle spielt Big Data in Ihrem eigenen Unternehmen und auf welche Technologien setzten Sie – nutzen Sie Lösungen aus dem Apache-Park wie Hadoop oder Spark?Als Service-Provider, der aktuell auch mehr als 1000 qm Rechenzentrumsfläche für Kunden betreibt, setzen wir natürlich auch selbst auf Big-Data-Technologien und Analysen. Daten haben wir reichlich!Wir haben zahlreiche Technologie-Stacks für unsere Kunden im Einsatz und ein breites Spektrum an Expertise und Erfahrung. Schwerpunkt liegt sicherlich auf Open Source Technologien. Hadoop und Spark sind häufig integraler Bestandteil zeitgemäßer Big Data Architekturen, aber nicht ausschließlich. Auch zahlreiche weitere Komponenten des Hadoop Ökosystems sind tägliche Begleiter, wie beispielsweise Hive, Impala, Oozie, Flume &amp; Co., im Bereich Realtime Streaming/Analytics setzen wir Kafka, Spark Streaming oder auch Storm ein, im Bereich Operational Intelligence Technologien wie Splunk oder ELK-Stack (ElasticSearch, Logstash, Kibana). Die Liste könnte beliebig fortgesetzt werden und verändert sich quasi monatlich.Zeichnet sich bereits jetzt ab, welche Technologie sich durchsetzen wird?Was sich ein Stück weit am Markt abzeichnet ist, dass sich die ein oder andere Technologie/Komponente deutlicher etablieren konnte in der letzten Zeit und eine Art De-facto-Standard bilden (Stand heute). Dazu zählen Hadoop (hier in erster Linie das HDFS Filesystem), Spark als Application Framework oder YARN (perspektivisch auch Mesos) für das Ressourcenmanagement. Auch Python und/oder R als Programmiersprache in Richtung der Daten ist quasi gesetzt.Die großen Datenmengen von Big Data versuchen die Realität abzubilden. Wie exakt lässt sich bereits ein Bild von der Welt durch die Daten zeichnen? Ermöglichen die Daten einen Blick in die Zukunft?Daten spiegeln nur ein Stück weit die Realität, in der sie produziert wurden, aber sie sind natürlich nie ein komplettes, geschweige denn objektives Abbild dieser. Ergo fehlen auch immer Daten, um dieses Bild zu vervollständigen. Lässt sich damit die Zukunft vorhersagen? Ja, in Teilen (sehr geringen), aber das ist zu abstrakt und globalgalaktisch von der Fragestellung her.Wir können aus Daten, die wir gesammelt haben und/oder aktuell sammeln, Muster extrahieren, diese für Vorhersagen in Richtung der zukünftigen Entwicklung dieser Muster nutzen und entsprechende Modelle entwickeln.Wenn es um personenbezogene Daten geht, herrscht die Meinung vor, dass uns Daten schon sehr genau als Person abbilden. Deswegen werden Big Data und die Entwicklungen, die mit der Digitalisierung einhergehen, oft kritisch beurteilt. Was entgegnen Sie so einer Kritik?Die Sicherheit der Daten ist ein entscheidender Aspekt und nicht hoch genug zu bewerten. Aktuelle Entwicklungen im Bereich der Big-Data-Technologien nehmen dies auch sehr ernst. Viele Lösungen sind bereits entwickelt und werden weiter verfeinert. Allgemein lässt sich aber sagen, dass es bei Big Data eben nicht um einzelne Datensätze geht (per definitionem), sondern um Datenmengen und die Muster, Rhythmen und Relationen, die in ihnen stecken.Natürlich können wir damit wieder Klassifizierungen vornehmen und Merkmale an einzelne Datensätze zurückschreiben, die de facto auch personenbezogenen Daten betreffen, erweitern und damit faktisch auch manipulieren können. Der zentrale Aspekt bei der Sache ist jedoch: das ist in den meisten Fällen weder sinnvoll, noch nötig, geschweige denn gewünscht. Es liegt also nicht an Big Data oder der Digitalisierung oder was auch immer, sondern an den Menschen, die mit den Daten umgehen.Was lässt sich tun, um die Akzeptanz von datengestützten Analysen, Datenwissenschaft und maschineller Intelligenz weiter zu steigern?Lehren, lernen, verstehen. Solange wir von etwas Unbekanntem, Undefinierbaren, Unheimlichen sprechen und nicht verstehen, was konkret hinter Begriffen wie Big Data, Künstlicher Intelligenz, dem Internet der Dinge etc. steht, ist es schwierig, von so etwas wie Akzeptanz zu reden. Wir müssen in die Getriebe der Black Boxes schauen, die Grenzen der Algorithmen kennen (lernen) und das Zusammenspiel komplexer Systeme durchleuchten und hinterfragen. Das war selbst bei Videorecordern in den 80ern nicht anders…Hat die Datenwissenschaft und Big Data das Potenzial, unser Leben oder einzelne Bereiche davon gravierend zu verbessern? Wie viel Veränderungen stehen uns noch bevor bzw. können wir uns jetzt schon vorstellen, welche Veränderungen Big Data und KI überhaupt noch bringen werden?Technologischer Fortschritt hat schon immer unser Leben verändert und sicherlich in der Mehrheit auch verbessert. So wird auch Big Data bzw. all das, für das Big Data steht, seinen Beitrag dazu leisten. Großes Potenzial – bis dato noch weitestgehend unausgelotet – steckt nicht zuletzt auch in Bereichen wie Medizin oder Wissenschaft.Wird sich dadurch etwas verändern?Ja, denn wir reden von Fortschritt! Können wir uns vorstellen, was diese Veränderungen in Zukunft alles bewirken werden? Nein, denn auch das ist Fortschritt! In diesem Fall höchstwahrscheinlich sogar exponentiell. Es bleibt somit sehr spannend!“Klaas Bollhoefer von @unbelievable_m spricht über #BigData, #KI und #DataScience.“ Twittern WhatsAppWir bedanken uns ganz herzlich für das Gespräch.";https://bigdatablog.de/2015/06/22/the-unbelievable-machine-company-im-interview/;BigDataBlog;Christian Schön
08. Jun 15;"     ""Big Data Analytics: Datenanalysen mit Geschichten""";Daten und Zahlen erzählen Geschichten. Infografiken sind sogar der entscheidende Schritt bei der Auswertung von Daten. Ein gutes Storytelling macht die Geschichten hinter den Daten erst richtig erfahrbar.Ein sehr gelungenes Beispiel ist die Kooperation von Nike und YesYesNo. In Nike+ City Runs wurden in den Städten New York, London und Tokio in den Nike Stores Installationen realisiert, die Laufdaten von einem Jahr visualisierten. Die verwendete Software analysierte sowohl das individuelle wie das kollektive Laufverhalten aller Teilnehmer. Das folgende Video zeigt die Visualisierung auf der Stadtkarte von New York, auf der nach und nach die Routen der Läufer erscheinen.Nike+ City Runs from yesyesno on Vimeo.Mehr als nur eine GeschichteDie Visualisierung der Rennstrecken ist aber weit mehr als nur eine Spielerei für Selbstoptimierer oder eine coole Werbekampagne. Die Darstellung der Rennrouten zeigt die beliebtesten Strecken der Läufer in einer Stadt. Diese Informationen stellen ein wertvolles Wissen für das Ökosystem Stadt dar. Angefangen vom individuellen Nutzen für Besucher, neu Zugezogene oder Sportanfänger beispielsweise. Sie alle profitieren von der Erfahrung der geübten bzw. ortskundigen Läufer und können die schönsten und für sie geeignetsten Routen erkunden. Nutzen bringt die Visualisierung auch der Stadtverwaltung. Sie kann erkennen, an welchen Stellen in der Stadt besonders viele Läufer unterwegs sind. An gefährlichen Kreuzungen können so etwa Warnhinweise aufgestellt werden, dass hier erhöhte Achtsamkeit herrschen oder die Geschwindigkeit reduziert werden muss.“Geschichten mit #BigData – wenn aus dem Zählen ein Erzählen wird.“ Twittern WhatsAppZählen und ErzählenData Warehouses sind also nicht nur die Lager für Datensammlungen. Aber erst wenn die Geschichten hinter den Zahlen erzählt werden, wenn also aus dem reinen Zählen ein Erzählen wird, verwandeln sich die Daten in wertvolle Erkenntnisse. Business-Strategien, politische Handlungspläne und individueller Nutzen sind also auf ein gutes Storytelling und gelungene Visualisierungen angewiesen.;https://bigdatablog.de/2015/06/08/big-data-analytics-datenanalysen-mit-geschichten/;BigDataBlog;Ibrahim Evsan
01. Jun 15;"     ""Die neue Realität: Erweiterung des Data Warehouse um Hadoop, NoSQL &amp; Co.""";"Datenintegration und Data Warehouse sind Technologien, die Unternehmen helfen, wertvolles Wissen aus ihren IT-Systemen zu bergen. Die Realität, in der BI-Werkzeuge eingesetzt werden, hat sich aber in jüngster Vergangenheit stark geändert: Viele Unternehmen erzeugen heute riesige Mengen an Daten und die wollen diese viel schneller als früher auswerten. Der klassische Data-Warehouse-Ansatz stößt in diesem Umfeld schnell an seine Grenzen. Big-Data-Technologien versprechen, den neuen Anforderungen gerecht zu werden und bieten vielversprechende Ansätze, um das althergebrachte Data-Warehouse-Konzept zu erweitern und zu modernisieren.Die Grenzen des klassischen Data Warehouse in Zeiten von Big DataEtwas angestaubt wirkt das aus den 80er Jahren stammende Konzept des Data Warehouse in Zeiten von Big Data, MapReduce und NoSQL. Das Data Warehouse ist eine Datenbasis, welche die steuerungsrelevanten Informationen aus allen operativen Quellen eines Unternehmens integriert. Während die operativen Systeme sich auf die Unterstützung der Tätigkeiten im Tagesgeschäft konzentrieren, liegt der Fokus des Data Warehouse auf Analysen und Berichten zur Steuerung des Unternehmens. Ziel ist es dabei, die Daten in Informationen zu verwandeln. Dieses Wissen hilft, bessere Entscheidungen zu treffen und Wettbewerbsvorteile zu erzielen: Wer seine geschäftlichen Tätigkeiten gut kennt, kann seinen Umsatz steigern und Kosten senken.Für spezielle Anwendungen oder Organisationseinheiten lassen sich Data Marts erstellen. Das sind aufbereitete Abzüge des Data Warehouse. Data Marts sind in der Regel multidimensional aufgebaut und daher optimal von analytischen Anwendungen nutzbar. Das Data Warehouse bzw. die Data Marts stellen die zentrale Datenbasis für alle Analysen und Berichte im Unternehmen dar.“#DataWarehouse bzw. #DataMarts als Basis für Analysen &amp; Berichte in Unternehmen.“ Twittern WhatsAppData-Warehouse-Systeme basieren auf relationalen Datenbanksystemen (RDBMS). RDBMS sind seit einigen Jahrzehnten der Standard für die Speicherung von Daten. Sie werden daher nicht nur für operative, sondern auch für analytische Systeme eingesetzt. RDBMS bieten eine Reihe von Vorteilen für den Einsatz im Data Warehouse-Umfeld, u.a. eine hoch entwickelte Datenbanksoftware, SQL als mächtige und standardisierte Abfragesprache, viele Business Intelligence-Frontends und eine hohe Zuverlässigkeit und Konsistenz.Konfrontiert mit extrem hohen Datenvolumina kann die Skalierung eines Data Warehouse aber schwierig sein. Verwendet man eine kommerzielle Datenbanksoftware, kann die Speicherung außerdem hohe Lizenzkosten nach sich ziehen. Eine weitere Schwierigkeit kann die fehlende Leistungsfähigkeit eines RDBMS bei hohen Datenvolumen sein. Bei umfangreichen Modellen ist es außerdem möglich, dass das Schema einer relationalen Datenbank nicht erweitert und angepasst werden kann. Daneben können auch die hohe Frequenz der Datenerzeugung und deren Speicherung eine Herausforderung darstellen. Weil immer mehr Daten in nicht standardisierten Formaten in den Mittelpunkt des Analyseinteresses rücken, stoßen relationale Datenbanken schnell an ihre Grenzen. Sie sind nicht auf die Speicherung von unstrukturierten Daten ausgelegt und stellen damit keine ideale Lösung für den Umgang mit heterogenen Datenformaten dar. PEN13Aus diesen Gründen haben sich unterschiedliche technologische Ansätze entwickelt, die ich in den folgenden Abschnitten vorstelle. Im Mittelpunkt steht dabei das Zusammenspiel zwischen dem Data Warehouse und den Big Data Stores, weil aus der Kombination beider Synergien entstehen können.Neue Mitspieler: analytische DatenbankenAnalytische Datenbanken sind eine vergleichsweise einfache und schnell durchführbare Erweiterung des Data Warehouse. Darunter werden Datenbankensysteme verstanden, die speziell für analytische Anwendungen konstruiert sind. Ihre Grundlage ist nach wie vor ein RDBMS, das aber nicht mehr die optimale Transaktionsverarbeitung zum Ziel hat, sondern schnelle Abfragen und eine hohe Verarbeitungsgeschwindigkeit.Analytische Datenbanken nutzen eine Reihe besonderer Technologien, wodurch sie eine ganze Reihe neuer Möglichkeiten eröffnen:Nutzung von Daten, deren Speicherung normalerweise zu teuer und deren Verarbeitung zu langwierig wäreNutzung von SQL als leistungsfähige, weit verbreitete AbfragespracheKompatibilität mit einer Vielzahl von Business Intelligence-FrontendsVergleichsweise schnell zu implementieren und einfach zu administrierenGeringe Ansprüche an die Hardware-Architektur“5 Möglichkeiten, die analytische Datenbanken eröffnen. #Technologie #BigData“ Twittern WhatsAppIm Kontext Big Data spielen diese Eigenschaften eine wichtige Rolle, weil sie die Abfrageperformance auf große Datenmengen massiv steigern. So können sehr große Datenbestände per SQL oder durch BI-Werkzeuge abgefragt und analysiert werden. Insbesondere letztere eröffnen Analysten und Controllern ein bislang unbekanntes Spektrum an Auswertungsmöglichkeiten, weil sich so große Datenbestände auch ohne IT-Kenntnisse intuitiv analysieren lassen.Was nun die Erweiterung des Data Warehouse betrifft, ergeben sich bei analytischen Datenbanken unterschiedliche Optionen. Zum einen können unabhängige Data Marts aufgebaut werden. In diesem Szenario hat man kein Data Warehouse mehr als integrative Datenschicht für das gesamte Unternehmen, sondern baut themen- oder organisationsspezifische Data Marts direkt auf den operativen Quellen auf. Dieser Ansatz lässt sich schnell umsetzen, bringt aber Nachteile mit sich, was die Integration mit anderen Datenquellen betrifft. Die zweite Option ist die Entwicklung von Data Marts, die aus dem Data Warehouse und nicht aus den Quellen direkt gespeist werden. Dadurch wird auf die integrative Schicht zugegriffen, sodass ihre Vorteile erhalten bleiben. Das ist mit höherem Aufwand für die Modellierung und Entwicklung verbunden. BEY10Erweiterung um NoSQL-DatenbankenNoSQL-Datenbanken helfen, die Probleme der relationalen Datenbanken zu vermeiden: RDBMS können in bestimmten Anwendungsszenarien (zum Beispiel bei Streaming-Media-Applikation oder bei Webseiten mit hohen Lastaufkommen) Schwierigkeiten mit der Performance bekommen. Die vertikale und horizontale Skalierung sind aber nur eingeschränkt möglich und kostspielig. Außerdem ist man weniger flexibel bei der Erweiterung des Schemas, z.B. das Hinzufügen einer Tabellenspalte in Kombination mit großen Datenmengen.NoSQL-Datenbanken können mit diesen Anforderungen besser umgehen (siehe Abbildung). Durch die horizontale Skalierbarkeit können sie große Datenmengen kostengünstig verarbeiten. Um die Ausfallsicherheit zu erhöhen, kann man die Daten auf mehrere Server replizieren. Durch die Verwendung einfacher Schemata in der Datenbank bietet NoSQL mehr Agilität und Flexibilität bei Anpassungen und Erweiterungen.NoSQL im Kontext des Data Warehouse | © PentahoBedingt durch ihre Skalierbarkeit und Flexibilität bieten NoSQL-Systeme vielseitige Lösungsansätze für die Anforderungen von Big Data. Leider verfügen sie aber über nur wenige Abfragesprachen, die nicht an die Möglichkeiten von SQL heranreichen. Grund ist, dass Daten im NoSQL-Umfeld so gespeichert werden, wie sie von bestimmten Applikationen benötigt werden. Eine komplexe und mächtige Abfragesprache ist somit obsolet. Manager, Analysten und sonstige Businessanwender eines Data Warehouse legen allerdings großen Wert auf Funktionen wie Adhoc-Reporting, Dashboards und OLAP-Analysen. Für die Bereitstellung dieser analytischen Services spielt SQL daher aktuell noch eine tragende Rolle.Zwei Alternativen gibt es hier: Zum einen können Berichte direkt auf der Datenbank über entsprechende Schnittstellen erstellt werden. Moderne BI-Software ermöglicht eine solche Umsetzung in Ansätzen. Der volle Funktionsumfang einer BI-Suite kann allerdings nicht erreicht werden, zumindest nicht beim aktuellen Stand der Technik. Die zweite Variante geht deshalb den Weg über das Data Warehouse basierend auf einem RDBMS. Die Vorteile von NoSQL werden also in den Applikationsdatenbanken genutzt, während die relationale Datenbank ihre Stärken als Data Warehouse ausspielt und die relevanten, aggregierten Daten speichert. CAS13Erweiterung um HadoopAuch Hadoop setzt dort an, wo traditionelle Data Warehouse-Systeme an ihre Grenzen geraten (siehe Abbildung). Hadoop ist ein auf Open Source basierendes Framework für die Erfassung, Organisation, Speichern, Suchen und Analysieren von unterschiedlich strukturierten Daten auf einem Cluster von Standardrechnern. Durch diese Architektur kann Hadoop extrem skalieren und sehr große Datenmengen performant verarbeiten. Durch die Verwendung von Standard-Hardware lassen sich die Kosten niedrig halten.Hadoop im Kontext des Data Warehouse | © PentahoHadoop stellt sowohl ein Datenarchiv als auch eine Plattform zur Datenanalyse und -aufbereitung dar. Es bietet die Basisfunktionalität eines Data Warehouse, ermöglicht also beispielsweise Aggregationen, Summen- oder Mittelwertbildungen. In Kombination mit anderen Technologien vervielfacht sich dadurch sein Nutzen enorm: Die Ergebnisse der Hadoop-Verarbeitung können im Data Warehouse oder Data Marts abgelegt werden, während die Rohdaten nur im Hadoop-System existieren. Die Analyse der veredelten Daten lässt sich dagegen mit allen Vorzügen einer Data Warehouse-Plattform durchführen, z.B. mit SQL als Abfragesprache (das von Hadoop unterstützte Hive ist nicht so leistungsfähig wie SQL).Obwohl viele Hadoop-Anbieter Werkzeuge für einen performanten SQL-Zugriff auf Hadoop-Daten entwickeln, ist es momentan sinnvoller, Hadoop mit dem klassischen Data Warehouse-Ansatz zu kombinieren. Dabei wird das Data Warehouse nicht ersetzt, sondern sinnvoll ergänzt, um die Vorteile beider Welten nutzen zu können. Trotz aller Vorzüge ist Hadoop nämlich nicht für jeden Anwendungsfall geeignet, z.B. wenn nur geringe Datenmengen analysiert werden sollen. Die Einführung von Hadoop ist außerdem mit dem Aufbau eines nicht unerheblichen Wissensschatzes im Unternehmen verbunden. ISR13TDW13Big Data-Technologien im ZusammenspielBig Data-Technologien sollten in der Realität nicht isoliert genutzt werden, sondern in Kombination. Ein Beispiel dafür ist in der nächsten Abbildung zu sehen: eine Firma betreibt eine Webapplikation oder Webseite mit hohem Besucherverkehr. Die Daten sollen gesammelt werden, um das Verhalten der Besucher zu analysieren. Ziel ist eine so genannte Clickstream-Analyse der Webseitenbesuche und der Aktionen der Anwender. Basierend auf den Ergebnissen dieser Analyse kann das Marketing die Besucher besser verstehen und wertvolle Erkenntnisse aus ihrem Verhalten ableiten. Die Rohdaten dieser Analysen liegen in den Logs der Webserver.Integrierte Big Data-Architektur | © PentahoDie Daten der Webapplikation werden in einer NoSQL-Datenbank gespeichert. Das ist sinnvoll, weil die Datenbank mit den großen Mengen an Log-Daten effizient umgehen kann und über die Flexibilität verfügt, neue Datenobjekte unkompliziert ergänzen zu können. Fallen täglich mehrere 100 Millionen neue Datensätze an, wird ihre Verarbeitung mit traditionellen ETL- und Data Warehouse-Technologien zum Problem. Aus diesem Grund werden die Daten in Hadoop abgelegt. Hadoop ermöglicht eine extrem leistungsfähige Batch-Verarbeitung, um die Daten für die gewünschten Analysen aufzubereiten. So werden die Logdaten zu den Werten „Stunden“, „Hosts“ oder „Page Level“ verdichtet und mit Informationen aus anderen Quellen angereichert.Nach diesem Schritt werden die relevanten Daten aggregiert in das Data Warehouse bzw. die analytische Datenbank geladen. Auf dieser Ebene steht der volle Funktionsumfang von SQL zur Verfügung und es werden unterschiedliche Technologien für performante Abfragen auf dem veredelten Datenbestand genutzt. Das sind optimale Voraussetzungen für Abfragen mit komplexen Filtern, Joins und Gruppierungen sowie die Nutzung von OLAP (Online Analytical Processing).Selbstverständlich sind bei dieser Architektur noch verschiedene Kombinationen bzw. Datenströme möglich. Wie bereits dargelegt, kann man zum Beispiel ein Reporting direkt auf NoSQL aufsetzen oder die Logdaten direkt in Hadoop ablegen.Totgesagte leben länger – Koexistenz des Data Warehouse mit den Big Data StoresWie in den vorangegangenen Absätzen deutlich wurde, ist das Konzept des Data Warehouse in Zeiten von Big Data aktueller denn je. Es bietet viele Vorteile und ermöglicht es, viele leistungsstarke BI-Frontends zu nutzen. Die Herausforderung bei der Konfrontation mit Big Data besteht deshalb mehr darin, es sinnvoll mit den neuen Technologien zu ergänzen und zu erweitern, um die Schwächen der klassischen Architektur auszugleichen. Um Daten aus unterschiedlichsten Quellen kombinieren und transformieren zu können, müssen außerdem Schnittstellen zu NoSQL, Hadoop, relationalen Datenbanken, Files und anderen Quellen vorhanden sein.“Die neue Realität: Erweiterung des #Data #Warehouse um #Hadoop, #NoSQL &amp; Co. #BigData“ Twittern WhatsAppDiese Mühe lohnt sich, weil dadurch wertvolles Wissen gewonnen wird. Dieses Wissen kann von den Fachabteilungen für qualitativ bessere Entscheidungen genutzt werden. Die neue Realität bedeutet also nicht das Ende des alten Data Warehouse-Ansatzes, sondern den Aufbruch in eine Zukunft, die bessere und schnellere Analysen bedeutet – werden alte und neue Ansätze richtig miteinander kombiniert.LiteraturverzeichnisBEY10 BeyeNETWORK: Analytical Platforms: Beyond the traditional Data Warehouse, 2010CAS13 Joe Caserta: Intro to NoSQL Databases, 2013INM96 William H. Inmon: Building the Data Warehouse. John Wiley &amp; Sons, 1996ISR13 isreport: Hadoop erschließt Big Data für Data Warehouses, 2013KIM13 Kimball, Ralph; Margy Ross: The Data Warehouse Toolkit: The Definitive Guide to Dimensional Modeling (3rd ed.). Wiley, 2013PEN13 Pentaho: Driving Big Data, 2013TDW13 TDWI: Where Hadoop fits in your Data Warehouse architecture, 2013";https://bigdatablog.de/2015/06/01/erweiterung-des-data-warehouse-um-hadoop-nosql-und-co/;BigDataBlog;Stefan Müller
29. Mai 15;"     ""Top 10 Big-Data-Visualisierungs-Tools für Entwickler""";"Rohdaten sind zunächst unanschaulich und bis zu einem gewissen Grad für das ungeübte Auge wenig gewinnbringend zu betrachten. Insbesondere bei Big Data sind die Datenmengen so groß, dass sie schon wegen ihrer Menge unanschaulich sind. Deswegen müssen Daten visualisiert werden und im Idealfall in eine ästhetische Form gebracht werden. Datenvisualisierung stellt aber nicht nur einen Prozess dar, Daten in eine besser wahrnehmbare Form zu bringen. In dem Vorgang der Darstellung stellt zugleich einen Teilaspekt der Analyse und Interpretation der Daten dar. Muster, und Zusammenhänge werden zum Teil erst durch die Visualisierung erkennbar. Dieser Vorgang ist deswegen nicht “nur” der abschließende Schritt bei der Big-Data-Projekten, damit die Ergebnisse präsentiert werden können.Die hier zusammengestellten Tools richten sich eher an Entwickler oder Menschen, die Erfahrung mit Programmierung und Datenbankmanagement haben. Das Angebot an Tools ist inzwischen sehr umfangreich und vielfältig. Je nach Zweck und Software-Skills gibt es unterschiedliche Speziallösungen. In den nächsten Teilen stellen wir Tools für Einsteiger und Fortgeschrittene vor. Die Möglichkeiten Daten zu visualisieren sind im Prinzip endlos und nur von der Fantasie der Nutzer und dem verwendeten Werkzeugen abhängig. Einen guten Überblick über die aktuellen Möglichkeiten zu haben, dient nicht nur der Ideen-Anregung, sondern zeigt zugleich, welche Optionen zur Verfügung stehen.Charts.js© Charts.jsDie Open Source Java-Script-Bibliothek Charts.js bietet lediglich sechs verschiedene Charttypen: Linien, Balken, Netz, Doughnut, Kuchen und Polar-Area. Obwohl das Programm nur wenige KByte groß bzw. klein ist, hat es im Vergleich zu den vielen anderen, ebenfalls kostenlosen Lösungen, einen großen Vorteil: Es produziert wirklich außergewöhnlich gut aussehende Diagramme auf Basis von HTML5, CSS3 oder JavaScript im Browser, die alle interaktiv sind. Die Dokumentation liefert schöne Beispiele für das Können von Charts.js.D3.js© D3.jsDie drei “Ds” im Firmennamen von D3.js stehen für “Data Driven Documents” und ist eine der besten Lösungen für Datenvisualisierung. Vor allem bekannt und geschätzt ist D3.js für seine großen Bibliothek mit einer atemberaubenden Fülle von Visualisierungen. Hier sollte für fast jeden etwas dabei sein. Für das folgende Beispiel nutzte etwa die New York Times.Egal ob für die Visualisierungen HTML, CSS und SVG genutzt wird, Charts mit D3.js sind eine anspruchsvolle Aufgabe. Zudem haben ältere Browser immer wieder Probleme mit der richtigen Darstellung der Charts.dygraphs© dygraphsMit dygraphs bekommt man ein Tool an die Hand, das schnell, flexibel und hochdynamisch ist. Die Open Source Java-Script-Bibliothek ist speziell auf große Datenmengen ausgelegt. Der Funktionsumfang ist deswegen vielleicht nicht so groß wie bei den Alternativen. Aber da die Fähigkeit, gut mit Big Data umzugehen, hier im Fokus steht, macht dygraphs zum Helden der Stunde. Viele Use-cases, die verdeutlichen, was dygraphs kann, finden sich in der Galerie auf der Homepage.FusionCharts© FusionChartsWenn es um Reichhaltigkeit bei der Sammlung von Charts, Karten und Graphen geht, wird D3.js nur von FusionCharts überflügelt. Bei knapp 100 Chart-Typen und 1000 verschiedenen Karten dürfte für jeden etwas dabei sein. Das Javascript-Framework kann Charts in PNG, JPEG, SVG oder PDF exportieren. Alle Charts lassen sich mit vielen kleinen Details und Features individualisieren und interaktiv gestalten. All das hat seinen Preis, der nach der kostenlosen Testphase zu Buche schlägt.Gephi© GephiDie Visualisierung von Netzwerken im weitesten Sinne ist das Spezialgebiet von Gephi. Der Funktionsumfang und der Reichtum der Darstellungsmöglichkeiten ist dabei beeindruckend. Das folgende Video gibt einen guten Überblick über die Möglichkeiten von Gephi: https://vimeo.com/9726202 Wer wissen möchte, wie sein Netzwerk bei Facebook, LinkedIn oder Xing als grafisches Netzwerk visualisiert aussieht, kann sich mittels des Tutorials an Gephi versuchen. Für Ungeübte sind letztere notwendig, da es nicht ganz einfach ist, die Daten so zu strukturieren, dass sie die gewünschten Ergebnisse liefern. Die Netzwerkforschung mit Gephi richtet sich nicht nur an interessierte Privatanwender, sondern vor allem an Wissenschaft und Wirtschaft. Im Bereich B2B oder im Marketing sind Netzwerkanalysen ein wertschöpfender Anwendungsfall von Data Mining.Google Charts© GoogleEs gibt kaum einen Bereich, in dem Google nicht aktiv ist. Mit Google Charts mischt Google auch im Bereich Datenvisualisierung mit. Eine große Stärke ist systemimmanent: Mit Google Charts kommt man in den Nutzen aller Vorteile des Internet-Giganten. Angefangen von guter Auffindbarkeit bei Google bis hin zur Kompatibilität über alle Browser und Betriebssysteme hinweg. Letzteres wird gewährleistet, indem die Charts mit HTML5/SCG gerendert werden. Im kostenlosen Angebot sind die wichtigsten Chart-Typen wie Kuchen, Balken, Pegel oder Landkarte enthalten.Highcharts© HighchartsLaut eigenen Angaben nutzen 61 der 100 umsatzstärksten Firmen der Welt eines der Produkte von Highcharts. Dennoch verschenken die sympathischen Norweger ihre Bibliothek an alle nicht-kommerziellen Projekte, die Highcharts verwenden möchten. Darin finden sich eine ganze Menge von gängigen Charts und Karten. Exportiert wird in JPG, PNG, SVG und PDF und verspricht maximale Browserkompatibilität.Seit die webbasierte, grafische Benutzeroberfläche entwickelt wurde, ist Highcharts sogar relativ leicht zu bedienen.iCharts© iChartsiCharts ist eine webbasierte Software, die Datenvisualisierung für kleine, mittlere und große Unternehmen anbietet. Das Firmenlogo wird automatisch in die Charts, Grafen und Diagramme eingearbeitet. Die Visualisierungsmöglichkeiten sind sehr ausgefeilt, können interaktiv gestaltet werden und bieten Anschlussmöglichkeiten an das Social Web. In der kostenfreien Basisversion lassen sich nur öffentliche Charts im iCharts-Channel erstellen. Mit den nach oben gestaffelten Preisgruppen, erhöht sich der Funktionsumfang dann bedeuten. Dann lassen sich die Charts und Diagramme auch auf die Firmenhomepage integrieren und für SEO optimieren.Quadrigram© QuadrigramQuadrigram kommt mit einer ganzen Bibliothek von interaktiven Visualisierungen daher. Diese können mit den eigenen Datenbanken kombiniert werden und da es keine vorgefertigten Muster gibt, muss für jeden Fall eine individuelle Lösung gefunden werden. Der Funktionsumfang der flexiblen Cloud-Software ist entsprechend groß. Quadrigram richtet sich sowohl an Kunden wie Start-ups, die mit ihrer Geschäftsidee noch am Anfang stehen, aber auch an Großkonzerne und lässt sich entsprechend anpassen. Entwickler werden mit Darstellungsoptionen wie Wärmekarten, 3D-Netzen und Polygonen ihre Freude haben.Visualize Free© Visualize FreeDie kostenlose Version Visualize Free basiert auf dem kommerziellen Service von InetSoft. Sie erlaubt es, große Datasets im Exel- (XLS und XLSX) oder CVS- bzw. TXT-Format hochzuladen. Mehrere Chart-Typen stehen zur freien Verfügung: Wortgrafiken, Histogramme, Blasen, Gitternetze, Bulletpoint-Graphen, Multi-Charts und Landkarten. Wenn mehrere Datasets mit einer recht simplen Drag- und Drop-Funktion eingerichtet sind, ist die Verarbeitung von Daten sehr einfach gehalten.“Ein Petabyte Daten ist viel &amp; unanschaulich. Diese 10 Tools helfen bei der Visualisierung. #BigData“ Twittern WhatsAppLese auch: Freie Big-Data-Tools – Entdecke die Möglichkeiten";https://bigdatablog.de/2015/05/29/top-10-big-data-visualisierungs-tools/;BigDataBlog;Ibrahim Evsan
21. Mai 15;"     ""Von den Rohdaten zur Erkenntnis: Die 5 Eckpunkte einer erfolgreichen Big-Data-Strategie""";Unternehmen werden sich erst langsam über den Wert ihrer Daten bewusst. Auf ihren Servern befinden sich derzeit noch eher Datenfriedhöfe als Data Warehouses. Die Voraussetzung für ihre Nutzung ist zunächst ein Bewusstsein für den Wert von Daten.Welche Zusammenhänge gibt es zwischen Daten aus dem Web (aus Blogs, Tweets und Kommentaren), Auftragsdatenbanken und den Produktionsdaten aus dem Maschinenpark?Der überwiegende Teil des Mittelstandes vernachlässigt bislang die Chancen, die mit der Digitalisierung und Big Data verbunden sind. Diejenigen Unternehmen, die neue Technologien zur Innovation nutzen, optimieren ihre Prozesse und erschließen neue Geschäftsfelder.Dabei ist es heute leichter denn je, Big-Data-Lösungen zu realisieren. Dank günstig verfügbarer Technologien wie Hadoop oder Spark sind Big-Data-Projekte, die bis vor wenigen Jahren nur führenden Großkonzernen zur Verfügung standen, erschwinglich. Mit kleineren Teams lassen sich so auch in kleineren und mittelständischen Unternehmen Big-Data-Technologien einsetzen. Um von den Rohdaten zur Erkenntnis zu gelangen sind die folgenden 5 zentrale Eckpunkte zu beachten.1. Datengenerierung und DatenerwerbDie Ausgangslage eines Unternehmens oder einer Organisation vor der Entscheidung, Big Data anzuwenden, kann sehr unterschiedlich sein. Daten können bereits in großen Mengen vorhanden sein. Oft müssen diese aber erst generiert oder durch generierte Daten erweitert werden. Dazu werden beispielsweise Maschinen in der Produktion mit Sensoren oder Produkte und Fertigungseinheiten mit Chips ausgestattet. Darüber lassen sich zunächst Prozesse optimieren, aber auch Daten über die Produktion insgesamt erzeugen. Je nach Interessenslage und finanziellem Rahmen einer Big-Data-Strategie können externe Daten auch hinzugekauft werden. Dabei handelt es sich in der Regel um Daten, die Kunden und ihre Interessen und Wünsche betreffen.2. Transformation und AggregationAuf diese Weise entsteht eine große Ansammlung von Daten – ein Data-Lake. In diesem “Daten-See” befinden sich Daten in strukturierter, unstrukturierter oder semistrukturieter Form vor. Zur weiteren Verarbeitung müssen die Daten deswegen auf ihre Qualität und Relevanz hin geprüft, transformiert und für die Analyse vor- und aufbereitet werden. Die Standardisierung von Daten ist ein ressourcenintensiver Prozess, bei dem einerseits die Gefahr besteht, die Aussagekraft der Daten zu verfälschen und andererseits wenige Automatisierungs-Tools bereitstehen, die die Arbeit erleichtern. Die Aggregation von Daten – der Vorgang bei dem Daten gruppiert und mit Metadaten versehen werden – nimmt bereits einen Teil der Interpretation vorweg.3. DatenanalyseBei der eigentlichen Datenanalyse geht es um die Kunst der Interpretation der Daten. Die Korrelationen von Auswertungen können Lücken und Fehler in Abläufen und Prozessen aufzeigen. Historische Daten erlauben des Weiteren Aussagen über Entwicklungen in der Vergangenheit. Der Bereich der Predictive Analytics versucht aus diesen Zusammenhängen in Kombination mit Trendanalysen zukünftige Entwicklungen vorherzusagen. Die Datenanalysen schaffen damit die Grundlage für in die Zukunft weisende Entscheidungen und minimieren das Risiko von Fehlinvestitionen.4. VisualisierungDie Visualisierung von Daten ist weit mehr als eine bloße Illustration der Datenanalysen. In den Grafiken liegt vielmehr der Schlüssel zum Verständnis der in den Daten enthaltenen Informationen. Da sie an der Scharnierstelle zwischen IT und anderen Abteilungen und Verantwortungsbereichen stehen, kommt ihnen eine bedeutende Stellung zu. Nur wenn Analysen auf eine verständliche Weise zur Darstellung kommen, können die Erkenntnisse aus den Daten wirksam werden. Visualisierungen schaffen eine Vertraunsbasis, dass eindeutige, nachvollziehbare und belastbare Fakten das Ergebnis der Untersuchungen sind.5. DistributionDie Distribution der Ergebnisse von Big-Data-Analysen ist nicht nur ein Vorgang innerhalb eines Unternehmens, bei dem die Entscheidungsträger mit Wissen versorgt werden. Distribution bedeutet auch die Weitergabe von Informationen an Kunden oder das Angebot von Daten zur weiterführenden Nutzung. Diese können sowohl im Bereich Forschung und Wissenschaft als auch der gewinnbringenden Vermarktung liegen.Big Data und die nötige Unterscheidung von Datenschutz und Schutz der DatenDer Begriff Datenschutz ist spätestens mit der wertsteigernden Nutzung von Big Data zweideutig geworden. Im gängigen Sprachgebrauch meint ‘Datenschutz’ vor allem den Aspekt des Schutzes der personenbezogenen Daten. Datenschutz bedeutet heute aber für Unternehmen sowie für private bzw. öffentliche Organisationen vor allem den Schutz der eigenen Daten vor fremden Zugriff. Für datengetriebene Unternehmen bedeutet Betriebsspionage im schlimmsten Fall ein Totalverlust.“#BigData Einmaleins: In fünf Schritten von den Rohdaten zur Erkenntnis.“ Twittern WhatsAppSichere Systemarchitekturen sind daher neben der Bereitschaft, Daten erkenntnis- und gewinnbringend zu nutzen, die Grundvoraussetzung.;https://bigdatablog.de/2015/05/21/von-den-rohdaten-zur-erkenntnis-die-zentralen-eckpunkte-einer-erfolgreichen-big-data-strategie/;BigDataBlog;Ibrahim Evsan
12. Mai 15;"     ""Big Data in der Praxis: Der intelligente Autositz""";Die technischen und rechtlichen Aspekte von Big Data sind mitunter äußerst kompliziert und nur schwer nachzuvollziehen. Ganz anders sieht es mit der praktischen Dimension von Big-Data-Technologien aus. Hier gibt es faszinierende und durchaus überraschende Erkenntnisse und daraus abgeleitete Einsatzszenarien. Beispielsweise sitzt jeder Mensch auf eine unverwechselbare Art und Weise, die so einmalig und individuell ist wie ein Fingerabdruck. Mit Big Data lässt sich diese Art des Sitzens exakt analysieren. Daraus ergeben sich eine ganze Reihe von Einsatzmöglichkeiten.the_ad id=“1381?Diebstahlschutz in neuer DimensionDie Sensoren im Autositz ermöglichen es dem Auto in Zukunft die Fahrer eines Autos genau zu identifizieren. Eine Vielzahl von Sensoren wertet dazu in Echtzeit das Sitzverhalten aus. Gewicht, Gewichtsverteilung und Sitzverhalten ergeben ein so typisches Muster, dass jeder für ein Fahrzeug zugelassener Fahrer sicher erkannt wird. Dadurch wird ein Diebstahlschutz in einer ganz neuen Dimension möglich. In Kombination mit einer Stimmererkennung könnten Zündschlüssel und andere Wegfahrsperren vollends überflüssig werden.Schutz bei Müdigkeit und TrunkenheitDer intelligente Autositz schützt aber nicht nur vor Diebstahl, sondern auch den Autorfahrer vor sich selbst. Wenn etwa spät nachts die Müdigkeit Oberhand gewinnt, macht sich dies auch in der Körperhaltung bemerkbar. Entsprechend können Gegenmaßnahmen eingeleitet werden. Ein lautes Signalgeräusch kann vor dem berüchtigten Sekundenschlaf am Steuer schützen. Auch denkbar sind Hinweise, die Pausen empfehlen oder vor der Weiterfahrt warnen. Wer sich wiederum betrunken ans Steuer setzt, wird sein Auto erst gar nicht starten können. Damit steigt durch den Big-Data-Autositz nicht nur die Sicherheit, sondern auch die der anderen Verkehrsteilnehmer.Das Hauptziel des vernetzten Autos: die Steigerung der SicherheitDatenerhebung, Datenverwertung und Datenspeicherung im Auto nimmt nicht nur immer mehr zu, sondern in einigen Fällen sind die Sensoren und Systeme vom Gesetzgeber vorgeschrieben. Sensoren, die etwa permanent den Reifendruck messen und Unfälle verhindern sollen, sind für viele Fahrzeuge bereits verpflichtend. Ab 2018 müssen alle Neuwagen, die in Europa verkauft werden, mit einem eCall-System ausgestattet sein. Im Notfall sendet diese Mobilfunkeinheiten automatisch einen Datensatz ab und versucht, eine Sprachverbindung herzustellen. Das eCall-System ist mit vielen anderen Datenquellen im Auto vernetzt und kann im Notfall detailliert Auskunft geben.Die Datenexplosion im Auto steht noch bevorBeispiele wie der Big-Data-Autositz sind nur die Spitze des Eisberges. Die eigentliche Datenexplosion im Auto steht erst noch bevor. Sobald das selbstfahrende Auto serienreif ist, werden Daten in enormen Mengen erhoben und verarbeitet.“Der Autositz der Zukunft erkennt den Fahrer an seinem Allerwertesten. Wie? #BigData macht’s möglich.“ Twittern WhatsAppAb einem bestimmten Punkt könnte sich die Technik selbst überholen: Irgendwann wird die Frage, wer in ein Auto steigt, ob er müde oder betrunken ist, vielleicht hinfällig sein. Und unabhängig vom körperlichen Zustand bringen selbstfahrende Autos ihre Insassen sicher zum Ziel.;https://bigdatablog.de/2015/05/12/big-data-in-der-praxis-der-intelligente-autositz/;BigDataBlog;Christian Schön
08. Mai 15;"     ""Big Data als Reiseführer: Warum die Reiseindustrie am Daten-Scheideweg steht""";Noch nie war das Reisen so einfach und bequem – und dennoch steigt unser Anspruch an den eigenen Urlaub immer weiter. Wir wollen Erholung am Strand, so viele Sehenswürdigkeiten wie möglich besuchen, die Kultur neuer Länder kennenlernen – und das Ganze bei möglichst geringem Aufwand und in kürzester Zeit.Der Wunsch nach intelligenten Reiseerlebnissen zwingt Unternehmen dazu, ihre Angebote mit Hilfe von Big Data individuell auf die Bedürfnisse und Vorlieben der Kunden abzustimmen. Noch wählen wir unsere Reiseziele selbst aus und planen Reiserouten, Stops und Restaurant-Besuche. In Zukunft könnte das alles Big Data für uns übernehmen.Der perfekte Road-Trip durch die USARandal Olson, Blogger und Doktorand der Michigan State University, hat es sich kürzlich zur Aufgabe gemacht, einen perfekten und möglichst effektiven Roadtrip durch die USA zu planen. Mithilfe eines selbst entwickelten Algorithmus berechnete er die besten Verbindungen und Distanzen zwischen den Top-Spots des Landes, damit die Reise so kurz wie möglich wird.Den Masterplan für Reisen, den er auch gleich auf Europa übertrug, hat Olson bei Google Maps visuell umgesetzt. Sein französischer Blogger-Kollege Simpki optimierte den Roadtrip durch Europa sogar noch und berechnete auf Grundlage der vorgegebenen Route die günstigste Reisemöglichkeit.Olsons Trip führt durch alle 48 Staaten des Festlandes der USA, lediglich Alaska und Hawaii werden nicht besucht. (Quelle: Randal S. Olson)Big Data wird die Reiseindustrie revolutionierenWas beide Blogger noch mühsam und für einen ganz besonderen Trip berechnet haben, könnte dank Big Data in Zukunft für jede Reise und jeden Kunden individuell angeboten werden und damit die Reiseindustrie revolutionieren.Schon heute finden Vergleichsportale sekundenschnell die besten Preise für Flüge, Hotels und Mietwagen. Apps geben uns einen Überblick über alle gebuchten Reisinformationen, Zoll- und Einreisebestimmungen und spielen den Reiseführer vor Ort.“Das richtige Hotel und perfektes #Sightseeing: Wie #BigData zum #Reiseführer wird.“ Twittern WhatsAppDoch die Grenzen von Big Data und der Vernetzung der gesammelten Daten sind lange nicht erreicht. Bereits 2013 zeigte eine große, weltweite Studie im Auftrag von Amadeus, dass die Reiseindustrie am Big-Data-Scheidweg steht.Lese auch: Smart Community: So hilft Big Data in Städten, Kommunen und GemeindenSchon in absehbarer Zeit könnten wir unsere Urlaubsplanung lediglich durch unser Reiseziel und die Dauer unseres Aufenthalts bestimmen. Der Flug mit unserer bevorzugten Airline, das zum Budget passende Hotel und der selbstfahrende Mietwagen am Flughafen werden automatisch gebucht.Big Data verrät uns, wohin wir reisen wollenVor Ort bieten uns Reiseveranstalter auf unsere Vorlieben abgestimmte Ausflüge, ein perfektes Wellness-Angebot oder verweisen per App auf die Geheimtipps unserer Facebook-Freunde. Die Kombination aus Mobil- und GPS-Daten sowie deren Auswertung sagt uns, wann ein beliebtes Touristenziel gerade besonders überlaufen ist. Und auch die Wetterentwicklung wird bei der Planung unseres Trips in Echtzeit berücksichtigt.Führt man dieses Gedankenexperiment zu Ende, werden Big Data und unsere Daten uns sogar verraten, wohin wir eigentlich reisen wollen.;https://bigdatablog.de/2015/05/08/big-data-als-reisefuehrer-warum-die-reiseindustrie-am-daten-scheideweg-steht/;BigDataBlog;Ibrahim Evsan
07. Mai 15;"     ""Hadoop bekommt Konkurrenz: Apache Spark ist 100 mal schneller""";Im Schatten der Erfolgsgeschichte von Apache Hadoop wurde fünf Jahre lang am Framework Spark entwickelt, vier Jahre davon ebenfalls als Open-Source-Projekt. Seit einem Jahr ist nun Apache Spark auf dem Markt und macht Hadoop Konkurrenz. Zwar vollbrachte Hadoop das kleine Wunder, den Einsatz von Big Data wirtschaftlich zu gestalten, ein Alleskönner ist das freie Framework aber nicht. Einer der Hauptkritikpunkte an Hadoop war, nicht für die Echtzeitdatennalyse zu taugen. In diese Lücke springt Apache Spark ein, das bis zu 100 mal schneller sein soll.Im Mai 2014 wurde die für Apache Frameworks wichtige Version 1.0.0 veröffentlicht. Seitdem ist der Erfolg von Apache Spark nicht mehr zu stoppen: In einer nicht-repräsentativen Umfrage gaben 65% der über 2000 befragten Unternehmen an, Spark entweder bereits zu nutzen, den Einsatz gerade prüfen oder bereits konkret für 2015 planen. Großkonzerne wie Alibaba, IBM oder die NASA setzen Spark bereits erfolgreich ein.Die Gemeinsamkeiten von Hadoop und SparkWie Hadoop ist auch Spark ein frei verfügbares Framework von Apache, das einfach von der Spark Homepage geladen werden kann. Einzelne Anwendungslösungen werden auf dieses Rahmengerüst aufgesetzt. Wie Hadoop erfordert auch Apache Spark keine besondere Hardware, sondern verspricht Superleistung mit normalem Equipment.Lese auch:Big Data und Hadoop: Apache macht das Unmögliche möglichDieser Aspekt ist für die Wirtschaftlichkeit entscheidend. In gewisser Weise baut Spark auf dem Prinzip von Hadoop auf: Um Spark zu betreiben wird ein Hadoop-Cluster benötigt, sprich: ein großer Verbund sicherer Netzwerk-Rechner.Das kann Apache Spark besserDer Erfolg von Hadoop basiert zu einem großen Teil auf dem Map-Reduce-Verfahren – dem Algorithmus, der auch dem Suchdienst von Google zugrunde liegt. Apache Spark setzt im Gegensatz dazu ganz auf In-Memory-Datenverarbeitung. Dabei werden die zu analysierenden Daten direkt im Arbeitsspeicher der Cluster-Knoten gespeichert und verarbeitet. Dieses Verfahren verschafft enorme Geschwindigkeitsvorteile gegenüber konventionellen Systemen, bei denen Daten zunächst vom Festspeicher geladen werden müssen. Erst wenn die Datenmengen zu groß werden, lagert auch Spark diese auf die Festplatten aus.Ein weiterer Vorteil von Apache Spark ist die Fähigkeit, mit einer Vielzahl unterschiedlicher Datenquellen zu arbeiten. Neben Daten aus dem Hadoop Distributed File System (HDFS) können Daten aus relationalen Datenbanken wie Hive und NoSQL-Datenbanken verarbeitet werden.Spark kann darüber hinaus weitaus komplexere Aufgaben bearbeiten als Hadoop. Der Map-Reduce-Algorithmus von Hadoop ist darauf ausgelegt, ein Problem in Einzelteile zu zerlegen, auf mehrere Server verteilt parallel zu bearbeiten und die Ergebnisse wieder zusammenzuführen. Im Vergleich dazu setzten die Entwickler von Apache Spark auf Machine-Learning. Seit dem Release 1.3 im Februar 2015 ist die Bibliothek MLIib (Machine Learning Library) enthalten. Diese Sammlung von intelligenten Algorithmen erlauben es beispielsweise Spotify, den Musikgeschmack seiner User nach nur drei Klicks vorherzusagen.Lese auch:Hadoop erhält KonkurrenzUm den Sieg nach Punkten abzuschließen, trumpft Spark schließlich noch beim Thema Skalierbarkeit. Während bei Hadoop mit kleinen Datenmengen nicht zurecht kommt, lassen sich mit Spark sowohl Daten im Bereich Megabyte als auch im Petabyte-Bereich verarbeiten.Ist das Ende von Hadoop in Sicht?Mit all diesen Vorteilen bringt Apache Spark Big-Data-Anwendungen in neue Dimensionen. Mit Echtzeitdatenanalysen, der Möglichkeit unterschiedlichste Datenformate und -quellen auswerten zu können sowie der Fähigkeit, komplexe Aufgaben zu lösen stellt das Framework eine echte Konkurrenz zu Hadoop dar.Hadoop oder Spark: Welche Big-Data-Anwendung ist zukunftsfähig?Eindeutig Spark - Hadoop hat ausgedientHadoop bietet immer noch eine gute Basis zur Auswertung, auch in ZukunftIch kenne weder Hadoop noch SparkGroße Softwareunternehmen wie Hortonworks, die bislang auf Hadoop gesetzt haben, setzen nun bereits vielfach auf Spark. Dies könnten möglicherweise die ersten Anzeichen für das frühe Ende von Hadoops Erfolgsgeschichte sein. In jedem Fall stößt Apache Spark in Bereiche vor, die für Hadoop unerreichbar sind.““Das Bessere ist der Feind des Guten” (Voltaire) – #Hadoop bekommt Konkurrenz durch #Spark.“ Twittern WhatsAppNicht nur deswegen halten Experten Spark für einen aussichtsreichen Kandidaten, die vereinheitlichende Big-Data-Technologie zu werden.;https://bigdatablog.de/2015/05/07/hadoop-bekommt-konkurrenz-apache-spark-ist-100-mal-schneller/;BigDataBlog;Christian Schön
05. Mai 15;"     ""Erst durch Metadaten wird Big Data beherrschbar""";Was haben die beiden Datentypen Big Data und Metadaten miteinander zu tun und welche Rolle spielen letztere? Über Funktionsweise und Nutzen von Metadaten.Das „Big“ in Big Data suggeriert, dass vor allem große Datenmengen entscheidend bei der Analyse von Big Data ist. Um richtige und aussagekräftige Erkenntnisse aus Daten abzuleiten, ist es jedoch wichtiger, über die wesentlichen Daten und Datentypen zu verfügen. Big Data bestehen aus unterschiedlichsten Formen von Daten. Bei der Analyse können interne und externe Daten verwendet werden. Daten aus dem Mailserver oder aus Social Media sind grundsätzlich verschieden von Sensordaten, die bei der Überwachung von Fertigungsanlagen anfallen. Metadaten stellen unter den verschiedenen Datentypen eine ganz besondere Form von Daten dar. Es sind Daten über Daten.“#Metadaten: Irgendwie einzigartig, irgendwie besonders. #BigData“ Twittern WhatsAppSchon rein ihrer Form nach sagen Metadaten also etwas über andere Daten aus. Verwertbare Aussagen sind demnach sehr schnell verfügbar. Letzteres wird bei der Anwendung von Big Data immer wichtiger: die Verfügbarkeit von Ergebnissen in Echtzeit.Metadaten am Beispiel von digitalen FotosJedes Foto, das mit einer digitalen Kamera aufgezeichnet wird, besteht aus Daten. Die einzelnen Pixel, die am Ende zusammen das Bild ergeben, sind Einzelinformationen über Farb- und Helligkeitswerte. Jede Bilddatei verfügt aber über noch viel mehr Informationen, nämlich über Metadaten:Wann wurde das Bild aufgezeichnet?Welche Verschlusszeit, welche Blendenöffnung und welches Programm wurde verwendet?Welcher ISO-Wert war eingestellt?Verfügt eine Kamera über den entsprechenden Sensor werden auch die GPS-Daten vom Aufnahmeort abgespeichert. Diese Daten sind allesamt Metadaten, die Aussagen über die eigentlichen Daten – wenn man so will die “Primärdaten” – zulassen. Mithilfe der Metadaten lassen sich Fotos in vielerlei Hinsicht untersuchen. Am Ende eines Jahres helfen die Daten zum Beispiel dabei, das eigene Fotografierverhalten auszuwerten.Zu welcher Uhrzeit schieße ich die meisten Fotos?An welchen Orten auf der Welt habe ich meine Bilder aufgenommen?Habe ich oft versucht bei zu wenig Licht zu fotografieren oder habe ich fast immer das Standardprogramm verwendet?Diese Informationen sind beispielsweise beim nächsten Kamerakauf interessant. Metadaten übernehmen aber noch eine weitere, überaus hilfreiche Funktion.Metadaten sind digitale ArchivareAllgemein gesagt, liefern Metadaten also Informationen über Daten. Damit stellen sie eine Art Verschlagwortung dar und helfen dabei, Daten bei einer Suche zu identifizieren. Über die Metadaten von Digitalfotos lassen sich sehr leicht bestimmte Bilder aus einer großen Menge Bilder herausfiltern. Angenommen für eine Werbeanzeige wird ein Bild von einem Sonnenuntergang in Thailand benötigt – zwei Suchkriterien genügen, um in einem Fotoarchiv mit tausenden Bildern die richtigen Aufnahmen schnell zu finden. Aufnahmezeitpunkt und GPS-Informationen reichen aus, eine riesige Menge für die Suche uninteressanten Bilder auszuschließen.Hier wird deutlich, warum Metadaten bei Big-Data-Analysen so entscheidend sind. Sie ermöglichen es, die Menge so stark zu verringern, dass eine Datenauswertung immer leichter wird. Zu früheren Zeiten legten Archivare Zettelkästen an, um den Aufenthaltsort von Dokumenten und Büchern in Archiven schnell zu finden. Diese Funktion übernehmen Metadaten in den digitalen Datenarchiven.“Metadaten sind digitale Archivare. Bei #BigData Analysen helfen sie, Aussagen in Echtzeit zu treffen.“ Twittern WhatsAppUnabhängig von der Rechenleistung erlauben Metadaten große Datenmengen handelbar zu machen. Je weniger Daten verarbeitet werden müssen, desto eher sind Ergebnisse in Echtzeit verfügbar. Die steigende Datenflut wird durch Metadaten beherrschbar und Big Data zu einem effektiven Arbeitsinstrument.;https://bigdatablog.de/2015/05/05/wie-metadaten-big-data-beherrschbar-machen/;BigDataBlog;Christian Schön
03. Mai 15;"     ""Warum Vergleiche mit Big Data schief gehen""";Wie soll man sich eine Vorstellung von etwas machen, das in seiner Art neu ist und das alle Maßstäbe des bisher Dagewesenen übersteigt? Ein Vergleich kann da nur hinken.Um ein neues, nie dagewesenes Phänomen zu verstehen, bauen wir uns Brücken zu der Welt und den Dingen, die wir kennen. Das funktioniert über Bilder, Metaphern und Vergleiche. Im Fall von Big Data war das nicht anders. Vor wenigen Jahren tauchte das Phänomen plötzlich auf und war, noch ohne richtig begriffen worden zu sein, in aller Munde. Ein viel bemühter und vielfach kopierter Vergleich, um Big Data verständlich zu machen, lautete in etwa folgendermaßen:Die gleiche Menge an Daten, die die Menschheit bis zum Jahr 2000 erzeugte, wurde kaum 10 Jahre später in 10 Minuten erzeugt.So oder so ähnlich lautete der Vergleich, der den Maßstab und das Potential von Big Data verständlich machen sollte. Doch dieser Vergleich hinkt so gewaltig und ist richtig wie falsch zugleich.Daten, Informationen und WissenBei dem Vergleich werden in einem Atemzug aus Daten Informationen und aus Informationen Wissen. Dabei unterscheidet sich die Art von Informationen und Wissen, die Big Data beinhaltet wesentlich von dem Wissen, das die Menschheit in den vergangenen Jahrtausenden erzeugt, gesammelt, archiviert und der nächsten Generation weitervererbt hat.Eine Flugzeugturbine erzeugt bei einem Flug über den Atlantik – datenoptimiert betrachtet – ein Vielfaches an Daten im Vergleich zur Allgemeinen Relativitätstheorie von Albert Einstein oder zum gesamten Werk von Honoré de Balzac. Allein die Datenmenge sagt noch nichts über den Gehalt oder die Qualität der Informationen aus, geschweige denn handelt es sich schon um Wissen.Masse und Macht der DatenBei den großen Mengen erzeugter Daten ist nicht die Einzelinformation an sich interessant. Nicht einmal alle Informationen zusammengenommen ergeben oft ein verwertbares Wissen. Das wertvolle Wissen der Daten liegt oft in ihrer Struktur, ihrer Anordnung und in ihnen enthaltenen Mustern. Oft stecken Erkenntnisse erst in den Metadaten, den Daten über die Daten.Das Wissen, das aus der Analyse von Daten gewonnen wird, ist zudem ein Wissen, das auf Korrelationen beruht. Es unterscheidet sich wesentlich von dem Wissen, das auf Kausalitäten basiert. Ein Beispiel: Isaac Newton genügte ein Apfel, der ihm der Legende nach auf den Kopf fiel, um die Gesetze der Anziehungskraft der Erde zu erkennen. Würde man die Gesetze der Gravitation mit Big-Data-Methoden untersuchen, würde man alle potentiell fallendenden Äpfel mit Sensoren ausstatten. Am Ende eines festgelegten Untersuchungszeitraumes untersucht man, welche Muster sich abzeichnen. Siehe da: alle Äpfel weisen mehrheitlich dasselbe Verhalten auf und fallen nach unten. Das Gesetz der Gravitation ist beschrieben.Das Datenwissen unterscheidet sich weiter auch wesentlich von dem Wissen, das in Literatur oder Musik zu finden ist. Rein mengenmäßig wird also tatsächlich – inzwischen innerhalb von wenigen Minuten – ein Vielfaches dessen produziert, was die Menschheit insgesamt hervorgebracht hat, wenn es in Daten übersetzt wird. Im Detail wiegt die isolierte Einzelinformation jedoch sehr wenig im Vergleich zu dem, was eine einzelne Information der Menschheitsgeschichte auszusagen vermag.Zur Verstehbarkeit von Daten und AlgorithmenDie Unterschiede zwischen Datenanalysen bzw. Datenwissen und anderen Wissensformen sind aber noch weit gravierender. Die Fortschritte auf dem Feld der Künstlichen Intelligenz, Mashine Learning und Deep Structured Learning führen dazu, dass von außen nicht mehr verstanden werden kann, wie ein bestimmtes Ergebnis einer Datenanalyse zustande kam. Wir werden in Zukunft also mit einem Wissen konfrontiert sein, das nicht mehr überprüfbar ist.“#BigData: Wir werden mit einem Wissen konfrontiert sein, das nicht mehr überprüfbar ist.“ Twittern WhatsAppJe nach Anwendungsfall kann dies ärgerliche Konsequenzen haben. Algorithmen können künftig die Kreditwürdigkeit überprüfen und dabei Dinge miteinander in Beziehung setzen, deren Zusammenhänge sehr komplex sind. Weder Kunde noch Bankberater noch Programmierer könnten am Ende das Urteil über die Kreditvergabe verstehen oder begründen. In diesem Video erklärt der Gründer von des Hamburger Start-ups Kreditech Sebastian Diemer, dass inzwischen nur noch der selbstlernende Algorithmus “weiß”, warum Kredite gegeben werden und warum nicht.Dabei sind Big-Data-Analysen nicht unfehlbar. Der Fall des Vorzeigeprojekts „Google Flu“ führt vor Augen, wie falsch Wissen sein kann, das nur auf Korrelation beruht. Google Flu wertete Suchanfragen aus, die auf Grippe-Epidemien hinwiesen und prognostizierte mit diesem Wissen reale Bedrohungen. Und scheiterte. Im besten Fall hätte dieser Algorithmus viele Menschenleben retten können und so ist sein Scheitern sehr bedauerlich. Doch was ist, wenn ein Algorithmus beim “Predictive-Policing” irrt und ein Verbrechen falsch vorhersagt?“#BigData: In 10 Minuten werden mehr Datenmengen erzeugt wie in den 40.000 Jahren davor.“ Twittern WhatsAppDamit Big Data zu einem sinnvollen Instrument wird, das den Menschen nutzt, ist es also notwendig, das Phänomen selbst und all seine Aspekte zunächst richtig zu verstehen. Dazu gehört es auch, die richtigen Begriffe und richtigen Vergleiche zu finden.;https://bigdatablog.de/2015/05/03/warum-big-data-vergleiche-schief-gehen/;BigDataBlog;Christian Schön
29. Apr 15;"     ""Big Data und Hadoop: Apache macht das Unmögliche möglich""";"Big Data lebt von großer Datenvielfalt, enormen Datenmengen und schneller Verfügbarkeit. Gleichzeitig sind dies die größten technischen Herausforderungen. Hadoop bietet all das: Billige Speichermöglichkeit, schnellen Zugriff und eine große Vielfalt. Wie geht das?Wie alles hat auch Big Data zwei Seiten. Doch die zwei Seiten der Big-Data-Medaille sind identisch. Die Vorteile auf der einen Seite sind zugleich die Nachteile auf der anderen Seite. Je größer die Datenmenge, umso besser lassen sich Muster erkennen und Vorhersagen treffen.Aber: Je größer die Datenmenge, umso teurer die Speicherung und umso langsamer der Zugriff bei der Verarbeitung. Je größer die Vielfalt der Daten, desto vielfältiger die Einblicke in komplexe Zusammenhänge. Aber: Je größer die Datenvielfalt, desto schwieriger wird es, alle Daten miteinander auszuwerten, die Daten zu speichern und entsprechend teurer ist die Anwendung.Hadoop von Apache schafft Abhilfe bei der DatenbewältigungDie Lösung für all diese Probleme heißt Hadoop – das freie Framework von Apache mit dem kleinen Elefanten als Emblem. Sowohl der Name als auch das Firmenlogo gehen auf ein Kinderspielzeug des Sohnes von Gründer Douglass Cutting zurück. Der Guardian kürte Hadoop 2011 noch vor dem iPad und Wikileaks zur Innovation des Jahres. Hadoop machte seither Big Data zu einer alltagstauglichen Anwendung auf einem regulären Computer. Die ersten Unternehmen, die Hadoop mit Erfolg anwandten, sind neben Google und Yahoo Facebook, AOL und IBM. Diese Unternehmen waren die ersten, die vor der neuen Herausforderung standen, riesige Datenmengen schnell zu verarbeiten. Viele Anbieter integrieren inzwischen Hadoop in ihre Werkzeuge und Produkte, da heute mehr und mehr reguläre Unternehmen vor denselben Problemen stehen wie Google und Yahoo vor 15 Jahren.Was genau ist Hadoop?Douglass Cutting, der Erfinder von Hadoop, war lange Zeit eine zentrale Figur der Open-Source-Bewegung. Als Google Labs um das Jahr 2003 einen Teil seines Map-Reduce-Algorithmus veröffentlichte, erkannte Cutting die Bedeutung und das Potential dahinter. Er begann mit der Entwicklung des Hadoop-Systems, das Allen ermöglichen sollte, in den Genuss einer Open-Source-Suchmaschine zu kommen.Heute verspricht Hadoop eine Lösung für die drei großen Big-Data-Probleme – auch die drei großen Vs genannt:Datenmenge (Volume),Datenvielfalt (Variety) undZugriffsgeschwindigkeit (Velocity).“Die 3 großen Probleme von #BigData: Volume, Variety, Velocity. Die Lösung: #Hadoop.“ Twittern WhatsAppZunächst ist Hadoop ein freies, in Java geschriebenes Open-Source-Software-Framework, also ein “Programmiergerüst”, von Apache. Das heißt, dass sich der Quellcode von Hadoop frei von der Homepage von Apache herunterladen lässt. Darüber hinaus ist keine spezielle Hardware notwendig. Denn das Grundprinzip von Hadoop ist, eine große Anzahl von Rechnern zu Clustern zusammenzuschließen. In so einem Hadoop-Cluster können riesige Datenmengen gespeichert und mit enormer Geschwindigkeit verarbeitet werden. Letzteres ist möglich, weil die Rechenleistung der einzelnen Knotenpunkte im Cluster zur Berechnung der Anfragen mit genutzt werden.So funktioniert HadoopHadoop hat derzeit vier einzelne Komponenten, aus denen es in der Basisversion besteht:Hadoop Common,Hadoop DFS/ HDFS,der Map-Reduce-Algorithmus undHadoop YARN.Hadoop Common liefert alle Programmdaten und Ressourcen, um eine Hadoop-Lösung aufzusetzen. Das “Hadoop Distributet File System” (HDFS) ist ein verteiltes Dateisystem. Das heißt, dass im HDFS Dateien wahlweise auf einem oder mehreren Punkten im Rechnerverbund gespeichert werden können. Es besteht sogar die Option, automatisch von allen Dateien Duplikate anzulegen, damit einzelne Daten nicht verloren gehen, wenn ein Knotenpunkt ausfällt. Das HDFS ist hochleistungsfähig und kommt laut Apache mit mehreren 100 Millionen Anwendungsdaten zurecht.Der bereits erwähnte Map-Reduce-Algorithmus ist der Kern von Hadoop. Er zerteilt die Rechenaufgaben zuerst in viele kleine Teile (Map-Prozesse), die parallel an unterschiedlichen Stellen bearbeitet werden. Die Ergebnisse werden in einem Zwischenschritt zusammengeführt und in der Reduce-Phase abschließend ausgewertet. Dieses Verfahren wird von Hadoop YARN ergänzt. YARN steht für “Yet Another Resource Negotiator” und stellt eine weitere Cluster-Verwaltungstechnik dar. Da YARN ähnlich wie der Map-Reduce-Algorithmus die im Rechnerverbund zur Verfügung stehenden Ressourcen managt, wird er manchmal auch Map-Reduce-2 genannt.Der kleine Elefant und die DatenrevolutionDiese vier Komponenten machen die Sprengkraft von Hadoop aus. Das Clustern und Verteilen von Datenpaketen auf einer Vielzahl von Servern eines Rechnerverbunds mit dem HDFS birgt auch Sicherheitsrisiken. Der Map-Reduce-Algorithmus und YARN, die die rasante Bearbeitung und günstige Speicherung riesiger Datenmengen ermöglichen, haben aus Hadoop in den letzten Jahren die Big-Data-Anwendung werden lassen. Durch die Kombination dieser Komponenten kann Hadoop komplexe Rechenaufgaben im Petabyte-Bereich (&gt; 1 000 000 Gigabyte) realisieren. Hadoop machte Anwendungen in dieser Größenordnung erstmals wirtschaftlich, weswegen mit dem System der Beginn der Datenrevolution eingeläutet wurde.“#Hadoop revolutioniert #BigData. Warum es dennoch kein Allheilmittel ist.“ Twittern WhatsAppLange war diese Aussage richtig. Dennoch ist Hadoop kein Allheilmittel. Müssen etwa nur kleinere Datenmengen verarbeitet werden, stellt Hadoop keine optimale Lösung dar. Eine weitere, viel entscheidendere Einschränkung besteht, wenn es um die Echtzeitverfügbarkeit von Daten geht. Das Grundprinzip von Hadoop, Daten in den Festspeichern der Knotenpunkte im Cluster zu speichern ist Vor- und Nachteil zugleich. Einerseits wird Hadoop gerade dadurch wirtschaftlich, andererseits gerade dadurch träge, wenn es um Echtzeit-Analysen geht. Dazu müssen die Daten mühsam von den Festplatten in die Arbeitsspeicher geladen werden, was ein zeitraubender Prozess ist.Es könnte allerdings eine Frage der Zeit sein, bis die Unterscheidung zwischen Fest- und Arbeitspeicher selbst hinfällig wird. Spätestens dann kann die Datenrevolution beginnen.";https://bigdatablog.de/2015/04/29/big-data-und-hadoop-apache-macht-das-unmoegliche-moeglich/;BigDataBlog;Christian Schön
27. Apr 15;"     ""Smart Community: So hilft Big Data in Städten, Kommunen und Gemeinden""";Bei der Verwaltung, Organisation und Durchführung der Aufgaben von Städten, Kommunen und Gemeinden lohnt sich der Einsatz von Big Data. Sensoren und Datenanalyse helfen beim intelligenten Umgang mit Ressourcen.Angesichts von überschuldeten Haushalten bei gleichzeitig steigendem Bedarf an Investitionen ist es an der Zeit für innovative IT-Lösungen im Bereich der Politik. Big-Data-Technologien bieten für viele Aufgaben der Städte und Kommunen Modelle, die einen effizienteren Umgang mit Ressourcen ermöglichen. Dadurch werden nicht nur die öffentlichen Kassen entlastet. Profitieren würden vor allem die Menschen, ihr direktes Lebensumfeld sowie die Umwelt.Die Auswertung großer Datenmengen, die bei der Echtzeit-Überwachung durch Sensoren anfallen, – also Big Data – verschafft Einblicke in Gegebenheiten, die sonst im Verborgenen bleiben.Sensoren in Mülleimern überwachen den FüllstandSensoren, die in Mülleimern und Müllräumen platziert sind, überwachen permanent den Füllstand in den Tonnen. Diese dabei entstehenden Daten werden direkt an die Müllentsorgungsunternehmen übermittelt. Das bringt gleich mehrere Vorteile.Überflüssige Touren zu leeren Mülltonnen werden vermiedenMülleimer, die wiederum überquellen, können wiederum auch außerplanmäßig schnell geleert werdenOrte, an denen vermehrt Abfallaufkommen registriert wird, können zudem künftig mit mehr bzw. größeren Behältern ausgestattet werden“Was machen Sensoren im Mülleimer? Sie sind ein Teil einer kommunalen #BigData-Strategie.“ Twittern WhatsAppVon den Verbesserungen in der Logistik profitieren alle Beteiligten: Die Mitarbeiter können effektiver eingesetzt werden, weniger Benzin wird verbraucht und die Städte und Gemeinden werden zuverlässig von Unrat befreit.Intelligente Beleuchtung in der Stadt und auf den StraßenDie Folgen von “Lichtverschmutzung” werden nicht mehr nur von Sternenbeobachtern beklagt. Das künstliche Licht, das nachts unsere Städte und Straßen erhellt, beeinflusst auch das Ökosystem um sie herum.Die Tier- und Pflanzenwelt reagiert ebenso auf die andauernde Helligkeit wie empfindliche Menschen. Der natürliche Schlaf- und Wachrhythmus kann durch die vielen Lichtquellen empfindlich gestört werden. Darüber hinaus kostet eine permanente Beleuchtung unnötig Geld und Energie, solange keine Menschen unterwegs sind.Auch hier schaffen Sensoren Abhilfe: Sie erkennen, wenn an bestimmten Stellen Personen unterwegs sind, oder Straßen befahren werden. Ist gerade niemand unterwegs, wird die Straßenbeleuchtung automatisch gedimmt.Parkplätze, Verkehrs- und Wetterentwicklung – eine mobile App informiert BürgerDie Parkplatzsuche kann eine nervenaufreibende Sache sein. Besonders dann, wenn man sich zuvor auf dem Nachhauseweg durch den Feierabendverkehr gekämpft hat. Über den nächsten freien Parkplatz könnte eine App ebenso informieren, wie über Staus und die aktuellen Verspätungen im öffentlichen Nahverkehr.Neben in die Straßen integrierten Sensoren ist dazu eine intelligente Verkehrsüberwachung in Echtzeit notwendig. Eine zentrale Stelle verarbeitet die Daten und hält die Bürger über eine App informiert. Gleichzeitig können über die Schnittstelle Informationen zur Wetterentwicklung sowie Unwetterwarnungen mitgeteilt werden.Lese auch: Intelligente Lösungen gegen den VerkehrskollapsDamit sinkt das Unfallrisiko auf Straßen und präventive Maßnahmen gegen Schäden an Häusern werden ermöglicht.Big Data und TrinkwasserhygieneWasser ist die Grundlage des Lebens und Wasserhygiene bekommt in Ballungsräumen und großen Städten eine hohe Priorität. Verunreinigtes und mit Schadstoffen belastetes Wasser trifft dort besonders viele Menschen. Mehrere Faktoren bei der Planung, dem Bau und dem Betrieb von Wasserversorgungssystemen tragen zu einer dauerhaft hohen Wasserqualität bei.Steht Wasser für eine längere Zeit still oder erhöht sich die Temperatur in den Leitungen, steigt das Risiko der Bildung von Bakterien. Big-Data-Lösungen versprechen hier Erkenntnisse, auf die vormals nicht verfügbar waren.Veränderte Strömungsverhältnisse und Temperaturschwankungen in Leitungen können durch den Einsatz von Sensoren ständig geprüft werden. Problemzonen werden so überhaupt erst erkannt und Gegenmaßnahmen schneller und zielgenau ermöglicht. Die Überwachung von Abwasserkanälen erlaubt darüber hinaus die punktgenaue Identifikation von beschädigten oder verstopften Rohren.Auf dem Weg zur Smart CommunityNicht nur bei der Wasser-, auch Energieversorgung spielt Big Data eine Schlüssel-Rolle auf dem Weg zu einer smarten Stadt bzw. Gemeinde.Lese auch: Willkommen in Boston, der Echtzeit-Big-Data-StadtWenn Gemeinden oder deren Bewohner Strom aus Solarkraft, Wind oder Biogas erzeugen, stellt das nur eine Seite der Medaille dar. Dem produzierten Strom müssen Informationen über den aktuellen Verbrauch gegenüber stehen, um nachhaltige Energiewirtschaft zu betreiben. Die Vernetzung der Dinge liefert die entsprechenden Daten von den Verbrauchern wie von Erzeugern. Durch Datenerhebung und Datenauswertung werden wertvolle Ressourcen geschont.“Diese Vorteile erhalten Städte, Kommunen und Gemeinden beim Einsatz von #BigData“ Twittern WhatsAppDie vielen Sensoren, die in Städten und Gemeinden zum Einsatz kommen könnten, erzeugen also neben Big Data große Einsichten. Sie tragen damit zu mehr Sicherheit, mehr Sauberkeit und mehr Komfort bei. Gleichzeitig helfen sie, die Umwelt weniger zu verschmutzen, die Haushalte weniger zu belasten und die Zukunft besser zu planen.;https://bigdatablog.de/2015/04/27/smart-community-so-hilft-big-data-in-staedten-kommunen-und-gemeinden/;BigDataBlog;Christian Schön
24. Apr 15;"     ""Big Data in Unternehmen: Die 10 wichtigsten Szenarien""";Big Data ist der neue Produktionsfaktor für die Wirtschaft. Die Zwecke, die Big Data für Unternehmen haben können, sind vielfältiger denn je. Eine Übersicht über die 10 wichtigsten Einsatzszenarien.In der klassischen ökonomischen Lehre gibt es bislang drei Produktionsfaktoren: (1) Kapital bzw. Betriebsmittel (2) Rohstoffe bzw. Werkstoffe und (3) Arbeitskraft bzw. Ausführung am Objekt. Im Zuge der umfassenden Digitalisierung aller Lebensbereiche muss dieser Reihe inzwischen ein vierter Produktionsfaktor hinzugefügt werden: die Daten bzw. Wissen. Es lässt sich kaum eine Branche benennen, für die es keine datenbasierten Lösungen gibt. Sei es in Industrie und Fertigung, bei Banken und Versicherungen, im Handel und Dienstleistungssektor oder in der Informations- und Kommunikationstechnik. So vielfältig die Einsatzorte sind, so unterschiedlich sind die Zwecke, für die Daten analysiert werden. Es gibt Big-Data-Projekte zur datenbasierten Planung, zur Einsparung von Kosten, zur Umsatzsteigerung, bei der Produktentwicklung, zur Produktivitätssteigerung oder bei der frühzeitigen Betrugserkennung.In dieser Vielfalt liegt die große Chance für den Industriestandort Deutschland. Die Idee von der vernetzten Industrie 4.0 bzw. die Smart Factory liefern den Unternehmen die Daten, auf deren Grundlage sich Big-Data-Lösungen konzipieren und umsetzen lassen.Obwohl jede einzelne Lösung sehr individuell ausfallen mag, lassen sich doch 10 Oberkategorien für Big-Data-Szenarien ausmachen.1. Fraud-DetectionEine Kernanwendung im Bereich Big-Data-Analyse ist die Aufdeckung von Betrugsfällen, auch Fraud-Detection genannt. Große Datenmengen bieten einen entscheidenden Vorteil auf diesem Gebiet. Je mehr Daten vorhanden sind, desto wahrscheinlicher lassen sich Muster erkennen und davon abweichende Verhaltensweisen identifizieren.Im Fall von Kreditkartenbetrug geht es um enorme Summen, deren Verlust es zu verhindert geht. Dabei geht es nicht nur um das individuelle Bezahlverhalten und die davon abweichenden Geldbewegungen. Auch der Abgleich mit dem Verhalten von vergleichbaren Kreditkartennutzern und mit vorangegangenem Betrugsverhalten vereinfacht die Echtzeiterkennung.2. RisikoabschätzungBanken und Versicherungen machen Geschäfte mit der Zukunft. Sei es beim Abschluss von Kreditverträgen, bei der Spekulation mit Aktien und Wertpapieren oder beim Versprechen, bei einem künftigen Schadensfall zu helfen – all diesen Geschäften ist gemeinsam, dass sie in die Zukunft gerichtet sind. Da die Zukunft aber an sich hat, unsicher zu sein, gibt es nur zwei Möglichkeiten damit umzugehen.Eine Möglichkeit ist, auf die Zukunft zu wetten und zu erraten, was passieren wird. Eine andere Methode, die den Vorzug hat viel sicherer zu sein, ist, sich Wissen von der Zukunft zu beschaffen. Mit komplexen Berechnungen und Algorithmen werden Vorhersagemodelle erstellt, um Risikomanagement bzw. Risiko-Controlling zu betreiben.3. Management und GovernanceEine umfassende Auswertung von Geschäftsdaten hat automatisch einen ebenso umfassenden Überblick über alle betrieblichen Szenarien zur Folge. Mit prädikativen, also vorhersagenden Modellen, lassen sich daraus wahrscheinliche Entwicklungen für die Zukunft ableiten. Das ermöglicht eine datenbasierte Planung und schafft eine verlässliche Grundlage für Geschäftsentscheidungen.Darüber hinaus wird die Transparenz von Abläufen innerhalb von Unternehmen gesteigert, sowie Zusammenhänge und Abhängigkeiten aufgezeigt. All diese Einsichten dienen zur Absicherung von weitreichenden Entscheidungen und führen zur effizienteren Unternehmensführung.4. Marktforschung und AbsatzprognosenEine genaue Kenntnis der Märkte in einer globalen Welt wird immer schwieriger. Hier kehrt sich das Verhältnis sogar um: Je mehr Informationen über die Märkte vorhanden sind, desto diversifizierter lassen sich Kundenwünsche beschreiben, Marktlücken erkennen oder das Verhalten der Konkurrenten am Markt beobachten. Durch den Einsatz von Big Data lassen sich nicht genutzte Potenziale identifizieren und realisieren.Eine Besonderheit ist das Markt-Monitoring in Echtzeit. Dazu werden beispielsweise alle Nachrichten bei Twitter zu einer bestimmten Entwicklung auf dem Markt ausgewertet, um Absatzprognosen abzuleiten und Investitionen zu tätigen.5. Marketing und KundenorientierungDie Vorteile von Big-Data-Lösungen im Bereich Kundenorientierung: Mithilfe von großen Datenbanken lassen sich nicht nur Wahrscheinlichkeiten von Verhaltensweisen ablesen. Vielmehr können individuelle Verhaltensmuster simuliert werden, um Erfolg und Misserfolg von Kampagnen im Vorfeld abzuschätzen. So kann das Ergebnis einer Simulation von 100 Millionen persönlichen Profilen noch während der Produktion berücksichtigt werden.Diese Entwicklung hat zur Folge, das Marketing immer spezifischer und auch auf kleine Kundengruppen gezielt zugeschnitten wird.6. Individualisierte ProduktempfehlungDer Klassiker unter den Big-Data-Anwendungsszenarien ist die individualisierte Produktempfehlung. Ein großer Teil des Erfolges von Google und Amazon beruht auf diesem Prinzip. Die Empfehlung von Produkten, die zu den individuellen Vorlieben der Kunden passen, stellt die Rückkehr des Tante-Emma-Prinzips in der digitalen Welt dar.Die Königsdisziplin in dieser Kategorie stellt das frühzeitige Erkennen von abwandernden Kunden dar, um diese mit einem passenden Angebot weiter zu binden.Dieser Prozess wird zum Teil durch individuelle Preisanpassungen und Rabatte unterstützt.7. Produktentwicklung und -verbesserungDie Entwicklung innovativer Produkte und die Verbesserungen bestehender Angebote sind zwei zentrale unternehmerische Antriebsfedern. Betriebliche Forschung und Produktentwicklung profitieren erheblich vom Einsatz von Big Data.Die Daten aus der Nutzung bestehender Produkte liefern eine Grundlage für die Anpassung von Featuren oder bei der Neuentwicklung. Sensoren in den bereits im Einsatz befindlichen Geräten zeigen, welche Funktionen bevorzugt genutzt werden.Alternativ kann die Auswertung von Social-Media-Daten neue Trends und die Wünsche der Kunden aufzeigen. Die Analyse von Forenbeiträgen, Postings oder Tweets kann beispielsweise die am häufigsten auftretenden Nebenwirkungen bei einem Medikament ermitteln. Auf diese Weise lassen sich Produkte, in diesem Fall Medikamente, anpassen oder durch andere ersetzen.8. M2M – Machine-to-Machine-KommunikationBei der sogenannten Machine-to-Machine-Kommunikation erfassen Sensoren an Maschinen oder Produkten entlang von Produktions- und Lieferketten Daten. Diese Daten geben darüber Auskunft, wann sich ein Produkt oder Fertigungsteil an welcher Stelle im Prozess seiner Fertigung befindet.Diese Informationen können direkt von anderen Maschinen ausgelesen und weiterverarbeitet werden. Durch die Automatisierung dieser Kommunikation können Kosten reduziert und die Produktivität gesteigert werden.Die große Herausforderung in diesem Bereich ist die Harmonisierung aller Unternehmensbereiche, samt Partnerunternehmen und Zuliefererindustrie.9. Vorausschauende Wartung und LogistikLogistikunternehmen oder -abteilungen sind darauf angewiesen, dass ihre zur Verfügung stehende Flotte zum einen stets einsatzbereit und zum anderen ständig in Bewegung ist. Das bedeutet auch die Standzeiten, die Aufgrund von Wartung und Reparaturen zustande kommen soweit wie möglich zu reduzieren.Die Auswertung von Daten aus unterschiedlichsten Quellen erlaubt diese Zeit so gering wie möglich zu halten. GPS-Daten, Daten zum Spritverbrauch und Reifenabnutzung, Motordaten und Fahrer-Feedback via Social Media geben einen genauen Überblick über den Zustand und Standort der Flotte.10. Monitoring und SteuerungUm den eigenen Energieverbrauch zu senken oder um die Produktivität zu erhöhe, lassen sich die Daten aus der Maschinenüberwachung auswerten. Diese liefern einen Überblick über Auslastung sowie Engpässe und Zuverlässigkeit. Anpassungen und Reaktionen auf Ereignisse lassen sich automatisieren.“Die 10 wichtigsten Szenarien für den Einsatz von #BigData in #Unternehmen.“ Twittern WhatsAppDie Smart Factory ist mit programmierbaren, vernetzten Thermostaten ausgestattet, verfügt über automatische Fahrzeuge und verwendet lernende Algorithmen.;https://bigdatablog.de/2015/04/24/big-data-in-unternehmen-die-10-wichtigsten-szenarien/;BigDataBlog;Christian Schön
22. Apr 15;"     ""Datenvisualisierung bei Big Data: Wenn Daten Schönheit erlangen""";"Big Data bleibt manchmal ein abstraktes Thema. Dabei steckt dahinter wertvolles Wissen und wertvolle Informationen. Damit Daten ihr Wissen preisgeben, werden sie von Big-Data-Artists visualisiert.Die weltweit unvorstellbar rasant wachsende Datenmenge stellt Big-Data-Scientists vor zwei wesentliche Aufgaben. Einerseits stellt sich die Frage, wie diese großen Mengen an Daten am effektivsten und sichersten gespeichert werden. Schneller Zugriff, Sicherheit und Speicherkapazität bei gleichzeitiger ökonomischer Rentabilität stellt hier die Herausforderung dar.Fast noch interessanter ist jedoch die Frage, wie die Datenflut entsprechend dargestellt wird. Denn die Daten liegen zunächst in einer nicht anschaulichen Form vor. Daten sind Bits und Bytes in riesigen Mengen, in strukturierter und unstrukturierter Form, in Flashspeichern, auf Festplatten oder Bandlaufwerken an unterschiedlichen Orten gespeichert.Mit statistischen Methoden und mathematischen Algorithmen lassen sich Ergebnisse in Tabellenform oder Verteilungskurven darstellen. Konkrete Aussagen, verwertbare Zusammenhänge, überraschende Einsichten oder gar schöne Darstellungen – das sind die komplexen Aufgaben der Daten-Analysten und Big-Data-Artists.Die Daten richtig verstehen„Wissen ist Macht“ lautet das geflügelte Wort, das auf den englischen Philosophen Francis Bacon zurückgeht. Diese Aussage trifft auch auf das Wissen zu, das aus den Daten gewonnen wird. Allerdings entfaltet dieses Daten-Wissen erst dann das in ihm steckende Machtpotential, wenn es richtig verstanden wird. Dazu ist es nötig, die aus den Daten gewonnenen Informationen ebenso zu übersetzen, als lägen sie in einer fremden Sprache vor. Mit der Aufbereitung und Visualisierung von Daten beschäftigen sich Grafik-Designer und Data-Artists. Informationen lassen sich in Form von Grafiken oder Bildern nicht nur besonders schnell erfassen, sondern haben einen weiteren Vorteil.Informationen sind schönWenn Daten lesbar gemacht werden, kann das Ergebnis dieses Prozesses sogar schön sein. Auf seinem Blog „Information is beautiful“ befasst sich der Daten-Journalist und Informations-Designer David McCandless mit der Welt der Datenvisualisierung.“Schluss mit dem Chaos! McCandless verwandelt Daten in anschauliche Grafiken. @infobeautiful #BigData“ Twittern WhatsAppIn den vielen dort frei verfügbaren Animationen und Grafiken führt McCandless vor, wie ästhetisch Bits und Bytes aussehen können. Der erst kürzlich erschienene reich befüllte Bildband „Knowledge is Beautiful“ wurde schnell zum hochgelobten Bestseller.Interessante Erkenntnisse aus den unterschiedlichsten Bereichen des Lebens beweisen, wie hilfreich, vielseitig und vor allem wie schön Datenanalyse sein kann, zum Beispiel der Online-Verdienst von Musikern.DESIGN &amp; CONCEPT: DAVID MCCANDLESS RESEARCH: ELLA HOLLOWOOD, MIRIAM QUICK ADDITIONAL DESIGN: FABIO BERGAMASCHI DATA: HTTP://BIT.LY/IIB_MUSICIANS2015";https://bigdatablog.de/2015/04/22/datenvisualisierung-bei-big-data-wenn-daten-schoenheit-erlangen/;BigDataBlog;Christian Schön
15. Apr 15;"     ""Digitalisierung und Big Data: Wir brauchen eine starke Netzinfrastruktur""";Cloud­ Computing, Industrie 4.0, Big Data, Social Media: Das alles sind Schlagwörter im Kontext der Digitalisierung und die Auflistung ließe sich noch eine ganze Weile fortsetzen. Die technologische Entwicklung hat innerhalb weniger Jahre die geschäftlichen Abläufe auf den Kopf gestellt. Und auch im Privaten sind die neuen Möglichkeiten für Jedermann spürbar.Betroffen sind nahezu alle Bereiche unseres täglichen Lebens. Die Rasanz der Digitalisierung betrifft alle denkbaren Stufen in Fertigung, bei der Produktgestaltung, dem Vertrieb und Konsum. Innovative Dienste zeigen die Vorteile einer digitalisierten Welt auf. Sobald Google das Bild eines autonom fahrenden Fahrzeugs veröffentlicht fühlt sich Science Fiction aus der Vergangenheit greifbar real an und die Euphorie Vieler kennt kaum Grenzen. Das Wirtschaftswunder 4.0 wird proklamiert und immer mehr Projekte werden publik, in denen die Datenanalyse eine zentrale Rolle spielt.Digitalisierung braucht eine starke NetzinfrastrukturEine unumstößliche Erkenntnis: Für die Nutzung all dieser Errungenschaften und kommenden Möglichkeiten der Digitalisierung ist der Ausbau einer leistungsfähigen Netzinfrastruktur die Basis. Viele wollen einen zwingenden Zusammenhang zwischen Digitalisierung und Breitbandausbau nicht wahrhaben. Denn während bei der Digitalisierung die Phantasie bei den Geschäftsmodellen und Anwendungen ins Unermessliche reicht, versagen beim Breitbandausbau bislang die Bemühungen. Tragfähige Konzepte für die Errichtung einer zukunftsfähigen Netzinfrastruktur liegen nicht vor.Langfristiges Ziel beim Breitbandausbau muss der Aufbau eines glasfaserbasierten FTTH­-Netzes sein, das direkt bis zu jedem Nutzer reicht. Mobilfunk wird eine tragende Rolle bei Big Data Anwendungen und der Digitalisierung insgesamt spielen. Und er ist insbesondere auf eine leistungsfähige Glasfaser­Infrastruktur angewiesen. Die Unmengen an Daten müssen von den Mobilfunkstandorten immer an irgendeinem Punkt in das Festnetz abgeführt werden. An der Glasfaser führt also trotz aller Kommunikationswege – ob drahtlos oder über die Luftschnittstelle ­- kein Weg vorbei. Die Experten streiten sich jedoch über den richtigen Weg hierhin.Wettbewerb bringt flächendeckendes BreitbandFlächendeckende Breitbandnetze kann man am schnellsten durch möglichst wettbewerbliche Strukturen erreichen. Wettbewerb zwingt die Marktbeteiligten zu einem Maximum an Innovation und Investition. Insbesondere die ehemaligen Staatsunternehmen und traditionellen Platzhirsche der Branche wurden erst durch Wettbewerb im Markt aus der Reserve gelockt. Nach etlichen Smartphone-­Generationen können sich die Digital Natives schon nicht mehr an das gute alte Wählscheiben­-Telefon vergangener Jahrzehnte erinnern. Die bisherigen Erfolge in 17 Jahren Liberalisierung des Telekommunikationsmarktes beweisen, dass Innovationen und Investitionen ihr Maximum im Wettbewerb erreichen. Trotzdem suchen alle händeringend nach einem Patentrezept zur weiteren Stimulierung des Breitbandausbaus.Immer wieder wird über die Verpflichtung der Unternehmen zum Breitbandbandausbau nachgedacht. Die Befürworter eines Breitband­-Universaldienstes machen es sich jedoch zu einfach. Wer sich des Wettbewerbsmodells entledigt, wird in die Ära der alten Monopole nach Vorbild der Bundespost zurückfallen. Gleichzeitig würde wie in den USA ein Milliardenmoloch geschaffen. Dort wird eine Sondersteuer von mehr auf 15 Prozent (!) auf die Umsätze der Netzbetreiber erhoben, die von den Unternehmen direkt an die Verbraucher weitergereicht wird. Die US­-Probleme beim Breitbandausbau sind trotzdem größer als in Deutschland.Wo bleibt die Nachfrage nach ultraschnellem Breitband?Eine weitere Hürde ist die bislang noch geringe Zahlungsbereitschaft der Kunden für sehr hohe Bandbreiten. Die „Take Up­ Rate“ ist in der Realität auch bei verfügbaren hohen Bandbreiten deutlich zu gering. Einem hohen Investitionsbedarf der Netzbetreiber stehen meist nur geringfügig höhere Einnahmemöglichkeiten gegenüber.“Digitalisierung braucht starke Netzinfrastrukturen: Wo bleibt die Nachfrage nach ultraschnellem Breitband?“ Twittern WhatsAppDen meisten Nutzern wird der Mehrwert höherer Bandbreiten nicht hinreichend deutlich. Dies gilt für Deutschland, Europa, die USA aber selbst für Japan, wo in seit Jahren mit FTTB/H ausgebauten Gebieten die Nutzung trotzdem bei unter 60 Prozent liegt. Fragen der Sicherheit, Netzneutralität, verbesserte Medienangebote, digitale Verwaltung, Tele­Medizin sind hier die Schlüsselthemen, auf deren Entwicklung die die Politik das Augenmerk richten muss. Die Angebote schaffen sich ihre Nachfrage. Der Staat kann hier nach dem Vorbild der skandinavischen und baltischen Staaten Anreize setzen. Halbherzige Versuche wie der elektronische Personalausweis oder die Gesundheitskarte sind das Lehrgeld für zukünftige Projekte.Der Regulierungsrahmen muss stimmenEin wesentlicher Grund für zu geringe Investitionen in den Ausbau liegt weiterhin in der nationalen, aber auch europäischen Regulierungspolitik. Natürliche Monopole wie Netzinfrastrukturen unterliegen einer Regulierung durch staatliche Stellen, um sie dem Wettbewerb zu öffnen. Die ständige – in kurzen Jahreszyklen erfolgende – Infragestellung des bestehenden Regulierungsrahmens führt zu einer Schwächung der Wettbewerbsinvestitionen in den allermeisten Ländern. Regulierungsferien – wie jüngst von einer Expertenkommission des Bundeswirtschaftsministeriums völlig unverständlich vorgeschlagen – sind ein Totschlagargument für Investitionssteigerungen. Exklusivrechte beim Infrastrukturausbau – wie von der Telekom für Vectoring beantragt – hinterlassen Kollateralschäden bei möglichen Finanziers der neuen Netze. Dabei sind es die auf eine stabile Zugangsregulierung angewiesenen Wettbewerberunternehmen, die vornehmlich in den FTTB/H-­Ausbau investieren.Obama und die amerikanische Regulierungsbehörde FCC haben das schwache Wettbewerbsniveau in den USA als Fehlentwicklung erkannt. Ihre Lösung lautet jedoch konsequenterweise nicht weiterer Regulierungsabbau, sondern vielmehr der Ausbau der Wettbewerbssituation. Nicht steigende Preise für die Kunden, sondern höhere Einnahmen der Unternehmen für attraktive neue Dienste müssen der europäische Weg sein. Nicht die Stärkung von Oligopolen, sondern Innovationskraft und das Nebeneinander kleinerer mittelständischer Unternehmen mit weltweiten Playern sichern den nachhaltigen wirtschaftlichen Erfolg und einen Technologieausbau, der weitgehend ohne milliardenschwere Steuerbelastungen der Bürger auskommt. Die eigentlichen Treiber sind Modelle zur Kostensenkung, zur verbesserten Finanzierung, zu steuerlichen Anreizen, zu transparenten Ausschreibungs­- und Förderverfahren.;https://bigdatablog.de/2015/04/15/digitalisierung-und-big-data-wir-brauchen-eine-starke-netzinfrastruktur/;BigDataBlog;Dr. Frederic Ufer
13. Apr 15;" Retweets,      ""Big Data Risiken: Worauf bei der Datenauswertung geachtet werden muss""";Die Auswertung von großen Datenmengen birgt neue, ganz eigene Schwierigkeiten. Damit Big Data erfolgreich angewendet werden kann, müssen diese neuen Risiken verstanden und im Blick behalten werden.Datenmengen in der Größenordnung von mehreren Terabytes (1.024 GB) oder Petabytes (1.048.576 GB) auszuwerten bringt ganz eigene Risiken mit sich. Sobald das Wort „Risiko“ im Zusammenhang mit Big Data fällt, taucht normalerweise zunächst die Datenschutzproblematik auf. Doch wenn hier von Risiken die Rede ist, sind darüber hinaus risikobehaftete Bereiche gemeint, die speziell beim Umgang mit großen Datenmengen und bei der Interpretation von diesen Daten auftauchen.Bereits bei der Planung von Big-Data-Projekten dürfen bestimmte Aspekte nicht außer Acht gelassen werden. Ein tief greifendes Verständnis dieser Bereiche führt zu einer realistischen Perspektive und räumt Fehlerquellen aus. Die neuen Risiken, die es bei der Anwendung von Big Data gibt, sprechen also nicht prinzipiell gegen die Nutzung von Data Intelligence.“Big Data Risiken: Was bei der Auswertung großer Datenmengen beachtet werden muss.“ Twittern WhatsAppVielmehr handelt es sich um Faktoren, die im Vorfeld beachtet werden müssen, um zu entscheiden, in welchen Fällen Big-Data-Lösungen sinnvoll sind. Werden etwa weitreichende Geschäftsentscheidungen mit Datenanalysen gestützt, ist bei der Beurteilung der Auswertungen wichtig, diese Risiken genau zu kennen. Fehlentscheidungen und die Gefahr von falscher Interpretation der Daten werden so minimiert. Rentabilität und Machbarkeit hängen von den Risiken auf mehreren Ebenen ab: auf juristischer, technischer, kommunikativer und auf analytischer Ebene.Kein Big-Data-Projekt ohne juristische BegutachtungIn juristischer Hinsicht erfordern Big-Data-Projekte eine umfassende Begutachtung. Schon bei der Planung eines Projektes sind die rechtlichen Rahmenbedingungen zu beachten. Erhebung, Speicherung und spätere Weiterverarbeitung von Daten unterliegen zum Teil strengen Regelungen. Beispielsweise spielt bei der Erhebung der Daten der geografische Standort der Datenquellen eine zentrale Rolle. Er entscheidet darüber, welche Datenschutzverordnung eingehalten werden muss. Bei Projekten von internationalem Ausmaß stellt das Zusammenspiel verschiedener Rechtsräume eine komplexe Herausforderung dar.Risiko Betriebsspionage – Sensible Daten benötigen besonderen SchutzDer Umgang mit großen Datenmengen, insbesondere bei Daten über betriebsinterne Zusammenhänge, stellt Unternehmen bzw. die IT-Abteilungen vor technische Herausforderungen. Die Balance zwischen Zugänglichkeit und Sicherheit muss genau austariert sein. Nicht jedes Unternehmen verfügt über die technischen Infrastrukturen, um diese Mengen zu speichern und zu verarbeiten. Cloud- und On-Demand-Lösungen bringen automatisch das Risiko des Datenverlusts oder Datendiebstahls mit sich. Sensible Daten müssen deshalb besonderen Schutz genießen, um digitale Betriebsspionage auszuschließen. Die Platzierung von gezielt gefälschten Datensätzen schafft ebenso Sicherheit wie Verschlüsselungstechniken. Datenverlust stellt datengetriebene Unternehmer vor ein finanzielles Problem: Sicherungskopien von Daten im Petabyte-Bereich sind kostspielig.Big Data als Gefahr bei der KundenwahrnehmungDie Nutzung von Big Data für Marktforschung, zur Unterstützung von Entscheidungen oder in der Forschung und Entwicklung ist ein vergleichsweise junges Feld. Entscheidet sich ein Unternehmen für Big-Data-Projekte, stellt sich die Frage, ob und wie dies nach außen kommuniziert wird. Die Reputation eines Unternehmens hängt auch von der Wahrnehmung der Kunden und Konkurrenten ab. Je nach Art der Analyse stammen Daten einerseits aus zum Teil sensiblen Quellen und anderseits ist Big Data als solches gesellschaftlich noch nicht vollständig akzeptiert. Aufgrund dieser Gründe besteht ein kommunikatives Risiko. Präsentiert sich ein Unternehmen als Big-Data-freundlich, könnte es auf der einen Seite als innovativ und fortschrittlich wahrgenommen werden, auf der anderen Seite aber auch in Misskredit geraten.Nur vollständige Daten gewährleisten richtige VorhersagenBei der eigentlichen Analyse der Daten kommen die Daten selbst ins Visier. Eine Risikoquelle ist die Qualität der Daten. Nicht alle Daten sind geeignet, die Realität in dem Maße abzubilden, wie dies die Fragestellung erfordert. Gute Entscheidungen und richtige Vorhersagen gelingen nur, wenn die Daten vollständig sind und ihre Qualität gesichert ist. Die Faktoren, die zu einer mangelnden Datenqualität führen, sind vielfältig:fehlerhafte Datensätzemehrfach abgespeicherte Datenkopienmissverständliche, falsche oder mehrdeutige Interpretationmangelnde Kontrolle oder fehlende Verantwortlichkeitungenaue oder fehlleitende FragestellungenAll diese Faktoren bergen das Risiko, dass aus den Daten ein Modell gebildet wird, das die Wirklichkeit nicht oder nur unzureichend beschreibt. Zur Sicherung der Datenqualität, und damit zur Sicherung der Aussagen, ist eine permanente Überprüfung in zweierlei Hinsicht nötig: Daten haben sowohl hinsichtlich ihrer Bedeutung (Semantik) als auch hinsichtlich ihrer Menge (Statistik) Aussagekraft.“ Um #BigData erfolgreich anzuwenden, müssen diese neuen Risiken richtig verstanden werden! #Daten“ Twittern WhatsAppSo komplex und vielfältig die Risiken von Big-Data-Projekten sind, so einzigartig und passgenau müssen die Lösungen ausfallen. Um Big Data erfolgreich in ein Unternehmen zu integrieren, ist das Zusammenspiel vieler Abteilungen bzw. Kompetenzen nötig. Gerade weil Big-Data-Strategien deshalb ressourcenintensiv werden können, lohnt es sich, die Risiken genau im Blick zu behalten. Nur wenn aus Daten abgeleitete Aussagen in der Wirklichkeit zutreffen, trägt Big Data zu Wertsteigerung bei.;https://bigdatablog.de/2015/04/13/big-data-risiken-worauf-bei-der-datenauswertung-geachtet-werden-muss/;BigDataBlog;Christian Schön
27. Feb 15;" Retweets,      ""Big Data und der rechtliche Schutz der Person""";Big Data – das ist viel mehr als nur eine große Menge Daten. Parallel zur Datenmenge steigt der Grad an Komplexität, was insbesondere rechtliche Aspekte von Daten und den Umgang damit betrifft. Viele Fragen sind noch unbeantwortet und müssen deswegen diskutiert werden.Allein die Frage, welche Gesetze zur Klärung herangezogen werden müssen, ist nicht abschließend geklärt. Obwohl es sich bei Big Data nicht um ein Bild oder einen Text im traditionellen Verständnis handelt, kann das Urheberrecht greifen. Denn wenn eine Datensammlung in ihrer Struktur so sehr von einem Urheber geprägt ist, dass sie als originäre Schöpfung gelten kann, greift das Urheberrecht. Es kann auch vorkommen, dass umfangreichere Texte in Big Data enthalten sind, die wiederum durch das Urheberrecht geschützt sind. Auch dann dürfen die Daten nur mit Zustimmung der Urheber weiterverarbeitet werden.Daneben sind das Telemediengesetz und im Fall von Eigentumsfragen das allgemeine Zivilrecht (BGB) relevant. Allen anderen voran wird jedoch das Bundesdatenschutzgesetz und die Datenschutzgesetze der einzelnen Ländern sowie die Europäische Datenschutzverordnung herangezogen, um den rechtlichen Rahmen für den Umgang mit Big Data zu bestimmen. Besonders im Fokus des Datenschutzes stehen die sogenannten “personenbezogenen Daten” und damit der rechtliche Schutz von Personen. Ohne Rechtssicherheit und Datenschutz wird in Zukunft kein Unternehmen die Wettbewerbsvorteile von Big Data nutzen können.Grund genug sich genauer vor Augen zu führen, welche Daten dem Datenschutz unterliegen und welche Schwierigkeiten es im Zusammenhang mit Big Data gibt. In einem ersten Schritt muss geklärt werden, was genau personenbezogene Daten sind. In einem zweiten Schritt kann man dann fragen, inwieweit es sich bei Big Data überhaupt um personenbezogene Daten handelt. Und in einem dritten Schritt lässt sich abschließend klären, wie die datenschutzrechtliche Seite von Big Data zu bewerten ist. Beim rechtlichen Schutz der personenbezogene Daten ist es zunächst ganz gleich, ob sie aus der analogen oder der digitalen Welt kommen.Daten aus der analogen WeltNehmen wir mal eine Stadt wie Köln, in der ich lebe. Wie viele Daten wären nötig, um mich als Individuum, rechtlich gesprochen als “natürliche Person” zu identifizieren? In Köln leben etwa 1 Millionen Menschen. Ich bin männlich wie circa 500.000 andere in dieser Stadt. Ich bin 39 Jahre alt, also in der Gruppe der 35-50jährigen – bleiben noch 120.000. Ich wohne und arbeite in der Innenstadt – es bleiben noch etwa 10.000 auf die all das auch zutrifft. Ich bin hier in Deutschland geboren, habe ein Konto bei der Sparkasse, bin nicht vorbestraft und fahre kein eigenes Auto. Bleiben vielleicht noch 3.000-5.000. Ich bin Firmengründer, schreibe Bücher, habe an einer Hochschule unterrichtet und trete regelmäßig als Keynote-Speaker öffentlich auf.Mit diesen Angaben bin ich sehr wahrscheinlich schon ziemlich eindeutig identifizierbar. Diese Daten über mich sind weder geheim noch schwer zu bekommen und dazu sogar recht grob gerastert. Es sind keine detaillierten Angaben wie mein genaues Geburtsdatum, meine Wohn- oder Arbeitsadresse, meine Kontonummer oder gar mein Name.Die deutschen und europäischen Gesetzestexte sind bei diesem Fall nicht eindeutig, ob es sich bereits um personenbezogene Daten handelt, die besonderem Schutz unterliegen. Denn eine einzelne Angabe für sich genommen würde zur erfolgreichen Identifizierung meiner Person nicht genügen. Erst das Zusammenspiel aller Daten können zu mir als Person führen.Daten aus der digitalen WeltIn der digitalen Welt sieht das einerseits ganz ähnlich aus und gleichzeitig ist alles ganz anders. Ich kaufe online ein, höre online Musik, schaue Videos, lese Blogs, google.Allein die Chronik meines Browsers stellt einen ziemlich einzigartigen, digitalen Fingerabdruck von mir dar. Über meine IP-Adresse ist es wiederum möglich, mich mithilfe meines Internetproviders eindeutig zu identifizieren. Deswegen zählt diese seit 2009 zu den personenbezogenen Daten – der Browser-Fingerprint hingegen nach wie vor nicht. Über meine IP-Adresse können jedoch auch andere Personen ins Netz, so dass diese Information nicht mit 100%iger Sicherheit zu mir führt.“Unsere digitalen Spuren sind vieldeutiger und gleichzeitig persönlicher als die analogen Angaben.“ Twittern WhatsAppIn der digitalen Welt sind die Spuren, die wir hinterlassen, in der Regel vieldeutiger und gleichzeitig persönlicher als die vergleichbaren analogen Angaben wie “männlich, 39 Jahre alt, Keynote-Speaker…”. Das macht die juristische Lage so schwierig. Die scheinbar eindeutige IP-Adresse führt nur mithilfe des Providers zu mir als natürliche Person. Aber an dieser Stelle verhält es sich ähnlich wie bei Autokennzeichen. Erst eine Behörde kann den registrierten Fahrzeughalter eindeutig identifizieren. Das heißt aber noch lange nicht, dass dieser mit dem auf ihn registrierten Auto fährt. Welche „natürliche Person“ gerade surft, lässt sich über die IP-Adresse nicht eindeutig bestimmen.Sind Big Data personenbezogene Daten?Was hat all das nun mit Big Data zu tun: durch Big Data können die unterschiedlichsten Informationen über einzelne, natürliche Personen erlangt werden. Jede Information für sich genommen, führt nicht unbedingt zu bestimmten Individuen, wie beispielsweise das Attribut “männlich” mich noch nicht unter 500.000 Kölnern identifiziert.Verknüpft eine Analyse eine große Menge Daten miteinander oder wird ein Datensatz mit Daten aus anderen Quellen angereichert, kann es aber durchaus passieren, dass genügend Informationen zusammenkommen, um einzelne Personen genau zu herauszufiltern. Kommen zum Beispiel Suchanfragen bei einer Auswertung hinzu – wer googelt sich hin und wieder nicht selbst? – wird die Sache meist sehr einfach.Auch Daten aus Nutzungs- und Reaktionsdaten oder “soziodemografische Daten” (Alter, Geschlecht, Wohnort, Familienstand etc.) sind sehr eindeutig. Bis hierher wäre die Sache schon schwierig genug. Doch das Ganze ist noch weit komplexer: Big Data beinhaltet nicht zwangsläufig Daten über Personen. Big Data sind Daten zu allem möglichen: Wetterdaten, Aktienkurse, Daten, die beim Betrieb einer Maschine in der Produktion entstehen, Daten zu Lagerbeständen, Verkehrsdaten, und so weiter. All diese Daten können in einem Daten-Pool gleichberechtigt neben Personendaten stehen, auch wenn letztgenannte bei einer Auswertung gar keine Rolle spielen.Analysen können auch vollständig ohne Personendaten auskommen oder im Ergebnis nichts mit natürlichen Personen zu tun haben – Big Data und Datenschutz haben also nicht per Definition miteinander zu tun, weil zu einem großen Teil Daten nicht personenbezogen sind. Verschiedene Datentypen erfordern aus rechtlicher Perspektive eine unterschiedliche und differenzierte Betrachtungsweise.Anonymisierung von DatenDie Frage, ob es sich bei Big Data um personenbezoge Daten handelt, lässt sich also nicht generell beantworten. Wie ich oben gezeigt habe, kann auch die Verkettung von anonymen Daten einen Personenbezug herstellen – in der analogen wie in der digitalen Welt.Die Datenschutzbehörden sind in diesem Punkt sehr restriktiv und vertreten die strenge Auffassung, dass auch Daten personenbezogen sind, die allein nicht genügen, um eine Person zu identifizieren. Die liberale Gegenposition dazu würde sogar die IP-Adresse, die an sich bereits eine anonyme Folge von Ziffern ist, nicht als personenbezogenes Datum werten. Selbst wenn die Instanzen, die Daten erheben, auswerten und darstellen, die Daten anonymisieren, befinden sie sich in der paradoxen Situation, dass sie im Grunde dem Interesse aller Seiten gerecht werden und trotzdem den Standards der Datenschutzgesetzen nicht genügen würden.Zur Zeitgemäßheit der DatenschutzgesetzeFür Big Data gibt es (noch) keine eigenen Gesetze. So lange es keine gesonderten Regelungen gibt, muss bei den Fällen, in denen personenbezogene Daten in Big Data vorkommen, auf das bestehende Gesetz zurückgegriffen werden.Die Datenschutzgesetze in ihrer jetzigen Form und Big Data stehen aber bis dahin in einem natürlichen Spannungsverhältnis. Das Datenschutzgesetz stammt aus einer völlig anderen Zeit – es wurde in den 70er und 80er Jahren konzipiert – und ist schon allein deswegen nur bedingt auf die technologischen Neuerungen anwendbar. Das Gesetz geht davon aus, dass nur so wenige Daten wie möglich erhoben und gespeichert werden.“Sind #Datenschutz und #BigData vereinbar?“ Twittern WhatsAppBig Data macht das Gegenteil davon. Die Anwendungen leben davon, auf möglichst große Datenmengen zugreifen zu können. Aber, Daten bedeutet dabei nicht automatisch, dass sie personenbezogen sind. Das zu betonen ist wichtig und macht nochmals deutlich, warum die Datenschutzgesetze bei Big Data nicht das alleinige Maß der Dinge sein können. Vielmehr müssen die Anwendungen solange an einer für alle beteiligten Interessen gerecht werdenden Lösung arbeiten, bis die Gesetzgebung den Anforderungen des digitalen Zeitalters gerecht wird.;https://bigdatablog.de/2015/02/27/big-data-und-der-rechtliche-schutz-der-person/;BigDataBlog;Ibrahim Evsan
19. Feb 15;" Retweets,      ""Big Data: Ist Deutschland international wettbewerbsfähig?""";Big Data beschäftigt ganze Länder auf unterschiedlichste Art und Weise. Da ist die Frage berechtigt: Ist Deutschland wettbewerbsfähig im Rausch nach dem Datengold?In der Informations- und Kommunikationstechnologie (IKT) ist von Trends die Rede. Trends sind Indikatoren für die Zukunft, die wir nicht verpassen dürfen. Wir müssen uns mit ihnen beschäftigen, um im nicht abreißenden Strom der Innovationen und Veränderungen vorne dabei zu bleiben.Wer sich nicht an vorderster Front mit den Entwicklungen der IKT auseinandersetzt, wird „Big Data“ in den letzten Jahren als einen dieser Trends wahrgenommen haben. Vielleicht als Motto der Cebit, in den unzählbaren Überschriften der Newsletter und Online-News oder als Schlagwort im Wirtschaftsteil der großen Gazetten.Trends bergen das Risiko, dass sie nur leichtfertig wahrgenommen werden – unterschwellig immer auf der Suche nach dem nächsten, dem neuesten Trend. Und spätestens an diesem Punkt wird die Bedeutung von Big Data und allgemein der digitalen Transformation unser Welt häufig unterschätzt und falsch eingeschätzt.Diese Beispiele verdeutlichen die Bedeutung von Big Data für UnternehmenDie Bedeutung der Datenerfassung für Unternehmen, für die Wirtschaft generell, aber auch für die Gesellschaft und unsere gesamte Kultur ist enorm hoch. Sie wurde in dem Moment begreifbar, in dem die Technologie deren massenhafte Speicherung ermöglichte. Belege hierfür gibt es unzählbar viele, hier nur wenige Beispiele:der alltägliche Umgang mit Bonussystemen im Einzelhandel (wie Payback seit dem Jahr 2000),den Mautsystemen auf deutschen Autobahnen (Toll Collect seit dem Jahr 2002)und nicht zuletzt das Bedürfnis eines Großteils der Online-User, sich im Internet zu exponieren (etwa Myspace seit dem Jahr 2003 und natürlich Facebook seit 2004),die elektronische Gesundheitskarte, der elektronische Personalausweis und die elektronische Steuererklärung,Cookies im Browser,Fitness-Apps mit 24/7 Datentracking,Scoring zur Bonitätsprüfung,die Verschiebung physischer Datenspeicher in die Cloud,Industrie 4.0 und das Internet der Dingeund die Diskussion um die Vorratsdatenspeicherung als wirksames Mittel zur Terrorbekämpfung.Daten sind tatsächlich zum Gold des digitalen Zeitalters geworden und ihre systematische Nutzung die Grundlage unternehmerischer sowie staatlicher Kreativität seit bereits vielen Jahren. Von einem bloßen Trend ist bei „Big Data“ längst keine Rede mehr.Warum Big Data im Konflikt mit dem Datenschutz stehtDer Umgang mit unseren Daten und ihre exzessive Nutzung ist vielmehr eine der maßgeblichen Weichenstellungen des digitalen Zeitalters. Diese ist zwar technologisch begründet, in ihren Auswirkungen aber bis hinein in unser gesellschaftliches Miteinander zu spüren. Und mit dieser Erkenntnis muss zwingend gefragt werden, ob wir auf diese Entwicklung ausreichend vorbereitet sind.Bei der Nutzung neuer Technologien ist nicht nur zu beachten, was diese alles „kann“, sondern ebenfalls, was man überhaupt „darf“. Dazu gibt es in Deutschland eine lange Tradition.Bereits im Jahr 1983 hat das Bundesverfassungsgericht das aus dem allgemeinen Persönlichkeitsrecht in Art. 2 Abs. 1 i.V.m. Art. 1 Abs. 1 des Grundgesetzes abgeleitete Recht auf informationelle Selbstbestimmung entwickelt (sog. „Volkszählungsurteil“). Dieses ist wiederum einfachgesetzlich durch datenschutzrechtliche Bestimmungen in den jeweiligen Fachgesetzen und insbesondere dem Bundesdatenschutzgesetz (BDSG) gewährleistet. Letzteres ist geprägt durch das sogenannte Verbotsprinzip mit Erlaubnisvorbehalt.Die Erhebung, Speicherung, Verarbeitung und Weitergabe personenbezogener Daten ist danach grundsätzlich verboten. Personenbezogene Daten gewähren „Einzelangaben über persönliche oder sachliche Verhältnisse einer bestimmten oder bestimmbaren natürlichen Person“ (vgl. § 3 Abs. 1 BDSG). Dem gesamten Regelungsregime zum Datenschutz liegt das Konzept der Datensparsamkeit und Datenvermeidung zugrunde. Dieses sieht ebenfalls vor, dass keine bzw. möglichst wenig personenbezogene Daten verarbeitet werden. Big Data und die massenhafte elektronische Datenverarbeitung sind nicht per se auf die Verwendung personenbezogener Daten angewiesen.Die Konflikte, die insbesondere bei der Kombination und Analyse von Daten aus unterschiedlichen Quellen entstehen, liegen jedoch auf der Hand. Zahlreiche rechtliche Fragestellungen sind unbeantwortet oder ergeben sich erst nachdem die Anwendungsszenarien von Big Data klarer werden.Big Data: Datenschutz in den USA und Europa ist grundsätzlich verschiedenDer Datenschutz in den USA ist hingegen kaum rechtlich durch Gesetze oder vergleichbare Vorschriften geregelt. Die beiden globalen Schwergewichte Europa und USA verfolgen jeweils andere Strategien beim Thema Datenschutz.In Europa und Deutschland regeln allgemeine Grundsätze und gleich mehrere Gesetze den Datenschutz übergreifend und für alle gleichermaßen (auch hier gibt es weitere speziellere Gesetze). In den USA gibt es hingegen lediglich einen sektoralen Schutz, der bestimmte Gruppen anspricht. Dazu zählen beispielsweise das Gesundheits- oder Finanzwesen (sog. GLB- und HIPA-Acts).Allgemein lässt sich sagen, dass in den USA kein dem europäischen oder deutschen Datenschutz vergleichbares Niveau besteht. Dies erklärt sich aus einer völlig anderen Kultur im Umgang mit Daten und Informationen.In Amerika wird das Datenschutzbemühen deutscher Behörden viel eher als Bevormundung und Einschränkung der individuellen Rechte aufgefasst. Datenschutz ist in den USA eher ein Verbraucherschutzthema und aus diesem Blickwinkel betrachtet ein besonders relevantes. Bei Verstößen gegen Datenschutzvorschriften in den USA werden absurd hohe Strafsummen und drakonische Strafmaßnahmen gegen Unternehmen verhängt. Datenschutz wird jenseits des Atlantiks ernst genommen. Das hierzulande stark ausgeprägte Misstrauen gegenüber dem Staat im Hinblick auf die Datensammlung steht in den USA einem deutlich aufgeschlosseneren Verständnis für obrigkeitliche Überwachung gegenüber.Die lange Tradition des Datenschutzes in Deutschland hat diesen gesellschaftlich hierzulande fest verankert. In den USA entsteht – insbesondere aufgrund der Snowden-Enthüllungen – jetzt erst ein feststellbares Bewusstsein.Big Data: Europa und Deutschland können sich an die Spitze setzenAus dem Gesagten ergeben sich für US-amerikanische Unternehmen auf den ersten Blick klare Vorteile. Diese betreffen Entwicklung und Anwendung von Big Data Applikationen. Die Restriktionen in Übersee sind deutlich geringer, gleichzeitig wird online ein globaler Markt adressiert. Dieser Wettbewerbsnachteil ist den Europäern deutlich bewusst.„Die Sammlung von und die Kontrolle über enorme Mengen persönlicher Daten sind eine Quelle von Marktmacht für die größten Unternehmen im globalen Markt für Dienstleistungen im Internet“, stellte der damalige Europäische Datenschutzbeauftragte (EDSB) Peter Hustinx am 26. März 2014 fest.Sein Nachfolger Giovanni Buttarelli hat Anfang Januar 2015 eine Strategie zum Umgang mit großen Datenmengen vorgestellt:Bereits im Jahr 2012 hat die EU-Kommission die Umsetzung einer Cloud-Computing Strategie forciert, u.a. zur Bewältigung der globalen datenschutzrechtlichen Herausforderungen. In Brüssel wird eine umfassende Reform des EU-Rechtsrahmens für den Schutz personenbezogener Daten diskutiert. Durch sie sollen individuelle Rechte gestärkt und die mit der Globalisierung und neuen Technologien verbundenen Herausforderungen besser bewältigt werden.Der Unterschied macht’s: Was ist in Europa vorzufinden?In Europa ist man nicht allein darauf bedacht, den vermeintlichen Wettbewerbsnachteil zu den USA mit Abbau von Vorschriften und Regularien auszugleichen. Vielmehr sind hier die Aktivitäten darauf ausgerichtet, die Interessen der Industrie und die der Verbraucher in Einklang zu bringen. Das ist gut so! Aus der bei uns vorhandenen datenschutzrechtlichen Sensibilität folgt ein erheblicher Startvorteil beim Thema Big Data. Denn wenn Technik sich in gesellschaftliche Grundstrukturen drängt – und das macht Big Data zweifelsfrei – muss dazu ein Konsens bestehen. Darüber, wie die Folgen dieser technologischen Entwicklung in unsere Werte und Vorstellungen passen.Fazit: Ist Deutschland oder die USA besser aufgestellt?“Ist Deutschland wettbewerbsfähig im Rausch nach dem Datengold? #BigData“ Twittern WhatsAppDie Herausbildung einer Datenethik kann nur erfolgen, wenn ein kritischer Diskussionsprozess die Verträglichkeit mit unseren Werten, Grundsätzen und Vorstellungen sicherstellt. Big Data wird nicht erfolgreich sein, wenn die erforderlichen Rahmenbedingungen nicht geschaffen werden. Und dafür sind Europa und Deutschland deutlich besser aufgestellt, als die vermeintlichen Technologie-Führer USA.;https://bigdatablog.de/2015/02/19/big-data-ist-deutschland-international-wettbewerbsfaehig/;BigDataBlog;Dr. Frederic Ufer
07. Feb 15;" Retweets,      ""Big-Data-Strategien: So werden Unternehmen erfolgreicher""";Big-Data-Strategien helfen Unternehmen Erkenntnisse zu gewinnen und Abläufe zu optimieren. Dabei entstehen neue Geschäftsmodelle, Märkte und neues Wachstum. Welche Vorgehensweise empfiehlt sich und was gibt es zu beachten?Big Data: Hinter diesen zwei kleinen Wörtern steckt eine komplexe Vielfalt von Themen, Visionen und Herausforderungen. Viele Unternehmen erkennen die Chancen von Big Data und wollen Daten nutzen und in Werte verwandeln. Bei der konkreten Umsetzung stellen sie schnell fest, dass es kein Patentrezept, keine All-in-one-Lösung gibt. Vielmehr bedeutet die Umsetzung einer Big-Data-Strategie in einem Fall das eine und im anderen etwas ganz anderes. Für Banken oder Versicherungsunternehmen, bei denen es um Risikoabschätzung, Betrugserkennung oder Investitionsentscheidungen geht, sind Echtzeitanalyse und Transaktionsdaten relevant. Für einen Autohersteller oder Maschinenbauer stehen Datenströme im Zentrum, die Sensoren liefern, während sie den Betrieb der Produktionsanlagen überwachen. Eine Marketingabteilung wiederum interessiert sich für Daten aus Social Media, Blogs und Foren, um Trends und Kundenwünsche zu erkennen. All diese Fälle erfordern individuelle Big-Data-Strategien, die Expertenwissen und spezifische technologische Lösungen voraussetzen.Big-Data-Strategien: Welche Kompetenzen werden benötigt und woher kommen sie?Die Erforschung und Verwertung von Big Data ist als Phänomen verhältnismäßig neu. Es gibt keine Berufsbezeichnungen, oder ein IHK Abschluss für diese neue Branche, aber trotzdem ist der Bedarf an Fachleuten immens. Die Anforderungen sind umfassender und komplexer, sodass Big-Data-Lösungen nicht „nur“ eine Nebenaufgabe von IT-Abteilungen sein kann. Diese Aufgabe gehört ins Management eines Unternehmens. Wenn Informatik, Statistik und Mathematik zu den Kompetenzen eines “Data Scientists” gehören, benötigt dieser weit mehr Kenntnisse.Noch fehlt es an ausreichenden Bildungsangeboten. Erst nach und nach werden Kurse innerhalb Studiengängen an Fachhochschulen und Universitäten etabliert. Ein eigener Studiengang für Data Scientists fehlt aber noch. IT- Unternehmen bieten Kurse, Schulungen und Weiterbildungen an, die nicht standardisiert sind. Betrachten wir Big Data als ganzen Prozess wird klar, dass es den Data Scientist als einzelne Person nicht geben kann. Vielmehr wird ein Team aus Experten vielmehr ein Bündel von Expertisen benötigt. Wissen aus Bereichen wie IT, Statistik und Mathematik werden ergänzt mit Jura, Grafikdesign, Psychologie und Kenntnissen aus den relevanten Fachbereichen.Erfolgreiche Big-Data-Strategien: Welche Herausforderungen entstehen für Unternehmen?Unternehmen stehen vor der Herausforderung strukturell umzudenken. Denn eine Big-Data-Strategie erfolgreich umzusetzen, bedeutet nicht, eine neue Abteilung zu schaffen. Die Erkenntnisse aus Big Data wirken sich über Abteilungsgrenzen hinweg aus. Deswegen sind Kenntnisse über Unternehmensstrukturen, Psychologie und Diplomatie nötig, um Ergebnisse und daraus folgende Konsequenzen zu kommunizieren.Je nach Projektaufgabe, nach Art der Daten, nach Unternehmensgröße und zur Verfügung stehenden Mitteln werden die Anforderungen an Big-Data-Strategien unterschiedlich aussehen.Auf dem Weg von den Rohdaten zur wertgenerierenden Erkenntnis bieten klar identifizierbare Bereiche bei der Planung einer solchen Strategie Orientierung.Sind Big-Data-Fragen Rechtsfragen?Bereits bei der Erhebung von Daten muss darauf geachtet werden, dass sie im Einklang mit den jeweils herrschenden Gesetzen stehen. Der Datenschutz und die Gewährleistung der Anonymität haben vor allem in Deutschland einen hohen Stellenwert.In anderen Ländern wie den USA sind die Bestimmungen weit weniger streng als hierzulande. Der Einsatz von Big Data wird dadurch zum Teil noch erschwert, solange der Politik die Gesetze nicht dem digitalen Zeitalter anpasst. Strategien stehen auf sicheren Füßen, wenn die rechtlichen Fragen im Vorfeld abgeklärt werden.Die rechtliche Perspektive auf Daten ist sehr weit, da Daten keine Landesgrenzen kennen. Datenschutzgesetze und Richtlinien gibt es auf Landes-, Bundes- und EU-Ebene. Werden beispielsweise bei einer Analyse Daten aus Social Media, Foren und Blogs einbezogen, genügt unter Umständen die Berücksichtigung des deutschen Datenschutzrechts nicht. Juristisch relevant sind alle Fragen, die die Speicherung und Weiterverwendung von historischen Daten betreffen. Was Metadaten betrifft, die Daten klassifizieren und beschreiben, besteht noch juristischer Klärungsbedarf.So fordern Big-Data-Strategien die IT herausDatenhaltung und -zugriff gehören zu den beiden Kernaufgaben der IT-Abteilungen und kommen hier zum Einsatz, indem Software- und Hardwarelösungen integriert werden. Die Datensicherheit hat neben der rechtlichen Seite eine technologische. Entsprechend muss eine Datenbankarchitektur gefunden werden, die den Anforderungen entspricht. Sensible Daten in einer Cloud zu speichern hat ein höheres Risiko, als auf einem internen Server.Der Einsatz von Verschlüsselungstechniken verschafft ebenfalls zusätzliche Sicherheit. Neben der Speicherung von Big Data, stellt sich die Frage nach der Notwendigkeit eines Backups zur Sicherung der Datenmengen. Die Wirtschaftlichkeit von Big Data misst sich nicht zuletzt an dem notwendigen Einsatz von Hardware und Daten-Management.Investitionen in die Infrastrukturen sind ebenso zu bedenken, wie in das Know-how und die Fähigkeiten der IT-Mitarbeiter, Entwickler und des Supports.Warum analytische Verfahren aus der Mathematik und Statistik nötig werdenEng verknüpft mit der IT sind die eigentlichen Analyseverfahren. Bei traditionellen Datenbanken mit sogenannten strukturierten Daten (SQL) und Data Warehouses sind die klassischen Methoden der Informatik gefragt. Mit mathematischen und statistischen Verfahren lassen sich Muster erkennen, Anomalien ausmachen und Korrelationen aufzeigen.Aufgaben wie Predictive Mainteneance zur Maschinenwartung, Data-Mining, Prognoseverfahren und Fraud Detection zählen zu den wichtigsten Anwendungsgebieten von Big-Data-Analytics.Auch das Machine Learning, manchmal “Deep Structured Learning” genannt, funktioniert nach wenigen mathematischen Algorithmen, Regeln und Gesetzmäßigkeiten: Maschinen sind in der Lage Optimierungspotentiale zuerkennen und umzusetzen.Datenvielfalt und ihre FolgenDaten sind nicht gleich Daten. Digitalisierte Informationen können in unterschiedlichen Formaten vorliegen. Sie können strukturiert, unstrukturiert oder in eine Mischform aus beiden vorkommen. Je nachdem mit welcher Art von Daten ein Unternehmen zu tun hat, sind andere Kompetenzen gefordert. Bilder und PDFs erfordern eine andere Analysetechnik als Metadaten oder Datenströme von Sensoren, die den Betrieb von Maschinen überwachen.Die Auswertung von Daten aus Social Media kann nicht statistisch erfolgen. Die Wortbedeutung und der Kontext sind entscheidend für die Bewertung und die Verwertung. Expertisen aus der Sprachwissenschaft und Textanalytik sind demnach nötig. Bei der Analyse von Meinungsbildern, Kundenwünschen oder Trendvorhersagen sind intelligente Text- und Bildanalyse-Tools gefragt.Datenbanken und Big Data: Drei wichtige Trends für die ZukunftWenn ein Unternehmen oder eine Institution eine umfassende Big-Data-Strategie betreibt und eine vielfältige Datenkultur pflegt, reichen traditionelle Datenbanken nicht mehr aus. Wer auf dem Markt bestehen will, kann von neuen Technologien profitieren.NoSQL: Das “No” steht für “Not only” SQL. Das ist ein Datenbanktyp, der “nicht nur” mit strukturierten Daten umgehen kann, wie eine reine SQL-Datenbank. Das ist wichtig, weil bestimmte Zusammenhänge sich erst durch die Kombination verschiedener Datentypen ergeben. Beispielsweise, wenn regelmäßige Muster bei Bestellvorgängen mit der Trendanalyse aus Social Media in Verbindung gebracht werden.Hadoop ist eine Open-Source-Lösung, die es ermöglicht riesige Datenmengen kostengünstig zu speichern und zu verwalten. Ich persönlich nutze nur Hadoop für unsere Big Data Analysen, auch für diesen Blog. Internetgiganten wie Facebook und Klout nutzen Hadoop, um die Unmengen an Daten zu verteilen und skalierbar zu machen. Diese Software hat das Potential eine Datenrevolution auszulösen.In-Memory-Computing: Der Arbeitsspeicher hat im Vergleich zum Festspeicher den Vorteil, dass die ausgelagerten Daten sofort verfügbar sind. Für Echtzeitanalysen ist dies ein entscheidender Vorteil. In-Memory-Computing macht dies im großen Stil Big Data in riesigen Arbeitsspreichermodulen. Wenn Big-Data-Strategien die Grundlage zu Handlungsentscheidungen sind, ist diese Technologie unverzichtbar.Welche Unternehmensbereiche sind bei Big Data betroffen?Die vielen Einsichten aus Big Data bleiben ohne Konsequenz, wenn sie nicht an der richtigen Stelle im Unternehmen ankommen. Damit dies gelingen kann, werden Data Artists die Resultate der Analysen in Diagrammen, Graphen, Bilder und Texte übersetzen.Die Visualisierung und Präsentation der Ergebnisse gehen Hand in Hand mit der Interpretation von Daten. Nur in Zusammenhang mit einer klaren Darstellung kann Big Data zu einem machtvolles Instrument werden.Erkenntnisse aus den Daten können weitreichende Entscheidungen für das obere Management nahelegen. Ein Data Scientist muss weitere Rollen beherrschen. Als Diplomat vermittelt er die Einsichten an die Geschäftsführung, das Management oder Abteilungsleiter. Als Impuls- und Ideengeber bringt er Innovationen in der Entwicklungs- und Marketingabteilung voran.Big Data berührt alle Unternehmensbereiche: Vom Management, über das Controlling bis hin zur Produktion. Als Grundlage von Entscheidungsprozessen wird Big Data zum Produktionsfaktor.Big Data ist mehr als die Summe seiner TeileBig Data ist keine einzelne Technik. Der betriebswirtschaftliche Nutzen von Big Data ergibt sich aus der engen Verzahnung von rechtlichen Rahmenbedingungen, Technik und Datensicherheit, Finanzen und Fachwissen sowie Kommunikation und betrieblicher Integration.“Wer die Chancen von #BigData nicht erkennt, wird langfristig wichtige Entscheidungen falsch treffen.“ Twittern WhatsAppAls Zusammenwirken von vielen Technologien und Fachleuten kann Big Data zu einem wertvollen Instrument werden. Es kann Produkte und Prozesse verbessern, neue Geschäftsfelder erschließen, den Wettbewerb analysieren, Finanzen planen, Preise optimieren, neue Vertriebswege finden, Trends erkennen und Sicherheit schaffen.So vielfältig die Anwendungsgebiete sind, so vielfältig werden die Big-Data-Strategien im Einzelfall ausfallen. Eine Big-Data-driven Company zu werden, ist eine große Herausforderung. Mit der richtigen Strategie bereichern die Daten den Unternehmensalltag und machen sie effizienter, zuverlässiger und sicherer.;https://bigdatablog.de/2015/02/07/big-data-strategien-werden-unternehmen-erfolgreicher/;BigDataBlog;Ibrahim Evsan
28. Jan 15;" Retweets,      ""Big Data: Welche Vorteile entstehen für Banken und ihre Kunden?""";Aggressive Verkaufsstrategien gehören zum alltäglichen Bankgeschäft wie die Verwaltung unseres Geldes. Wenn Banken Big Data optimal wahrnehmen würden, könnten sie Vertrauen aufbauen und personalisierte Angebote verkaufen. Mit Online-Banking ist dieser Schritt längst nicht getan. Wie können Banken reagieren? Die Digitalisierung veränderte die Art, wie wir mit Banken in Kontakt treten, fundamental. Früher war die Filiale und das Gespräch mit dem Berater der Weg, um Bankgeschäfte zu tätigen. Durch das Online-Banking wurde der Web-Browser zur Bankfiliale und das Web-Interface zum Bankschalter. Mobile Banking machte Bankgeschäfte von überall möglich. In Zukunft lösen digitale Bezahldienste wie Apple Pay und Google Wallet das traditionelle Tagesgeschäft der Banken weiter ab und virtualisieren das Geld.Digitalisierung schafft allerdings nicht den Bedarf an Beratung ab. Kunden sind nach wie vor an guten Anlagemöglichkeiten interessiert. Die Banken als Gebäude werden dabei nur weniger wichtig. Deswegen besteht Handlungsbedarf. Die Digitalisierung und Big Data schaffen neue und bessere Möglichkeiten in der Beratung und der Kundenansprache. Wenn eine Bank ihre Daten dazu nutzt, um den persönlichen Bedürfnissen der Kunden besser zu entsprechen, dann gewinnen sie etwas, das darüber hinaus noch wichtiger ist: das Vertrauen der Menschen. In Zeiten von steigendem Wettbewerb bringen Vertrauen und Kundenloyalität den entscheidenden Vorteil.Datenerhebung beherbergt Gefahren – doch die relevanten Informationen sind der Schlüssel zum ErfolgNachdem Banken das Online-Banking einführten, brauchten die Kunden nicht automatisch weniger Hilfe mit ihren persönlichen Bankgeschäften. Sie wählten nicht von selbst die für sie passenden Produkte und konnten nicht ohne Beratung alle Aufgaben übernehmen, bei denen die Bank sie bis dahin unterstützte. Kunden erwarten Beratung, die auf ihrer persönlichen Finanzsituation basieren, während Banken ihnen oft mit aggressiven Verkaufsangeboten begegnen. Provisionen, die beim Abschluss bezahlt werden, bildeten eher die Grundlage als die konkreten Bedürfnisse des Kunden. Die Online-Portale der Banken begegneten den Nutzern nicht als vollwertiger Ersatz für den Berater. Kunden erwarten von Apps und Anwendungen, dass diese sie ebenso gut kennen, wie ihr persönlicher Berater.Die Erfahrung mit den Online-Angeboten von Banken ist weniger personalisiert als institutionalisiert. Ein Weg, der sich Banken als Ausweg bietet, besteht darin, datenbasierte Lösungen zu suchen. Datenauswertungen erlauben wertvolle Einsichten in das Verhalten von Kunden, stellen aber noch nicht die Lösung des Problems dar – das Vertrauen der Kunden zu gewinnen. Vielmehr liegt in der Datenerhebung selbst auch eine potentielle Gefahrenquelle: große Geldinstitute haben Zugriff auf enorme Datenarchive und Echtzeit-Traffic. Die große Datenmenge kann erst dann zum Wohle der Kunden eingesetzt werden, wenn Wege gefunden werden, aus den strukturierten und unstrukturierten Daten die relevanten Informationen herauszufiltern. Big Data im Bankwesen richtig anzuwenden, führt zu der Frage, was die Bedürfnisse und Wünsche der Kunden und der Banken sind.Die individuellen Bedürfnisse der Menschen und die Verkaufsstrategien der Banken können unterschiedlicher kaum seinFinanzdienstleistungen sind eine komplexe Angelegenheit, für die Fachwissen und Erfahrung nötig sind. Der Übergang vom traditionellen Bankgeschäft zu Online Services führte bei den Kunden nicht dazu, keinen Beratungsbedarf mehr zu benötigen oder kein Interesse an der Anlage ihres Vermögens zu haben. Lediglich der räumliche Abstand zur Bank hat sich vergrößert. Der technologische Fortschritt hält jedoch Innovationen bereit, mit denen Banken ihre Dienstleistungen weiter anbieten können.Die entstandene Lücke durch die Verlagerung des Bankgeschäftes hin zum Online-Banking kann durch Big Data geschlossen werden. Das Verhalten der Bankkunden, ihre persönliche finanzielle Situation und ihre Wünsche lassen sich aus ihren Daten decodieren. Die Datenanalyse ermöglicht maßgeschneiderte Beratung und identifiziert die passenden Produkte. Wenn Banken sich wieder auf die individuellen Bedürfnisse der Menschen konzentrieren, entsteht bei Kunden wieder das Vertrauen zu Banken.Wie kann Digitalisierung und Big Data den Banken helfen, den Kundenwünschen gerecht zu werden?Ein einfaches Szenario hilft den Nutzen von Big Data gestütztem Banking deutlich zu machen. Angenommen ein Bankkunde nutzt ein laufendes Konto für alle regelmäßigen Zahlungen und Abbuchungen wie Miete, Raten für Versicherungen, Gebühren, etc. Manche Buchungen werden monatlich vorgenommen, andere vierteljährlich, wieder andere jährlich. Damit das Konto gedeckt ist, richtet er eine automatische Buchung eines fixen Betrages ein. Kommt es außerplanmäßig zu einer erhöhten Abbuchung, weil etwa die Nebenkosten bei der Miete gestiegen sind oder eine Sonderzahlung bei der Versicherung fällig ist, kann das Konto ins Minus rutschen, automatisch Rückbuchungen veranlasst und Strafgebühren fällig werden.Stellen wir uns dasselbe Szenario in einer Smart Bank vor, die Big-Data-Methoden für einen personalisierten Kundenservice einsetzt. Das Datenprofil dieses Kontos weist sowohl die regelmäßigen Eingänge als auch die regelmäßigen Abbuchungen auf. Ebenso im Kundenprofil enthalten sind die Informationen zum Sparguthaben, dem Tagesgeld und den fest verzinsten Anlagen. Wenn nun die Situation mit der außerplanmäßigen Abbuchung auftritt, könnte automatisch eine E-Mail, eine SMS oder ein Alarm auf die Buchung hinweisen. Sogar ein Lösungsvorschlag wird direkt unterbreitet, wenn die Deckung auf den anderen Konten ausreichend und liquide ist.Die Ziele von Big Data im BankwesenVerständnis der Bedürfnisse der Kunden durch Auswertung der historischen Daten und der Echtzeit-Analyse der Geschäftsaktivitäten.Verlässlichkeit. Big Data gibt den Bankberatern ein zusätzliches Messinstrument an die Hand, um Entscheidungen zu überprüfen.KostenreduktionGewinnsteigerung durch schlankeren und automatisierten Kundenservice und mehr Effizienz im operativen Geschäft.Gewinn von Vertrauen der Kunden.Gewinn von Kunden durch Vertrauen.Die Vorteile von Big Data im BankwesenRisikominimierung bei der Geldanlage und der Kreditvergabe.Kundenorientiertes Marketing, bei dem die wirklichen Bedürfnisse der Kunden im Zentrum stehen.Individualisierte Servicedienstleistungen erlauben jeden Kunden als Individuum mit all seinen Bedürfnissen anzusprechen.Echtzeitanalyse aller Transaktionen ermöglicht sofortiges Handeln, wenn sich gute Anlagemöglichkeiten bieten oder außerplanmäßige Zahlungen fällig sind.Betrugsprävention durch die Registrierung von Unregelmäßigkeiten und Abweichung vom individuellen Verhalten des Kontoinhabers.“Schluss mit aggressiven Verkaufsstrategien! Kann #BigData die Lösung im Bankwesen sein?“ Twittern WhatsAppDie positive Wahrnehmung von Banken durch die Kunden und der Gewinn von Vertrauen sind die zentralen Anliegen der digitalen Transformation des Bankwesens. Big Data gestützte Services, die personalisierte Lösungen ermöglichen, sind nur die Mittel, um dieses Ziel zu erreichen. Die Technologien, die zur Daten-Analyse in Echtzeit, zur Konzeption von individualisierten Produkten und Anwendungen notwendig sind, stehen bereit. Nötig zur Umsetzung ist nur ein Wandel der Mentalität, die den Menschen mit seinen Wünschen und Bedürfnissen in den Mittelpunkt stellt. Ein Unternehmen, das diese Möglichkeit bereits zum Teil erkannt hat, ist Kreditech.;https://bigdatablog.de/2015/01/28/big-data-fuer-banken-schafft-es-vertrauen-auf-kundenseite/;BigDataBlog;Ibrahim Evsan
07. Jan 15;" Retweets,      ""Smart Data: Was der Mittelstand von OTTO lernen kann""";OTTO ist die Toyota Motor Corporation unter den Handelsunternehmen, zumindest hierzulande. Wie der japanische Autohersteller die Produktion durch den Lean-Gedanken revolutioniert hat, wendet der Multi-Channel-Retailer Big Data Analytics auf den gesamten Produktlebenszyklus an – und kommt dabei zu erstaunlichen Ergebnissen. Was können mittelständische Unternehmen vom Beispiel OTTO lernen?Allein in seinem Onlineshop setzt OTTO rund 20 Millionen Artikel jeden Tag um. Menschenunmöglich vorherzusagen, welche Artikel wann, wie in welcher Größe, Farbe und so weiter nachgefragt werden. Daher hat sich der Konzern frühzeitig mit Predictive Analytics, der vorausschauenden Analyse riesiger Datenmengen, beschäftigt. Heute wertet die Software täglich Millionen von Datensätzen aus und verbessert die Absatzplanung nachhaltig. Das Wetter, neue Trends, Feiertage und Ferienzeiten: Alles wirkt sich auf den Umsatz mit Bademode, T-Shirts, Stiefel, Kinderbekleidung und vielen anderen Artikeln aus.Mit Predictive Applications können sowohl sämtliche interne Daten als auch externe Faktoren in die Prognosen einbezogen werden. So können Artikelabsätze präzise vorhergesagt und Bestellungen auf dieser Grundlage automatisiert werden. Gründe für Retouren lassen sich eindeutig bestimmen. Preise werden in Echtzeit optimiert und Trends frühzeitig erkannt. Letztlich entsteht aus vielen verschiedenen Datenanalysen und Prognosen eine ganzheitliche Betrachtung, die für mehr Transparenz, aber vor allem auch für mehr Umsatz sorgt.Schritt für Schritt und out-of-the-box“#Predictive #Analytics können immer mehr Faktoren in Prognosen einbeziehen. #Logistik #BigData“ Twittern WhatsAppDie „OTTO-Story“ und die anderen Cases, die Unternehmen wie wir, Blue Yonder, zu erzählen haben, stoßen bei Managern von mittelständischen Unternehmen zwar auf offene Ohren, aber sie haben Bedenken. „Wir haben weder die Zeit noch das Geld, um unser Unternehmen mit Big Data ‚smart‘ zu machen“, heißt es dann etwa. Dabei ist es heute mit verhältnismäßig geringem Aufwand möglich, ein Big-Data-Projekt aufzusetzen, das sich in wenigen Monaten rechnet. Der deutsche Sushi-Hersteller Natsu etwa macht das vor. Das Mindesthaltbarkeitsdatum der Convenience-Produkte liegt bei lediglich drei bis fünf Tagen. Mit Absatzprognosen von Blue Yonder ließ sich der Verlust durch abgelaufene Ware deutlich reduzieren. Unabhängig von der Unternehmensgröße sind Entscheider gefordert, sich realistische Ziele zu setzen und planvoll vorzugehen. Dazu haben wir ein dreistufiges Vorgehensmodell entwickelt:In einem Kick-off-Workshop mit Fachexperten und Entscheidungsträgern wird besprochen, welche Fragen mit Predictive Analytics geklärt werden sollen und definiert, welche Daten dazu benötig werdenWir prüfen das vorhandene Datenmaterial und beurteilen die zu erwartende Prognosequalität auf dieser GrundlageGemeinsam mit dem Kunden werden die einzelnen Umsetzungsschritte geplant, um eine werthaltige Lösung produktiv zu betreibenWer heute in das Thema Big Data einsteigt, profitiert von der Pionierarbeit, die andere, meist große Unternehmen, in den letzten Jahren geleistet haben. Mittlerweile gibt es vorkonfigurierte Prognoseanwendungen für mittelständische Unternehmen. Als SaaS-Lösung lassen sich die Anwendungen mit wenig Aufwand in die bestehende IT-Systemlandschaft integrieren und erfordern keine Investitionen in Hardware. SaaS steht für Software as a Service, also für Mechanismen, die in der Cloud, vom externen Dienstleister betrieben, für den Kunden arbeiten. Blue Yonder beispielsweise bietet Absatzprognoselösungen, die sehr unternehmens- und branchenspezifische Faktoren integrieren. Automobilzulieferer schätzen so ihren Teilebedarf realistisch ein, wenn sie neben historischen Verkaufszahlen auch Wetter-, Lieferanten- und Marktforschungsdaten einbeziehen. ;https://bigdatablog.de/2015/01/07/smart-data-was-der-mittelstand-von-otto-lernen-kann/;BigDataBlog;Dunja Riehemann
22. Dez 14;" Retweets,      ""Big Data und wir: über Nutzen und Möglichkeiten von Big Data""";Technologische Entwicklungen lassen sich nicht aufhalten, geschweige denn umkehren. Deswegen sollten alle Energien darauf verwendet werden, zu fragen, wie wir die neuen Technologien einsetzen. Im Falle von Big Data sind wir mit einer sprudelnden Quelle von Informationen und Wissen über fast alles konfrontiert. Ein geflügeltes Wort, das auf den englischen Philosophen Francis Bacon zurückgeht, lautet: “Wissen ist Macht”. Das neue Wissen, um das es hier geht, wird aus dem Rohstoff Daten gewonnen. Es ist ein Wissen, das es ermöglicht, die Bedürfnisse, Wünsche und das Verhalten der Menschen noch besser kennenzulernen. Mithilfe des Internets der Dinge, kann in Zukunft Wissen sogar von Bereichen erhoben werden, die sich unserer Wahrnehmung normalerweise entziehen. Daraus können Erkenntnisse gewonnen werden, die dabei helfen, einen intelligenteren Umgang mit Ressourcen zu betreiben. Oder Erkenntnisse, die unseren Umgang mit uns selbst und miteinander verbessern. Gerade wegen dem großen Potential, das in diesem Wissen liegt, sollten sich sowohl Unternehmen, als auch Staaten und jeder Einzelne für das Thema Big Data interessieren und über den Wert, den Nutzen und die Möglichkeiten bewusst werden.Big Data zwischen Verschiebung und Auflösung von GrenzenDie ersten sichtbaren Folgen von Big Data und Data Analytics ist das Verwischen von Grenzen. Eine exakte Unterscheidung zwischen dem Social Media Management und dem Customer Relationship Management in einem Unternehmen ist kaum noch zu treffen. Und vor allem: Wer managt überhaupt wen? Der Social Media Manager die Menschen im Social Media? Oder die Menschen in den sozialen Netzwerken die Marketing- und die Entwicklungsabteilungen? An allen Ecken und Enden werden sowohl die Grenzen zwischen Abteilungen innerhalb von Unternehmen als auch die Grenzen zwischen Unternehmen und Kunden aufgelöst. Wenn alle Grenzen durchlässiger werden, dann eröffnen sich damit völlig neue Chancen und Vorteile und zwar für beide Seiten.Die ersten messbaren Erfolge mit Big Data im MarketingDie Tools zur Echtzeitanalyse im Marketing stehen inzwischen bereit und warten nur darauf genutzt zu werden. Einer Studie von Forbes Insight zufolge erzielen Unternehmen, die Big Data im Marketing einsetzen, zu einem großen Teil mehr Gewinne, als für den jeweiligen Zeitraum erwartet wurde. Durch Datenanalysen in Echtzeit werden zwei Dinge erzielt. Erstens kann ein individueller Dialog mit den Kunden ermöglicht werden. Zweitens können die Erkenntnisse aus den Analysen von Big Data, etwa aus dem Marketing, direkt in die Forschungs- und Entwicklungsabteilungen zurückfließen. Beide dieser neuen Entwicklungen kommen vor allem dem Kunden zugute. Schon lange hat man erkannt, dass glückliche Kunden wertvoller sind als die bestplatzierteste Werbung. Der Rat eines Freundes wiegt mehr als die beste Werbeanzeige. Forbes Insights zufolge sind die Strategien, die auf das Vertrauen der Kunden zielen, auf mittlere und lange Sicht die erfolgreichsten. Das darf beim Thema Big Data nicht aus dem Blick geraten: Am anderen Ende von Big Data steht eine echte Community in sozialen Netzwerken. Oder die Bewohner einer Stadt: mit Sensoren in Mülleimern und Big-Data-Analysen ist es gelungen ein exaktes Datenprofil über das Müllaufkommen in Manchester zu erstellen. Anhand dieses Profils wurden die Versorgungswerke besser koordiniert und der Müll ökologischer entsorgt. Seither ist die Stadt sauberer und die Menschen darin glücklicher.Der Anfang der Veränderung“Big Data und wir: über Nutzen und Möglichkeiten von #BigData.“ Twittern WhatsAppDer Marketingbereich ist nicht der einzige Unternehmensbereich, der durch Big Data eine vollständige Veränderung erfahren wird. Über die Echtzeitanalyse von Big Data wird der Kunde, also die Menschen, ins Rampenlicht gerückt: Welche Kunden reagieren auf welche Form der Ansprache? Wer genau sind überhaupt meine Kunden – welches Alter und welches Geschlecht haben sie? Sind es eher Stadtbewohner oder Menschen, die auf dem Land leben, und welchen Stil pflegen sie? Je genauer ein Unternehmen seine Kunden kennt, desto besser kann es auf ihre Wünsche und Bedürfnisse eingehen. Zur Umsetzung sind Veränderungen auch innerhalb der Unternehmen notwendig. Es werden neue Berufe entstehen, die Daten erheben, auswerten, darstellen und Konsequenzen daraus ziehen. Wir werden den Data Scientist und den Data Visualizer ebenso benötigen wie den Data Architect oder Daten Ingenieure. Sie sorgen dafür, dass wertvolle Einsichten gewonnen und umgesetzt werden. Dadurch werden alle an den Wertschöpfungskette beteiligten Prozessen, die beim Kunden anfangen und beim Kunden enden, überhaupt wahrnehmbar und in der Folge enger verzahnt werden.Big Data und wirDie Veränderungen werden auch jeden Einzelnen betreffen. Big-Data-Apps im Privatanwenderbereich bringen mehr Komfort und schaffen einen Spielraum zur Selbstoptimierung. Mit Wearables bekommen wir den nötigen Zugang zu Daten in allen möglichen Bereichen. Bereits Smartphones sind eine ideale Quelle für Daten, die Aufschluss über bestimmte Aspekte unseres Lebens geben können, die uns bisher verborgen geblieben sind. Die App „Reporter“ wertet Daten über unseren Alltag aus und stellt diese grafisch dar. Am Ende des Jahres lässt sich auf diese Weise ein Rückblick in Datenform realisieren. Diese personal Big Data helfen dabei, sich über Dinge bewusst zu werden und Veränderungen planbar zu machen. Das kann Essensgewohnheiten betreffen oder unser Verhalten beim Geldausgeben. Die App gewährt Einblick in die persönliche Fitness, zählt jeden Tag unsere getanen Schritte und zählt die Treppen, die wir hochsteigen. Vielleicht stehen wir jeden Tag auf dem Weg zur Arbeit eine Stunde im Stau, während wir für genau die Strecke eine U-Bahn oder den Bus nehmen könnten. Was technisch ausgedrückt Selbstoptimierung heißt, ist in diesem Fall gleichbedeutend mit wertvoller Lebenszeit, die man hinzugewinnt.;https://bigdatablog.de/2014/12/22/ueber-nutzen-und-moeglichkeiten-von-big-data/;BigDataBlog;Ibrahim Evsan
19. Dez 14;" Retweets,      ""Big Data: Risiko oder glorreiche Zukunft?""";Big Data – das Stichwort war 2014 in aller Munde und wird von Menschen ganz unterschiedlich aufgenommen. Während die einen mit glänzenden Augen von Chancen und schier unerschöpflichen Potenzial schwärmen, sehen andere die Privatsphäre bedroht und sprechen von Überwachung.Sowohl Befürworter als auch Gegner der Entwicklung haben valide Argumente auf ihrer Seite, Fakt ist jedoch: Was auch immer man von Big Data halten mag, die Entwicklung schreitet weiter voran und ist nicht (mehr) aufzuhalten. Für Konsumenten ist es daher wichtig, sich über Big Data und die damit verbundenen Chancen und Risiken zu informieren.Big Data: Analyse, Verknüpfung und Nutzung von DatenBig Data ist ziemlich genau das, was der Name vermuten lässt: Die automatisierte Erfassung, Analyse, Auswertung und Nutzung enormer Datenmengen, die sich nur durch Computer und vernetze Rechenzentren bewerkstelligen lässt. In diesem Kontext fällt oft das Stichwort des Internet of Things oder – wie es inzwischen oft genannt wird – des Internets of everything.Damit werden die wachsende Vernetzung verschiedenster Gegenstände, Fahrzeuge und Technologien und die dadurch entstehenden Möglichkeiten beschrieben. Vernetze Autos können beispielsweise Echtzeitdaten zur Verkehrssituation erhalten und dadurch Staus frühzeitig umfahren. Der immer wieder beschworene vernetzte Kühlschrank erkennt automatisch, dass Milch fehlt und bestellt diese autonom nach. Und die Smartwatch – und andere Wearables – am Handgelenk sammelt Fitness- und Gesundheitsdaten und kann diese automatisch dem Hausarzt zur Verfügung stellen.All diese Möglichkeiten basieren im Grunde auf der Nutzung von Big Data. Verkehrsinformationen, personalisierte Angebote, die intelligente Steuerung des Stromverbrauchs – all das ist nur möglich, weil aus Millionen von Datensätzen Muster und Durchschnittswerte berechnet und daraus Empfehlungen abgeleitet werden können.Big Data: Komfort, Effizienz und LebensqualitätDie Chancen und Möglichkeiten, die sich aus der Kombination vom Big Data und der zunehmenden Vernetzung aller Lebensbereich ergeben, sind gewaltig. Supermärkte und Online-Anbieter können durch die Analyse das Kaufverhaltens individualisierte Angebote optimal auf jeden Kunden zuschneiden, der Stromverbrauch tausender Haushalte liese sich durch so genannte Smartgrids – intelligent vernetze Stromnetzen und Haushalte – auf der Grundlage von Big Data Ergebnissen optimieren, Staus auf Autobahnen und die Prozesse in Unternehmen effizienter gestalten.Die konsequente Nutzung von Big Data eröffnet Optimierungs- und Sparmöglichkeiten in bisher nicht gekanntem Ausmaß. Das Resultat: erhöhter Komfort und eine steigende Lebensqualität für Konsumenten. Die folgende Grafik stammt aus einer Studie des Frauenhofer Instituts zum Thema Big Data und zeigt eindrucksvoll, in welchen Wirtschaftsbereichen die Ergebnisse von Big Data zum Einsatz kommen oder zum Einsatz kommen könnten.Big Data: Kommt der gläserne Kunde?“#BigData: Risiko oder glorreiche Zukunft?“ Twittern WhatsAppBei allen Vorteilen und Chancen birgt Big Data allerdings auch Risiken. Eines der größten: Durch die wachsende Vernetzung und die detaillierte Analyse von Datensätzen – und vor allem die Vernetzung der Ergebnisse – rückt der oft diskutierte “gläserne Bürger/Kunde” in greifbare Nähe.Zwar geben Unternehmen an, dass Datensätze anonymisiert ausgewertet werden, doch ob das tatsächlich so stattfindet, lässt sich nur schwer kontrollieren. Deutschland hat im internationalen Vergleich einer der, wenn nicht sogar die strengsten Datenschutzgesetzgebungen überhaupt.Diese soll Bürger und Konsumenten vor Datenmissbrauch schützen – ob dies gelingt steht auf einem anderen Blatt – schränkt dadurch jedoch auch die Möglichleiten von Big Data in Deutschland ein.Dazu kommt, dass viele der relevanten Daten durch die Nutzung von Online-Diensten anfallen, deren Server nicht in Deutschland stehen. Und US-amerikanische Unternehmen unterliegen den dortigen Datenschutzrichtlinien und -gesetzen, die deutlich weniger Schutz der Privatsphäre gewährleisten als die deutschen Richtlinien.Big Data: Chancen bei achtsamer NutzungBig Data ist ein langanhaltender Trend bzw. eine Entwicklung, die die Gesellschaft nachhaltig verändern wird. Aktuell wird nur ein Bruchteil der Möglichkeiten ausgeschöpft, die Entwicklung steht noch ganz am Anfang. Aufhalten lässt sich sich jedoch nicht mehr, dazu spielt Big Data bereits in zu vielen Lebensreichen eine wichtige Rolle.Und vielleicht ist es auch gar nicht wünschenswert, die Entwicklung zu stoppen. Immerhin bietet sie Konsumenten zahlreiche Vorteile und Möglichkeiten. Die Herausforderung besteht darin, die Chancen zu nutzen ohne die Risiken zu stark werden zu lassen. Dazu können informierte Konsumenten durch einen achtsamen Umgang mit den eigenen Daten beitragen.;https://bigdatablog.de/2014/12/19/big-data-risiko-oder-glorreiche-zukunft/;BigDataBlog;Ibrahim Evsan
15. Dez 14;" Retweets,      ""Digitalisierung &amp; Big Data setzen sich durch – das Beispiel “Uber”""";"“Das Bewegen von Lasten ist etwas, womit sich schon die Menschen in der Antike beschäftigten.” Das stellt Karl Haeusgen gegenüber der Süddeutschen Zeitung (Quelle: Süddeutsche Zeitung, Nr. 274, Freitag, 28. November 2014. “Die Fehler der anderen” von Varinia Bernau) fest. Als Leiter eines Herstellers von Hydrauliksystemen in München steht er für Old Economy. Dennoch – oder gerade daher – weiß er: Die Beliebtheit der Digitalisierung wächst, die heutigen Spielregeln diktiert zunehmend die New Economy, Big Data erlaubt inzwischen, ganze Datenflüsse zu erfassen, auszuwerten und daraus neue Erkenntnisse und Strategien zu generieren.Müssen traditionelle Unternehmen jetzt deshalb um ihre Existenz bangen? Werden sie von Big Data verdrängt? Werden sie Dank der Digitalisierung aussterben? “Unwahrscheinlich”, sagt Haeusgen. “In der Industrie 4.0 kommt es auf ein Miteinander an.”Seinem Optimismus zum Trotz ist es eine Tatsache, dass viele Branchen von der Digitalisierung erfasst und dabei stark ramponiert, wenn nicht sogar vollständig vernichtet wurden. “Es ist längst nicht das erste Mal, dass eine neue Technologie eine etablierte Branche niederwalzt. Warum lernen die Gebeutelten so wenig voneinander?”, schreibt Varinia Bernau in der Süddeutschen Zeitung (Quelle: Süddeutsche Zeitung, Nr. 274, Freitag, 28. November 2014. “Die Fehler der anderen” von Varinia Bernau).Wer im Strukturwandel nicht hoch genug pokert, verliert.Ein Bekenntnis zum bestehenden Geschäft ist zwar wichtig, dämpft aber gleichzeitig den Mut zum Risiko, das das Unternehmen bereit ist, einzugehen. Denn mit der Größe eines Unternehmens steigt auch seine Verantwortung für seine Mitarbeiter, seine Partner und seine Leistungen. Umso schwieriger ist es, Wachstumschancen rechtzeitig zu erkennen und zu nutzen, die Beispielsweise eine Anwendung der Big Data auf ihre bisherigen Leistungen erlauben würde. Das Resultat: je größer ein Unternehmen, desto träger.“Es bringt nichts, sich einer Idee zu verweigern, die die Kunden schätzen. Dann suchen sich die Kunden das nämlich anderswo.”, behauptet Frank Briegmann im Interview mit der Süddeutschen Zeitung. Er arbeitet für den Musikkonzern “Universal” – ein Unternehmen, das rechtzeitig mit der Zeit gegangen ist und nun mit digitaler Musik und Streamdiensten 40 % seiner Einnahmen macht und illegalen Downloads Parole bietet. Ebendieses “anderswo” bezeichnet die Chance kleiner Start-ups, in die Lücken zu drängen, die träge, ungelenke Großkonzerne hinterlassen.Uber – Ein Beispiel für erfolgreiche Start-ups in der New Economy“1 Mal tippen und dein Uber ist in wenigen Minuten da.” So wirbt das Unternehmen Uber für seine App. Es ist ein Paradebeispiel für ein erfolgreiches Start-up in der New Economy. Das Grundprinzip: über eine App ermöglicht Uber die Nutzung von Autos, die gerade nicht gebraucht werden. Es nutzt vakante Ressourcen und Kapazitäten und ist dadurch umso günstiger, unkomplizierter und vorteilhafter für seine Nutzer. Eine schlichte, aber geniale Idee, die inzwischen zu einem Service gewachsen ist, bei dem man bei Bedarf bequem sogar eine Limousine mit Chauffeur ordern kann.Offen, präsent, benutzerfreundlich und für jedermann leicht zugänglich spaltet es dennoch die Gemüter. Denn entgegen Behauptungen von Fürsprechern, Uber mache die Welt besser, hält sich ebenso stabil der Vorwurf, Uber mache sein Geld auf dem Rücken von anderen. Die Argumente der Gegner:Als Anbieter fremder Leistungen übernehme Uber kaum Verantwortung für die Qualität ihrer Leistungsträger.Mit einer Beteiligung in Höhe von 20% des jeweiligen Gesamtpreises profitiere Uber immens von den Leistungen seiner Leistungsträger.Durch die Nutzung von Schwächen des Taxigewerbes unterlaufe Uber soziale Standards und dränge ohnehin schlecht bezahlte Taxifahrer sogar teilweise in Nebenjobs.Obwohl diese Argumentation genügt hat, um Uber in drei deutschen Städten – auch auf Druck der Taxibranche – gerichtlich zu untersagen, sieht gerade Reiner Hoffmann – Vorsitzender des Deutschen Gewerkschaftsbundes – Potential, wie Big Data zum Wohle der Menschen genutzt werden kann.“Old Economy bringt eine Menge Erfahrungen mit dem Strukturwandel ein.”“#Digitalisierung &amp; #BigData setzen sich durch – das Beispiel “#Uber”“ Twittern WhatsAppSo setzt sich der Gewerkschafter für Old Economy ein. Da die Idee der Gewerkschaften schließlich gut 150 Jahre alt ist, gehöre es nämlich auch gewissermaßen dazu. Zusammen mit modernen, erleichterten Teilnahme- und Nutzungsvoraussetzungen wie einer umstandslosen, unkomplizierten, digitalen Bedienung involvieren New Economy Start-ups Teilnehmer, Nutzer und Kunden, ohne sie aus ihren etablierten Positionen in der Old Economy zu drängen. Ausgerechnet ein Einsatz der statistischen und technischen Aspekte von Big Data könnte eine Erfassung und einen optimalen Einsatz ebendieser Erfahrungen ermöglichen – sollten Großunternehmen darauf eingehen.Der ideale Weg Hand-In-Hand zu einer optimalen Kombination von Old und New Economy unter dem Gesichtspunkt der Big Data. Außerdem: “Manche Menschen machen einen Nebenjob auch aus Spaß und nicht aus purer Notwendigkeit. Wir haben einen Bereichsleiter bei uns, der abends noch als Türsteher in einer Table-Dance-Bar jobbt.”, so Karl Haeusgen. ";https://bigdatablog.de/2014/12/15/digitalisierung-big-data-setzen-sich-durch-das-beispiel-uber/;BigDataBlog;Ibrahim Evsan
10. Dez 14;" Retweets,      ""Big Data &amp; das Internet der Dinge zwingen Unternehmen zum Umdenken""";"Big Data funktioniert in gewisser Weise wie eine Taschenlampe. Sie bringt Licht dorthin, wo bislang keines war. Durch dieses Licht werden Zusammenhänge in einem bisher nicht denkbaren Maßstab beleuchtet und damit erst erkennbar gemacht. Der digitale Wandel, durch den dieses neue Wissen ermöglicht wird, hat eine viel stärkere und intelligentere Vernetzung zur Folge. Miteinander vernetzt werden jedoch nicht mehr nur Menschen oder Unternehmen. Die Vernetzung, von der hier die Rede ist, betrifft nahezu alles. Jetzt wird uns klar: diese Entwicklung erfordert von uns ein neues Denken. Intelligente und vernetzte Produkte bringen ihren Nutzern nicht nur neue Funktionalität, sondern sie verschaffen dem Produzenten einen Überblick über den gesamten Wertschöpfungsprozess. Durch die aus Big Data gewonnenen Erkenntnisse und durch die Möglichkeiten, die aus der Vernetzung der Welt entstehen, lassen sich Produkte und Prozesse analysieren und automatisieren, überwachen, steuern und optimieren. Das alles und noch viel mehr ist möglich, wenn wir offen sind diese Themen verstehen und nutzen zu wollen.Die neue Übersicht mit Big DataBig Data entsteht überall dort, wo eine Vielzahl von neuen Technologien intelligent miteinander verschaltet wird: Schnelle Rechenleistung von Prozessoren, enorme Speicherkapazitäten von Daten, Cloudtechnologien, smarte Software und Sensoren, die über das Internet mit unzähligen Quellen, Menschen und Datenarchiven verbunden sind. Die Daten, die bei diesem Zusammenspiel entstehen, können Geschichten erzählen. Wenn es gelingt, ihnen diese Geschichten zu entlocken und die richtigen Schlüsse daraus zu ziehen, kann die Welt intelligenter und ein Stück besser werden. Ein Überflug über den Atlantik betrachtet aus Datenperspektive, kann zu enormen Einsparungen führen und damit gleichzeitig zum Klimaschutz beitragen. Dazu werden die Daten aus den Flugzeugturbinen, mit den Daten aus den Sensoren, die den Luftdruck und die Temperaturen messen, historischen Daten aus den Wetterdatenbanken und Angaben zu der Anzahl der Passagiere und der Fracht miteinander in Beziehung gesetzt, Muster analysiert und ausgewertet. Die Daten aus den Flugzeugturbinen erhöhen ganz nebenbei die Sicherheit und minimieren das Risiko eines Ausfalls, da Störungen frühzeitig erkannt und behoben werden können. Auf ähnliche Weise können wir Daten von fast allem erheben und mit diesen Daten völlig neuen Einsichten gewinnen.Das Internet of EverythingIn den 90er Jahre wurde das Internet zu einem Massenmedium und stellte die Unternehmen vor die Herausforderung mit den Menschen, die sich im Netz oder in Social Media bewegten, in Kontakt zu treten. Der Wettbewerbsvorteil resultierte aus einem gekonnten Auftritt im Web oder der gelungenen Erweiterung der Kommunikationskanäle zwischen Firmen und Kunden. In Zukunft werden Wettbewerbsvorteile dadurch zu erreichen sein, wenn Unternehmen das Internet der Dinge zu nutzen wissen. Der Maßstab dieser Herausforderung ist im Vergleich zu früheren Aufgaben ungleich größer. Cisco spricht dabei vom Internet of Everything und hat errechnet, dass heute gerade einmal 10 Milliarden Dinge mit dem Internet verbunden sind. In den nächsten Jahren warten jedoch 1,5 Billionen Objekte darauf, ebenfalls mit Sensoren ausgestattet und vernetzt zu werden. Das daraus resultierende Wachstumspotential beziffert Cisco auf 14,4 Billionen US-Dollar. Um bei diesem Wettbewerb konkurrieren zu können, ist es nötig in den Bereichen Forschung und Entwicklung, der Konzeption und Herstellung von Produkten, im Vertrieb und den IT-Infrastruktur, bis hin zum Ausbau der After-Sales-Services neu zu denken und zu investieren.Algorithmen und ihre Einbettung in komplexe SystemeAuf der ersten Ebene betrachtet, fügen Sensoren den Produkten neue Funktionen hinzu. Der Regensensor unter der Windschutzscheibe eines Autos kann feststellen, ob es gerade regnet beziehungsweise wie stark es regnet und entsprechend kann der Scheibenwischer angeschaltet und die Frequenz reguliert werden. In dieser einfachen Beschreibung liegt das kausale Grundprinzip von allen Algorithmen vor, mit denen Sensoren und Rechner operieren. Ein Algorithmus beruht immer auf dem Schema „Wenn … dann…“: Wenn es regnet, dann den Scheibenwischer anschalten. Wenn es stärker regnet, dann schneller wischen. Wenn die Umgebung dunkel ist, dann das Licht anschalten. Algorithmen können aber auch sehr komplex werden und weiter entfernte Dinge miteinander in Beziehung setzen. Wenn die eingebauten Sensoren mit einer Cloud kommunizieren oder über das Netz mit anderen Systemen verbunden ist, kann ein Auto auf Gegebenheiten reagieren oder Vorschläge machen, die den „Sensoren“ des Fahrers, also seinen Sinnen, entgehen oder außerhalb ihrer Reichweite liegen. Wenn ein Auto mit einem intelligenten Verkehrsleitsystem verbunden ist, können Staus umfahren oder sogar vermieden werden. Schlaglöcher in der Straße können direkt an die Stadtverwaltung gemeldet werden, während umgekehrt der Fahrer in der Innenstadt auf freie Parkplätze hingewiesen werden kann. Und das beste ist, wir hätten ganz neue Art von Wetterdaten. Ein intelligentes Auto ist nicht länger nur ein intelligentes Auto, sondern Teil von viel komplexeren Systemen.Wie sollen Wertschöpfung und Werterschließung funktionieren?Die Sensoren und die aus ihnen gewonnenen Informationen fördern wertvolles Wissen über den tatsächlichen Gebrauch, über Prozesse und die Gewohnheiten der Kunden zutage. Diese Erkenntnisse können zurückfließen in die Entwicklungsabteilungen und dort bei der Erweiterung und der Neukonzeption von Produkten helfen. Features, die in der Produktion teuer sind und in der Praxis vielleicht gar nicht gebraucht werden, können eingespart werden. Beim Herstellungsprozess helfen die Sensoren und die Daten bei der Überwachung von Produktionsketten, registrieren Störungen und unterstützen dabei, Abläufe zu optimieren.Aus der Digitalisierung beziehungsweise aus dem Prozess der Erzeugung und Analyse von Big Data leiten sich neue Geschäftsmodelle ab. Denn mit den neuen Daten und den großen Datenmengen erwachsen selbst neue Aufgaben, die zum Teil erst noch erschlossen werden müssen und volkswirtschaftlich ein enormes Potential darstellen. Die Erhebung, die Verarbeitung und Weiterverwertung von Daten und nicht zu vergessen der Schutz von Daten sind unternehmerische Felder, auf denen noch viel brach liegt.Kooperation in einer vernetzten WeltProdukte und Prozesse können in Zukunft nicht mehr isoliert betrachtet werden, sondern werden immer als ein Teil eines Systems wahrgenommen. Ein System wiederum ist eingebettet in andere Systeme, mit denen es interagieren kann. Neue Geschäftsfelder entstehen an den Schnittstellen von neuen und traditionellen unternehmerischen Aufgaben. Das Internet der Dinge, Big Data und die Smart Industry sind die Technologien, mit denen sich die Energiewende gestalten lässt. Eine Waschmaschine, die Teil eines Smart Homes ist, kann sich genau dann einschalten, wenn sie von der Windkraftanlage, die mit Smart-Grid-Technologie ausgestattet ist, die Information erhält, dass gerade Energie erzeugt wird und damit günstiger, regional produzierter Ökostrom genutzt werden kann.Ganze Branchen werden sich, aufgrund der Vernetzung der Dinge, verändern. Unternehmen, die ihre Wettbewerbsfähigkeit erhalten wollen, müssen sich auf die Folgen, die daraus entstehen, einstellen. Aus den neuen Kooperationsmöglichkeiten werden alle Beteiligten profitieren. Produktionsanlagen, die mit Sensoren zur Überwachung ausgestattet sind, können über eine Cloud, in der Daten über den normalen Betrieb gespeichert sind, permanent abklären, ob alles in Ordnung ist. Wenn ein Fehler oder auch nur von der Norm abweichende Daten gemessen werden, kann die Maschine direkt über das Internet eine Meldung an den Hersteller senden, die Art des Fehlers mitteilen und sogar präventiv Maßnahmen einleiten. Der Hersteller wiederum hat die Möglichkeit den Fehler über ein Update zu beheben oder einen Mitarbeiter, der die Diagnose schon erhalten hat, mit dem richtigen Ersatzteil los schicken, ohne dass es zu einem Produktionsausfall kommt. Dadurch werden die Reparaturkosten minimiert und Ausfälle verhindert.Unternehmen müssen aufgrund von Big Data umdenken“#BigData &amp; das #Internet der Dinge zwingen #Unternehmen zum Umdenken.“ Twittern WhatsAppDurch Big Data, Sensortechnik und Vernetzung werden Produkte auf dem Markt intelligenter und im Vergleich zu vorherigen Produkten attraktiver. Da sie mit dem Hersteller, den Zulieferern, den Kunden und anderen Systemen verbunden sind, wird eine bislang nicht gekannte Interaktivität ermöglicht, aus der Nutzen und Vorteile entstehen. Durch diese Intelligenz der Produkte werden alle herkömmlichen Grenzen gesprengt und ganze Branchen revolutioniert. Die Auswirkungen auf die gesamte Volkswirtschaft werden enorm sein. Doch auch die Wertschöpfungsketten werden durch die neuen Technologien berechenbarer. Die Sensoren überwachen den Betrieb und im Abgleich mit Datenbanken helfen sie bei der Steuerung und Optimierung der Produktion. Die Veränderungen in all diesen Bereichen zwingen Unternehmen zum überdenken und nachdenken: Wie verändert sich meine Stellung am Markt? Ist mein Geschäftsmodell noch marktkonform und konkurrenzfähig? Welche neuen Aufgaben kommen durch die Veränderungen hinzu? Welche Vorteile und Nutzen erwächst aus den neuen Möglichkeiten?Unternehmen werden sich diesen Fragen stellen müssen, um ihre Stellung am Markt zu erhalten oder zu verbessern. Durch die Neuerungen werden nicht nur die Produkte immer intelligenter, die Produktions- und Distributionsprozesse sicherer und optimiert – von den Neuerungen durch den digitalen Wandel wird die gesamte Volkswirtschaft und damit alle Mitglieder der Gesellschaft profitieren.";https://bigdatablog.de/2014/12/10/big-data-das-internet-der-dinge-zwingen-unternehmen-zum-umdenken/;BigDataBlog;Ibrahim Evsan
02. Dez 14;" Retweets,      ""Big Data – Google is watching you""";Google is watching you. Datenschützer, Politiker und User fordern bereits seit geraumer Zeit mehr Transparenz vom Internetgiganten Google. Schließlich werden zahlreiche Dienste des Unternehmens von uns genutzt – was dabei mit unseren Daten passiert, ist oftmals nicht nachvollziehbar. Mit dem 2009 eingerichteten Dashboard kam Google uns zumindest ein wenig entgegen, denn plötzlich konnten User gesammelte Informationen einsehen und über entsprechende Links Einstellungen vornehmen. Ein Blick in die Privatsphäreeinstellungen kann daher sehr hilfreich sein.Zudem können unter Datentools beispielsweise gespeicherte Lesezeichen und Nachrichten archiviert und heruntergeladen werden.Der Online-Speicherdienst Cloud Defender hat zudem sechs hilfreiche Links zusammengestellt, um im Datendschungel einen Überblick zu behalten.1. Personenprofil bei GoogleUm personalisierte Werbung schalten zu können, sammelt Google Daten wie Geschlecht, Alter, Sprachen, Interessen, aber auch blockierte Kampagnen von Werbetreibenden. Was Google bereits über dich gespeichert hat, kannst du hier überprüfen.2. Suchanfragen auf google.deJede einzelne Suchanfrage wird von Google in der History gespeichert. Wer das nicht möchte, kann die Suchergebnisse kurzfristig stoppen, einzelne lassen sich gar löschen. Wird auf eine Werbeanzeige geklickt, weiß Google auch das.3. StandortGoogle Now (Android-Version) verbindet nicht nur einzelne Dienste wie Maps und Mail miteinander, sondern zeichnet auch Geodaten auf. Diese können aber hier eingesehen und gelöscht werden.4. IP-Adresse, Gerätenutzung und Standort des GerätsAuch diese Informationen greift Google ab. Herauszufinden sind die Daten hier.5. Apps und Erweiterungen, die auf Google-Daten zugreifenApps die Zugang zu Google-Diensten haben, werden unter folgender Adresse gelistet, können aber auch abgeschaltet werden.6. YouTube-SuchanfragenÄhnlich wie bereits auf google.de wird auf youtube.com jede einzelne Suchanfrage gespeichert. Wer also nicht mehr weiß, nach welchen Videos er gesucht hat, dem hilft dieser Link.;https://bigdatablog.de/2014/12/02/big-data-google-is-watching-you/;BigDataBlog;Ibrahim Evsan
20. Okt 14;" Retweets,      ""Beacons. Signalsender für das Internet der Dinge:  Irrlichter oder Leuchtturm?""";"Die Beacon-Technologie (von Signal, Licht) ermöglicht einem den Traum von „sprechenden Gegenständen“ á la Disney. Per App lassen sich Besucherströme in Stadien lenken, komplizierte Wege zum Flughafen-Gate zeigen und freie Tische im Restaurant finden – und direkt die Bestellung erledigen. Was zunehmend zum Ortsleit- und Auskunftssystem ausgebaut werden soll, steckt noch fest im Gerangel um die richtigen Industriestandards. Das Signal der stationären, münzgroßen Beacons kann von Bluetooth-Empfängern in der Umgebung abgetastet werden. Dank einer Minibatterie arbeiten sie für über ein Jahr autark. Das Internet der Dinge vernetzt sich für den persönlichen mobilen Service – überall im öffentlichen Raum. Der Kellner erfährt Live, wo er gebraucht wird und der Besucher im Freizeitpark, wo die Achterbahn mit der geringsten Wartezeit zu finden ist. Stets im Hinterkopf der Betreiber sind dabei ein Mehr an Informationen und Effizienz. Also Kostensenkung. Und die Möglichkeit, in Echtzeit (kommerzielle) Angebote zu machen.Wenn einem das Werbeplakat individuelle Empfehlungen ausspricht.Via Beacons und App gelangt man direkt zu einem Verkaufspunkt – wo man dieselbe Schnittstelle gleich zum Bezahlen benutzt. Apple Stores sind weltweit mit der Technologie (ibeacon) ausgestattet, Disney kooperiert und McDonalds zieht nach.“Wo ist meine Stadionbox? Wo ist mein Kellner? #Beacons erleichtern das Leben – und die Werbung“ Twittern WhatsAppZur Überwachung von Passanten, wie im Fall von Titan, eignet sich das System nur bedingt. Die günstigen Sender sammeln selber keine Informationen. Aber die SmartPhone-Apps können auch auch unauffällig als Trojaner installiert werden oder als Teil von größeren Diensten wie Facebook und Google Maps. Wie diese Daten in der Cloud dann verarbeitet werden – und welche überhaupt erfasst werden, ist unklar. Die Überwachungs- und Nutzungsabsichten von Staat und Unternehmen sind jedoch ernst zunehmen. Jüngst wurde ein pakistanisches Unternehmen in den USA dafür verurteilt, Spionage-Apps frei verkauft zu haben. Die Hoffnungen der Unternehmen konzentrieren sich jedoch auf die freiwillige Nutzung der Apps. Angebote sollen nur auf die Handys gepusht werden, wenn dies gewünscht ist. Der Kunde soll die Wahl bekommen, ob er im Restaurant lieber per Smartphone bestellt oder der Kellner geholt werden soll.Interaction on demand – oder doch nur Werbung?Zielgenaue Kommunikation zwischen Mensch und Umwelt (Maschine) verringert den Suchaufwand – und für diesen Nutzen haben wir bereits weitreichende Einschränkungen unserer Freiheit akzeptiert. WLAN-Netzwerke haben ähnliche Funktion wie Beacons und werden schon heute für die Standortbestimmung genutzt – ungefragt, unkontrolliert. Alternative Technologien versuchen bei geringeren Kosten den Funktionsumfang auszuweiten. Die mit Bluetooth verbundenen Beacon-Apps verbrauchen knappe Smartphone-Energie. Zwar weniger, als die aktuelle NFC-Technologie, aber dennoch. Neue Standards auf LTE-Basis haben das Potenzial, noch viel mehr herauszuholen. So ist der Ping, das Signal, bei LTE nicht nur in einem Umkreis von bis zu 15 Metern (Bluetooth) oder 80 Zentimetern (NFC) empfangbar, sondern im Umkreis von bis zu 500 Metern. Und das bei geringerem Energieaufwand bei Sender und Empfänger. Das letzte Signal ist also noch lange nicht gesendet, wenn es darum geht, auch die materielle Welt zu vernetzen.// <!CDATAjQuery('#slideshare_embed_31116426').click(function() {    if(confirm('Sie sind dabei eine SlideShare Präsentation zu öffnen. SlideShare nutzt Tracking-Technologien, die Ihre IP Adresse an Dritte weitergibt. Wir als Seitenbetreiber haben auf die Tracking-Methoden des externen Anbieters leider keinen Einfluss. Sollten Sie damit nicht einverstanden sein, wählen Sie bitte abbrechen!')){        jQuery('.wp-image-2894').remove();        jQuery('#slideshare_embed_31116426').append('');    }else{        alert('Die SlideShare Präsentation finden Sie unter http://www.slideshare.net/slideshow/embed_code/31116426');    }});// &gt;";https://bigdatablog.de/2014/10/20/beacons-signalsender-fuer-das-internet-der-dinge-irrlichter-oder-leuchtturm/;BigDataBlog;Tobias Hauck
14. Okt 14;" Retweets,      ""Beacons auf dem Vormarsch – über ihre Verwendung wird gestritten""";Die New Yorker Stadtverwaltung hat den Plakatierer Titan dazu aufgefordert, sein innerstädtisches System an „Beacons“ abzubauen. Trotzdem sich der Anbieter von Werbeflächen auf Telefonzellen vor der Installation der kleinen Signalsender von einem städtischen Amt eine Genehmigung einholte, sei dieser Schritt nun nötig, so Phil Walzak, Sprecher der Stadtveraltung.  “Die #Beacons sind unter uns – in New York werden die ersten Sender aus Datenschutzgründen wieder abgebaut.“ Twittern WhatsApp Mithilfe der App iBeacon Detector war von dem Medienportal BuzzFeed herausgefunden worden, dass rund 20 von über 500 Beacons in der Nähe des Times Square verbaut wurden. Dies sei Laut Titan zu Wartungszwecken geschehen. Darüber hinaus können die kleinen Sender der Firma Gimbal dazu genutzt werden, Bewegungsprofile von Menschen in ihrer Umgebung zu erstellen – ohne deren Wissen. Da es keinen öffentlichen Entscheid und Informationen über die Anwendung der Technologie gab, rückt sie in die Grauzone des Rechtmäßigen und wird nun vom Betreiber abgebaut.Ein Porträt der Technologie und der neuesten Entwicklungen gibt es an dieser Stelle in den nächsten Tagen. Zu den in New York verwendeten Beacons gibt es ein Info-Video:;https://bigdatablog.de/2014/10/14/beacons-auf-dem-vormarsch-ueber-ihre-verwendung-wird-gestritten/;BigDataBlog;Tobias Hauck
10. Okt 14;" Retweets,      ""E-Health – ein Drama in fünf Akten""";PrologE-Health, das sind (Big Data-)Technologien im und um den menschlichen Körper herum. Implantiert, als Uhr getragen, im Gürtel, der Kontaktlinse, als Brille, Schuheinlage oder eingenäht in die Kleidung. Bald wird jede Regung des menschlichen Körpers verfolgt werden können. Ständig.Erster AktDer Patient betritt die Klinik. Zum ersten Mal in dieser Stadt. Am Empfang liegen alle Daten bereits vor, in der Sprechstunde hat der Arzt Zugriff auf die gesamte relevante Krankengeschichte und im angeschlossenen Diagnosezentrum wird automatisiert der Check-up sämtlicher auf dieser Krankengeschichte beruhenden und altersbedingten Risiken überprüft. Automatisiert bis hin zur Ergebnisanalyse. Erst bei der Therapie wird der Arzt zu Rate gezogen. Das ist effektiv, schnell, angenehm, billig – und ziemlich transparent. Gegenüber vielen Menschen.Ist das eine praktische Hilfe oder dreiste Kontrolle?Fest steht, es macht vieles einfacher. Zweiter AktAuffällige Werte, seltsame Symptome: Der Patient hat den Jackpot geknackt. Eine komplizierte und seltene Krankheit hat sich breit gemacht. Man braucht mehr Informationen – und den direkten Vergleich zu anderen Fällen, seien es noch so wenige. Kabellose biometrische Sensoren werden heute so billig, genau und vertraut, dass sie bald unsere ständigen Begleiter werden. Von der einfachen Gewichtserfassung zu bis zu hochempfindlichen EKG. Die Kombination dieser Daten und der Abgleich digitaler Krankenakten mit anderen Patienten macht Muster sichtbar. Immer gleiche Zusammenhänge. Und den Erfolg von Therapien. Diagnosen werden genauer und Studien schneller.Aber: Um diese Datenmassen in brauchbare Informationen zu verwandeln braucht es einen regen Informationsaustausch und zentrale Auswertungseinheiten. Deswegen steht schon heute am Anfang die Unterschrift des Patienten, seine Zustimmung zur Datennutzung. Bist Du bereit, Deinen Körper transparent zu machen und Erkenntnisse mit einem anonymen System zu teilen?Dritter AktEs kommt immer schlimmer, als man denkt. Die Krankheit ist kein Einzelfall. Denn sie ist hoch ansteckend. Gott sei Dank wissen die E-Health-Nerds damit umzugehen und schützen die Gesellschaft.Es grenzt an Magie.Die verborgenen Aufzeichnungen der digitalisierten Welt sagen uns schon heute Entscheidendes über Gegenwart und Zukunft. Die richtigen, schlau gewichteten Daten geben eine Masse, die einmal durch das Analysefilter gequetscht, am Ende eine Essenz von Wahrheit und Rettung tröpfeln lässt. Nicht nur Staus und Erdbeben wird mit solchen Methoden der Kampf angesagt. Google Flu Trends steht zwar aktuell wegen seiner Unzuverlässigkeit in der Kritik, was aber nichts an der prinzipiellen Chance ändert: Epidemien frühzeitig erkennen oder gar vorhersagen. Google verlässt sich dabei auf Veränderungen im Suchverhalten der Nutzer und hat 45 Schlüsselbegriffe identifiziert, die eng mit dem Grippeaufkommen verbunden sind. Die Live-Erfassung von Daten aus Diagnosezentren, Apotheken oder Wearables wird das Verfahren in Zukunft verfeinern.Vierter AktAbgleich von Live-Daten für neue Erkenntnisse und Therapien – es ist keine Binsenweisheit. Manchmal muss man nur genauer hinschauen, denn:Das Glück liegt immer vor der Nase. Intelligente Schuhsohlen entdecken Geschwüre von Diabetikern schon Tage vor der Ausbildung. 95 Prozent zuverlässig ist diese Fußbetreuung und kann in der Forschung neue Erkenntnisse bringen. Computer, die bald vollständig den Körper auf Nervenbasis simulieren können, bilden das Gegenstück zum digital verbesserten Menschen. Aufwendige, randomisierte und doppelblinde Studien, geringe Aussagekraft aufgrund zu vieler Einflussfaktoren und langwieriges Datensammeln werden zwar keine Ende haben – aber, so die Hoffnungen, weniger entscheidend sein.“#E-Health und #BigData sorgen zusammen für eine einfachere, schnellere, gesündere Welt. http://bigdatablog.de/?p=778 “ Twittern WhatsAppFünfter AktDu bist gezwungen, Deine Gesundheit zu optimieren – und doch wird jemand anderes stets die besseren Möglichkeiten dazu haben. Gesundheitssysteme sind nicht gerecht. Du. Bist. Allein. Wenn die Sohle einem sagt, dass der Fuß sich in ein Geschwür verwandelt, die App, wann etwas einzunehmen ist und der Videochat Ärzte und Hilfegruppen ersetzt, ist die Gesundheit digitalisiert und Du physisch alleine. Franz-Joseph Bartmann, Präsident der Ärztekammer Schleswig-Holstein, hat das erkannt: Es ist ein biologischer Prozess, das heißt, die nachwachsende Generation wird mit Daten ganz anders umgehen als diejenige, die kurz vor Aufgabe ihrer praktischen Tätigkeit ist“.Neu leben lernen.Ganz neue Kompetenzen werden da gebraucht, um zu unterscheiden, wann die Technik mir helfen soll und wann ich mir einen Menschen als Ansprechpartner leisten will. Wachstum der Lebenserwartung und Gesundheit darf kein Selbstzweck sein. Auf die Lebensqualität kommt es an. Denn eines ist klar: E-Health wird das Preisgefüge im Gesundheitssystem umkrempeln. Dass es so sozial bleibt, wie es heute ist, muss erneut erkämpft werden.Senkung von gemein-schaftskosten durch       E-Health (in Mrd. $) | Create Infographics;https://bigdatablog.de/2014/10/10/e-health-ein-drama-in-fuenf-akten/;BigDataBlog;Tobias Hauck
23. Sep 14; Retweets;Muster. Gleichbleibende Strukturen im großen Chaos sucht man überall dort, wo Orientierung und Markenbildung gefragt sind. Im Marketing der Überflussgesellschaft zum Beispiel. Da wird mit Produktmustern gearbeitet. Oder beim Netzwerken, wo eine Signatur einen individuelles Muster prägt. Oder auch in Datensilos, die Muster als sinnhaltige Essenz aus sich herauspressen. Hat man das Muster einmal gefunden, bleibt die Frage, wie man es präsentiert. Visuelle Hilfsmittel erleichtern den Schritt vom großen Datenchaos zum auf einen Blick erkennbaren Kern.Viele Analysetools für quantitative und qualitative Daten integrieren die visuelle Darstellung direkt in ihre Anwendung. So zum Beispiel NVivo, Deduuse oder auch MATLAB und andere Data Mining Tools. Oft hat man als Ergebnis aber bereits ein sauberes und reduziertes Dataset oder klare Verhältnisse, die man abbilden und visualisieren will.Google Chart und Fusion TablesDie Google Labs bauen auf dem Fundament der Rechenmaschine intuitive und interaktive Visualisierungsanwendungen auf. Diese einfachen Tools ermöglichen es jedem, Diagramme und Karten zu erstellen. Es ist einfach, den konzentrierten Datensatz hochzuladen und die verschiedene Darstellungsoptionen zu testen, um eine respektable und aufschlussreiche Präsentation zu erstellen. Bei Fusion Tables stößt man jedoch auch an Grenzen, wenn man die Darstellungen in Details anpassen möchte.TableauEine Anwendung, die eine enorme Vielfalt an Visualisierungsmöglichkeiten bietet – und dabei eine gute Figur macht. Einmal Daten importiert und einen Darstellungsvorschlag des Programms gewählt, erstellt Tableau eine stark personalisierbare Visualisierung, mit interaktiven Filtern, Legenden, Zoomfunktion und vielem mehr.CartoDBDieses web-basierte Werkzeug kartiert und konzentriert Informationen aus anderen Anwendungen. Interaktiv, intuitiv gestaltet und mit einem großen Funktionsumfang ist CartoDB eine Allzweckwaffe für alle raumbezogenen Daten.GephiDaten darstellen und visuell erkunden: Komplexe Zusammenhänge wurden noch nie so schnell intuitiv dargestellt. Dynamisch, hierarchisch und personalisierbar hilft Gephi Daten zu erkunden, zu sortieren, zu wälzen und schöne Darstellungen zu schaffen.FusionChartsGeht es darum, interaktive Diagramme in Web einzubetten, sind die JavaScript-basierten FusionCharts die professionelle Lösung der Wahl. Eine breite Palette von Infografik-Gerüsten und Graph-Designs stehen zur Auswahl, aber auch die Möglichkeit, diese selber zu erstellen. Die Besonderheit liegt in der intuitiven dreidimensionalen Darstellung und den grenzenlosen Personalisierungsmöglichkeiten. Zoom, Scroll, pop-up und kinetische Musterbewegung machen die FusionCharts zu interaktiven Erklärbären.“Datenvisualisierung. Diese Tools zeigen, was man hat. #BigData #Visuals“ Twittern WhatsAppKartographOhne auf kommerzielle Karten wie Google Maps zurückzugreifen, baut Kartograph einem persönliche Vektorkarten. Ebenfalls interaktiv kann Kartograph mit GIS und SVG  besonders gut arbeiten.Wolfram AlphaWolfram Alpha baut de Visualisierungsmöglichkeiten mathematischer Probleme konstant aus. Als Suchmaschine für „computational knowledge“ stellt sie schon über die Suchfunktion einbettbare interaktive Graphen zur Verfügung. Jeglicher Personalisierungsaufwand entfällt.TimeFlow Analytical Timeline und timeline.jsHistorische Daten anschaulich dargestellt: Viele Anwendungen lösen dieses Problem. Timeline macht es einfach und angenehm, bindet problemlos Quelle wie YouTube, Social Media und anderes ein. TimeFlow ist dafür inhaltlich stärker bei komplexen Herausforderung.Infogr.amEinfache Aussagen und Zusammenhänge lassen sich auf zeitgemäße und angenehme Art in Infogr.ams simplen Infografiken darstellen. Ein vorgefertigtes Grundgerüst und viele Individualisierungsmöglichkeiten machen es zu einem pompösen Tool bei asketischer Reduzierung auf das Wichtigste.Weitere Tools:Freie Big-Data-ToolsDas Tool Followerwonk;https://bigdatablog.de/2014/09/23/praesentationen-auf-datenbasis/;BigDataBlog;Tobias Hauck
18. Sep 14;" Retweets,      ""Freie Big-Data-Tools – Entdecke die Möglichkeiten""";Kreativität und Innovation, neue Lösungen, schneller, genauer, getrieben durch Start-ups am anderen Ende der Welt und durch die eigenen Kunden. Puh! Was wäre es schön, wenn es eine Quelle für prägnante Einsichten und gute Konzepte gäbe. Moment – die gibt es doch. Und das sogar kindersicher und kostenlos. Schon die Einsteigertools helfen, die wahre Natur einer Problemstellung zu erkennen.die Intuition und den Kern von Trends zu entziffern.komplexe Probleme zu durchdringen und zu einem gegenwärtigen Verständnis zu führen.RechercheDie Suche im Web mit Spezialsuchmaschinen führt einem mehr vor Augen, als man seriös verarbeiten kann: pdf-Suche, Google Scholar und Google Books, Wolfram, und viele mehr. Genau deswegen sind sie ein idealer Einstieg in ein Thema. Wichtig ist jedoch eine vorab klar definierte Fragestellung, um sich nicht zu verlieren.Analytics Web/ SEOGoogle TrendsHistorische Suchtrends und Themenpopularität sind mit Google Trends nachzuvollziehen.Google Keyword ToolÜber Google Keyword Tool von AdWords sind die optimalen Keywords für das eigene Angebot identifizierbar. Das Keyword-Tool ist der Maßstab der SEO-Optimierung. Über einfaches Ausprobieren und Empfehlungen von Google ergibt sich die optimale Wortumgebung.SEOquake für Chrome Die Browsererweiterung, die zu jeder besuchten Seite SEO-Informationen anzeigt, ist im Chrome Web Store zu haben.Consumer BarometerDas Consumer Barometer zeigt, wie Nutzer die Google-Suche in einem Kaufprozess einsetzen. Das ist vor allem für technische und informationsintensive Produkte nützlich, da hier der Nutzen von Information und Vergleich am häufigsten gesucht wird.Social Media AnalyticsFacebook AnalyticsVon „Wo sind meine Fans?“ bis „Wie erreiche ich mehr Interaktion mit meinen Lesern?“ bietet Facebook umfangreichste Daten. In begrenzten Umfang hilft das auch bei der Wettbewerberbeobachtung.Topsy Topsy ist die erste Adresse für Twitter-Analysen. Wer wie oft mit wem? Mit welchen Inhalten, Emotionen und Influencern? Wie steht meine Marke da? Hier liegen die Antworten. CompeteCompete bietet Besuchertrends, Traffic-Informationen, Keywordanalyse und Informationen zu Saisonalität.DatenverarbeitungOpenRefine“Diese Tools geben Einblicke in Big Data – und lernen kann man dabei auch schon etwas!“ Twittern WhatsAppOpenRefine ist das bessere Excel. Sowohl Text als auch zahlenbasierte Daten kann die Anwendung geordnet erfassen und verarbeiten. Insbesondere csv, also mit Komma getrennte Werttabellen und JSON-Dateien, kann das Programm verarbeiten wie kaum ein anderes.Data WranglerData Wrangler ist ein Web-Service der Stanford Universität und ordnet und säubert Datensätze, damit auch andere Programme und Tabellentools damit arbeiten können. Das wird besonders vereinfacht durch die interne Texterkennung des Data Wrangler. DatenvisualisierungGoogle Fusion Tables Google Fusion Tables verarbeiten die Datensätze in intuitive und interaktive Grafiken. Ob Diagramm, Karte, Heatmap, Scatter Plot, Zeitleiste oder Animation – immer übersichtlich und in begrenztem Maße personalisierbar – die Fusion Tables machen auch große Datensätze fassbar.TableauTableau macht wie Fusion Tables Visualisierung zu einem Fest. Individuelle Lösungen und komplexere Darstellungen sind hier möglich.InfogramInfogram macht die Infografik zur Lösung vielfältiger Erkenntnis und jeder noch so komplexem Datensatz. Nach der Analyse und vor der Präsentation bringt dieses Tool die Erkenntnisse auf den Punkt.;https://bigdatablog.de/2014/09/18/freie-big-data-tools-entdecke-die-moeglichkeiten/;BigDataBlog;Tobias Hauck
12. Sep 14;" Retweets,      ""Das Netz als Datentankstelle. Wenn wichtige Informationen fehlen""";Als durchleuchteter, stets verfolgter Nutzer von Google, Facebook und Co. kann es ein Erweckungserlebnis sein, den Spieß einmal umzudrehen. Einmal selbst fremde Datenbanken auslesen. Endlich einmal selber unerreichbare Informationen abzapfen.Der NSA-verprellte abenteuerliche Neuland-Entdecker kann mit einfachsten Mitteln die eigene Datendecke aufbessern. Scraping, Crawler und andere Tools helfen. Und zwar besonders dann, wenn einzelne Informationen zwar in Masse vorhanden sind, aber (wie beim öffentlichen Adressbuch) nicht gesammelt und als Datenbank auswertbar zugänglich sind.Zu Data Mining gehört die Wissensentdeckung. Nützliche und möglicherweise unerwartete Muster sollen freigelegt werden. Häufig muss man dazu vor allem eines: Vorhandene Daten mit neuen, anders strukturierten Daten verknüpfen. Einzelhandelsumsatz muss mit Ortsdaten und regionalen Kaufkraftdaten verbunden werden. Wetterdaten mit Verkehrsdaten. Die fehlenden Kontextinformationen bietet das Netz. Von Google Maps bis zur Amazon Produktdatenbank sind diese Informationen und viele mehr doch eigentlich vorhanden.Wie komme ich an die fehlenden Datenmengen?“ Viele wissen nicht, was man mit Scraping noch aus dem Netz herausholen kann.“ Twittern WhatsAppEs gibt Webindexierungsmethoden, die einem diese im Hintergrund der Webseiten gespeicherten Daten auslesen. Webcrawler, Bots und Spraping-Anwendungen helfen dabei. Oft ist dies aber gar nicht nötig. Es bietet sich eine Recherche an:Zunächst suche bei den Anbietern nach webbasierten APIs – öffentlichen Datenschnittstellen. Was für Webunternehmen längst Standard ist, verstehen auch Behörden immer häufiger. Denn geteilte Informationen können allen helfen.Nutze für Daten, die in pdf, hmtl und anderen Formaten versteckt sind, spezielle Mining-Tools. Mach Daten maschinenlesbar. Die Formate csv, xml, json, xls sind dafür besonders geeignet.Sollte sich nichts finden und auch kostenpflichtige Datenanbieter nicht helfen, bleibt oft nur Suche auf eigene Faust in den Codes des Internet mittels Scraping-Tools. Die helfen, Daten aus den sichtbaren und unsichtbaren Teilen des Netz „herauszuschälen“. Und das geht, je nach Suchterrain und mit den richtigen Helfern, sehr leicht.Was können Scraper?ScraperDiese Chrome-Browser-Erweiterung hilft bei einfacher Textextraktion von vielen Websites auf einmal. Besonders nützlich: die „Scrape similar“-Funktion. Einfach einen Teil einer Website auswählen und die App transferiert alles Ähnliche schnell und einfach in ein auswertbares Format. Scraper ist weniger geeignet für komplexe Website-Strukturen oder Bilder. Wie zu jeder anderen Anwendung gibt es hier hilfreiche Anleitungen.Outwit HubDiese Anwendung ist für größere Datenmengen kostenpflichtig. Allerdings ist sie ein ideales Tool zum Kennenlernen und Üben, denn viele vorgefertigte Lösungen stehen für erweiterte Möglichkeiten und größere Komplexität bereit. Lade alle pdfs und Bilder einer Serie mit einem Befehl herunter – kein Problem. Verschiedene Online-Tutorials helfen. Ein Beispiel:OpenRefine (ehemals Google Refine)Diese freie Anwendung ist eine große Hilfe, unstrukturierte Daten nutzbar zu machen, zu säubern und in andere Formate zu überführen. Sortieren und Sammeln wird dadurch zur einfachen Übung.FirebugFür Firefox ist diese Erweiterung ebenfalls ein einfacher Weg, Rohdaten zu entdecken und zu sammeln, die beim Surfen im Hintergrund mitlaufen. Kimono LabsKimono scrapet Webdaten und schafft als in Hintergrund laufende Anwendung eine ständige API auf der Basis einer von URLs. Die Anwendung ist einfach zu bedienen, hat jedoch bestimmte Leistungsbeschränkungen.Import.ioGanz ähnlich, aber etwas differenzierter als Kimono Labs schafft einem Import.io eine websitebasierte API.ScraperWikiEine Sammlung von vorgefertigten Einsteigertools und gleichzeitig Online-Platform für kollaborative Problemlösung und komplexere Recherchen. Hier findet man alles, von einem Scraper zum Auslesen von Twitter bis zu ganz speziellen Lösungen. Da hier viele Scraping-Codes von anderen Usern eingestellt werden, ist die Platform ein guter Ort, um in Kontakt zu treten und um zu lernen.BeautifulSoupAls Wiki und Kollaborationsplattform für Fortgeschrittene richtet sich BS an Anwender, die schon Programmiersprachen wie Python können. Trotzdem ist man hier noch auf Einsteiger und einfache Anwendungen ausgerichtet. Viele vorprogrammierte Anwendungen machen es attraktiv, ein paar Python-Grundlagen aufzufrischen.ScrapyÄhnlich wie BeautifulSoup, bietet Scrapy jedoch mehr Code und mehr Personalisierungsmaßnahmen. Ebenfalls in Python ist dieses robuste Rahmenwerk ein Profi-Tool.Piggy BankAls Firefox-Erweiterung leistet diese Anwendung die Arbeit einer Abrufzentrale. Sie ermöglicht, Daten aus verschiedenen Webseiten abzurufen, zusammenzufügen und lokal zu speichern.Die offene Code-Platform Github bietet darüber hinaus für viele spezielle Scraping-Herausforderungen schon Lösungen und Codes an, die nur noch übernommen oder angepasst werden müssen. Hier finden sich auch die Experten, die einem bei speziellen Fragen weiterhelfen können.Übrigens: Scraping ist nicht (unbedingt) mit Diebstahl gleichzusetzen.Es ist heute häufig gewünscht. So bieten YouTube und Slideshare Einbettcodes, die Teile ihrer Seiten (Videos) direkt in anderen Kontexten verfügbar machen. In anderen Fällen, wie beim massiven Auslesen von Adressen aus Google Maps („Ich brauche eine Liste aller Waffengeschäfte in Deutschland“) werden technische Hürden gesetzt und die Rechtslage wird schnell unklar.  Immer mehr Institutionen und Datenhubs richten von sich aus heute APIs ein. Schnittstellen wie diese schaffen die Geschwindigkeit des Web heute, weil sie den geordneten Austausch von Daten ermöglichen – und diese nicht nur in doc, pdf oder html darstellen. Mitverantwortlich für diese Entwicklung ist die Open-Data-Bewegung, die Informationsfreiheitsgesetze durchgesetzt hat und die Kultur des Teilens mit der Allgemeinheit rundum die Welt propagiert. Offene Systeme wie Android und Apps wie Evernote versuchen zunehmend, Entwickler zur Nutzung von APIs zu bewegen.“Offene Systeme wie Android und Apps wie Evernote versuchen, über API-Entwickler anzusprechen.“ Twittern WhatsAppDie Attraktivität der Systeme soll dadurch erhöht werden. Wenn eine Stadt hingegen ihre erhobenen live-Verkehrsdaten über eine API zugänglich macht, können kommerzielle oder gemeinnützige Anbieter damit arbeiten und Anwendungen für sich und die breite Masse entwickeln. Insbesondere staatlich erhobene und durch Steuern finanzierte Daten sollen der Öffentlichkeit zugänglich gemacht werden. Somit wird in den Archiven schlummernder Nutzen freigesetzt und kann zu mehr Effizienz, Geschwindigkeit und besseren Entscheidungen führen.Auch Privatpersonen und kleine Betriebe können diese Möglichkeiten nutzen, um anstehende Entscheidungen besser zu treffen und sich zu entwickeln. Die fehlenden Daten liegen oft bereit – und sie zu nutzen, macht einen zum souveränen Wissensarbeiter.;https://bigdatablog.de/2014/09/12/das-netz-als-datentankstelle-wenn-wichtige-informationen-fehlen/;BigDataBlog;Tobias Hauck
27. Aug 14;" Retweets,      ""Partnersuche per Big Data: Wo Algorithmen die Liebe hinwerfen""";"„We Experiment On Human Beings!“. Dies gab jüngst einer der Chefs von OKCupid, Christian Rudder, in einem Blogtext zu. In den USA gehört OKCupid zu den beliebtesten Partnerbörsen und belegt gegenwärtig Platz 123 der meistbesuchten Seiten insgesamt. Was sie von anderen Anbietern unterscheidet, ist die relativ offenherzige Erklärung darüber, auf welche Art und Weise ihr Algorithmus funktioniert.„Aus Big Data wird Big Dating“ – dazu hat Tobias Hauck hier im Big Data Blog erst kürzlich einen lesenswerten Artikel verfasst. Der Algorithmus ist das Herzstück einer Online-Partnerbörse. Er  nimmt dem Benutzer das stundenlange Suchen ab und liefert Vorschläge, bei denen die Kontaktaufnahme möglichst viel Erfolg verspricht. Doch funktioniert das Kennenlernen, weil der vorgeschlagene Partner wirklich zu einem passt, oder eher deshalb, weil beide aufgrund des Vorschlags einfach glauben, dass sie füreinander bestimmt sind?Liebes-Spiele für die WissenschaftOKCupid hat nun bei Benutzern das Ergebnis des Algorithmus manipuliert. Wer rechnerisch zu nur 30% übereinstimmte, bekam stattdessen einen Wert von 60% oder 90% angezeigt – oder auch umgekehrt. Dann wurde gemessen, wie sich die Konversation änderte. Als tatsächliche Konversation galt das Austauschen von mindestens vier Nachrichten.Erhöht also der Algorithmus tatsächlich die Chance, dass zwei gut zueinander passende Personen einander finden? Oder könnte auch ein willkürliches Ergebnis angezeigt werden, da die Nutzer ihr Konversationsverhalten ohnehin danach ausrichten? Erfreulicherweise hat OKCupid die Ergebnisse dieses Experiments detailliert in einer Tabelle veröffentlicht.Diese Tabelle zeigt zunächst die Chance auf eine Konversation, wenn zwei Nutzer tatsächlich laut Algorithmus eine Kompatibilität von 30%, 60% oder 90% besitzen. Sie beträgt 10%, 13% oder 20%. Offenbar ist der Wert des Algorithmus nicht direkt übertragbar: Auch bei einem Ergebnis von 90% wird nur aus jeder fünften Anfrage eine Konversation. Andererseits steigt diese Chance mit zunehmender, rechnerischer Kompatibilität tatsächlich an.Kann Statistik die Kontakt-Chance erhöhen?Wie verändern sich nun die Chancen, wenn statt des tatsächlichen Wertes ein manipulierter Wert angezeigt wird? Bei einer tatsächlichen Kompatibilität von 60% sind die Unterschiede gering. Die Chance von 13% bleibt dieselbe, wenn stattdessen ein niedrigerer Wert von 30% angezeigt wird. Bei 90% steigt hingegen die Chance um drei Prozentpunkte auf 16% an.Spannend wird es bei den Extremfällen. Zeigt OKCupid zwei Benutzern, die eigentlich zu 30% zusammenpassen, stattdessen einen Wert von 90% an, so steigt die Chance auf eine Konversation von 10% auf 17%. Hätten die beiden aber tatsächlich zu 90% „zusammengepasst“, dann wäre die Chance bei 20% gelegen. Damit besitzt der Algorithmus gegenüber dem manipulierten Wert einen Vorsprung von relativ gesehen knapp 18%. Dies ist also dessen „Eigenleistung.“Im umgekehrten Fall, wenn 30% statt der „wahren“ 90% angezeigt werden, sind die Unterschiede noch stärker: Statt der erwarteten Konversations-Chance von 10% beträgt die tatsächliche Chance nun 16% und hat sich damit relativ um 60% erhöht. Wieder bestätigt das, dass der Algorithmus mehr leistet als der Zufall: Wer zu 90% zusammenpasst, wird auch eher eine Konversation beginnen; selbst wenn OKCupid nur einen niedrigen Wert anzeigt.Big Data – eine echte Chance für die LiebeDer Algorithmus liefert also einen deutlichen Mehrwert gegenüber rein zufälligen Partnervorschlägen. Das hat OKCupid durch dieses Experiment nachgewiesen, auch wenn über die Hälfte der Konversationen bereits ohne den Algorithmus stattgefunden hätte. Dieses Fazit zieht selbstkritisch auch der OKCupid-Blogger. Sicher kann man über den ethischen Aspekt eines solchen unfreiwilligen Experimentes streiten. Andererseits lässt sich nur durch solche Versuche und eine gründliche statistische Auswertung überprüfen, ob eine bestimmte Methode auch wirklich funktioniert und ihr Geld wert ist.Weiterführende Links:Dieses und ähnliche Experimentehttp://blog.okcupid.com/index.php/we-experiment-on-human-beings/http://blog.okcupid.com/index.php/the-real-stuff-white-people-like/http://www.spiegel.de/netzwelt/web/facebook-rechtfertigt-psycho-experiment-auf-neuigkeitenseiten-a-978253.htmlAlgorithmus bei OKCupid und allgemeinhttp://ed.ted.com/lessons/inside-okcupid-the-math-of-online-dating-christian-rudderhttps://netzpolitik.org/2014/how-to-analyze-everyone-teil-v-der-algorithmus-weiss-besser-als-du-wer-zu-dir-passt/Rechtlicheshttps://www.okcupid.com/legal/privacyhttp://www.huffingtonpost.com/2014/07/30/okcupid-experiment_n_5632351.htmlSinglebörsen im Vergleichhttp://de.statista.com/statistik/daten/studie/76504/umfrage/anzahl-der-nutzer-von-online-dating-boersen-seit-2003/http://www.singleboersen-vergleich.de/presse/online-dating-markt-2011-2012-de.pdfhttp://www.alexa.com/siteinfo/okcupid.com";https://bigdatablog.de/2014/08/27/partnersuche-per-big-data-wo-algorithmen-die-liebe-hinwerfen/;BigDataBlog;Katharina Schüller
19. Aug 14;" Retweets,      ""How 2 b big in Big Data""";Michael Hummel betreibt die deutsche Verlängerung des Silicon Valley: ParStream ist ein echtes, kreatives IT-Unternehmen, 2008 von drei Partnern gegründet – und das in Köln. Die Firma schließt nicht nur eine Lücke, die große Konzerne oder wilde Start-ups in Kalifornien offen gelassen haben. Sie entwickelt mit der Anwendung ParStream an vorderster Fortschrittsfront Big-Data-Datenbanken für die Zukunft von eCommerce, Geoanwendungen und Prozessanalysen von Produktion und Vertrieb – und rechnet in Lichtgeschwindigkeit. Dieser Ort innovativsten Gründergeists ist damit ein Musterbeispiel eines Silicon Valley-Unternehmens Made in Germany.“Big-Data-Experten von @ParStream bringen #ecommerce und #einzelhandel in die Zukunft. “ Twittern WhatsAppDie Entwickler eines „better tomorrow“ stehen beim Barbecue nahe San Francisco unter tiefblauem Abendhimmel und dem flackern der ersten Sterne. Hier stehen Start-Upper neben Venture Capitalists, Kai Diekmann auf Axel-Springer-Sommerfrische neben Google-Mitarbeitern und SAP neben Michael Hummel. „In Europa sind B2B-Technologieunternehmen überall verteilt. Deswegen schafft man da vielleicht zwei Termine am Tag. Im Silicon Valley sind es fünf!“, so Hummel. Es ist nicht zuletzt diese Effizienz und die offene Kommunikationskultur, die Ideen und Menschen so schnell zusammenbringt. Und deswegen spricht man bei ParStream in Köln auch nur Englisch.Panta rhei im Silizium-Tal.Deutsche Unternehmen wie Siemens oder die Deutsche Bank entwickeln daher zunehmend dort, wo sowieso nichts anderes gemacht wird – oder bauen in Zentren wie Paris oder London Innovationscenter auf. Das enttäuscht den ParStream-Mitgründer. Aber es liegt auf der Hand: Wer gründet, braucht viel Kommunikation und Absicherung. Thought leadership. Was glaubt man, kommt als nächstes? Wer kann ein Partner, ein Investor, ein Mentor sein? In Deutschland wird dies nicht gefördert. Vodafone baut mit Xone das hauseigene Innovationscenter in London. „Vodafone xone, delivering the promise“.Beim Barbecue in Kalifornien, wo Hummel noch immer wochenweise im Jahr anzutreffen ist, wurde ihm klar: „In den USA bekommt man zwar mehr Geld, aber kaum Loyalität. Und man muss vor Ort sein, sonst funktioniert es auch mit dem Geld nicht.“ Als ehemaliger Unternehmensberater hat er mit zwei Kollegen zunächst ohne Investorenbeteiligung gegründet. Seine Ziele unterscheiden sich von denen anderer Start-ups: „Ein Exit steht nicht auf dem Programm. Uns geht’s um das Produkt.“ Und das ist im Zeitalter des Internet of Things immer wichtiger.Die Menschen sind die FirmaDeswegen ist gerade die Lage in der globalen Start-up-Peripherie (nach Silicon Valley-Maßstäben) für Hummel etwas sehr reizvolles. Köln ist dafür der perfekte Standort: Berlin sei bisweilen viel zu „unstet und overhyped“. Mit Vodafone in Düsseldorf und der Telekom in Bonn gibt es in NRW Kunden und Netzwerke, die tragen. Die ParStream-Entwickler haben im Schnitt dreizehn Jahre Entwicklungserfahrung in c++, wenn sie von einem dieser großen Unternehmen zu ParStream kommen.“#Big-Data Analytics. ParStream ist der Riese aus der Nachbarschaft. “ Twittern WhatsApp„Wir glauben an räumliches Beisammensein“, bestätigt Hummel. Von fünfzig Mitarbeitern arbeiten dreißig im technischen Bereich, zehn im Vertrieb und zehn in der Verkaufsvorbereitung und der Administration. Damit ist das Unternehmen extrem entwicklungsfixiert und hat trotzdem in Deutschland keine Probleme, neue hochqualifizierte Mitarbeiter zu finden. Alle drei Monate ein neuer Start-up-Preis, ein spannendes und relevantes Produkt und Kunden, die es laut Hummel nicht fassen konnten: „Das ist ja der Hammer! Ich kann ein völlig neues Produkt bauen und Kundenbegeisterung aufbauen.“ Freude macht auch immer wieder die Reaktion der Fachleute, die staunten: „So etwas geht doch gar nicht“. Es ist diese Faszination, die neue Programmierer zu ParStream führt und das Unternehmen vom Silicon Valley abhebt. Der Kampf um die Köpfe ist in Deutschland definitiv erträglicher für ein Unternehmen wir ParStream, als Platzhirsch und lokales Fenster in die Globale Entwicklerlandschaft. „Wir finden hier alles, was wir brauchen.“ In den USA wären nur zehn Angestellte im technischen Bereich und vierzig im Vertrieb. ParStream aber könne es sich leisten, mehr auf Entwicklung guter Lösungen zu setzen. Keine zwei Sätze später wird jedoch klar, dass das doch nicht stimmt.Die Start-up-Förderung funktioniert nicht„Gerade in der Gründungsphase hat uns das wirklich gefehlt, denn wir wollten uns selbst hoch ziehen und nicht von Investoren reinreden lassen.“ Der Gründer bemängelt gigantischen Verwaltungsaufwand für Fördermittel. „Und jetzt warten wir noch auf das vom BMWI zugesagte Geld vom letzten Jahr. Wir wurden vertröstet, weil dieses Jahr das Geld nicht da ist. Wir sollen nächstes Jahr nachfragen.“ Und die Kunden. Die könnten in Deutschland etwas interessierter sein – die hiesigen Unternehmen sind sehr zurückhaltend und erkennen bisher nicht das Potential von Big Data. Da hat es ParStream in anderen Ländern leichter.Kooperationen mit der Universität Sussex und Prof. Markl von der TU Berlin, einem führenden Datenbankexperten, und einem Beirat aus US-Experten bringen dem Unternehmen Risikominimierung. Die braucht es, um eine erfolgreiche Entwicklung weiterzuführen und das Produkt um neue Anwendungen zu erweitern, „die noch niemand auf der Welt erdacht hat“. Entweder man lerne so von den Fehlern anderer oder entdecke neue Möglichkeiten, die noch keiner hatte. Dafür ist dann der Austausch mit den neuen ständigen Mitarbeitern in den USA wichtig. Die kümmern sich um den Vertrieb in der Bay-Area von San Francisco und die neusten Trends der Branche.ParStream ist eine führende Platform für Echtzeit-Big-Data-Analysen.;https://bigdatablog.de/2014/08/19/how-2-b-big-in-big-data/;BigDataBlog;Tobias Hauck
01. Aug 14;" Retweets,      ""Music Analytics. Hören, wie das Gras wächst.""";Wer dachte, die Zeiten penetranter Radiowerbung à la Seitenbacher, Hornbach und Mömax seien mit Spotify und anderen Streamingdiensten vorbei, der irrt. Der genusssüchtige Spotify-Gratisnutzer und geneigte Klassikliebhaber wird zwischen den Sätzen eines Bach-Violinkonzertes durch Ballermanhits geschockt: Werbung. Inklusive der penetrantesten Refrains. Dass das in Zeiten von Service-Customization und Empfehlungsalgorithmen nötig ist, verstört den Geigenfreund – gelinde gesagt.Big Data krempelt die Musikindustrie um, seit sich diese auf die Digitalisierung eingelassen hat. Dafür haben im letzten Jahr mehrere Analyseunternehmen den Besitzer gewechselt: Spotify hat The Echo Nest für 100 Mio. Dollar gekauft. Music Metrics war einer der Pioniere. Der Mitbewerber Music Analytics wurde von Techcrunch zuletzt auf 1.8 Milliarden Dollar Marktwert geschätzt. Aber warum genau?Streamingdienste sind das neue Zuhause der Musik. Ein Browser ähnlich Firefox oder Chrome – allein für Musik. Alle Alben, alle Richtungen, jede Epoche. 20 bis 30 Millionen Stücke. Die annähernd erschöpfenden Musiksammlungen sind ein Panoptikum: Jeder Klick, jede Cursorbewegung, jede Songauswahl, Sucheingabe und Songunterbrechung, Lautstärkeregelung und jedes Nutzerprofil sind auswertbar: Spotify macht in Echtzeit sichtbar, was bisher verborgen war: Die ganz private Nutzung von Musik. Und die verändert sich rasant – jetzt steht viel auf dem Spiel. Der nächste Nummer Eins-Hit. Die Gewinnung zahlender Kunden für die Musikflatrate oder eben die Optimierung von Werbung und Empfehlungen.Wie weitreichend die Folgen schon heute sind, zeigt ein Blick in die Musikindustrie. Als Künstler gewinnt man die Möglichkeit, live zu sehen, welche Songs wo wie populär sind. Man wird demographische Informationen über die Fans bekommen und so die immer wichtiger werdenden Live-Auftritte strategisch und zielgenau planen. Die Verknüpfung der Dienste mit Facebook und Twitter demaskiert das Publikum und führt es geradewegs in eine noch immersivere Klangwelt.Aus Rezensionen, Konzertverkäufen, Radio, Streaming, Pirateriedaten natürlich der Social Media-Performance eines Künstlers lässt sich die Entwicklung von Songs ableiten. Der Nummer-1-Hit in drei Monaten ist mit 90-prozentiger Wahrscheinlichkeit schon bekannt. Ebenso wie die Spotify in der Lage war, die Grammy-Gewinner 2013 mit einer Fehlerquote von 33% vorherzusagen. Problemlos können Versuchs- und Kontrollgruppen gebildet werden, um auszuprobieren wir ein neues Webdesign, ein neuer Künstler etc. wo ankommt.Spotify-Nutzer hinterlassen 600 GB an Daten – pro Tag! Und 28 Petabyte ist die Speichermenge der vier Datencenter des Unternehmens. Too Big Data um mit herkömmlichen Methoden unter Zeitdruck auszuwerten. Luigi heißt die Hauseigene Lösung: Ein in Python geschriebene Datenflussbett, dass in der Lage ist viel zu kanalisieren, zu steuern und auszulesen. Die meisten der Daten sind Nutzerbezogen und erlauben es, individuelle Playlists zu empfehlen und Geschäftsentscheidungen zu treffen. Der große Streamingpool schwächt so zwar die Marktmacht der Plattenfirmen – sie können keine exklusiven Einzelpakete und CDs mehr verkaufen – aber dafür gewinnen sie Einblicke in aktuellste Trends. Der richtige Künstler, der richtige Song – heute eine Frage er Datenauswertung und der Experimentierweise.Zwischen 20 und 30 Millionen Musikstücke haben die Anbieter Spotify, Simfy, Rdio, Deezer und Napstar im Angebot. Und es gibt noch mehr. Die größten haben 40 Millionen Nutzer – zahlende und nichtzahlende Kunden – die dafür Werbung und bestimmten Restriktionen ausgesetzt werden.;https://bigdatablog.de/2014/08/01/music-analytics-hoeren-wie-das-gras-waechst/;BigDataBlog;Tobias Hauck
22. Jul 14;" Retweets,      ""Geschnitten Brot sieht anders aus – Widerstände im Data-Business""";Harte Nüsse. Das Hochlohn- und Qualifiziertendeutschland tut sich bei der Nutzung von Big Data-Potentialen schwer. Dabei sehen Konzerne, Start-Ups, Forscher und Kommunen durchaus die strahlende Zukunft, die ihnen gemalt wird. Sie alle wollen eine Scheibe vom Kuchen. Aber sie sind hierzulande bisweilen nicht engagiert genug, sich zu informieren und die richtigen Angebote zu finden – oder dürfen es schlicht nicht.Paragraph 75 des zehnten Sozialgesetzbuches trennt Menschen und Welten. Der Paragraph trennt die Forscher von Millionen von Patientendaten und anderen Datensätzen, die recht leicht und durchaus anonym Fakten über Krankheitsverläufe andere sozio-demografische Informationen bereitstellen. Ohne “konkreten Verdacht” darf an diesen Datenbanken nicht geforscht werden. Einwilligungen müssten von Betroffenen neu eingeholt werden, und “Genehmigung durch die oberste Bundes- oder Landesbehörde, die für den Bereich, aus dem die Daten herrühren, zuständig ist” ebenso. Das ist ein großer Standortnachteil für Forscher wie Iris Pigeot vom Leibnitz-Institut für Präventionsforschung und Epidemiologie BIPS in Bremen. Sie hat die 17 Millionen Versichertendaten im Institut – und darf sie nicht nutzen. Auf ähnliche Probleme stößt aber auch jeder andere sozialbezogene Forscher. Ökonomen und Sozialwissenschafter mussten schon von je her größere Bürden auf sich nehmen als ihre amerikanischen und britischen Kollegen. Ob Datenbanken oder Experimente in Soziallabors: Gesetzliche Schranken erfüllen ihren Dienst viel zu gut. So gut, dass sich wissenschaftliche Mitarbeiter oft mit einem Fuß im Knast sehen – und mit dem anderen im Arbeitsamt. Denn das Risiko, unbeabsichtigterweise sensible Daten in Arbeiten mit eigentlich zusammenfassendem Charakter mitzuveröffentlichen, ist nicht gering.Das FDZ in Nürnberg, Forschungsdatenzentrum der Bundesagentur für Arbeit (BA) im Institut für Arbeitsmarkt- und Berufsforschung, legt neben ausführlichen vertraglichen Verpflichtungen Stapelweise Datenschutzrichtlinien vor. Jedes Forschungsprojekt, dass auf die Daten zugereifen möchte, hat sich und die genauen Analyseschritte im voraus(!) anzugeben und exakt zu befolgen. Da sich neue Analyseschritte aber nicht vorhersehen lassen, ist für jeden ein gesonderter Antrag fällig, der bearbeitet und beschieden werden muss. Bürokratie galore! Das dürfe “aber nicht über die Tatsache hinwegtäuschen, dass Big Data für die empirische Sozialforschung und viele andre Disziplinen ein bisher weitgehend ungenutztes Potenzial darstellt“, sagt York Sure-Vetter vom GESIS-Institut der Leibnitz-Gesellschaft dem Scinexx.Neben dem Datenschutz behindern durch Lizenzen geschütze Datenbanken oder der in der Gesellschaft eher unbekannte Nutzen von Big Data-Anlysen den Betrieb. Unternehmen kontrollieren den Zugang zu Daten – manchmal selbst dann, wenn sie die Daten im Auftrag oder mit im Kontext des Staates gesammelt haben. Das gefährdet sowohl die wissenschaftliche Praxis als auch die Innovationsfreudigkeit von Unternehmen. Eine überflüssige Barriere. Andererseits gebe es Datenbanken aber heute in einen Überfluss, der nicht einmal ansatzweise durch die wenigen Forscher genutzt werden kann.Der Astronom Alex Szalay sucht in den Datenbergen seiner Teleskope und Sensoren nach Mustern, Korrelationen und neuen Formen. Früher ist man dabei gezwungenerweise deduktiv vorgegangen, also von einer theoretischen Forschungsfrage ausgehend “ins Feld”, um sie an der empirischen Realität zu testen. Heute werden Daten en masse auf die Server geschoben und erst einmal veröffentlicht. Im zweiten Schritt können sich ganz unabhängig von den Datensammlern Experten, Fachfremde und Laien an die Analyse und Interpretation machen. In den sozialwissenschaftlichen Disziplinen ist so ein Vorgehen kaum denkbar.Eine globale Initiative für allgemeinverfügbare Datennutzung ist Open Data. Sie kämpft dafür, dass auch Fachfremde aus anderen Disziplinen mitzuarbeiten und Erkenntnisse zu produzieren. Die Crowd wird dabei zunehmend eingebunden. Gamification ist der nächste Schritt, der die Mitmach-Analyse auch für unattraktive Aufgaben interessant machen soll. So soll auf der Basis eines gesellschaftlichen Konsensus eine verlässliche Arbeitsgrundlage entstehen. Ohne Unsicherheiten bei Datenerhebung und Veröffentlichung.Daten müssen frei und offen sein, damit sie ihre Schätze freigeben und dort verfügbar sind, wo sie gebraucht werden.Big Data kommt für Alex Szalay deshalb auch außerhalb von Natur- und Sozialwissenschaft eine große Rolle zu, was ihn zu interdisziplinären Spekulationen veranlasst. Die Idee: Big Data-Analysen können live und ähnlich eines Mikroskops jede medizinische Analyse und Therapie begleiten, sodass man nicht nur das Kleine sehe, sondern auch den großen Überblick über die vielen alten und neuen Krankheitsbiografien, Befunde und anatomisch-biologischen Eigenheiten des vorliegenden Patienten erhalte. Was wäre, wenn einem ein Bildschirm oder eine Datenbrille zu jedem Schritt automatisch die richtigen Statistiken, Wahrscheinlichkeitsanalysen und Alternativmedikationen auf der Basis aktueller Patientenewerte anzeigen könnte? Ebenfalls ein Fall von Datenschutz, der persönlichen und gesellschaftlichen Nutzen verhindert. Die Bedenken sind ernst zu nehmen – der Blick in die USA ist jedoch immer häufiger neidvoll.Ähnlich sehen das Big Data-Experten, die es schwer haben, ihre Dienste und Visionen an die deutsche Wirtschaft zu vermitteln. Man sei im internationalen Vergleich “zwei Jahre zurück”, sagt Michael Hummel, Mitgründer des Kölner Big Data-Startup Parstream. Er, aber auch andere Firmen und Initativen wie Attivio versuchen, die Möglichkeiten von Big Data an Organisationen und Institutionen zu vermitteln. Mehrwert, Perspektiven, Kosten und Techniken. Dabei stoßen sie insbesondere beim deutschen Mittelstand nur sehr mühsam durch die Wände in den Köpfen.;https://bigdatablog.de/2014/07/22/widerstaende-im-data-business/;BigDataBlog;Tobias Hauck
16. Jul 14;" Retweets,      ""Bereit für Big Data?""";"Big Data – an diesem Thema kommen Unternehmen nicht mehr vorbei. Big Data verschafft einen Blick in die Zukunft, ob in der Industrie, der Logistik oder in der Werbung. Doch nur wenige Unternehmen sind überhaupt bereit für die ganz großen Daten.Um Big Data nutzbar zu machen, bedarf es Platz zu schaffen für die großen Datenmengen. Doch das Speichern ist nicht das, was Big Data auszeichnet. Es ist nur der erste Schritt: Wer die Daten lediglich sammelt, wird weder von schnelleren Entscheidungen und Prozessen, noch von einer besseren Kenntnis des Kundenverhaltens oder einer gesteigerten Produktivität profitieren. Der Trumpf liegt in der Analyse der Daten.Alles steht und fällt mit dem richtigen TeamAber welche Daten sollte man wie verknüpfen und mit welcher Technologie? Das Ziel ist es, aus Rohdaten unternehmensrelevante Informationen zu gewinnen. Doch darin liegt auch die große Herausforderung: Neben technischem Know-how fehlen in firmeninternen IT-Abteilungen vor allem die nötigen fachlichen Skills im Umgang mit den großen Datenmassen. Bisher konnten lediglich 12 Prozent der Unternehmen eine Big-Data-Initiative auf die Beine stellen und als festen Bestandteil der Unternehmensprozesse etablieren (BARC Studie „Big Data Analytics“, 2014).Kein Big Data ohne CloudEin weiterer Aspekt sind die Kosten. BI-Lösungen, die auf Hadoop oder In-Memory-Computing basieren, sind mit hohen Investitionen verbunden. Der Schlüssel liegt deshalb in skalierbaren IT-Ressourcen aus der Cloud. Erst mit Diensten wie Software-as-a-Service oder Hadoop-as-a-Service wird Big Data wirtschaftlich nutzbar, da die notwendigen High-Performance-Technologien nach Bedarf bezogen werden.Aber: Unternehmen müssen sich schon im Vorfeld damit beschäftigen, wo erfolgversprechende Anwendungsgebiete liegen und welche Fragen sie beantworten möchten. Beispielsweise bietet In-Memory-Computing eine besonders hohe Geschwindigkeit in der Datenverarbeitung und ermöglicht etwa Risikoanalysen in Echtzeit; Hadoop hingegen stellt die Weichen für vorausschauende Prognosen im Energiebereich oder der Produktentwicklung. Am Anfang von Big Data steht demnach immer eine konkrete Strategie.Nicht zu unterschätzen: Datensicherheit und -schutzCompliance- und Governance-Richtlinien sind elementarer Bestandteil jeder Big-Data-Strategie. Besonders für die Analyse von Personendaten, zum Beispiel aus Social Media, gelten strenge rechtliche Vorgaben. So muss durch die Anonymisierung eine Identifikation der Personen aller Daten ausgeschlossen werden. Auch die IT-Infrastruktur muss Datenschutzanforderungen Stand halten und entsprechend gesichert sein. Hier kommen nur IT-Services aus der Private Cloud infrage, die einen hohen Sicherheitsstandard gewährleisten.FazitBig Data bietet enormes Innovationspotenzial für die deutsche Wirtschaft, sei es im Kundenservice, in der Produktion (Stichwort: Industrie 4.0) oder im Marketing. Doch Unternehmen müssen vorbereitet sein und sollten folgende Schritte auf keinen Fall auslassen:Die nötigen Big-Data-Kompetenzen in der eigenen IT-Abteilung bündeln oder auf das Expertenwissen Dritter zurückgreifen.Den Schritt in die Cloud wagen, um Investitionskosten gering zu halten.Die Aspekte Datenschutz und -sicherheit bereits in der Strategie verankern und ebenso bei der Wahl des Providers beachten.Mehr zum Thema:http://cloud.t-systems.de/cloud-themen/big-data";https://bigdatablog.de/2014/07/16/bereit-fuer-big-data/;BigDataBlog;Dr. Ferri Abolhassan
01. Jul 14;"     ""Wissen, wo man steht – Das Tool Followerwonk""";"Big Data schafft Probleme. Denn “big” heißt in der Praxis meistens “too big”. Als Unternehmen beißt man sich manchmal die Zähne daran aus, wie die Daten strukturiert und genutzt werden können. Dabei geht es um das Kerngeschäft: Das Verständnis des Konsumenten und der Konkurrenz zu verbessern. Unzählige Tools und Services können dabei helfen.Erst letzte Woche, während ich an einem Projekt für einen Klienten gearbeitet habe, konnte ich wieder feststellen, wie schnell es möglich ist, interessante „Insights“ zu erhalten. Innerhalb eines Tages konnte ich dem Software-Start-up eine genaue Übersicht liefern, wo sich die eigenen Social-Media-Follower im Vergleich zu denen der Konkurrenz befinden. Besonders interessant für das junge Unternehmen aus Dublin war, dass 60 Prozent ihrer Follower aus Berlin kamen. Besser noch: Es handelte sich größtenteils um Early Adopters, also neuen Trends und Innovationen sehr aufgeschlossener Meinungsführer. Beste Voraussetzungen zur Kundenakquise. Weitere Recherche zeigte, dass breites Interesse an dem Produkt in Deutschland besteht – nun expandiert das Team mit seiner Payroll-Software nach Berlin.Ich arbeitete mit Followerwonk, ein einfaches Tool mit vielen Möglichkeiten. Das Tool und die Schritt-für-Schritt-Anwendung wird in der Präsentation unten vorgestellt. Hier aber zunächst zu den Vorteilen: Schnell aus den online gesammelten Daten Erkenntnisse gewinnen. Aufschlussreiche Informationen konnte ich schnell erhalten:Woher kommen die Follower meiner Konkurrenz? Hieraus habe ich einen potentiellen neuen Absatzmarkt der Firma erkennen können.Wie alt sind die Accounts der Follower? Sind die Follower Early Adopter oder Late Follower?Wer sind die Meinungsführer in der Branche, an der meine Firma interessiert ist? Wie kann ich diese kontaktieren und auf Interaktion abzielen?Besonders interessant finde ich, Meinungsführer und Trendsetter schnell erkennen zu können. Viele Unternehmen haben Schwierigkeiten diese zu identifizieren, um dann später mit diesen zu interagieren. Mit dem Einsatz von Tools wie Followerwonk ist dies einfach und schnell möglich – und das kostenlos. In dem oben beschriebenen Projekt habe ich eine Excel-Liste mit allen „Opinion Leadern“ aus Berlin angefertigt, die sobald das Produkt für den deutschen Markt fertiggestellt ist, über Social Media und darüber hinaus angesprochen werden können.Abschließend noch eine EmpfehlungNehmt Euch immer die Zeit, die neu gewonnenen Erkenntnisse zu screenshotten und in einer Präsentation zu analysieren. Dies gewährleistet zum Einen, dass die gewonnen Erkenntnisse jederzeit an Kollegen weitergegeben werden können. Zum Anderen kommen noch mehr Ideen zustande, wenn man sich in der Gruppe und immer wieder intensiv mit der Thematik beschäftigt.// <!CDATAjQuery('#slideshare_embed_16124291').click(function() {    if(confirm('Sie sind dabei eine SlideShare Präsentation zu öffnen. SlideShare nutzt Tracking-Technologien, die Ihre IP Adresse an Dritte weitergibt. Wir als Seitenbetreiber haben auf die Tracking-Methoden des externen Anbieters leider keinen Einfluss. Sollten Sie damit nicht einverstanden sein, wählen Sie bitte abbrechen!')){        jQuery('.wp-image-2894').remove();        jQuery('#slideshare_embed_16124291').append('');    }else{        alert('Die SlideShare Präsentation finden Sie unter http://www.slideshare.net/slideshow/embed_code/28163395');    }});// &gt;";https://bigdatablog.de/2014/07/01/wissen-wo-man-steht-das-tool-followerwonk/;BigDataBlog;Julian Gottke
25. Jun 14;"     ""Big Dating – Wenn Daten über Liebe sprechen""";ElitePartner, Neu.de und Parship: Hierzulande werden Dating-Portale gerade von junger wilder amerikanischer Konkurrenz überrollt. Was die kann, zeigen Hacker wie Chris McKinlay oder der aktuelle Marktwert von Tinder. Diese neue App mit dem Prinzip “Hot Or Not” ist angeblich 500 Millionen US-Dollar wert, bei 35 Mitarbeitern. Definitiv hot! Der emotionale Wert dieser Big-Dating-Konzepte ist da schon schwerer zu bestimmen. Chris McKinlay ist dem auf den Grund gegangen und hat OKCupid gehackt – auf der Suche nach seiner großen Liebe.Der erste Blick zeigt auch bei OKCupid ganz herkömmliche Nutzerprofile. Die bieten aber nur einen kleinen Teil der vom Nutzer hinterlegten Informationen. Im Hintergrund matcht der Computer die Mitglieder auf der Basis von hunderten freiwilliger Antworten auf “Schlüsselfragen”.Big Data ermöglicht Big Dating„Do you like horror movies?“„Have you ever travelled around another country alone?“„Wouldn’t it be fun to chuck it all and go live on a sailboat?“Wenn zwei Nutzer auf diese Fragen die gleichen Antworten geben, ist die Chance auf eine zukünftige Partnerschaft am höchsten. Der Blog des Unternehmens enthüllt bizarre Einsichten in Vorlieben und Selbstverständnis der Menschen. Bei Frauen wird ein signifikanter Zusammenhang von Fitnesstraining mit Orgasmushäufigkeit festgestellt – mehr Training macht mehr Orgasmen, oder umgekehrt. Anderes Beispiel: vegetarische Ernährung und Bereitschaft zum Oralsex scheinen miteinander zu korrelieren. Ebenfalls vielsagend: Ein hohes pro-Kopf Einkommen und ein freizügiges Liebesleben gehen häufig miteinander einher. Die Statistiker der Liebe haben ein spannendes Feld beackert. Übrigens: je kontroverser die Frauen bei den Männern ankommen, umso mehr Verehrer können sie auf sich vereinen. Big Data-Analysen hatten hier ermöglicht, pro Fragestellung hunderttausende Antworten und Metadaten auszuwerten. Dass einem selbst das aber nicht hilft, wenn der potentielle Partner nicht ebenfalls die richtigen Fragen im OKCupid-System beantwortet hat, hat den Mathematiker McKinley zur größtmöglichen Professionalisierung getrieben.Nachts lag der Analyst auf einer Minimatratze im Büro und hat den Großrechner der Uni das Portal auslesen lassen. Seine zwölf Profile schauten sich tausende von Frauen an, beantworteten randomisiert – also zufällig und breit gestreut – alle Fragen des Katalogs und analysierten, wie dies die Trefferwahrscheinlichkeit für Übereinstimmungen, Interessen etc. beeinflusste. Die Algorithmen gaukelten den Sicherheitsschranken des Systems vor, authentische Nutzer mit einer scheinbar schlaflosen Nacht zu sein. McKinlay konnte so Cluster identifizieren, also Frauentypen, die besonders viele Eigenschaften und Wünsche teilen. Optimale Zielgruppen sozusagen. Von den gefundenen interessierte er sich jedoch nur für zwei. Für diese erstellte der Hacker das jeweils optimale Profil mit Antwortenkatalog. Er log dabei angeblich nicht, betonte und gewichtete seine Antworten aber auf der Basis seiner Analyse. Ein Nutzerkonto zeigte ein Foto von ihm beim Klettern. Das andere beim Gitarrespielen. Die Zielgruppenanalyse empfahl es so.80 Dates folgten, trotz enormer Übereinstimmungszahlen waren die Begegnungen jedoch ernüchternd. Erst nach vielen Anläufen fand er eine Frau bei 91% Übereinstimmung, mit der er Kontakt hielt und der er, wie wired.com berichtet, einen Antrag machte. Das System zeigt: die Masse an Daten, die das Portal zur Verfügung hat, bieten tiefere und robustere statistische Einblicke in die menschliche Psyche als jede Volkszählung. Es entstehen enorme Datenmengen, die den Administratoren auch klar zeigen, ob die eigenen Angaben wirklich das widerspiegeln, was das eigene Suchverhalten auf den Dating-Seiten enthüllt. Amazon sagt: “Kunden, die dieses Buch kauften, interessieren sich auch für XY”. OKCupid ist schon heute in der Lage, ähnliche Aussagen über Menschen zu treffen. Die Macher wissen schon heute: Männer lügen meist bei Alter, Größe und Einkommen – Frauen bei Alter, Gewicht und Figur.Die Selbsteinschätzung der Menschen ist miserabel. In Zukunft wird die Einbettung von Facebook-Likes, dem Nutzungsverhalten von Spotify und Streaming-Diensten wie Netflix womöglich direkt Auskunft darüber geben, welche Präferenzen man wirklich hat. Die wenigsten jedenfalls treten als die Schlagerliebhaber auf, die sie sind. Beobachtungen sind die Zukunft, Selbstauskünfte kaum relevant. Und zum übernächsten Schritt wird schon angesetzt. Die App Hinge durchforstet die eigenen Facebook-Kontakte bis zum dritten Grad: Freunde von Freunden von Freunden. Sie schlägt potentielle Partner vor und wie man sie kennenlernen kann. Damit soll eine größere Authentizität, Sicherheit und Motivation erreicht werden. Wer gemeinsame Bekannte hat, ist ehrlicher und motivierter.Schließlich aber müssen auch weiterhin Eigeninitiative und echte Zuneigung da sein, um aus einem Match eine andauernde Begegnung zu machen. Ob der Gedanke, schon den mathematisch perfekten Partner gefunden zu haben, eine ähnliche Bindungskraft entfalten kann, wie eine zufällig-überraschende Begegnung? Ob die Dynamik eines Abendessens ohne vorherigen Abgleich schlechter sein muss? Angesichts der enormen Partner-Auswahl im Netz und einem nie abreißenden Strom neuer Kontakte sind neue Probleme und Hoffnungen entstanden.Wird Online-Dating dank Big Data zum zuverlässigen Zulieferer neuer Lebensabschnittspartner?Die vielen in Liebesdingen bemühten Start-ups wollen genau das unter Beweis stellen. Und sie sind sich des Wertes ihrer Daten bewusst – es wird so viel gesammelt wie möglich. Andererseits sind manche auch feinfühlig im Geschäft. Sie geben Garantien ab. Klare Absagen an Datenverkäufe und Initiativen gegen poröse Privatsphäre. Sie glauben: Das wird im neuen System das zentrale Argument für Kundenbindung sein. Denn wenn eine App wie Tinder in Deutschlang ohne Werbung in wenigen Monaten rund eine Million regelmäßiger Nutzer hat, die ihre Facebook-Daten für das Dating öffnen, dann steht Big Dating eine spannende Zukunft und stehen vielen von uns interessante Begegnungen bevor.Hier noch Daten, die von der Berkeley University dankenswerterweise zusammengefasst wurden.Big Dating Infografik, Quelle: Berkeley Datascience;https://bigdatablog.de/2014/06/25/big-dating-wenn-daten-ueber-liebe-sprechen/;BigDataBlog;Tobias Hauck
13. Jun 14;"     ""Data Governance – eine europäische Herausforderung""";Egal ob Demokratie oder Diktatur: Staaten haben Geheimnisse. In den USA sind bis zu 4,9 Millionen Menschen Geheimnisträger. Informationen, die von Behörden als “Secret” klassifiziert wurden, könnten also von bis zu 4,9  Millionen Menschen eingesehen werden. Manche dieser Menschen haben zwangsläufig einen größeren Überblick: Die Administratoren der Informationsschnittstellen. Edward Snowden gehörte zu dieser wachsenden Gruppe mit Zugang zu Schnittstellen, die mehr Informationen bündeln, als die konkrete Aufgabe benötigt. Das geht kaum anders: Zugriffsrechte zu verwalten ist ein enormes Problem. Eines, das immer mehr durch Outsourcing von IT-Aufgaben verschärft wird. Eines, das durch nachhaltige Data Governance auch für Privatpersonen und die Wirtschaft geregelt werden muss.“Data Governance is a system of decision rights and accountabilities for information-related processes, executed according to agreed-upon models which describe who can take what actions with what information, and when, under what circumstances, using what methods.” – The Data Governance InstituteData Governance ist kein starrer Begriff Zusammen mit der technischen und gesellschaftlichen Entwicklung wandelt sich Data Governance. Dass dabei die Ansprüche, die an Data Governance gestellt werden, sehr unterschiedlich sein können, zeigt, wie umkämpft das Feld ist. Es fehlen formale Lösungen für informelle Probleme: Vertrauensfragen, Verantwortlichkeiten, Effizienz, Entscheidungsfindung, rechtliche Absicherung, Überblick und Kontrolle.  Eine häufige Herausforderung ist die Klassifizierung von Informationen und Personen nach Vertraulichkeit. Große Organisationen stellt das vor kaum lösbare technische und strukturelle Probleme. Mitarbeiter erhalten entweder keinen Zugang zu den Unternehmensdaten , die sie benötigen und am besten nutzen können oder müssen sich mir schlechterer Datenqualität und und viel Bürokratie herumschlagen. Frust macht sich breit.Deswegen setzten Firmen jetzt auf eigene Datenstrategien. Sie fragen: Was speichern? Wie speichern? Wie lange speichern, sodass Verfügbarkeit, Zugriff und Auswertung so funktionieren, wie man es will? Also reibungslos, und ausschließlich einsehbar für den Besitzer?Daten sind der Rohstoff für die Produkte der Zukunft. Das Smart Car weiß genau, wo du wie schnell fährst.  Es kennt Wetter und Verkehr. Diese Daten sind für viele von Interesse, haben einen hohen gesellschaftlichen Nutzwert und sind kritisch – weil privat. Kein Wunder also, dass Datenpolitik auch Abseits von Kundeninteressen, Cyberkriminalität und Geheimdiensten in die erste Reihe wichtiger Themen vorrückt. Bei der Europawahl standen nicht nur Martin Schulz und Jean-Claude Juncker zur Wahl, sondern auch mehr oder weniger zaghaft formulierte Datenpolitiken.Die Piraten wollten ein netzneutrales, barrierefreies, sicheres und anonymeres Internet und hatten damit viele gute Argumente. Trotzdem: Sie überzeugte nicht. Nur 1,3 Prozent der Deutschen wählten die Partei. Ähnlich sieht es bei der FDP aus, die sich sehr gegen geheimdienstliche Überwachung aussprach. Die SPD hatte eine “Digitale Agenda” – und einen Wahlkampfschwerpunkt bei gänzlich anderen Themen. Konkrete Vorschläge der CDU: Ein europäisches Routingsystem, Vorratsdatenspeicherung und eine Meldepflicht für im Ausland gelagerte Daten waren leider Randnotizen des Wahlkampfes. Die Politik muss antworten geben, die im Europawahlkampf allenfalls zaghaft vorformuliert wurden.Planungssicherheit und Vertrauen in den IT-Standort Europa entstehen so nicht. Besonders sichtbar wird das im Smart Metering-Bereich der Energieversorger. In Deutschland gibt es nur kleine Pilotprojekte, weil unklar ist, welche Technik auf mittlere Frist verbaut werden darf. Gleiches gilt auch intern für Datenbanken – welche ist für welche Daten rechtssicher?  Fraglich! Wie lange muss und darf ein Datum mindestens und höchstens gespeichert werden? Im internationale Vergleich widersprüchlich!Welche Datenqualität ist ausreichend? Welche Sicherheitsstufe für welches Datum? Im internationalen Feld Europas kaum beantwortbar. Und angesichts außereuropäischer Anbieter von Cloud, Analyse und Intelligence ein Sicherheitsrisiko. Es scheint also erkannt worden zu sein, dass Datengesetzgebung kein Spielfeld für rein nationale Initiativen ist. Angesichts dieser Erkenntnis geben die neuen Mitspieler AfD oder der starke Front National Anlass für Bedenken: Der IT-Branchenverband BITKOM warnt vor einer „Fragmentierung des digitalen Binnenmarktes„. Verbandspräsident Kempf warnt vor “digitaler Kleinstaaterei”. Datenschutz funktioniere nur mit gemeinsamen Standards. Die werden von Bürgern und Wirtschaft eingefordert. Und sie sind hart umkämpft.Bei 1,4 Millionen Menschen, die in den USA auf die Hochsicherheitsklasse “Top Secret” zugreifen können, ist es kaum möglich, Kontrolle, Datensparsamkeit und Verhältnismäßigkeit zu überprüfen. Daher scheint selbst in den USA erkannt worden zu sein, dass Data Governance und auch internationale Zusammenarbeit diskutiert werden müssen. Eine Gelegenheit, die europäische Netzpolitiker sich nicht entgehen lassen sollten.;https://bigdatablog.de/2014/06/13/data-governance-eine-europaeische-herausforderung/;BigDataBlog;Tobias Hauck
04. Jun 14;"     ""WM-Tippspiel gegen den Algorithmus asertoni""";Die Fussball WM in Brasilien steht vor der Tür und die ganze Welt ist wieder dem Fieber verfallen. Panini-Bilder, Public Viewing, Tippspiele …! Tippspiele sind auch eine Leidenschaft der Mitarbeiter der aserto GmbH. Das Smart-Data-Team fordert nun die ganze Welt zum Tippen heraus. Doch sie treten nicht selber mit ihren Tipps an, sie lassen einem eigens entwickelten Algorithmus den Vortritt.Wer auch gegen den Algorithmus asertoni antreten möchte kann sich auf http://asertoni.de/tippspiel/ kostenlos registrieren.Der Algorithmus arbeitet auf der Grundlage von Predictive Analytics. Das Smart-Data-Team von aserto arbeitet mit solchen Verfahren ansonsten, um Sachverhalte im Marketing und in der Kommunikation möglichst treffsicher zu prognostizieren. Ob das auch bei der Fußball WM in Brasilien funktionieren wird, werden wir ab dem 12. Juni live miterleben.;https://bigdatablog.de/2014/06/04/wm-tippspiel-gegen-den-algorithmus-asertoni/;BigDataBlog;Jan Golka
04. Apr 14;" Retweets,      ""Crowdsourcing und Big Data""";"Crowdsourcing hat sich in den vergangenen zehn Jahren quasi zu einem Allheilmittel für alle möglichen Probleme entwickelt, die immer noch zu komplex sind, als dass man sie erfolgreich automatisieren könnte. Viele dieser Probleme sind aber für Menschen nicht so schwierig zu bearbeiten. Ein schönes Beispiel ist zooniverse.org. Dort werden „Citizen Science Projects“ vorgestellt, also Projekte für „Bürgerwissenschaftler“, bei denen es um die Einordnung von wissenschaftlichen Bildern geht, wie beispielsweise Galaxien zu klassifizieren oder die Klimageschichte der Erde zu modellieren.Der Klimawandel ist vermutlich die am besten abgesicherte Hypothese in der aktuellen Wissenschaft; keine andere Theorie muss sich ständig so intensiv gegen Versuche wehren, widerlegt zu werden (vielleicht gleichauf mit der Evolutionstheorie, die ja bekanntlich einen ähnlich ideologischen Kampf führen muss). Aber jenseits der Prognose, wie sich das Klima ändern wird und wie dies das Wetter vor Ort beeinflusst, sind uns die wirtschaftlichen Konsequenzen noch wenig klar, die aus den unterschiedlichen Szenarien folgen. Dabei sollte die Wirkung des Klimas auf die gesamte Wirtschaft nicht vernachlässigt werden, wenn man untersucht, wie Rückkopplungs- und Aufschaukelungsprozesse vielleicht in eine Katastrophe münden – oder auch nicht.Wir alle sind KlimaforscherZeean.net ist ein Open-Data und Open-Source Projekt, das auf die wirtschaftlichen Einflüsse des Klimawandels zielt. Datensammeln findet per Crowdsourcing statt – jeder kann Schlüssel-Indikatoren von geo-wirtschaftlichen Zusammenhängen beitragen. Beispiele sind Waren- und Geldflüsse von Angebot und Nachfrage zwischen Regionen und lokal. Diese Daten kann jeder selbst hochladen, wie bei Wikipedia. Und wie bei der Online-Enzyklopädie erfolgt die Überprüfung durch die anderen registrierten Benutzer. Mit den hochgeladenen Daten werden dann Modellsimulationen gefüttert.Das Team hinter Zeean, unter Leitung von Anders Levermann am Potsdam Institute for Climate Impact Research ist direkt mit dem Intergovernmental Panel on Climate Change verbunden, dem IPCC, das für die Vereinten Nationen zentral für Klimaforschung verantwortlich ist und daher als die prominenteste wissenschaftliche Organisation in diesem Bereich gilt.Klimaforschung als VorbildDie Ergebnisse aus Zeean.net sind natürlich nicht auf Klimaforschung beschränkt. Wenn Märkte aus anderen Gründen geschwächt werden oder ganz ausfallen, kann man damit den Effekt auf andere Regionen gut abschätzen. Auf dem Bild oben sieht man zum Beispiel, welche Wirkung der Konflikt in der Ukraine auf die übrige Weltwirtschaft haben kann.Diese Beispiele von Crowdsourcing in der Wissenschaft zeigen, wie man mit Big Date umgehen kann, falls die Daten schlichtweg „zu big“ sind, um erfolgreich mit Computern bearbeitet zu werden. Auf diese Weise sind Crowdsourcing und Big Data komplementär – sie ergänzen sich.";https://bigdatablog.de/2014/04/04/crowdsourcing-und-big-data/;BigDataBlog;Jörg Blumtritt
31. Mrz 14;"     ""Google Suggest: Schöner suchen mit Big Data – Teil 4""";Indikator für das NutzerverhaltenImmer wieder wird behauptet, die Vorschläge von Google lieferten eine exzellente Möglichkeit, Schlüsse über Einstellungen und Verhalten der Netzgemeinde zu ziehen. Das stimmt aber nur sehr eingeschränkt. Erstens gibt es Manipulationsmöglichkeiten, durch Kampagnen oder durch ein manuelles Eingreifen von Google. Zweitens werden in jedem Vorschlag aufgrund der Bayesianischen Algorithmen auch alle Informationen verarbeitet, die Google über den individuellen Benutzer, der die Anfrage stellt, selbst besitzt. Im Zweifel werden diese höher gewichtet als die Durchschnittswerte der Allgemeinheit.Wer wissen will, wie sich andere verhalten, der muss zumindest anonym suchen. Auf jeden Fall lässt sich aus den Ergebnissen mehr auf das Nutzerverhalten schließen als auf die Realität, gerade was negativ besetzte Vorschläge über Personen und Unternehmen angeht. Dies ist eine direkte Folge der Selbstverstärkungs-Effekte, welche die in Teil 3 dieser Serie skizzierten Experimente nachgewiesen haben.Der Preis des KomfortsMit Google ist es im Prinzip wie mit zahlreichen anderen Angeboten im Netz: Dem Zeit- und Komfortgewinn steht stets auch der Preis gegenüber, dass Daten über einen gesammelt werden. Zahlreiche Anbieter versuchen mehr und mehr, den Benutzer aktiv zu beeinflussen.Dieser Handel beginnt aber im Grunde genommen schon, wenn ich überhaupt im Internet surfe, mit EC-Karte zahle, Freunden SMS schicke, anstatt sie persönlich zu treffen und so weiter. Die Vorstellung, hier tatsächlich anonym zu bleiben, ist für die meisten Menschen wohl illusionär. Auch wenn ich Google nicht benutze, sammelt das Netz trotzdem Daten, die potenziell analysiert werden können.Google macht hier also nichts Neues, sondern erweitert nur das, was in der Informationsgesellschaft ohnehin schon gilt: Information ist Macht und wer mehr Daten besitzt, hat auch mehr davon. Diesen Preis zahlt jeder, der auf all die praktischen Erleichterungen der täglichen Internetnutzung nicht verzichten möchte.FazitGoogle Suggest ist nicht kostenlos. Ich bezahle für die Zeitersparnis und die Informationen mit meinen Daten.Google Suggest ist nicht neutral. Ich erfahre nicht das, was ist, sondern das, von dem ein Algorithmus glaubt, dass es mich interessiert.Negativer Selbstverstärkung kann ich mit verantwortungsbewusstem Verhalten begegnen. Das heißt erstens: Vorschläge wie „Promi xy alkoholsüchtig“ nicht allzu ernst nehmen. Zweitens, wenn ich es mir verkneifen kann: nicht darauf klicken.Dieser Beitrag beschließt eine vierteilige Serie über die Chancen und Risiken von Google Suggest.Weiterführende Links:http://www.sueddeutsche.de/digital/umstrittene-autocomplete-funktion-google-ist-nicht-neutral-1.1463525http://www.wordtracker.com/academy/brent-payne-interview;https://bigdatablog.de/2014/03/31/google-suggest-schoener-suchen-mit-big-data-teil-4/;BigDataBlog;Katharina Schüller
27. Mrz 14;"     ""Google Suggest: Schöner suchen mit Big Data – Teil 3""";"Nachteile und RisikenSchlägt Google aufgrund des Suchverhaltens der Nutzer negative Begriffe vor, so kann das  als Rufschädigung ausgelegt werden, obwohl der Algorithmus selbst ja keine bösen Absichten hat. Zahlreiche Prominente oder Unternehmen haben deswegen schon Prozesse gegen Google begonnen, die zumindest in bestimmten Fällen auch gewonnen wurden. In diesem Fall zensiert Google sozusagen die Vorschlagsliste.Umgekehrt könnte eine PR-Firma etwa über soziale Netzwerke die Benutzer zur Eingabe bestimmter Suchbegriffe animieren und so eine Werbekampagne mittels einer Platzierung positiver Assoziationen durchführen. Dies möchte Google ebenfalls verhindern.Entgegen kommt dem, dass Menschen offenbar negative Assoziationen spannender finden und sich diese auch besser merken können. In einem Forschungsexperiment wurde beobachtet, dass die Benutzer ein Suchergebnis um ein Vielfaches häufiger angeklickt haben, wenn der damit assoziierte Begriff negativ konnotiert war. Während die Klickrate für einen positiven Begriff im Mittel bei nahezu 0 Prozent lag, stieg sie bei negativen Assoziationen auf durchschnittlich 5 Prozent.Damit erhöht sich die Häufigkeit solcher Suchbegriffe aber noch; sie wandern in der Vorschlagsliste weiter nach oben. Ich kann dies ausprobieren, indem ich einmal die Namen von Prominenten google – meist sind die Ergänzungsvorschläge nicht besonders schmeichelhaft.Ein anderes Experiment hat untersucht, wie Menschen auf negativ konnotierte Suchvorschläge zu Personen und Unternehmen reagieren. 20 Prozent der Versuchspersonen hatten danach gegenüber den Unternehmen und sogar die Hälfte gegenüber Personen einen negativen Eindruck. Zwischen 25 und 35 Prozent behielten den negativen Vorschlag auch in Erinnerung. Google ist also ein wahres Eldorado für Gerüchte und Negativkampagnen.Bequemlichkeit versus PrivatsphäreEs stellt sich manche sicher die Frage, wie viel Google über den Einzelnen weiß und ob es ein persönliches Profil anlegt. Auch hier bleiben die Details geheim; klar ist jedoch, dass Google die eigenen Suchbegriffe speichert, wenn ich mit dem Konto eines der Google-Dienste angemeldet bin. Auch ohne Anmeldung werden über die IP des Benutzers Daten gespeichert. Jeder wird schon einmal erlebt haben, dass nach dem Besuch einer bestimmten Seite plötzlich an einer völlig anderen Stelle Werbevorschläge eingeblendet werden, die erstaunlich gut zu der vorherigen Seite passen. Das kann ich, je nach Toleranzschwelle, praktisch finden oder mich dadurch verfolgt fühlen.Es betrifft alle Google-Benutzer und lässt sich auch durch das Abmelden nicht beseitigen. Dass Google und zahlreiche andere Anbieter meine Daten sammeln und statistisch analysieren, ist und bleibt wohl eine Tatsache. Es bleibt nur die Möglichkeit einer wechselnden IP über Proxy-Server, aber dabei werden schnell technische Dimensionen erreicht, die die meisten Alltagsbenutzer vermutlich überfordern.Dieser Beitrag gehört zu einer vierteilige Serie über die Chancen und Risiken von Google Suggest.Weiterführende Links:Studien zur (negativen) Wirkung:http://www.seo-united.de/blog/google/studie-zur-autocomplete-funktion-von-google.htmhttp://www.dailymail.co.uk/sciencetech/article-2326101/Is-Google-making-RACIST-researchers-claim-auto-complete-function-perpetuates-prejudices.htmlhttp://www.research.lancs.ac.uk/portal/en/clippings/daily-mail-how-googles-autocomplete-reveals-racist-sexist-and-homophobic-searches-researchers-claim-search-function-perpetuates-prejudices(d94eb7c5-8beb-4f71-b0a4-62b094d37b05).htmlhttp://www.revolvermaenner.com/sports-reputation-management/reputation-von-top-fussballern-im-internetEntscheidung des Bundesgerichtshofs:http://www.telemedicus.info/article/2574-BGH-Die-Grenzen-der-Rechtmaessigkeit-bei-Googles-Autocomplete.htmlhttp://www.zeit.de/digital/datenschutz/2013-05/google-autocomplete-bgh-urteil";https://bigdatablog.de/2014/03/27/google-suggest-schoener-suchen-mit-big-data-teil-3/;BigDataBlog;Katharina Schüller
24. Mrz 14;"     ""Google Suggest: Schöner suchen mit Big Data – Teil 2""";"Vorteile und ChancenWer einen Namen aufgeschnappt, aber den genauen Zusammenhang vergessen hat, dem hilft Google Suggest mit häufig damit verbundenen Begriffen weiter. Auch wer zu einem Thema recherchieren muss und keine Ahnung hat, wo er beginnen soll, den kann die Funktion auf eine erste Spur bringen.Nutze ich Google Suggest jedoch als einziges Navigationssystem für die Recherche, so werde ich eher über die Autobahn geleitet als über die verborgenen Seitenpfade. Die Chance, einzigartige Schätze im Netz zu entdecken, ist dadurch naturgemäß geringer.Umgekehrt lässt sich rasch feststellen, welche Assoziationen zu bestimmten Begriffen besonders populär unter den Benutzern sind. Kürzlich erwuchs daraus sogar eine Kampagne der Vereinten Nationen, nachdem die Vorschläge zu „Women should“ nicht besonders von emanzipatorischen Gedanken geprägt waren.Wenn künstliche Intelligenz versagtDer Algorithmus ist kein denkender Mensch; er kann nur auf die Eingaben der Benutzer reagieren. Daraus ergeben sich teils sinnlose und oft auch sehr komische Vorschläge. Bei der Autokomplettierung von „Ich hasse es wenn“ handelt es sich um ein regelrechtes Netzphänomen, welches zu solchen Vorschlägen wie Ich hasse es wenn Jesus Dinosaurier in mein Haus reitet führen kann. Möglicherweise sind bestimmte Vorschläge also das Ergebnis einer gezielten wiederholten Eingabe der Netzgemeinde mit nicht ganz ernsten Absichten. Deutlich wird also: Es steckt im Normalfall keine menschliche Intelligenz hinter den Vorschlägen. Angesichts der Datenmengen wäre das ohnehin unmöglich.Allerdings bedeutet das nicht, dass die Ergebnisse von Google nie von Menschen beeinflusst sind. Beispielsweise ist die Funktion bei vulgären Suchbegriffen deaktiviert. Dies ist der Grund, warum man zu Manuel Neuer zahlreiche Suchvorschläge erhält, während sein früherer Kollege Jörg Butt relativ ergebnisarm bleibt. Dies erklärt sich rasch aus der Bedeutung des Nachnamens im englischen Sprachraum. Auch das oben erwähnte Women should liefert mittlerweile keine frauenfeindlichen Ergebnisse mehr.Doch zumindest kann das Spiel mit Assoziationen einen regnerischen Aprilnachmittag versüßen. Dazu hat etwa die Süddeutsche Zeitung zwei amüsante Artikel veröffentlicht.Dieser Beitrag gehört zu einer vierteilige Serie über die Chancen und Risiken von Google Suggest.Weiterführende Links:Zur UN-Women-Kampagne:http://www.blu-news.org/2013/10/23/bei-autovervollstaendigung-sexismus/http://www.i-ref.de/2013/10/22/women-shouldnt-eine-kampagne-der-un-gegen-sexismus/Spaß mit Autocomplete:http://www.sueddeutsche.de/digital/google-suche-kann-man-eine-meerjungfrau-werden-1.1463568http://sz-magazin.sueddeutsche.de/texte/anzeigen/41730";https://bigdatablog.de/2014/03/24/google-suggest-schoener-suchen-mit-big-data-teil-2/;BigDataBlog;Katharina Schüller
20. Mrz 14;"     ""Google Suggest: Schöner suchen mit Big Data – Teil 1""";"Im Jahr 2004 wurde der Begriff „googeln“ in den Duden aufgenommen. Seitdem ist viel geschehen. Wer regelmäßig Google nutzt, hat vermutlich bemerkt, dass in der englischsprachigen Version seit 2008 und in der deutschen Version seit 2009 schon ab dem ersten eingetippten Buchstaben Vorschläge von Google kommen, das Wort zu vervollständigen. Meist erscheinen auch dazu passende Suchbegriffe. Je mehr eingetippt wird, desto näher kommt Googles Vorschlag dem eigenen Suchwunsch.Diese Funktion nennt sich Google Suggest und ist selbst nicht nur ein Vorschlag. Denn sie lässt sich nicht deaktivieren. Nicht verwechselt werden sollte sie mit Google Instant. Dabei handelt es sich um eine Funktion, die bereits während der Eingabe Suchergebnisse liefert. Hat das etwas mit Statistik zu tun? Ja, sogar eine ganze Menge.Autocomplete, Big Data und StatistikDie jeweiligen Vorschläge basieren auf der Analyse der Häufigkeiten der Suchanfragen anderer Nutzer sowie dem eigenen Suchverhalten. Die häufigsten Suchbegriffe, welche mit den eingetippten Buchstaben beginnen, und dann die am häufigsten in Kombination mit dem Suchbegriff eingegeben Wörter erscheinen oben in der Liste der Vorschläge.Doch Google kann sich auch dem Benutzer individuell anpassen: Besitze ich ein Konto bei Google Mail, Google+, YouTube oder einem anderen mit Google verbundenen Dienst, dann schlägt Google mir auch die eigenen bereits eingegebenen Suchbegriffe zur Vervollständigung vor – zur Unterscheidung zu den allgemeinen Vorschlägen farblich markiert. Google berücksichtigt zudem den geografischen Hintergrund des Benutzers: Wer aus Berlin nach „Bahnhof“ sucht, erhält andere Ergebnisse als jemand aus Köln; deutsche Suchen nach „McDonalds“ liefern andere Ergebnisse als eine Suche aus Großbritannien.Personalisierte SucheDie Algorithmen, welche Google zur Erstellung der jeweiligen Vorschläge verwendet, sind ein gut gehütetes Betriebsgeheimnis. Analysiert werden aber nicht nur die Häufigkeiten der eingegeben Suchbegriffe selber, sondern auch die Wortkombinationen. Verfügt Google über weitere Informationen, beispielsweise über Wohnort, Geschlecht, Alter oder bevorzugte Seiten des Benutzers, werden die Vorschläge noch individueller. Von der statistischen Methodik her ist das zwar nicht besonders schwierig, aber die Kunst besteht in der Auswertung gewaltiger Daten- und Informationsmengen, bevor dann bis zu vier Vorschläge auf dem Bildschirm aufleuchten. Das bedeutet aber auch, dass jeder einzelne Benutzer Google durch seine Suche mit verändert – es handelt sich dabei also gewissermaßen um ein „Gemeinschaftsprojekt“.Google verwendet somit nicht nur die unbedingten oder marginalen relativen Häufigkeiten von Anfragen, die sich (stark vereinfacht ausgedrückt) als der Anteil der jeweiligen Suchanfrage an allen Anfragen zum jeweiligen Zeitpunkt errechnen lassen. Vielmehr nutzt Google auch bedingte relative Häufigkeiten, ein Instrument der Bayesianischen Statistik. Dabei werden weitere Eigenschaften der Person, die die Anfrage stellt, berücksichtigt.  Dieser Beitrag eröffnet eine vierteilige Serie über die Chancen und Risiken von Google Suggest.Weiterführende Links:https://support.google.com/websearch/answer/106230?hl=enhttp://searchengineland.com/how-google-instant-autocomplete-suggestions-work-62592http://www.seo-analyse.com/seo-lexikon/a/autovervollstaendigung/https://www.google.com/intl/de/insidesearch/howsearchworks/algorithms.html";https://bigdatablog.de/2014/03/20/google-suggest-schoener-suchen-mit-big-data-teil-1/;BigDataBlog;Katharina Schüller
18. Mrz 14;"     ""Meinungsvielfalt durch Datenanalyse""";Apps der großen und kleinen Nachrichtenportale erobern die Smartphones. Die Führungsposition teilen sich in dem hart umkämpften deutschen Markt der Spiegel und die Bild-Zeitung. Im Apple iTunes Store führt der Spiegel die Rangliste der kostenlosen Nachrichten-Apps an und Bild die der kostenpflichtigen. Bei Googles Play Store wird Bild ebenfalls als kostenlos geführt und liegt hier sogar vor dem Spiegel auf Platz 1.Die Apps bieten, ebenso wie Websites, eine neue Möglichkeit der Berichterstattung. Zauberwort ist hier die individualisierte Berichterstattung mit Hilfe von Big-Data-Analystics. Kein Nutzer liest eine komplette App oder Website, die wenigsten lesen jede Titelüberschrift. Die Nachrichten-Leser selektieren die für sie relevanten Nachrichten. Theoretiker entwickelten dafür kulturabhängige und -unabhängige Nachrichtenfaktoren, wie beispielsweise die Nähe zum Ereignis und die Relevanz der Informationen, aber auch so etwas wie Prominenz.Individualisierbare Nachrichten-AppsDa jeder Leser bei der Selektion von Nachrichten ein eigenes Set von Kriterien anwendet, ist es eigentlich erstaunlich, dass Apps und Websites von Nachrichtenportalen noch nicht individuell auf den jeweiligen Nutzer reagieren. Denkbar wären hier Konzepte wie bei großen Online-Shops, als Paradebeispiel wäre Amazon zu nennen. Jeder Amazon-Nutzer (ob Kunde oder nicht) sieht eine auf ihn angepasste Ansicht. Die Wahrscheinlichkeit, dass zwei Kunden exakt die gleichen Seite sehen, geht gegen Null. Dafür speichert und analysiert Amazon jede Interaktion auf ihrer Website. Anbieter wie die Exasol AG bieten vergleichbare Lösungen, beispielsweise für Zalando. Das Prinzip ist dasselbe: Auch hier erhält jeder Nutzer eine individuelle Ansicht auf Basis der gespeicherten Datenbasis. Durch die zunehmende Boulevardisierung und Kommerzialisierung der Medien sind vergleichbare Entwicklungen bei Nachrichtenportalen zu erwarten.Vorstellbar ist etwa eine Spiegel-App, die durch das Auswerten meiner Lesegewohnheiten weiß, dass ich mich für Kultur, Amerika und die SPD interessiere, aber kein Interesse an Fußball habe. Die App würde mir auch während der Fußball-Weltmeisterschaft vorrangig die für mich interessanten Beiträge zu Politik und die wichtigsten News aus den USA liefern, ohne meine Startseite mit Fußball-Meldungen zu spamen. Zu Fußball würde ich allenfalls eine Meldung in Vermischtes finden.Bei der Entwicklung individueller Nachrichtenportale sind die sogenannte Reader-Apps den Nachrichtenanbietern momentan voraus. Apps wie Flipboard, Linkedin Pulse, Taptu, Goggle Currents oder auch Facebook Paper bieten bereits individuelle Services an. Allerdings mit einem anderen Ansatz: Der Nutzer stellt sich seine Nachrichten selbst zusammen, anstatt durch ein Analyseverfahren idealisierte Ergebnisse geliefert zu bekommen.Im Endeffekt führt ein individualisiertes Nachrichtensystem zu einem differenzierteren Meinungsbild und verbessert die Kundenzufriedenheit. Spannend bleibt die Frage, welcher Nachrichtenverlag sich Big Data Anwendungen zuerst öffnet und diese dann vom Anzeigenmarkt auch auf das redaktionelle Angebot ausweitet.Mehr zum Thema:http://www.exasol.com/produkte/https://www.taptu.comhttp://www.spiegel.de/dienste/a-668207.htmlhttp://www.bild.de/digital/mobil/bildmobil/bild-apps-33555566.bild.html;https://bigdatablog.de/2014/03/18/meinungsvielfalt-durch-datenanalyse/;BigDataBlog;Jan Golka
14. Mrz 14;"     ""422 South macht Big Data sichtbar""";Heute Morgen sind wir bei Stern.de über ein faszinierendes Big-Data-Video gestolpert. Darin haben die Visualisierungsexperten von 422 South in Bristol den europäischen Flugverkehr für die britische Flugsicherungsbehörde NATS visualisiert. Das Ergebnis ist absolut faszinierend.Wer bislang noch keine Ahnung hatte, was Big Data eigentlich ist, bekommt mit dem Video eine leise Vorahnung davon, welche Power in der Verarbeitung der riesigen Datenmengen steckt. Sehenswert sind eigentlich alle Videos im Youtube-Kanal von 422 South. Gut gefallen hat uns zum Beispiel auch die Darstellung der Wanderflugroute des Wespenbussards.Leider hat 422 South das Einbetten der Youtube-Videos deaktiviert, so dass wir an dieser Stelle nur darauf verlinken können.European air traffic data visualization for NATSDataviz of Honey Buzzard migrationData Visualization of London Traffic;https://bigdatablog.de/2014/03/14/422-south-macht-big-data-sichtbar/;BigDataBlog;Ibrahim Evsan
13. Mrz 14;"     ""Intelligente Lösungen gegen den Verkehrskollaps""";Nie zuvor gab es so viele Daten über die Verkehrsströme auf unseren Straßen wie heute. Doch wie lassen sich diese Daten nutzen, um die Leistungsfähigkeit der Verkehrsnetze zu verbessern? Das ist Thema einer Podiumsdiskussion bei International Transport Forum 2014 vom 21. bis 23. Mai in Leipzig.Während die einen glauben, dass sich durch die Zusammenführung und Auswertung der Datenmengen neue Lösungen für Verkehrsprobleme ergeben werden, warnen Skeptiker  vor voreiligen Entschlüssen. Ihnen zufolge wird Big Data nur dann einen Nutzen bringen, wenn seitens der Verkehrspolitik die richtigen  Fragen gestellt werden.Im Verlauf der Podiumsdiskussion „Big Data im Verkehrsbereich: Anwendungen, Folgen, Grenzen“ sollen die folgenden Themen erörtert werden:Was ist Big Data – und was ist es nicht? Wo liegen die Grenzen, welche innovativen Anwendungen ermöglicht es und was sind die Risiken der Arbeit mit unterschiedlichen Datenquellen?Wie können Regierungen, Verkehrsbehörden, Unternehmen und Anbieter das Potenzial von Big Data maximal ausschöpfen, den Informationsaustausch verbessern und die Gefahr verzerrter Wahrnehmung in den Griff bekommen?Wie verändert Big Data individuelle Mobilitätsentscheidungen von Fahrgästen?Welche neuen Erkenntnisse können Big Data und Open Data liefern und was macht sie relevant für Verkehrspolitik?Regierungen und Unternehmen nutzen schon heute Big Data zur Verkehrskontrolle, Logistik-Optimierung und zum Ertragsmanagement. Immer häufiger werden dabei auch Bewegungsdaten von Mobiltelefonen herangezogen. So hat etwa der britische Telefonanbieter Orange 2012 eine Fallstudie erstellt, wie Big Data helfen könnte, die Transport-Infrastruktur der Elfenbeinküste zu optimieren. Andere Beispiele sind:Die Großstädte Dublin (Irland), Stockholm (Schweden) und Da Nang (Vietnam) nutzen Big-Data-Anwendungen von IBM, um Verkehrsstaus vorzubeugen.Die Fluggesellschaften Lufthansa, Air France-KLM und Swiss nutzen Big Data, um Ihre Kosten und Erträge zu optimieren. Auch British Airways will sich mit Hilfe der Analyse der riesigen Datenmengen einen Vorsprung vor den Wettbewerbern, gerade aus dem Low-Cost-Segment, verschaffen. Die Daten sollen helfen, den Kundenservice weiter zu optimieren.Für alle, die mehr zum Thema erfahren wollen, gibt es auf der Webseite zur Podiumsdiskussion zwei interessante PDFs zum Download:Big Data and Transport (pdf)2010 Euro Directive regarding data exchange:DIRECTIVE 2010/40/EU OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 7 July 2010 on the framework for the deployment of Intelligent Transport Systems in the field of road transport and for interfaces with other modes of transport (pdf);https://bigdatablog.de/2014/03/13/intelligente-loesungen-gegen-den-verkehrskollaps/;BigDataBlog;Ibrahim Evsan
12. Mrz 14;"     ""Welche Rolle spielt Deutschland bei Big Data?""";Der Branchenverband Bitkom hat zur Cebit 2014 Ergebnisse einer Umfrage präsentiert, die einem zu denken geben. Demnach ist die Unternehmenswelt in Deutschland dreigeteilt: Während 31 Prozent der Unternehmen den Einsatz von Big Data bereits konkret planen, haben sich 28 Prozent noch keine abschließende Meinung dazu gebildet. 33 Prozent haben sich noch gar nicht mit dem Thema befasst.Daraus lässt sich der Schluss ziehen, dass die Mehrheit der Unternehmen ganz offenbar momentan noch keinen echten Nutzen von Big Data sieht. Warum ist das so?Eine Antwort mag darin liegen, dass Big Data als Begriff enorm sperrig ist, den einen oder anderen vielleicht sogar abschreckt. 62 Prozent der Deutschen bereiten die wachsenden Datenmengen Sorgen. Auch das hat die Bitkom herausgefunden. Sitzen die Bedenkenträger etwa auch in den Entscheidersesseln der Unternehmen?Visionäres GroßbritannienZur Eröffnung der Cebit hat der britische Premier David Cameron in Hannover die Absicht Großbritanniens unterstrichen, sich zur innovativsten Nation im Verbund der acht wichtigsten Industrieländer (G8) entwickeln zu wollen. „Wir haben den roten Teppich ausgerollt für die Kreativen dieser Welt.“ Das waren seine Worte.Die deutsche Kanzlerin hingegen brachte das zum Ausdruck, was offenbar 62 Prozent der Bundesbürger denken: Datensicherheit sei eine unabdingbare Voraussetzung für die Entwicklung von Internet-Diensten. Geht es nach Angela Merkel, sollen schon bald neue Datenschutz-Vereinbarungen getroffen werden. Innenminister Thomas de Maizière will noch in diesem Jahr einen Entwurf für ein IT-Sicherheitsgesetz auf den Weg bringen.Euphorie sieht anders ausChance oder Risiko – so einfach lässt sich die Trennlinie zwischen britischer und deutscher Position sicherlich nicht ziehen. Dennoch wird die Haltung der deutschen Kanzlerin nicht dazu beitragen, Big-Data-Spezialisten heute für den Standort Deutschland zu begeistern. Euphorie sieht anders aus. Und so ermuntert Merkel auch die Unternehmen nicht, sich für den Nutzen von Big-Data-Anwendungen zu interessieren.Google-Manager Eric Schmidt hat auf der  SXSW-Konferenz in Texas die Meinung vertreten, dass der Verlust der Privatsphäre über kurz oder lang unausweichlich ist. Damit liegt er so falsch nicht, schaut man sich an, wie viel unserer Daten schon jetzt in Netzwerken gespeichert sind. In Zukunft werden es garantiert mehr. Und: Google Glass, Smartwatches werden nicht die einzige „Wearables“ bleiben. Wer hofft, dass die IT-Labs rund um den Globus aus Datenschutz-Bedenken ihre Arbeit einstellen, wird vermutlich enttäuscht werden.Welche Rolle spielt Deutschland? Um wirkliche Impulse in Sachen Datenschutz und Privatsphäre zu setzen, müsste Deutschland vorne mit die Richtung bestimmen, statt hinten auf der Bremse zu stehen. Schon heute ist es erschreckend, wie wenig deutsche Unternehmen zur Entwicklung künftiger Netz-Technologien beitragen.Vor diesem Hintergrund verwundert es nicht, dass die Mehrzahl der deutschen Unternehmen mit Big Data (noch) wenig anfangen kann.;https://bigdatablog.de/2014/03/12/rolle-deutschland-big-data/;BigDataBlog;Ibrahim Evsan
07. Mrz 14;"     ""CNN und Dataminr: Echtzeit-News via Tweets""";Der Nachrichten Sender CNN gibt dem Internet als Nachrichtenquelle eine neue Chance. Gemeinsam mit dem Startup Dataminr sollen Tweets weltweit in Echtzeit analysiert werden, um relevante Nachrichten herauszufiltern.CNN folgt mit dem gemeinsamen Projekt Dataminr for News dem weltweiten Big-Data-Trend. Das Programm soll Journalisten bei der Auswertung der Fülle an Informationen aus dem Internet dienen. Insbesondere im Hinblick auf Eilmeldungen verspricht man sich durch die gezielte Analyse von Twitter-Nachrichten schneller zu sein als die Konkurrenzn. Die Entwicklung von “Dataminr for News” soll bis Ende des Jahres abgeschlossen sein und dann an Nachrichtensender, Agenturen und Redaktionen vertrieben werden.Dataminr arbeitet bereits seit 2009 an Algorithmen zur Auswertung von Twitter. Der Kundenstamm des Unternehmens liegt bislang überwiegend in der Finanzbranche. Dataminr-Kunden wussten 2013 beispielsweise schneller als andere von der geplatzten Blackberry-Finanzierung und vermieden dadurch Millionenverluste.Die Möglichkeit, der Konkurrenz immer einen Schritt voraus zu sein, sollen mit dem neuen Projekt jetzt auch Journalisten bekommen. Die einzelnen Twitter-Nachrichten werden dabei nach individuellen Einstellungen selektiert und strukturiert. Findet der Algorithmus ein bestimmtes Muster, werden die Tweets gesammelt als Einmeldung an den Journalisten gesandt.Dataminr for News ist nicht der erste Versuch einer größeren Nachrichtenagentur, Mehrwert aus der Geschwindigkeit von Social Media zu ziehen. 2010 sorgten fünf Journalisten für Aufsehen, die für fünf Tage auf einen abgelegenen Bauernhof zogen, und versuchten, nur mit Hilfe von Facebook und Twitter über das Weltgeschehen zu berichten. Der erhoffte Erfolg bliebt damals jedoch aus.Inwieweit Dataminr for News die Journalisten künftig unterstützen und die Berichterstattung verbessern kann, bleibt abzuwarten.Mehr zum Thema:http://www.dataminr.com/press/announcing-dataminr-for-news/  ;https://bigdatablog.de/2014/03/07/cnn-news-in-echtzeit/;BigDataBlog;Jan Golka
07. Mrz 14;"     ""Weniger Kriminalität dank Big Data?""";An den 13. Februar 2014 werden sich die Polizeibeamten der Foothill Division noch lange erinnern. An diesem Donnerstag registrierten sie in ihrem Einsatzgebiet im Norden von  Los Angeles keine einzige schwere Straftat.24 Stunden ohne Mord, ohne Vergewaltigung, ohne Raubüberfall, ohne Autodiebstahl, ohne Einbruchsdelikt – keiner der Polizisten kann sich erinnern, dass es einen solchen Tag in der 50-jährigen Geschichte der Division je zuvor gegeben hat.Das Los Angeles Police Department vermeldete den Erfolg stolz in einer Pressemitteilung. Ein Tag ohne Schwerverbrechen sei „eine enorme Leistung und ein Meilenstein für Foothill“ – und auch das Ergebnis von Big Data.Positive Entwicklung seit 2009Seit vier Jahren verzeichne das LAPD in Foothill einen kontinuierlichen Rückgang der Straftaten. Zählten die Beamten 2009 noch 5.000 Straftaten, waren es 2013 nur noch 3.800 – ein Rückgang um rund 30 Prozent. Die Zahl der Menschen, die Opfer eines Gewaltverbrechens wurden, sank von 888 auf 510.  Und der Trend scheint sich fortzusetzen: In den ersten Wochen dieses Jahres lag die Kriminalitätsrate 29 Prozent unter der Marke des Vorjahres.Das LAPD führt den Erfolg zum einen auf eine verbesserte Zusammenarbeit, unter anderem mit lokalen Behörden und sozialen Diensten zurück. Entscheidend dazu beigetragen habe jedoch auch die Analyse von Big Data, die den Foothill-Cops vorhersagt, wo in ihrem Revier in den nächsten Stunden höchstwahrscheinlich ein Verbrechen geschehen könnte.Datenanalyse für die CopsBevor sie auf Streife gehen, schnappen sich die Polizisten eine Karte, auf der von der Software errechnete Kriminalitätsschwerpunkte mit rot Rahmen markiert sind. Das Ergebnis: Die Beamten sind vor den Verbrechern am Tatort.Predictive Policing nennt sich die Technologie, die das LAPD seit November 2011 einsetzt. Herstellt wird die entsprechende Software von der Firma PredPol, Inc. in Santa Cruz. In Kombination mit weiteren Anwendungen wie einer Ermittlungssoftware von Palantir sowie Echtzeit-Visualisierung und Analyse-Tools von Omega Group lassen sich detaillierte Vorhersagen treffen.„Minority Report“ schon Realität?Das Programm beschwört unweigerlich beklemmende Zukunftsvisionen herauf. In seinem Science-Fiction-Thriller „Minority Report“ malt Steven Spielberg das Schreckensbild einer Gesellschaft, in der die Polizei mittels präkognitiver Methoden zu wissen glaubt, welcher Mensch eine Straftat begehen wird.Davon allerdings ist man in Foothill weit entfernt. Die Polizei dort sieht ihre Anwendung als Maßnahme zur reinen Verhinderung von Straftaten. Ihr geht es nach eigener Darstellung nicht darum, mehr Verbrecher zu verhaften, und Daten zu erfolgten Festnahmen flössen nicht in die Software ein. Tatsächlich sei die Zahl der Festnahmen seit Beginn der Testphase gesunken.;https://bigdatablog.de/2014/03/07/predictive-policing-weniger-kriminalitaet-dank-big-data/;BigDataBlog;Ibrahim Evsan
07. Mrz 14;"     ""Was steckt hinter Big Data?""";Big Data, „große Daten“, ist ein seltsamer Begriff – irgendwie unernst, albern, gar nicht technisch und erst recht nicht „erhaben“. Dabei fasst Big Data eine ganze Reihe von Entwicklungen zusammen, die in ihrer Bedeutung für fast jeden Bereich unseres Lebens gar nicht überschätzt werden können: Es handelt sich um nicht weniger, als einen vollständigen Paradigmenwechsel, technologisch getrieben, aber schon längst weit jenseits der Technologie wirksam.Nach dem World Wide Web vor 20 Jahren, Social Media (Web 2.0) vor 10 Jahren, ist es die dritte Welle von Technologie, die aus dem Internet entstanden ist und sich weltweit ausbreitet.Aber was ist dieses „Big Data“?Oft ist von den „Drei Vs“ die Rede: Volume, Velocity, Variety. Große Datenmengen, die in schneller Folge gemessen werden, sind aber für sich genommen noch nichts Neues. Datenbanken, die auf gewaltigen Servern in Rechenzentren hochperformant „EDV“ machen, gibt es schon lange. Relativ neu aber ist, dass es Betriebssysteme und Datenbanken gibt, die auf billiger Standard-Hardware laufen: viel leistungsfähiger und nahezu beliebig skalierbar, und das meiste davon ist also Open-Source-Anwendung frei verfügbar .Hadoop: das Big-Data-BetriebssystemHadoop kann man als das „Betriebssystem“ von Big Data bezeichnen. Hadoop liefert ein Filesystem und eine Prozesssteuerung, die Daten und Rechenaufgaben auf viele einzelne Rechner verteilt. Um Hadoop hat sich schnell ein „Ökosystem“ aus frei verfügbaren sowie kommerziellen Anwendungen entwickelt – ähnlich wie wir es „damals“ mit Microsoft Windows und dem PC erlebt haben.Cloud-Computing, wie Amazons Elastic Cloud Computing (EC2), ermöglicht es ohne großen Aufwand und ohne tiefe Fachkenntnisse, größte Datenmengen zu verarbeiten. Da sich die Preisgestaltung an der Rechenzeit orientiert, haben Kunden die Möglichkeit zunächst mit kleinen Datensätzen zu experimentieren und nach erfolgreichem Test zu skalieren. Selbst kleine Teams oder Einzelpersonen ist es damit möglich, eine Datenanalyse vorzunehmen, die vor kurzem ausschließlich größten Rechenzentren vorbehalten war.Neue Datenbank-WeltDarüber hinaus gibt es eine neue Datenbank-Welt, deren Protagonisten – um nur ein paar zu nennen – MongoDB, Couchbase oder Voldemort heißen. Diese Datenbanken sind dafür gemacht, unstrukturierte und teilstrukturierte Daten (z.B. Texte) zu verarbeiten. Der dafür häufig verwendete Begriff „NoSQL“ leitet allerdings in die Irre: Auch wenn viele der Big-Data-Datenbanken keine relationalen Systeme mit Tabellenlogik sind, haben die meisten eine Abfragesprache, die sich stark an der SQL-Struktur orientiert.Das Dateiformat, dass sich in der Big-Data-Kultur für alle Metadaten durchgesetzt hat, ist die Java Script Object Notation. In der JSON werden Informationen als Paare von Schlüsseln und Werten dokumentiert, den sogenannten Key-Value Pairs (KVP). Metadaten sind größtenteils „unsichtbare“ Daten, die beispielsweise Bilder oder Videos beschreiben. Hierin sind etwa Datumsangaben, Ortsdaten oder Kameraeinstellungen vermerkt.Python wird zum StandardFür Datananalyse wird die Programmiersprache Python mehr und mehr zum Standard. Python ist relativ einfach zu lernen, da es in der Anwendung viel intuitiver als die meisten anderen Programmiersprachen ist. Für Python gibt es schon heute riesige Code-Bibliotheken, die praktisch jeden Bereich von Datenanalyse abdecken.Kurzum: Es sind diese fünf Zutaten, die Big Data im Wesentlichen ausmachen: (1) Betriebssystem Hadoop, (2) Cloud-Computing, (3) Datenbanken für unstrukturierte Daten, (4) Metadaten in JSON und (5) Datenanalyse, beispielsweise in Python. Für jemanden, der sich noch nie intensiv mit Big Data befasst hat, mag das auf den ersten Blick verwirrend erscheinen. Doch das Beste an dieser dritten Technologiewelle ist, dass sich auf nahezu alle Fragen im Netz jede Menge Antworten finden lassen. Also: Keine Angst vor Big Data – einfach ausprobieren!;https://bigdatablog.de/2014/03/07/was-steckt-hinter-big-data/;BigDataBlog;Jörg Blumtritt
06. Mrz 14;"     ""Wie einfach Unternehmen Big Data nutzen können""";Big Data hört sich doch irgendwie ein wenig Angst einflößend an. Wir werden kontrolliert, und von uns werden teils persönliche Daten gesammelt. Während des Web Summits 2013 in Dublin sagte Jewgeni Kaspersky, wir müssten uns damit abfinden, dass alles was wir online ins Keyboard eingeben, gespeichert werden kann. Wir persönlich sollten vielleicht ein wenig sensibler mit unseren Daten umgehen und uns dessen bewusst sein, dass unsere E-Mail-Adresse oder ein Like hier und da im Kollektiv für das Geschäftsmodell einiger Unternehmen sehr wertvoll sind.Doch wie können Firmen, kleine wie große,  von der Masse an Informationen und Daten profitieren. Während meines ersten Master-Semesters sollten wir einen „Digital Landscape Review“ anfertigen. Einfach gesagt sollten wir eine digitale Situationsanalyse für ein kleines- oder mittelständiges Unternehmen anfertigen. Ich fertigte diese Analyse für ein Start-up an, das eine Steuersoftware anbietet. Ich nutzte Tools wie Followerwonk, Open Site Explorer (MOZ), Tweetstats und war überrascht, was diese kostenlosen Angebote alles können. Nach zwei Tagen Arbeit hatte ich eine ausführliche Analyse mit unzähligen Screenshots, sowohl für die Firma, die ich betreute, als auch für ihre Konkurrenz. Klar, ich hatte bereits zuvor einen Blick auf diese Tools geworfen, aber noch nie mit penibler Genauigkeit wie dieses Mal.Kostenlose Analyse-ToolsDer Sinn meines Beitrags ist es, jede Firma dazu zu ermutigen, selbst einmal eine solche Analyse anzufertigen. Denn Einblicke in die Interesse der eigenen Kunden zu gewinnen ist von schier unbezahlbarem Wert. Followerwonk etwa bietet eine Vielzahl von Grafiken an, die Aufschluss darüber geben, wie alt die Accounts meiner Follower sind. Hieraus ließe sich zum Beispiel ableiten, ob meine Kunden zu den Early Adoptern zählen oder nicht. Weiterhin ist anhand der Analyseergebnisse auf einen Blick zu sehen, woher der Großteil meiner Follower kommt, und Themen, die sie interessieren, sind in verschiedenen Schlagwörter-Clouds zusammengefasst.Um einen guten und schnellen Einstieg in die Analyse zu gewährleisten, möchte ich meine Struktur, die sich als gut erwiesen hat, mit Euch teilen:Search: MOZ, Google Trends, Topsy, Google Keyword PlannerSocial Bookmarking: Reddit, DiggMedia Aggregators: YouTube, Scribd, SlideShare Social Networks: Followerwonk, Kred, Tweetreach, PeerIndex, TwitalyzerNachdem ich diese Tools genutzt und die Analyse-Screenshots in einer Powerpoint-Präsentation aufbereitet hatte, versuchte ich eine neue Struktur für eine 15-seitige Key-Findings-Datei zu finden. Darin faste ich zusammen, welche Einflüsse die neu gewonnenen Erkenntnisse über Kunden der jungen Firma, auf deren externe Kommunikation haben könnte. Nach meiner Präsentation konnte die Firma nach eigenen nach Angaben ihre Kunden tatsächlich deutlich besser ansprechen, und auch ihre Interaktion mit den Nutzern hat sich verbessert. Ich führe das unter anderem darauf zurück, dass es mir durch die Analyse gelungen war, in Erfahrung zu bringen, was die Kunden der Firma wirklich bewegt und interessiert.;https://bigdatablog.de/2014/03/06/wie-einfach-unternehmen-big-data-nutzen-koennen/;BigDataBlog;Julian Gottke
06. Mrz 14;"     ""Bitkom: Big Data verspricht großes Wachstumspotenzial""";Big Data ist großer Gewinner und verspricht großes Wachstum sowie steigende Umsätze in Deutschland.Im laufenden Jahr sollen die Umsätze um 59 Prozent auf weit über 6,2 Milliarden Euro ansteigen. 2016 sollen es bereits 13,6 Milliarden Euro sein, heißt es in den Berechnungen des IT-Marktforschungs- und Betratungsunternehmens Crisp Research im Auftrag der Bitkom.„Die Auswertung großer Datenmengen in Echtzeit ermöglicht völlig neue Anwendungen“, sagte Bitkom-Präsident Dieter Kempf am 5. März. „Vor allem im Zusammenspiel mit Technologien wie Cloud Computing oder dem mobilen Internet entwickeln sich laufend neue Einsatzmöglichkeiten.“;https://bigdatablog.de/2014/03/06/bitkom-big-data-grosses-wachstum/;BigDataBlog;Ibrahim Evsan
06. Mrz 14;"     ""Cebit 2014: Bringt Datability mehr Privatsphäre?""";An Big Data führt auf der Cebit 2014 in Hannover kein Weg vorbei. Typisch für eine Messe blicken die Veranstalter dabei natürlich in die Zukunft: Wohin geht die Big-Data-Reise?Klare Antworten darauf hat momentan freilich kaum jemand.Und wohl auch weil das so ist, hat sich die (wie lange noch?) weltgrößte Computermesse in Hannover selbst unter das Motto Datability gestellt: Big Data trifft  Credibility. Was ist damit gemeint?Diese Frage stellt sich offenbar auch Simon Hülsbömer, Leitender Redakteur der Computerwoche. Anfang der Woche bat er seine Leser, Datability in 140 Zeichen zu erklären. Bislang mit eher mäßigem Erfolg. Twitter-User  Carsten Waetke (@CWaetke) etwa mutmaßte mit leichter Ironie: „Datability: Die sofortige Vermeidung der Sammlung u. Verarbeitung von massenhaften nutzlosen Informationen #cwcebit„Big Data – wer übernimmt Verantwortung?Die Cebit selbst definiert ihr Kunstwort Datability als „die Fähigkeit, große Datenmengen in hoher Geschwindigkeit verantwortungsvoll und nachhaltig zu nutzen“. Im Messe-Blog gibt es eine informative Grafik dazu. Der ist zu entnehmen, dass sich die im Jahr 2012 mit Big Data erzielten Einnahmen von 4,6 Milliarden Euro bis 2016 mehr als verdreifachen werden.  Auf nordamerikanischen Servern liegt demnach schon jetzt mehr als 3.500 Petabyte Daten, in Europa sind es gerade mal über 2.000 Petabyte. Unvorstellbare Zahlen, aus denen sich alles Mögliche herauslesen lässt.Allein die Frage, wer hier wofür Verantwortung übernehmen sollte, bleibt offen. Die Unternehmen sind hier (zunächst) die falschen Adressaten. Gefragt ist vielmehr die Politik. Sie müsste eine Strategie formulieren, zum Beispiel wie wir Big Data und Privatsphäre in Einklang bringen wollen.Was macht die Politik?Gefragt ist vielmehr die Politik. Doch mehr als die „Big Data Days“ und ein Förderprogramm für den intelligenten Umgang mit großen Datenmengen („Smart Data – Innovationen aus Daten“) fördert die Google-Suche nach „bundesregierung big data“ bislang nicht ans Tageslicht. Der eigentliche Kern des  Themas „Datability“ ist in Berlin offenbar noch nicht angekommen.Unter dem Eindruck der Snowden-Enthüllungen ist man in den USA bereits einen Schritt weiter. Präsident Barack Obama  hat im Januar eine Studie in Auftrag gegeben, die die technologische Dimension von Big Data und Privatsphäre untersuchen soll. Das Ergebnis soll in gut zwei Monaten vorliegen, und es dürfte zumindest einen Weg aufweisen, welche Daten wir wirklich sammeln wollen, wie wir sie verfügbar machen und wozu sie verwendet werden.Merkel will „umfassenden Dialog“Bundeskanzlerin Angela Merkel hat in ihrem Grußwort zur Cebit 2014 angekündigt, dass auch „wir“ 2014 in einen umfassenden Dialog darüber eintreten wollen, wie unsere digitale Zukunft aussehen kann und soll.Wir dürfen also gespannt sein, welche Impulse nicht nur von der Messe und den Unternehmen, sondern auch von der Politik ausgehen.;https://bigdatablog.de/2014/03/06/cebit-2014-datability-privatsphaere/;BigDataBlog;Ibrahim Evsan
06. Mrz 14;"     ""Cebit 2014: Sicherheitsrisiken durch neue digitale Revolution?""";Big Data ist das Top-Thema der Cebit 2014 in Hannover. Zu den Unternehmen, die sich im Rahmen der am Montag (10. März) beginnenden IT-Messe mit Big Data befassen, zählt auch die Software AG aus Darmstadt.Karl-Heinz Streibich, Vorstandsvorsitzender der Software AG, sieht in der Kombination von Big Data mit den anderen drei technologischen Megatrends – Cloud, Mobile und Social Collaboration – eine neue digitale Revolution: das komplett digitalisierte Unternehmen. „Das Aufeinandertreffen dieser vier Megatrends zum gleichen Zeitpunkt ist einmalig in der IT-Branche“, so Streibich in einem Statement auf der Cebit-Webseite. „Wir haben es hier mit einer Veränderung zu tun, deren Auswirkungen größer sind als alles, was wir bisher in der IT-Welt erlebt haben. Die digitale Revolution wird jeden betreffen: jede Branche, jedes Unternehmen und jeden Kunden.“Sicherheitsrisiken durch Big Data?Doch die digitale Big-Data-Revolution hat ihre Schattenseiten. Harald Schöning, Forschungsleiter der Software AG, sieht durch die Fähigkeit, große Datenmengen in hoher Geschwindigkeit verarbeiten zu können, neue Sicherheitsrisiken. „In Zukunft wird die Verknüpfung und Verarbeitung riesiger Business-Datenberge immer wichtiger. Schreckensszenarien wie die Steuerung von Fabriken über Cyber-Angriffe finden schon heute ihren Niederschlag in Science-Fiction-Filmen und werden denkbar. Ein rechtzeitiges Gegensteuern auf der Forschungsseite ist unabdingbar.“ Zugleich gibt er jedoch Entwarnung. Zwar könne Big Data für das eigentliche Geschäft von Unternehmen bereits gefährlich werden, allerdings hält er im Zusammenhang mit Business-Software das akute Risiko für noch gering.Im Profil: Software AGDie Software AG ist laut Truffle 100 nach SAP und Wincor Nixdorf das drittgrößte Softwarehaus Deutschlands und zählt zu den Top 7 in Europa. Das Unternehmen bietet Technologien für Big Data, Integration und Geschäftsprozessmanagement, mit denen Firmen ihre Effizienz steigern können. Die Software AG beschäftigt nach eigenen Angaben über 5.200 Mitarbeiter in 70 Ländern und erzielte 2013 einen Umsatz von rund 973 Millionen Euro.Mehr zum Thema:http://www.cebit.de/de/news-trends/trends/big-data-datability/ausstellerstimmen-zu-big-data.xhtmlhttp://www.truffle100.com/2013/ranking.php;https://bigdatablog.de/2014/03/06/cebit-2014-sicherheit-digitale-revolution/;BigDataBlog;Ibrahim Evsan
05. Mrz 14;"     ""Erste deutsche Jung-Professur für Big Data gestartet""";"Big Data hat es an die Universität geschafft und zwar in Weimar. Dort wurde von der Fakultät für Medien der Bauhaus-Universität die erste deutsche Jung-Professur für den Bereich Big Data Analytics eingerichtet.Visualisierung, Retrieval &amp; Algorithm EngineeringGefördert wird die Professur vom Bundesministerium für Bildung und Forschung sowie von sieben regional ansässigen Unternehmen, die mit ihrer Finanzierung das Projekt für die nächsten fünf Jahre sichern. Junior-Professor Dr. Matthias Hagen hat dabei die ehrenvolle Aufgabe eine Gruppe von Nachwuchsforschern zu leiten. Neben der Entwicklung und Skalierung von Datenanalyse- und Mining-Techniken wird sich die Jungforschergruppe mit Themen wie Visualisierung, Retrieval und Algorithm Engineering auseinandersetzen.Mithilfe von Jungprofessor Dr. Hagen, seiner Forschungsgruppe und weiteren Experten will die Bauhaus-Universität neue Werkzeuge sowie Algorithmen aufbauen, die zur Unterstützung von Analyseprozessen im Bereich Big Data dienen sollen. Außerdem soll Thüringen sich mit dem Aufbau einer Big Data-Professur als Wirtschaftsstandort neu aufbauen – auf Seiten der Infrastruktur profitiert die Universität zudem durch den Forschungsneubau “Digital Bauhaus Lab” (weitere Informationen unter digital-bauhaus-lab.de), heißt es auf der Homepage der Uni Weimar.Mit Big Data den idealen Nutzer identifizierenMatthias Hagen hat vor allem großes Interesse darin, aufzuzeigen, wie sich Nutzer im World Wide Web bewegen und auf welche Art und Weise sie vorhandene Informationen nutzen. Damit beispielsweise Suchalgorithmen verbessert werden können, will Hagen wissen, wie Anfragen gestellt, welche Begriffe genutzt oder auch, ob ganze Fragen oder nur Stichworte bei der Informationssuche eingegeben werden. Das sind nur einige Fragestellungen mit deren Antwort die Forscher den optimalen Nutzer im Web simulieren und herausfinden wollen, welche Suchoptionen am wirkungsvollsten sind.Jung-Professor Dr. Hagen und sein Forschungsteam werden alles daran setzen, ihre Ziele in den kommenden fünf Jahren zu realisieren. Bis dahin schauen wir gespannt auf die Bauhaus-Universität und die Entwicklung der Big Data-Professur.";https://bigdatablog.de/2014/03/05/jung-professur-fuer-big-data/;BigDataBlog;Ibrahim Evsan
05. Mrz 14;"     ""Modehaus ASOS setzt auf Big Data""";Big Data ist in aller Munde – auch der Mode-Gigant ASOS aus Großbritannien setzt seit zwölf Monaten auf entsprechende Software und Programme, um Informationen über Verbraucherwünsche und Konkurrenten in Echtzeit zu bekommen. Bereits mehr als 200 Mitarbeiter haben Zugriff auf das Big Data-Programm.Ein Grund für den Einsatz von Big Data-Software ist schnell gefunden. Maria Hollins ist Managerin bei ASOS und verrät in einem Interview mit dem Online-Portal RetailWeek: “Erster in der Modebranche zu sein, heißt für uns auch ständig konkurrenzfähig zu sein und auf das richtige Sortiment zu setzen. Wir verwenden die Software jeden Tag, um Hilfe bei kritischen Kauf- und Handelsentscheidungen zu bekommen.”Das Unternehmen, das auch in Deutschland einen großen Kundenstamm besitzt, konnte im vergangenen Jahr zwischen August und Dezember eine mehr als 30-prozentige Umsatzsteigerung durch Big Data-Programme erzielen. ASOS erhält dabei dank Big Data Informationen darüber, an welchen Produkten die Konkurrenz arbeitet oder welche Modetrends zeitnah anstehen. Aufgrund der ausgewerteten Daten kann ASOS bessere Entscheidungen treffen, welche Produkte bestellt beziehungsweise auf Lager sein sollten und welche Preise und Rabatte bei den einzelnen Artikel gesetzt werden können. ;https://bigdatablog.de/2014/03/05/asos-setzt-auf-big-data/;BigDataBlog;Ibrahim Evsan
;"     ""Was ist Künstliche Intelligenz und was kann sie leisten?""";Um die großen Datenmassen beherrschbar zu machen und Nutzen aus ihnen zu gewinnen, sind mehr als herkömmliche Analysewerkzeuge nötig. Obwohl das Phänomen schon seit vielen Jahrzehnten bekannt ist, gewinnt es erst im Big-Data-Zeitalter seine eigentliche Relevanz. Was ist neu an der jüngsten Euphorie rund um Künstliche Intelligenz, Machine Learning und Deep Neutral Networks? Um das zu verstehen, soll im Folgenden sowohl ein genauerer Blick auf KI und die einzelnen Methoden geworfen werden.Was ist überhaupt Künstliche Intelligenz?Künstliche Intelligenz (KI) wird im Allgemeinen als die Fähigkeit von Maschinen definiert, nicht nur mechanische Vorgänge zu beherrschen, sondern auch komplexe mentale Prozesse vollziehen zu können. Die Entwicklung im Bereich der Künstlichen Intelligenz ist bereits einige Jahrzehnte alt. Einer der in diesem Zusammenhang in der Regel genannten Ursprünge ist der Aufsatz von Alan Turing (1912-1954) „Computing Machinery and Intelligence“ aus dem Jahr 1950. Auch der nach ihm benannte „Turing-Test“ wird immer wieder als Kriterium angeführt, anhand dem beurteil werden kann, ob eine Maschine als wahrhaft „intelligent“ bezeichnet werden kann – doch dazu gleich noch mehr.Warum gerade heute KI so interessant istDass gerade in den letzten Jahren verstärkt über Künstliche Intelligenz gesprochen und sogar ihr Durchbruch gefeiert wird, hängt nicht mit einem vertieften wissenschaftlichen Verständnis von Künstlicher Intelligenz oder von der Entwicklung einer neuen Programmiersprache ab. Vielmehr hängt das hauptsächlich damit zusammen, dass die Rechenleistung von Hochleistungsprozessoren so gut geworden ist, dass viele Billionen Rechenoperationen pro Sekunde durchgeführt werden können. Zur Verdeutlichung: Der „konventionelle“ Prozessor im iPhone X schafft mehr als 600 Milliarden Rechenoperationen pro Sekunde. Gleichzeitig sind die Speichertechnologien so günstig geworden, dass große Mengen von Daten sehr schnell verarbeitet werden können. Diese Kombination aus Rechenleistung und Speicherplatz macht es möglich, Maschinen mit einem hohen Grad an Intelligenz auszustatten.Zur Unterscheidung von „starker“ und „schwacher“ Künstlicher IntelligenzZu Beginn einer Auseinandersetzung mit Künstlicher Intelligenz muss aber zunächst eine weitere wichtige generelle Unterscheidung vorgenommen werden, die vor allem begrifflicher Natur ist. Oft wird zwar der Begriff „Künstliche Intelligenz“ verwendet, aber im Grunde genommen wird über zwei sehr unterschiedliche Dinge gesprochen. Es gibt zwei verschiedene Weisen, „Künstliche Intelligenz“ zu definieren: Eine „starke“ und eine „schwache“. Mit „starker KI“, die manchmal auch als „Superintelligenz“ bezeichnet wird, ist jene Form von Künstlicher Intelligenz gemeint, die den Turing Test besteht. Sie könnte also einen Menschen bei einer Versuchsanordnung – dem Turing-Test –, bei dem weder ein Sicht- noch eine Hörkontakt zu den Gesprächspartner vorhanden ist, davon überzeugen, dass sie eine menschliche Person ist, die ein bestimmtes Geschlecht imitiert.Alternativen zum Turing-TestDer Turing-Test ist nicht unumstritten und so gibt es daneben auch andere Versuche, eine starke KI zu definieren. Dabei wird vor allem betont, dass eine starke KI mindestens den Grad an Intelligenz von Menschen aufweisen müsse oder diese übertreffen müsse. Dabei wird angenommen, dass das menschliche Gehirn mit den unterschiedlichsten Aufgaben sehr gut zurecht kommt. Ähnliches muss für eine KI gelten. Diese kann dann als generell intelligent bezeichnet werden, wenn sie viele Aufgaben gleichermaßen gut beherrscht. Als „schwache KI“ wird hingegen das Vermögen definiert, einzelne kognitive Aufgaben mindestens ebensogut oder besser wie Menschen durchführen zu können.Bislang hat keine KI den Turing-Test zweifelsfrei bestanden und die zahlreichen KI-Lösungen, dienen in der Regel einem ganz bestimmten Zweck. Zwar kann IBMs KI mit dem Namen Watson sowohl so programmiert werden, dass er bei einer Quiz-Show teilnehmen und gewinnen kann. Es ist allerdings ebenso möglich, ihn so zu trainieren, dass er bei der medizinischen Diagnose von MRT-Aufnahmen und der Interpretation von Röntgenbildern helfen kann. Beides kann „er“ aber nicht zur gleichen Zeit.Mensch vs. Maschine: Was ist Intelligenz?Auch die Frage „Was ist Intelligenz“ wird nicht oft genug in Zusammenhängen wie diesen gestellt. Dabei ist es durchaus relevant, sich vor Augen zu führen, welche Aspekte von dem, was als Intelligenz verstanden wird, durch Künstliche Intelligenz abgedeckt werden kann. Intelligenz lässt sich unter zahlreichen Gesichtspunkten betrachten. Als emotionale, kognitive oder als soziale Intelligenz. Sie lässt sich aber auch unter dem Aspekt des Bewusstseins diskutieren. Unabhängig vom Stellenwert der einzelnen Komponenten dürfte evident sein, dass menschliche Intelligenz sehr viel mehr Facetten hat. Die bloße Fähigkeit zur Bewältigung von intellektuellen und kognitiven Prozessen macht uns nicht zu intelligenten Wesen. Der Vergleich Mensch-Maschine in Bezug auf den Begriff „Intelligenz“ hinkt auch noch in einer anderen Hinsicht. Der Lösungsweg einer KI ist ein grundlegend anderer als der eines intelligenten Menschen.Eine KI nutzt eine enorme Rechenpower, um in kürzester Zeit gigantische Datenmengen zu durchforsten. Beispielsweise, um Muster in medizinischen Daten zu erkennen, die auf eine Krebserkrankung hinweisen. Ein Arzt, der dieselbe Aufgabe lösen muss, geht nicht im Geiste alle Fälle und alle Referenzen durch, um schließlich zu einem Ergebnis zu kommen. Vielmehr nutzt er Heuristiken, Erfahrung, Abstraktion und regelhaftes Wissen, um schließlich zu einer Einschätzung zu kommen. Es soll hier nicht gesagt werden, dass das eine besser oder schlechter als das andere ist. Es ist aber wichtig festzustellen, dass es sich um grundlegend verschiedene Herangehensweisen handelt. Und auch etwas anderes gehört zur Wahrheit. Ist ein intelligenter Algorithmus erst einmal trainiert, kann er bestimmte Aufgaben oft sehr viel besser bewältigen als Menschen.Künstlich neuronale Netzwerke, Deep Learning und Machine LearningDer eben im Nebensatz erwähnte Aspekte, dass intelligente Algorithmen „trainiert“ werden müssen, ist zentral. Ganz im Gegensatz zu als anderen Computerprogrammen handelt es sich bei KI-Software nicht um „Out-of-the-box-Lösungen“. Sie werden nicht einmal erstellt und können dann in der Folge wie andere Software immer wieder angewendet werden. Vielmehr handelt es sich um lernfähige Algorithmen, die zunächst trainiert werden müssen und im Laufe ihrer Anwendung immer besser werden. Das macht auch den Charme von bestimmten Geräten und Maschinen aus. Dank Machine Learning werden beispielsweise die Kameras in Smartphones von Google permanent verbessert. Sie lernen mit der Zeit zu verstehen, was auf Bildern zu sehen ist und nehmen entsprechend Optimierungen vor.Dabei gibt es unterschiedliche Lernmethoden wie „künstlich neuronale Netzwerke“, „Deep Learning“ oder „Machine Learning“, um nur die derzeit wichtigsten zu nennen. Auch Machine Learning lässt sich wiederum nochmal nach Lernmethoden unterscheiden. Einerseits in solche Algorithmen auf Basis eines Trainings-Sets selbst optimieren können. Andererseits in solche, bei denen Programmierer beziehungsweise Anwender dem Programm ein Feedback geben, ob bestimmte Ergebnisse richtig sind oder nicht. Je nachdem wie das Feedback ausfällt, kann ein intelligentes Programm Anpassungen für die Zukunft vornehmen. So kann es im Lauf der Zeit immer bessere Antworten geben. Ein entscheidender Aspekt, wenn es darum geht, die Anwendbarkeit und das Potenzial von KI einzuschätzen. ;https://bigdatablog.de/2017/11/13/kuenstliche-intelligenz/;BigDataBlog;Christian Schön
;" Retweets,      ""Smartes Bier ohne Reinheitsgebot? Mit Big Data und Predictive Analytics dem perfekten Bier auf der Spur.""";In Deutschland und insbesondere in der Hauptstadt des Bieres in München ist die Sache mit dem Bier eigentlich klar: Nichts geht hier über das Reinheitsgebot. In Belgien, Schweden oder den USA sind die Brauereien sehr viel experimentierfreudiger und zumindest in Wettbewerben um das beste Bier des Jahres gibt ihnen der Erfolg recht. Die meisten der ausgezeichneten Biere kommen aus diesen Ländern und oft handelt es sich um kleine Mikro-Brauereien, die sich auf eine Marktnische spezialisiert haben.Der Brauprozess ist sehr komplex und durch die Auswertung von Sensordaten und mit der Hilfe von Algorithmen kann der Biergenuss heute verbessert werden. Um das perfekte Bier zu brauen, setzt beispielsweise eine Brauerei aus Portland, Deschutes Brewery, auf Big Data. Ihr Business-Plan sieht vor, pro Jahr verschiedene neue Sorten auf den Mark zu bringen und gleichzeitig die Qualität auf einem konstant hohen Niveau zu halten – und das zum Teil auch bei kleineren Produktionsmengen. Insbesondere für eine kleine Brauerei ist dies ein schwierig zu erreichendes Ziel, das aber dank Predictive Analytics erreicht werden kann.“Mit Big Data zum perfekten Bier: Wie Predictive Analytics hilft, den Brauprozess zu optimieren“ Twittern WhatsApp.Mit Sensordaten und Datenmodellen zum perfekten BierDer Brauprozess wird in etliche unterschiedliche Phasen eingeteilt: Mälzen, Schroten, Maischen, Läutern, Würzekochen, Hopfengabe, Würzeklärung, Würzekühlung, Gärung, Reifung, Filtrieren und schließlich die Abfüllung. Jede dieser einzelnen Phasen hat Auswirkungen auf das Endergebnis – darüber hinaus können auch verschiedene Hopfen- oder Gerstesorten eine charakteristische Note hinterlassen. Jedoch haben insbesondere die Vorgänge, bei denen der angesetzte Sud erhitzt und abgekühlt wird, einen entscheidenden Einfluss auf den Geschmack und die Qualität des Bieres.Mit Sensoren lassen sich die Übergänge zwischen den Vorgängen des Befüllen des Tanks und der Fermentation, der Zugabe der Hefe sowie zwischen dem Abkühlen und dem Reifungungsprozess überwachen. Sie zeichnen dabei zahlreiche Werte wie Temperatur, Druck oder PH-Werte auf. Zunächst werden die Algorithmen mit Testdaten gefüttert, damit sie nach und nach genaue Vorhersagen über die Entwicklung des Brauprozesses machen können. Wenn erst mal ein Modell für den Übergang zwischen zwei Phasen erstellt ist, lassen sich die Erkenntnisse auf andere Prozesse übertragen. Das Ziel ist es, den gesamten Brauprozess einerseits so effizient wie möglich zu machen, um die Kapazitäten der Anlagen voll auszuschöpfen. Andererseits können die Predictive-Analytics-Algorithmen die Qualität des Bieres immer gleich hoch und das Geschmackserlebnis konstant gehalten werden. Da chemische Prozesse nicht immer exakt gleich ablaufen, ist diese Form des Smarten Bierbrauens, das auf Echtzeitanalysen basiert, gegenüber herkömmlichen Methoden im Vorteil.Der Braumeister Brian Faivre von Deschutes Brewery setzt auf Big Data: Neben chemischen Prozessen sind nun auch technische Prozesse fehleranfällig.AI bringt Bierbrauer und Kunden zusammen: Gelebte KundenzentrierungNeben dem Fokus auf den Brauprozess mit Big Data, lassen sich Algorithmen auch dazu verwenden, um Bier dem Geschmack der Kunden anzupassen. Kunden können heute via Smartphone den Unternehmen eine direktes Feedback geben, ob sie mit dem gekauften Produkt zufrieden sind. Ein britischer Bierbrauer macht sich diese Informationen zunutze und passt mithilfe Künstlicher Intelligenz (AI) den Geschmack seines Biers den Wünschen der Kunden an. Gleichzeitig versucht er durch das Feedback die Qualität seines Erzeugnisses permanent zu verbessern:Durch das Einscannen eines individuellen Codes auf den Etiketten kann ein AI-Bot die Bewertungen der Kunden exakt zuordnen und auswerten.“AI und Mashine Learning lassen Kunden direkt mit Brauereien kommunizieren mit einem gemeinsamen Ziel: Besseres Bier“ Twittern WhatsAppNicht nur der Brauprozess kann mit Hilfe von Datenauswertungen verbessert werdenDer Genuss eines Bieres hängt auch vom richtigen Zeitpunkt des Öffnens der Flasche und der perfekten Trinktemperatur ab. Auch hier können Algorithmen helfen: Ab welchem Zeitpunkt verliert ein abgefülltes Bier den charakteristischen Geschmack, für den man es gekauft hat? Dieser Zeitpunkt lässt sich exakt messen und beispielsweise auf dem Etikett angeben. Auch die perfekte Lager- und Trinktemperatur lassen sich so ermitteln und kommunizieren. Vorreiter ist auch hier die Deschutes Brewerey, die nicht mehr das Haltbarkeitsdatum ihrer Biere auf dem Etikett angibt, sondern auch exakt den Zeitpunkt bestimmen kann, ab dem bestimmte Geschmackskomponenten nicht mehr zu schmecken sind.Solche Vorteile lassen sich nicht nur bei Getränken wie Bier nutzen, sondern auch bei Whiskey. Jede Flasche der Marke Johnnie Walker ist heute ein Teil des Internet der Dinge. Für das Unternehmen ergeben sich daraus eine ganze Reihe von Vorteilen – so kann beispielsweise die gesamte Lieferkette überwacht und auf diese Weise der Diebstahl von Waren minimiert werden. Aber auch für den Kunden selbst ergeben sich Vorteile: Er erhält einen genauen Überblick über die Historie der Lagerung jeder einzelnen Flasche. So kann sichergestellt werden, dass beispielsweise die Lagertemperatur immer optimal war.Mit jedem Smartphone lassen sich zahlreiche Daten über den Lebenszyklus der individuellen Flasche und zusätzlicher Content für die Kunden abrufen.Die Digitalisierung aller Lebensbereiche macht auch vor dem Reinheitsgebot nicht haltDer Kontrast könnte größer kaum sein: Auf der einen Seite das Reinheitsgebot und alte Kellergewölbe, in denen Fässer mit Bier, Wein oder Whisky lagern und auf der anderen Seite Sensoren, Algorithmen und Big Data. Die Digitalisierung aller Lebensbereiche kennt kaum Grenzen und bringt zahlreiche Vorteile für die Kunden und für Unternehmen mit sich. Mit dem Wissen über den Brauprozess können beispielsweise kleinere Brauereien wie die Deschutes Brewerey ihr Geschäftsmodell kontinuierlich erweitern: Neue Geschmacksrichtungen lassen sich schneller entwickeln und entsprechend vermarkten. Die Kunst besteht heute darin, geeignete Datenquellen zu finden oder zu schließen, um einerseits Daten zu sammeln und andererseits aus diesen Datenmengen konkretes Wissen zu ziehen. Die Digitalisierung schafft damit etwas, das besonders hierzulande als heilig gilt: Die Verbesserung des Reinheitsgebotes.Wenn wir das aber anders denken wollen, zitiere ich gerne Antje Neubauer, die sagt: „Wir haben als Unternehmen auch eine gesellschaftliche Verantwortung, die wir ernst nehmen, indem wir Haltung zeigen und auf Werte verweisen, die in unserer Demokratie selbstverständlich sein sollten.“ Big Data hat Vorteile, aber die Verantwortung über die Daten und damit verbunden die Werte die ein Unternehmen hat, ist immer ethisch zu durchdenken.;https://bigdatablog.de/2017/02/10/smartes-bier-ohne-reinheitsgebot-mit-big-data-und-algorithmen-dem-perfekten-bier-auf-der-spur/;BigDataBlog;Ibrahim Evsan
09. Jan 17;" Retweets,      ""Big Data und Deep Learning: Wie Maschinen Bilder verstehen können und visuelle Navigation die Welt der Bilder erschließen.""";Bilder sind Zeichen. Diese wichtige Einsicht ist nicht selbstverständlich. Da Bilder auch die Realität abbilden, können wir „verstehen“, was wir auf Bildern sehen, weil es diese Ähnlichkeitsbeziehung gibt. Sehen wir ein Bild von einem mit Gras bewachsenen Hügel, auf dem ein Mensch steht, der den Sonnenuntergang betrachtet, verstehen wir dieses Bild, weil wir Hügel, Mensch und Sonnenuntergängen aus unserer Erfahrung heraus kennen. Aber wie bringt man einem Computer bei, was auf Bildern zu sehen ist, wo ihnen doch sowohl diese Form der Seh-Erfahrung und das Weltwissen als auch unser Bild-Verständnis fehlen? Diese Frage ist alles andere als trivial, weil sie doch darüber entscheidet, ob und wie Unternehmen, die über gigantische Archive verfügen, Bilder verwalten und die darin steckenden Informationen nutzen können.Die Frage ist also, wie es gelingen kann, Computern beziehungsweise intelligenten Programmen beizubringen, was überhaupt auf Bildern zu sehen ist und wie dieser Bildinhalt zu verstehen ist. Es gibt eine ganze Reihe von Bildtypen, die wir problemlos lesen können, und das obwohl es in einigen Fällen den eingangs erwähnten Realitätsbezug nicht gibt oder obwohl die Bedeutung dessen, was ein Bild darstellt, komplexer ist als der reine Ähnlichkeitsbezug. Ein Symbol ist beispielsweise mehr als ein bloßes Abbild eines Gegenstandes. Mit einer Rose oder einem Herz können wir starke Affekte wie Liebe zum Ausdruck bringen und wollen dem Gegenüber nicht nur das Bild einer bestimmten Pflanze oder eines Organs zeigen. Auch Gemälde oder mit Photoshop veränderte Fotografien zeigen nicht mehr einfach nur die Realität, sondern erzählen Geschichten, machen vielleicht Anspielungen auf andere Bilder oder verwenden einen speziellen Code.In Anlehnung an René Magrittes Gemälde „Ceci n’est pas une pipe“ (© Katja Gerasimova @ Shutterstock.com). Können Computer mit Hilfe von Deep Learning den Unterschied zwischen einer Pfeife und dem Bild von einer Pfeife verstehen?Warum Google Bilder findet… und warum nichtDie Bildsuche von Google gibt es zwar seit vielen Jahren. Diese funktionierte aber lange Jahre nur deshalb, weil der Suchalgorithmus von Google die Texte durchsuchte, in denen auch Bilder vorkamen. Die Begleittexte lieferten die gesuchten Stichworte und so kam das Ergebnis zustande. Die Annahme, dass Texte und Bilder zusammengehören und sich gegenseitig erklären, musste als Annahme gegeben sein, damit die Google Bilder-Suche erfolgreich war. Während Google inzwischen auf Deep-Learning-Algorithmen umgestellt hat, basieren Bildsuchen in anderen Bildarchiven großen Teils noch auf der Verschlagwortung von Bildern, sprich: Metadaten. Bildagenturen wie Shutterstock oder 500px lassen ihre Kunden beim Upload die entsprechenden Keywords hinzufügen. Eine solche Kombination aus Text und Bild stellt eine effektive Möglichkeit dar, um mit Bildern umzugehen. Aber diese Methode hat genau an diesem Punkt auch ihre Grenzen: Beispielsweise aufgrund von Homonymen, also Wörtern die etwas unterschiedliches bezeichnen, obwohl sie genau gleich klingen oder geschrieben werden. Sie können zu falschen Suchergebnissen führen. Eine Bildsuche nach „Golf“ kann sowohl zu Ergebnissen beinhalten, die ein Auto der Marke VW enthalten oder die eine bestimmten Sportart darstellen, aber sogar eine Meeresformation zeigen (z.B. „Golf von Mexiko“).Auch Falschschreibungen von den Suchenden selbst können das Suchergebnis verfälschen oder zumindest die Suche erschweren. Nicht nur, dass solche Methoden per se keine adäquate Lösung des Problems darstellt, es stellt sich darüber hinaus die Frage: Was machen wir mit Bildarchiven, die aus vielen Millionen von Bildern bestehen? Diese von Hand durchzugehen und mit Stichworten zu versehen, ist ein Ding der Unmöglichkeit oder zumindest hochgradig ineffizient. Deep Learning, ein Teilaspekt von Machine Learning, basiert auf der Idee von neuronalen Netzwerken. Algorithmen dieser Art können, sind sie erst einmal trainiert, erstaunliche Dinge leisten: “Mit der Hilfe von #DeepLearningAlgorithmen lernen Computer, Bilder zu verstehen. #BigData“ Twittern WhatsAppWie klassifiziert Facebook alle Bilder, die auf die Plattform hochgeladen werden?Auch Facebook arbeitet seit längerem an einem auf künstlicher Intelligenz beruhenden Algorithmus, der dazu in der Lage ist, Bilder zu verstehen. Das Ziel ist es unter anderem die nur visuell wahrnehmbaren Inhalte auch all den Menschen zugänglich zu machen, die blind oder sehbehindert sind. Allein in Deutschland, für das bezeichnender Weise exakte Zahlen fehlen, handelt es sehr wahrscheinlich um weit mehr als 1,2 Mio. Menschen (Stand: 2002).Wie genau und wie umfassend Facebook dabei ist, Bilder erkennen zu lassen, lässt sich mit der frei erhältlichen Extension für Google Chrome nachvollziehen. Mit„Show Facebook Computer Vision Tags“ lassen sich alle Labels sichtbar machen, die Facebook einem Bild automatisch zuordnet:Auch mit dem kostenlosen Online-Tool „akiwi“ ist es möglich, für seine eigenen Bilder solche Keywords zur Verschlagwortung automatisch finden zu lassen. Akiwi entstand als Studentenprojekt im Kontext der Forschung der Hochschule für Technik und Wirtschaft Berlin (HTW), wo man sich seit längerem mit Deep Learning und visueller Navigation sowie Bildanalyse beschäftig. All diese Versuche stehen aber an der Schnittstelle zwischen Bilderkennung und Verfahren zur textbasierten Bildsuche.Bilder als wichtige Datenquelle erschließenInzwischen verfügen nicht nur soziale Netzwerke, Bild- und Medienagenturen sowie Foto-Communities wie Flickr und E-Commerce-Händler über unvorstellbare Mengen an Bilddaten. Nahezu jedes größere Unternehmen hat in seinen Daten-Archiven neben Textdokumenten, Tabellen oder Maschinendaten auch Bilder gespeichert. Angesichts der Unzulänglichkeiten der textbasierten Bildsuche stellt sich die dringliche Frage, wie es möglich ist erstens Algorithmen zu entwickeln, die selbständig Bilder erkennen und zweitens ob es neue Wege gibt, Bilder zu durchsuchen. Darum beschäftigen sich die Forscher an der HTW mit neuen visuellen Verfahren, zur Darstellung von großen Bilddatenbanken und zur Bildsuche. Das Prinzip von Google-Maps aufgreifend stellt Picsbuffet Bilder in Form einer Bilder-Landkarte dar, über die die Nutzer in Vogelperspektive fliegen.Dieses neue Prinzip, Bilder darzustellen, hat gegenüber herkömmlichen Darstellungsformen, bei denen Listen von oben nach unten durchsucht werden müssen, viele Vorteile: Duplikate werden sofort erkannt und ähnliche Bilder in Gruppen dargestellt. Nimmt man das praktische Beispiel E-Commerce, so wird es durch die alternative Darstellungs- und Suchform möglich, ähnliche Produkte auf einen Blick zu finden. Aber nicht nur bei der Produktsuche können mit Deep-Learning-Algorithmen neue Präsentationsformen entwickelt werden. Auch bei der Suche nach urheberrechtswidrig gebrauchten Bildern wäre eine auf den Prinzipien von Deep Learning basierte Bildsuche überlegen.Deep Learning: Maschinen lernen, Bilder zu sehen und zu verstehenEntwicklungen in diesem Bereich sind darum so wichtig, weil sie einen wichtigen Bestandteil beim Autonomen Fahren spielen. Bilderkennung ist insofern wichtig, als dass autonom fahrende Autos Bilder wie Verkehrszeichen sehen und richtig kategorisieren können müssen. Sobald die Deep-Learning-Algorithmen einmal trainiert sind, leisten sie erstaunliches. Das „German Traffic Sign Recognition Benchmark“ des Instituts für Neuroinformatik der Universität Bochum übertraf schon vor vielen Jahren die Leistung von Menschen bei der selben Aufgabe bei weitem. Doch nicht nur weil Deep-Learning-Algorithmen exakter und schneller arbeiten, sind sie in Zukunft unverzichtbar. Allein durch die schiere Menge der Datenmassen ist es notwendig, Maschinen das Sehen und Verstehen von visuellen Informationen beizubringen. Deep-Learning-Netzwerke schaffen die Grundlage zur Verwaltung von Big Data, erschließen das wirtschaftliche Potenzial, das in Bilddaten steckt, und stellen einen wichtigen Baustein für weitere künftige Entwicklungen dar. Während heute daran gearbeitet wird, die Mensch-Maschine-Kommunikation mittels natürlicher Sprache zu optimieren, wird es in Zukunft möglich sein, dass Maschinen ihre Umwelt sehen und interpretieren können.;https://bigdatablog.de/2017/01/09/big-data-und-deep-learning-wie-maschinen-bilder-verstehen-koennen-und-visuelle-navigation-die-welt-der-bilder-erschliessen/;BigDataBlog;Christian Schön
;;;https://connectedleadership.de/digital-leadership-workshops/;BigDataBlog;
