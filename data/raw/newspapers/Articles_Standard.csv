Datum;Titel;Text;Link;Quelle;Autor 
05.05.2018;"Data Scientist: ""Der Großteil der Daten ist Müll""";"Seine Aufgabe: Aus einer Masse an Daten Informationen zu gewinnen. Oft verlaufe die Suche länger ergebnislos, sagt ein Data-Scientist, mit dem wir gesprochen haben. ""Jeder von uns erzeugt abertausende Daten pro Tag – ob als Autofahrer mit dem Navi oder als Konsument beim Einkaufen, ob mit dem Smartphone oder der Kreditkarte. Die Aufgabe von uns Data-Scientists ist es, aus dieser unglaublichen Fülle sinnvolle Informationen herauszufiltern. Dafür spielen wir sehr viel mit Daten herum, drehen sie, wenden sie, werten sie aus – um herauszufinden, wie man damit ein Produkt verbessert, die Ansprache der Kunden oder neue Erkenntnisse für die Wartung hervorbringt und so schließlich Zeit und damit Geld sparen kann. Ein gutes Beispiel für den Nutzen von Data-Science liefern die Wiener Linien: Sie sammeln beinahe minutiös verschiedene Informationen über die Straßenbahnen – messen die Temperatur der Bremsen ebenso wie den Neigungswinkel der Führerkabine. Anhand der Ergebnisse wird schließlich versucht, zu prognostizieren, wann die Bahn kaputtgeht, damit man rechtzeitig ein Wartungsteam hinschicken, so die Stehzeiten minimieren und letztendlich effizienter sein kann. Data-Science wird aber noch in vielen anderen Gebieten angewendet, es ist ein sehr breites Feld. Der Online-Versandhändler Amazon sieht sich beispielsweise an, was Kunden gekauft haben, um sie noch zielgenauer anzusprechen. Jeder kennt den Hinweis: ""Kunden, die diesen Artikel gekauft haben, kauften auch ..."" Dahinter steckt Data-Science. Eine Querschnittsmaterie. Ein mathematisches Verständnis, um gewisse Trends zu erkennen, ist Grundvoraussetzung für den Beruf. Im Grunde genommen ist er jedoch eine Querschnittsmaterie. Man braucht neben Analysefähigkeiten auch Fachexpertise, Programmierkenntnisse, und am allerwichtigsten ist Kommunikationsstärke. Denn die besten Ergebnisse nutzen nichts, wenn ich sie nicht visualisieren und verständlich weitergeben kann. Mit einem bloßen Excel-Sheet kann niemand etwas anfangen. Die Geschäftsführung muss wissen, was die Zahlen sagen. Die Erwartungen an Data-Scientists sind inzwischen sehr hoch. Jeder Geschäftsführer scheint zu glauben, dass er damit das Zauberrezept findet. Man braucht viel Geduld. Das Problem: Data-Science ist keine genaue Wissenschaft, sondern immer nur eine Annäherung. Ich schaue mir sehr viele Daten an, und der Großteil ist kompletter Müll, komplett ergebnislos. Es geht also sehr viel Zeit verloren, ohne Resultat. Da braucht man in der Geschäftsführung schon sehr viel Geduld, und gerade in Österreich sind viele Geschäftsführer nicht die geduldigsten. Typischerweise führt der Weg in den Job über ein IT-Studium. Österreichische Universitäten und Fachhochschulen bieten ebenfalls schon entsprechende Spezialisierungen und Aufbauprogramme an. Es gibt mittlerweile auch Fernstudien. Die waren mir allerdings zu langwierig, deshalb habe ich in den letzten zwei Jahren verschiedene Lehrgänge absolviert. Ich bin also nicht der klassische Data-Scientist, sondern ein Sonderfall. Ich habe Publizistik studiert, war Pressesprecher und habe zuvor 15 Jahre lang im Journalismus gearbeitet. Für Quereinsteiger öffnen. Die meisten Stellenausschreibungen richten sich nicht an Quereinsteiger wie mich, sondern an Mathematik- und Informatikabsolventen. Es ist so wie bei vielen Jobs: Die Firmen suchen idealerweise 23-Jährige mit einem akademischen Abschluss und sieben Jahren Auslandserfahrung, die fünf Sprachen sprechen. Und den Job am besten für 1700 Euro brutto machen. Das spielt es nicht. Die ersten akademischen Programme werden erst in ein bis zwei Jahren abgeschlossen sein und Absolventen bringen. Man ist also auf Quereinsteiger angewiesen. Sich zu öffnen und nicht nur an der Technischen Universität nach Mitarbeitern zu suchen wäre keine blöde Idee. Zumal die Firmen sich auch nicht so richtig darüber im Klaren zu sein scheinen, was genau sie eigentlich wollen oder suchen. In den Stelleninseraten wird alles Mögliche – zum Beispiel Business-Intelligence und Datenbankexpertise – mit Data-Science vermischt. Jeden Tag etwas Neues. Was ich an meinem Beruf mag, ist, dass ich jeden Tag etwas Neues lerne. Man muss unglaublich wissbegierig sein, weil sich gerade in diesem Feld unglaublich viel tut. Es gibt tagtäglich neue Funktionen, Algorithmen. Ein Nachteil ist vielleicht, dass ich jetzt mehr am Computer sitze als in meinem früheren Job. Da man die Datenanalysen ja aber nicht zum Selbstzweck macht, bin ich auch viel im Haus unterwegs und führe Gespräche mit den Fachabteilungen. Was die Zukunftsträchtigkeit dieses Berufs angeht, bin ich zuversichtlich. Künstliche Intelligenz wird in Zukunft sicherlich viele Tätigkeiten ersetzen – deskriptive Statistiken können Computer beispielsweise viel besser, viel schneller und exakter erstellen als der Mensch. Ich habe dennoch keine Angst, in den nächsten Jahren automatisiert zu werden. Denn sobald es darum geht, Ergebnisse zu kommunizieren, kann die künstliche Intelligenz nicht mithalten.""";https://www.derstandard.at/story/2000078801319/data-scientistder-grossteil-der-daten-ist-muell;Standard;Lisa Breit
27.10.2018;"""Sexiest job"": Data Scientist";"Hohe Einstiegsgehälter und Fachkräftemangel: So gut wie jede Branche sucht Datenanalysten. Was den Job so sexy macht. Es beginnt schon beim Schuhekaufen im Internet: Unternehmen sammeln permanent Daten über ihre Kunden. Daten gelten als das ""Erdöl des 21. Jahrhunderts"". Um daraus auch einen wirklichen Nutzen zu ziehen, müssen sie jedoch erst analysiert und ausgewertet werden. Das ist die Aufgabe von ""Data Scientists"". Der Data Scientist wurde von der Harvard Business Review zum ""sexiest job of the 21st century"" gekürt. Diese Spezialisten haben auch gute Chancen auf einen Job. In einer Umfrage der Personalberatung Russell Reynolds Associates gaben 58 Prozent der weltweit befragten Führungskräfte einen Mangel an. Data Science wird quasi in allen Bereichen gebraucht – von Banken und Versicherungen bis in die Medizin. Und das wird sich künftig nicht ändern: Das globale Datenvolumen verdoppelt sich angeblich alle zwei Jahre. Den Begriff Data Scientist gibt es seit ungefähr zehn Jahren. Laut Medienberichten wurde er von den Datenspezialisten bei den Online-Netzwerken Facebook und LinkedIn geprägt. ""Wir wussten nicht, wie wir uns selbst nennen sollten"", sagt Dhanurjay Patil, der zu dieser Zeit das Daten-Team bei LinkedIn leitete. ""Research Scientist"" habe zu akademisch geklungen, ""Economist"" hätte ""den Statistikern das Blut in die Augen getrieben und umgekehrt"", sagt Patil. Er testete verschiedene Berufsbezeichnungen und fand heraus, dass sich auf ""Data Scientist"" auf LinkedIn die meisten Bewerber mit den richtigen Qualifikationen meldeten. Voilà, ein neuer Beruf war entstanden. Ausbildungen entstehen. In den USA gibt es schon seit 2013 Ausbildungen für Data Scientists, beispielsweise an der Harvard University. An der New York University bietet man mittlerweile sogar schon PhD-Programme für Data Scientists an. Auch das Massachusetts Institute of Technology hat offenbar ein entsprechendes Doktorandenprogramm in Arbeit. Österreich zieht nach. Einige Universitäten und Fachhochschulen haben bereits einschlägige Kurse geschaffen (siehe Kasten). Voraussetzung für den Beruf sind: ""Analytisches Denken, ein Gefühl für Zahlen"", sagt Andreas Rauber von der Technischen Universität Wien, wo diesen Monat ein Masterprogramm startet. Auch Kommunikationsstärke ist wichtig. Denn ""der Data Scientist ist nicht der Nerd, der hinter dem Computer hockt und sich nur in seine Daten hineinvertieft"", so Eva Werner, Rektorin der FH Krems, die ebenfalls ein neues Programm lanciert hat. Die besten Ergebnisse nutzen nichts, wenn sie nicht verständlich an die Verantwortlichen weitergegeben werden. Sie müssen wissen, was die Zahlen aussagen. Außerdem brauche man Kreativität, um auf neue Lösungen zu kommen – das ist auch das, was die Spezialisten den Maschinen voraushaben und wahrscheinlich noch die nächsten Jahre voraushaben werden. Kirim über sich ... ""Für viele Probleme gibt es keine einfachen Lösungen, da braucht es Kreativität. Dieses Tüfteln, das mag ich besonders an meinem Beruf. Ich habe Informatik an der Technischen Uni Wien studiert. Da hatte ich ein paar Lehrveranstaltungen über Machine Learning. Über ein Projekt bin ich vor eineinhalb Jahren zu meinem jetzigen Job gekommen. Wir betreiben eine Flohmarkt-App, über die Menschen ungenutzte Dinge verkaufen können. Wenn sie ein Produkt hochladen, können sie eine Kategorie angeben. Manchmal vergessen sie das oder sind zu bequem. Und da komme ich ins Spiel: Ich programmiere ein System, das erkennt, dass das auf dem Bild eine Lampe ist und in die Kategorie 'home' gehört. Sie können auch Texte erkennen. Außer mir gibt es noch drei andere Data Scientists im Unternehmen. Wir sind nicht nur Statistiker, sondern auch Entwickler. Manche Tools eignen sich nicht für unsere Daten. Deshalb entwickeln wir auch eigene, damit wir neue Probleme besser in Angriff nehmen können.""";https://www.derstandard.at/story/2000088978702/sexiest-job-data-scientist;Standard;Lisa Breit
22.10.2020;Die Zukunft der Stadt besser voraussagen;"Mit Methoden der künstlichen Intelligenz versuchen Forscher, die Prognostik der Bevölkerungsentwicklung in Graz zu verbessern. Entscheidungsträger von Städten sind permanent mit Fragen langfristiger Tragweite konfrontiert. Wohin soll man Krankenhäuser, Schulen oder Kindergarten bauen? Und wie viele? Benötigt man in einem bestimmten Zeitrahmen mehr Ressourcen in der Altenpflege? Wie wird sich der Verkehr entwickeln? Da man die Zukunft bekanntlich nicht vorhersehen kann, behilft man sich meist mit klassischer Statistik. Doch vielleicht lässt sich mit Methoden der künstlichen Intelligenz (KI) präziser vorhersagen, wie sich die Bevölkerungsstruktur im Stadtgebiet entwickeln wird? Ein Team um Wolfgang Granigg, Leiter der Studiengänge Business in Emerging Markets und Data and Information Science an der FH Joanneum, prüft derzeit in einem Forschungsprojekt verschiedene Ansätze aus der KI daraufhin, wie gut sie sich für derartige Prognosen im Fall von Graz eignen. Das Projekt ist Teil des vom Digitalisierungsministerium geförderten Rahmenprogramms Big Data Analytics & Artificial Intelligence Research Center, kurz: FIT4BA. Ein Projektpartner ist die Stadt Graz, welche die – natürlich anonymisierten – statistischen Bevölkerungsdaten zur Verfügung stellt. Die Stadt ist dabei vor allem an zwei Kenngrößen interessiert: an der Altersstruktur und an der Bevölkerungsdichte in den 17 Grazer Bezirken. Am Ende des Projekts soll sie ein fertiges Modell erhalten, mit dem die Verantwortlichen auf Grundlage aktualisierter Monatsdaten bis zu 20 Jahre in die Zukunft modellieren können, wie viele Menschen in welchem Bezirk wohnen. Retrospektive Prognose. ""Traditionell bedient man sich bei der Vorhersage von Bevölkerungsentwicklungen statistischer Methoden"", erklärt Granigg. ""Wir möchten herausfinden, ob modernere Verfahren zu plausibleren Ergebnissen führen und vielleicht interessante Muster in den Daten freilegen."" Zu den betrachteten Methoden gehören die mathematische Modellierung mittels Differenzialgleichungen, sogenannte Markow-Ketten, der Einsatz künstlicher neuronaler Netze sowie die agentenbasierte Simulation. Doch wie lässt sich verifizieren, ob ein Modell wirklich gut ist? Um die Qualität einer Prognosemethode zu testen, behilft man sich eines simplen Tricks. Man wendet die Methode auf Daten aus der Vergangenheit an und prognostiziert damit, quasi retrospektiv, Daten einer anderen bereits vergangenen Periode. So kann man beispielsweise auf Basis der Bevölkerungsdaten von 2000 bis 2010 ein Prognosemodell erstellen und dieses zur Vorhersage der Entwicklung von 2010 bis 2015 verwenden. Da diese Entwicklung bereits bekannt ist, zeigt sich unmittelbar, wie treffsicher das Modell ist. Welche Methode sich letztlich als die erfolgreichste erweisen wird, ist für Granigg derzeit noch nicht absehbar. Entscheidend sei jedenfalls nicht nur, dass die Vorhersage möglichst präzise ist. Genauso wichtig ist die Transparenz. ""Statistische Analysen sind eine Art Black Box"", so Granigg. ""Man steckt vorn etwas hinein und bekommt hinten etwas heraus. Aber wie das Ergebnis genau zustande kommt, erkennt man nicht."" Virtuelle Personen. Anhand der agentenbasierten Simulation lässt sich intuitiv zeigen, was mit der gewünschten Transparenz gemeint ist. Dabei werden im Rechner voneinander unabhängige, virtuelle Personen definiert, im Fall von Graz etwa knapp 300.000. Jede dieser Einheiten hat außerdem mehrere Eigenschaften, beispielsweise Alter, Wohnbezirk oder Geschlecht. Dann definiert man gewisse Regeln, wie sich die virtuellen Personen im Zeitverlauf verhalten sollen. Beispielsweise kann man vorgeben, mit welcher Wahrscheinlichkeit jemand in einen anderen Bezirk zieht, und lässt das Modell rechnen. Das Resultat ist eine schrittweise Entwicklung der Gesamtbevölkerung. Da man jede virtuelle Person einzeln oder auch zu Gruppen zusammengefasst auf der Mikroebene betrachten kann, lassen sich Ursachen für das Makroverhalten der Gesamtbevölkerung identifizieren. Man erkennt also nicht einfach bloß, wie viele Menschen zu einem gegebenen künftigen Zeitpunkt wahrscheinlich in welchem Bezirk leben werden. Man sieht auch den Einfluss von Faktoren wie Zuzug und Wegzug, von Geburten und Sterbefällen. Ein weiterer Vorteil: Man kann Hypothesen aufstellen und prüfen, indem man einfach die Regeln entsprechend umprogrammiert. ""Mit diesem Ansatz können wir tief in die Dynamik der Bevölkerungsentwicklung hineinsehen"", sagt Granigg. ""Wir wollen nicht nur das Ergebnis der Dynamik sehen, sondern auch die Dynamik selbst verstehen.""";https://www.derstandard.at/story/2000120024616/die-zukunft-der-stadt-besser-voraussagen;Standard;Raimund Lang
13.11.2020;Browsererweiterung bringt die gute alte Google-Suche zurück;"Simple Search will demonstrieren, wie viel Platz mittlerweile für die Integration von Google-eigenen Diensten aufgewendet wird. Was macht eine gute Suchmaschine aus? Lange war die Antwort auf diese Frage recht simpel: Die zu einem Suchbegriff relevantesten Links müssen in einer Liste entsprechend gereiht präsentiert werden. Doch Google hat mittlerweile andere Vorstellungen: Statt einer Such- will man eine Wissensmaschine sein, die nicht bloß gereihte Ergebnisse, sondern gleich direkt passende Antworten liefert. Blickpunkte. Google argumentiert damit, dass dies im Sinne eines Großteils der Nutzer sei, denen es nur darum gehe, so schnell wie möglich passende Informationen zu erhalten. Gleichzeitig ist aber unleugbar, dass durch all die zusätzlichen Infokästen die eigentlichen Suchergebnisse weniger Relevanz erhalten – was zuletzt auch aus einer kartellrechtlichen Sicht zuletzt immer stärker in die Kritik gekommen ist. Simple Search. Unter dem Namen ""Simple Search"" gibt es nun eine neue Browsererweiterung für Chrome und Firefox, die eine Rückkehr zur alten Suchansicht von Google ermöglicht. Ganz so, wie sie über viele Jahre angeboten wurde. Hinter der Erweiterung stecken allerdings keine nostalgischen Fans des alten Google, sondern die Publikation ""The Markup"", die damit ein konkretes Ziel verfolgt. Simple Search soll demonstrieren, wie viel Raum Google-eigene Dienste mittlerweile unter den Suchergebnissen einnehmen. Entsprechend ersetzt die simple Ansicht auch nicht die aktuelle Darstellung, sondern wird als Fenster über diese geblendet. Hintergrund. Eine Untersuchung von ""The Markup"" hatte schon vor einigen Monaten gezeigt, dass bei 15.000 populären Suchanfragen im Schnitt Google-Services 41 Prozent des Raums auf der ersten Seite einnehmen. Auf mobilen Geräten seien es gar 63 Prozent.";https://www.derstandard.at/story/2000121668085/browsererweiterung-bringt-die-gute-alte-google-suche-zurueck;Standard;
10.09.2020;Die andere Corona-Ampel;"Schon seit April ist eine Corona-Ampel des Big-Data-Thinktanks CSH Vienna online. Die Regierung zeigte daran kein Interesse – und bastelte selbst eine. Um sich zu vergewissern, welche Gefährdungsfarbe die Corona-Kommission des Gesundheitsministeriums der jeweiligen Wohnumgebung zugeteilt hat, ist es ratsam, die entsprechende Corona-Homepage des Ministeriums abzurufen. Die Onlinesuche kann allerdings erhebliche Verwirrung stiften. Denn: Es existieren mittlerweile nämlich zwei Corona-Ampeln in Österreich. Eine, die kürzlich gestartete, offizielle Ampel des Ministeriums und jene schon seit dem 8. April permanent aktualisierte des Big-Data-Forschungszentrums Complexity Science Hub Vienna (CSH). ""Die Ampel ist zur Information der Bevölkerung gedacht, damit jeder nachprüfen kann, wie groß das Risiko in der eigenen Umgebung ist. Wir zeigen die Infektionszahlen und wie groß die Wahrscheinlichkeit ist, sich anzustecken"", sagt CSH-Präsident Stefan Thurner, Professor für Komplexitätsforschung an der Med-Uni Wien, im Gespräch mit dem STANDARD. Zudem liefert das CSH neben den Corona-Zahlen Daten über die Ärztedichte, eine Analyse der Regionen nach Risikogruppen samt Anzahl der Neuinfizierten und per Klick zudem sämtliche Arztadressen samt Öffnungszeiten. Dazu auch Covid-Analysen etwa über Angst und Sozialverhalten in der Corona-Krise. Stefan Thurner kann in den Berechnungen und Analysen auf einen exquisiten Thinktank zurückgreifen. In seinem wissenschaftlichen Beratungsgremium sitzen internationale Topexperten renommierter Universitäten aus den USA, Europa und Asien. Getragen wird die Forschungseinrichtung übrigens unter anderem von den Technischen Universitäten Wien und Graz, der Medizinischen Universität Wien, der Wirtschaftsuniversität (WU) Wien, dem Austrian Institute of Technology (AIT), der Wirtschaftskammer und dem Internationalen Institut für angewandte Systemanalyse (IIASA). ""Sind nicht gefragt worden"" Die im CSH angewandte Komplexitätswissenschaft verbindet Mathematik, Modellierung und Informatik mit grundlegenden Fragen aus Disziplinen wie Wirtschaft, Ökologie, Sozialwissenschaften und eben auch der Medizin. Was zur Frage führt: Warum hat die Regierung nicht auf das wissenschaftsbasierte Ampelmodell des öffentlich finanzierten Big-Data-Spezialisten CSH aufgebaut, sondern eine eigene gebastelt? ""Ich weiß es nicht"", sagt Stefan Thurner, ""wir hätten natürlich gerne unser Wissen zur Verfügung gestellt, sind aber nicht gefragt worden. Natürlich würden wir uns freuen, wenn man auf uns zukommt und sagt, lasst uns zusammenarbeiten."" Es gebe zwar informelle Kontakte mit Mitgliedern der ministeriellen Ampel-Kommission. Einen offiziellen Auftrag der Zusammenarbeit gebe es allerdings nicht, sagt Thurner. ""Es wäre natürlich kein Problem gewesen"", sagt Thurner, die CSH-Ampel für die neuen Anforderungen rasch zu adaptieren. Im Büro Minister Rudolf Anschobers wird auf STANDARD-Nachfrage darauf verwiesen, dass ""die CSH-Ampel auf nur einem Indikator aufbaut, bei der Corona-Ampel werden mehrere Indikatoren zur Bewertung der Lage berücksichtigt"". Zudem würden die Daten ""auch diskutiert und gewichtet"". ""Es geht ja nicht nur um Zahlen"", heißt es im Büro Anschober.";https://www.derstandard.at/story/2000119895607/die-andere-corona-ampel;Standard;Walter Müller
09.10.2020;Abwärme und Co intelligenter steuern;"Ein neues Josef-Ressel-Zentrum an der FH Vorarlberg entwickelt digitale Services zur Nutzung der Daten von thermischen Systemen. Thermischen Systemen ein Gehirn geben: Das ist das Ziel eines neuen Josef-Ressel-Zentrums an der Fachhochschule Vorarlberg. ""Firmen sammeln Betriebsdaten zu ihren Maschinen und Anlagen. Alle wollen die Daten besser nutzen und träumen von datenbasierten Services. Aber die Verbindung dazwischen fehlt oft"", sagt Markus Preißinger, Leiter des Forschungszentrums, das kürzlich eröffnet wurde. Gemeinsam mit seinem zehnköpfigen Team will er Algorithmen liefern, mit denen dann etwa die Wartung verschiedener thermischer Systeme optimiert werden kann. Dafür arbeiten die Wissenschafter mit fünf regionalen Unternehmen zusammen. Diese enge Kooperation mit den jeweiligen Wirtschaftspartnern prägt das Format der Josef-Ressel-Zentren: Das Ziel ist die praxisorientierte Forschung. Einerseits um Innovationen in regionalen Unternehmen zu fördern und ihnen Zugang zu den Ressourcen von FHs zu geben. Andererseits um die Forschungskompetenz auszubauen und die FH-Ausbildung zu stärken. Gefördert werden die Zentren für je fünf Jahre durch die Christian-Doppler-Gesellschaft (CDG) und das Wirtschaftsministerium. Fehlerfrüherkennung. Eine thematische Ausrichtung gibt es in den Förderrichtlinien nicht – man überlasse es den Unternehmen und Antragstellenden, die relevantesten Bereiche vorzuschlagen, erklärt CDG-Generalsekretär Jürgen Pripfl: ""Diese Strategie hat sich bewährt. Dadurch sind die Zentren immer am Puls der Zeit."" Aktuell beschäftigen sich die meisten Josef-Ressel-Zentren mit Digitalisierung, insbesondere mit Nachhaltigkeit, Industrie 4.0 und IT-Security. Derzeit gibt es in Österreich insgesamt 13 Josef-Ressel-Zentren – und es sollen bald mehr werden. Die CDG hofft, dass es ab spätestens 2025 stets 20 aktive Zentren geben wird, so Pripfl. Im Fall des neuen Josef-Ressel-Zentrums für intelligente thermische Energiesysteme sollen die Projektpartner zum Beispiel dabei unterstützt werden, schon vor dem Ausfall einer wichtigen Pumpe davon Kenntnis zu erlangen, dass diese ausgetauscht werden muss. Für einen weiteren Projektpartner ist ein System geplant, mit dem die Abwärme der Produktion intern noch sinnvoller genutzt werden kann. Und für ein Biomassekraftwerk entwickeln die Forscher eine Fehlerfrüherkennung für die Wasseraufbereitung. Mathematik und Energietechnik. ""Wenn es um Digitalisierung geht, erhält der elektrische Bereich momentan sehr viel mehr Aufmerksamkeit in der Öffentlichkeit. Aber auch thermische Systeme müssen modernisiert werden"", sagt Preißinger. Hier gelten andere Voraussetzungen als im elektrischen Bereich – etwa sind thermische Prozesse träge und lassen sich nicht von einem Moment auf den anderen ein- und ausschalten, sagt der Energieforscher. Um nun auch digitale Prozesse für den thermischen Bereich zu verbessern, forscht sein Team interdisziplinär zwischen angewandter Mathematik, Data Science und thermischer Energietechnik. ""Das ist nicht immer einfach. Stellen Sie sich vor, ein Datenspezialist muss einem Kraftwerkbetreiber erklären, zu welchen Forschungsergebnissen er gekommen ist"", schildert Preißinger. Aber genau das mache es auch so spannend. Zwar behandelt die Forschergruppe sehr spezifische, unternehmensbezogene Probleme. Doch hofft sie, in dem seit Februar laufenden Projekt bis zum Ende der Laufzeit auch größere Synergien zu erkennen. Die Idee für das Projekt sei 2018 entstanden, als er mit einem Kollegen aus dem Bereich der angewandten Mathematik überlegte, mit welchen Problemen die Unternehmen der Region derzeit kämpfen und welche Unterstützung die Forschung bieten könnte, erzählt Preißinger. ""Ich wusste, wie man Anlagen zur Abwärmenutzung baut und betreibt, aber verstehe wenig von mathematischen Formeln. Mein Kollege hatte wenig Erfahrung mit thermischen Systemen, konnte aber das Optimierungsproblem mathematisch sauber beschreiben. Da hat sich die Zusammenarbeit perfekt angeboten.""";https://www.derstandard.at/story/2000120530429/abwaerme-und-co-intelligenter-steuern;Standard;Alicia Prager
07.04.2020;Novid 20: Österreichische Corona-App für Georgien, Code als Open Source veröffentlich;"Georgien nutzt als erstes Land die vom österreichischen Verein Novid 20 entwickelte App – Entwickler machen Quellcode öffentlich zugänglich. Seit rund drei Wochen entwickelt ein Team aus Österreich die Corona-Tracking-App Novid 20. Als erstes Land setzt nun Georgien diese App im Kampf gegen die Covid-19-Epidemie ein, teilte der Verein Novid 20 am Dienstag mit. Zudem machen die Entwickler den Quellcode der Applikation öffentlich zugänglich. Funktionsweise. Ziel des von Experten aus den Bereichen Softwareentwicklung, Data Science, Datenschutz und Recht getragenen gemeinnützigen Vereins war es, nach dem Vorbild von Tracking-Apps aus Asien eine adaptierte europäische Lösung zu entwickeln, die hilft, die Ansteckungsrate rasch zu minimieren, und gleichzeitig Datenschutzstandards entspricht. Die App Novid 20 verwendet Bluetooth um festzustellen, welche Smartphones sich in unmittelbarer Nähe zueinander befinden. Die beiden Telefone speichern dann die Identifikationsnummer des jeweils anderen Gerätes, sowie Datum und Uhrzeit des Kontaktes lokal und in verschlüsselter Form jeweils auf dem Handy. Wenn ein Nutzer eine Erkrankung meldet und diese verifiziert ist, wird die Kontaktinformation laut den Entwicklern anonymisiert an die jeweiligen Handy übertragen. Damit können potenziell infizierte Personen gewarnt werden. Den Entwicklern zufolge ist die App nun erstmals in Georgien unter dem Namen Stop Covid im Einsatz und kann dort kostenlos für Android und iOS heruntergeladen werden. Regierungen wird eine kostenlose Lizenz für die App gewährt, es werden nur Anpassungs- und Betriebskosten in Rechnung gestellt. Rotes Kreuz unterstützen. Nachdem es in Österreich bereits mit der ""Stopp Corona""-App des Roten Kreuzes eine derartige Anwendung gibt, mache eine zweite App hierzulande keinen Sinn, sagte ein Vertreter von Novid 20 der APA. Man wolle das Rote Kreuz aber bei der Weiterentwicklung seiner App unterstützen. Angestrebt wird auch eine Kompatibilität mit der europäischen Corona-App-Initiative PEPP-PT, um die internationale Zusammenarbeit zu erleichtern. Die Entwickler stellen den Quellcode der App öffentlich zur Verfügung. Dieser ist ab sofort auf Github abrufbar. ""Die Veröffentlichung als Open Source ist ein essenzielles Mittel, um sicherzustellen, dass die App auch tut was sie verspricht"", so Andreas Petersson vom beteiligten Unternehmen Capacity Blockchain Solutions. Für ihn ist Datensouveränität ein essenzielles Recht von Menschen, auch in Zeiten von Corona seien User nicht gewillt, die Kontrolle über Ihre Daten aufzugeben.";https://www.derstandard.at/story/2000116604627/novid20-oesterreichische-corona-app-fuer-georgien-code-als-open-source;Standard;
26.02.2015;Große Nachfrage nach Data Scientists;"Die Datenmengen wachsen, für die Analyse fehlen Fachexperten. Welche Kompentenzen Data Scientists mitbringen sollen. Big Data wird als große Errungenschaft gefeiert. Jede Spur, die jemand in der virtuellen Welt hinterlässt, wird gespeichert. Damit einhergehend sind auch neue Berufsbilder entstanden, denn aus der Vielzahl an Daten müssen schließlich die richtigen herausgefiltert werden. Dem Data Scientist kommt dafür eine Schlüsselrolle zu. Wer weiß, wie sich strategisch wichtiges Wissen aus der großen Datenmenge filtern lässt, hat im Beruf sehr gute Chancen. Data Scientist gehören zu den gefragtesten Experten am Arbeitsmarkt. Das Statistik-Portal Statista schätzt einen Bedarf von mehr als vier Millionen Fachkräfte für 2015. Gesucht seien keine reinen IT-Spezialisten, erklärt Norbert Seibel, Educational Manager DACH beim Softwarehersteller SAS. Für diese Positionen brauche es einen bunten Mix aus Funktionen und Fähigkeiten. Dazu gehören: Analytisches Talent: Die Aufgabe des Data Scientist ist es, Besonderheiten in großen Datenmengen zu erkennen, die für das Geschäft des Unternehmens wichtig sein können. Voraussetzung dafür ist eine sehr gute statistische Ausbildung. Fach- und Business-Wissen: Ein Data Scientist muss ein gutes Verständnis für die Aufgaben haben, Interesse an den Fachabteilungen ist ebenfalls wichtig, damit er bei Auffälligkeiten in den Daten nützliche Fragestellungen eigenständig formulieren kann. Kommunikationsfähigkeit: Der Data Scientist muss Ergebnisse der Datenanalyse für die Fachabteilungen ebenso verständlich aufbereiten wie für das Top-Management, das auf dieser Basis weitreichende Geschäftsentscheidungen treffen soll. Forschungsdrang und Kreativität: Der Data Scientist begibt sich auf Spurensuche in den Datendschungel. Auf diese Weise eröffnen sich neue Geschäftsmodelle und Wege zur Optimierung von Prozessen. Koordinationstalent: Ein Data Scientist ist immer auch ein Projektmanager. Denn er übernimmt nicht alle Aufgaben selbst: Für die Beschaffung der Daten oder die Gestaltung konkreter Maßnahmen mit den Fachabteilungen beispielsweise kann durchaus ein anderer Mitarbeiter zuständig sein. Für Interessierte hat SAS einen Test bereitgestellt. Der Test definiert insgesamt zehn verschiedene Typen von Data Scientists, darunter den ""Geek"" als Person mit natürlicher Neugier an Technikthemen sowie starken Logik- und Analytik-Fähigkeiten, den ""Guru"", der einerseits eine reaktive Introvertiertheit zeigt, andererseits über Überzeugungskraft und soziale Kompetenz verfügt, oder den selbstbewussten und ergebnisorientierten ""Driver"", der seine Ziele mit großer Bestimmtheit verfolgt und umsetzt.";https://www.derstandard.at/story/2000012040062/grosse-nachfrage-nach-data-scientists;;
11.07.2020;Digital Innovation: Staat will Know-how der KMU verbessern;"Seit vergangenem Jahr entstehen Digital Innovation Hubs, um heimischen KMUs bei der vielbesprochenen Digitalisierung auf die Sprünge zu helfen – denn es gibt Nachholbedarf, wie Wissenschaft, Wirtschaft und Politik wissen. Neben dem Digital Makers Hub sind bereits der Digital Innovation Hub Ost und der Digital Innovation Hub West entstanden. Es handelt sich dabei laut dem zuständigen Digitalisierungsministerium um Kompetenznetzwerke, beteiligt sind Forschungszentren, FHs und Universitäten, aber auch Wirtschaftsagenturen wie die Tiroler Standortagentur und Ecoplus aus Niederösterreich. Der Digital Innovation Hub West zum Beispiel soll kleine und mittlere Unternehmen (KMUs) aus Tirol, Salzburg und Vorarlberg den Zugang zu Know-how von Forschungseinrichtungen erleichtern. Das ist kein triviales Unterfangen. Digitalisierungswissen sei ein entscheidender ""Hebel"", der wirtschaftliche Innovationen und Zukunftsperspektiven ermögliche, hieß es. Themen wie etwa ""E-Commerce und Cloud-Dienstleistungen"" seien gegenwärtig nicht nur wegen der Corona-Krise wichtig. Der Digital Innovation Hub West (DIH-West) wurde bereits als ""praxisnaher Know-how-Lieferant"" bejubelt. Breites Angebot. Die wissenschaftliche Leiterin des DIH-West, Ruth Breu von der Universität Innsbruck, spricht in Bezug auf diese Bündelung von einem ""breiten Angebot für alle Unternehmen"". Zugleich wollte man aber auch mit ganz spezifischen Themen auf ""innovative Unternehmen"" zugehen. 15 Experten seien im DIH-West dazu in der Lage, etwa Gebiete wie Data-Science, IT-Security oder künstliche Intelligenz (KI) abzudecken. Bei den Angeboten agiere man ab sofort ""agil"" und ""lerne noch dazu"", was die Bedürfnisse der KMUs betreffe. In einer zweiten, von der Forschungsgesellschaft FFG durchgeführten Ausschreibung stehen vier Millionen Euro zur Verfügung. Die frischen Mittel stellt die Nationalstiftung mit Unterstützung des Digitalisierungsministeriums zur Verfügung. Einrichtungen mit Forschungsschwerpunkten im Digitalisierungsbereich können sich bis zum 12. Oktober bewerben. Die Förderquote liegt bei bis zu 50 Prozent. Eine weitere Million Euro ist für bestehende Innovation Hubs und eine diesbezügliche Ausschreibung gegen Ende des Jahres 2020 reserviert.";https://www.derstandard.at/story/2000118563471/digital-innovation-staat-will-know-how-der-kmu-verbessern;Standard;
26.11.2020;"Open-Science-Expertin: ""Brauchen Dateninfrastrukturen für Krisenzeiten""";"Es sei höchste Zeit, robuste Schnittstellen für die unabhängige Forschung zu schaffen, sagt die Soziologin Katja Mayer. Jetzt könne man nur Feuer löschen. Nicht erst seit Corona ist der eingeschränkte Zugang zu Daten der Verwaltung und der amtlichen Statistik in Österreich für die Wissenschaft ein Problem. Mitten in der Pandemie zeigt sich umso deutlicher, wie essenziell Daten sind, um politische Entscheidungen aufgrund wissenschaftlicher Evidenz treffen und transparent kommunizieren zu können. Immer wieder fordern Experten eine solide Datenbasis für die Forschung, darunter auch die Plattform Registerforschung, ein Zusammenschluss von Forschungseinrichtungen sowie Wissenschafterinnen und Wissenschaftern. Eine von ihnen ist Katja Mayer, Open-Science-Expertin am Zentrum für Soziale Innovation (ZSI). STANDARD: Seit dem Ausbruch der Pandemie sind Daten so schnell verfügbar wie noch nie, andererseits wird oft mangelnde Transparenz kritisiert, was die Datengrundlage für politische Maßnahmen betrifft. Was hat sich denn durch Corona verändert in Sachen Open Science? Mayer: Durch die Pandemie sind Problemlagen, auf die schon seit vielen Jahren hingewiesen wird, sichtbarer geworden. Sie hat für die Gesellschaft greifbarer gemacht, wie wichtig Open Science ist. Man muss allerdings unterscheiden: Biomedizinische Daten, wie etwa genetische Codes des Sars-CoV-2-Virus, werden relativ schnell geteilt. Hier gibt es schon gute Infrastrukturen und internationale Zusammenarbeit. Auf der anderen Seite ist die Datenlage im Public-Health-Sektor sowie im Sozial- und Wirtschaftsbereich problematisch. Hier gibt es viele beteiligte Akteure auf Länder- und Bundesebene, allein innerhalb österreichischer Grenzen. Daten werden ganz unterschiedlich erzeugt und gehandhabt. STANDARD: Was genau ist das Problem? Mayer: Man hat in den letzten Jahren verabsäumt, sowohl Infrastrukturen als auch Governance-Strategien zu schaffen, die in Krisenzeiten greifen. Man wurde überrumpelt. Jetzt kann man nur versuchen, Feuer zu löschen. Allein wie unterschiedlich die Institutionen Daten in größere Systeme einpflegen, war bisher nicht genau geregelt und war auch in dieser Schnelligkeit bisher nie nötig. Es ist natürlich schwierig, innerhalb eines halben Jahres alles neu aufzustellen, aber es wäre sehr wichtig, jetzt die Weichen zu stellen, um in Zukunft bessere und robuste Schnittstellen zu schaffen. In Krisen sind nicht nur Zugänge zu Gesundheitsdaten und biomedizinischer Forschung wichtig, sondern auch Zugänge zu Wissen über öffentliches Leben, Arbeitsmarkt, Bildung, Umwelt und Wirtschaft. Wenn wir offen und evidenzbasiert agieren wollen, müssen wir wissen, mit welchen Daten wir es zu tun haben und auf welche wir uns verlassen. STANDARD: Die Frage, inwieweit Schulen das Infektionsgeschehen beeinflussen, war ja zuletzt höchst umstritten. Mayer: Die Diskussion um Schulschließungen ist ein Paradefall. Die wissenschaftliche Forschung dazu ist bisher nicht schlüssig. Es gibt zwar erste Studien zur Altersverteilung bei Infektionen in Schulen, die das Testverhalten miteinberechnen, also zeigen, was eigentlich abgebildet wird in den Studien und ob es tatsächlich Effekte gibt. Aber diese Daten sind selten zugänglich. Neben den fehlenden Infrastrukturen fehlen uns hier Mechanismen, um auch sensible Daten für die unabhängige Forschung zugänglich zu machen, und zwar ohne den Datenschutz zu verletzen. Entscheidungen einer Kommission, wer welchen Zugang zu welchen Daten erhält, sollten zudem transparent dokumentiert werden. STANDARD: Wie könnte der Zugang zu sensiblen Daten für die Forschung funktionieren? Mayer: Da gibt es verschiedene Modelle. Es geht nicht darum, alle Daten vollständig für alle offenzulegen oder dass Wissenschafter die Daten aus den Systemen saugen, sondern dass umgekehrt Forschende zu den Behörden und Agenturen hinkommen, wo sie ihre Methoden auf deren Servern laufen lassen können. Im Regierungsprogramm ist der Aufbau eines ""Austrian Micro Data Center"" vereinbart, das vom Wissenschaftsministerium umgesetzt werden soll. Hier geht es genau darum, dass die akkreditierte, unabhängige Wissenschaft Zugriff auf Registerdaten bekommt, also Daten aus öffentlichen Registern der Ministerien und amtlichen Statistiken. Auch von den Daten der Telekomprovider könnte man über eine Pandemie viel lernen. STANDARD: An welchen Daten fehlt es konkret? Mayer: Es geht vor allem um sozioökonomische Daten, zum Beispiel alles rund um die Auswirkungen von Maßnahmen zur Eindämmung des Virus. Das fängt schon damit an, dass wir nicht wissen, welche Branchen welche Beträge aus den Corona-Unterstützungsfonds bekommen haben, bis hin zu Gesundheitsdaten von den Sozialversicherungen, etwa zu Frequenz und Arten von Arztbesuchen während eines Lockdowns. Es ist sehr schwierig, Aussagen und Prognosen über eine Pandemie zu machen und immer wieder zu adaptieren, wenn man keinen Zugriff auf solche essenziellen Daten hat. STANDARD: Gibt es Beispiele, wo das Corona-Datenmanagement besser funktioniert? Mayer: Was noch nirgends funktioniert, sind automatische Schnittstellen, hier lernen alle noch. Aber es gibt eine wesentlich großzügigere Handhabung des Zugangs für die unabhängige Forschung in einigen Ländern, wie etwa in Großbritannien. Auch Dänemark gilt als Vorreiter, wo viele Register und auch Sozialversicherungsdaten für die Forschung freigegeben sind. Natürlich gab es anfangs in diesen Ländern auch Pannen und Datenlecks, aber man hat daraus gelernt, und die Infrastrukturen sind nun wesentlich sicherer. STANDARD: Wäre nicht eine zumindest EU-weite Datenharmonisierung in Bezug auf Corona wünschenswert? Mayer: Ja, unbedingt. Doch das ist sehr schwierg. In der Medizin wird an dem Ziel der Heilung gearbeitet, aber in der sozioökonomischen-politischen Welt gibt es sehr viele verschiedene Ziele. Es wäre dennoch wichtig, gewisse Indikatoren zu Standardisieren, und zwar so rasch wie möglich. Man könnte sich etwa auf einheitliche Daten zu Übersterblichkeit und Testpositivitätsraten einigen und im Sinne einer Informationsoffenlegungspflicht verlangen, dass Altersstrukturen mitveröffentlicht werden. Da könnte von der EU wesentlich mehr Druck gemacht werden. Wir sehen dazu auch erste Bemühungen. STANDARD: Sind Sie zuversichtlich, dass es zu einem Umschwung in Richtung mehr Datenoffenheit und Transparenz geben wird? Mayer: Die Tendenz geht in diese Richtung, das lässt sich nicht mehr aufhalten. Ich glaube, dass viele Staaten verstanden haben, dass es unabhängige, nichtprofitorientierte Dateninstitutionen braucht. Es gibt ein Umdenken in der Politik, dass es sich hier um kritische Infrastrukturen handelt, die hochsensibel sind. Es wird in den nächsten Jahren auch mehr Forschungsgelder für die Verbindung von Computer-, Rechts- und Sozialwissenschaften geben, Stichwort digitaler Humanismus. Dass Daten jeder Art zugänglich gemacht werden müssen, ist jetzt noch klarer geworden. Andererseits muss immer die Gefahr einer totalen Überwachung berücksichtigt werden. Indem man Daten Standardisiert und zusammenführt, schafft man auch ein unglaublich starkes Kontrollinstrumentarium. Gerade deswegen ist es wichtig, dass hier übergeordnete Gremien transparente Entscheidungen treffen. Politik und Gesellschaft müssen lernen, mit dieser Offenheit gewissenhaft umzugehen.";https://www.derstandard.at/story/2000121950172/open-science-expertin-brauchen-dateninfrastrukturen-fuer-krisenzeiten;Standard;Karin Krichmayr
06.07.2020;"""Digital Innovation Hub West"" soll KMU-Digitalisierung vorantreiben";"KMU aus Tirol, Salzburg und Vorarlberg sollen durch Hub von ""Know-how der Forschungseinrichtungen profitieren"". Ein neu eingerichteter Hub – der ""Digital Innovation Hub West"" – soll Klein- und Mittelunternehmen (KMU) aus Tirol, Salzburg und Vorarlberg den Zugang zu Know-how von Forschungseinrichtungen erleichtern. ""KMU haben im Bereich der digitalen Kompetenzen schließlich Aufholbedarf"", betonte Wirtschaftsministerin Margarete Schramböck (ÖVP) bei einer Pressekonferenz am Montag in Innsbruck. Der Hub soll sich schließlich zu einer ""Plattform für Unternehmen mit unterschiedlichem Digitalisierungsgrad entwickeln"", wünschte sich der Rektor der Universität Innsbruck, Tilmann Märk. Es gehe bei diesem vornehmlich darum ""digitales Wissen in die Wirtschaft zu tragen"", ergänzte er. Schramböck wiederum bezeichnete ebenjenes Digitalisierungs-Wissen als entscheidenden ""Hebel"", der wirtschaftliche Innovationen und Zukunftsperspektiven ermögliche. Themen wie etwa ""E-Commerce und Cloud-Dienstleistungen"" seien gegenwärtig nicht nur wegen der Corona-Krise wichtig. Der ""Digital Innovation Hub West"" (DIH-West) sei diesbezüglich ein ""praxisnaher Know-How-Lieferant"". Insgesamt nehme man dazu fünf Mio. Euro in die Hand. ""Praxisnaher Know-How-Lieferant"" Damit wolle man erreichen, gute ""Rahmenbedingungen auch für kleine Unternehmen zu gewährleisten"", ergänzte Tirols Wirtschaftslandesrätin Patrizia Zoller-Frischauf (ÖVP). Dazu bündle man nunmehr unter anderem die Kräfte von Bund, Land und Fachhochschulen. Die wissenschaftliche Leiterin des DIH-West, Ruth Breu, sprach in Bezug auf diese Bündelung von einem ""breiten Angebot für alle Unternehmen"". Zugleich wollte man aber auch mit ganz spezifischen Themen auf ""innovative Unternehmen"" zugehen. 15 Experten seien im DIH-West dazu in der Lage etwa Gebiete wie Data Science, IT-Security oder Künstliche Intelligenz (KI) abzudecken. Bei den Angeboten agiere man ab sofort ""agil"" und ""lerne noch dazu"", was die Bedürfnisse der KMU betreffe.";https://www.derstandard.at/story/2000118535851/digital-innovation-hub-west-soll-kmu-digitalisierung-vorantreiben;Standard;
02.12.2020;Mindshare Österreich holt sich Social-Media-Etat von Gaggenau Global;"Service: Etatwechsel und andere Aufträge in der Kommunikationsbranche im Überblick Hier liefert derStandard.at/Etat Etatwechsel in der Kommunikationsbranche im Überblick. Grob sortiert nach Kalenderwochen, in denen die Infos einlangten. Quellen: Presse- und eigene Infos, andere Branchendienste wie Horizont.at und medianet.at. Wenn Sie Infos für uns haben, bitte ein Mail an etat@derstandard.at schicken. Woche 49 / 2020. Der deutsche Küchengerätehersteller Gaggenau setzt seine erste globale Social-Media-Kampagne um. mit Mindshare Österreich um. Den Zuschlag dafür holte sich Mindshare Österreich. Woche 45 / 2020. Demner, Merlicek & Bergmann sicherte sich den Digitaletat der Automarke VW. Seit 1. Oktober 2020 firmiert das Einrichtungshaus Dänisches Bettenlager in Österreich unter der neuen Marke JYSK. Den Rebranding-Prozess begleitete Grayling Austria als Kommunikationsagentur mit einem Maßnahmen-Paket. Woche 38 / 2020. Hutchison Drei Austria vergibt seinen Kommunikationsetat nach einer mehrstufigen Wettbewerbspräsentation an Wien Nord Serviceplan. Die zu Jahresbeginn aus dem Zusammenschluss von Wien Nord und Serviceplan entstandene Agentur im Haus der Kommunikation Wien wird die strategische und kreative Markenführung, laufende Entwicklung von Kampagnen in allen Kanälen sowie Social Media und Content Marketing verantworten, heißt es. Kampagnenstart ist in wenigen Wochen. Woche 37 / 2020. Die Werbeagentur Obscura gewann den Ritter Sport Etat. Die ersten Kampagne für den Schokoladenhersteller aus der Schmiede der Agentur soll im Herbst starten. Woche 35 / 2020. Die PR-Agentur ipk Wien kommuniziert künftig für Magenta Telekom. Dabei sollen die Aktivitäten im B2B-Bereich weiter ausgebaut und das Portfolio an Kommunikations- und Digitalisierungslösungen im Businessbereich verstärkt kommuniziert werden. Woche 19 / 2020. Die Wiener PR-Agentur Himmelhoch um Geschäftsführerin Eva Mandl betreut ab 1. Juni den PR-Etat des österreichischen Wäscheherstellers Palmers. Woche 17 / 2020. Die MMC Agentur sichert sich den Social Media Etat von A1now. Die Zusammenarbeite umfasst neben Facebook und anderen sozialen Medien auch den Aufbau eines TikTok Channels. Woche 10 / 2020. Die im September aus Alphaaffairs und Kobza Integra hervorgegangene Agentur Alpha_Z betreut nun das österreichische Familienunternehmen philoro Edelmetalle GmbH, laut Agentur mit ""strategischer Positionierung, Content-Development sowie Arbeit mit Medien und Influencern"", in Abstimmung mit Netzwerkpartner Weber Shandwick auch in Deutschland und der Schweiz. Die Agentur Accelent ist nun externer Ansprechpartner für Exputec (Software/Data Science/Biotech) für Medien und Influencer. Woche 9 / 2020. Die Agentur &US von Helmut Kosa und Erich Silhanek arbeitet künftig mit der Donau Versicherung zusammen. Neben der Weiterentwicklung der Marke umfasst der Etat auch die Beratung der digitalen Marketing- und Vertriebsmaßnahmen sowie die Umsetzung der Werbemaßnahmen. Woche 8 / 2020. Die Mediaagentur Mediaplus Austria begleitet seit Anfang des Jahres die Markteinführung der Produkte Bloom Beauty Essence Day Spa & Night Spa in Österreich. Woche 7 / 2020. Jung von Matt/Donau entwickelt neue Kampagne für Knabber Nossi, die Kampagne startet im ersten Quartal. ""Knabber Nossi ist eine tolle österreichische Traditionsmarke. Viele von uns sind mit Knabber Nossi aufgewachsen. Daher ist die Freude riesig, dass wir nun die neue Kampagne entwickeln dürfen"", sagt Jung von Matt-Geschäftsführer Fedja Burmeister zum Etat-Gewinn. Woche 4 / 2020. Die Kommunikationsagentur The Skills Group übernimmt die Wiener Digital- und Innovationsagentur Datenwerk. Die Gruppe beschäftigt nunmehr laut eigenen Angaben über 40 Mitarbeiter und kommt auf einen Honorarumsatz von 4,5 Millionen Euro. Woche 3 / 2020. adverserve übernimmt digitalen Mediaetat der Österreichischen Post AG, die Agentur unterstützt das Unternehmem im Bereich der klassischen und programmatischen Onlinewerbung. Publicis Media Austria organisiert seine digitalen Geschäftsbereiche neu und bündelt diese in einer neuen Unit: Publicis Media Digital. Als Leiter fungiert Christoph Purkart. Unterstützt wird er von den beiden Teamleiterinnen Marlene Weseslindtner (Client Service) und Sophie Feiks (Performance). Woche 48 / 2019. Wien Nord wurde von der Oesterreichischen Kontrollbank mit dem Marken- und Designprozess beauftragt, der das Serviceangebot klarer positionieren und für die Nutzer leichter erfassbar machen soll. Woche 47 / 2019. PKPBBDO wirbt neu mit ""Reich werden mit Klasse"" für die Klassenlotterie der Österreichischen Lotterien mit einem ""neuen Millionär"", schon seit Oktober in TV, Radio, Print, Out of Home und digital. Eine weitere Welle ist für 2020 geplant. Die Kreativagentur Jung von Matt/Donau ist mit der Neupositionierung und Markenschärfung des Wiener Traditionsunternehmens Augarten Porzellan betraut. Woche 45 / 2019. Die beiden Agenturen Unique & Fessler sicherten sich den Etat von Betonmarketing Österreich. Die neue Kampagne ist für das Jahr 2020 avisiert. Woche 42 / 2019. Die Kreativagentur Zum goldenen Hirschen freut sich über den Etatgewinn des Stifts Klosterneuburg. Unter der kommunikativen Klammer ""EinOrt. Tausend Geschichten."" werden die Sehenswürdigkeiten des Stifts mit Anekdoten aufgeladen. Woche 41 / 2019. ""Denk wozu vorsorgen"" ist das Motto der Herbstkampagne von Springer & Jacoby für die Uniqa Versicherung in Instagram-Story-Optik. Die crossmediale Kampagne umfasst TV-Spots, Point of Sale, Social Media und Onlinemedien. Valerie und Konstantin illustrieren in zwei neuen Werbesujets für die RMA (Regionalmedien Austria) die ""Nähe"" der Gratiswochenzeitungen zu ihren Leserinnen und Lesern. Donnerwetterblitz hat die Sujets entwickelt, Stefan Gergely fotografierte. Die Petz Hornmanufaktur (Wien/Graz) hat ikp Wien mit strategischer Kommunikationsberatung und Influencer Relations beauftragt. Woche 36 / 2019. Havas Media Austria gewinnt klassischen Media-Etat des WienTourismus, der Wiener Tourismusverband vertraut nach einem mehrstufigen Ausschreibungsverfahren auf das Media-Know-how von Havas Media Austria im klassischen Bereich, Wien soll als Tourismusstandort weltweit in den relevanten Zielmärkten weiter positioniert werden.";https://www.derstandard.at/story/2000108272312/mindshare-oesterreich-holt-sich-social-media-etat-vongaggenau-global;Standard;
30.09.2020;Was wollen wir künstlicher Intelligenz erlauben?;"Im Umgang mit künstlicher Intelligenz fehlen weitgehend die Regeln. Experten fordern die Auseinandersetzung mit ethischen Prinzipien. Sehr viele Jobs werden wegfallen, sagte der Computerwissenschafter Sepp Hochreiter von der Johannes-Kepler-Universität (JKU) Linz schon vor einigen Jahren dem Standard, vor allem mechanische Arbeiten, die aus einfachen, klar begrenzten Handgriffen bestehen: am Fließband, im Callcenter, am Lenkrad eines Pkws oder Lkws. Es werde aufgrund der raschen Entwicklung des autonomen Fahrens in relativ naher Zukunft auch keine Taxifahrer mehr geben, dafür aber KI-Experten, die das Netz der fahrenden Maschinen überwachen. Mechaniker müssten dann wohl auch in der Wartung dieser Fahrzeuge geschult werden, wären also höher qualifiziert als jetzt. Statistische Auswertungen könnten KI-Systeme übernehmen, sagte Hochreiter außerdem. Erste Diagnosen von Brust- oder Hautkrebs sind von KI aufgrund des raschen Zugriffs auf Daten und Vergleichsmöglichkeiten schneller und präziser als von Ärzten möglich. Ihre Interpretation müssten dann selbstverständlich wieder Menschen überlassen werden. Im Journalismus könnten auch Bots auswerten, das wie vielte Tor ein Fußballer nun insgesamt geschossen habe. Sportergebnisse und Börsenkurse werden schon jetzt in einigen Redaktionen über Algorithmen eingespielt. Kürzlich hat die britische Zeitung Guardian erstmals einen von einer KI geschriebenen Text veröffentlicht. Robotergesetze. Die Menschheit befindet sich inmitten eines gesellschaftlichen Umbruchs. Dafür braucht es Regeln, darüber sind sich Experten einig. Der Informatiker Justus Piater von der Uni Innsbruck ist prinzipiell für ethische Prinzipien im Umgang mit künstlicher Intelligenz – sie würden sich allerdings kaum über datenbasierte Regelwerke so formalisieren lassen, dass sich das KI-System danach richten könne, meint der Wissenschafter. Piater arbeitet mit Haushaltsrobotern und will deren Interaktion mit Menschen verbessern. Der legendäre Science-Fiction-Autor Isaac Asimov habe das ethische Problem in der Interaktion Mensch-Maschine bereits mit den Robotergesetzen in seinen Romanen beschrieben. Piater schränkt aber ein: ""Die Bücher handeln davon, wie die Roboter diesen Gesetzen folgen und gerade dadurch den Interessen der Menschen zuwiderhandeln."" Die Konsequenz könne nur heißen: ""Das menschliche Schicksal darf von keinem KI-System abhängig sein."" Die Einschränkungen seien auf unterschiedlichsten Ebenen wichtig. Oberstes Gebot sei der Schutz von persönlichen Daten. Piater: ""Das beginnt beim Viertel, in dem man wohnt, und endet bei der Hautfarbe. Die Verknüpfung solcher Daten verfestigt die in den vorhandenen Daten abgebildeten Tendenzen und Vorurteile."" Trolley-Problem ist überbewertet. Keine Nachteile für den Menschen erwartet Piater vom autonomen Fahren, sobald die verbleibenden technischen Probleme gelöst sind. Autonome Fahrzeuge würden eines Tages sicherer fahren als handgesteuerte. Das vielzitierte Trolley-Problem sei in diesem Zusammenhang überbewertet: Das autonome Fahrzeug muss einen Unfall verhindern und kann dabei zwischen einem Crash mit einer Pensionistin und einem Kind wählen. Wie sollte es wählen? Piater sagt, diese Frage sei zweitrangig, weil das Ziel sein müsse, dass man gar nicht erst in eine solche Notfallsituation gerät. ""Die Maschine ist in dem Fall potenziell besser als der Mensch, der in solchen Situationen in Panik und nicht nach genauen Überlegungen handelt."" Ehe das nicht sichergestellt sei, werden sich autonome Fahrzeuge nicht durchsetzen, ist der Wissenschafter überzeugt. Ein viel größeres Problem sieht Piater im schon jahrelang andauernden Trend, politische Meinungen mithilfe von Social Media zu verbreiten. ""Da werden deren datengetriebene Empfehlungssysteme genutzt, um Wahlen zu beeinflussen und Verschwörungstheorien und Fake-News zu verbreiten. Das ist im höchsten Maß demokratiegefährdend"", sagt der Wissenschafter. Dagegen gebe es noch kein wirklich probates Mittel. Möglicherweise hat das Forschungsteam um Ross King vom Austrian Institute of Technology (AIT) bald eine Lösung für das Problem. King ist Head der Competence Unit Data Science & Artificial Intelligence und berichtet von einem Projekt, das sein Team in Zusammenarbeit mit anderen Forschungsgruppen im AIT Center for Digital Safety & Security am 1. 10. beginnt. Dem liegt eine Grundidee zugrunde: Ein Mensch kategorisiert eine Reihe von Nachrichten als ""richtig"" oder ""falsch"". Das System, das auf maschinellem Lernen basiert, wird anhand dieser Kategorisierungen trainiert. Anschließend wird es auf andere Testdaten angewendet, die ebenfalls von einem Menschen kategorisiert wurden, und erreicht eine Übereinstimmung von immerhin 80 Prozent. Ross King gibt dabei zu bedenken, dass wir Menschen uns oft untereinander nicht einig sind, was Fake-News sind und was nicht. Das Projekt wird über das Forschungsprogramm KIRAS der Österreichischen Forschungsförderungsgesellschaft FFG und des Klimaschutzministeriums finanziert und läuft zwei Jahre. Danach soll aber noch nicht Schluss sein: King hofft neben der Unterstützung durch das AIT selbst auch auf Förderungen durch das nächste EU-Forschungsrahmenprogramm ""Horizon Europe"". Ko-Evolution von Mensch und KI. Es scheint so, als würde die Menschheit wieder einmal von den Technologien und den damit verbundenen Möglichkeiten überrascht werden. KI kann sich schneller entwickeln als der allen gesellschaftlichen Normen konforme Umgang mit ihr. Ross King zeigt sich nicht überrascht von der Schnelligkeit der Entwicklungen. Die Technologie sei immer vor allen Regelwerken der Gesellschaft nutzbar. Wie weit ist die Evolution fortgeschritten? Kann sich eine KI zu irgendeinem Zeitpunkt verselbstständigen? Im Klassiker 2001 – Odyssee im Weltraum hat ein Bordcomputer namens Hal, der abgeschaltet werden sollte, das Kommando über das Raumschiff übernommen und sich letztlich brutal an den Menschen gerächt. Von einer Realisierung derart dystopischer Vorstellungen seien wir allerdings weit entfernt, sagt King, wiewohl manche Entwicklungen in diesem Bereich schon erstaunlich seien: Der AIT-Forscher meint damit etwa das Computerprogramm Alpha Go vom Entwickler Deep Mind, das autodidaktisch funktioniert hat und sich selbst das Brettspiel Go beigebracht hat. Aber man muss gar nicht so weit gehen wie Arthur C. Clarke in seinem berühmten Odyssee im Weltraum-Roman, um sich Sorgen machen zu können. Stefan Woltran von der TU Wien schlägt vor, die Kontrolle von Entwicklungen in der künstlichen Intelligenz nicht dem Zufall zu überlassen. Er bestätigt: Derzeit gebe es europaweit kaum eine legistische Handhabe – und die Beeinflussung der Gesellschaft durch KI sei weitreichend, von der Nutzung in der Medizin bis zu Änderungen am Arbeitsplatz. Deswegen müsse man die KI so kontrollieren, wie man die Entwicklung von Medikamenten und Impfstoffen kontrolliert. Die Systeme müssten eingehend geprüft werden, ehe eine Kooperation mit Menschen möglich ist. Der fehlende Diskurs. Es fehle ein öffentlicher Diskurs über Chancen und Risiken im Umgang mit KI. Die Gesellschaft wisse nichts mit dem Thema anzufangen. Woltran kann dem Gedanken zwar etwas abgewinnen, dass KI vermutlich klarer und verständlicher wird, wenn sie in menschähnlichen Robotern implementiert wird. Dann sollte man sich aber wieder die Frage stellen, wie das Mantra, die KI habe dem Menschen zu dienen, negativ auf zwischenmenschliche Beziehungen Einfluss nehmen könnte. ""Wir sollten einander unbeeinflusst davon respektvoll begegnen."" Der Mensch und das KI-System, ein weites Feld: Woltran verweist auch auf das Wiener Manifest für digitalen Humanismus von vergangenem Jahr, das der TU-Informatiker Hannes Werthner federführend betrieben hat, ein Aufruf zum Nachdenken über digitale Technologien, also auch KI-Systeme. Darin heißt es: ""Digitale Technologien verändern die Gesellschaft fundamental und stellen unser Verständnis infrage, was unsere Existenz als Menschen ausmacht. Viel steht auf dem Spiel. Die Herausforderung einer gerechten und demokratischen Gesellschaft mit dem Menschen im Zentrum des technologischen Fortschritts muss mit Entschlossenheit und wissenschaftlichem Einfallsreichtum bewältigt werden."" Dafür braucht es Kontrollmechanismen. Die Frage ist nur, ob sie derzeit wirklich umfassend bestehen. Weltweit sind zahlreiche Strategien in den vergangenen Jahren entstanden, um zumindest die Infrastruktur für eine Gesellschaft aufzubauen, die mit KI leben soll. In Österreich wartet die KI-Strategie noch auf ihre Umsetzung: Der Regierungswechsel von ÖVP-FPÖ auf ÖVP-Grüne sei ein entscheidender Faktor für die Verzögerung der Umsetzung gewesen, heißt es. Selbstverständlich war auch die Corona-Krise mit schuld daran. Die Strategie wird unter anderem auf Basis einer Studie umgesetzt, die im Juni 2019 im Auftrag des heutigen Klimaschutzministeriums erstellt wurde. Die Autoren waren Erich Prem (Strategie- und Unternehmensberatung Eutema) und Sascha Ruhland (KMU Forschung Austria). Sie sehen ein hohes Potenzial für KI in Österreich. Auch hierzulande werden vor allem Fachkräfte gesucht. Laut dem Positionspapier ""Forderungen der Austrian Society for Artificial Intelligence (ASAI) zur zeitnahen Etablierung einer konkreten österreichischen AI-Strategie"", an dem Woltran mitgearbeitet hat, hinkt man hinter Ländern wie Deutschland hinterher. Dort wurden von 150 Maßnahmen der KI-Strategie des Bundes 2019 bereits 100 gestartet. Dabei wurden bis zu 30 KI-Humboldt-Professuren, dotiert jeweils mit bis zu fünf Millionen Euro, vergeben. Außerdem wurde die Förderung für fünf universitäre KI-Zentren und eine Forschungsinstitution verdoppelt und zusätzlich 15 weitere bundesweite Fördermaßnahmen begonnen. In den Niederlanden investieren Amsterdams Universitäten 300 Millionen Euro in künstliche Intelligenz in den nächsten zehn Jahren, um 150 neue Professuren und PhD-Stellen zu schaffen: Forschungsangebote zu KI. In Österreich wird zum Thema bereits umfassend geforscht, nicht nur am Austrian Research Institute for Artificial Intelligence (OFAI) und am Austrian Institute of Technology (AIT), auch am Joanneum Research. Universitäten wie die Johannes-Kepler-Uni, die TU Wien, die TU Graz, aber auch die Uni Salzburg und die Uni Innsbruck bieten Schwerpunkte an. Im Bereich der FHs sind unter anderem Salzburg, St. Pölten und die Fachhochschule Joanneum sehr aktiv. Die österreichische Bundesregierung will auch den Plan umsetzen, eine eigene TU Linz zu gründen, die sich um Lehre und Forschung im Bereich Digitalisierung und daher wohl auch im Bereich der künstlichen Intelligenz bemühen soll. Wissenschafter und Innovationsökonomen kritisierten dieses Vorhaben. Erich Prem, einer der Autoren der besagten Studie, hat im Zuge dieser Arbeit eine Vielzahl an Empfehlungen in Papierform gelesen. Viele von ihnen, auch jene der OSZE und des österreichischen Roboterrats, hätten einen bemerkenswerten Passus. KI müsse sich an geltende Gesetze halten. Prem dazu: ""Irgendetwas muss verkehrt sein in dieser Welt, wenn man das extra betonen muss.""";https://www.derstandard.at/story/2000120270228/was-wollen-wir-kuenstlicher-intelligenz-erlauben;Standard;Peter Illetschko
06.06.2020;Wogegen ein Kraut gewachsen ist;"An der Universität Wien widmet man sich anhand historischer Unterlagen den Pflanzen in der Volksmedizin. Schwere Infektionen der Atemwege gibt es schon seit Menschengedenken – sie werden von unterschiedlichen Viren und Bakterien ausgelöst, und diese wurden schon bekämpft, lange bevor es Impfungen und Antibiotika gab. An der Universität Wien kombiniert man nun traditionelles Wissen aus alten Schriften mit hochmodernen Methoden, um mögliche neue Wirkstoffe gegen diese Plagen zu finden. Judith Rollinger vom Department für Pharmakognosie der Universität Wien arbeitet seit rund 15 Jahren auf dem Gebiet der Ethnopharmakologie, eines Forschungszweigs, der sich mit der Verwendung von Pflanzen in der Volksmedizin beschäftigt. Seit zehn Jahren etwa liegt ihr Schwerpunkt dabei auf Erregern respiratorischer Erkrankungen, zum Beispiel Influenza-Viren und den Schnupfen verursachenden Rhinoviren. Mit finanzieller Unterstützung durch den Wissenschaftsfonds FWF durchforsteten Rollinger und ihre Arbeitsgruppe, darunter Ulrike Grienke und die Virologin Michaela Schmidtke aus Jena, bis zu 2000 Jahre alte Texte, wie etwa De materia medica des griechischen Militärarztes Pedanios Dioskurides. Etabliertes Forschungsfeld. ""Dioskurides ist mit dem Heer im Römischen Reich weit herumgekommen und hat dabei alles aufgezeichnet, was in den verschiedenen Regionen an Heilmitteln verwendet wurde"", erzählt Rollinger. Zusätzlich arbeitete sich die Forschungsgruppe auch durch Werke der Volksheilkunde und der Traditionellen Chinesischen Medizin. Das klingt exotischer, als es wirklich ist: ""Die Ethnopharmakologie ist ein gut etabliertes Forschungsfeld"", versichert Rollinger. ""Das Besondere ist allerdings, dass wir dieses traditionelle Wissen mit modernen chemometrischen und chemoinformatischen Methoden verknüpfen. Rund 300.000 Naturstoffe sind bisher identifiziert worden, und wir können diese Molekülstrukturen virtuell nach potenziellen Bindetaschen absuchen. Die Verknüpfung des empirischen Wissens aus der traditionellen Medizin mit Big-Data-Science ist bestens geeignet für das Aufspüren von Wirkstoffen"", sagt Rollinger mit einer spürbaren Begeisterung. Bindetaschen des Proteins. Um dieses Gefühl nachvollziehen zu können, muss man zuerst einmal wissen, was Bindetaschen sind: Das sind spezielle Stellen an einem Zielprotein, wie zum Beispiel einem Enzym oder einem Rezeptor, an denen eine Interaktion mit einem anderen Stoff, etwa einem Arzneimittel, möglich ist. Es gibt verschiedenste Bindetaschen, und in jede passen nur bestimmte Molekülstrukturen, ähnlich wie sich ein Schloss nur mit dem passenden Schlüssel öffnen lässt. Diese passenden Moleküle nennt man Liganden, ihr Andocken in der Bindetasche kann verschiedene Effekte hervorrufen: So können die entsprechenden Zielproteine in ihrer Aktivität blockiert oder stimuliert werden. Kennt man die dreidimensionale Struktur der Bindetasche und im besten Fall auch noch Liganden davon, kann man mit diesem Wissen Modelle generieren, die bei virtuellen Screeningverfahren aus zigtausenden Strukturen neue potenzielle Liganden vorhersagen können. Dank der mittlerweile erreichten Leistungsfähigkeit von Computern kann man so in kurzer Zeit vielversprechende Arzneistoff-Kandidaten identifizieren. Wirkung gegen Grippe. Das Interesse der Forscher galt dabei vor allem der Bindetasche der Influenza Neuraminidase, kurz NA. Es handelt sich dabei um eine Familie von Enzymen, die unter anderem in einigen Viren und Bakterien vorkommen, für deren Vermehrung sie notwendig sind. Nach ihren Studien der alten Quellen und dem entsprechenden Screening hatten Rollinger und ihre Gruppe knapp 30 Pflanzen- und Pilzextrakte identifiziert, die die Influenza NA blockierten und somit eine Wirkung gegen Grippe versprachen. In Zellkulturen verglichen die Virologen aus Jena danach die Leistung dieser Substanzen mit derjenigen von Oseltamvir, einem synthetisch erzeugten Arzneistoff, der im bekannten Grippemedikament Tamiflu enthalten ist. Wie sich herausstellte, waren die pflanzlichen Mittel oft nicht so effektiv wie die synthetischen, hatten aber einen großen Vorteil: Gegen Oseltamvir bilden sich sehr rasch Resistenzen, gegen die Pflanzenstoffe aber nicht, womit sich Rollingers Hoffnungen bestätigten. Der Maulbeerbaum. Eine der Substanzen, die sich dabei als besonders vielversprechend erwies, stammt aus der Wurzelrinde des Maulbeerbaumes. Sie wirkt nicht nur an der NA-Bindestelle von Influenza-Viren, sondern auch an der von Pneumokokken, jenen Bakterien, die maßgeblich für eine Lungenentzündung verantwortlich sind. Laut Rollinger starben bei der Spanischen Grippe 95 Prozent der bereits durch die Grippe geschwächten Patienten an einer durch Pneumokokken verursachten Lungeninfektion. In einem aktuell geplanten Projekt wollen Rollinger und ihre Mitarbeiter 160 altbewährte Heilpflanzen auf ihre Wirksamkeit gegen das Coronavirus überprüfen. Ein anwendbares Medikament steht dabei freilich noch lange nicht zur Diskussion, weil der Zulassungsprozess jenseits der finanziellen Möglichkeiten eines Uni-Institutes liegt, aber es werden die Grundsteine dafür gelegt und Wissen generiert, das der Öffentlichkeit zugutekommen soll. ""Wir werden zu einem großen Teil von der öffentlichen Hand finanziert. Da ist es nur recht und billig, dass wir unsere Ergebnisse publizieren und sie anderen Forscherinnen und Forschern frei zugänglich machen"", sagt die Wissenschafterin. Phytopharmaka. Kaum jemand weiß übrigens, dass immerhin rund 60 Prozent der heute zur Verfügung stehenden Medikamente direkt aus Naturstoffen stammen oder zumindest davon abgeleitet sind, der Rest sind Synthetika. Wenn man in der Apotheke etwas ""rein Pflanzliches"" verlangt, werden Phytopharmaka angeboten. Dabei handelt es sich immer um ein Gemisch aus mehreren Stoffen, wie Rollinger aufklärt, aber es sind echte Arzneimittel, das heißt, sie durchlaufen ein entsprechendes Zulassungsprozedere und unterliegen wie jedes andere Medikament den strengen Qualitätskontrollen der Arzneimittelbehörde. Das beinhaltet auch, dass sie auf pharmazeutische Qualität, Wirksamkeit und Unbedenklichkeit geprüft sind. Um hier gleich ein häufig auftretendes Missverständnis aufzuklären: Nahrungsergänzungsmittel, Bachblüten und homöopathische Mittel sind keine Phytopharmaka.";https://www.derstandard.at/story/2000117847146/wogegen-ein-kraut-gewachsen-ist;Standard;Susanne Strnadl
17.01.2020;AI for Dummies: Online-Crashkurs erklärt künstliche Intelligenz;"Finnland wollte seine Bürger über künstliche Intelligenz fortbilden und startete einen Onlinekurs. Seit kurzem ist er auf Deutsch verfügbar. Künstliche Intelligenz ist überall, aber kaum jemand versteht sie wirklich. Kein Wunder, denn letztlich sind selbstlernende Algorithmen eine Ansammlung komplizierter mathematischer Modelle. Trotzdem: Die finnische Regierung beschloss 2018, dass die Finnen mehr über die Zukunftstechnologie lernen sollten, und stellte den kostenlosen Onlinekurs ""Elements of AI"" ins Internet. Das Ziel: Zumindest ein Prozent der Finnen – 55.000 Menschen – sollte mit den Grundzügen von KI vertraut werden. Das Einprozentziel ist erreicht, insgesamt haben sich inzwischen mehr als 330.000 für den Kurs angemeldet, 40 Prozent davon seien Frauen – ein doppelt so hoher Anteil wie in Informatik-Studiengängen. Seit Dezember ist der Kurs auch in vielen weiteren Sprachen, darunter Deutsch, verfügbar. Keine Vorkenntnisse erforderlich. Viele andere Kurse zum Thema KI zielen darauf ab, Menschen das Programmieren beizubringen. ""Elements of AI"" setzt hingegen auf sehr einfache Sprache und lebensnahe Beispiele. Der Onlinekurs ist außerdem aufwendig illustriert, Programmier- oder höhere Mathematikkenntnisse sind nicht erforderlich. Neben technischen Grundlagen vermittelt der Kurs auch philosophische Implikationen, warnt vor Gefahren und klärt Begrifflichkeiten, etwa die Abgrenzungen zwischen maschinellem Lernen, Robotik und Data-Science. Zweiter Teil erscheint in Kürze. Am Ende jeder Lektion wird das gelernte Wissen anhand von Fragen abgeprüft. Insgesamt dauert der Kurs etwa sechs Wochen, wobei es sich hier nur um eine Empfehlung handelt und auch eigenes Tempo möglich ist. Wer bis zum Ende durchhält, bekommt sogar ein Zertifikat, finnische Studenten können sich den Kurs in ECTS-Punkten anrechnen lassen. Zur ersten Verleihung der Zertifikate 2018 kam der finnische Präsident sogar persönlich. Der zweite Teil des Kurses, ""Building AI"", soll noch Anfang 2020 erscheinen. In diesem wird es darum gehen, eigene Anwendungen zu programmieren.";https://www.derstandard.at/story/2000113377444/ai-for-dummies-online-crashkurs-erklaert-kuenstliche-intelligenz;Standard;
23.10.2018;AMS-Algorithmus: Wo die Statistik aufhört, beginnt die Politik;"Das Arbeitsmarktservice wird einen Algorithmus zur Chancen-Bewertung am Arbeitsmarkt verwenden. Eine Einordnung in sieben Schritten. 1. Das AMS bedient sich keiner Zauberei, sondern einer Standardmethode der Sozialwissenschaft. Künstliche Intelligenz, Data-Mining, maschinelles Lernen – mit der schönen neuen Welt der Computerwissenschaft hat der AMS-Algorithmus nichts zu tun. Die Methode des Arbeitsmarktservice (eine Serie von logistischen Regressionsmodellen) ist seit Jahrzehnten Standard in den Sozialwissenschaften und wird etwa an der Universität Wien regulär im Masterstudium Politikwissenschaft gelehrt. Im Prinzip handelt es sich dabei um eine mathematische Gleichung, bei der auf der linken Seite ein Wert für den Wiedereinstieg in den Arbeitsmarkt steht (1 für Erfolg, 0 für Misserfolg) und auf der rechten eine Reihe von persönlichen Merkmalen (Alter, Geschlecht, Ausbildung, berufliche Laufbahn, Staatsbürgerschaft und so weiter). Anhand von vorhandenen AMS-Daten kann man für jedes dieser Merkmale einen Koeffizienten berechnen, der angibt, wie sich dieses Merkmal auf die Chance auswirkt, innerhalb einer bestimmten Zeit wieder Arbeit zu finden. So kommt man zu der hier abgebildeten Gleichung, die eine stark simplifizierte Variante des verwendeten Modells (oder eher: der verwendeten Modelle) darstellt. Es wird sowohl die Chance auf kurzfristige (binnen sieben Monaten) als auch auf langfristige Integration (binnen zwei Jahren) in den Arbeitsmarkt berechnet. 2. Neue Fälle werden auf Basis älterer Fälle eingestuft. Die anhand existierender AMS-Klienten aufgestellten Gleichungen errechnen dann für jede neu beim AMS gemeldete Person einen Prozentwert (oder mehrere, zum Beispiel kurzfristige und langfristige Prognosen), der die Einstellungschancen beziffert. Es werden also immer Daten aus der Vergangenheit herangezogen, um für gegenwärtige AMS-Kunden Vorhersagen zu erstellen. Das ist auch gar nicht anders möglich, weil man zur Schätzung der Regressionsgleichungen die Information zu erfolgter oder nicht erfolgter Wiedereingliederung in den Arbeitsmarkt braucht. 3. Die Einstufung von AMS-Kunden in drei Gruppen ist problematisch. Alle beim AMS gemeldeten Fälle werden durch den Algorithmus in drei Kategorien einstuft: Hohe Chancen (H) hat, wem eine Wahrscheinlichkeit größer 66 Prozent vorhergesagt wird, binnen sieben Monaten 90 Beschäftigungstage vorzuweisen. Niedrige Chancen (N) hat man, wenn die vorhergesagte Wahrscheinlichkeit, binnen zweier Jahre 180 Beschäftigungstage zu erreichen, weniger als 25 Prozent beträgt. Alle anderen kommen in eine Mittelkategorie. Wichtig ist, dass diese drei Gruppen bei weitem nicht gleich groß sind. Laut diesem Dokument (Seite 15) wurden bei der Berechnung für 2018 nur vier Prozent der Fälle in die N-Gruppe eingestuft, jedoch 32 Prozent in die H-Gruppe. Die Mittelgruppe macht folgerichtig also mit 64 Prozent den Löwenanteil aller Fälle aus. Problematisch an dieser Kategorisierung ist, dass an den Grenzwerten zwischen den Gruppen harte Schnitte erfolgen (wobei AMS-Berater jede Person gemäß persönlicher Einschätzung umstufen können), die durch die Datenlage wohl schwer zu rechtfertigen sind. Eine Person mit 25 Prozent Wiedereinstiegswahrscheinlichkeit braucht wohl ähnliche AMS-Angebote wie eine mit 24 Prozent, kommt aber durch die Kategorisierung in denselben Topf wie jemand mit einem Wert von 60 Prozent. 4. Jede Prognose ist unsicher – auch bei guten Modellen. Ein weiteres Problem ist, dass jede der vorhergesagten Wahrscheinlichkeiten mit einer gewissen Unsicherheit behaftet ist. Vielleicht ist also die tatsächliche Wiedereinstiegschance einer Person mit Wert 24 Prozent nicht 24 Prozent, sondern 20, 28 oder 36 Prozent. Das verschärft das oben beschriebene Problem der Kategorisierung, weil wir nicht sicher sein können, dass die Person mit Prognose 24 Prozent tatsächlich eine (geringfügig) schlechtere Aussicht hat als jene mit 25 Prozent. Die Vorhersagekraft des Modells insgesamt liegt für die N-Gruppe (geringe Chancen auf Wiedereinstieg) bei 85 Prozent, für die H-Gruppe (hohe Chancen) bei 80 Prozent (siehe wiederum hier, Seite 15). Das sind für sozialwissenschaftliche Modelle vergleichsweise hohe Werte. Dennoch gilt: Von fünf Personen mit hoher Einstufung findet eine wider Erwarten keinen Job im anvisierten Zeitraum, während von sieben Personen mit schlechter Prognose eine wider Erwarten doch in Beschäftigung kommt (für die Mittelgruppe gibt es hier keine Zahlen). Besonders wichtig ist also, dass aus der Modellprognose keine sich selbst erfüllende Prophezeiung wird – das würde die Schwächsten besonders hart treffen. 5. Algorithmen verleiten zu strategischem Verhalten der Betroffenen. Vor allem wenn ihre Funktionsweise öffentlich bekannt ist, bieten Algorithmen Anreize für Betroffene, sich strategisch zu verhalten. Es ist nur logisch, dass AMS-Kunden danach trachten werden, eine möglichst günstige Einstufung zu erhalten (etwa wenn teurere Kurse verstärkt der Mittelgruppe angeboten werden). Viele Merkmale, mit denen der Algorithmus gefüttert wird, sind zwar nicht veränderbar (Geschlecht, Alter, Berufslaufbahn), manche aber schon. So hat etwa die AMS-Regionalstelle, bei der man vorstellig wird, einen großen Einfluss auf die Chancenberechnung (siehe Punkt 5 hier). Nicht unvorstellbar also, dass manche – so die Möglichkeit besteht – ihren Hauptwohnsitz gezielt verlegen (etwa zu Verwandten oder Bekannten), um bessere Einstufungen zu bekommen. 6. Menschen haben genauso ihre ""Algorithmen"". Ein oft gehörtes Argument gegen das Verwenden von Algorithmen ist, dass diese bestehende Ungleichheiten verfestigen würden. Der AMS-Algorithmus weist etwa Frauen schlechtere Chancen zu – ganz einfach, weil Frauen etwas geringere Wiedereinstiegswahrscheinlichkeiten haben und die statistischen Modelle versuchen, diese Tatsache ""korrekt"" abzubilden. Frauen werden etwa im 2018er-Modell nur zu 21 Prozent in die Gruppe mit hohen Vermittlungschancen kategorisiert, Männer hingegen zu 39 Prozent (siehe wiederum hier, Seite 15). Diesem Vorwurf kann man allerdings entgegenhalten, dass Menschen ähnliche – vielleicht sogar schlimmere – Tendenzen haben. Jeder AMS-Berater wird die Arbeitsmarktchancen einer 40-jährigen alleinerziehenden Pflichtschulabgängerin mit Migrationshintergrund a priori niedriger einschätzen als die eines ""hiesigen"" 27-jährigen alleinstehenden HTL-Absolventen. Dabei ist die Grenze zwischen nützlichem Erfahrungswissen und hinderlichen Stereotypen fließend. Der Algorithmus birgt genauso die Gefahr, bestehende Benachteiligungen zu verfestigen, er muss diese Dinge aber explizit machen und ist daher transparenter (soweit er nicht geheim gehalten wird) und somit leichter kritisier- und änderbar. 7. Es geht weniger um den Algorithmus als darum, was man damit tut. Grundsätzlich ist es ein lobenswerter Schritt, dass das AMS versucht, die Jobchancen seiner Kunden möglichst objektiv zu bewerten. Damit schafft man eine solide Grundlage für die aktive Arbeitsmarktpolitik. Die größten Bedenken gegenüber einem solchen Algorithmus beginnen in der Regel dort, wo es heißt: Was jetzt tun damit? Kein Algorithmus kann AMS-Beratern, der AMS-Führung oder der Regierung die Entscheidungen über die Ressourcenzuweisung an einzelne Personen, bestimmte Zielgruppen oder das AMS insgesamt abnehmen. Mit anderen Worten: Dort, wo die Statistik aufhört, beginnt die Politik.";https://www.derstandard.at/story/2000089853994/ams-algorithmus-wo-die-statistik-aufhoert-beginnt-die-politik;Standard;Laurenz Ennser-Jedenastik
09.01.2019;"Statistiker Gelman: ""Wählen ist wie ein Lotterielos kaufen""";"Warum es sinnvoll ist, wählen zu gehen – mit Fragen wie dieser beschäftigt sich der US-Amerikaner Andrew Gelman. Von Demokratie in Nordamerika über Neuropsychologie bis hin zur Wissenschaftsforschung erstreckt sich das weite Forschungsfeld von Andrew Gelman von der Columbia University in New York City. Dem Direktor und Gründer eines Forschungszentrums für angewandte Statistik ist es ein Anliegen, die Verwendung statistischer Methoden in der Forschung zu verbessern und ihre Bedeutung auch Laien zu vermitteln. Dabei untersucht der Politikwissenschafter und Statistiker auch die Rationalität und Mathematik politischer Wahlen. STANDARD: Viele Menschen haben den Eindruck, dass es keinen Unterschied macht, ob sie wählen gehen oder nicht. Was sagt die Statistik dazu? Gelman: Statistisch gesehen ist es sehr unwahrscheinlich, dass meine einzelne Stimme bei einer Wahl einen Unterschied macht. 2018 fanden in den USA Kongresswahlen statt. Bei einer solchen liegt die Chance, dass 434 Wahlbezirke genau zwischen Demokraten und Republikanern aufgeteilt sind und meine Stimme im 435. Bezirk entscheidend ist, etwa bei 1 zu 800.000. STANDARD: Sie sagen, es sei dennoch sinnvoll, wählen zu gehen. Gibt es eine statistische Größe, mit der sich das beziffern lässt? Gelman: Dafür kann man die Wahrscheinlichkeit, dass meine Stimme einen Unterschied macht, mit meinem Nutzen daraus multiplizieren. Dieser Nutzen kann ein persönlicher Gewinn sein, der dadurch entsteht, dass eine bestimmte Partei den Kongress dominiert. Zum Beispiel, wenn meine Steuern um tausend Dollar gesenkt werden. Oder die Jobsituation verbessert wird, sodass ich eine Arbeit finde und so 50.000 Dollar bekomme. Wenn wir diesen Wert in die Gleichung einsetzen, also mit einer Wahrscheinlichkeit von 1 zu 800.000 multiplizieren, kommt nur etwa 0,06 als Maßwert für die ""Rationalität"" heraus. Aus diesem Grund zu wählen wäre eher irrational. STANDARD: Was wäre ein besserer Grund? Gelman: Historisch gesehen wurde Rationalität in den Sozialwissenschaften oft mit Egoismus verbunden. Man kann allerdings auch rational, aber nicht egoistisch sein. Oder irrational und egoistisch. Bei einer nichtindividualistischen Perspektive könnte man die Situation so betrachten: Der Sieg meiner bevorzugten Partei ist gleichwertig mit einem Gewinn von tausend Dollar für jeden Bürger, weil ich denke, dass meine Partei besser für die Wirtschaft oder das Gesundheitssystem ist. Bei 300 Millionen Amerikanern ist das eine ziemlich große Summe. Setzt man das in die Gleichung ein, sind wir bei einem Wert von 300.000, was viel höher ist als 0,06. STANDARD: Das könnte man also als rational betrachten? Gelman: Ja. Man könnte sagen, wählen gehen ist wie ein Lotterielos kaufen, von dem aber alle profitieren. Die Wahrscheinlichkeit, dass das Ereignis eintrifft und meine Stimme einen Unterschied macht, ist sehr klein, kann aber einen großen Effekt für die Allgemeinheit haben. STANDARD: Sie beschäftigen sich auch mit Meinungsumfragen im Vorfeld von Wahlen. Warum sehen diese oft anders als das letztendliche Ergebnis aus? Gelman: Man kann sich das mathematische Zufallsmodell folgendermaßen vorstellen: Man greift in einem Behälter nach Bällen, und die Wahrscheinlichkeit für jeden Ball, gezogen zu werden, ist gleich groß. Bei Wahlumfragen verhält sich das allerdings so, als ob manche Bälle gezogen werden wollen und manche nicht. In den 1950er-Jahren hätten sich die meisten Leute an Umfragen beteiligt. Heute sind es weniger als zehn Prozent der Angefragten: Millionen von Menschen bekommen dauernd Fragebögen, und es gibt oft keinen Grund für sie, diese auszufüllen. Diejenigen, die antworten, sind oft ein bisschen einsam oder wollen sich ausdrücken. STANDARD: Wovon hängt das Ergebnis noch ab? Gelman: Wenn ein bestimmter Kandidat gerade erfolgreich ist, werden mehr Menschen, die diesen Kandidaten unterstützen, auf Befragungen reagieren. Wenn es bei ihm wieder schlechter aussieht, werden auch seine Anhänger seltener antworten. So ergeben sich Variationen des Enthusiasmus, die sich bei der Wahl nicht unbedingt abbilden. Eine realistische Einschätzung ist: Die wahre empirische Abweichung ist etwa doppelt so groß wie die angegebene Schwankungsbreite von Wahlumfragen. STANDARD: Welche Besonderheiten hat das politische System der USA verglichen mit Europa? Gelman: Europäische Großparteien sind normalerweise weniger weit voneinander entfernt, wenn es um wirtschaftliche Leitlinien geht, als Republikaner von Demokraten. Zumindest bis vor ein paar Jahren gab es einen größeren Konsens, im Gegensatz zu den USA, wo die beiden Parteien etwas weiter auseinandergehen. Die meisten Wähler sind mit ""ihrer"" Partei unzufrieden, aber sie hassen die andere eben noch mehr. Diese Negativität kann das Regieren schwieriger machen.";https://www.derstandard.at/story/2000095679721/statistiker-gelman-waehlen-ist-wie-ein-lotterielos-kaufen;Standard;Julia Sica
23.03.2020;Quantencomputer: Fehlerkorrektur durch Vervielfältigung;"Digitalisierung, Big Data, Künstliche Intelligenz – trotz enormer Rechenleistung stoßen Supercomputer auch an ihre Grenzen. Quantencomputer sollen in Zukunft diese Grenzen weiter verschieben. Probleme, die mit unseren heutigen Computern nicht lösbar sind, sollen dann geknackt werden. Ein Quantencomputer arbeitet auf den Grundlagen der Quantenphysik. Im Gegensatz zu den klassischen Computern, wo ein Bit entweder den Zustand 0 oder 1 annimmt, können die Qubits des Quantencomputers beide Zustände gleichzeitig haben. Durch diesen Überlagerungszustand können Berechnungen zeitgleich stattfinden, die normalerweise nacheinander erfolgen müssten, was die Quantencomputer ausgesprochen schnell macht. Messung zerstört Information. Auch in Österreich wird an der nächsten Generation der Quantentechnologie geforscht. Die Universität Innsbruck arbeitet beispielsweise mit IBM zusammen. Aber trotz technischer Fortschritte und ""Big Tech""-Wettrennen sind noch immer einige Probleme ungelöst, die einer Routineanwendung von Quantencomputern im Wege stehen. Zum einen sind die Qubit-Systeme extrem anfällig gegenüber äußeren Störfaktoren. Zum anderen zerstört das Auslesen der Bits ihre Information, was Fehlerkorrekturen erschwert. Ein Konzept des Quantencomputers nutzt sogenannte Spins von Elektronen in winzigen Silizium-Strukturen. Für die Messung des Spins, den man sich wie einen Drehimpuls vorstellen kann, wird er in eine Ladung überführt, die sich leicht auslesen lässt. Dabei wird der Spin des Elektrons allerdings verändert. Forscher am RIKEN Center for Emergent Matter Science in Japan haben nun eine Methode entwickelt, mit der sich der Spin messen lässt, ohne ihn zu zerstören. Kopierfehler statt Messfehler. In der Studie, die im Wissenschaftsjournal ""Nature Communications"" erschienen ist, nutzten die Wissenschafter weiterhin die Umwandlung von Spin zu Ladung, rührten dabei aber das Elektron selbst nicht an. Stattdessen übertrugen sie seine Eigenschaften auf ein zweites Qubit. Auch wenn die Information der Kopie bei der Messung zerstört wurde, blieb das Original unverändert. Die Übertragung zwischen Original und Kopie konnte wiederholt und damit der Spin mehrfach ausgelesen werden. Die Messung erfolgte in nur wenigen Mikrosekunden, was für die kurzlebigen Spins von Bedeutung ist. Durch das Auslesen des Qubits, ohne es zu verändern, können Störungen im System erkannt und korrigiert werden. Durch wiederholte Messungen der Kopie kann auch der Messfehler minimiert werden, wodurch die Wissenschafter eine Auslesegenauigkeit von 95 Prozent erreichten. Die verbleibende Fehlerrate sei vor allem auf Kopierfehler bei der Informationsübertragung zurückzuführen. Weitere Forschung soll diese Schwachstelle verbessern.";https://www.derstandard.at/story/2000115249392/quantencomputer-fehlerkorrektur-durch-vervielfaeltigung;Standard;Friederike Schlumm
30.04.2020;Österreicher gingen bereits vor Ende der Ausgangsbeschränkung vermehrt hinaus;"In den letzten beiden Aprilwochen ist die Mobilität im Land deutlich gestiegen, zeigen aktuelle Auswertungen von Handydaten. Vor allem für Geschäfte mit Laufkundschaft und ab Monatsmitte für die Gasthäuser ist es wichtig, dass die Österreicher im Mai wieder mehr und weitere Wege zurücklegen. Die Zeichen dafür stehen gut: Schon bevor die Ausgangsbeschränkungen Ende April auslaufen, gehen die Österreicher wieder vermehrt vor die Tür. Das zeigen aktuelle Auswertungen von Handydaten des Telekomanbieters A1 und des TU-Graz-Spin-offs Invenium. Die Wissenschafter unterteilen die Bevölkerung in eine stationäre Gruppe (wenigerals ein Kilometer), die eingeschränkt Mobilen (ein bis zehn Kilometer) und die Mobilen, die über zehn Kilometer unterwegs sind. Am vergangenen Sonntag waren wieder so viele Menschen in einem Radius von über zehn Kilometern unterwegs wie an jenem Sonntag vor sechs Wochen, bevor die Ausgangsbeschränkungen in Kraft getreten waren. Ein Einbruch der Disziplin lasse sich daraus aber nicht ableiten, erklärt Mario Mayerthaler, Innovationsleiter bei A1. Denn die Österreicher haben sich bereits vor den offiziellen Maßnahmen verstärkt isoliert. ""Ich war selbst überrascht, wie strikt die Vorgaben befolgt wurden"", sagt der Forscher. Zum Vergleich: An den ersten beiden Sonntagen im März waren noch rund 25 Prozent der Bevölkerung mobil – und das bei kaltem Wetter. Unmittelbar vor den Maßnahmen sowie zuletzt lag der Anteil der mobilen Menschen bei 14 Prozent. Im Verlauf der letzten Wochen gab es außerdem Lockerungen der Corona-Maßnahmen für Geschäfte oder bei den Bundesgärten. Einzelne Hotspots in Wien wie der Volksgarten, der Bereich entlang der Donau oder der Yppenplatz zeichnen sich in den Daten der Forscher deutlich ab. Für den Handel wichtig: Auch in den vor kurzem noch leergefegten Innenstädten sieht man wieder mehr Aktivität. Normalisierung Mitte Mai erwartet. Nicht nur am Wochenende, sondern auch an Werktagen waren die Österreicher im Verlauf der letzten Wochen mobiler. Vergangene Woche blieben noch 45 Prozent der Menschen innerhalb eines Radius von einem Kilometer. Auf dem Höhepunkt der Isolation lag dieser Wert bei 56 Prozent. Vor der Krise lag der Anteil unter 30 Prozent. Bis Mitte Mai, wenn auch die Gasthäuser wieder öffnen dürfen, rechnen die Forscher mit einer deutlichen Zuname der Mobilität an den Wochenenden, sagt Michael Cik, Mitgründer von Invenium. Für die Mobilität an Werktagen wird es auch eine Rolle spielen, wie lange Unternehmen noch auf Home-Office setzen. Bewegungsdrang nach Ostern. Dass seit Ostern mehr Österreicher ihre unmittelbare Umgebung verlassen, zeigt auch eine aktualisierte Auswertung des Complexity Science Hub (CSH) mit der TU Wien. Dort berechnen die Wissenschafter mitunter den mittleren Bewegungsradius. Nach dieser Methode ist die Mobilität der Österreicher fast auf das Niveau vor den Ausgangsbeschränkungen zurückgekehrt. Bevor die Maßnahmen eingeführt wurden, bewegte sich der durchschnittliche Österreicher demnach in einem Umkreis von rund 14 Kilometern am Tag. Infolge der Ausgangsbeschränkungen halbierte sich der durchschnittliche Bewegungsradius der Bevölkerung auf knapp acht Kilometer. Nach dem Osterwochenende wurden mit über zwölf Kilometern fast die früheren Werte wiedererreicht. Ob die gesteigerte Mobilität der Bevölkerung mit einem neuerlichen Anstieg bei den Corona-Infizierten einhergehen wird, lässt sich nicht ableiten. Immerhin liegt Ostern über zwei Wochen zurück, und die Zahl neuer Fälle ging stetig zurück. Die anonymisierten Handydaten zeigen nicht, ob sich die Menschen nur mit Haushaltsmitgliedern und stets auf Abstand zueinander bewegen. Das Tracking funktioniere nicht wie beim GPS auf den Meter genau, sondern für einen Radius von einem Kilometer, gibt Michael Cik von Invenium zu bedenken. Die Analysen von Invenium beruhen auf Information darüber, welche Mobiltelefone von A1-Kunden sich über den Tag verteilt an welchen Handymasten melden. Jedes Handy erhält täglich eine für das Tracking automatisch und zufällig generierte Nummer zugewiesen. Der Prozess verläuft damit anonymisiert. Die Bewegungsprofile teilt die Forscherguppe seit Mitte März mit dem Krisenstab der Bundesregierung und machte sie im April auch öffentlich. Beim CHS und der TU Wien kombinieren die Forscher mehrere Datensets.";https://www.derstandard.at/story/2000117201126/oesterreicher-gingen-bereits-vor-ende-der-ausgangsbeschraenkung-vermehrt-hinaus;Standard;Leopold Stefan
17.01.2018;"Jobsuche: ""UX Designer"" ist kein Fremdwort mehr";"Eine Auswertung zeigt, dass digitale Jobs 2017 häufiger gesucht wurden als noch im Jahr davor. Das Wochenende ist gerade vorbei, die Arbeitswoche fängt wieder an. Manche Erwerbstätige starten mit Elan in den wohl unbeliebtesten Arbeitstag, andere sehnen schon am Montag den Feierabend am Freitag herbei. Ob es die Energie vom Wochenende oder die Unzufriedenheit im Büro ist: Montagvormittag um elf Uhr suchen Arbeitnehmerinnen und Arbeitnehmer am häufigsten nach Jobangeboten. Und zwar vom Desktop aus. Mobil suchen die Berufstätigen am liebsten dienstags in ihrer Mittagspause nach Karrieremöglichkeiten oder vergleichen Stellenangebote. Zu diesen Ergebnissen kommt eine aktuelle Auswertung der Zugriffe auf die Jobbörse Stepstone im Jahr 2017. Am häufigsten wurde nach Außendienstmitarbeitern, Verkäufern und Bürojobs gesucht. Neben diesen klassischen Berufen ist auch das Suchvolumen nach Jobs, die durch die Digitalisierung entstanden sind, gestiegen. Mehr Jobs in IT gesucht. Bis vor zwei Jahren benötigten viele Jobsuchende noch ein Wörterbuch, um zu wissen, welcher Job sich hinter einem Agile Project Manager, Scrum Master oder UX Designer verbirgt. Mittlerweile ist vielen klar, dass es sich bei den beiden ersten um Projektmanager handelt, die agile Prozesse der Softwareentwicklung koordinieren, und ein UX Designer für die User Experience zuständig ist und etwa Webseiten oder Apps einheitlich gestaltet. Das sagen zumindest die Daten der Stepstone-Auswertung: Vergangenes Jahr wurde der Agile Project Manager 3,5-mal häufiger gesucht als noch 2016, der Scrum Master mehr als doppelt so oft und der UX Designer um 51 Prozent häufiger. ""Das zeigt eine wachsende Vielfalt von neuen und veränderten Jobprofilen. Gerade im IT-Bereich spezialisieren sich die Berufsbilder immer mehr"", sagt Rudi Bauer, Geschäftsführer von Stepstone Österreich. Außerdem können sich die Jobsuchenden eine Tätigkeit als Data Scientist oder im Bereich der Suchmaschinenoptimierung (SEO) vorstellen. Bauer rät daher Unternehmen, in den Stellenanzeigen nicht nach IT-Experten zu suchen, sondern die Stelle möglichst konkret zu beschreiben. Große Städte, große Firmen. Besonders Wien war als Arbeitsstätte gefragt, gefolgt von Graz, Linz und Salzburg. Nicht nur die großen Städte sind beliebt, sondern auch große Firmen. Die scheinbar beliebtesten Arbeitgeber unter den Stepstone-Suchenden sind die Österreichische Post, Siemens, Billa und Ikea. Starke Marken würden bei den Bewerberinnen und Bewerbern gut ankommen, sagt Bauer.";https://www.derstandard.at/story/2000072379155/jobsuche-ux-designer-ist-kein-fremdwort-mehr;Standard;
13.11.2018;Universität Wien schreibt 70 neue Professuren aus;"In fünf Entwicklungsfeldern werden 51 normale und 19 Tenure-Track-Professuren ausgeschrieben. Wien – Die Universität Wien hat auf einen Schlag 70 neue Professuren ausgeschrieben. Diese entstehen einerseits in zukunftsgerichteten Themenfeldern wie Data Science oder Künstliche Intelligenz und andererseits in besonders überlasteten Fächern. Die neuen Stellen werden durch Mittel aus den ab 2019 geltenden neuen Leistungsvereinbarungen mit dem Bund finanziert. Quanten und Gesellschaft. Konkret wurden 51 ""normale"" und 19 sogenannte Tenure-Track-Professuren ausgeschrieben, die Nachwuchswissenschaftern langfristige Karriereperspektiven eröffnen sollen. Die ausgeschriebenen Stellen verteilen sich auf die fünf Entwicklungsfelder ""Data Science & Digital Humanities"", ""Gesundheit & Mikrobiom"", ""Molekulare Biologie & kognitive Neurowissenschaften"", ""Quanten & Materialien"" sowie ""Gesellschaft & Kommunikation"" und erfassen fast alle Fakultäten. Unter anderem werden dabei Geistes- und Sozialwissenschaften mit digitalen Arbeitsmitteln und Methoden kombiniert, die Voraussetzungen für ein gesundes Leben an der Schnittstelle von Lebenswissenschaften, Molekularbiologie und Chemie analysiert oder die Spezialgebiete Quantenoptik und Quantenkombination gestärkt. Bis Ende November werden außerdem gemeinsam mit der Medizin-Uni Wien drei weitere neue Professuren ausgeschrieben (Computational Medicine, Molecular Biology und Public Health Nutrition). Die Bewerbungsfristen laufen bis Jänner. Nicht von der Ausschreibung umfasst sind Nachbesetzungen bereits bestehender Professuren – diese erfolgen unabhängig davon. Insgesamt erhält die größte Uni des Landes im Zeitraum 2019 bis 2021 eine Budgetsteigerung von 17 Prozent. Das entspricht einem Plus von rund 207 Millionen Euro.";https://www.derstandard.at/story/2000091211174/universitaet-wien-schreibt-70-neue-professuren-aus;Standard;
28.07.2015;"""Complexity Science Hub Vienna"" nimmt Arbeit auf";"""Verein zur wissenschaftlichen Erforschung komplexer Systeme"", eine Initiative der TU Wien und Graz, der Medizin-Uni Wien und des AIT. Wien – ""Sinnvolles Wissen aus Big Data gewinnen"" – das ist laut Komplexitätsforscher Stefan Thurner das Ziel des ""Complexity Science Hub Vienna"". Mit der Gründung des ""Vereins zur wissenschaftlichen Erforschung komplexer Systeme"" haben die Kooperationspartner – die Technischen Unis Wien und Graz, die Medizin-Uni Wien und das Austrian Institute of Technology (AIT) – nun das Projekt offiziell gestartet. Die vier Projektpartner tragen mit jeweils 200.000 Euro pro Jahr zu dem neuen Zentrum bei: jeweils 40.000 Euro in bar sowie 160.000 Euro in Form von zwei Laufbahnstellen für einen Senior- und einen Junior-Wissenschafter, sagte der wissenschaftliche Geschäftsführer des AIT, Wolfgang Knoll, zur APA. Die Stellen sollen im Herbst ausgeschrieben und idealerweise im Februar des kommenden Jahres die ersten Forscher angestellt werden, so Thurner, der an der Med-Uni Wien Professor für Komplexitätsforschung ist. Um die Verwaltung des ""Complexity Science Hub"" (CSH) gering zu halten, werden die Wissenschafter vom jeweiligen Partner angestellt und dem Zentrum zugeordnet. Inhaltlich sollen die Kooperationspartner nur grobe Linien vorgeben, etwa Komplexität im Zusammenhang mit ""Smart City"" oder ""Medizinische Versorgung"". Dafür sollen dann die besten Leute gesucht und angestellt werden, die selbst entscheiden, zu welchen konkreten Themen sie arbeiten. Externes Wissen zu diesen Themen soll dann gezielt und projektbezogen über Gastwissenschafter nach Wien gebracht werden. Austausch mit Singapur. Am CSH sollen auch Doktoranden tätig sein. Laut Knoll will die Österreich-Tochter des Technologiekonzerns Infineon zwei PhD-Studenten finanzieren. Gearbeitet wird auch an einem Austauschprogramm für PhD-Studenten mit der Technischen Universität Nanyang (NTU) in Singapur, wo vor kurzem ein Institut für Komplexitätsforschung gegründet wurde. Im Vollausbau könnten fünf bis zehn Senior-Forscher insgesamt 15 bis 30 Post-Docs und PhD-Studenten projektbasiert beschäftigen. Für die Finanzierung von Projekten hat nach Angaben der beteiligten Wissenschafter das Infrastrukturministerium Mittel in Aussicht gestellt. Weitere Förderungen könnten sich im September entscheiden. Neben Thurner und Knoll waren die Wissenschaftsforscherin und Ex-Präsidentin des Europäischen Forschungsrates, Helga Nowotny, und der Chef des Wiener Wissenschafts- und Technologiefonds (WWTF), Michael Stampfer, an der Ausarbeitung des Konzepts für das Zentrum beteiligt. Nowotny wird auch den wissenschaftlichen Beirat des CSH leiten. Unverstandene vernetzte Systeme. Der Physiker Stephen Hawking hat die Komplexitätsforschung als ""die Wissenschaft des 21. Jahrhunderts"" bezeichnet. Hintergrund ist, dass Probleme zunehmend systemisch werden und regionale Entwicklungen globale Auswirkungen haben können. ""Systemische Risiken etwa im Zusammenhang mit Klimawandel, Finanzmärkten, Naturkatastrophen, Migration, etc. werden derzeit wissenschaftlich meist nicht verstanden und können daher letztlich auch nicht strategisch gemanagt werden"", erklärt Thurner den Hintergrund, vor dem der CSH gegründet wurde. Solche Risiken entstehen vor allem durch die drastisch zunehmende Vernetzung von Menschen, Einrichtungen, Computern, Märkten, etc. – die sich auch in einer ebenso drastischen Zunahme von Daten widerspiegelt. Und aus diesen Daten wollen die Komplexitätsforscher ""nutzbaren Sinn"" holen, wie Thurner sagte. Ziel des CSH sei, aus der Analyse von ""Big Data"" ""systemische Risiken zu verstehen, sichtbar zu machen und Wege für deren Management zu entwickeln"". Mut zu politikrelevanten Resultaten. Grundsätzlich soll in dem Zentrum Grundlagenforschung gemacht werden, ""wir haben aber keine Angst, wenn politikrelevante Resultate herauskommen, da wollen wir mutig sein"", sagte Thurner. Man wolle auch ""so offen wie möglich sein: Wenn Daten da sind, wollen wir diese der Gesellschaft zugänglich machen, damit Debatten auf höherem Niveau stattfinden können"". Für den Wissenschafter ist es wichtig, dass die Öffentlichkeit die Hoheit über kritische Daten behält, sofern das noch möglich ist. Zudem will das Zentrum eine ""Plattform für ethische Fragen sein, die mit Big Data einhergehen"". Thurner ortet hier eine ""Revolution mit ungelösten Fragen, was würdevoll, ethisch und vertretbar ist, wo die Privatsphäre verletzt wird und wo nicht"". Auch der Gesetzgeber sei mit der Geschwindigkeit, mit der Daten verfügbar werden, ""komplett überfordert"".";https://www.derstandard.at/story/2000019836995/complexity-science-hub-vienna-nimmt-arbeit-auf;Standard;
01.04.2019;Kapsch BusinessCom übernahm Wiener AIMC komplett;Einstieg bei dem Artificial-Intelligence-Spezialisten erfolgte bereits im Februar 2017. Die Kapsch BusinessCom, ein Unternehmen der Kapsch-Gruppe, hat sich 100 Prozent an der Tochtergesellschaft Advanced Information Management Consulting GmbH (AIMC) gesichert. Das Artificial-Intelligence-Unternehmen sei per Anfang 2019 vollständig übernommen worden, teilte Kapsch am Montag mit. Angaben zum Kaufpreis wurden keine gemacht. Im Februar 2017 war der Ankauf von 25 Prozent erfolgt. AIMC wurde 2002 gegründet und ist im Bereich Data Science tätig. Ein Schwerpunkt der Tätigkeit liegt im medizinischen Consulting.;https://www.derstandard.at/story/2000100606175/kapsch-businesscom-uebernahm-wiener-aimc-komplett;Standard;
13.06.2017;"Udacity: Mit dem ""Nanodegree"" ins Silicon Valley";"Die von einem Stanford-Professor gegründete Online-Uni gibt es nun auch in Europa. Nicolas Dittberner, Leiter der Dach-Region, im Gespräch. Wie digitale Bildung die Verlierer zu Gewinnern machen könnte: Darüber spricht Nicolas Dittberner, während er in einem hippen Café in Prenzlauer Berg seinen Kaffee umrührt. Dittberner, 36 Jahre, gemusterte Jacke, direkter Blick, hat Udacity, eine auf Weiterbildung spezialisierte Online-Uni, in Europa aufgebaut. Der junge Mann ist überzeugt, dass mit Kursen im Netz Menschen erreicht werden können, die keine Möglichkeit zu einem klassischen Universitätsstudium haben. Auch für bereits Berufstätige, die abgehängt zu werden drohen, könne sie Chancen bieten – sie mit jenen digitalen Skills ausstatten, nach denen Unternehmen derzeit händeringend suchen. Aber der Reihe nach. Begonnen hat die Geschichte von Udacity nämlich nicht in Berlin, sondern rund 9000 Kilometer entfernt, an der Eliteuni Stanford in Kalifornien. Dort machten 2011 zwei Professoren, Sebastian Thrun und Peter Norvig, ihr Seminar zum Thema künstliche Intelligenz kostenlos im Netz zugänglich. Der Erfolg war spektakulär: 160.000 Menschen aus 190 Ländern meldeten sich an, 23.000 bestanden die Abschlussprüfung. Thrun sah Potenzial, kündigte seine Professorenstelle und seinen Job als Forschungschef bei Google und gründete 2012 Udacity. Sein Ziel: Bildung nicht nur einigen Privilegierten, sondern möglichst vielen zugänglich zu machen. Geld-zurück-Garantie. Wie Udacity dieses Ziel aktuell einzulösen versucht, soll Nicolas Dittberner fünf Jahre später in Berlin beantworten. ""Durch Internationalisierung"", sagt er. Udacity gibt es neben Europa mittlerweile auch in Brasilien, China und Indien. ""Und wir sind weiter am Wachsen."" In vielen Gegenden seien Möglichkeiten zum Studieren rar, Programme teuer. Bei Udacity sind viele Kurse gratis. Für die, die 200 Euro pro Monat kosten und von Tutoren betreut werden, kreierte Udacity einen eigenen Abschluss, genannt ""Nanodegree"". Schließt man das Programm in weniger als zwölf Monaten ab, bekommt man die Hälfte der Gebühren erstattet, verspricht Udacity. 40.000 Personen Teilnehmer haben die Nanodegree-Programme derzeit weltweit, davon 5.500 in Europa und 2.200 im deutschsprachigen Raum. Vermittelt wird in den Kursen Know-how, das Tech-Firmen derzeit so dringend suchen: Data-Science, Machine-Learning, Virtual Reality oder Robotics. Wie gut klappt digitales Lernen? Die Nähe zur Industrie wird bei Udacity überhaupt großgeschrieben, weshalb man mit Firmen kooperiert – in Deutschland etwa mit Bosch, Daimler oder IBM. Sie lieferten inhaltliche Impulse. Was außerdem ""hilft, wichtige Themen zu identifizieren"": der Unternehmenssitz im Silicon Valley. ""Wir sind sehr nah dran an technologischen Trends"", sagt Dittberner. Vor dem Hintergrund des permanenten Fortschritts müsse der oder die Einzelne künftig anders, besser ausgebildet sein, ist er überzeugt: ""Wenn alles stärker automatisiert ist, wird es jemanden brauchen, der die Software steuert und überwacht."" Ob er der Meinung ist, dass staatliche Universitäten bei technischen Ausbildungen mithalten? ""Sie versuchen, die Bereiche zu besetzen, können aber nicht schnell genug agieren"", so Dittberner. Verantwortlich dafür macht er mangelnde Kontakte zur Industrie. Diese Praxisnähe soll bei Udacity auch in puncto Lehre im Vordergrund stehen, Studierende sich ihr Wissen in gemeinsamen Projekten erarbeiten. ""Jeder Teilnehmer erhält so lange Feedback, bis das Projekt als erfolgreich abgeschlossen gilt."" Alles also virtuell, auch der Austausch mit Kommilitonen. Didaktiker sind skeptisch, ob sich bei rein digitalem Lernen ein Lernerfolg einstellt. Wichtig sei der Kontakt mit Professoren und Mitstudierenden. Dittberner sagt dazu: ""Wir konzentrieren uns auf technologische Berufe. Da findet die Kommunikation schon von jeher online statt."" Zudem würden eigene Community-Manager darauf angesetzt, Probleme in den Foren zu identifizieren – ""Gibt es bei einem Thema Nachholbedarf?"" – und gegebenenfalls zusätzliche Informationen zu liefern. Es gebe zusätzlich Meet-ups, regelmäßige persönliche Treffen. Was die Zukunft bringt. Investor von Udacity ist etwa Bertelsmann. In Europa sei man aber bereits profitabel, sagt Dittberner. Dort gehe es weniger darum, Bildung zu demokratisieren, als vielmehr darum, lebenslanges Lernen zu ermöglichen. Zielgruppe sind vor allem Berufserfahrene, die sich für die stark wandelnde Arbeitswelt wappnen wollen. ""Was wir herausgefunden haben, ist, dass es ein Segment gibt, um das sich niemand kümmert: die Erwachsenenfortbildung"", sagte Udacity-Gründer Thrun kürzlich in einem Interview mit der deutschen Zeitung Die Zeit. In diesem Sektor ortet Thrun ""gute Chancen mitzuspielen"", da steige der Bedarf, denn: ""Es wird nicht mehr reichen, einmal im Leben zur Uni zu gehen.""";https://www.derstandard.at/story/2000059025344/udacity-mit-dem-nanodegree-ins-silicon-valley;Standard;Lisa Breit
27.08.2020;Wie wir Informationen aus großen Datenmengen gewinnen;"Machine Learning ist die perfekte Grundlage für eine effiziente Datenanalyse. Der Mensch ist ein Sammler. Wir sammeln unvorstellbar große Datenmengen. Wozu? Wir sind auf der Jagd nach Wissen - der Mensch ist nämlich auch ein Jäger. Das europäische Forschungsnetzwerk ""Europlanet 2024 - Research Infrastructure (RI)"" sammelt riesige Datenmengen von Weltraummissionen, Laborexperimenten, Teleskopnetzwerken und Simulationen und macht sie der wissenschaftlichen Gemeinschaft zugänglich. Eine der größten Herausforderungen der modernen Datenanalyse ist es, die Information und das Wissen, das in diesen Daten steckt, hervorzuholen und nutzbar zu machen. Genau damit beschäftigt sich ein Arbeitspaket von ""Europlanet 2024 RI"", das unter der Leitung des Grazer Instituts für Weltraumforschung (IWF) der Österreichischen Akademie der Wissenschaften (ÖAW) steht. Und da uns Wissenschafterinnen und Wissenschafter am IWF und den Kolleginnen und Kollegen an unseren Partnerinstituten nur eine begrenzte Lebenszeit für die Verarbeitung und Analyse von schier unbegrenzten Datenmengen zur Verfügung steht, holen wir uns Hilfe. Maschinelles Lernen. Maschinelles Lernen (engl. machine learning, ML) ist ein Zweig der künstlichen Intelligenz (KI) und im Prinzip eine Methode der Datenanalyse. KI beschäftigt sich damit menschliche Fähigkeiten zu simulieren, wobei ML hierbei das Ziel hat, Systeme so zu trainieren, dass sie aus Daten lernen, Muster erkennen und darauf aufbauend Entscheidungen treffen können. Das Konzept von ML ist nicht neu und viele mathematische Algorithmen dafür gibt es bereits seit Jahrzehnten. Jedoch hat ML in den letzten Jahren einen großen Aufschwung erfahren, da die immer größer werdenden Datenmengen (Stichwort Big Data) die perfekte Grundlage für effiziente Datenanalyse mit Hilfe von ML bilden. Mit den immer günstiger werdenden Rechen- und Speichersystemen ist es heutzutage möglich, schnell komplexe Modelle automatisch auf Daten anzuwenden und in kurzer Zeit Ergebnisse zu erzielen. Erkenntnisse, die das System aus bestimmten Daten gewinnt, lassen sich auf neue Daten übertragen und so für neue Problemlösungen und Ergebnisse verwenden. Praktische Anwendungen von ML kennt mittlerweile jeder von uns und wir produzieren mit Sicherheit auf die eine oder andere Weise Daten dafür. So sind auf die eigenen Vorlieben zugeschnittene Werbebanner oder Online-Empfehlungen auf Plattformen wie Amazon oder Netflix Paradebeispiele für ML im Internetalltag. Ebenso sind automatische Bild-, Text- und Gesichtserkennungen oder selbstfahrende Autos weitere Anwendungsbereiche. Den Menschen ganz wegdenken kann man sich aus diesem Prozess jedoch (noch?) nicht. Denn bevor die Maschine anfängt zu lernen, muss sie mit Trainingsdaten und Algorithmen versorgt werden. Je nach Algorithmus gibt es verschiedene Lernkategorien. Die am weitesten verbreiteten Kategorien sind überwachtes und unüberwachtes Lernen. Überwachtes Lernen. Beim überwachten Lernen (engl. supervised learning) werden bestimmte Muster und Gruppenzugehörigkeiten (Klassen) vorab in sogenannten Trainingsdaten spezifiziert und gekennzeichnet (labelled data). Das System lernt mit diesen Daten die Regeln, wie man solche Muster oder Klassenzuordnungen erkennt. Die Trainingsdaten enthalten dabei auch die richtigen Antworten, sodass das Modell weiß, ob es mit seiner Vorhersage richtig oder falsch liegt. Diese Erkenntnis verwendet es, um seine internen Parameter dahingehend anzupassen, die Trainingsergebnisse bei weiteren Versuchen zu optimieren. Das so trainierte Modell soll dann selbstständig bei neuen, unbekannten Daten die richtigen Zuordnungen durchführen oder Trends in den Gruppen erkennen. Zwei Kategorien kommen in den meisten Fällen beim überwachten Lernen zur Anwendung: das Regression- oder Klassifikationsverfahren. Bei Regressionsproblemen wollen wir eine Funktion - zum Beispiel eine Gerade - finden, die unsere Datenpunkte bestmöglich abbildet. Es werden, salopp gesagt, einzelne Werte vorhergesagt. Wollen wir hingegen ein Klassifikationsproblem lösen, treffen wir eine Vorhersage, welche Datenpunkte zu welcher Gruppe oder Klasse gehören. Beispiele für überwachtes Lernen sind das Erkennen von Spam-Emails oder die Vorhersage von Wohnungspreisen am Immobilienmarkt aufgrund verschiedener Faktoren. Um die Erfolgschancen für diese Art des Lernens zu maximieren, sind natürlich große, annotierte Datenmengen zum Trainieren des Systems von Vorteil. Wichtig hierbei ist auch, dass das Trainingsset eine möglichst große Varianz aufweist, also alle möglichen Variationen des Sachverhalts abbildet. Das kann zu einem hohen Aufwand in der Vorbereitung der Trainingsdaten führen. Der Vorteil des überwachten Lernens ist jedoch, dass man relativ einfach nachvollziehen kann, was passiert und was die Maschine tut. Allerdings bleiben dadurch auch neue Lösungen auf der Strecke. Das Modell wird keine neuen Muster finden, sondern immer im zuvor abgesteckten Rahmen bleiben. Unüberwachtes Lernen. Beim unüberwachten Lernen (engl. unsupervised learning) erkennt das System eigenständig in den Daten vorhandene Muster, Zusammenhänge und Ähnlichkeiten, ohne dass es mit Trainingsbeispielen gefüttert wird (unlabelled data). Algorithmen des unüberwachten Lernens helfen, die vorliegenden Daten besser zu verstehen und eventuell versteckte Strukturen, neue Muster, Zusammenhänge oder auch Anomalien zu entdecken. Sogenannte Deep-Learning-Algorithmen bilden die Grundlage für unüberwachtes Lernen und basieren auf künstlichen neuronalen Netzwerken (KNNs, oder engl. artificial neural networks, ANNs). KNNs versuchen die Funktion menschlicher Nervenzellen zu imitieren. Die Eingabe wird von künstlichen Neuronen, die oftmals in vielen, verdeckten Schichten im Netzwerk vorliegen, verarbeitet. Jedes Neuron lernt dabei etwas und gibt sein Wissen und das Gelernte durch verschiedene Verbindungen (Synapsen) an andere Neuronen weiter, bis am Ende das Problem gelöst ist und die Ausgabe erfolgt. Der Begriff Deep Learning kommt daher, dass ein NN eine Vielzahl an verdeckten Schichten (hidden layers) haben und somit eine sehr tiefe Netzstruktur aufweisen kann. Deep-Learning-Algorithmen und NNs werden heutzutage in vielen Bereichen  eingesetzt, nicht nur beim unüberwachten Lernen. So werden auch Problemstellungen des überwachten Lernens, oder aber auch Kombinationen aus verschiedenen Lerntypen, mit KNNs gelöst - von Bildsegmentierung, über Mustererkennung zu medizinischer Diagnostik. Die Fähigkeit von tiefen KNNs beliebig komplexe Funktionen zu approximieren, ist aber zugleich einer der Kritikpunkte, da man aufgrund der großen Komplexität und Abstraktion der Datenrepräsentation nicht mehr wirklich nachvollziehen kann, was genau passiert - das KNN ist eine Black Box. Da die Anwendungen von ML zunehmen, wird auch die Erklärbarkeit der Deep-Learning-Algorithmen in Zukunft eine wichtigere Rolle spielen. Was wir in den Daten suchen. In unserem Arbeitspaket verbinden wir Methoden des ML mit der Notwendigkeit, auch in den wissenschaftlichen Bereichen, konkret in den planetaren Wissenschaften, immer größer werdende Datenmengen zu analysieren. Expertinnen und Experten im Bereich ML und Deep Learning erstellen in Zusammenarbeit mit Wissenschaftlerinnen und Wissenschaftern Analysetools, um verschiedene wissenschaftliche Problemstellungen zu untersuchen. Die entwickelten Modelle werden auf sehr unterschiedliche Daten angewandt. So werden zum Beispiel Bilder der Marsoberfläche analysiert, um verschiedene Oberflächenstrukturen auf dem Mars automatisch zu erkennen und zu klassifizieren. Damit möchten wir Rückschlüsse auf deren Entstehungsgeschichte ziehen können. Eine weitere Anwendung der von uns entwickelten Modelle ist die automatische Erkennung und Vorhersage von interplanetaren koronalen Massenauswürfen (engl. coronal mass ejection, CME) in Sonnenwinddaten. Abhängig von den Plasmaparametern innerhalb einer CME kann es weitreichende Auswirkungen haben, wenn diese die Erde trifft. Man spricht hier von der Geoeffektivität einer interplanetaren CME. Um vorgewarnt zu sein und eventuell auch auf ein derartiges Ereignis im Vorhinein reagieren zu können, arbeiten Wissenschafterinnen und Wissenschafter weltweit an Vorhersagemodellen. Eine automatische Erkennung und Klassifikation solcher Ereignisse in den Daten würde bei der Erstellung von Modellen natürlich immens hilfreich sein. Die fertigen ML-Codes werden der wissenschaftlichen Gemeinschaft frei zugänglich zur Verfügung gestellt, sind open source und können somit modifiziert auf eigene Problemstellungen und Daten angewandt werden. Europlanet 2024 - Research Infrastructure. Das Forschungsnetzwerk ""Europlanet 2024 RI"" wird im Rahmen von Horizon 2020 der Europäischen Union in der Finanzhilfevereinbarung Nr. 871149 gefördert. Dieses Forschungsnetzwerk, das seine Anfänge im Jahr 2005 hat und seitdem kontinuierlich erweitert wurde, stellt verschiedene Services bereit, um der wissenschaftlichen Gemeinschaft Daten aus Experimenten, Simulationen, Weltraummissionen und Teleskopnetzwerken zur Verfügung zu stellen. Unter der Leitung des Grazer Weltrauminstituts stellen sich im Arbeitspaket ""Machine Learning"" folgende Institutionen den neuen Herausforderungen der Datenanalyse in den planetaren Wissenschaften: das Grazer Know-Center, die Universität Passau, das Deutsche Zentrum für Luft- und Raumfahrt (DLR), das französische ACRI-ST, das italienische National Institute for Astrophysics (IN-AF), das Institute of Atmospheric Physics der Tschechischen Akademie der Wissenschaften (IAP-CAS), das nordirische Armagh Observatory and Planetarium (AOP) und die russische Lomonossow-Universität Moskau. Europlanet Science Congress (EPSC). Europlanet organisiert jährliche Treffen für die wissenschaftliche Gemeinschaft. EPSC ist die wichtigste europäische Konferenz im Bereich der planetaren Wissenschaften. Aufgrund von Covid-19 wird EPSC2020 heuer vom 21. September bis 9. Oktober erstmals als virtuelles Meeting abgehalten, wodurch auch nicht registrierte Personen an verschiedenen Workshops teilnehmen können. InspiredByOtherWorlds - Ein Wettbewerb für alle Kreativen! EPSC2020 lädt Schulen und Raumfahrt-Enthusiasten aller Altersgruppen ein, kreativ zu werden und ihre von anderen Welten inspirierten Kunstwerke und Performances in einem Wettbewerb namens #InspiredByOtherWorlds zu teilen. Erlaubt ist alles: Zeichnungen, Geschichten, Bilder, Videos, Modelle, Kunsthandwerk oder Kunstinstallationen. Das Ziel der kreativen Reise können Planeten, Monde, Asteroiden, Kometen, Meteoriten oder Exoplaneten sein. Alle Beiträge werden in einer virtuellen Ausstellung im Rahmen von EPSC2020 präsentiert.";https://www.derstandard.at/story/2000118440133/wie-wir-informationen-aus-grossen-datenmengen-gewinnen;Standard;Ute Amerstorfer
04.08.2020;Gott würfelt nicht, KI schon - wie viel Zufall steckt in modernen Algorithmen?;Vor wenigen Jahren galt die Digitalisierung noch für manche als Segen, für andere als Fluch. Die Auswirkungen auf die Arbeitswelt treten nun nach und nach zu Tage, mit Licht und Schatten, positiven und negativen Auswirkungen. Doch der nächste Schritt in Richtung Industrie 4.0, „Künstliche Intelligenz“, ist beschritten als eines der großen neuen Themengebiete, die oft in einem Atemzug genannt werden mit selbstlernenden Maschinen und virtueller Realität. Big Data, KI, AI – what? Klingt alles sehr fancy, doch was steckt nun wirklich dahinter? Von künstlicher Intelligenz wird gesprochen, wenn das jeweilige System (zum Beispiel eine Maschine mit entsprechender Software) Informationen nicht nur aufnehmen kann, sondern diese auch weiterverarbeitet, beurteilen kann und selbstständig Entscheidungen, basierend auf den erhaltenen Daten, trifft. Die Grundlage für diese Möglichkeit des eigenständigen Lernens und Entscheidens sind möglichst umfangreiche Datenmassen, denn je mehr Daten vorhanden sind, desto mehr „Futter“ steht dem Algorithmus zur Verfügung, um sich selbst weiterzuentwickeln. Big Data, ein weiteres Schlagwort in diesem Zusammenhang, kommt hier also sinnvoll zur Anwendung und verdient somit sein negatives Image nicht immer. Während die meisten Menschen mit Big Data gedanklich sofort an Überwachungsmaßnahmen oder zumindest Tracking von Onlineaktivitäten zu Marketingzwecken denken, gibt es schließlich auch sinnvolle Einsatzmöglichkeiten der Technologie. Altes Wissen neu verpackt. Während alles rund um Big Data und selbstlernende Computer topmodern klingt, sind die Grundideen und Modelle, die genutzt werden, altbekannte Berechnungsmethoden. Ein typisches Beispiel dafür ist der Monte-Carlo-Algorithmus. Diese Methode arbeitet ergebnisoffen und nutzt ein einfaches Prinzip: den Zufall. Der Algorithmus liefert ständig Zwischenergebnisse, für die definiert wird, „wie falsch“ sie sozusagen sein dürfen, es gibt also gewissermaßen eine Obergrenze bei der bekannt ist, ab hier macht es keinen Sinn mehr. Bei allen anderen Werten, die diese Grenze nicht übersteigen, ist es hingegen so, dass sie wie für ein Näherungsverfahren genutzt werden können und so nach und nach – quasi selbstlernend – ein Resultat ermittelt wird. Das klingt nun erstmal sehr theoretisch, doch die praktischen Anwendungsmöglichkeiten sind geradezu endlos. Ein simples Beispiel dafür ist die Anwendung des MC-Algorithmus bei Loskalkulatoren im Lotto. Anhand dieses Beispiels zeigt sich gut, dass Technologien noch so innovativ sein können, die Basis bildet fundiertes, historisch gewachsenes Wissen, das (damals noch) menschlichen Gehirnen entsprungen ist und nicht von Maschinen oder Programmen generiert wurde. Zufälle zu berechnen ist mathematisch aber eine gewaltige Herausforderung. Denn Mathematik ist eben genau das nicht: zufällig. Mathematik versucht in ihrem Kern eine Systematik zu sein. Und Maschinen sind mathematisch Manifestation. Echte Zufälle mathematisch zu generieren bedeutet also Höchstleistung für Maschinen. Die heutigen Technologien sind somit Anwendungen, die auf historischem, menschlich produzierten Wissen basieren und diese Erkenntnisse nur in digitaler, automatisierter Form nutzen – hoffentlich, um die Menschheit voran zu bringen und das Beste aus unserem bisherigen Wissen zu machen. Lernen nie verlernen. Entscheidend sind zwei Dinge: Erstens, dass neue Technologien stetig entwickelt werden und wir uns nicht nur technologisch, sondern auch gesellschaftlich weiterentwickeln. Zweitens, dass nicht darauf vergessen wird, dass die Wissensbasis aller neuen Technologien stets der Mensch erschaffen hat. Es besteht also eine gewisse Verantwortung, die Bedeutung von Bildung, Ausbildung und Wissensvermittlung generell, innerhalb der Gesellschaft als wichtiges Gut zu verankern. Es muss schlaue Köpfe geben, die neue Technologien entwickeln, das menschliche Wissen ausbauen und gleichzeitig dafür sorgen, dass innerhalb der Gesellschaft nie die Situation entsteht, dass das selbstständige Denken verloren geht und schlicht Maschinen überlassen wird. Wer selbst so wenig Wissen und Fähigkeiten hat, dass stets Google befragt werden muss, wird nicht in der Lage sein, sinnstiftenden Tätigkeiten nachzugehen. Deshalb ist es wichtig, bei allem Vorantreiben von Technologien nie außer Acht zu lassen, wie diese entstehen – und dass es menschliches Wissen braucht, um technologischen Fortschritt zu erschaffen. Auswirkungen auf Gesellschaft und Arbeitswelt. Immer, wenn Automatisierung und Industrialisierung vorangetrieben wurden, gab es große Sorgen um Arbeitsplätze. Das ist auch im Zuge der Digitalisierung ein Dauerthema – welche Menschen sind ersetzbar, welche Leistungen können zukünftig durch Softwarelösungen erledigt werden? Gesellschaften werden durch diese Entwicklungen gespalten. Auf der einen Seite bilden sich Gruppen, die Innovationen vorantreiben möchten, die den Fortschritt herbeisehen und es nicht erwarten können, neue Technologien anzuwenden. Auf der anderen Seite stehen Menschen, die schlichtweg in Sorge geraten, ihre Jobs könnten durch Programme und Maschinen ersetzt werden. Daher stellt sich die Frage, wie technologischer Fortschritt zu bewerten ist. Was genau ist wirklich Fortschritt? Ist es schon Fortschritt, wenn AI dazu führt, dass Maschinen Tätigkeiten präzisier, schneller und selbstständig ausführen können? Oder ist es gesellschaftlicher Schaden in Form von verlorenen Arbeitsplätzen? Innovationen werden zum gesellschaftspolitischen Spannungsfeld. Die Antworten von Seiten der Politik sind allerdings ähnlich konträr. Während wirtschaftsliberale Parteien die Ansicht vertreten, im Zuge der Globalisierung wäre jede Verlangsamung beim Vorantreiben von Innovationen eine volkswirtschaftliche Katastrophe, möchten linksgerichtete Politiker mit höheren Mindestlöhnen, Maschinensteuer und bedingungslosem Grundeinkommen für Ausgleich sorgen. Welche Auswirkungen es schlussendlich wirklich durch Technologien wie AI, selbstlernende Maschinen und neue Anwendungsoptionen von Big Data haben wird, ist ungewiss. Ebenso unklar ist, welche Veränderungen unsere Gesellschaft dadurch erleben wird und mit welchen politischen Maßnahmen es möglich sein wird, diesen Veränderungen entgegenzutreten, um sie gesellschaftlich verträglich zu gestalten. Fazit: Corona-Boom für KIs? Eine Gesellschaft mit hohem Bildungsniveau muss neue Technologien nicht fürchten, ganz im Gegenteil. Gleichzeitig sollte sie sich nicht auf sie verlassen und stets dafür sorgen, den Menschen nicht so viel Denkarbeit abnehmen, dass eine Gesellschaft entsteht, in der das Wissen der Menschen keinen Stellenwert mehr hat, weil ohnehin alles gegoogelt werden kann. Die Corona-Pandemie wird die Gesellschaft nicht plötzlich digitalisieren, da viele Technologien ohnehin bereits genutzt werden, ohne dass sie gewaltig im öffentlichen Bewusstsein wären. Es ist ein langsamer Prozess, der schlussendlich erfolgreich sein dürfte: Das langsame Probieren, Testen und Akzeptieren neuer Technologie als nützlich sowie deren Integration in den Alltag.;https://www.derstandard.at/story/2000118928373/gott-wuerfelt-nicht-ki-schon-wie-viel-zufall-steckt-in;Standard;Christian Allner
30.08.2018;AI-Start-up Ondewo erhält sechsstelliges Investment von Softwarefirma Catalysts;"Ondewo hat eine AI-Plattform für intelligente Chatbots und Sprachassistenten entwickelt. Der Linzer Softwarehersteller Catalysts hat sich unlängst mit fünf Prozent an Ondewo beteiligt und investiert somit eine sechsstellige Summe in das Wiener AI-Start-up. Ondewo wurde im Juni 2017 gegründet und hat eine ""Conversational AI-Plattform"" entwickelt, die es Maschinen ermöglicht, selbstständig ""natürliche Konversationen"" mit Menschen zu führen. Die Technologie der Plattform basiert auf Deep Learning und künstlicher Intelligenz. Wie der Gründer und Geschäftsführer von Ondewo, Andreas Rath, im Gespräch mit dem STANDARD betont, gehe man mit dem jüngsten Investment von Catalysts eine strategische Partnerschaft ein. Dadurch soll das Firmengeschäft in Österreich und die internationale Expansion von Ondewo vorangetrieben werden. Automatisierung des Kundenkontakts von Unternehmen. Ondewo bietet die ""Conversational AI-Plattform"" speziell für B2B-Kunden als On-Premises-Softwarelösung an, die intelligente Chatsbots und Sprachassistenten in ihre eigenen Kommunikationsplattformen integrieren möchten. Zu den Kunden zählen größere Unternehmen wie Telefongesellschaften, Callcenter-Betreiber und Versicherungen. Im Mittelpunkt stehe dabei meist der Customer-Support, wobei immer mehr Kunden Ideen für innovative Produkte wie zum Beispiel einen automatisierten Verkäufer oder Berater anfragen, so Rath.Wie er betont, seien immer mehr Dienstleistungsunternehmen bestrebt, in diesem Bereich ihre Kosteneffizienz durch Automatisierung zu steigern. Zudem liefere die Technologie den Vorteil, dass Kunden mit Unternehmen ""rund um die Uhr"" Kontakt aufnehmen können. Potenziale der AI-Technologie für ""kommunikative Maschinen"". Mittel- bis langfristig wolle man auch intelligente Sprachzentren für Maschinen und Roboter entwickeln, damit diese mit Menschen auf eine natürliche Art und Weise kommunizieren können. Rath sieht in diesem Technologie-Segment ein hohes Entwicklungspotenzial: ""In Zukunft werden Menschen sich beispielsweise mit ihrer Bohrmaschine unterhalten können, um sich über den richtigen Umgang und die Wartung des Werkzeugs zu informieren."" Zudem kann man laut Rath mit dieser Technologie im Bereich der Industrie menschliche Fehler reduzieren, um in der Folge kostspielige Reparaturen bei Industriemaschinen zu vermeiden. Das Gründerteam. Rath hat Ondewo vor rund einem Jahr gemeinsam mit Alexander Schult gegründet. Rath und Schult waren jahrelang Kollegen bei der Unternehmensberatung McKinsey und haben an Digitalisierungsstrategien für Unternehmen gearbeitet. Mit dem erworbenen Wissen bestand dann der Wunsch, sich selbstständig zu machen, so Rath. Unter anderem hat er neben seinem Doktorat in Informatik an der TU Graz fünf Jahre zu künstlicher Intelligenz am Grazer Know-Center geforscht – Österreichs führendem Forschungszentrum für Data-driven Business. Mit Umwegen zur Geschäftsidee. Die Gründungsidee von Ondewo konzentrierte sich zunächst auf die Entwicklung einer Plattform für die Vermittlung von Friseur- und Handwerker-Dienstleistungen. Um die Kommunikation zwischen Dienstleistern und Kunden zu automatisieren, entwickelte das Start-up einen automatisierten Chatbot. ""Wir sind sehr schnell draufgekommen, dass die Technologie, die wir verwendet haben, eigentlich viel spannender für größere Unternehmen ist"", so Rath. Mit dem Know-how hat man schlussendlich die ""Conversational AI-Plattform"" entwickelt, die Firmen in ihre eigenen Plattformen integrieren können. Big Player Google, IBM und Co als Mitbewerber. Am Zukunftsmarkt für automatisierte Kommunikation zwischen Maschinen und Menschen gibt es für Ondewo eine Vielzahl an Mitbewerbern. Dazu zählen Big Player wie IBM mit seiner Softwarelösung ""Watson"" oder Google mit ""Dialogflow"". Hinsichtlich der Konkurrenz betont Rath: ""Die großen Anbieter setzen auf die breiten Massen, wir hingegen wollen mit unserer 'Conversational AI-Plattform' eine Lösung für die komplexen und speziellen 'Use-Cases' von B2B-Kunden anbieten"". Dabei setze das Start-up auf höchste Erkennungsraten, eine hohe Anpassungsfähigkeit der Algorithmen und On-Premises-Lösungen, die Unternehmen zudem bei Datenschutzfragen unterstützen sollen, so Rath. Zukunftsmärkte mit komplexen Sprachen. Der Kernmarkt des Wiener Start-ups ist der österreichische Markt. Zudem beginne man laut Rath ab kommenden September mit ersten internationalen Projekten. So startet Ondewo beispielsweise ein großes Investitionsprojekt mit einem finnischen Telekommunikationsanbieter. Rath spricht auch über die Herausforderung, die eine AI-Software bei grammatikalisch komplexeren Sprachen meistern muss: ""Das Spannende an Finnland ist, dass die finnische Sprache unglaublich schwierig zu erlernen ist und damit auch die Computerlinguistik vor große Herausforderungen stellt."" Bei komplexeren Anwendungsfeldern könne man jedoch am meisten Know-how generieren, so Rath. Heimische Förderlandschaft hat Start-up geholfen. Die erste Finanzierung des Start-ups erfolgte im Juni 2017 über ein AWS-Pre-Seed-Programm. Zudem konnte Ondewo Förderungen der FFG an Land ziehen. Mit der österreichischen Förderlandschaft habe Ondewo weitgehend positive Erfahrungen gemacht. ""Die Förderstellen AWS, FFG und die WKO waren immer für Gespräche und konstruktives Feedback bereit"", so Rath. Recruiting und Skalierung als Herausforderungen. Auf die Frage, mit welchen Herausforderungen das Start-up konfrontiert sei, antwortet Rath: ""Insbesondere das Recruiting von geeignetem Personal ist sehr herausfordernd, da wir in diesem hochtechnologisierten AI-Bereich die besten Köpfe brauchen."" Das Start-up suche derzeit Experten in den Bereichen Data-Science, Machine-Learning und Natural-Language-Processing. Derzeit verfügt Ondewo über zwölf Mitarbeiter. Wie bei anderen Start-ups stelle laut Rath natürlich auch die Skalierung das junge Unternehmen vor große Herausforderungen. Durch die strategische Partnerschaft mit Catalysts erhoffe man sich in diesem Bereich einen Know-how-Transfer, um sich voll auf die technologische Weiterentwicklung der ""Conversational AI-Plattform"" konzentrieren zu können.";https://www.derstandard.at/story/2000086347343/ai-start-up-ondewo-erhaelt-sechsstelliges-investment-von-softwarefirma-catalysts;Standard;Martin Pacher
06.10.2020;Der Meteoritenrausch am Bielersee;"Die Schweiz schenkt Österreich ein Stück des Twannberg-Meteoriten. Durch gezielte Suchkampagnen konnte das Streufeld des eiszeitlichen Meteoritenfalles rekonstruiert werden. Es muss ein gewaltiges Schauspiel am Himmel gewesen sein, das sich vor zwischen 155.000 Jahren und 195.000 Jahren am Nordufer des heutigen Bielersees ereignete. Schade, dass es vermutlich niemand beobachten konnte: Zwar hätte der Feuerball, der beim Eindringen des Twannberg-Eisenmeteoriten in die Atmosphäre entstand, zufällig anwesende Vertreter von Homo neanderthalensis vermutlich tief beeindruckt. Doch der Fall ereignete sich wahrscheinlich während der zweitjüngsten Kaltzeit, dem Riß-Glazial, als die Eismassen der Alpengletscher sich weit über die Schweizer Grenzen erstreckten. Die Trümmer des explodierten Brocken verteilten sich über ein großes Gebiet im Schweizer Jura, wo heute im Kanton Bern nahe der Stadt Biel die Ortschaft Twann liegt. Hier wurden sie über die Jahrtausende umgelagert und versanken im Boden. Eines dieser Stücke ist nun im Naturhistorischen Museum in Wien (NHM) gelandet: die Schweiz stiftete Österreich ein Exemplar. Dieses wurde am vergangenen Donnerstag vom Präsidenten des Schweizer Ständerates Hans Stöckli an das NHM übergeben. Der Neuerwerbung geht eine mehrere Jahrzehnte lange Geschichte der Forschung und ein regelrechter Meteoritenrausch voraus. Erster Fund 1984. Im Mai 1984 fand die Bäuerin Margrit Christen nach dem Pflügen eines Haferfeldes beim Auflesen von störenden Gesteinsbrocken einen auffällig schweren Stein mit einer verdächtig rostigen Kruste. Das Stück entpuppte sich als Meteorit – mit 15,9 Kilogramm der schwerste der mittlerweile 11 bekannten Schweizer Meteorite (zum Vergleich: Österreich brachte es bisher nur auf sieben verschiedene Meteorite). Die Geschichte des Twannberg-Meteoriten könnte hier enden, wäre nicht im September 2000 der Blick des Antikmöbelhändlers Marc Jost auf einen merkwürdigen Metallklumpen auf einem Dachboden eines altes Hauses in Twann gefallen, der dort in einer Wand eingemauert war. Jost hatte zuvor einen Meteoriten im Museum gesehen und schöpfte Verdacht. Er nahm Kontakt mit dem Naturhistorischen Museum in Bern auf, der dortige Experte Beda A. Hofmann bestätigte seine Vermutung. Meteoritenjäger. Das Zusammentreffen markierte den Beginn einer fruchtbaren Zusammenarbeit: Für Jost war der Fund der Auslöser, auf Meteoritenjagd zu gehen, auf Hofmann kam in den folgenden Jahren jede Menge Arbeit mit dem Twannberg-Meteoriten zu. Dass das zweite Stück vom selben Fall wie der 1984 gefundene Brocken stammte, war naheliegend und bestätigte sich bei Untersuchungen – und wo zwei Stücke gefunden wurden, könnte noch mehr sein. Ein drittes Exemplar tauchte im Jahr 2005 auf, und zwar ebenfalls nicht an der Stelle wo es vom Himmel gefallen war: ausgerechnet in der Gesteinssammlung des Naturhistorischen Museums in Bern. Hier wurden Dauerleihgaben aus dem Bieler Museum Schwab aufgearbeitet. Darunter fand sich auch ein mit einer rostigen Patina überzogenes verdächtig nach Meteorit aussehendes Stück mit einem alten Sammlungskärtchen mit der Aufschrift ""Eisenglanz"", also Hämatit. Das Etikett stammte von Anfang der 1930er Jahre, mindestens seit damals muss also das fehlbestimmte Exemplar im Bieler Museum gelegen sein. Meteoritenrausch. Im Jahr 2007 fanden zwei Goldwäscher zwar nicht das, wonach sie suchten, aber sie holten drei kleinere Meteoritenstücke aus dem Twannbach. Damit war endgültig klar, dass es sich bei Twannberg um ein Meteoritenstreufeld handelt – das einzige in der Schweiz. Doch wie sollte man gezielt nach weiteren Stücken suchen, wenn fünf der sechs bekannten Exemplare nicht an ihrem ursprünglichen Einschlagsort gefunden wurden, sondern von Menschen oder dem Bach transportiert worden waren? In den folgenden Jahren ging es jedoch Schlag auf Schlag: aus dem Twannbach wurden dutzende Exemplare geborgen, und auch im Gebiet des Fundes von 1984, das damals schon abgesucht wurde, tauchten weitere Stücke auf – nicht zuletzt dank des Einsatzes von Jost, den sein Dachbodenfund nicht ruhen ließ. Streufeld rekonstruiert. Hofmann wiederum organisierte im Rahmen eines Citizen-Science-Projektes gezielte Suchkampagnen, mit denen das Fundgebiet ausgeweitet werden konnte und das ursprüngliche Streufeld Gestalt annahm. Mehr als 1700 einzelne Stücke mit zusammen mehr als 140 Kilogramm konnten insgesamt geborgen werden, jedes Exemplar wird penibel erfasst und wissenschaftlich untersucht. Im Zuge der Suche wurde auf dem Mont Sujet sogar ein Steinmeteorit gefunden. Der Meteorit im NHM trägt die Nummer 441, bringt stolze 370,1 Gramm auf die Waage und hat die typische Patina der Oxidationsrinde. Gefunden wurde er im Jahr 2016. Ludovic Ferrière, der Kurator der Meteoritensammlung des NHM, plante seit längerem, ein Twannberg-Exemplar für die Sammlung in Wien zu erwerben. Im September 2016 versuchte er – erfolglos – sogar selbst sein Glück im Fundgebiet. Mit Hilfe des Schweizer Botschafters bei den Internationalen Organisationen in Wien, Wolfgang Amadeus Brülhart, konnte schließlich die Schenkung eingefädelt werden. In Studien wurde berechnet, dass der Twannberg-Meteorit eine Masse von 33.000 Tonnen und einen Durchmesser von mindestens vier, vielleicht bis zu zehn Metern gehabt haben muss, bevor er in die Erdatmosphäre eindrang. Ein großer Teil davon ist auf dem Weg zum Boden verglüht, dennoch muss einiges darauf warten, gefunden zu werden. Kerne vergangener Himmelskörper. Eisenmeteorite stammen aus den Kernen längst vergangener Asteroiden oder Kleinplaneten, die in ihrer Geschichte eine Differenzierung in eine Kruste, einen Mantel und einen metallischen Kern durchlaufen haben. Sie machen nur einen kleinen Anteil aller bekannten Meteorite aus: Die Meteoritical Society listet mittlerweile mehr als 64.000 anerkannte Meteorite auf. Doch nur 1255 davon sind Eisenmeteorite – nicht einmal zwei Prozent. Diese stammen vermutlich von mehreren Dutzend verschiedener Ursprungskörper. Seltenheit. Twannberg gehört innerhalb der Gruppe der Eisenmeteorite zu einer der seltensten Klassen: nur ganze sechs Stück sind in der Klassifizierung IIG eingeordnet, Twannberg ist der einzige Fund in Europa. Er zeichnet sich in seiner Zusammensetzung durch einen mit 4,5 Prozent sehr geringen Nickelanteil aus, enthält dafür umso mehr Phosphor, der vor allem in großen Kristallen des Minerals Schreibersit enthalten ist. Das seltene Eisenphosphid Schreibersit kommt fast ausschließlich in Meteoriten vor und wurde nach dem österreichischen Naturwissenschafter Karl Franz Anton von Schreibers benannt. Die Typlokalität des Minerals liegt übrigens in Wien und zwar in Form des Meteoriten Magura in der Sammlung des NHM. Meteoriteneisen ist auf der Erde dennoch häufiger, als es der geringe Anteil an der Gesamtzahl erscheinen lässt, denn bei der Masse sind die Eisenmeteorite ganze vorne mit dabei: angeführt vom 60 Tonnen schweren Hoba belegen sie die ersten 17 Plätze der größten Eindringlinge aus dem All, in den Top 100 stellen sie 73 Vertreter. Nächstes Ziel Santa Filomena. Weil die Erde einem ständigen Bombardement aus dem All ausgesetzt ist, kann Ferrière auch nach der Sammlungserweiterung mit dem Twannberg-Exemplar nicht ruhen. Speziell auf ein Exemplar des jüngsten Meteoritenfalls hat er ein Auge geworfen: Am 19. August schlugen bei Santa Filomena im brasilianischen Bundesstaat Pernambuco zahlreiche Fragmente eines mehr als fünfzig Kilogramm schweren Steinmeteoriten ein. Einzelne Stücke durchschlugen Hausdächer, ein anderes traf einen Baumstamm. Mehrere Menschen berichteten, nur um wenige Meter verfehlt worden zu sein. An einen Erwerb eines Exemplars aus Santa Filomena für die Sammlung des NHM ist ohne Sponsoren jedoch nicht zu denken. Vielleicht war es aber ein glücklicher Zufall, dass bei der Übergabezeremonie des Twannberg-Meteoriten auch der brasilianische Botschafter Jose Antonio Marcondes de Carvalho anwesend war?";https://www.derstandard.at/story/2000120430601/der-meteoritenrausch-am-bielersee;Standard;Michael Vosatka
09.09.2020;Wie man sich anderswo für den Corona-Herbst wappnet;"Nur kurz währte im Sommer das Aufatmen: In Europa und anderen Teilen der Welt droht die Corona-Pandemie nun mit voller Wucht zurückzukehren. Vor allem im Süden Europas wächst die Nervosität wieder. In jenen Ländern, die im Frühling besonders von der Corona-Pandemie betroffen waren und in denen die Regierungen mit der größten Härte an deren Eindämmung gearbeitet haben, droht die Situation abermals außer Kontrolle zu geraten. Doch nicht nur dort. DER STANDARD hat sich umgesehen: Frankreich. Weil seit 24. August die kritische Schwelle von 50 Neuinfektionen pro 100.000 Einwohnern überschritten ist, hat die Hauptstadt Paris eine Maskenpflicht eingeführt — und zwar nicht wie bisher nur in geschlossenen Räumen wie Supermärkten, in der Métro oder auf Ämtern, sondern überall, also auch im Freien. Dies gilt auch im Süden des Landes, in Marseille etwa, wo zudem ein Alkoholverbot nach 23 Uhr verhängt wurde. Allzu oft, so die Behörden, trübe nämlich ausgelassene Partystimmung den Blick auf das Wesentliche: Abstand und Handhygiene. Die Masken könnten aber auch in anderen Landesteilen schnell zurückkehren: In der Nacht auf Montag befand der Staatsrat, das oberste Verwaltungsgericht Frankreichs, nach einem Rechtsstreit den verpflichtenden Mund-Nasen-Schutz auch in den Großstädten Lyon und Straßburg für rechtmäßig. Am Montag wurden schließlich insgesamt sieben Départements zu neuen ""roten Zonen"" erklärt, darunter die Großstädte Lille, Rouen, Le Havre und Straßburg, wo das EU-Parlament tagt. Die nächsten dort geplanten Sitzungen wurden am Dienstagabend nach Brüssel verlegt. Indien. Zu Beginn der Woche ist Indien auf einer traurigen Rangliste einen Platz nach oben gewandert: Mit 4,2 Millionen registrierten Infektionen steht das Land nun auf Platz zwei weltweit und somit vor Brasilien – nur die USA haben in absoluten Zahlen noch mehr Corona-Fälle. Nirgendwo anders steigen die erfassten Neuinfektionen seit Wochen so schnell wie in Indien, zuletzt kamen rund 90.000 Fälle pro Tag dazu. Auch gibt es – anders als in den USA oder Brasilien – keinen Hinweis auf ein Abflachen der Kurve. Besondere Sorgen bereitet den Behörden, dass das Virus inzwischen auch auf dem Land angekommen ist, wo die Gesundheitsversorgung besonders schlecht ist. Neben dem alarmierenden Infektionsgeschehen bereitet der Regierung von Narendra Modi auch die katastrophale Wirtschaftsentwicklung Kopfzerbrechen: Zwischen April und Juni, als ein strenger Lockdown galt, sank das Bruttoinlandsprodukt (BIP) um 23,9 Prozent und damit stärker denn je seit der Unabhängigkeit von Großbritannien 1947. Millionen Indern droht damit der Rückfall in die Armut. Israel. Auch in Israel, das zu Beginn der Epidemie als Musterschüler galt, steigen die Fallzahlen aktuell dramatisch. Fast zehn Prozent aller Corona-Tests fallen positiv aus, seit Tagen kommen 2.000 bis 3.000 Neuinfektionen dazu, am Sonntag überschritt das kleine Land am Mittelmeer die Zahl von 1000 Menschen, die an oder mit Covid-19 gestorben sind. Ministerpräsident Benjamin Netanjahu, anfangs von Bundeskanzler Sebastian Kurz als Vorbild in der Bekämpfung von Corona genannt, räumte inzwischen zu schnelle Lockerungen ein. Die Hotspots aktuell: jüdisch-ultraorthodoxe und arabische Gemeinden und Ortschaften. Seit Montag gelten in 40 Städten und Dörfern wieder strengere Regeln, der anfangs vom Corona-Beauftragten Ronni Gamzu — ""ganz Israel ist im Krieg"" — geplante Lockdown fiel nach Intervention Ultraorthodoxer aber weniger strikt aus. Von 19 Uhr bis fünf Uhr herrscht in den betroffenen Orten ab sofort eine Ausgangssperre. Ob das jüdische Neujahrsfest Rosch ha-Schana Mitte September so beschwingt wie üblich gefeiert werden kann, weiß bislang aber nur Gott. Spanien. Zwischen Barcelona und Cádiz scheint das Licht am Ende des Tunnels ferner denn je. Vergangene Woche stellte Madrids Regionalpräsidentin Isabel Díaz Ayuso eine düstere Prognose: ""Die Menschen infizieren sich gerade, die Kinder infizieren sich gerade, alle infizieren sich gerade."" Abermals kristallisiert sich just die Hauptstadt als Spaniens Corona-Hotspot heraus. Fast 500 Infizierte kommen dort auf 100.000 Einwohner; nirgendwo in Europa verbreitet sich das Virus so rasch. Weil, anders als im März, nun vor allem Junge betroffen sind, sind die Krankenhäuser aber – noch – nicht so überlastet wie damals. Ministerpräsident Pedro Sánchez, der sich im Sommer noch für die Rückkehr der Touristen starkgemacht hat, tritt derweil auf die Bremse: Familienfeiern und Partys trügen den Löwenanteil zu der neuerlichen Ausbreitung bei. Die Madrilenen, so die Regionalregierung, sollen sich in Hinkunft höchstens mit neun anderen Freunden oder Verwandten treffen. Tschechien. Alarmstimmung auch im Norden: Wegen neuer Rekordwerte bei der Zahl der Neuinfektionen bezeichnete die Weltgesundheitsorganisation am Dienstag die Situation in Tschechien als ""besorgniserregend"". Ministerpräsident Andrej Babiš hatte zuvor erklärt, dass seine heillos unterbesetzten Behörden die Infektionsketten schon bald nur noch bei ernsten Krankheitsverläufen zurückverfolgen könnten. Und doch ist man nicht untätig: Ab heute, Mittwoch, muss in Prag in Geschäften wieder Maske getragen werden, Restaurants und Bars müssen nachts schließen.";https://www.derstandard.at/story/2000119865150/wie-man-sich-anderswo-fuer-den-corona-herbst-wappnet;Standard;Florian Niederndorfer
25.10.2018;Frauen in der IT: Große Chance statt schiefes Image;"Grazer Unis starten Initiative für mehr Informatikerinnen. Ada Lovelace, Hedy Lamarr, Grace Hopper: Wer nach weithin bekannten Informatikerinnen sucht, wird hauptsächlich in den Pionierzeiten der Computerwissenschaften fündig. Natürlich gibt es auch heute bahnbrechende Frauen in der Informatik, bloß muss man sie mit der Lupe suchen. Denn trotz zahlreicher erfolgreicher Initiativen, Mädchen und Frauen für die IT zu begeistern, bleiben die Resultate überschaubar. Der Anteil an Frauen in Informatikstudien dümpelt bei etwa 15 bis 20 Prozent dahin, obwohl Unternehmen händeringend nach Programmierern und anderen IT-Fachkräften suchen. ""Es ist davon auszugehen, dass schon das Wording des Studiengangs entscheidend ist, um die Attraktivität zu steigern"", hat Isabel Roessler, Leiterin des Forschungsprojektes ""Fruit: Frauen in IT"" am deutschen Centrum für Hochschulentwicklung, herausgefunden. Das kann auch Roderick Bloem, Dekan der Fakultät für Informatik und Biomedizinische Technik der TU Graz, bestätigen: In der Fachrichtung ""Information und Computer Engineering"" liege der Frauenanteil bei zehn Prozent, im Fach ""Softwareentwicklung und Wirtschaft"" bei 20 Prozent. Know-how-Austausch. Um nicht nur etwas am Wording zu ändern, sondern verschiedene Disziplinen tatsächlich zusammenzubringen, starten TU Graz und Universität Graz mit einer Kick-off-Veranstaltung am 25. 10. die Kooperation Route 63: Dabei sollen sich Informatikstudierende Kenntnisse in Betriebswirtschaft, Soziologie und Psychologie aneignen können und umgekehrt Studierende der Uni Graz Fähigkeiten zur Programmierung einfacher Web-Anwendungen und Apps, und sie sollen ein Verständnis der Methoden und Möglichkeiten von Data-Science erwerben. Zusätzlich startet ab 6. 11. die Vortragsreihe CS Talks, zu der ausschließlich Computerwissenschafterinnen eingeladen werden. ""Wir wollen hervorstreichen, dass Informatik nicht dem schiefen Image von einsamen Programmierern entspricht, sondern Teamarbeit ist, gesellschaftlich relevante Themen behandelt und für Frauen viel mehr Möglichkeiten bieten kann, als vielen bewusst ist"", sagt Bloem. In interdisziplinären Teams wie etwa im Kompetenzzentrum Know Center der TU Graz sei der Frauenanteil vergleichsweise hoch. Hartnäckige Klischees. Dass Österreich zu den Ländern mit den geringsten Anteilen an Technikerinnen in Europa zählt, führt die Österreichische Computer Gesellschaft denn auch auf hartnäckige Klischees zurück sowie auf Einflüsse des Elternhauses, fehlende Rollenbilder, die erschwerte Situation von Frauen in männlich geprägten Berufen sowie die Rolle von Schule und Lehrenden. Initiativen, all die Vorurteile zu entkräften, gibt es nach wie vor viele: So wurde mit dem Hedy-Lamarr-Preis für innovative Informatikerinnen, der kürzlich zum ersten Mal vergeben wurde, eine Auszeichnung geschaffen, die vielleicht den Geist der Pionierinnen ein Stück weit in die Gegenwart holt.";https://www.derstandard.at/story/2000090060489/frauen-in-der-it-grosse-chance-statt-schiefes-image;Standard;Karin Krichmayr
21.05.2020;Neue Erkenntnisse über Superspreader-Ereignisse;"Forscher vermuten, dass bis zu 80 Prozent der Covid-19-Ansteckungen auf nur zehn Prozent der Infizierten zurückgehen könnten. Die jüngste Häufung von Covid-19-Fällen in Österreich gibt es in Wien und Umgebung: Infektionen in zwei Postverteilerzentren in Inzersdorf und Hagenbrunn hängen eng mit jenen in einer Logistikzentrale eines Möbelhauses in Floridsdorf, mit erkrankten Kindern und Betreuern in einem Kindergarten in Liesing und infizierten Personen in Flüchtlingsunterkünften in Wien zusammen. Dieser Cluster ist der jüngste von bisher insgesamt 268 solcher Häufungsfälle, die von der Agentur für Gesundheit und Ernährungssicherheit (Ages) aufgearbeitet wurden (Stand 19. Mai). Von insgesamt 16.266 nachgewiesenen Covid-19-Erkrankungen lassen sich damit 4.672 einem dieser Cluster zuordnen. Das ist mittlerweile deutlich mehr als ein Viertel aller nachgewiesenen Infektionen. Die drei Cluster aus dem Bereich Freizeit fallen dabei besonders auf: Sie decken nicht weniger als 1.093 Infektionen ab. Einer dieser Cluster bezieht sich auf die Infektionskette, die vom Wintersportort Ischgl und den dortigen Après-Ski-Aktivitäten ihren Ausgang genommen hat, einem zweiten liegt das Après-Ski in St. Anton am Arlberg zugrunde, wie Daniela Schmid bestätigt, Leiterin der Abteilung Surveillance und Infektionsepidemiologie der Ages. Superspreader und ihre Settings. Bei solchen Clusteranalysen fällt auf, dass es am Beginn oft einzelne Superspreader gab – oder eher: Superspreader-Ereignisse. So ist davon auszugehen, dass in Ischgl nicht eine einzelne Person die Virenschleuder war, die alle anderen ansteckte, sondern dass es in den Après-Ski-Bars mehrere Infizierte gleichzeitig gab. Das wäre auch deshalb naheliegend, weil im Extremfall neue Virusträger bereits nach ein bis zwei Tagen selbst Viren abgeben können. Ein internationales, seit kurzem gut untersuchtes Superspreader-Ereignis ist eine Chorprobe im US-Bundesstaat Washington, bei der eine Person 52 weitere Sänger (von insgesamt 60) bei einer Probe ansteckte, zwei Chormitglieder starben. Ein anderes betrifft Unterkünfte von Arbeitsmigranten in Singapur mit fast 800 Fällen. Und zuletzt infizierte in Südkorea ein Mann bei seiner Nachtclub-Tour Anfang Mai etliche andere Menschen. Bis jetzt wissen die Gesundheitsbehörden von 170 Fällen, die mit dem Mann mutmaßlich im Zusammenhang stehen – bei tausenden potenziellen Kontakten. Wenn einige viele anstecken. Aber wie kommt es, dass einzelne Menschen besonders viele andere anstecken können? Und was bedeutet das für das Covid-19-Pandemiegeschehen sowie für dessen Eindämmung? Diesen Fragen geht ein neuer Bericht nach, den das Wissenschaftsmagazin ""Science"" in seinem Online-Nachrichtenteil veröffentlichte. In den bisherigen Modellierungen der Pandemie stand meist der Reproduktionsfaktor (R) als entscheidende Maßzahl im Zentrum. R gibt an, wie viele Personen eine weitere infizierte Person im Schnitt ansteckt. Ohne Maßnahmen der sozialen Distanzierung beträgt dieser Wert bei Sars-CoV-2 in etwa 3. Doch tatsächlich stecken die meisten Infizierten keine einzige andere Person an, wie Jamie Lloyd-Smith (University of California, Los Angeles) gegenüber ""Science"" erklärt. Die neue Maßzahl k. Deshalb wird zur epidemiologischen Modellierung zusätzlich zu R eine zweite wichtige Maßzahl herangezogen: der weniger bekannte Dispersionsfaktor (k), der beschreibt, wie stark sich eine Krankheit häuft. Je niedriger der Wert k ist, desto mehr Ansteckungen gehen auf eine kleine Anzahl von infizierten Personen zurück – wie etwa bei Sars im Jahr 2003 mit einem Wert von k = 0,16. Bei der Spanischen Grippe lag dieser Wert etwa bei 1, was bedeutet, dass Superspreader und Cluster keine große Rolle spielten. Doch wie hoch ist dieser Wert bei Sars-CoV-2? Darüber sind sich die Experten uneins: Gabriel Leung, ein einflussreicher Epidemiologe aus Hongkong, geht davon aus, dass dieser Wert etwas höher ist als bei Sars, Cluster also nicht ganz so wichtig sind. Ein Team um Adam Kucharski von der London School of Hygiene & Tropical Medicine argumentiert hingegen in einem Preprint (also einem Fachartikel, der von Kollegen noch nicht fachbegutachtet wurde), dass k für Covid-19 nur 0,1 betragen könnte. Laut den Berechnungen der Forscher dürften nur zehn Prozent der Infizierten für 80 Prozent der Ausbreitung verantwortlich sein.";https://www.derstandard.at/story/2000117603498/neue-erkenntnisse-ueber-superspreader-ereignisse;Standard;Klaus Taschwer
26.03.2018;Wirtschafts-Uni Wien: Das Paradies der Unentschlossenen;"Wirtschaft studieren – oder lieber gleich in die Wirtschaft gehen? Wenn schon als Studium, dann eher BW, IBW, VW, Sozök oder doch WiRe? Die Entscheidung fällt zwar schwer – wer aber sagt, dass man sie gleich fällen muss? ""Lieber Gott (falls es dich gibt) beziehungsweise Liebe Göttin (falls du doch eine Frau bist), habe Erbarmen, denn sie wissen nicht, was sie tun."" Wenn ich der WU-Rektorin Edeltraud Hanappi-Egger einen unkonventionellen Ratschlag erteilen dürfte, wie sie trotz lästiger Vielleicht-doch-nur-vorübergehend-Studierender nicht in Verzweiflung gerät, dann wäre es dieses kleine Nachtgebet. Obwohl scheinbar ""alle"" Wirtschaft studieren, muss man es denjenigen, die es tatsächlich tun, immerhin lassen, dass sie sich dabei viele Wege offen halten. Erste Haltestelle. Mit teils erleichtertem, teils selbstironischem Lächeln, blicke ich auf meine bisherige Studienzeit zurück. Auf die Höhen, die Tiefen und das Mittelgebirge meines studentischen Daseins. Auf die erste bestandene, die erste nichtbestandene Prüfung. Auf periodische Glücksgefühle in Zeiten der Monotonie, verstreute Ausgelassenheit in Zeiten der Autonomie. Auf eine Freiheit, die vielleicht gar keine ist – vielmehr ein Kredit, den ich jetzt noch sorglos auskoste, der mich später allerdings noch so einiges mehr kosten könnte. Doch da spricht die Ökonomin in mir, nicht die Studentin; diese entgegnet unbeschwert: ""Koste es, was es wolle!"" Besonders einprägsam ist mir noch der Tag der Inskription in Erinnerung. Der Tag der Entscheidung, wie man meinen möchte. Ich erinnere mich daran, wie bedacht ich war, alles richtig zu machen. Mit meinen fein sortierten Dokumenten stand ich am Schalter und wartete gespannt auf die Einführung. Die Schlange schritt schnell voran, und noch bevor ich realisierte, an der Reihe gewesen zu sein, war ich auch schon wieder draußen. Die Frage, die mich am meisten drängte, blieb offen. Bin ich hier richtig? Mit der Zeit sollte ich entdecken, dass ich mit einem Studium an der WU eigentlich nicht sehr viel falsch machen kann. Umsteigen, bitte! Auf der WU kann ich wagen, wofür ich sonst mit einem Führerscheinentzug rechnen müsste. Ich kann ohne zu blinken von einer Spur auf die nächste wechseln, auf mehreren Spuren gleichzeitig fahren, auf der Mitte der Strecke reversieren und an Kreuzungen statt haltzumachen einfach weiterdüsen. Sozial- und Wirtschaftswissenschaften heißt der großzügige Überbegriff, der es erlaubt, die verschiedenen Zweige an der WU bis an ihre Grenzen zu erforschen. Bis an den Punkt, an dem man feststellt, dass die Welt doch keine Scheibe ist und der Kreislauf der Dinge einen ungewollt an seinen Ausgangspunkt zurückführen kann. Wer denkt, dass man Wirtschaft studiert, weil man wissen möchte, wie die Welt funktioniert, der irrt und hat gleichzeitig auch recht. Genau auf solche Widersprüche muss man sich einlassen, wenn man Wirtschaft studiert. Aus meiner Sicht ist ein Wirtschaftsstudium ein Eingeständnis an die eigene Ignoranz. Man weiß nicht, was man möchte. Man weiß nicht, was die Zukunft bringt. Man möchte nur möglichst unbeschadet irgendwo landen, wo man sich dann hoffentlich keine philosophischen Sinnfragen mehr stellen muss. Dabei gibt es nirgendwo sonst so viel zu hinterfragen wie in der Wirtschaft. Wem kann man es dann verübeln, nicht in der Wirtschaft zu bleiben – vor allem dann, wenn man fast überall mit offenen Armen empfangen wird, sobald man aus der Wirtschaft kommt? Endstation. Vielleicht ist es im vierten Semester noch etwas voreilig, um Schlüsse zu ziehen, doch viel eher als endgültige Aussagen stelle ich hier wohl befristete Vermutungen auf. Wer weiß, wie sehr sich die Welt, die Wirtschaft und die Weltwirtschaft noch verändern werden? Wer weiß, wie sehr ich mich noch verändern werde? Man mag sich fragen, was aus mir wird. In der Zwischenzeit bin ich im Wesentlichen eines: unentschlossen. Zum Glück, bin ich glücklich damit, denn Unwissenheit nährt die Neugier. Der Entschluss kommt zum Schluss.";https://www.derstandard.at/story/2000076471327/wirtschafts-uni-wien-das-paradies-der-unentschlossenen;Standard;Anna-Maria Apata
19.09.2020;Wie konnte das passieren?;"Ist es die zweite Corona-Welle oder nur eine neue Phase? Regierung und Experten sind sich uneinig. Fakt ist, dass die Infektionszahlen in Österreich rascher steigen als in anderen Ländern. Der Versuch einer Aufarbeitung. Es ist noch nicht so lange her, da galt Österreich als Corona-Musterland. Dank ganz besonders niedriger Infektionszahlen konnte Österreich kurz nach Ostern mit den ersten Lockerungen des Lockdowns beginnen, und bis zum Frühsommer kehrte das Leben zu einer gewissen Normalität zurück. Die Zahlen blieben niedrig, die Probleme waren auf ein paar Cluster beschränkt, die rasch entdeckt und eingedämmt werden konnten. Gingen anderswo die Covid-19-Zahlen in die Höhe, erließ das Außenministerium Reisewarnungen und -beschränkungen. Österreich ist nicht das einzige Land, das diese Erfahrung gemacht hat. Fast überall in Europa sind die Zahlen seit dem Sommer deutlich gestiegen. Aber stand Österreich Ende Juni noch besser da als Deutschland, hat sich das nun umgedreht. Abgesehen von Israel und Tschechien hat kaum ein anderes Land in den vergangenen Wochen im Kampf gegen die Pandemie so viel an Boden verloren wie Österreich. Ganz überraschend war diese Entwicklung nicht. Die Reproduktionszahl, die besagt, wie viele Menschen ein Corona-Infizierter im Durchschnitt ansteckt, lag seit Mitte Mai fast immer über eins, was eine Zunahme erwarten lässt. Peter Klimek, Forscher am Complexity Science Hub, macht dafür vor allem Reisende verantwortlich, die aus Ländern mit einem stärkeren Infektionsgeschehen zurückkehrten, allen voran aus Kroatien. Dazu kamen die Folgen der Lockerungen – offene Gastronomie, kulturelle Veranstaltungen, Sportevents und private Partys, wo mehr Menschen wieder zusammentrafen – und eine nachlassende Disziplin bei der Einhaltung von Corona-Grundregeln: Abstand, Handhygiene und Mund-Nasen-Schutz. ""Wir haben uns von den niedrigen Fallzahlen zu sehr einlullen lassen"", sagt Klimek. Dass es auch anders geht, zeigt das Nachbarland Italien, das anfangs besonders hart getroffen war. Die Zahl der Neuinfektionen liegt in Relation zur Bevölkerung derzeit um zwei Drittel unter dem österreichischen Niveau. Italienurlauber berichten übereinstimmend über die beeindruckende Disziplin der Italiener, die sonst nicht für diese Tugend bekannt sind. Auch die Bundesregierung hat trotz der Appelle an die Selbstverantwortung zur Sorglosigkeit beigetragen, sagt der Klagenfurter Intensivmediziner Rudolf Likar, Co-Autor des Corona-Buches Bereit für das nächste Mal. ""Zuerst war die Botschaft Angstmache, und dann hieß es plötzlich, im nächsten Sommer wird alles normal. Das war zu optimistisch, denn wir haben keine zielgerichtete Therapie, und die Impfstoffe sind noch lange nicht getestet."" Der entscheidende Fehler aber war ein anderer, ist Klimek überzeugt: Die Regierung habe die ruhigen Sommermonate nicht dazu genutzt, ausreichend Kapazitäten fürs Testen und die Kontaktnachverfolgung, das Contact-Tracing, aufzubauen. ""Als es dann zum Ende des Sommers durch die Reiserückkehrer zum leichten Anstieg kam, waren diese Einrichtungen bald zu sehr in Anspruch genommen"", sagt er. ""Und wenn man mit Testen und Contact-Tracing nicht mehr nachkommt, dann breitet sich das Virus weiter aus."" Es fehlt an Personal. Dieses Versäumnis ist dieser Tage unübersehbar. Bei den Teststationen gibt es stundenlange Wartezeiten, die Ergebnisse liegen erst Tage später vor, weshalb auch die Nachverfolgung von Kontakten zu spät einsetzt. Quarantänebescheide treffen oft erst dann ein, wenn der Infizierte gar nicht mehr ansteckend ist. Das alles hätte man mit stärkeren und vor allem früheren Anstrengungen vermeiden können. Laut Empfehlung der Weltgesundheitsorganisation WHO braucht ein Land wie Österreich 1.300 Tracer, und allein die Stadt Wien 700. Stattdessen waren es in der Hauptstadt bisher nur 100 im Büro für Sofortmaßnahmen, 250 weitere in den Gesundheitsbehörden. Erst jetzt wird das Personal aufgestockt. Das ist möglicherweise noch nicht zu spät, glaubt Klimek. Noch ist die tägliche Steigerungsrate der Infektionen zu niedrig, als dass man von einem exponentiellen Wachstum sprechen könne, und das Infektionsgeschehen scheint sich noch auf begrenzte Cluster zu konzentrieren. Aber das könne sich rasch ändern. ""Wir sind gerade an einem Wendepunkt"", sagt Klimek. ""Die Prognosen sind sehr unsicher. Das pessimistische Szenario ist, dass wir aus einer Cluster-basierten Ausbreitung in eine Entwicklung kommen, in der sich das Virus weit in der Gesellschaft verbreitet."" Dann würde die Zahl der Neuinfektionen über 1.000 am Tag steigen und auch wieder viele ältere Menschen betroffen sein. ""Wenn wir das weiterlaufen lassen, dann kommen wir in einen Bereich, wo es auch Folgen für das Gesundheitssystem hat"", warnt Klimek. Noch schlimmer wären die wirtschaftlichen Folgen, wenn Österreich von immer mehr Reisewarnungen betroffen wäre. Appell an Eigenverantwortung. Zum jetzigen Zeitpunkt lasse sich jedenfalls noch ein Lockdown durch weichere Maßnahmen, wie sie zuletzt von der Regierung angeordnet wurden, verhindern, ist Klimek überzeugt. Man könne und sollte auch nicht alle Sozialkontakte verhindern, müsse aber noch stärker an die Eigenverantwortung appellieren. ""Vergleichsstudien haben gezeigt, dass Länder, die sehr scharfe Maßnahmen gesetzt haben, nicht viel effektiver waren als andere."" Es gebe auch nicht eine einzelne Maßnahme, die einen ausreichenden Schutz biete; wirkungsvoll sei immer nur ein Maßnahmenmix, das aus Maskentragen, Contact-Tracing, eine begrenzte Zahl von Teilnehmern bei Veranstaltungen und auch Beschränkungen von Reisen in Länder mit einem stärkeren Infektionsgeschehen. Der Kärntner Mediziner Likar plädiert auch wieder für eine stärkere regionale Differenzierung, so wie es die Corona-Ampel eigentlich erreichen sollte. Es sei ein Fehler, dass sich die Regierung kurz nach Einschalten der Ampel wieder für gleichförmige bundesweiten Maßnahmen entschieden habe. ""Man kann nicht sagen, wenn es in Wien brennt, dann muss ich in Kärnten löschen. Warum nehmen wir die ganze Bevölkerung in Geiselhaft, wenn wir ein Ampelsystem haben? So kann man die Menschen nicht motivieren, sich von sich aus anders zu verhalten."" Aber die Ampel gilt kurz nach ihrer Einführung für die meisten Beobachter als gescheitert – und als einer der Gründe, warum Österreichs Corona-Politik im Augenblick so schlechte Noten erhält. Fast alle Staaten setzten im März auf strenge Maßnahmen, um die Ausbreitung des Virus einzudämmen. Die Infektionszahlen sanken. Die Regelungen wurden im Mai wieder gelockert, doch im Verlauf der Sommermonate breitete sich das Virus vor allem in Spanien, Frankreich und Österreich erneut stärker aus. Die Ampel. Wie die Politik ein sinnvolles Instrument zerstört. Noch vor wenigen Wochen wurden große Hoffnungen in die Ampel gesteckt. ""Ein einfaches vierstufiges Schema von Rot über Orange und Gelb bis Grün soll auf wissenschaftlicher Basis und wissenschaftlichen Kriterien automatisiert die aktuelle Corona-Lage sichtbar machen und auch definieren, wann Zusatzmaßnahmen erforderlich sind und wann Maßnahmen gelockert werden können"", beschrieb Gesundheitsminister Rudolf Anschober Anfang Juli das Konzept. Zuvor hatte er sich lange gegen das System gewehrt und die Ampel, die bereits im Complexity Science Lab im Betrieb war, ignoriert. Doch bereits bei der ersten öffentlichen Schaltung am 4. September zeigte sich, dass die Ampel so nicht funktionierte. Damals wurden Wien, Kufstein, Graz und Linz auf Gelb gestellt, was vor allem in der oberösterreichischen Hauptstadt Empörung auslöste. Denn anderswo waren die Infektionszahlen höher. Dabei gab es laut Klimek einen guten Grund, dass sich die Ampelkommission so entschieden hatte: In Linz war nur ein geringer Teil der Neuinfektionen auf Reiserückkehrer zurückzuführen, die Quelle der Ansteckungen war bei den meisten unbekannt. Ab diesem Augenblick war die Ampel ein Politikum. Kein Wunder: Ein Großteil der Kommission ist politisch besetzt. Jedes Land schickt je einen Vertreter, die Regierung noch einmal fünf. Das jeder mit Interessen im Gepäck zur Sitzung geht, ist klar. Dass diese die fünf Expertinnen überstimmen können, auch. Wie hitzig sie in stundenlangen Ampelsitzungen debattieren, wird nach jeder Sitzung erneut nach außen getragen – unbeachtet der Verschwiegenheitspflicht der Mitglieder. Die nunmehrige Ampel folge ""weniger medizinischen und epidemiologischen Kriterien, sondern eher rein politischen"", sagt auch Public-Health-Experte und Ex-Corona-Taskforce-Mitglied Martin Sprenger im STANDARD-Gespräch. Die nächste Schaltung hatte keine konkreten Folgen mehr, weil die Regierung nun andere Entscheidungen gab und etwa die bei Orange vorgesehene Einführung von Heimunterricht für die Oberstufe bei Orange gar nicht umsetzte. Bei der jüngsten Schaltung am Donnerstag war die Ampel endgültig in der Bedeutungslosigkeit versunken. Erneut berieten 19 Köpfe über die Risikosituation des Landes, in einer Phase, in der die Zahlen massiv steigen. Doch ihre Entscheidung versank in der öffentlichen Erregung um neue Verschärfungen, die der Kanzler am selben Tag ankündigte. Anschober hatte sich das wohl ganz anders vorgestellt. Aber zu glauben, dass in Österreich so gravierende Entscheidungen wie Maskenpflicht oder Beschränkungen von Versammlungen an der Politik vorbei allein von Experten entschieden werden könnten, war wohl von Anfang an naiv. Corona-Tests. Immer noch zu wenig, immer noch zu langsam. Es war Ende März, als Bundeskanzler Sebastian Kurz 15.000 Tests am Tag in Aussicht stellte. Für viele Experten gelten häufige Tests, die auch Menschen ohne Symptome leicht in Anspruch nehmen können, als Schlüsselfaktor für die Rückkehr zur Normalität. Dieses Ziel wurde bis heute nicht erreicht. Immer wieder stößt das System an Kapazitätsgrenzen, sei es beim Abstrich oder bei der Auswertung im Labor. Aber auch die in Österreich angewandte Teststrategien sind voller Schwächen. So ging man etwa im Lauf der Pandemie davon, nur Kontaktpersonen mit einem Risikofall zu testen, dazu über, Personen, die nur Symptome haben, auch mit einzubeziehen. Später wurde dies reduziert auf ein Symptom. Dennoch werden viele Personen nicht getestet, oder müssen lange auf ihr Ergebnis warten. Betroffene erzählen, dass sie über zwei Wochen auf ein Ergebnis warten – länger als sie eigentlich in Quarantäne sein müssten. Viele warten tagelang, bis das Testteam erst ankommt, in Schulen ist heute oft noch unklar, was mit anderen Schülerinnen und Schülern passiert, wenn ein Kind positiv oder ein Verdachtsfall ist. Doch ohne rasche Tests gibt es auch kein effizientes Contact-Tracing. Wer rasch einen Test mit Ergebnis haben will, muss zu einem der privaten Labors gehen und bis zu 140 Euro hinlegen. Nur dann kann innerhalb weniger Stunden wissen, ob man positiv ist oder nicht. Dazu kommt, dass die Bundesländer die Ergebnisse unterschiedlich einspeisen und ausweisen. Die Zahlen sind daher oft nicht vergleichbar. Contact-Tracing. Immer noch zu wenig, immer noch zu langsam. Als Ende April die Ausgangsbeschränkungen ausliefen, wurde mit jedem Lockerungsschritt die Bedeutung des Contact-Tracings betont, um die Pandemie unter Kontrolle zu halten. ""Containment 2.0"" hieß die Strategie, die das Gesundheitsministerium am 20. Mai per Erlass allen Landeshauptleute mitteilte. Ihr Ziel war, mit Sars-CoV-2 Infizierte ""ehest möglich"" zu finden und zu isolieren. Die Ermittlung möglicher Kontaktpersonen habe innerhalb von 24 Stunden ab der Absonderung eines Verdachtsfalls zu geschehen, hieß es darin. Mittlerweile wurde diese Vorgabe angepasst: Innerhalb von 24 Stunden soll der Erkrankte isoliert sein. In weiteren 24 Stunden sind die Kontaktpersonen zu erheben, und gegebenenfalls nochmals 24 Stunden später sollten auch diese zur Heimquarantäne aufgefordert werden. Doch die Realität ist ganz anders. Das liegt vor allem an der Überlastung der Hotline 1450, von wo die Nachverfolgung ihren Ausgang nehmen sollte. In vielen Bundesländern fehlt es an Personal, die größte Zahl an Mitarbeitern weist mit 400 derzeit Niederösterreich auf. Aber auch das ist zu wenig, wenn die Zahl der Infektionen sprunghaft ansteigt. Wie lang die von Bürgermeister Michael Ludwig angekündigte Aufstockung im Wiener Büro für Sofortmaßnahmen von 100 auf 600 dauern wird, ist derzeit nicht abschätzbar. Im Entwurf des neuen Covid-Gesetzes wurde in der Begutachtung ein besonders umstrittener Punkt betreffend Contact-Tracing gestrichen: Ursprünglich sollten ""Betriebe, Veranstalter und Vereine"" verpflichtet werden, personenbezogene Kontaktdaten von Gästen, Besuchern, Kunden und Mitarbeitern 28 Tage aufzubewahren. Massive Kritik daran gab es vor allem aus der Gastronomie. Geplant ist nun nur noch die Verpflichtung, Daten über grenzüberschreitende Reisen auf Verlangen dem Gesundheitsministerium zur Verfügung zu stellen. Explizit angesprochen werden etwa Hotels, Fluglinien und die Bahn. Eine Pflicht zur Datensammlung ergibt sich daraus aber nicht, heißt es in den Erläuterungen. Vielmehr zielt die Regelung nur auf Daten, die ohnehin zur Verfügung stehen. Stopp-Corona-App. Debatte über Zwang dämpfte die Begeisterung. Erleichtert werden sollte das Contact-Tracing auch durch die Stopp-Corona-App vom Roten Kreuz, ein an sich sinnvolles Instrument. Österreich war Vorreiter bei Entwicklung und Einführung. Doch eine Debatte über einen möglichen Zwang zur App, die von der ÖVP losgetreten wurde, dämpfte die Begeisterung. Mittlerweile steht die Entwicklung der App quasi still. Zuletzt hatten sie 966.000 Menschen auf ihren Handys installiert. Andere Länder sind weiter. Die deutsche App hatte bereits Mitte Juli 15 Millionen Nutzer, ein höherer Anteil der Bevölkerung. Außerdem ist die deutsche App grenzüberschreitend einsetzbar, was die österreichische erst werden soll – wann, kann das Rote Kreuz nicht sagen. Doch als Wunderwaffe hat sich die App noch in keinem Land erwiesen, höchstens als Hilfsmittel. Für Complexity-Forscher Klimek ist die Lage trotz aller Versäumnisse nicht außer Kontrolle. Dass sich Verschärfungen und Lockerungen je nach Infektionsgeschehen abwechseln, sei der richtige Weg. Der Herbst werde jedenfalls neue Herausforderungen bringen, wenn die Menschen wieder mehr Zeit in Innenräumen verbringen. ""Da kommt noch viel auf uns zu"", sagt er. ""Aber die Intensivstationen werden nicht gleich übergehen. Und wenn es tatsächlich schlechter wird, werden wir in ein paar Wochen noch härtere Maßnahmen erlassen müssen."" Party in Corona-Zeiten. Schlupflöcher, die geschlossen werden mussten. Die Nachtgastronomie habe bei der Zunahme der Fälle in letzter Zeit eine besondere Rolle gespielt, verkündete Bundeskanzler Sebastian Kurz am Donnerstag. Einige Clubs, die großteils seit März geschlossen hatten, haben in den letzten Wochen private Feiern geschlossener Gesellschaften in ihren Räumlichkeiten ermöglicht. Not macht eben erfinderisch – allerdings wurden dabei sämtliche Corona-Regeln missachtet. Fotos und Videos von tanzenden Menschenmengen gibt es auf Instagram zuhauf, keine Spur vom Babyelefanten. Gefeiert wurde außerdem bis in die Morgenstunden, denn für geschlossene Veranstaltungen galt die Sperrstunde von 1 Uhr nicht. Aber: Nicht nur die geschlossenen Veranstaltungen in Clubs waren in den Augen der Regierung problematisch. Auch im normalen Bar-Betrieb kam es in den letzten Wochen zu Schwierigkeiten. So gibt es in Dornbirn beispielsweise ein Party-Cluster. In dem betroffenen Lokal hielten sich viele Jugendliche auf, Abstände wurden konsequent missachtet. Alldem soll nun ein Riegel vorgeschoben werden: Mehr als zehn Leute sollen nicht mehr zusammenkommen, ob beim Essen, einer Geburtstagsfeier, einer Hochzeit oder beim Bier nach dem Fußballtraining. Die Polizei werde die neuen Regeln verstärkt kontrollieren, versicherte Innenminister Karl Nehammer. Cluster und Communities. Zu viele Fälle können nicht auf einen Cluster zurückgeführt werden. Nicht nur Abstandhalten, das Tragen einer Maske, viele Tests und gutes Contact-Tracing sind ein Schlüssel, um die Ausbreitung von Sars-CoV-2 im Griff zu haben. Je mehr Fälle in Cluster eingeordnet werden können, desto besser. So kann die Ausbreitung nämlich analysiert werden. Das tut unter anderem die Agentur für Gesundheit und Ernährungssicherheit (Ages). In der ersten Septemberwoche – das sind die aktuellsten Zahlen, die auf der Website der Ages abrufbar sind – gab es knapp 170 Cluster. Der größte Teil davon (130) waren sogenannte ""lokale Häufungen"", etwa 20 betrafen Reisegruppen. Unter den lokalen Häufungen ist die Ansteckungsquelle in den meisten Fällen der Haushalt, Freizeitaktivitäten und der Arbeitsplatz folgen. Die Ages weist außerdem aus, wie viele Fälle die Quelle im Ausland hatten. Hier war Anfang August der Anteil am höchsten. Von 1.441 Fällen hatten 624 die Quelle im Ausland. Anfang September nahmen von 2.242 Fällen 347 ihren Ursprung im Ausland. Mindestens 60 bis 70 Prozent aller Infektionen mit dem Coronavirus sollten sich idealerweise auf einen Cluster als Quelle zurückführen lassen, heißt es von Seiten der Ages. Falle dieser Wert unter 50 Prozent, werde es schwieriger, wirkungsvoll mit Präventivmaßnahmen zu arbeiten. Genau das war in Österreich die letzten Tage der Fall. Nur etwa die Hälfte der über 30.000 bisher bestätigten Covid-Fälle kann einem bestimmten Cluster zugewiesen werden. Die Bundesländer kommunizieren dabei recht unterschiedlich, denn es gibt keine bundesweit einheitliche Richtlinie dafür. Zurückhaltend agiere beispielsweise vor allem die Bundeshauptstadt, hieß es diese Woche in der APA, die sich auf Insider berief. Wien kommuniziere bewusst nicht jeden Cluster. Aus Sicht der Ages ein ""gangbarer Weg"". In der Vorwoche war in Wien nur bei 55 Prozent der Neuinfektionen die Quelle klar. 20 Prozent waren auf Auslandsaufenthalte zurückzuführen, 31 Prozent ließen sich einem lokalen Cluster zuordnen. Vier Prozent konnten durch Screeningaktivitäten identifiziert werden. Laut Ages komme es nicht unbedingt auf die Größe eines Clusters an. Vielmehr lasse sich durch das Setting und den Ausgangspunkt eine Risikoeinschätzung und eine mögliche Krisenbewältigung ableiten. Bei der Cluster-Analyse gehe es darum, den Ausgangspunkt möglichst genau zu beschreiben und das Risikoverhalten zu verstehen. Verständnis für Präventivmaßnahmen schaffe man vor allem, wenn das Risikoverhalten eingehend kommuniziert werde.";https://www.derstandard.at/story/2000120112387/wie-konnte-das-passieren;Standard;Eric Frey, Johannes Pucher, Gabriele Scherndl, Lara Hagen
03.04.2020;Österreich und das Datendilemma;"Verfügbare Daten müssen qualitativ auf den höchstmöglichen Stand gebracht werden, und nur mit solchen Zahlen darf jongliert werden. Appell eines Arztes. Als Bundeskanzler Sebastian Kurz (ÖVP) vergangenen Montag in der ""ZiB spezial"" mitteilte, dass man nicht wolle, dass österreichische Intensivstationen ""übergehen wie in Italien"", und bereits am selben Tag via APA die Meldung erschien, dass nur noch ein Drittel der Beatmungsgeräte in Österreich frei sei, ließ die Reaktion der Bundesländer nicht lange auf sich warten. Von Vorarlberg bis ins Burgenland wurden korrigierte, höhere Zahlen zu verfügbaren Beatmungskapazitäten vermeldet. Wie kommt's? Eine Vermutung, die ein großes Datendilemma aufzeigt. Erstes Datendilemma. Kurz bezieht sich laut APA vom 30. März auf ""2.584 Beatmungsgeräte"" in Österreich – mit unterschiedlichen Pro-Kopf-Verteilungen und noch freien Kapazitäten in den Bundesländern. Diese Zahl, so scheint es, ist der ""Jahresmeldung Krankenanstaltenstatistik"" des Gesundheitsministeriums entnommen. Diese führt für Österreich 2.547 Intensivversorgungsbetten an – wohlgemerkt in den zugänglichen Zahlen für das Jahr 2018. Die erste Krux, die sich hier neben dem Faktum, dass diese Betten auch Kapazitäten für Kinder und Jugendliche beinhalten, zeigt, ist, dass gemäß Qualitätskriterien des Österreichischen Strukturplans Gesundheit (ÖGS) nicht für jedes Intensivversorgungsbett in Österreich auch gleich ein Beatmungsgerät zur Verfügung stehen muss. Beispielsweise kann eine Intensivstation der Stufe 1 eine ""Anzahl der Betten minus drei Geräte – mindestens ein Gerät"" aufweisen, also drei Beatmungsgeräte weniger als Intensivbetten haben, um als Intensivstation gelistet zu werden – bei einer Mindestgröße von sechs Betten pro Intensivstation. Einer Intensivüberwachungseinheit (IMCU) muss gar überhaupt nur ein Beatmungsgerät pro Station zur Verfügung stehen. Es könnte also mit jedem der knapp 2.600 Betten die Möglichkeit zur Beatmung bestehen – muss es (und tut es) aber natürlich nicht. Der Umrechnungsfaktor 1 zu 1 von Betten auf Beatmungsgeräte wäre also unzulässig.Zweites Datenproblem. Die Reaktion der Länder, in der geradezu empört von höheren Verfügbarkeiten berichtet wird, zeigt das zweite Datenproblem. Wenn am folgenden Tag beispielsweise Gesundheitsstadtrat Peter Hacker (SPÖ) in Wien ""1.058 Beatmungs-Intensivbetten (853 Intensivbetten für Erwachsene und 205 für Kinder)"" auflistet, dann ist das ein gewaltiger Sprung von den 696 gemäß Jahresstatistik 2018 zur Verfügung stehenden Intensivstationsbetten. Wie wurde hier nun gerechnet? Es scheint, als würden nun tatsächlich (fast) alle Beatmungsgeräte in allen öffentlichen Krankenhäusern, die man theoretisch neben ein Patientenbett stellen kann, in diese Zahlen aufgenommen. Beispielsweise besitzen auch Aufwachräume bereits im täglichen Routinebetrieb – also ohne große ""Coronavirus-Umrüstung"" – oftmals Möglichkeiten zu intensivmedizinscher Überwachung und Therapie, eben auch zur Beatmung mittels Beatmungsgerät. Hier werden und wurden natürlich durch Umrüstungen – zusätzlich zu den bestehenden, planmäßig aufgestellten Intensivbetten – noch weitere Verfügbarkeiten geschaffen. Auch verfügen Krankenhäuser standardmäßig oftmals über Reserven an Geräten beziehungsweise könnten auch initial nicht dafür eingesetzte Beatmungseinheiten (zum Beispiel solche, die sonst nur für den innerklinischen Transport von Patienten vorgehalten werden) nun für beatmungspflichtige Patienten mit Coronavirus verwendet werden. Miteinberechnet werden müssten aber natürlich weiterhin Reserven für Geräteausfälle und Patiententransporte. Ganz abgesehen davon, dass nicht jedes Krankenhaus und jede Abteilung durch bloße Verfügbarkeit eines Beatmungsgerätes in der Lage ist, Patienten über längere Zeit zu beatmen beziehungsweise zu überwachen und zu therapieren. Die intensivmedizinische Versorgung schwerkranker Patienten beschränkt sich (auch bei Patienten mit Coronavirus) ja zu einem Großteil nicht auf bloße Atmungsunterstützung. Ob diese Faktoren eingerechnet wurden, bleibt aufgrund der Äußerungen aus den Bundesländern unklar. Meldungen wie jene aus Niederösterreich, laut der ""in den ursprünglich genannten Zahlen irrtümlich um 100 freie Geräte zu wenig angegeben waren"", lassen jedenfalls zweifeln. Für eine valide Berechnung der zur Verfügung stehenden intensivmedizinischen Kapazitäten scheinen die von den Bundesländern nun veröffentlichten Zahlen aber ebenso wenig geeignet. Geradezu grotesk wird es nämlich dann, wenn – wie zum Beispiel in Tirol und im Burgenland – zu den zur Verfügung stehenden Beatmungsgeräten auch ""Narkosegeräte"" aus den OP-Sälen hinzugezählt werden. Anhand dieser ultimativen Notfallressource zu planen würde jegliche akutmedizinische Versorgung (es gibt weiterhin auch noch andere Notfälle während der Coronavirus-Pandemie, die unter Umständen eine operative Versorgung benötigen) gefährden und kann keinesfalls zur Gestaltung der Maßnahmen zur Eindämmung der Pandemie planmäßig einkalkuliert werden. Es gibt keinen guten Grund anzunehmen, dass wir auf einen solchen massiven Notfallbetrieb zusteuern. Drittes Dilemma. Das dritte, wenn auch nicht letzte – aber dafür vermutlich größte – Problem im Zusammenhang mit diesen ""Berechnungsmethoden"" stellt sich in der – bisher öffentlich komplett ausgeblendeten – Frage nach dem Personal. Es würde wohl niemand auf die Idee kommen, die Transportkapazität aller Lkws in Österreich anhand deren bloßer physischer Existenz zu berechnen. Es braucht auch Personen, die diese lenken können (und dürfen), also mit einem Führerschein der entsprechenden Klasse, idealerweise inklusive Fahrpraxis, oder – wenn man sich schon im Notfallmodus befindet – zumindest bloßer Erfahrung im Steuern dieser Fahrzeuge ausgestattet sind. Sonst wird der entsprechende Transporter – im besten Fall – nie in Betrieb genommen. Oder fährt in den Graben. Hier muss also für eine realistische Berechnung der Betreuungskapazität beatmungspflichtiger Patienten unbedingt das medizinische Personal, insbesondere die verfügbaren Intensiv- und Anästhesie-Pflegefachkräfte sowie Intensivmediziner, inklusive aller zu erwartenden Ausfälle, miteinberechnet werden. Zusammenfassend muss man feststellen: Was im Routinebetrieb, abseits von Pandemien und Katastrophen, störend und mühsam ist, wird nun tatsächlich brisant und gefährlich – nämlich die Qualität der verfügbaren Daten. Natürlich sind in einer neuartigen Situation mit einem neuartigen Virus gewisse Zahlen einfach nicht von vornherein verfügbar, müssen erst erarbeitet und können vermutlich teilweise auch erst im Nachhinein beurteilt werden. Ebenso ist es vermutlich noch zu früh, um mit Sicherheit sagen zu können, welche der Maßnahmen, die nun gesetzt werden, richtig, falsch oder sinnlos sind. Was jedoch an Daten verfügbar ist und als Berechnungsgrundlage für – teils gravierende – Maßnahmen dient, muss in bestmöglicher Qualität aufgearbeitet werden. Hier zeigt sich ein – teilweise äußerst unnötiges – österreichisches, aber auch europäisches Datendilemma. Es muss also an alle Beteiligten – insbesondere die Bundesregierung – der Appell ergehen, die verfügbaren Daten qualitativ auf den höchstmöglichen Stand zu bringen und nur mit solchen Zahlen zu jonglieren, die auch einer tiefergehenden Überprüfung standhalten. Gerade wenn in Zusammenhang mit der Umsetzung von Maßnahmen zur Eindämmung der Pandemie bereits mehrmals der – sicher nicht immer korrekt verwendete – Begriff ""Big Data"" fällt, muss zuerst einmal alles darangesetzt werden, die zur Verfügung stehenden ""Small Data"" korrekt aufzuarbeiten. Es besteht hier nämlich die große Gefahr eines Dominoeffekts an Entscheidungen, die anhand inkorrekter Grundlagen getroffen werden, der sich durch alle Entscheidungsträger und verantwortlichen Berufsgruppen ziehen könnte – oder dies bereits tut.";https://www.derstandard.at/story/2000116490922/oesterreich-und-das-datendilemma;Standard;Lukas Infanger
05.10.2018;Globaler CO2-Ausstoß 2018 neuerlich stark gestiegen;"Die Emission von Treibhausgasen wird 2018 voraussichtlich einen Höhepunkt erreichen. Gleichzeitig gibt es eine Rekordschmelze des Grönland-Eisschilds. Es ist mehr als ein Vierteljahrhundert her, dass sich Staatschefs aus aller Welt 1992 beim UN-Klimagipfel in Rio de Janeiro darauf verständigt haben, die gefährlichen menschengemachten Einflüsse auf das Klima zu bremsen. Von wesentlicher Bedeutung dabei ist die Reduktion von Kohlenstoffdioxid, das bei der Verbrennung fossiler Energieträger wie Öl, Kohle und Gas freigesetzt wird. Denn das Treibhausgas CO2 gilt als wesentlicher Faktor für die Erderwärmung. Doch trotz der politischen Absichtserklärungen der vergangenen Jahrzehnte konnte der globale CO2-Ausstoß nicht gebremst werden. Im Gegenteil: Wie Forscher in den Wissenschaftsjournalen ""Nature"", ""Earth System Science Data"" und ""Environmental Research Letters"" berichten, deuten die Berechnungen darauf hin, dass der CO2-Ausstoß 2018 gegenüber dem Vorjahr um 2,7 Prozent angestiegen ist. Laut den Forschern liegt der Bereich, in dem der Anstieg jedenfalls liegen wird, zwischen 1,8 und 3,7 Prozent. Energiewende noch nicht geschafft. Bei 2,7 Prozent Zuwachs wird heuer so viel Kohlenstoffdioxid wie in keinem Jahr zuvor freigesetzt werden, nämlich rund 37 Milliarden Tonnen (siehe Grafik). Die führenden CO2-Verursacher aktuell sind China (27 Prozent), die USA (15 Prozent), die EU (zehn Prozent) und Indien (sieben Prozent). 2018 ist damit das zweite Jahr in Folge, in dem der globale CO2-Ausstoß gestiegen ist, 2017 betrug die Zunahme 1,6 Prozent. Zwischen 2014 und 2016 sind die Emissionen hingegen annähernd konstant geblieben. Damals hatten Wissenschafter auf eine Trendwende gehofft, 2017 noch einen einmaligen Ausrutscher für möglich gehalten. Mit dem erneuten Anstieg 2018 scheint aber klar, dass die Energiewende noch längst nicht geschafft ist. Ziellinie nicht in Sicht. Um das 2015 in Paris vereinbarte Ziel von 1,5 Grad Celsius Temperaturanstieg gegenüber dem vorindustriellen Zeitalter zu erreichen, müssten die Emissionen bis 2030 um 50 Prozent reduziert werden, bis 2050 müssten sie überhaupt auf null sinken, betont die leitende Forscherin der Studie, Corinne Le Quéré, Direktorin des Tyndall-Zentrums für Klimaforschung und Professorin für Klimawandel an der University of East Anglia in Norwich, England. ""Davon sind wir weit entfernt"", sagt Le Quéré. ""Die Emissionen müssen stark zurückgehen, um dem Klimawandel zu begegnen. Doch mit dem Zuwachs an Emissionen in diesem Jahr sieht es so aus, dass der Peak noch nicht in Sicht ist"", so die Klimaforscherin. Dabei hätten die Menschen in diesem Jahr gesehen, welche enormen Auswirkungen der Klimawandel haben kann, indem er etwa weltweit zu Hitzewellen führt. ""Die Waldbrände in Kalifornien sind nur ein Schnappschuss der zunehmenden Folgeerscheinungen, wenn wir die Emissionen nicht rasch zurückfahren"", sagt Le Quéré. Rückgang bei Emissionen trotz Wirtschaftswachstum. Dass es auch anders möglich wäre, zeigen 19 Länder, die ihre CO2-Emissionen trotz Wirtschaftswachstums im vergangenen Jahrzehnt reduzieren konnten: Nach Angaben der Wissenschafter sind Schwergewichte wie die USA und Frankreich darunter, mit der Schweiz, Slowenien und der Slowakei auch einige unserer Nachbarländer, nicht aber Österreich selbst. Eine weitere aktuelle Studie in ""Nature"" macht auf die Langzeitfolgen der durch menschliche Aktivitäten beschleunigten Erderwärmung aufmerksam: Wie ein Forscherteam berichtet, hat die Schmelze des grönländischen Eisschilds im Vergleich mit den vorangegangenen Jahrhunderten seit Mitte des 19. Jahrhunderts dramatische Ausmaße erreicht. ""Das Schmelzen des Grönland-Eisschilds hat sich gewaltig beschleunigt"", sagt der Erstautor der Studie Luke Trusel von der Rowan University in New Jersey. Mehr Schmelzwasser"". Als Folge davon trägt die Grönland-Eisschmelze mehr denn je zuvor in den vergangenen dreieinhalb Jahrhunderten zum Anstieg des Meeresspiegels bei"", sagt Trusel. Wobei anzumerken ist: Der Anstieg der Gletscherschmelze setzte zur selben Zeit ein, als die Industrialisierung ab Mitte der 1800er-Jahre die Atmosphäre veränderte. ""Wir haben einen 50-prozentigen Anstieg des Schmelzwassers verglichen mit dem vorindustriellen Zeitalter festgestellt"", sagt Co-Autorin Sarah Das, Gletscherforscherin an der Woods Hole Oceanographic Institution. Die Daten deuten auf eine weitere Beschleunigung der Eisschmelze selbst bei nur geringem Temperaturanstieg hin.";https://www.derstandard.at/story/2000093238936/globaler-co2-ausstoss-2018-neuerlich-stark-gestiegen;Standard;Tanja Traxler
06.10.2020;Zugang zu Registerdaten: Angst im ehrenwerten Haus;"Der Wissenschaft können anonymisierte Daten aus dem Gesundheits-, Arbeitslosen- und Pensionswesen helfen, die Corona-Pandemie besser zu verstehen. ""Doch ich sah auch die Angst, die so viele zur Einsicht bringt"", trällerte Udo Jürgens 1982, während der saure Regen auf seinen Kopf tröpfelte. Es war die Zeit des Kalten Krieges, und in Westeuropa hatte man Angst, dieser Krieg könnte bald heiß werden und in einem Armageddon enden. Hunderttausende gingen auf die Straße, um gegen die Stationierung der Pershing II in der BRD und für die einseitige Abrüstung des Westens zu demonstrieren. Interessanterweise demonstrierten kaum Westdeutsche gegen die sowjetischen SS20, die genau auf sie gerichtet waren. Angst ist ein seltsamer Ratgeber, der uns bisweilen verhöhnt. Mit Covid-19 ist dieses Gefühl der unmittelbaren Angst wieder zurückgekehrt. Die Pandemie wirft berechtigte existenzielle Sorgen auf, um die eigene Gesundheit und das Leben nahestehender Menschen, um den Arbeitsplatz oder das eigene Unternehmen, um die Bildung und Betreuung der Kinder. Manche befürchten auch, kein Klopapier mehr zu ergattern oder durch die Pflicht zum Mund-Nasen-Schutz von der Regierung versklavt zu werden. Was hält das Ministerium vom Zugang ab? Das österreichische Gesundheitsministerium ist ein ehrenwertes Haus. Aber auch dort ist man ängstlich. Nicht erst seit Covid-19, aber die Pandemie hat die Symptome verstärkt. Es ist die Angst, der Wissenschaft systematischen und rechtlich verbindlichen Zugang zu anonymisierten beziehungsweise pseudonymisierten Registerdaten aus dem Gesundheits-, Pflege-, Arbeitslosen- und Pensionswesen zu ermöglichen. Solche Datenbestände enthalten keine Identitätsinformationen mehr; vor allem wenn sie miteinander verknüpft werden, bergen sie aber viel Informationsgehalt über das Infektionsgeschehen von Sars-CoV-2 in der österreichischen Bevölkerung. Worin besteht also die Angst? Wir können nur spekulieren: Vielleicht will man im öffentlichen und wissenschaftlichen Diskurs die Interpretationshoheit nicht verlieren; vielleicht sind es andere politische und wirtschaftliche Eigeninteressen, weshalb man es gar nicht so genau wissen will. Der Linzer Gesundheitsökonom Martin Halla hat vor kurzem auf Twitter anhand dreier Beispiele den Mehrwert, den Österreich aus der wissenschaftlichen Nutzung dieser Daten ziehen könnte, konkret benannt: Begünstigen die Arbeitsbedingungen in manchen Branchen Covid-19-Infektionen? Spielt die Zusammensetzung der Haushalte – mehrere Generationen in einem Haushalt – eine Rolle beim Infektionsgeschehen? Wie wirkt sich das Pendeln zum Arbeitsplatz auf die Verbreitung von Covid-19 aus? Italienische und spanische Daten für Österreich. Das sind eigentlich einfache und grundlegende Fragen, deren Beantwortung wichtig wäre, um die Covid-19-Pandemie effektiv stoppen zu können. Doch obwohl die Daten vorhanden sind, können diese Fragen nicht beantwortet werden. Der Zugang zu den Daten bleibt der Wissenschaft nämlich verwehrt. Auf öffentlichen Druck hin hat das Gesundheitsministerium zwar ein Datenportal für die Forschung eingerichtet. Die dort zur Verfügung gestellten Daten sind allerdings für wissenschaftliche Analysen, welche unter anderem die oben gestellten Fragen beantworten möchten, unbrauchbar. Deswegen musste etwa das Institut für Höhere Studien (IHS) bei der Erstellung einer Covid-19-Studie über Österreich auf italienische und spanische Daten zurückgreifen, wie Thomas Czypionka, Gesundheitsökonom am IHS, zuletzt im Ö1-""Morgenjournal"" berichtete. Die Aussagekraft einer solchen Studie ist allerdings beschränkt, da die Pandemieverläufe und die betroffenen Personengruppen in Italien und Spanien andere waren als in Österreich. Damit ist auch der unmittelbare Nutzen der Studie für angemessene Maßnahmen in Österreich begrenzt. Czypionka selbst übrigens macht sich keine Illusionen: ""Wir sind sicherlich innerhalb der entwickelten Staaten Schlusslicht"", bringt er im Interview die Lage auf den Punkt. Mehr Effizienz. Das Problem ist aber selbstverständlich nicht auf Daten zur Covid-19-Pandemie beschränkt. So leidet etwa die Erforschung von Diagnose- und Behandlungspfaden und Ko-Morbiditäten bei Krebspatienten ebenso unter der Unzugänglichkeit vorhandener Daten wie Versuche, die Effizienz des stationären Versorgungssystems in Österreich zu evaluieren. Auch Daten zum Pflegegeld bleiben der Wissenschaft verschlossen. Diese Beispiele zeigen, dass es nicht nur um viel Geld geht, das im Gesundheits- und Pflegewesen vielleicht effizienter eingesetzt werden könnte. Noch viel mehr geht es darum, menschliches Leid zu verringern und auf der Basis wissenschaftlicher Evidenz die Qualität von Krankenbehandlung und Pflege zu verbessern. Dafür benötigt die Wissenschaft Daten. Das Gesundheitsministerium soll der Wissenschaft daher den Zugang zu den Datenbeständen in seinem Wirkungsbereich ermöglichen, indem es diese in das zukünftige Austrian Micro Data Center der Statistik Austria einbringt. Damit sind auch höchste Datenschutz- und Datensicherheitsstandards garantiert, und es kommt nicht zu redundanten Insellösungen, die nicht miteinander kompatibel sind. Die Daten verbleiben immer im Austrian Micro Data Center und können nicht heruntergeladen oder weitergegeben werden. Dies wäre ein gigantischer Sprung für die österreichische Wissenschaft, von dem das ganze Land profitieren würde. ""Jemand sagte zu mir, dass die Zukunft grad' jetzt beginnt ... in diesem ehrenwerten Haus"", würde Udo Jürgens das in einem seiner berühmten Medleys wohl kommentieren.";https://www.derstandard.at/story/2000120432948/zugang-zu-registerdaten-angst-im-ehrenwerten-haus;Standard;Jesus Crespo Cuaresma, Thomas König, Harald Oberhofer, Caroline Schober-Trummler, Gerhard Schwarz, Michael Strassnig
30.07.2020;Sonde mit Mars-Rover Perseverance hatte technische Probleme;"Perseverance hob am Dienstag ab, im Erdschatten versetzte sich der Rover wegen Unterkühlung in den Sicherheitsmodus. Laut Nasa gibt es keinen Grund zur Sorge. Der neue Mars-Rover der Nasa ist unterwegs: Um 13.50 Uhr MESZ hob der Curiosity-Nachfolger Perseverance vom Startkomplex 41 des Weltraumbahnhofs Cape Canaveral in Florida an Bord einer Atlas-V-541-Rakete erfolgreich ab. Die Atlas V ist eine der größten derzeit verfügbaren Raketen für interplanetare Flüge. Mit dem gleichen Raketentyp wurden auch die Nasa-Missionen Insight und Curiosity zum Mars geschickt. Nun hat die Trägerrakete nach Angaben der US-Raumfahrtbehörde NASA allerdings technische Probleme. Daten deuteten darauf hin, dass die Rakete in einen Sicherheitsmodus umgeschaltet habe, teilte die NASA am Donnerstagabend mit. Möglicherweise sei ein Teil des Raumschiffs im Schatten der Erde zu kalt geworden. Temperaturen haben sich normalisiert. Auch beim Aufbau einer Kommunikationsverbindung mit dem Jet Propulsion Laboratory der NASA in Pasadena im US-Bundesstaat Kalifornien kam es den Angaben zufolge zu Verzögerungen. Inzwischen habe die Rakete den Erdschatten verlassen und die Temperaturen hätten sich normalisiert, teilte die NASA weiter mit. Derzeit werde eine umfassende Überprüfung vorgenommen. Der stellvertretende Missionsleiter Matt Wallace erklärte, der Vorfall sei nicht allzu besorgniserregend. ""Das ist vollkommen in Ordnung, die Rakete ist glücklich"", sagte er. Das Team prüfe derzeit die Funktionen der Rakete. ""Bisher sieht alles, was ich gesehen habe, gut aus. In Kürze werden wir mehr wissen."" Wenn eine Trägerrakete in den abgesicherten Modus übergeht, schaltet sie alle bis auf die notwendigen Systeme ab. Schwierige Mission. Eine sichere und sanfte Landung auf dem Mars zählt zu den technisch schwierigsten und riskantesten Manövern der modernen Raumfahrt. Während in Science-Fiction-Filmen die Raumschiffe oft majestätisch herniederschweben, gleicht die dramatische Ankunft auf dem Mars in Wahrheit zumindest derzeit noch dem Ritt auf einem einschlagenden Asteroiden. Entsprechend sieht auch die Bilanz aus: 18-mal hat man bisher versucht, mobile oder stationäre Instrumente auf den Mars zu platzieren, aber nur bei zehn kann man von einem wissenschaftlichen Erfolg sprechen – neun davon waren US-amerikanische Missionen. Mit der Sonde Mars 3 gelang es der Sowjetunion zwar 1971, als erste Nation ein künstliches Objekt auf den Mars zu bringen, doch die wie eine umgekehrte 1,2 Meter große Schüssel geformte Sonde verstummte nur 20 Sekunden nachdem sie mit der ersten Panoramaaufnahme der Umgebung begonnen hatte. Nasa hat Erfahrung mit Marslandungen. Auch bei den Rovern haben die USA mit Abstand die Nase vorn – vier rollende Roboter haben bislang die Marsoberfläche erreicht, allesamt im Rahmen von Nasa-Missionen: 1997 kurvte Sojourner als erster erfolgreich operierender Rover für drei Monate auf dem Mars umher. Das nur elf Kilogramm schwere Gefährt kam etwa hundert Meter weit. 2004 folgten die berühmten Zwillinge Spirit und Opportunity. Während der Kontakt zu Spirit 2007 in einem Sandsturm verloren ging, ereilte Opportunity erst elf Jahre später das gleiche Schicksal. 2012 schließlich setzte mit Curiosity (eigentlich Mars Science Laboratory, kurz MSL) ein regelrechter Medienstar auf dem Mars auf. Viel Verkehr auf dem Weg zum Mars. Nun hat sich mit dem Rover Perseverance (auf Deutsch ""Durchhaltevermögen"") der fünfte Nasa-Rover auf den Weg zum Roten Planeten gemacht. Von dem bislang technisch ausgefeiltesten Rover, den die US-Raumfahrtbehörde je zum Mars geschickt hat, erhofft sich die Nasa einen ähnlich erfolgreichen Missionsverlauf wie beim MSL. Der Name des Vehikels war übrigens im Rahmen eines Wettbewerbs von einem Siebtklässler aus dem US-Bundesstaat Virginia vorgeschlagen worden. Was Reisen zum Mars betrifft, fiel der Start von Perseverance in eine verkehrsreiche Zeit, immerhin haben sich in den vergangenen zehn Tagen bereits zwei andere Sonden auf den Weg gemacht: Am 19. Juli hob mit Al Amal die erste Marsmission der Vereinigten Arabischen Emirate vom japanischen Weltraumbahnhof Tanegashima ab. Und am 23. Juli schickte China Tianwen-1 los – für das Reich der Mitte ebenfalls eine Premiere, und eine recht ehrgeizige dazu: die ""Himmelsfrage-1"" besteht aus einer Sonde, einem Landemodul und einem Rover von der Größe eines Golfmobils. ""Sieben Minuten des Terrors"". Alle drei Missionen sollen im Februar 2021 ihr Ziel erreichen. Dann wird wohl auch beim Perseverance-Team der Nasa nach der vergleichsweise entspannten Cruise-Phase der Adrenalinspiegel wieder deutlich steigen. Nach dem Einschwenken in eine niedrige Marsumlaufbahn am 18. Februar beginnt nämlich der kritischste Missionsabschnitt nach dem Start: Für die heikle Landung des 2,5 Milliarden Dollar teuren Rovers greift die Nasa auf die Sky-Crane-Technologie zurück, die bereits bei Curiosity erfolgreich zum Einsatz gekommen ist. Bevor der MSL-Rover am 6. August 2012 unbeschadet auf der Marsoberfläche zum Stehen kam, hatte er ""sieben Minuten des Terrors"" hinter sich zu bringen. So hatten Nasa-Ingenieure diesen harten Abwärtsritt durch die Marsatmosphäre getauft, bei dem die Sonde samt Rover in den besagten sieben Minuten ganz autonom von annähernd 21.000 Kilometer pro Stunde auf null abgebremst hat. Den ersten Teil davon übernimmt auch diesmal wieder ein Hitzeschild, der die Geschwindigkeit des Flugkörpers auf unter 1.000 Kilometer pro Stunde reduziert. Ab einer Höhe von zwölf Kilometern sorgt ein riesiger Überschallfallschirm dafür, dass sich der Abstieg der Landeeinheit auf geschätzte 300 Kilometer pro Stunde weiter verlangsamt. In acht Kilometern Höhe wird der Hitzeschild abgeworfen. 80 Sekunden später löst sich der Sky Crane, der mitsamt dem Rover mit 280 Kilometern pro Stunde dem Boden entgegenrast. Abgebremst von acht Raketentriebwerken soll diese Vorrichtung den Rover schließlich an über sieben Meter langen Kabeln sanft zu Boden abseilen. Landen in einem ehemaligen See. Als Landepunkt von Perseverance hat die Nasa den Krater Jezero auf der Nordhalbkugel des Mars auserkoren. Als Alternativen kamen auch frühere hydrothermale Quellen im Nordosten der Hochebene Syrtis Major oder bei den Columbia Hills infrage. Der 49 Kilometer durchmessende Krater enthielt vor rund vier Milliarden Jahren einen 500 Quadratkilometer großen See mit einem großen Einzugsgebiet, dessen Ablagerungen der Rover für mindestens ein Marsjahr, also 687 Erdtage, auf potenzielle Lebensspuren in Augenschein nehmen soll – eine Aufgabe, die er mit seinem Vorgänger Curiosity gemein hat. Unterschiede und Gemeinsamkeiten. Auch was das Aussehen betrifft, gibt es zwischen Curiosity und Perseverance oberflächliche Ähnlichkeiten. Bei genauerem Hinsehen offenbaren sich freilich einige Unterschiede: Perseverance ist mit 1.025 Kilogramm etwas schwerer und robuster als sein Vorgänger und mit anderen wissenschaftlichen Instrumenten bestückt, unter anderem mit zwei Mikrofonen, 19 Kameras und einem Laser. Weiters wurde der Instrumententräger am ""Arm"" des Rovers sowie seine Räder modifiziert. Neben der Suche nach Lebenshinweisen und geologischen sowie Klima-Untersuchungen soll Perseverance auch neue Technologien testen. Dazu zählt erstmals auch das Aufsammeln von Bodenproben für einen möglichen späteren Rücktransport zur Erde. Spektakulärer noch ist Ingenuity (auf Deutsch ""Einfallsreichtum""), eine 1,8 Kilogramm schwere Kamera-bestückte Hubschrauberdrohne, die kurze Testflüge durch die dünne Marsatmosphäre unternehmen und damit beweisen soll, dass das Fliegen auf dem Mars möglich ist. Damit die vier Rotorblätter aus Kohlefasern die Drohne in der dünnen Atmosphäre in Schwebe halten können, rotieren sie wesentlich schneller als jene von irdischen Hubschraubern. Bis zu vier Flugversuche könnte Ingenuity auf dem Roten Planeten starten. Treibstoff und Luft für künftige Marsbesucher. Und noch eine Technologie weist in eine ambitionierte Zukunft: Ein Experiment an Bord des Rovers soll zeigen, ob Kohlendioxid aus der Marsatmosphäre in Sauerstoff umgewandelt werden kann, mit dem theoretisch Raumfahrer versorgt und Treibstoff für Rückflüge zur Erde hergestellt werden könnten. Damit solle die ganze Perseverance-Mission darauf hinführen, ""dass eines Tages Menschen nicht nur auf dem Mond leben und arbeiten, sondern auch auf einem anderen Planeten"", wie Nasa-Chef Jim Bridenstine erklärte. Zumindest in Form einer Namensliste wird Perseverance schon jetzt annähernd elf Millionen Personen zum Mars fliegen: 10.932.295 Menschen aus der ganzen Welt haben nach einem entsprechenden Aufruf ihre Namen eingesandt. Diese wurden auf drei Fingernagel-große Chips gebrannt und in dem Rover verbaut. Startfenster schließt sich am 11. August. Damit das möglich wird, musste Perseverence aber erst einmal die Erde verlassen, was an sich schon eine technische Herausforderung ist. Immerhin wiegt das Gesamtpaket aus Raumschiff, Rover und Rakete über 500.000 Kilogramm. Um Treibstoff und Zeit für die Reise zum Mars zu sparen, wählte man einen günstigen Startzeitpunkt, zu dem Erde und Mars einander nahe stehen. Ein solcher Transfer auf der sogenannten Hohmannbahn ist etwa alle zwei Jahre möglich. Hätte der Rover das aktuelle, bis zum 11. August geöffnete Startfenster verpasst, hätte sich das Perseverance-Team bis 2022 gedulden müssen.";https://www.derstandard.at/story/2000119054355/technisch-ausgefeiltester-mars-rover-macht-sich-auf-den-weg;Standard;
02.09.2020;"MIT-Informatikpionier: ""Wir brauchen Datengewerkschaften""";"Die Menschen müssen Kontrolle über ihre Daten erlangen, fordert der MIT-Forscher Alex Pentland. Datenkooperativen könnten als Basis für eine gemeinnützige Planung dienen. Er experimentierte bereits vor 30 Jahren mit tragbaren Geräten, die Daten über das Verhalten von Menschen aufzeichnen, war am Aufbau des renommierten MIT Media Lab in Cambridge, Massachusetts, beteiligt und wurde vor einigen Jahren von ""Forbes"" als einer der sieben wichtigsten Computerwissenschafter der Welt genannt. Alex ""Sandy"" Pentland publiziert unentwegt, hat dutzende Firmen gegründet, berät unter anderem den UN-Generalsekretär und ist im World Economic Forum aktiv. Bei den Technologiegesprächen in Alpbach – organisiert von AIT und Ö1 in Kooperation mit Wissenschafts-, Wirtschafts- sowie Klimaschutzministerium – hielt er eine Onlinekeynote. STANDARD: Sie sagen, zentralisierte, einheitliche Optimierung – also die Art und Weise, wie große Teile unserer Gesellschaft organisiert sind – ist ein Rezept für Scheitern und Tod, wenn es darum geht, mit globalen Problemen wie der Covid-19-Pandemie umzugehen. Wie meinen Sie das? Pentland: Covid-19 ist in jeder Hinsicht ein einzigartiger Umstand. So etwas mit einheitlichen Regeln bewältigen zu wollen kann fatal sein. Ein Beispiel: In den USA ist die Covid-19-Mortalitätsrate in armen Vierteln bis zu viermal höher als in reichen. Das liegt nicht an der medizinischen Versorgung, sondern an einer schlechteren öffentlichen Gesundheit, das heißt, es gibt mehr Übergewichtige, mehr Menschen mit Vorerkrankungen etc. Vor der Pandemie wurde das nicht als großes Problem wahrgenommen. Wenn man die lokalen Daten zur öffentlichen Gesundheit gehabt hätte, hätte man womöglich eine Menge Leben retten können. Gesundheitsfragen müssen den jeweiligen Gegebenheiten angepasst werden – das darf keine zentralisierte Sache sein, wo dir nur geholfen wird, wenn du schon sehr krank bist. Dasselbe gilt für die Wirtschaft, die Verwaltung und viele andere Bereiche. STANDARD: Was ist die Alternative? Pentland: Entscheidend ist, dass die Menschen die Kontrolle über ihre Daten bekommen. Die meisten Stadtverwaltungen, Bezirke und Provinzen haben nur überkommene und unvollständige Daten, aufgrund derer sie planen können. Es gibt kaum Wege, rasch zu analysieren, ob eine Maßnahme wirkt. Konzerne wie Google haben dagegen extrem detaillierte Daten. Mit dieser Art von Datenreichtum könnten Gemeinden viel besser identifizieren, welche Infrastruktur es braucht, was wie besteuert werden sollte, wie das Gesundheitssystem, die Schulen verbessert werden könnten. Die Datenschutzgrundverordnung (DSGVO), an deren Einführung ich mitgewirkt habe, gibt jedem Menschen das Recht auf den Besitz seiner eigenen Daten. Es wird immer darüber diskutiert, dass diese Daten viel Geld wert sind – das ist aber für den Einzelnen nicht viel, vielleicht ein paar Hundert Dollar im Jahr. Viel nützlicher wäre es, wenn es so etwas wie Daten-Kooperativen gäbe, die alle Daten einer Gemeinschaft verwalten, als Grundlage für eine Politik, die den lokalen Bedürfnissen folgt. STANDARD: Sie sprechen in diesem Zusammenhang von Data Unions, von Datengewerkschaften. Pentland: Die Analogie dazu sind die ersten Agrargenossenschaften, die im späten 19. Jahrhundert entstanden, als Banken begannen, die Bauern auszubeuten. Als im Zuge der Industrialisierung Unternehmer die Arbeiter ausbeuteten, gründeten sie Gewerkschaften, um über bessere Löhne und Arbeitsbedingungen verhandeln zu können. Das Gleiche können wir heute machen, indem wir Datengewerkschaften etablieren, um einen besseren Deal zu bekommen, von den Unternehmen und auch von den Regierungen, die Daten über uns sammeln. STANDARD: Wie soll das konkret funktionieren? Werden die Leute auch dafür streiken müssen so wie einst die Arbeiter und Arbeiterinnen? Pentland: Die DSGVO sieht vor, dass jeder eine Kopie all seiner Daten verlangen kann, von den Suchanfragen bis zu den Bewegungsdaten am Smartphone, einfach alles. Man braucht keine Erlaubnis dafür, aber kaum jemand tut es. Die Bürgerinnen und Bürger müssen erkennen, dass sie gemeinsam die Kontrolle über die Daten zurückgewinnen können. Es ist wie bei den traditionellen Gewerkschaften: Man muss sich zusammentun, um bessere Bedingungen verlangen zu können. Wenn Sie etwa der Stadtverwaltung die Nutzung Ihrer Daten überlassen und diese eine Kooperative einrichtet, die die Daten für Sie hält, muss Google bei der Kooperative um Ihre Daten bitten. Es gilt also, den großen Datenkonzernen eine Konkurrenz zu bieten. STANDARD: Die Daten einer Gemeinde sollten also öffentliches Gut sein? Pentland: Genau. Personalisierte Daten sind damit natürlich nicht gemeint. Es sollte ein Pool von aggregierten Daten und deskriptiven Statistiken geben, damit auf deren Basis kollektive Maßnahmen ergriffen werden können. Es braucht natürlich auch umfassenden Datenschutz, sodass etwa nicht ganze Datenbanken weitergegeben werden können, sondern nur bestimmte Anfragen beantwortet werden. So kann man stets die Kontrolle über die Nutzung behalten. STANDARD: Was genau soll dann mit den Daten geschehen? In Ihrer Neuinterpretation der Soziophysik haben Sie gezeigt, wie sich anhand all unserer digitalen Interaktionen und der Datenspuren, die wir hinterlassen, menschliches Verhalten abbilden und prognostizieren lässt. Pentland: Das Konzept der Soziophysik stammt aus dem frühen 19. Jahrhundert und bestand darin, Statistik zu benutzen, um die Gesellschaft zu verstehen. Heute gibt es viel mehr Menschen, schnellere Interaktionen und natürlich viel mehr Daten. Das Konzept braucht also ein Update – einen lokalen, reichhaltigen Zensus. Unsere Forschungen haben gezeigt, dass Gemeinden durch die Analyse ihrer Daten ihre wirtschaftliche Entwicklung dramatisch verbessern können. Studien haben auch belegt, dass sich Orte, wo die Menschen das Gefühl haben, dass es ein lokales Empowerment gibt, wo die Kontrolle in der Community liegt, auch ökonomisch viel besser entwickeln. STANDARD: Sie haben als Digitalpionier schon einige Entwicklungen vorausgesehen. Was erwartet uns im nächsten Jahrzehnt? Pentland: Man kann ein wenig in die Zukunft schauen, wenn man in Länder wie China blickt. Es gibt dort praktisch kein Geld mehr, es wird nur mehr über das Smartphone bezahlt. China ist aber auch eine Warnung, weil die Bürger der Regierung und den Unternehmen erlauben, ihre Daten zu besitzen. Ich stelle mir ein digitales Leben vor, in dem die Menschen selbst Kontrolle über ihre Zukunft erlangen können. Ich denke, es wird letztlich eine Periode geben wie nach dem Zweiten Weltkrieg, in der internationale Organisationen entstanden, um den Wiederaufbau zu stemmen. Ich hoffe, dass wir es schaffen, in Zukunft ein neues internationales Regime einzurichten, das das Lokale erhält, aber viel leichtere und billigere Kooperationen zwischen Ländern ermöglicht, die auf Daten und digitalen Technologien basieren, um sich auszutauschen und voneinander zu lernen. Es wird nicht perfekt sein, aber ich glaube, wir müssen sicherstellen, dass es so schnell und friedlich wie möglich passiert.";https://www.derstandard.at/story/2000119724316/mit-informatikpionier-wir-brauchen-datengewerkschaften;Standard;Karin Krichmayr
03.10.2018;Universität Wien schränkt Studienzugang ab 2019 weiter ein;"Bedingung für ein Budgetplus von 207 Millionen Euro sind bessere Studienbedingungen. Geplant sind Aufnahmeverfahren in Chemie, Jus, Anglistik, Translationswissenschaften, Soziologie, Politikwissenschaft sowie Sozial- und Kulturanthropologie. Rektor Heinz Engl sitzt nach wochenlangen Verhandlungen über das künftige Unibudget und einer letzten Marathonsitzung im Wissenschaftsministerium zufrieden in seinem Büro. Die eben abgeschlossene Leistungsvereinbarung für die Jahre 2019 bis 2021 bringt für die Uni Wien eine Budgetsteigerung von 17 Prozent – in absoluten Zahlen 207 Millionen Euro. Geld, das der Chef der größten Universität dringend gebrauchen kann. Rund 60 Millionen davon müsse man aufwenden, ""um den Status quo aufrechtzuerhalten"", aber der Rest: ""Damit kann man schon etwas anfangen"", erklärt der Rektor. STANDARD: An welche Konditionen ist die Auszahlung der Budgetmittel gebunden? Engl: In Fächern, in denen die Studienbedingungen nicht optimal sind, muss sich das Betreuungsverhältnis verbessern. Außerdem gibt der Bund vor, dass wir uns auf die Aufrechterhaltung und Stärkung der internationalen Konkurrenzfähigkeit konzentrieren. Da können wir jetzt wirklich massiv investieren – also für unsere Verhältnisse massiv. In Deutschland wurden gerade zwei Milliarden zusätzliches Geld in die Exzellenzinitiative gesteckt, so weit sind wir noch nicht. STANDARD: Die Verteilung des Geldes hängt stark von der Zahl prüfungsaktiver Studierender ab. Wo liegt da die Universität Wien? Engl: Prozentuell im Mittelfeld. Wir haben 50.000 prüfungsaktive Studierende, allerdings bei rund 93.000 Studierenden. Eine der Vorgaben ist: Das müssen wir steigern. Ab November startet die Ausschreibung für rund 40 neue Professuren. Hinzu kommen 30 Tenure-Track-Stellen, also zeitlich befristete Verträge junger Wissenschafter, die bei Erreichen von Zielvorgaben in eine Lebenszeitprofessur münden können. Inhaltlich will man neben einem Schwerpunkt auf Data-Science oder künstlicher Intelligenz auf neue Verbindungen verschiedener Disziplinen setzen. Mit der Med-Uni Wien plane man etwa die Professur ""Computional Medicine"", ein Zentrum für Mikrobiomforschung soll entstehen. Auch ein Masterstudium ""Philosophy and Economics"" ist in Planung. Engl will bereits bei den Berufungen jene Bewerber identifizieren, ""die bereit sind, sich auf Kooperationen mit anderen Fächern einzulassen"". Neue Hürden. Um bessere Studienbedingungen zu erreichen, setzt die Uni Wien auch auf neue Zugangsbeschränkungen. Konkret soll ab 2019 der Zugang für folgende Studienrichtungen neu geregelt werden: Chemie, Rechtswissenschaften, Translationswissenschaften und Anglistik. Bei den Sozialwissenschaften kommen die Fächer Soziologie, Politikwissenschaft sowie Kultur- und Sozialanthropologie hinzu. STANDARD: Wie viele Studierende sollen in diesen Fächern künftig aufgenommen werden? Engl: Die Zahlen stehen in den Leistungsvereinbarungen, ich möchte zuvor jedoch unsere Dekane und Studienprogrammleiter informieren. Grob gerechnet entsprechen die Aufnahmezahlen künftig der Anzahl derer, die bislang aufgenommen wurden – minus ""No Shows"", also derjenigen, die keine einzige Prüfung abgelegt haben. Das heißt, es muss sich niemand fürchten. Wer sich ordentlich vorbereitet, hat gute Chancen. STANDARD: Wie sollen die Aufnahmeverfahren aussehen? Engl: Die Online-Selbst-Assessments dienen nur der eigenen Orientierung. Dann wird es schriftliche Tests geben. Und in manchen Studien, etwa beim Lehramt, haben wir auch noch Interviews vorgesehen. Was wir nicht machen, sind Motivationsschreiben. Wer soll die alle lesen? In der Psychologie habe sich gezeigt, dass Prüfungsaktivität, Erfolgsquote und Studiengeschwindigkeit mit den Aufnahmetests gestiegen sind, sagt Engl. Noch etwas wurde mit dem Ministerium vereinbart: ""Wir sollen an ausgewählten Studien die Studierbarkeit überprüfen."" Es brauche eine ""intensive Analyse"", ob ein Studium überfrachtet ist, ob es Engpässe gibt, die für Verzögerungen verantwortlich sind. Ein Kandidat könnte die Publizistik sein, die hier bereits Vorarbeiten geleistet hat.";https://www.derstandard.at/story/2000088613771/uni-wien-schraenkt-zugang-ab-2019-weiter-ein;Standard;Karin Riss
27.11.2020;Zweite Covid-Welle bringt hohe Übersterblichkeit in Österreich;"Von 9. bis 15. November wurden 2.286 Todesfälle registriert. Die Sterblichkeit der über 65-Jährigen ist in fast allen Bundesländern deutlich über Durchschnitt. Wien – Die zweite Covid-Infektionswelle hat in Österreich erstmals seit Jahren eine hohe Übersterblichkeit ausgelöst. Für die Woche vom 9. bis zum 15. November weist die Statistik Austria 2.286 Todesfälle aus – der dritthöchste Wert seit 2000. Das europäische Mortalitätsmonitoring attestiert Österreich damit erstmals seit Ausbruch der Pandemie eine ""hohe Übersterblichkeit"". Im Frühjahr lagen die Todesfälle nur leicht über dem Durchschnitt. In Deutschland sind sie deutlich niedriger. Höher als in den jüngsten Daten war die Sterblichkeit laut den Zahlen der Statistik Austria zuletzt nur in den ersten beiden Kalenderwochen des Jahres 2017. Damals führte eine besonders starke Grippewelle zu einem deutlichen Anstieg der Todesfälle auf 2.293 beziehungsweise 2.340. Von diesen Rekordwerten ist Österreich allerdings auch heuer nicht mehr weit entfernt – trotz der seit Anfang November geltenden Ausgangsbeschränkungen. Auch wenn man die Zahlen in Relation zur langjährigen demografischen Entwicklung setzt, wird die Übersterblichkeit deutlich sichtbar. Die für 9. bis 15. November errechnete Sterberate von 25,6 Fällen pro Woche und 100.000 Einwohner war für diese Jahreszeit seit der Jahrtausendwende mit Abstand am höchsten. So wurde das bisherige November-Maximum im Jahr 2011 mit 18,7 Todesfällen pro 100.000 Einwohnern gemessen. Insgesamt wurden lediglich in den bereits erwähnten beiden Jänner-Wochen 2017 mit Sterberaten von 26,5 bzw. 26,1 höhere Werte erreicht. Die Sterblichkeitsrate in Relation zur Bevölkerung: Der aktuelle Linienausschlag in der Liniengrafik ganz rechts liegt nahe am Rekord der letzten 20 Jahre. Altersgruppe ab 65 außerhalb der Norm. Von den 2.286 zwischen 9. und 15. November (Kalenderwoche 46) registrierten Verstorbenen waren laut den Daten der Agentur für Gesundheit und Ernährungssicherheit (Ages) 417 mit dem Coronavirus infiziert. In der Woche darauf ist die Zahl der mit einer Covid-19-Infektion Verstorbenen noch einmal auf 469 angestiegen. Ob damit auch ein neuer Rekord bei den gesamten Sterbefällen erreicht wird, ist noch offen. Die Statistik Austria veröffentlicht ihre Statistik für die 47. Kalenderwoche am kommenden Donnerstag. Klar ist aber schon jetzt, dass der Anstieg der Todesfälle in der zweiten Welle deutlich außerhalb des normalen Rahmens liegt. Das zeigen Berechnungen der Wiener Landesstatistik, die auf Basis der Erfahrungswerte der vergangenen Jahre und der Bevölkerungsentwicklung für jede Kalenderwoche eine Bandbreite ermittelt, innerhalb derer sich die Zahl der Sterbefälle im Normalfall bewegen sollte. In der Altersgruppe 65 plus zeigt die Auswertung für ganz Österreich und für die meisten Bundesländer außer Vorarlberg deutliche Abweichungen nach oben. Auch das europäische Mortalitätsmonitoring Euromomo attestiert Österreich eine ""hohe Übersterblichkeit"" (High Excess Mortality). Alter der Infizierten steig weiter an. Schon mehrmals warnte der Gesundheitsminister Rudolf Anschober (Grüne) davor, dass das Alter der mit Corona infizierten Personen wieder ansteigt. Daten der Ampelkommission bestätigen das. So waren in der Kalenderwoche 40 noch 35 Prozent der Infizierten zwischen 40 und 64 Jahre alt, zehn Prozent waren über 65. In der Kalenderwoche 46, aus dieser sind die aktuellsten vorliegenden Daten, waren es 42 Prozent unter den 40- bis 64-Jährigen und 15 Prozent in der Altersklasse 65 plus. Und auch die Situation in Alters- und Pflegeheimen verschärft sich weiter. Laut der aktuellen Morgenmeldungen der Bundesländer sind aktuell 3.353 Personen in Heimen infiziert. Vor einem Monat lag diese Zahl bei 844 – sie hat sich also seit Ende Oktober vervierfacht. Seit Beginn der Pandemie starben 1.148 Heimbewohner und -bewohnerinnen mit dem Virus. Weniger Covid-Todesfälle in Schweden und Deutschland. Die von Kanzler Sebastian Kurz (ÖVP) noch im Frühjahr oft gehörte Aussage, dass Österreich ""besser als andere"" durch die Pandemie komme, gilt im Herbst somit nicht mehr. Sowohl in Deutschland als auch in Schweden ist die Zahl der Todesfälle mit Covid-Bezug deutlich geringer. In Deutschland waren es laut einer Aufstellung der Datenplattform ""Our World in Data"" zuletzt drei Tote pro Tag und Million Einwohner, in Schweden noch etwas weniger. Selbst Frankreich lag mit 8,6 zuletzt hinter Österreich mit täglich neun Covid-Toten pro Million Einwohner. Deutlich mehr Corona-Tote gab es zuletzt allerdings in den Nachbarländern Ungarn und Italien (je elf) und in Slowenien (13 pro Million Einwohner und Tag – Zahlen jeweils Wochendurchschnitt)";https://www.derstandard.at/story/2000122038540/zweite-welle-bringt-hohe-uebersterblichkeit-in-oesterreich;Standard;
29.01.2019;"Industriemechaniker: ""Von meinem Gehalt spare ich 2.000 Euro""";"Ein knapp 30-Jähriger erhält mit Zulagen monatlich 3.000 Euro netto. Mehr als die Hälfte spart er für eine Immobilie, dafür wohnt er noch bei seinen Eltern. ""Bereits in der Schule wusste ich, dass ich später keinen Bürojob machen will. Ich war nie der ruhige Typ, der lange am Tisch sitzen konnte, zudem wollte ich etwas Handwerkliches machen. Deshalb habe ich mit 15 eine Lehre als Industriemechaniker angefangen – in einem Vorarlberger Betrieb. Mittlerweile bin ich seit bald 15 Jahren in der Firma, ich wurde nach der Lehre übernommen. Als Industriemechaniker bin ich dafür verantwortlich, die Maschinen instandzuhalten, Fehler zu beheben und systematische Verbesserungen der Produktionsanlage durchzuführen. Ich arbeite Vollzeit, allerdings in Schichtdiensten. Das heißt, dass ich eine Woche von sechs Uhr in der Früh bis 14 Uhr und die folgende Woche dann von 14 bis 22 Uhr arbeite. Ab und zu habe ich auch Nachtschichten von 22 Uhr bis sechs Uhr morgens. Abgesehen von den Arbeitszeiten habe ich aber viele Freiheiten, kann zum Beispiel meine Tätigkeiten so ausführen, wie ich es für richtig halte. Hauptsache ist, dass die Maschinen einwandfrei laufen. Und obwohl die Firma groß ist, haben wir alle ein gutes Verhältnis, können auch Privates untereinander besprechen oder gehen nach der Arbeit etwas trinken. Es gibt selten Mitarbeiterwechsel, dadurch kenne ich die meisten, seit ich als Lehrling angefangen habe. Auch mit meinem Gehalt bin ich zufrieden. Derzeit komme ich monatlich auf etwa 3.000 Euro netto. Da ist schon die Schichtzulage samt 15 bis 20 Überstundenzuschlägen pro Monat dabei. Woanders würde ich nicht viel mehr verdienen, das ist ein durchschnittlicher Lohn für die Metallindustrie. Sparen für eigene Immobilie. Damit kann ich momentan gut leben: Ich habe keine Schulden, die ich zurückzahlen muss, und weil ich derzeit für eine eigene Immobilie spare, wohne ich trotz meiner bald 30 Jahre noch bei meinen Eltern. Derzeit ist es bekanntlich schwierig, eine geeignete und bezahlbare Immobilie zu finden. Meine Eltern wollen keine Miete, dafür helfe ich ihnen vor oder nach der Arbeit im familiären Betrieb. Die meiste Zeit verbringe ich aber mittlerweile bei meiner Freundin, die eine Eigentumswohnung hat. Deshalb zahle ich ihr für die Betriebskosten und sonstige Ausgaben 150 Euro pro Monat, außerdem 100 Euro für die Lebensmittel, die wir benötigen. Allfällige Kosten teilen wir. Meine sonstigen monatlichen Ausgaben belaufen sich auf 15 Euro für mein Handy, 35 Euro für meine Versicherung – die Altersvorsorge lege ich selber auf meinem Konto an – und circa 200 Euro für mein Auto inklusive Versicherungen und Sprit. Hinzu kommen noch 40 Euro für Kleidung, darunter fällt auch Arbeitskleidung, die ich mir selbst kaufen muss. Für Reisen gebe ich, aufs Monat verteilt, etwa 250 Euro aus. Heuer werde ich drei Wochen durch Südostasien reisen, das kostet natürlich mehr. Und auch sonst mache ich kurze Städtetrips. Andere kostspielige Hobbys habe ich eigentlich keine, außer dass ich ab und zu mal Ski fahre. Hin und wieder gehe ich mit meinen Freunden abends aus. Monatlich komme ich für Freizeitaktivitäten auf circa 200 Euro. Den Rest spare ich, wie gesagt, für eine Immobilie. Ich lege keinen fixen Betrag am Anfang des Monats auf die Seite, sondern das, was übrig bleibt – was im Monat durchschnittlich 2.000 Euro sind. Ich spare zwar, aber ich würde mich nicht als besonders sparsam bezeichnen. Zum Beispiel möchte ich nicht beim Urlaub aufs Geld schauen, und wenn ich ausgehe, ist es nicht so, dass ich mir das letzte Bier nicht leisten will. Keine Jobsorgen. Sorgen um meinen Job mache ich mir wegen der Digitalisierung keine. Ich sehe bei uns an den Produktionsanlagen, dass die Aufgaben derzeit noch viel zu komplex sind für Roboter – es wird also auch in 20 Jahren noch Mechaniker brauchen, um individuelle Problemstellungen zu lösen. Wahrscheinlich übernehmen Roboter künftig also einfache Tätigkeiten. Deshalb wird mein Beruf auch anspruchsvoller, teilweise zeigen sich erste Veränderungen bereits jetzt. Zum Beispiel müssen alle Arbeitsabläufe erfasst werden. In der Wartung bin ich also nicht nur mehr nur an der Maschine, sondern analysiere auch Daten am Computer. Etwa, wie häufig die Anlage im Jahr stillsteht und warum oder welche Fehler man wo und wie vermeiden könnte. Derzeit habe ich keine konkreten Pläne für meine berufliche Zukunft, aber ich könnte mir vorstellen, in einen anderen Aufgabenbereich zu wechseln. In unserer Firma ergeben sich immer wieder Möglichkeiten, aufzusteigen. Auch weil ich nicht bis ins hohe Alter als Mechaniker arbeiten will – es ist doch auch körperlich anstrengend. Aber solange ich jung bin, mache ich das gerne.""";https://www.derstandard.at/story/2000097111422/industriemechaniker-von-meinem-gehalt-spare-ich-2000-euro;Standard;Selina Thaler
12.06.2020;Laura Rudas ist wieder im Mediengeschäft – im Verwaltungsrat des Boulevardriesen Ringier;"Nach der Faymann-SPÖ an die Uni Stanford und zum umstrittenen Datenriesen Palantir, ist die Ex-Abgeordnete nun zuständig für digitale Kompetenz im Aufsichtsgremium des Schweizer Medienkonzerns. Sie ist zurück im Mediengeschäft, aber ganz woanders, als sie 2014 aufgehört hat: Laura Rudas, einst Bundesgeschäftsführerin der für sehr nutzenorientierte und machtbewusste Medienpolitik bekannten Faymann-SPÖ, wird demnächst Mitglied des Verwaltungsrats im zweitgrößten Schweizer Verlagskonzern Ringier mit der dort zweitgrößten Boulevardzeitung ""Blick"". Zwischen der vielkritisierten und auch belächelten SPÖ-Jungpolitikerin Rudas mit aufsehenerregenden ORF-Personalia wie einem wegen Widerständigkeit abgesetzten ORF-Infodirektor Elmar Oberhauser und einem beinahe als ORF-Generalsbürochef installierten Niko Pelinka aus Rudas' rotem Freundeskreis und nun der Ringier-Verwaltungsrätin aus Österreich liegt nicht allein beruflich Wesentliches. Stanford und Palantir. Die heute 39-Jährige hat ihre SPÖ-Funktionen und ihr Mandat Anfang 2014 zurückgelegt und ging nach Kalifornien, um an der so digitalen wie elitären Universität Stanford den ""Master of Science in Global Innovation and Leadership"" zu machen. Der ""Kurier"" schrieb damals, ihr Mann Markus Wagner habe ihr dazu geraten. Wagner hat sein 2000 gegründetes Mobilkommunikations-Start-up Xidris, fusioniert mit der 3 United AG, zu schönen Konditionen 2006 an Verisign in die USA verkauft und danach noch laut CV mit US-Medienkonzernen wie NBC, CBS, Fox und Clearchannel interaktive Angebote entwickelt. Wagner ist inzwischen, sehr grob gesagt, Tech-Investor, etwa mit i5 Invest (wo Österreichs Medienkonzerne und -macher von Dichand über Styria bis Russ in den vergangenen Jahren mit in digitale Hoffnungen investierten). Rudas wiederum ist seit ihrem Stanford-Studium Executive Vice President Strategy bei dem amerikanischen Softwareunternehmen Palantir Technologies. Palantir ist auf die Analyse von Big Data spezialisiert, so stellte Ringier seine neue Verwaltungsrätin diesen Mittwoch mit den Geschäftszahlen für 2019 vor. Palantir sorgt gerade für Schlagzeilen mit seiner Software ""Gotham"", die Europol seit 2017 für die ""operative Analyse von Daten zu Terrorismusbekämpfung"" einsetzt. Gotham verwenden auch Geheimdienste und Polizeibehörden in den USA. Und Palantir wie die Software sind, so formulierte es etwa der ""Spiegel"" gerade, ""umstritten"" – wegen persönlicher Verbindungen zu Cambridge Analytica und des Verdachts auf Datenabfluss in die USA. Digitale Kompetenz. ""Als Palantir-Verantwortlicher für Strategie und internationales Wachstum liegt ihr Fokus auf der Unterstützung von Vorständen globaler Firmen sowie Regierungsvertretern bei der digitalen Transformation"", erklärte Ringier sein neues Verwaltungsratsmitglied Laura Rudas. Rudas' Vorgängerin im Ringier-Verwaltungsrat war Christiane zu Salm, um 2000 Geschäftsführerin von MTV Deutschland und danach Gründerin und Gesellschafterin des Gewinnspielfernsehkanals 9 Live, bis 2018 war Salm verheiratet mit dem ProSieben-Gründungsmanager Georg Kofler. Neu im Ringier-Verwaltungsrat mit Rudas ist nun auch Roman Bargezi, Sohn von Ringier-Mitbesitzerin Evelyn Lingg-Ringier. Ringier-CEO und -Aktionär Marc Walder erklärt die Neuzugänge per Presseinfo so: ""Der Ausbau der digitalen Kompetenz auf Stufe des Verwaltungsrates ist ein deutliches Zeichen der Ringier-Aktionäre, konsequent an der Fortsetzung der digitalen Transformation der Unternehmensgruppe festzuhalten."" Neu unter den familiendominierten Ringier-Aktionären ist seit diesem Jahr der Schweizer Versicherungskonzern Mobiliar mit 25 Prozent, der auch gleich zwei neue Ringier-Verwaltungsräte stellt (den Mobiliar-Verwaltungsratspräsidenten und den CEO der Versicherungsgruppe). ""Heute""-Konkurrenz. Die Nummer eins in der Schweizer Massenpresse ist die Gratiszeitung ""20 Minuten"" und im Verlagsgeschäft nach Umsatz deren Mutterkonzern TX Group (bis vor wenigen Monaten noch als Tamedia branchenbekannt). Die TX Group ist inzwischen in Österreich an einem Medium beteiligt, das Faymänner in und aus der SPÖ gegründet beziehungsweise groß gemacht haben: der Gratiszeitung ""Heute"".";https://www.derstandard.at/story/2000118025060/laura-rudas-ist-wieder-im-mediengeschaeft-im-verwaltungsrat-des-boulevardriesen;Standard;
19.03.2016;Vernetzte Welt: Complexity-Science-Hub wird im Mai eröffnet;"Forschungszentrum bezieht Palais Strozzi in Wien-Josefstadt.Wien – Im Palais Strozzi in Wien-Josefstadt wird künftig die Welt der komplexen Systeme erforscht. Wie funktionieren  die zahlreichen Netzwerke, aus denen die Gesellschaft besteht, und kann man sie nach Analyse der von ihnen gelieferten Datenmengen so steuern, dass sie für die Menschheit optimale Ergebnisse bringen? Mit diesen Fragen wird sich der Complexity-Science-Hub beschäftigen, der in das Gebäude einzieht – als zweites Forschungszentrum nach dem Institut für höhere Studien (IHS) im vergangenen Jahr. Derweil sitzt nur der Generalsekretär vor Ort: Philipp Marxgut, zuletzt Wissenschaftsattaché in den USA, ist für die Geschäfte des Hub verantwortlich. Stefan Thurner, Professor für Complex Systems an der Med-Uni Wien, ist der wissenschaftliche Leiter. Gemeinsam versuchen sie, eine ""kritische Größe"" an Komplexitätsforschern nach Wien zu holen. Am Hub sind insgesamt fünf Unis und Forschungseinrichtungen beteiligt: die TU Wien, die TU Graz, die Med-Uni Wien, die WU Wien und das Austrian Institute of Technology (AIT). Das Institut für angewandte Systemanalytik IIASA wird in Kürze beitreten. Sie alle tragen jeweils zwei Laufbahnstellen für einen Senior- und einen Junior-Wissenschafter bei. Weitere Unterstützungen kommen vom Verkehrsministerium und von der Nationalstiftung. Simulationen als Abbild der Wirklichkeit. Thurner: ""Idealerweise bauen die Forscher hier kleine Gruppen auf, sodass wir irgendwann 30 bis 50 Wissenschafter an ihren Rechnern sitzen haben."" Wissenschafter, die mit mathematischem Gespür an große Datenmengen herangehen und Simulationen als Abbild der Wirklichkeit bauen. Sie sollen versuchen, Chancen aufzuzeigen, komplexe Systeme ""sehr viel besser als bisher zu verstehen und irgendwann auch zu steuern."" Wie das funktionieren könnte? Zunächst analysiert man auf Basis großer Datensätze die Netzwerke, die das System zusammenhalten. Danach überlegt man, ob man die beobachtbaren Phänomene dieser Systeme künstlich reproduzieren kann. Wenn das gelingt, kann man eventuell auch quantitative Vorhersagen machen, die dann mit Big Data veri- oder falsifizierbar sind. An dieser Stelle kann man auch überlegen, welche Einflüsse dieses Modell dann verändern könnten und zu welchen Konsequenzen das führt. Dem Forscher ist bewusst, dass der Umgang mit Daten heikel ist. ""Wir wollen in Workshops auf das Dilemma zwischen wissenschaftlichem Nutzen und Verlust von Privatheit aufmerksam machen – und arbeiten selbst nur mit anonymisierten Daten.""";https://www.derstandard.at/story/2000032972965/vernetzte-welt-complexity-science-hub-wird-im-mai-eroeffnet;Standard;
19.08.2020;Wenn Yoga-Lehrerinnen das 5G-Netz stoppen;"Die Gegner des Mobilfunkstandards 5G erzielen Erfolge in der Gemeindepolitik. Dabei zeigt sich: Wer erfolgreich agitiert, der kann auf Argumente gerne verzichten. Wer mit der ""ärztlich geprüften"" Yoga-Lehrerin Iris K. telefonisch Kontakt aufnehmen will, der muss eine Mobiltelefonnummer wählen. K. hat sich mit dem 4G-Netz gut arrangiert, aber jetzt ist Schluss mit lustig. Der geplante 5G-Netzausbau ist der Yogalehrerin ein Dorn im Auge, in ihrer Heimatgemeinde Geboltskirchen hat sie eine Bürgerinitiative initiiert und Unterschriften gesammelt, gegen den neuen Mobilfunkstandard und für eine gesunde Gemeinde. Das Anliegen gelangte in den Gemeinderat des Orts im Hausruckviertel, dieser sprach sich gegen ""gegen die Aufstellung und Anbringung von Sendeanlagen und sonstigen Anlagenteilen aus."" Dürfen die ÖBB nur mehr mit der Dampflok durch den Ort? Rechtlich ist das im besten Falle irrelevant und mit einem Gemeinderatsbeschluss vergleichbar, der den Österreichischen Bundesbahnen vorzuschreiben versucht, künftig nur noch mit der  Dampflok durch das Ortsgebiet fahren zu dürfen. Das Forum Mobilkommunikation weist darauf hin, dass politische Beschlüsse dieser Art auf Gemeindeebene rechtswidrig sein können. Top in der Agitation, Flop in Sachen Information? Doch wir sind neugierig und wollen wissen, worauf die Bedenken der Bürgerinitiative und der Gemeinde fußen. K. erklärt auf eine Anfrage per E-Mail, dass sie zunächst meine Intention hinter der Anfrage klären möchte, um dann für sich zu klären, ob es Sinn macht, ""ihre Ressourcen dorthin zu lenken."" Ich präzisiere: Ich schreibe zu dem Thema und hätte nur gerne Hinweise auf die Quellen oder Studien, auf die man sich im Geboltskirchener Widerstand gegen 5G beruft. Ich vermute, der dazu nötige ""Ressourceneinsatz"" ist der Initiatorin einer Bürgerinitiative zumutbar. Es folgt ein langes Telefonat, in dem K. durchklingen lässt, dass ""ihre Ressourcen"" für den ""reporterischen Eifer"" meinerseits, den sie ganz deutlich spüre, vermutlich nicht sinnvoll verwendet seien. Ich flehe fast um Quellen und Hinweise und buckle wie der ""dreibeinige Hund"" auf der Yogamatte, doch K. bleibt hart. Sie spüre, dass die Schwingungen in der Causa nicht so recht passen. Jetzt darf ich es verraten: Frau K. hat ihre Ressourcen letztlich nicht für die Stiftung Gurutest vergeudet. Der Giftschrank mit den wohl heißen Papieren zu 5G und Gesundheitsgefährdung, er bleibt verschlossen. Ein kleiner Trost: Wir stehen damit nicht alleine da. Dem Bürgermeister von Geboltskirchen, Friedrich Kirchsteiger, ist die Sache spürbar unangenehm. Der Funke mit dem Widerstand gegen 5G sei aus Bayern nach Oberösterreich übergesprungen, vermutet er. Quellen und Belege zur Gefährlichkeit von 5G sei K. auch ihm schuldig geblieben. Er selbst habe sich vor allem auf Wikipedia zum Thema informiert. Das Conclusio der Online-Enzyklopädie wäre freilich kein Wasser auf die Mühlen der Mobilfunkskeptiker. Wikipedia stellt unromantisch fest: ""Innerhalb der zugelassenen Grenzwerte gab es hingegen bei keiner der über 200 durchgeführten Studien auch nur Hinweise auf eine gesundheitsschädliche Wirkung."" Geboltkirchner Forscher schämt sich ein wenig für seine Gemeinde. Stefan Uttenthaler schüttelt den Kopf ob der Vorgänge in seiner Heimatgemeinde. Uttenthaler hat ein Doktorat in Physik und betreut beim Wissenschaftsfonds FWF Forschungsprojekte in Experimentalphysik, theoretischer Physik und Astrophysik. Er könnte in seiner Heimatgemeinde etwas Licht ins Dunkel bringen in der Sache der angeblich ungesund strahlenden, brummenden, surrenden und schwingenden 5G-Stationen. Der Physiker fasst die Sache nüchtern zusammen: ""Es ist kein physikalischer Mechanismus bekannt, der bei Einhaltung der Strahlungsgrenzwerte zu irgendwelchen gesundheitlichen Auswirkungen am Menschen führen könnte. Die von Mobilfunkgegnern oft behaupteten 'nicht-thermischen Effekte' wurden längst wissenschaftlich widerlegt. Heute noch über die Gesundheitsgefahr von 5G zu diskutieren ist ungefähr so plausibel, wie über die Gefahr von eckigen Augen durch zu viel Fernsehen zu diskutieren."" Es geht nicht um Diskurs, sondern um einen Angriff auf die Wissenschaft. Worüber sich Wissenschafter keine Illusionen machen sollten: Es geht im Widerstand gegen den Ausbau des 5G-Netzes nicht um wissenschaftliche Argumente. Es geht um einen Angriff auf die wissenschaftliche Diskussion per se. Der manifestiert sich im rührend-naiv anmutenden Geschwurbel einer Yoga-Lehrerin, die sich um die Gesundheit Sorgen macht und endet bei Verschwörungsplauderern, für die 5G eine Chiffre zur Eröffnung einer weiteren Front im imaginären Endkampf gegen die böse Elite ist. Die Fantasie der Plauderer kennt kaum Grenzen: 5G sei eine Mikrowellenstrahlung, die unser Gehirn grillt, wo auch immer das neue Netz in Betrieb genommen wurde und uns als ferngesteuerte Zombies versklavt. Vögel fielen tot vom Baum, wo auch immer 5G-Röstanlagen installiert wurden, und der Corona-Virus sei von den klandestinen Lenkern der Welt nur deswegen in die Welt gesetzt worden, um von den desaströsen Wirkungen der 5G-Strahlen ablenken zu können. Investigativer Journalist warnt: Man fällt um, wenn 5G kommt. In Österreich tingelt der ""investigative Journalist"" Steve Whybrow (aka Wyborova) von Corona-Demo zu Corona-Demo, um in drastischen - wenngleich nicht widerspruchsfreien - Rants vor 5G zu warnen. ""Wir wissen nicht, welche Frequenzen die nutzen werden, und wenn dann die Leute beginnen umzufliegen - wie gesagt 60 Gigaherz, das ist der Bereich, in dem die 5G nutzen wollen, da fällst Du um, wenn der Körper den Sauerstoff nicht mehr absorbieren kann."" Whybrow vermutet messerscharf: Der 5G-Tote wird dann als Corona-Toter gezählt, so ergänzten sich die Dinge wunderbar im Sinne der Neuen Weltordnung. Whybrow kleckert nicht in Sachen jener Mission, die er zu erfüllen angetreten ist: In Wien brüllt er bei einer der Demos in die Menge, dass er ""seit 10 Jahren in harter Forschung"" an einer Frage arbeite: ""Wie kann ich die ganze Elite stürzen"". Hie und da legen besorgte 5G-Gegner tatsächlich Hand an, um wenigstens ein paar Funkmasten zu stürzen, um damit den Unbill der 5G-Teufelsfrequenz abzuwehren. Vor allem in Großbritannien und in den Niederlanden gab es in den vergangenen Monaten Dutzende Sabotageakte gegen Telekommunikationsanlagen, Mitarbeiter von Mobilfunkunternehmen wurden von ""Aktivisten"" attackiert. Auch andere Gemeinden engagieren sich. Geboltskirchen ist nicht die einzige Kommune, die sich gegen 5G positioniert. In Kärnten machen FPÖ-Politiker in fünf Gemeinden mobil. Auch in den oberösterreichischen Gemeinden Kallham und Pötting reagiert die Politik auf besorgte Bürger. Die Gemeinden versichern, man wolle anstelle von 5G den Ausbau des Glasfaserkabelnetzes forcieren. Das wird die künftigen Nutzer mobiler Technologie, in der smarten Landwirtschaft und die Nutzer autonomer Fahrzeuge freuen. Die erhalten dann von der Gemeinde vermutlich eine große Kabelrolle für Ausflüge ins finstere Tal.";https://www.derstandard.at/story/2000119200593/wenn-yoga-lehrerinnen-das-5g-netz-stoppen;Standard;Christian Kreil
30.06.2020;War der fortgesetzte Lockdown wirklich nötig?;"Zwei neue Bücher wagen einen kritischen Blick auf die frühen Corona-Monate. Wien – Als das Coronavirus vergangenen Dezember in China epidemisch ausbrach, sei es von Österreichs Politikern und den meisten Experten wenig ernst genommen worden. Als sich das Infektionsgeschehen dann nach Europa verlagert hatte und sich die Seuche in der Lombardei zur ""Freak-Wave"" aufbäumte, habe man hingegen radikal und richtig reagiert. Die Maßnahmen, um direkte Kontakte auf allen gesellschaftlichen Ebene einzuschränken, seien alternativlos gewesen. Dann jedoch, zwei Wochen später, sei die Sache aus dem Ruder gelaufen. Statt erste Exit-Schritte aus dem Lockdown zu überlegen, wie es einer Mehrheitsmeinung von Experten aus der Coronavirus-Taskforce des Gesundheitsministeriums entsprochen habe, seien die Social-Distancing-Regeln nochmals drastisch verschärft worden: Maskenpflicht und Angstparolen von Kanzler Sebastian Kurz statt kalmierender Aufklärung – dabei habe sich das tägliche Infektionsplus schon damals stark eingebremst gehabt, von 30 Prozent Ende März auf zwölf Prozent. Kompakte journalistische Kost. So weit die Kernaussage von zwei in diesen Tagen erscheinenden Büchern, die – beide in Form einer Chronologie – einen ersten kritischen Rückblick auf die frühen Corona-Monate in Österreich wagen. Das eine, verfasst von einer Autorengruppe rund um den Chefredakteur der Rechercheplattform ""Addendum"", Michael Fleischhacker, liefert kompakte journalistische Kost. Es bietet viel Hintergrundinformation, etwa über die desorganisierten Zustände im Gesundheitsministerium nach dem türkis-blauen Umbau. Auch die für die Lockdown-Fortsetzung vielleicht relevanten Verflechtungen von Kurz-Vertrauten mit der Strategieberatung Boston Consulting Group sind interessant. Sprengers Tagebuch. Das andere Buch, veröffentlicht von dem vorzeitig aus der Gesundheitsministeriums-Taskforce ausgeschiedenen Mediziner und Public-Health-Experten Martin Sprenger, ist persönlicher. Es fasst dessen redigierte Tagebucheintragungen aus den dramatischen Wochen zusammen. In diesen befleißigt sich Sprenger eines dem Public-Health-Ansatz entsprechenden soziologischen Blicks auf die Folgen des Lockdowns in der Gesellschaft. Ihre zerstörerischen Auswirkungen, von Vereinsamung hin zu Existenzzerstörung, könnten den Verlust an Lebensjahren durch das Virus um einiges übertreffen, schreibt er. Im Expertenrat war er mit dieser wichtigen Sichtweise offenbar ziemlich allein. Frustrierende Verschärfung. Entsprechend frustriert reagierte der Tiroler am 30. März, als Kanzler Kurz sein Worst-Case-Szenario mit bis zu 100.000 Toten verkündete und weitere Verschärfungen dekretierte. Ob jedoch zu diesem Zeitpunkt wirklich eine Mehrheit von Taskforce-Experten Lockerungen das Wort redete, kann bezweifelt werden. Dem Protokoll zumindest ist das nicht zu entnehmen.";https://www.derstandard.at/story/2000118392320/war-der-fortgesetzte-lockdown-wirklich-noetig;Standard;Irene Brickner
24.04.2020;Das Hubble-Teleskop: Vom Flop zur Ikone der Popkultur;"Das populärste aller Weltraumteleskope hat unser Bild vom Universum für immer verändert. Du weißt, dass du es geschafft hast, wenn dich die Welt unter deinem Vornamen kennt, verkündete die NASA einmal. Und erhob damit das Hubble Space Telescope – oder eben schlicht Hubble – in den Rang einer popkulturellen Ikone wie Elvis oder Cher. Andere Teleskope, ob im Weltraum oder am Boden, mögen für ihre Leistungsfähigkeit gewürdigt werden. Hubble aber wird geliebt. Stotterstart. Einer der Gründe dafür ist die uramerikanische Geschichte vom Loser, der es doch noch schafft. Nachdem das Teleskop vor genau 30 Jahren, am 24. April 1990, in den Weltraum gebracht worden war, bereitete es als erstes nämlich eine gewaltige Enttäuschung. Wegen eines Konstruktionsfehlers am 2,4 Meter großen Hauptspiegel lieferte es nicht wie erhofft kosmische Panoramen von bis dato ungeahnter Qualität, sondern nur unscharfe Bilder. Spott und Hohn folgten – wer beim nächtlichen Zappen über eine Wiederholung des Leslie-Nielsen-Films ""Die nackte Kanone"" stolpert, wird in einer Szene sehen, wie Bilder des Hubble-Teleskops, der Titanic und der Hindenburg an der Wand eine Galerie des Scheiterns bilden. Aber diese kurze Phase der Schmach hat Hubble längst hinter sich gelassen. Die Wende brachte eine Mission im Jahr 1993: Die NASA schickte das Space-Shuttle Endeavour zum Teleskop und ließ die Astronauten eine aufwendige Reparatur durchführen. Der Spiegelfehler konnte mit einer Hilfskonstruktion weitestgehend ausgeglichen werden – dem Teleskop wurde gewissermaßen""eine Brille aufgesetzt"", wie es damals hieß. Vier weitere Servicemissionen, die letzte davon 2009, versahen das Teleskop mit zusätzlichen Upgrades. Und seitdem liefert Hubble atemberaubende Bilder in stetem Strom. Seine Aufnahmen reichen von unserer unmittelbaren Nachbarschaft, also den anderen Himmelskörpern in unserem Sonnensystem, bis zum ""Rand"" des Universums: Aufnahmen wie das Hubble Deep Field oder das Hubble Ultra-Deep Field zeigen Himmelsregionen, aus denen das Licht 12 bis 13 Milliarden Jahre zu uns unterwegs war. Sie sind damit Fenster in eine Zeit nicht allzu lange nach dem Urknall selbst. Bis heute hat das Teleskop etwa 1,3 Millionen Beobachtungen gemacht, ein Ende ist nicht in Sicht. Und auch das hat zu Hubbles Popularität beigetragen: Es hört nicht auf zu liefern. Menschen schätzen es, wenn technische Geräte über ihre ursprünglich anvisierte Lebensdauer hinaus wacker weiterarbeiten, wie die Mars-Rover Spirit und Opportunity gezeigt haben. Nach Hubble wurden noch Dutzende weitere – kleinere und/oder auf bestimmte Wellenlängenbereiche spezialisierte – Weltraumteleskope ins All gebracht. Und viele davon sind schon längst wieder außer Betrieb, während Hubble weitermacht. Einmal wurde kurz überlegt, es aus Budgetgründen stillzulegen – es folgte ein öffentlicher Aufschrei, und der Plan wurde schnell wieder in einer Schublade versenkt. Einige tausend Studien bauen auf Hubble-Beobachtungen auf, doch das Teleskop gehört nicht allein der Wissenschaft. Die Breitenwirksamkeit der Bilder Hubbles beruht nicht zuletzt darauf, dass das Teleskop ein in jeder Beziehung buntes Universum zeigt. Während man sich früher den Kosmos als Schwärze voller kleiner Lichtpunkte vorstellte, enthüllte Hubble ein Feuerwerk aus wabernden Nebeln und bunt schillernden Supernova-Überresten. Die Farben, die wir auf den veröffentlichten Fotos zu sehen bekommen, entsprechen freilich nicht ganz dem, was das Teleskop tatsächlich sieht: Hubble nimmt ein breiteres Spektrum wahr als das menschliche Auge, bis in den nahen Infrarot- und Ultraviolettbereich hinein. Die Originalbilder müssen also etwas bearbeitet werden, um ""ihre Essenz zu erhalten"", wie es Ray Villard vom Space Telescope Science Institute ausdrückt. Das berühmteste dieser Bilder hat als ""Säulen der Schöpfung"" Geschichte geschrieben. Die 1995 gemachte Aufnahme zeigt einen Ausschnitt des 7.000 Lichtjahre von uns entfernten Adlernebels und ist zu einer Ikone des Weltraumzeitalters geworden – wie auch das Teleskop selbst. Hubble-Bilder zieren nicht nur das übliche Merchandising wie T-Shirts und Tassen. Sie wurden auch schon auf E-Gitarren, Plattencovern und gewagten Kreationen auf der New York Fashion Week gesichtet. ""Hubble ist vollkommen von der Popkultur absorbiert worden"", sagt David Leckrone, der langjährige Chefwissenschafter des Hubble-Teams. Ob das stets als ""designierter Nachfolger"" Hubbles gehandelte James-Webb-Teleskop einen vergleichbaren Kultstatus erreichen wird, steht in den Sternen. Ganz kann es ohnehin nicht in die Rolle Hubbles schlüpfen, da es fast ausschließlich Beobachtungen im Infrarotbereich machen wird – das jedoch erheblich leistungsstärker als Hubble. Sein Start hat sich aber schon über mehrere Jahre hinweg immer wieder verzögert (aktueller Planungsstand: Frühling 2021). Und schon jetzt steht fest, dass Hubble weiterlaufen wird, auch wenn sein ""Nachfolger"" im All ist. Wann kommt das Ende? Nicht die Energieversorgung setzt der Lebensdauer des mit Solarpaneelen ausgestatteten Teleskops eine Grenze, sondern sein vergleichsweise niedriger Orbit. Hubble kreist in knapp 550 Kilometern Höhe. Dort gibt es immer noch einen zwar extrem dünnen Rest von Atmosphäre, der aber ausreicht, um das Weltraumteleskop im Lauf der Zeit abzubremsen und dadurch in einen immer niedrigeren Orbit zu zwingen. Wird nichts unternommen, wird es unweigerlich irgendwann abstürzen. Exakt vorhersagen lässt sich das nicht, es könnte noch in den 2020ern dazu kommen, oder auch erst in den 2040ern. Die vorerst letzte Chance, Hubble aufzusammeln und ihm einen Ehrenplatz in einem Museum auf der Erde einzuräumen, ist mit dem Ende des Space-Shuttle-Programms verstrichen. Derzeit gibt es nichts, mit dem sich der 13 Meter lange und 11 Tonnen schwere Zylinder zur Erde zurücktransportieren ließe. Das könnte sich durch das verstärkte Auftreten privater Raumfahrtbetreiber aber noch ändern. Einfacher wäre eine Mission, um das Teleskop wieder ein Stück anzuheben, wie es auch die ebenfalls stetig sinkende Internationale Weltraumstation immer wieder tun muss. Noch gibt es keine konkreten Pläne für eine solche mit erheblichen Kosten verbundene Mission, aber die Idee wurde bei der NASA zumindest schon einmal ins Gespräch gebracht. Hubble mag man eben.";https://www.derstandard.at/story/2000117070015/das-hubble-teleskop-vom-flop-zur-ikone-der-popkultur;Standard;
20.08.2018;Leseanleitung zum AMS-Algorithmus;"Das AMS wird die Chancen von Arbeitslosen künftig mit einem Algorithmus bewerten. Jobsuchende werden in drei Kategorien eingeteilt. Eine Frau, Mitte 40, mit zwei Kindern verliert nach vielen Jahren ihren Arbeitsplatz in Wien-Simmering und meldet sich beim Arbeitsmarktservice AMS. Was der Algorithmus nun leisten soll, ist, anhand ihrer Daten zu berechnen, wie hoch die Wahrscheinlichkeit ist, dass sie binnen sieben Monaten einen Job findet, in dem sie mindestens drei Monate bleibt. Jobsuchende, bei denen die Chance über 66 Prozent liegt, haben gute Perspektiven am Jobmarkt. Statt eines menschlichen Beraters übernimmt die Einschätzung ein Programm. Abgebildet sind alle Variablen, so wie sie laut AMS in die Rechnung einfließen. Ohne dass noch individuelle Daten berücksichtigt werden, liegen die Chancen auf rasche Reintegration am Arbeitsmarkt bei 52 Prozent. Als Grundlage für diese Berechnung dient ein junger, gesunder Mann mit österreichischer Staatsbürgerschaft, der im Dienstleistungsbereich arbeiten will und nur über einen Pflichtschulabschluss verfügt. Wohnhaft ist der Mann in Bregenz oder Amstetten, einem Ort, mit sehr guter Perspektive am Jobmarkt. Der Basiswert einer solchen Referenzperson liegt bei 0,10. Was geschieht, wenn nun alle anderen Merkmale gleich bleiben, aber eine Frau beim AMS vorstellig wird? Dann muss vom Ausgangswert, also den 0,10, etwas abgezogen werden, und zwar 0,14. Das Ergebnis dieser Subtraktion fließt als neue Basiszahl in eine komplexe Wahrscheinlichkeitsrechnung im Hintergrund ein. Das Ergebnis: Bleibt alles andere gleich, liegen die Chancen für die Frau, am Jobmarkt rasch vermittelt zu werden, bei nur 49 Prozent. Ersichtlich wird, dass jede Variable einen unterschiedlich starken Einfluss hat. Der Abzug bei einem Arbeitslosen über 50 ist deutlich höher und wirkt sich entsprechend aus. In der Gruppe 50 plus liegt die Wahrscheinlichkeit auf rasche Vermittlung am Jobmarkt bei nur mehr 35 Prozent. Wird nun eine über 50-jährige Frau bewertet, ergibt das einen doppelten Abzug. Ihre Chancen liegen nur noch bei 32 Prozent. Dagegen gibt es ein Plus und damit höhere Chancen für Menschen mit Lehre und Matura. Ein abgeschlossenes Studium bringt keinen Bonus bei der Bewertung der Perspektiven. Für Kritik sorgt, dass in der Beurteilung der Chancen auch nicht beeinflussbare Variablen miteinfließen wie das Geschlecht. Ebenso wird kritisiert, dass Betreuungspflichten zu einer Chancenverschlechterung führen – aber nur bei Frauen. Das System ""erkennt"" die Betreuungspflichten daran, ob jemand in Karenz war oder eine Geburt hatte. Bei Synthesis heißt es, dass man intensiv geprüft habe, dieses Kriterium auch für Männer zu berücksichtigen. Nur hätten bei Männern Betreuungspflichten statistisch gesehen keine Folgen für die Jobperspektive. Eine ganz wichtige Rolle bei der Jobsuche spielt die Frage, wo jemand Arbeit finden will. Die Erbauer des Algorithmus bei Synthesis unterscheiden in fünf Regionen, und zwar je nachdem, bei welcher AMS-Geschäftsstelle man sich arbeitslos meldet. Für diese Regionen stehen die Abkürzungen ""RGS_Typ"". Die miserabelste Perspektive haben Jobsuchende aus Simmering, Favoriten, Floridsdorf (Typ 4). Sucht jemand aus diesen Wiener Bezirken einen Job, sinken die Chancen bei sonst gleich bleibenden Variablen auf 33 Prozent. Für gesundheitlich Beeinträchtigte sind die Chancen schlechter, für Menschen, die in der Industrie Jobs suchen, dagegen besser. Eine Reihe an Variablen bewerten schließlich die Vergangenheit eines Arbeitssuchenden: Wenige Beschäftigungstage in den vergangenen vier Jahren sorgen für geringere Chancen. Dafür ist es ein Vorteil, wenn man in dieser Zeit öfter beim AMS war (""Frequenz-Geschäftsfall"") und viel gearbeitet hat. Damit wird die bessere Perspektive von Menschen abgebildet, die zwar öfter arbeitslos werden, aber rasch etwas finden. Wer lange arbeitslos war in der Vergangenheit (""Geschäftsfall lang"") und öfter an AMS-Qualifikationsmaßnahmen teilnahm (""Teilnahme 1-3""), hat laut Erfahrungen wiederum etwas schlechtere Karten. Als Basis für die Wiener GmbH Synthesis Forschung, die den Algorithmus entwickelt hat, dienen Daten zu AMS-Kunden aus der Vergangenheit. 2017 etwa waren 900.000 Menschen zwischenzeitlich arbeitslos gemeldet. Die Erfahrungen mit diesen Kunden füttern den Computer und liefern Informationen darüber, bei welcher Gruppe wie schnell eine Vermittlung gelingt. Die Trefferquote der Prognosen liegt bei 85 Prozent. Synthesis nützt nicht nur diese Berechnung. Es gibt ein Modell, das die Langzeitperspektive analysiert, also bewertet, wie hoch die Wahrscheinlichkeit ist, dass es jemandem binnen zweier Jahre gelingt, für mindestens sechs Monate in Beschäftigung zu kommen. Als Basis dafür dienen dieselben Variablen, also Alter, Geschlecht usw. Sie werden anders gewichtet. Die Gewichtung der Variablen ändert sich zudem mit der Zeit. Kommt also ein Arbeitsloser zum AMS, erfolgt automatisch ein Update bei der Perspektivenberechnung nach drei Monaten. Mit der Zeit ist es bei der Beurteilung der Langzeitperspektive von Vorteil, eine Frau zu sein. Insgesamt verwendet Synthesis 96 verschiedene Modelle.";https://www.derstandard.at/story/2000089720308/leseanleitung-zum-ams-algorithmus;Standard;András Szigetvari
16.08.2017;Shanghai-Ranking: Nur vier heimische Unis unter Top 500;"Die besten drei Hochschulen kommen aus den USA und Großbritannien – Uni Wien und Innsbruck auf Rängen 151–200. Wien – Erneut dominieren beim Shanghai Academic Ranking of World Universities US- und britische Unis: Die Top 10 setzen sich rein aus Hochschulen aus diesen beiden Ländern zusammen. Mit der Uni Wien und der Uni Innsbruck schafften es heuer zwei heimische Hochschulen in die Gruppe mit den Rängen 151–200. Nur zwei weitere Austro-Unis finden sich unter den Top 500. Das ist eine weniger als im Vorjahr. Nachdem bereits 2016 die Uni Graz aus der Gruppe der ersten 500 gefallen ist, folgt ihr in der diesjährigen Ausgabe auch die Medizin-Uni Graz, die sich im vergangenen Jahr noch auf den Rängen 401–500 befand (ab Platz 101 wird nur mehr in 50er-Schritten gerankt, ab Platz 201 in 100er-Schritten). Für die gereihten österreichischen Unis gab es wie in den Jahren davor Mittelfeld-Plätze: Hinter der Uni Wien und Innsbruck landeten die Medizin-Uni Wien (201–300) und die Technische Uni Wien (401–500) rangmäßig unverändert gegenüber 2016. ETH Zürich auf Platz 19. Die vorderen Plätze machen sich – wie regelmäßig bei allen internationalen Hochschulrankings – wieder US-Unis aus: Wie schon 2016 landete die Harvard University an der Spitze, gefolgt von der Stanford University. Auf dem dritten Platz folgt diesmal mit der Cambridge University allerdings die erste europäische Hochschule (2016: Platz vier). Gegenüber dem Vorjahr unverändert liegt die Uni Oxford auf Rang sieben. Als erste kontinentaleuropäische Uni kommt die ETH Zürich auf den 19. Rang. Auf Platz 30 liegt die Universität Kopenhagen. Die erfolgreichsten deutschen Hochschulen sind laut Shanghai Ranking die Uni Heidelberg (Platz 42) und die Technische Uni München (Rang 50). Das stark forschungsorientierte Shanghai-Ranking basiert ausschließlich auf öffentlich verfügbaren Datenbanken. 60 Prozent der Bewertung basieren auf der Zahl wissenschaftlicher Publikationen und Zitierungen, Veröffentlichungen in den Magazinen ""Science"" oder ""Nature"" zählen dabei besonders stark. 30 Prozent des Ergebnisses beruhen auf der Anzahl der Nobelpreis- und Fields-Medaillen-Gewinner, die an der jeweiligen Uni studiert oder gelehrt haben, und zehn Prozent auf der Produktivität pro Forscher.";https://www.derstandard.at/story/2000062711657/shanghai-ranking-nur-vier-heimische-unis-unter-top-500;Standard;
29.08.2016;"Zwei Drittel aller Mathematiker entstammen 24 ""Familien""";"Analysen mit Daten des Mathematics Genealogy Project, der größten Datenbank zur Geschichte des Fachs, kommen zu erstaunlichen Ergebnissen. Fargo – Unter all den verschiedenen Stämmen von Forschern ist jener der Mathematiker zweifellos einer der interessantesten. Ihm gehören nicht nur einige der hellsten Köpfe, sondern viele der größten Exzentriker in der Geschichte der Wissenschaften an. Interessant ist aber auch, wie die Fachvertreter selbst ihre geistigen Verwandtschaften und Genealogien beschreiben. Ein origineller Gradmesser ist die sogenannte Erd?s-Zahl, benannt macht dem aus Ungarn stammenden Mathematik-Genius Paul Erd?s (1913–1996). Erd?s war einer der produktivsten Mathematiker des 20. Jahrhunderts und publizierte gemeinsam mit über 500 verschiedenen Wissenschaftern. Die Erd?s-Zahl gibt an, wie viele Autoren ein Forscher von einer Publikation mit Erd?s ""entfernt"" ist. Durchschnittliche Erd?s-Zahl von 4,65. Er selbst hat die Zahl 0, die rund 500, die mit ihm veröffentlichten, die Zahl 1. 268.000 Wissenschafter, für welche im Rahmen des Erd?s-Zahl-Projektes ein endlicher Wert ermittelt werden konnte, haben eine durchschnittliche Erd?s-Zahl von 4,65. Dies rührt nicht zuletzt auch daher, dass Erd?s in vielen Teilbereichen der Mathematik gearbeitet hat. An der North Dakota State University gibt es eine etwas andere Mathematiker-Datenbank: das Mathematics Genealogy Project (MGP). Sie widmet sich der Geschichte des Fachs anhand seiner Vertreter, geht bis ins 15. Jahrhundert zurück und ist mit den Einträgen von über 200.000 Mathematikern, ihren jeweiligen Doktorvätern oder -müttern sowie ihren eigenen Dissertanten als ""Nachfahren"" die größte einschlägige Datenbank. 84 mathematische ""Familien"". Eine neue Auswertung hat nun ergeben, dass sich nahezu alle Fachvertreter genealogisch auf nur 84 wissenschaftliche ""Ur-Familien"" (im übertragenen, nicht im biologischen Sinn) rückverfolgen lässt, zwei Drittel der Mathematiker überhaupt nur auf 24, wie ein Team um Floriana Gargiulo herausgefunden hat, die an der belgischen Universität von Naumur Netzwerkanalysen betreibt und für ihre Berechnungen ein eigenes Software-Programm erstellte. Mit anderen Worten: Viele der heute tätigen Mathematiker gehen in ihrem ""Dissertations-Stammbaum"" laut Gargiulos Analysen letztlich auf Leibniz, Euler oder Gauß zurück. Der wichtigste Stammvater unter den 24 ""Familien"" ist aber kein Mathematiker, sondern der italienische Mediziner Sigismondo Polcastro, der im frühen 15. Jahrhundert an der Universität Padua lehrte. Er hat laut den Analysen über 56.000 ""Nachkommen"". Die Zentren der Mathematik. Gargiulo und ihre Kollegen analysierten anhand der Anzahl der Mathematik-Dissertationen aber auch, welche Länder zu welcher Zeit Zentren des Fachs waren. Und dabei zeigte sich, dass bereits rund um 1920 (und nicht erst nach dem Zweiten Weltkrieg) die USA zur Mathe-Supermacht aufstiegen und dabei Deutschland beerbten. Ihre Berechnungen machen aber auch offensichtlich, dass Österreich-Ungarn bis zum Ersten Weltkrieg noch eine echte Hochburg der Mathematik war, was womöglich in der relativ starken Stellung des Fachs in Österreich bis heute nachwirkt.";https://www.derstandard.at/story/2000043421848/zwei-drittel-aller-mathematiker-entstammen-24-familien;Standard;Klaus Taschwer
26.02.2019;"""Star Trek"", Super-KI, Aliens: Wo bleibt der Universalübersetzer?";"Mein Großvater sprach sechs Sprachen. Fließend. Deutsch, Spanisch, Ungarisch, Englisch, Rumänisch, Französisch. Letzteres nicht so gern, weil es mit seinem geliebten Spanisch interferierte. Verdrehen Sie jetzt nicht die Augen, er war kein Snob. Aber in dieser Hinsicht war er Perfektionist. Denn er erkannte früh, dass ihm Sprachen die Tür zur Welt öffnen. Menschlich wie beruflich.

Das war vor dem Internet. Und bevor Star Trek das Konzept eines Universalübersetzers populär machte. Mithilfe eines ""Communicators"" (siehe Replikat oben) und künstlicher Intelligenz (KI) im Hintergrund können sich Kirk, Spock und Co fließend mit jeder Spezies verständigen, ohne deren Sprache zu verstehen. So weit der Weg dorthin angesichts holpriger Google-Übersetzungen und Siri-Stotterer noch erscheint, das Fundament dafür wird bereits gelegt.
Erste Sprechversuche

Befinden Sie sich im Ausland, können Sie schon heute auf Instant-Übersetzer wie den Microsoft Translator zurückgreifen. Hat man eine solche App installiert, kann man in sein Handy sprechen, und eine Computerstimme übersetzt mit nur wenigen Sekunden Verzögerung in mehrere Sprachen. Ebenso kann man die Antwort seines Gegenübers entschlüsseln. Das Start-up Mymanu bietet ein solches System gleich kompakt als Ohrstöpsel namens Clik an. Und bei den Olympischen Spielen 2020 will Japans nationales IT-Institut mittels Voicetra-App Millionen Besuchern die Verständigung erleichtern.

Aber seien Sie gleich gewarnt: Es könnte Ihnen trotz digitaler Dolmetscher passieren, dass Ihnen anstatt veganer Suppe ein Oktopus serviert wird, wenn Sie mithilfe eines dieser Instant-Übersetzer bestellen. Denn wir befinden uns erst am Anfang der maschinellen Kommunikationsrevolution, und gerade im Zusammenspiel mit Spracheingabe lässt die Erfolgsrate noch zu wünschen übrig.
Flourish logoA Flourish data visualization
Grafik: Die laut Ethnologue weltweit am häufigsten gesprochenen Sprachen. Hochentwickelte Übersetzungs-KI könnte zum Erhalt gefährdeter Sprachen beitragen. Unter den 7.000 aktiven Sprachen dominieren lediglich eine Handvoll. KI könnte es ermöglichen, Sprachbarrieren zu überwinden, ohne seine Muttersprache vernachlässigen zu müssen. Fraglich ist, ob der technologische Fortschritt oder das Aussterben der Sprachen schneller voranschreitet. Linguisten schätzen, dass bis 2100 bis zu 90 Prozent aller heute gesprochenen Sprachen verschwinden könnten.
Maschinen, die nichts verstehen

Egal ob Textübersetzer wie Deep L oder Programme mit Spracheingabe, alle gegenwärtigen Systeme haben gegenüber menschlichen Dolmetschern einen großen Haken: Sie können zwar Sprache erfassen und übersetzen, die transportierten Inhalte verstehen sie aber nicht. Dass ein Hund ein treuerer Freund als eine Katze ist, weiß Google Translate genauso wenig, wie dass Veganer keinen Oktopus essen. Und so kommt es, dass Maschinenübersetzer heute eine Hilfe, aber kein adäquater Ersatz fürs Sprachenlernen sind. Diese ""Unwissenheit"" maschineller Übersetzer führt nicht selten zu Fehlern, die uns gerade deshalb amüsieren, weil sie für unseren Verstand so kurios erscheinen. Nehmen Sie beispielsweise diesen über Twitter geteilten Gag von Bill Murray:

""I bet giraffes don't even know what farts smell like.""

Übersetzt man das Zitat mit Google Translate auf Deutsch, kommt das dabei heraus:

""Ich wette, Giraffen wissen nicht einmal, wie Furzgeruch aussieht.""

Für maschinelle Übersetzer ist Sprache nicht mehr als ein Konstrukt. Ähnlich wie die Einzelteile eines Bauwerks und kein Mittel zur Kommunikation. Bedeutung und Intention fallen gänzlich unter den Tisch. ""Der Mensch hat sehr viel Wissen über die Welt und was in der Welt möglich ist und was nicht möglich ist. Und die Effizienz von Sprache basiert darauf, dass dieses 'Weltwissen' in jedem Moment verwendet werden kann, um Sprache zu interpretieren. Der Computer versteht die Welt nicht und kann deswegen Sprache nicht interpretieren, sondern muss eine Übersetzung des Inhalts ohne Verständnis des Inhalts durchführen"", erklärt Peter Schüller, KI-Forscher und Linguist an der TU Wien, im Interview mit dem STANDARD.
Rasante Entwicklung

Dennoch ist die Geschwindigkeit, mit der die Entwicklungen aktuell voranschreiten, erstaunlich. Seit 60 Jahren wird an Übersetzungssystemen geforscht, doch bis vor kurzem war etwa die sinngemäße Maschinenübersetzung vom Japanischen ins Englische kaum möglich. Laut Google-Entwickler Melvin Johnson wurden durch den Wechsel auf sogenannte künstliche neuronale Netze und selbstlernende Algorithmen um das Jahr 2016 herum quasi über Nacht mehr Fortschritte erzielt als in den zehn Jahren davor, wie er in einem Vortrag für die Universität Stanford schildert.

""Ein neuronales Netz bekommt als Eingabe einen Text und fährt wie das menschliche Auge von Anfang des Texts bis zum Ende über die Worte. Jedes Wort wird in seiner Umgebung (also die Worte davor und danach) klassifiziert, und so entsteht eine Tabelle mit Zahlen für jedes Wort. In einem zweiten Schritt fährt ein weiteres neuronales Netz über diese Folge von Tabellen und sagt voraus, welches Wort in der Zielsprache ausgewählt werden soll"", erläutert Schüller den Vorgang.

Wenn Sie jetzt ausgestiegen sind, grämen Sie sich nicht. Denn laut dem Computerlinguisten kreieren Entwickler dieser Systeme zwar die Methodik und den Aufbau. Was die riesigen Zahlentabellen bedeuten und wie sie zu den Ergebnissen gelangen, wissen sie jedoch selbst nicht. Entscheidend ist: Neuronale Netze erkennen selbstständig Muster, die vom Menschen nicht vorgegeben werden müssen. Das heißt, ein neuronales Netz lernt, was in der Eingabe wesentlich ist und was passieren soll. Und für die stetige Verbesserung sorgt ein Algorithmus, der ohne menschliches Zutun selbst lernt. Das bedeutet: Google Translate weiß vielleicht auch künftig nicht, dass Furze riechen, aber die erfolgreiche Übersetzung des Gags ist nur eine Frage der Zeit.
Entwickler Melvin Johnson erklärt, wie maschinelle Übersetzung auf Basis neuronaler Netze funktioniert und weshalb die Technologie besser funktioniert als die Methoden in den Jahrzehnten davor.
stanfordonline
Grundlegende Probleme

Der enorme Bedarf an schnellen und präzisen Übersetzungssystemen sorgt für eine laufende Verbesserung dieser neuronalen Übersetzer. Allein über Google Translate werden täglich mehr als 140 Milliarden Wörter übersetzt. Mit ein Grund dafür: 50 Prozent der gesamten Informationen im Internet sind nur in englischer Sprache erhältlich, dabei spricht nur ein Siebentel der Weltbevölkerung Englisch. Ein Großteil der Menschen hat allein dadurch nur beschränkten Zugang zum weltweit publizierten Wissen.

Ein Ungleichgewicht, das wiederum auch ein Hindernis für die Entwicklung leistungsfähiger Maschinenübersetzer darstellt. Denn für ein effektives Training neuronaler Netze werden tatsächlich gewaltige Mengen an sogenannten Paralleltexten benötigt. Also Texten, die bereits von Menschen korrekt übersetzt in mehreren Sprachen existieren. IT-Giganten wie Google, Facebook und Microsoft saugen hierfür wortwörtlich das Internet ab und lesen hierfür veröffentlichte Parlamentsprotokolle genauso aus wie die Bibel und die mehrsprachigen publizierten Beiträge der BBC.

Und hier ist der Haken: ""Wenn es für ein paar der Sprachen keine Paralleltexte gibt, kann man derzeit kein neuronales Netz darauf trainieren"", erklärt Schüller. Eine Möglichkeit ist dann der Umweg über eine dritte Sprache: Steht etwa für einen deutschen Text kein Paralleltext in Farsi zur Verfügung, aber dafür in Englisch, kann man den englischen Text zur Übersetzung nutzen. Hier sinkt jedoch die Qualität der Übersetzung beträchtlich.

Diese Limitierungen sind auch der Grund, weshalb Dienste wie Deep L, Microsoft Translator oder Google Translate aktuell nur für einige Dutzend Sprachen zur Verfügung stehen. Für viele der 7.000 aktiven Sprachen existieren die benötigten Textkorpora einfach nicht.
Nächster Schritt: Weltwissen

Die Limitierung bei den Trainingstexten und das fehlende Verständnis für die eigentlichen Inhalte erklären, weshalb menschliche Dolmetscher sich zumindest vorerst noch keine Sorgen um ihre Jobs machen müssen. Gerade bei fachspezifischen Übersetzungen benötigt es nicht bloß sprachliche Fähigkeiten, sondern viel Wissen über das Themenfeld selbst. Aber selbst bei scheinbar trivialen alltäglichen Konversationen fehlen Maschinen oft sehr wesentliche Fertigkeiten zur Interpretation von Aussagen. Konnotation, Ironie, all das, was beim Reden und Schreiben noch so mitschwingt, fällt bei der maschinellen Auswertung unter den Tisch. Ganz abgesehen davon, dass Kommunikation zu großen Teilen nonverbal stattfindet.

""Für die nächste Generation der KI-Übersetzer wird es daher entscheidend sein, Systeme zu bauen, die den Text wirklich interpretieren und verstehen und dadurch eine korrekte Übersetzung liefern können"", sagt Computerlinguistiker Schüller. Und das ist eine Mammutaufgabe. Wissenschafter arbeiten seit den 1990er-Jahren an maschinell auswertbaren Datenbanken, die das Weltwissen repräsentieren. Herausgekommen ist dabei allerdings noch kaum etwas. ""Computern Weltwissen beizubringen ist so wie mit Fusionskraftwerken ... – In den nächsten zehn Jahren wird es vielleicht etwas, nur dass das seit 30 Jahren gesagt wird"", so Schüller.

Ein Produkt, das es bereits gibt, das am ehesten dem Verständnis von der Welt entspricht, ist ein Biologietextbuch, das den gesamten Text logisch repräsentiert hat und es erlaubt, Fragen zu stellen – und das auch dem Leser Fragen stellen kann. Das Inquire Project ist allerdings mit sehr viel manueller Arbeit verbunden gewesen und ist keine allgemein funktionierende Repräsentation der Welt.

Was die Arbeit an der maschinellen Repräsentation von Weltwissen so schwierig macht, ist, dass es dazu überhaupt noch keinen Lösungskonsens innerhalb der Wissenschaft gibt. ""Ich persönlich glaube, wir verstehen noch gar nicht, wie Menschen überhaupt denken, deshalb schaffen wir es nicht, die Welt des Menschen im Computer so abzubilden, dass der Computer sich in dieser Welt zurechtfindet"", sagt Schüller.
Sie fragen sich möglicherweise, wie Ihre Stimme klingen wird, wenn sie in der Zukunft von einer KI übersetzt wird. Eine Option sind generische künstliche Stimmen. Wie authentisch diese bereits mit Hilfe neuronaler Netze klingen, demonstriert Googles Wavenet. Wirklich spannend wird es allerdings, wenn diese Technologien genutzt werden, um Ihre persönliche Stimme zu synthetisieren. Das Programm Lyrebird beispielsweise erstellt mittels eingesprochener Beispielsätze einen Klon Ihrer Stimme und kann danach jeden beliebigen Satz mit der geklonten Stimme wiedergeben. Noch klingt das Ganze wie eine Computerstimme mit persönlicher Färbung, aber es ist abzusehen, wo die Reise hingeht.
Lyrebird
Unkontrollierbare Super-KIs

Für die Techies unter Ihnen mag das jetzt ein Dämpfer sein. Für KI-Skeptiker ist das jedoch auch kein Grund zu spotten. Denn ""rein rechnerisch müsste es mit den gegebenen technischen Mitteln möglich sein. Es ist keine Limitierung von Rechenleistung oder Speicherplatz"", meint Schüller. ""Es ist möglich, dass man mit geeigneten Trainingsdaten neuronale Netzwerke so trainieren kann, dass sie Strukturen der alltäglichen Welt abbilden und 'verstehen'. Problematisch an neuronalen Netzwerken ist nur, dass wir nicht verstehen, was die riesigen Zahlenmatrizen bedeuten. Das heißt, selbst wenn das Netz gut funktioniert und Sprache dann versteht, könnten wir nicht sagen, warum es funktioniert und was es intern berechnet. Das führt auch derzeit zu (berechtigten) Ängsten bezüglich einer potenziellen Super-KI, die wir nicht mehr verstehen und irgendwann auch nicht mehr kontrollieren können. Aber das ist ein eigenes sehr großes Thema.""

Spooky.

Bevor wir nun jedoch dieses Untergangsszenario präventiv mit einem Glas Rotwein begießen oder mit CBD-Dampf einnebeln, kommen wir zurück zur Ausgangsfrage: Wann werden wir einen echten Universalübersetzer wie in Star Trek haben?
Wann kommt der Universalübersetzer?

Nun, ein konkreter Zeithorizont lässt sich nicht festmachen, dennoch ist die Antwort laut Schüller ebenso logisch wie bedeutungsschwer: ""Wenn diese Übersetzungssysteme die Welt verstehen, dann werden sie weitgehend fehlerfrei simultan übersetzen können.""

Das gleiche gelte für KIs zur Spracherkennung, wie man sie von heutigen noch rudimentären Assistenten wie Siri kennt. Das fehlerfreie Interpretieren von undeutlicher und durch Hintergrundgeräusche verfälschter akustischer Sprache wird ebenfalls nur dadurch möglich, dass wir die Welt verstehen und so Fehler korrigieren können. Erst wenn Computer dasselbe können, werden sie Sprache perfekt erkennen.

Hört man in die Industrie hinein, wird aber klar, dass hier sehr viele Menschen sehr intensiv damit beschäftigt sind, den Traum vom Universalübersetzer real werden zu lassen. ""Wir arbeiten weltweit daran, diese Vision Realität werden zu lassen"", bestätigt Microsoft-Österreich-Manager Roland Gradl gegenüber dem STANDARD. Und auch die Konkurrenz sieht ihre Pläne auf Spur: ""Der Weg, auf dem wir jetzt sind, ist der richtige"", glaubt DeepL-Sprecher Lee Turner Kodak. Und was ist mit den Aliens?

Die Implikationen eines solchen Universalübersetzers für das Bildungswesen, den Arbeitsmarkt, Beziehungen – ja, unser gesamtes Leben – sind gewiss enorm. Werden wir in Zukunft noch Fremdsprachen lernen müssen? Würde ein universeller KI-Übersetzer einst lokale Jobs für tausendfach mehr Bewerber öffnen? Oder anders gefragt: Würde er Menschen aus kleineren Sprachräumen schlagartig mehr berufliche Optionen ermöglichen? Und hoffentlich werden uns solche Systeme auch dabei unterstützen, die im schlimmsten Fall fatalen Übersetzungsfehler zu verhindern.

Aber Sprache ist nicht nur Mittel zur Verständigung, sondern stark verwoben mit unserem Kulturkreis und unserer Herkunft. Sprache ist Teil unserer Identität. Sprache ist ganz viel davon, was Menschen ausmacht. Und blicken wir in die Sterne und in die noch fernere Zukunft, kann ich mich als – vielleicht naiver – Sci-Fi-Fan nicht dem Gedankenexperiment entziehen, wie eine intelligente Übersetzungs-KI uns eines Tages dabei helfen könnte, Kontakt mit Besuchern aus dem All herzustellen.

Würde KI uns theoretisch wie in ""Star Trek"" erlauben, mit ""Außerirdischen"" zu kommunizieren? ""Wenn wir Zugang zu einer großen Menge an Daten über die Welt und Sprache von Aliens hätten, könnte ein KI-System eventuell daraus ein Übersetzungssystem erstellen"", sagt Schüller. ""Ohne umfangreiche Informationen über die Realität, in der eine Sprache verwendet wird, wird es aber nicht möglich sein."" Zwar schaffen es menschliche Wissenschafter immer wieder, alte Texte, wo es keine Sprecher mehr gibt, zu interpretieren und zu übersetzen. Wenn uns Aliens jedoch kein umfangreiches Referenzmaterial zur Verfügung stellen werden, könnte ein Austausch sehr schwierig werden. Zurück zur Erde

Fragt man Astrophysiker, dürfte der First Contact freilich schon an galaktischen Unwahrscheinlichkeiten scheitern. Das Weltall ist schier unendlich groß und ebenso alt, und wir sind Lichtjahre entfernt von der Chance, ein interstellares Tinder-Date mit E.T. auszumachen. Ein guter Grund, die Gedanken auf den Erdboden zurückzubeamen und sich auf den blauen Planeten und seine 7,6 Milliarden Menschen zu fokussieren.

Von den 7.000 aktiven Sprachen konnte mein Großvater übrigens auch ein wenig Russisch. Er erzählte immer gern von dem Freund, der nicht mit Wodka, sondern mit Spiritus anstoßen wollte. Noch Jahre nach Opas Tod habe ich seine Stimme mit einer seiner liebsten Übersetzungen im Kopf: ""Je mehr Sprachen man spricht, desto mehr Menschen ist man.""

Ich weiß, dass er damit keinen futuristischen Universalübersetzer im Sinn hatte. Doch wenn uns Technologie eines Tages ermöglicht, globale Sprachbarrieren zu überwinden, werden wir alle ein ganzes Stück näher aneinanderrücken. Und das war ihm letztlich stets am wichtigsten. Und okay, wer weiß: Vielleicht klappt es so irgendwann auch mit den Aliens.";https://www.derstandard.at/story/2000098391853/star-trek-super-ki-aliens-wo-bleibt-der-universaluebersetzer;Standard;Zsolt Wilhelm
24.01.2018;Was gibt es in den Life Sciences zu tun?;"Mit der Entschlüsselung des menschlichen Genoms wurde der Grundstein für die Medizin der Zukunft gelegt: Künftig soll jeder Patient eine auf seine DNA zugeschnittene Behandlung erhalten. Doch das ist nicht der einzige Trend in den sogenannten Life-Sciences. Auch immer resistenter werdende Viren lassen Forscher an neuen Medikamenten tüfteln.

Unter Life-Sciences werden Berufe im Bereich der Biotechnologie, der Molekularbiologie, der Pharmaindustrie und der Medizintechnik verstanden. Sie sind in Österreich nicht nur ein großes Forschungsfeld, sondern auch ein starker Wirtschaftszweig. Im Jahr 2014 gab es 823 Unternehmen in der Branche, die knapp 20 Milliarden Euro erwirtschaftet haben, wie der aktuellste Life-Science-Report von 2015 zeigt. Und derzeit knapp 60.000 Studierende sorgen dafür, dass es künftig genug Berufseinsteiger gibt.

Orientieren im Berufsfeld

Doch welche Bedürfnisse haben Studierende der Life-Sciences, und wie sieht der Arbeitsmarkt für künftige Absolventen aus? Erwachsen aus dem Problem, dass er während seiner Dissertation kaum Möglichkeiten hatte, sich mit anderen Doktoranden über ihre Erfahrungen auszutauschen, entsprang Jonas Ramoni die Idee eines nationalen Netzwerks für Studierende und Jungwissenschafter der Life-Sciences. 2016 gründete der gebürtige Innsbrucker die Young Life Scientists Austria (YLSA), die zur gemeinnützigen Österreichischen Gesellschaft für Molekulare Biowissenschaften und Biotechnologie, kurz ÖGMBT, gehört. ""Jungwissenschafter entscheiden maßgeblich, wie sich die Branche weiterentwickelt. Doch es gab für sie kaum Unterstützung in Karrierefragen"", sagt Ramoni. Die Schwierigkeit für viele liege nämlich darin, dass man mit einem Studium in den Life-Sciences in vielen unterschiedlichen Bereichen tätig sein kann. Viele wüssten bis zum Abschluss nicht, was sie später arbeiten möchten. Deshalb organisiert Ramoni für die YLSA-Mitglieder nicht nur Vernetzungstreffen, sondern auch Firmenbesichtigungen, und er lädt etwa einen Patentanwalt oder eine Editorin vom Springer-Nature-Verlag zu Vorträgen ein. ""Da sieht man, wie deren Laboralltag abläuft, und bei Diskussionen im kleinen Rahmen erfährt man Details, die nicht auf der Firmenwebseite stehen, und kann Kontakte knüpfen."" Zusätzlich würden die Vortragenden Tipps geben, welche Stellen gerade ausgeschrieben sind, wie man am besten Kontakt zur Personalabteilung aufnimmt und sich erfolgreich bewirbt. Diese Netzwerke seien bei der großen Anzahl an hochqualifizierten Absolventen besonders wichtig, um einen Job zu finden und zu bekommen. Dafür ist zumindest ein Master nötig. Mit diesem kann man etwa als Labortechniker in einem Pharmaunternehmen arbeiten, doch häufig wird noch ein Doktor draufgesetzt, mit dem man entweder in der Forschung bleiben oder in die Industrie gehen kann. Und wer einen IT-Schwerpunkt hat oder sich mit Big Data auskennt, habe noch bessere Startvorteile.

Gründungen fördern

Die Kontakte zur Industrie sind auch dann entscheidend, wenn man als Forscher selbst gründen möchte. Viele Branchenverbände sprechen sich derzeit für eine Verbesserung des Gründerbewusstseins aus, auch das Strategiepapier zu Life-Sciences des Wissenschaftsministeriums will Start-ups stärker fördern. ""Wenn man die Resultate seiner Dissertation in einem Start-up weiterführen kann, ist das ein guter Einstieg ins wirtschaftliche Leben"", sagt Ramoni. Dennoch müssten die Unis als Inkubatoren noch mehr unterstützt werden. Auch die Entwicklungen rund um das Vienna Biocenter zu einem Life-Science-Hub seien zu begrüßen: Hier werden explizit für Jungunternehmer – ansonsten sehr teure – Laborflächen und Büroarbeitsplätze angeboten. Diese Notwendigkeit zeigt auch eine Studie der Modul University Vienna, die 2016 Experten und Start-ups der Branche befragt hat: Biotech-Start-ups mangelt es an geeigneten Gewerbeimmobilien und Inkubatoren.

Ramoni sieht das Biocenter als Chance: ""Der Mix aus akademischer Forschung und privaten Biotechfirmen ergibt positive Synergieeffekte."" Auch für ihn: 2007 begann er hier molekulare Biotechnologie an der FH Campus zu studieren, nach dem Master an der Uni Wien und einem Doktor an der TU Wien ist er jetzt wieder in St. Marx gelandet. Er arbeitet als Projektmanager für das Biotech-Start-up Ares Genetics an Verfahren zur Diagnose antibiotikaresistenter Infektionen. Und seit 2018 ist auch die ÖGMBT-Geschäftsstelle in den Räumen der FH Campus im Biocenter angesiedelt.";https://www.derstandard.at/story/2000072012412/was-gibt-es-in-den-life-sciences-zu-tun;Standard;Selina Thaler
25.10.2019;Auf welche Technologien wir uns im Katastrophenfall verlassen können;"Eher selten treten jene Katastrophen ein, die man am meisten fürchtet. Das ist vor allem der menschlichen Gefahrenwahrnehmung geschuldet, an der die Ratio nur in beschränktem Maß beteiligt ist. So haben etwa viele Menschen immer noch mehr Angst vor einem Flugzeugabsturz als vor einem Verkehrsunfall, obwohl Letzterer um ein Vielfaches wahrscheinlicher ist. Aber dieser Umstand wird ebenso erfolgreich verdrängt wie die Gefahren, die im Gefolge des Klimawandels immer weiter wachsen. Dass im vergangenen Jahr in Österreich sehr viel mehr Menschen an den Folgen von Hitze gestorben sind als im Autoverkehr oder dass sich die jährlichen Schäden durch den Klimawandel bis 2030 voraussichtlich auf über vier Milliarden Euro vervierfachen werden, sickert erst langsam ins allgemeine Bewusstsein ein.

Im Bereich der Forschung aber wird längst an unterschiedlichsten Technologien gearbeitet, um die Folgen der künftigen Naturkatastrophen so weit wie möglich abzumildern. Die heurigen Austrian Disaster Research Days am 14. und 15. Oktober gaben einen Einblick, wie wissenschaftliche Entwicklungen bei der Bewältigung von Hochwasser, Lawinen- oder Murenabgängen, von Erdbeben, Wirbelstürmen oder bei Schutzmaßnahmen für kritische Infrastruktur helfen können.
Big-Data und Satelliten

Organisiert wurde die Konferenz vom Disaster Competence Network Austria (DCNA), das letztes Jahr von der Universität für Bodenkultur Wien und der Technischen Universität Graz gegründet wurde. Vorrangige Ziele des Kompetenznetzwerks sind der Transfer wissenschaftlicher Erkenntnisse aus der Sicherheits- und Katastrophenforschung in die Praxis, die Vernetzung der unterschiedlichen Krisenmanager sowie eine bessere Vorbereitung der Bevölkerung auf mögliche Katastrophen.

Unter den zahlreichen bereits heute im Katastrophenfolgen-Management eingesetzten Technologien komme den Geoinformationssystemen eine besondere Rolle zu, betonte Keynote-Speaker Alexander Siegmund von der Universität Heidelberg.

""Sie machen die Folgen von Katastrophen überhaupt erst sichtbar und könnten eigentlich noch viel intensiver genutzt werden"", so der Sprecher der European Science & Technology Advisory Group des UN Office for Disaster Risk Reduction (UNDRR).

Auch der Einsatz von Big-Data-Technologien zur Verarbeitung und Auswertung riesiger Datenmengen sei mittlerweile unverzichtbar, etwa für kurzfristige Sturmvorhersagen. Satellitentechnologien könne man beispielsweise auch für optimierte Raumplanung nutzen – ""damit es erst gar nicht zu einer Katastrophe kommt"". Sie leisten zudem einen wichtigen Beitrag für verbesserte Vorhersagen und eine effizientere Versorgung im Ernstfall.
Medikamente per Drohne

Ein mächtiges, aber erst in den Kinderschuhen steckendes Hilfsmittel in der Katastrophenbewältigung sind Drohnen. Mit ihrer Hilfe kann man etwa unzugängliche Katastrophengebiete aus der Luft erkunden und mit diesen Daten gezielte Hilfsmaßnahmen einleiten.

Sind Straßen unpassierbar, können sie sogar dringend benötigte Hilfsmittel wie etwa Medikamente oder Defibrillatoren transportieren. ""Bislang werden in Österreich mit Drohnen vor allem Luftbilder gemacht, unter anderem bei der Suche nach vermissten Personen"", berichtet Markus Gutmann vom Institut für Vernetzte und Eingebettete Systeme an der Universität Klagenfurt.

""Ausgestattet mit der entsprechenden Sensorik, etwa einer Infrarotkamera, können sie Menschen auch in dichten Wäldern oder eingestürzten Gebäuden aufspüren"", so der Forscher und Rot-Kreuz-Mitarbeiter.

Falls Telefonnetze oder Internet durch Zerstörungen am Boden ausfallen, können Drohnen auch zum Aufbau einer Notfall- und Überbrückungskommunikation genutzt werden. Insgesamt stehe der Einsatz der unbemannten Fluggeräte in der Katastrophenhilfe zwar erst am Anfang, ""doch das Potenzial dieser Technologie ist enorm"".

Um dieses auszuschöpfen, bedürfe es aber noch intensiver Forschungsanstrengungen. So müssen etwa die Batterien noch deutlich leichter werden, um die mögliche Einsatzdauer, die zurzeit noch im Minutenbereich liegt, zu verlängern. Ein großes Forschungsthema sei auch der Einsatz von Multi-Drohnen-Systemen, sogenannten Drohnenschwärmen.
Eigenverantwortung jedes einzelnen

Insgesamt existiert also bereits sehr viel technologisches Know-how zur Abmilderung von Katastrophenfolgen. Aber gelangt dieses an verschiedensten Universitäten und Forschungseinrichtungen generierte Wissen auch zu den betroffenen Menschen und jenen Organisationen und Stellen, die wie Rotes Kreuz, Feuerwehr oder Bürgermeisterämter in einer Krisensituation unmittelbar reagieren müssen? ""Leider wissen die Akteure oft nicht, welche hilfreichen Technologien es überhaupt gibt, welchen Nutzen diese haben und wie man sie einsetzt"", sagt Alexander Siegmund. Was man brauche, seien effiziente Info-Kanäle, aber auch ein Bewusstsein für die Eigenverantwortung jedes Einzelnen.

""Naturkatastrophen sind so vielfältig, dass der Staat allein nicht alles regeln kann."" Deshalb wäre es wichtig, das Wissen über potenzielle Katastrophen und die nötige Handlungskompetenz im Ernstfall bereits in der Schule zu vermitteln.

Dass dieses Wissen ebenso wie das Ausmaß der Gefährdung durch eine Katastrophe höchst ungleich verteilt ist, machte der Vortrag Karin Webers von der Universität für Bodenkultur Wien deutlich. Geschlecht, sozioökonomischer Status oder Alter bestimmen maßgeblich mit, wie und ob man sich auf Naturgefahren vorbereiten und sich von ihnen erholen kann.

""Wer du bist und was du tust, bestimmt dein Schicksal auch im Katastrophenfall"", zitierte sie einen Helfer nach dem verheerenden Tsunami 2004 in Indien, bei dem viermal mehr Frauen als Männer verunglückten.
Ungleiche Risiken

Die Verankerung von Diversity- und Gender-Mainstreaming im Feld der Katastrophenrisikoreduktion wurde von den Vereinten Nationen schließlich als unverzichtbar anerkannt und in das Sendai Framework for Disaster Risk Reduction 2015-2030 aufgenommen.

In Österreich wurde vor drei Jahren zudem das Netzwerk we4DRR (women exchange for Disaster Risk Reduction) gegründet. Expertinnen in den Bereichen Forschung, Verwaltung, Politik und Naturgefahrenmanagement haben sich darin zusammengeschlossen, um Politik und Öffentlichkeit auf die ungleiche Verteilung der Risiken aufmerksam zu machen und Gender- und Diversity-Wissen für die Katastrophenrisikoreduktion besser zu nutzen sowie neues zu erarbeiten.

Bis zum 31. Oktober können im Rahmen des we4DRR-Student-Awards übrigens noch wissenschaftliche Publikationen, Master- und Doktorarbeiten zum Thema ""Gender und Disaster Risk Reduction"" eingereicht werden.";https://www.derstandard.at/story/2000110182090/auf-welche-technologien-wir-uns-im-katastrophenfall-verlassen-koennen;Standard;Doris Griessing
29.12.2018;Das Internet der Kühe;"Wenn vom Internet der Dinge die Rede ist, mag jeder an etwas anderes denken: An das auch während seiner Abwesenheit effizient geheizte Haus, an besser versorgte unfallgefährdete Familienmitglieder oder auch an die optimierte Lagerhaltung von Gütern. Ein kürzlich gegründetes Konsortium aus 31 Wirtschafts- und 13 Forschungsinstitutionen denkt dabei an Kühe, Landwirte und Milchprodukte.

Das Projekt, das im Rahmen des Kompetenzzentrenprogramms Comet vom Wirtschafts- und vom Wissenschaftsministerium gefördert wird, nennt sich D4Dairy und soll die Milchwirtschaft mit modernster Technik voranbringen.

Die vier D im Projektnamen stehen für die Begriffe Digitalisierung, Datenintegration, Detection (Entdeckung bzw. Erkennung) und Decision-Making (Entscheidungsfindung). So sollen Daten aus landwirtschaftlichen Betrieben und Partnern entlang der Wertschöpfungskette Milch digitalisiert, standardisiert und zusammengeführt beziehungsweise ausgetauscht werden, um einen Mehrwert zu erzielen.
Apps für Landwirte und Tierärzte

Eine wesentliche Rolle dabei spielen Sensoren, die eine Art Internet der Kühe ermöglichen, wenn man so will. Außerdem kommen Methoden wie Big-Data-Analysen zum Einsatz, um gesundheitliche Risikofaktoren und Vorbeugemaßnahmen zu erkennen.

Und zu guter Letzt sollen die so gewonnenen Daten und Erkenntnisse die Grundlage für Tools bilden, die dem Landwirt und Tierarzt Entscheidungshilfen bieten. So könnte etwa eine entsprechende App Gesundheitsempfehlungen für die Tiere auf Basis dessen geben, was über ihre Krankengeschichte oder Umweltfaktoren bekannt ist.

Auch in der Zucht bieten sich neue Möglichkeiten. Von den rund zwei Millionen Rindern, die in Österreich gehalten werden, sind die meisten schon lange sorgfältig überwacht: Daten wie Abstammungen, Paarungen, Geburten, Krankheiten, Milchleistung und vieles mehr werden seit Jahrzehnten erhoben und aufgezeichnet.

Dabei geht es unter anderem um den sogenannten Zuchtwert, also die Einschätzung, mit welcher Wahrscheinlichkeit ein bestimmtes Tier erwünschte Eigenschaften an seine Nachkommen weitergibt. Das ist nicht so einfach, weil ja bei der Verschmelzung der elterlichen Chromosomen nicht nur deren Gene gemischt, sondern auch in unterschiedlichen Kombinationen auf die Nachkommen verteilt werden. Die Töchter einer Kuh, die viel Milch gibt, müssen daher nicht automatisch auch gute Milchlieferantinnen sein.
Gesammelte Gendaten

Bis vor etwa zehn Jahren waren für die Beurteilung der genetischen Qualitäten eines Tieres neben äußeren Kriterien aufwendige Aufzeichnungen darüber nötig, wie weit Nachkommen seine Eigenschaften geerbt hatten. Diese Erhebungen sind zwar noch immer sehr wichtig, aber nicht mehr die einzige Informationsquelle.

De facto belegen die Daten aus diesen herkömmlichen Quellen beim Rinderdatenverbund, der die diesbezüglichen Daten von rund zwei Millionen lebenden deutschen und österreichischen Kühen und deren Vorfahren verwaltet, nur noch etwa 30 Prozent des Speicherplatzes.

Der Rest stammt aus genetischen Daten, genauer gesagt aus SNPs (für Single Nucleotide Polymorphism – Einzelnukleotid-Polymorphismus). SNPs oder ""Snips"", wie sie im Laborjargon heißen, sind Stellen in der Erbsubstanz, in der die zugehörige Basenfolge in mehr als einer Variante vorkommt. ""50.000 solcher Snips kann man heute um 50 Euro bestimmen lassen"", sagt Johann Sölkner vom Institut für Nutztierwissenschaften der Wiener Universität für Bodenkultur, ""und sie liefern so viel Information über den Zuchtwert eines Stieres wie 25 seiner Töchter.""

Auf Basis dieser ""genomischen Selektion"" geht die Zuchtwahl nicht nur erheblich schneller als bisher, sie kann auch neue gewünschte Merkmale identifizieren, ""etwa Biomarker für Euter- oder Klauengesundheit"", wie Sölkner ausführt. Eine ganz andere Form der Gesundheitsvorsorge ermöglichen Sensoren. So gibt es Messfühler, die die Rinder an einem Band um den Hals oder an den Beinen tragen und die aufzeichnen, wie viel Zeit die Tiere mit Fressen oder Liegen verbringen. Im Rahmen von D4Dairy hofft man, aus entsprechend großen Datenmengen gesundheitliche Probleme früher erkennen zu können: ""Wenn eine Kuh zum Beispiel viel liegt und wenig frisst, könnte das ein Hinweis darauf sein, dass ihr das Stehen Schmerzen bereitet, und davon könnte man auf Klauen-Probleme schließen"", sagt D4Dairy-Leiterin Christa Egger-Danner.

Ein eigens für Rinder entwickelter Sensor dringt übrigens noch tiefer in die Materie ein: Das etwa sieben Zentimeter lange Gerät wird von den Kühen verschluckt und misst mitten im Pansen pH-Wert, Temperatur und Bewegungsaktivität.
Big Data zur Gesundheitsvorsorge

Auch Big Data soll bei D4Dairy zum Einsatz kommen: Peter Klimek von der Med-Uni Wien, der sich am Complexity Science Hub bisher mit Gesundheitsdaten der österreichischen Bevölkerung befasst und dabei auch Zusammenhänge zwischen verschiedenen Krankheiten erforscht hat, wird sein dabei entwickeltes Analysemodell nun auch auf Milchkühe anwenden.

Besonderes Augenmerk soll dabei auf häufigen Krankheiten wie Euter- oder Gebärmutter-Entzündungen liegen. Dass die ""Data"" dafür ""big"" genug sind, darf angenommen werden: Immerhin sind zwei Millionen Milchkühe in Österreich und Deutschland daran beteiligt.

All diese und noch mehr Daten sollen den Landwirten letztendlich in Form praktikabler Softwaretools zugutekommen. ""Wir wollen die enormen Datenmengen vernetzen, austauschen – selbstverständlich unter strengem Datenschutz – und gemeinsam ausloten, um sowohl das Tierwohl und die Tiergesundheit weiter zu verbessern als auch die Arbeit der Landwirte einfacher zu machen und sie wettbewerbsfähig zu erhalten"", resümiert Egger-Danner. ";https://www.derstandard.at/story/2000094242271/das-internet-der-kuehe;Standard;Susanne Strnadl
11.06.2016;"Dirk Helbing: ""Wir müssen die Demokratie digital neu erfinden""";"Sie warnen vor dem Ende der Demokratie, davor, dass die datengetriebene Gesellschaft in eine Katastrophe münden könne. Das klingt alles sehr fatalistisch. Bringen uns die neuen technologischen Möglichkeiten nicht Chancen auf ein besseres Leben, wie uns immer wieder versprochen wird? Helbing: Man muss sowohl die Chancen als auch die Risiken sehen. Man darf die Risiken nicht kleinreden und muss sich aktiv mit ihnen auseinandersetzen, um sie zu minimieren und die Chancen zu maximieren. Das braucht eine öffentliche Diskussion. Es muss uns klar sein, dass die Situation historisch gesehen kritisch ist.

STANDARD: Inwiefern?

Helbing: Wir stehen am Übergang zur digitalen Gesellschaft. Ähnliche Übergänge in der Vergangenheit – von der Agrar- zur Industriegesellschaft und dann zur Servicegesellschaft – liefen nicht glatt ab. Da gab es Finanz- und Wirtschaftskrisen, Revolutionen und Kriege. Ersteres haben wir offensichtlich schon, und auch soziale und politische Destabilisierungen zeichnen sich in verschiedenen Ländern ab. Das Charakteristikum dieser Übergänge ist, dass die Erfolgsprinzipien aus der Vergangenheit an das Ende ihrer Wirksamkeit kommen. Heute sind das Prinzipien wie Regulierung und Optimierung. Wir sind in einem Zeitalter von Überregulierung. Man kommt nicht mehr hinterher, bei dem Tempo, das die digitale Revolution vorlegt. Wenn wir so weitermachen wie bisher, enden wir automatisch im Chaos.

STANDARD:Wie kann man die digitale Revolution bändigen?

Helbing: Wir müssen uns die Frage stellen, in welcher Art von Gesellschaft wir leben möchten. Eine datenbasierte wird es auf jeden Fall sein. Es kann der Feudalismus 2.0 kommen, wo nur einige wenige über die Daten verfügen, oder der Kommunismus 2.0, wo der Staat alle bemuttert. Es kann der Faschismus 2.0 sein – oder Demokratie 2.0 und Kapitalismus 2.0. Von oben gesteuerte Gesellschaftsmodelle scheinen schon auf dem Weg zu sein, haben aber das Problem, dass sie dazu neigen, die Diversität zu reduzieren. Genauso wie die Biodiversität geschützt werden muss, müssen wir auch die Soziodiversität schützen. Denn Diversität ist die Voraussetzung für Innovation, kollektive Intelligenz und gesellschaftliche Resilienz, also die Fähigkeit, mit unerwarteten Entwicklungen zurechtzukommen, die der Klimawandel, demografische und technologische Veränderungen mit sich bringen. Wenn wir Pluralismus als Grundprinzip haben, ist die Chance am größten, dass es irgendwo eine Lösung gibt. Innovation passiert meistens bottom-up. Das ist auch der Grund, warum bisher Demokratie und Kapitalismus die leistungsfähigsten Systeme waren. Wir müssen sie jetzt digital neu erfinden.

STANDARD: Das Internet ermöglicht schon jetzt partizipative Strukturen, siehe Shared Economy oder Crowdsourcing. Warum müssen sich Demokratie und Kapitalismus neu erfinden?

Helbing: Es gibt in der vernetzten Ökonomie und Gesellschaft eine natürliche Entwicklung in Richtung neuer Prinzipien – Kokreation, Koevolution und kollektive Intelligenz. Der Staat hat da nicht viel Dünger hingestreut. Man dachte, es ist damit getan, dass man eine große IT-Infrastruktur schafft, die alle möglichen Daten sammelt, und optimale Lösungen ermittelt, die dann technokratisch durchzusetzen sind. Diese Idee des wohlwollenden Diktators scheint sich vielerorts verbreitet zu haben, ganz nach dem Motto: Wenn man nur genug Daten hat, enthüllt sich die Wahrheit von selbst. Diese Big-Data-Fantasie hat keine wissenschaftliche Basis.

STANDARD: Wächst uns Big Data über den Kopf?

Helbing: Ich glaube, dass wir in eine Sackgasse geraten sind in dieser ersten Phase der Digitalisierung, die stark von Big Data und künstlicher Intelligenz gekennzeichnet war. Man sieht aber jetzt zunehmend die Probleme. Dieses Denken hat nicht zu dem allgemeinen Zuwachs an Wohlstand geführt, den man sich von der digitalen Revolution erwartet hatte. Ich habe aber das Gefühl, dass ein Umdenken eingesetzt hat. Man beginnt jetzt zu erkennen, dass es eine Art riesige Sharing Economy braucht, wo sich jeder einklinken kann mit seinen Services und Ideen. Wir müssen alle an Bord nehmen bei der Digitalisierung, wenn es ein Erfolg werden soll. Wenn nicht jeder mitmachen kann, bricht die Gesellschaft auseinander, und dann haben wir ein riesiges Problem.

STANDARD: Wie soll diese digitale Mitmachgesellschaft funktionieren?

Helbing: Wir müssen uns fragen, wie wir uns unabhängiger machen können von den großen Playern wie Google, die unsere Daten sammeln und uns sozusagen ihre Weltsicht aufdrücken. In jedem Smartphone haben wir Sensoren, mit denen wir Lärm, Standort, Helligkeit, Geschwindigkeit und vieles mehr messen und Daten über unsere Umwelt erzeugen können. Solche Daten können wir anonymisiert teilen – und eine Art Wikipedia für Echtzeitdaten aufbauen. Wir arbeiten an der ETH Zürich gemeinsam mit internationalen Partnern an dem Projekt Nervousnet, einer Plattform, die es jedem erlaubt, eigene Apps, Leistungen und Services zu entwickeln oder auch Spiele, die solche Sensordaten verwenden. Die Idee ist es, ein offenes, partizipatives Informationssystem zu schaffen, in dem die Leute ihrer Kreativität freien Lauf lassen können.

STANDARD: Inwieweit betrifft das das kapitalistische System?

Helbing: Unser Vorschlag schließt ein zusätzliches Finanzsystem ein, in dem alle Geld verdienen können. Durch das Produzieren und Teilen von Daten würde man verschiedene Arten von Geld schöpfen, je nachdem, um welche Daten es geht. Das würde völlig neue Märkte ermöglichen.

STANDARD: Wie soll sich das Nervousnet finanzieren?

Helbing: Es geht um das Kreieren von neuem Geld, ähnlich wie bei Bitcoin. Das, was heute von der EZB von oben in das System hineingepumpt wird und weder bei Unternehmen noch bei uns ankommt, das würde in Zukunft von unten, von uns, erzeugt, sodass es an jedem vorbeifließt und jedem nutzt. Das ist einfach ein neuer Ansatz, mit dem man Geld schöpft. Es muss klar sein, dass sich mit der Digitalisierung jede Branche ändern wird, die Verwaltung, die Politik, die Forschung, insofern ist es klar, dass sich die gesamte Wirtschaft und auch das Geldsystem ändern werden.

STANDARD: Wer soll das Netzwerk aufbauen?

Helbing: Wir sind dabei, an den ersten Grundlagen dieser Plattform zu basteln, und wollen möglichst viele Menschen mobilisieren, mitzumachen. Natürlich sind auch Unternehmen eingeladen. Um ein dezentrales Bottom-up-System aufzubauen, braucht es aber auch staatliche Rahmenbedingungen. Kooperation ist Europas einzige Chance, eine wettbewerbsfähige digitale Ökonomie aufzubauen. Wir sollten jetzt einen Kapitalismus 2.0 erschaffen, der kompatibel ist mit Demokratie, mit Liberalismus, sozialen und ökologischen Interessen, und zwar auf der Basis von Marktmechanismen, also nicht durch Regulation, sondern auf der Basis von Selbstorganisation.

STANDARD: Das klingt alles nach einer allzu schönen Vision ...

Helbing: ... und das ist es, was wir jetzt brauchen. Weil es uns an Zukunftsvisionen, an Aufbruchstimmung mangelt, haben diejenigen, die zu den Gesellschaftsmodellen der Vergangenheit zurückkehren wollen, so viel Zulauf. Um sich die Zukunft vorstellen zu können, muss man digital denken lernen. Die digitale Welt ist auf Ideen aufgebaut. Jene Länder, die verstehen, was man mit Information alles machen kann, welche Entfaltungskraft darin liegt, anstatt sie zu beschränken, zu kontrollieren und in irgendwelche Schemata zu zwängen, werden am Ende führend sein.";https://www.derstandard.at/story/2000038390074/dirk-helbing-wir-muessen-die-demokratie-digital-neu-erfinden;Standard;Karin Krichmayr
29.07.2019;Können Frauen die Umwelt retten?;"""Our house is on fire. I am here to say, our house is on fire"", sagte Greta Thunberg. Unser Lebensstil bedroht unsere Existenz. Wir zerstören den Boden, auf dem wir leben, und wenn wir unser bequem gestaltetes Leben jetzt nicht radikal ändern, haben wir keine Chance mehr, die Klimakatastrophe abzuwenden. Da wir in einer repräsentativen Demokratie leben, liegt es jedoch nicht nur an uns, den Bürgerinnen und Bürgern, etwas zu verändern: weitreichende umweltpolitische Entscheidungen fallen in Parlamenten. Doch was denken Abgeordnete über umweltpolitische Fragen? Und handeln sie auch im Einklang mit ihren Einstellungen?
Gender Gap

Dank umfassender Forschung wissen wir, dass linke Einstellungen auf Bürgerebene eng mit einer umweltfreundlichen Haltung verwoben sind. Forschung auf Ebene der Bürger und Bürgerinnen zeigt jedoch auch, dass ein weiterer Faktor von großer Bedeutung ist: das Geschlecht. Länderübergreifend sind es Frauen, die sich stärker für Umweltschutz aussprechen und sich Umweltrisiken eher bewusst sind als Männer. So schätzen Frauen beispielsweise ein breites Spektrum von Umweltfragen — sei es der Klimawandel, die Kernenergie oder Wasserverschmutzung — als problematischer ein. Aber das ist noch nicht alles: Empirische Untersuchungen zeigen, dass Bürgerinnen auch ihr Verhalten entsprechend ihrer Einstellungen anpassen. Frauen handeln umweltfreundlicher, was sich beispielsweise in nachhaltigem Konsumverhalten oder auch in aktiver Mitgliedschaft in Umweltorganisationen äußert.

Will man diese Unterschiede erklären, deutet vieles auf eine geschlechtsspezifische Sozialisierung hin. Während Frauen dazu sozialisiert werden, mitfühlend, kooperativ und fürsorglich zu sein, lernen Männer, dass die Erfüllung männlicher Verhaltensnormen Konkurrenzdenken und Unabhängigkeit erfordert. Diese Geschlechtererwartungen prägen das Denken und Handeln im späteren Leben, so der Sozialisierungsansatz. Selbst wenn noch nicht gänzlich geklärt ist, wie es zu geschlechterspezifischen Unterschieden kommt, die Existenz des Gender Gaps bezüglich umweltpolitischer Fragen ist unbestritten. Machen weibliche Abgeordnete andere Politik als ihre männlichen Kollegen?

Aber trifft das auch für die ""Eliteebene"" zu? Gibt es solche Geschlechterunterschiede auch unter Abgeordneten? In unserem neuesten Forschungsartikel in der Fachzeitschrift ""Environmental Politics"" gehen wir dieser Frage nach und untersuchen sowohl Einstellungen als auch Abstimmungsverhalten von Mitgliedern des Europäischen Parlaments (MdEPs).

Intuitiv würden wir es für unwahrscheinlich halten, dass individuelle Merkmale von MdEPs wie eben das Geschlecht die (Umwelt-)Gesetzgebung beeinflussen. Die Umweltpolitik zählt nämlich nicht zu jenen Bereichen, die im Verdacht stehen, durch Geschlechterunterschiede gekennzeichnet zu sein: Während die Forschung in sogenannten ""Frauenthemen"" Unterschiede in den Bereichen Bildung, Gesundheitsversorgung oder der Sozialpolitik feststellt, ist der Effekt von Geschlecht auf andere Politikbereiche umstritten. Darüber hinaus sind Abgeordnete nicht nur Individuen mit eigenen Interessen und Idealen, sondern auch Parteienvertreterinnen und -vertreter, die von internen Nominierungsprozessen abhängig sind.

Forschung bezüglich des Abstimmungsverhaltens von Parlamentarierinnen und Parlamentariern konzentriert sich daher auch weniger auf persönliche Merkmale der Abgeordneten, sondern vor allem auf Parteizugehörigkeit und Nationalität. Monika Mühlböck und Nikoleta Yordanova widmeten sich hier im Politikwissenschaftsblog vor kurzem der Frage, wie MdEPs abstimmen und welche Forschungsergebnisse hierzu existieren. Dabei stellten sie fest, dass Studien individuellen Attributen von Parlamentariern bisher noch wenig Aufmerksamkeit schenken. MdEPs, wie auch nationale Abgeordnete, sind relativ homogen, was Bildungsgrad, Alter und Einkommen betrifft. Die Annahme, dass persönliche Attribute deshalb keine Rolle spielen, hat jedoch in der Literatur eine Lücke geschaffen: Welche konkreten Auswirkungen das Geschlecht auf das Abstimmungsverhalten hat, blieb lange unerforscht.
Frauen stimmen häufiger Umweltschutzgesetzen zu

Um diese Lücke zu schließen, werfen wir in unserem Blogbeitrag einen Blick auf das weltweit einzige multinationale Parlament: das Europäische Parlament (EP). Das EP ist für dieses Vorhaben in besonderer Weise geeignet: Zum einen haben Frauen einen — verglichen mit vielen nationalen Parlamenten — hohen Anteil an Parlamentssitzen und besetzen zudem einflussreiche Ausschusspositionen. Zum anderen ist die Gesetzgebung des EP in umweltpolitischen Fragen über die Jahre hin sehr weitreichend geworden. Wenn das umweltpolitische Verhalten von Eliten durch Geschlechterunterschiede gekennzeichnet ist, dann sollten wir diesen Effekt also genau hier sehen: zwischen männlichen und weiblichen MdEPs.

Basierend auf Umfragedaten, die im Zeitraum des sechsten (2004–09) und siebten Europäischen Parlaments (2009–14) gesammelt wurden, untersuchen wir im ersten Schritt die Einstellungen von MdEPs. Entgegen unseren Erwartungen finden wir keine substanziellen Einstellungsunterschiede zwischen weiblichen und männlichen Abgeordneten in Umweltthemen. Zwar sehen wir anfangs einen Geschlechterunterschied, dieser verschwindet jedoch, wenn wir die Ideologie beziehungsweise die Zugehörigkeit der MdEPs zu ihren jeweiligen EP-Fraktionen in unsere Analyse miteinbeziehen. Dies bedeutet, dass Ideologie beziehungsweise Fraktionszugehörigkeit die Unterschiede zwischen Abgeordneten besser erklärt als das Geschlecht. Mit anderen Worten, innerhalb dieser Fraktionen können wir keine signifikanten Unterschiede feststellen.

Richten wir unsere Aufmerksamkeit allerdings auf konkretes Verhalten — und zwar auf Abstimmungen zu Umweltthemen aus demselben Zeitraum –, dann zeigt sich ein ganz anderes Bild: Frauen stimmen weitaus häufiger für Gesetze und Maßnahmen ab, die dem Umweltschutz zugute kommen, als ihre männlichen Kollegen. Entscheidend ist, dass dieser Zusammenhang robust gegenüber anderen Faktoren wie etwa der Parteizugehörigkeit oder Nationalität der MdEPs ist. In Anbetracht der hohen Homogenität der Abgeordneten, ihrer Parteibindungen und ihrer nationalen Loyalität — allesamt Faktoren, die es unwahrscheinlicher machen, dass persönliche Attribute noch einen Einfluss auf ihr Verhalten ausüben können — sind diese Erkenntnisse bemerkenswert. Obwohl Delegierte vor ihrer EP-Mitgliedschaft zunächst einmal in eine Partei eintreten mussten und damit bereits im Vorfeld ideologische Entscheidungen getroffen haben, beobachten wir bei der Entscheidung über Umweltschutzmaßnahmen dennoch Geschlechterunterschiede innerhalb von Parteigruppen.
Frauen handeln umweltbewusster als Männer

Zusammengenommen zeigen unsere Ergebnisse, dass sich Männer und Frauen gleichermaßen für Umweltschutz aussprechen, es aber vorwiegend Frauen sind, die ihren Worten Taten folgen lassen. Diese Erkenntnisse sind von enormer politischer Relevanz. Der Frauenanteil im Europäischen Parlament ist von 16 Prozent nach den ersten Europawahlen im Jahr 1979 auf 41 Prozent im Jahr 2019 angewachsen und liegt damit deutlich über dem Durchschnitt nationaler Parlamente (22,2 Prozent in 2014). Doch noch immer sind Frauen auf allen institutionellen Ebenen der Legislative unterrepräsentiert. Unsere Forschungsergebnisse liefern ein weiteres Argument dafür, warum ein ausgewogenes Geschlechterverhältnis von so großer Bedeutung für unsere Gesellschaft ist: Frauen handeln in Bezug auf umweltpolitische Fragen anders als ihre männlichen Kollegen. Damit bilden sie die Geschlechterunterschiede, die auf Bürgerebene bestehen, ab und tragen dazu bei, dass weibliche Interessen gleichermaßen repräsentiert werden. Nein, mehr Frauen ins Parlament zu wählen wird nicht die alleinige Lösung der Klimakatastrophe sein. Und doch, es ist ein entscheidender Schritt in Richtung Zukunftsgerechtigkeit. ";https://www.derstandard.at/story/2000106604688/koennen-frauen-die-umwelt-retten;Standard;Lena Ramstetter, Fabian Habersack
24.09.2019;Interaktive Karte zeigt den Rückgang der Gletscher;"In Island wurde im vorigen Monat der erste Gletscher für tot erklärt, und auch hierzulande ziehen sich die Eismassen seit Jahrzehnten massiv zurück. ""Selbst wenn ab heute keine Treibhausgase mehr ausgestoßen würden, schmelzen die Gletscher noch Jahrzehnte weiter und können nicht mehr gerettet werden"", erklärt Marion Greilinger von der Fachabteilung Klimamonitoring und Kryosphäre der Zentralanstalt für Meteorologie und Geodynamik (Zamg).

Denn die Eismassen reagieren langsam, aber sehr sensibel auf Veränderungen – bis zum Ende des Jahrhunderts werden 80 Prozent der österreichischen Gletscher unaufhaltsam verschwunden sein. Um den Gletscherschwund zu stoppen, wäre Handeln, nach Einschätzung der Expertin, viel früher notwendig gewesen.
Wie sich die Gletscher verändert haben

Gebirgsgletscher gehören zu den sichtbarsten Indikatoren einer Klimaveränderung. Sie passen sich geänderten Temperaturen und Niederschlägen an, indem sie sich in größere Höhen zurückziehen oder in tiefere Lagen vorstoßen. Die Anpassung kann bei kleinen Gletschern einige Jahre dauern, bei größeren Gletschern dauert sie mehrere Jahrzehnte.

""Natürliche Klimaänderungen gab es immer und wird es auch immer geben. Die derzeitige Klimaänderung passiert jedoch deutlich schneller aufgrund der explosiv gestiegenen Emission von Treibhausgasen. Dieser rasante Anstieg beschleunigt das Abschmelzen der Gletscher, wie es durch die natürliche Klimaänderung in der Vergangenheit nicht passiert ist"", sagt Greilinger.
Schwierigkeiten bei der Vermessung

Die genaue Vermessung der ganzjährig von Eis bedeckten Flächen ist keine einfache Aufgabe. Für die Messung der Längenmeter werden die Oberflächenänderungen anhand von Messpunkten bestimmt – verschieben sich diese im Laufe der Zeit, kann die Änderung des Gletschers nur schwer rekonstruiert werden. Bei der Erhebung von Gletscherfläche und Eismasse durch Luftaufnahmen und Satellitenbilder kann die Abgrenzung der Gletscherumrisse Schwierigkeiten bereiten, da Eisränder von Altschnee bedeckt und somit nicht immer eindeutig bestimmt werden können. Außerdem variiert die Anzahl der Gletscher, wenn viele große Eismassen in mehrere kleine Teile zerfallen, erklärt Jan-Christoph Otto vom Fachbereich für Geografie und Geologie der Universität Salzburg.

In der Regionalinformation des Bundesamts für Eich- und Vermessungswesen (BEV) werden Gletscherflächen erst seit einer Gesetzesänderung 2012 erhoben und befinden sich in einem noch nicht abgeschlossenen Erfassungsstand. ""Die Gletscher sind zwar seit der dritten Landesaufnahme von 1869 bis 1887 auch in zahlreichen Karten grafisch wiedergegeben, die Anzahl und die Fläche sind aber nur mit entsprechend großem Aufwand zu ermitteln"", sagt Michaela Katzinger von der Abteilung für Marketing und Vertrieb des BEV.

Wie sich die Eismassen zwischen 1998 und 2015 verändert haben, zeigt die interaktive Karte. Die schwarze Umrandung gibt den früheren Umfang an, die weißen Flächen zeigen, wie die Gletscher 2015 aussahen. Die Daten für die interaktive Karte stammen aus dem österreichischen Gletscherinventar, das System erfasst seit 1969 erstmals alle österreichischen Gletscher nahezu zeitgleich und wird seitdem in unregelmäßigen Abständen erneuert. Das Inventar wird mittels Auswertungen von Luftaufnahmen erstellt. Für die Gletscherhochstände um 1850 wurde außerdem ein Inventar auf Grundlage historischer Karten und sichtbarer Moränen erarbeitet.

""Grundsätzlich sind alle Gletscher seit der Aufzeichnung zurückgegangen. Im Vergleich zum Maximalstand 1850 war im Zeitraum der letzten Erfassung nur noch ein Drittel der Gletscherfläche übrig"", beschreibt Otto die Entwicklung.
Ökologische und wirtschaftliche Folgen

Aufgrund ihrer Größe haben die österreichischen Gletscher keine nennenswerte Auswirkung auf das österreichische oder gar das globale Klima. Expertinnen und Experten warnen jedoch vor den ökologischen und wirtschaftlichen Folgen in der Region. Direkt betroffen sind demnach Alpenflüsse und der Wasserhaushalt, vor allem wenn in den heißen Sommermonaten der Wasserpegel sinkt. In einigen umliegenden Regionen gehen so wertvolle Ressourcen für Grundwasser und Böden verloren, betont Otto.
Flourish logoA Flourish data visualization
Die Aufnahmen zeigen den Brandner Gletscher in den Jahren 2003 und 2015 (links: Alpenverein/Kaufmann, rechts: Alpenverein/Gross).

Das Abschmelzen der Gletscher führt außerdem zu Rückkopplungen, beispielsweise steigt die oberflächennahe Temperatur, wenn die Gletscheroberfläche durch Abschmelzen nach Jahrzehnten 100 Meter tiefer liegt, oder das Muster der Schneeverfrachtung verändert sich, wenn sich die Form der Gletscheroberfläche durch Schmelze wandelt. Auch die Artenvielfalt wird dabei laut Greilinger maßgeblich beeinflusst, Pflanzen und Tiere, die den kühlen Lebensraum des hochalpinen Geländes bevorzugen, können irgendwann nicht mehr weiter nach oben ausweichen.

Ein weiterer Faktor ist der Permafrost, der in Verbindung mit Gletscher relevant ist. Er gilt als der Klebstoff der Berge. Tauen die Permafrostböden auf, verschärft sich die Problematik von Hangrutschungen oder Felsstürzen. Aus wirtschaftlicher Sicht spielt das Abschmelzen der Gletscher vor allem im Hinblick auf Gletscherskigebiete eine entscheidende Rolle, und auch für die Energiewirtschaft in glazial geprägten Abflussregimen sind die Gletscher von Bedeutung.
Das ewige Eis schmilzt weltweit

Doch nicht nur in den Alpen verschwinden die Gletscher – auch global ist das ewige Eis fast uneingeschränkt auf dem Rückzug. Die wirklich großen Eismassen befinden sich in Patagonien, im Himalaja, in Alaska, in der Arktis, in Grönland und in der Antarktis. Wenn diese auf den Klimawandel reagieren, sind die Auswirkungen wesentlich größer.

Im Gegensatz zu Europa sind in Zentralasien oder Südamerika Millionen von Menschen von der Wasserversorgung der Gletscher abhängig und stehen durch die Gletscherschmelze vor dem Problem der Wasserverknappung. Die verheerendste Auswirkung der schmelzenden Eismassen ist der Anstieg des Meeresspiegel, der vor allem durch das Schmelzwasser der Gletscher und Eisschilder in der Antarktis und Grönland vorangetrieben wird. Die Forschergruppe um Jonathan Bamber von der University of Bristol geht in einer aktuellen Studie davon aus, dass hunderte Millionen Menschen vertrieben, Millionen Quadratkilometer Land vernichtet und Inselstaaten zur Gänze im Meer ertränkt werden könnten.";Interaktive Karte zeigt den Rückgang der Gletscher;Standard;Anika Dang
16.01.2017;"""Wissenschaft hat die Zukunftsvision übernommen""";"STANDARD: Sie leiten den Masterstudiengang Art and Science an der Angewandten. Was kann man sich darunter im Detail vorstellen?

Widrich: Rektor Gerald Bast hat 2008 die Idee entwickelt, etwas mit Art und Science zu machen. Weil er wusste, dass ich eine gewisse Affinität dazu habe, bat er mich, einen Lehrgang zu entwickeln, der relativ rasch umgesetzt wurde. Seit 2009 läuft er nun schon, er geht über vier Semester, mit zehn bis zwölf Studierenden im Jahr. Von Beginn an hat sich gezeigt, dass wir aus der Angewandten rausgehen müssen, weil es dort nicht die Ressourcen im Bereich der Wissenschaft gab. Zudem haben wir wegen der Internationalität der Studierenden die Sprache auf Englisch umgestellt. Zwei Drittel haben einen Kunsthintergrund, ungefähr ein Drittel kommt aus der Wissenschaft.

STANDARD: Wissenschaft zeichnet sich durch Methodik aus, die Kunst spielt vielleicht eher mit Methoden, ist jedenfalls nicht daran gebunden. Wie bringt man das in eine Balance?

Widrich: Die Balance ist ein ständiger Kraftakt. Es ist zwar relativ leicht möglich, Kooperationspartner in der Wissenschaft zu finden, also in der Regel überbeanspruchte Wissenschafter, die noch ein bisschen Zeit für die Kunst aufbringen. Idealerweise sollte es aber nicht so sein, dass die Künstler die Wissenschaft für ihre Zwecke ausbeuten. Für Künstler sind Wissenschafter manchmal nur Helfer bei einer Recherche, aus deren Material sie dann etwas Neues generieren. Das ist nicht die Idee. Die komplementäre Illusion vonseiten der Wissenschaft gibt es auch: Hurra, da sind Leute, die uns schöne Bilder machen, die können das illustrieren, was wir herausfinden. Idealerweise ist es bei uns so, dass mit den Methoden der Kunst und mit denen der Wissenschaft an die Projekte herangegangen wird. Die Projekte sind sehr offen. In jedem Fall geht es um ein Verlassen der jeweiligen Box. Manchmal ist die Box so stark, dass ein Wissenschafter unter Pseudonym studiert hat, weil er befürchten muss, sich sonst zum Outlaw in seinem Fach zu stempeln.

STANDARD: Für die Künste ist diese Entgrenzung schon seit langem Programm, aber auch in der Wissenschaft ist Interdisziplinarität sehr wichtig.

Widrich: Bis vor einigen Jahrhunderten waren Kunst und Wissenschaft ja ohnehin nicht getrennt, wenn wir an jemanden wie Leonardo da Vinci denken. Seit der Romantik ist die Kunst die Domäne des Subjektiven, des Unwiederholbaren, während die Wissenschaft das Objektive und das Nachvollziehbare beansprucht. Also sehr gegenteilige Konzeptionen. Aber alle suchen auf eine gewisse Weise ""nach der Wahrheit"", da gibt es auch sehr ähnliche Werkzeuge. Die apparative Wahrnehmung der Welt und die Interpretation der Daten sind etwas, worin die Vorgänge einander sehr ähnlich sind.

STANDARD: In der Kunst hat schon eine Weile der Begriff ""Artistic Research"" Konjunktur. Das meint etwas anderes, als das, was Art & Science macht. Aber das haben Sie wohl auch im Blick?

Widrich: Natürlich beobachten wir das, aber es ist tatsächlich nicht genau unser Feld. Wir machen vielleicht etwas, was noch gar keinen Namen hat, und wir wollen das auch gar nicht zu genau definieren. Unser Senior Lecturer Bernd Kräftner hat wie ich einen Filmhintergrund, ist als Filmemacher vielleicht am ehesten offen für Universalwerkzeuge. Art-Scientist ist ja kein Berufsbild.

STANDARD: Gibt es Bereiche in den Wissenschaften, die eine Affinität im weitesten Sinn zur Kunst haben? Zum Beispiel so etwas wie theoretische Physik?

Widrich: Wir haben aktuell eine Kooperation mit dem Institut für Hochenergiephysik der Österreichischen Akademie der Wissenschaften, das im fünften Bezirk in Wien in einem sehr unscheinbaren Haus untergebracht ist. Dort bauen sie Sensoren für den Large Hadron Collider am Cern. Es geht um hochphilosophische Fragen, manche sind gar nicht so weit weg von künstlerischen Visionen. Aber die Zeit, in der jemand wie Einstein Probleme rein denkerisch löste, mit Papier und Bleistift sozusagen, die ist wohl vorüber. Heute beruht fast alles auf Datenmengen, man analysiert Big Data und Noise, den Lärm oder Schmutz in den Daten. Und die Wissenschaft hat von der Kunst die Zukunftsvisionen übernommen. Früher haben Menschen wie Fritz Lang oder Arthur C. Clarke visionär vorausgedacht. Dieser Rang wurde der Kunst abgelaufen. Heute spricht man davon den Genpool zu reinigen, aber es sind nun die Wissenschafter, die so etwas in den Raum stellen. Daraus ergeben sich ethische Fragen, da geht es ganz schnell um Geld und Macht.

STANDARD: Spielt denn Ethik überhaupt eine Rolle im Studiengang?

Widrich: In unserem Curriculum sind die Angebote dazu limitiert, diese Dinge machen die Studierenden teilweise an anderen Instituten. Aber die Fragen tauchen immer wieder auf. Wir hatten kürzlich eine unglaublich interessante Diskussion mit dem Radiologen Paul Kainberger über Mammografien. Diese Röntgenbilder von der weiblichen Brust werden im Moment noch von Menschen ausgewertet, bis zu 2000 Bilder am Tag. Nun wird versucht, das Maschinen beizubringen. Und die Maschinen beginnen langsam so gut zu werden wie die Menschen. Dann entscheidet aber eine Maschine über Leben und Tod. Was ist das Beste aus beiden Welten? In solchen Fällen erweist es sich als hilfreich, dass unsere Studierenden aus aller Welt kommen. Eine Studentin aus dem Iran machte uns darauf aufmerksam, dass in ihrem Heimatland der Arzt immer zuerst mit der Familie spricht, nicht mit den Patienten selbst.

STANDARD: Empfinden angehende Künstler die Begegnung mit der Wissenschaft manchmal als Kränkung? Immerhin kratzt das alles ja stark an der Vorstellung vom Originalgenie.

Widrich: Wir sind momentan in einer Zwischenphase, in der wir herausfinden, wie intelligent die Maschinen werden können. Einer unserer Studenten hat zum Beispiel eine Algorithmic Search for Love programmiert, für die Suche hat er 5000 Filme inklusive aller Dialoglisten eingespeist. Und dann hat er alle Momente daraus, in denen das Wort Liebe vorkommt, zu einem Found-Footage-Film zusammengeschnitten, den man als Film absolut ernst nehmen kann.

STANDARD: Zugleich wirkt das wie eine Fortsetzung von Konzeptkunst: Der Künstler macht eine Partitur, die Maschine führt durch. Die Arbeit hat aber auch etwas Wissenschaftliches, wenn man vergleichsweise an eine datengestützte Literaturwissenschaft denkt, die auch große Mengen Text nach Parametern durchsucht.

Widrich: Diese Zusammenhänge treffen ganz gut den Punkt. Im Cern entstehen gigantische Datenmengen. Das vielbeschworene Gottesteilchen ist eine Datenspur. Der Aufprall ist ein fotografisches Phänomen, davon werden Datenmengen gesammelt, gereinigt, und dann intensiv durchberechnet. So schauen die Teilchenforscher in die Welt, das ist eigentlich auch ein sehr langsames Geschehen. Aber im strengen Sinn gibt es da nichts zu sehen, man schaut auf einen Film.";https://www.derstandard.at/story/2000050501665/wissenschaft-hat-die-zukunftsvision-uebernommen;Standard;Bert Rebhandl
15.07.2019;Mutierte orange Menschen: Was passiert, wenn wir Marsianer werden;"Der rote Planet übte immer schon eine Faszination auf die Menschheit aus. Der Mars ist der einzige Planet in unserem Sonnensystem, der sich in absehbarer Zeit bereisen und letztlich kolonisieren ließe. Tech-Entrepreneur Elon Musk will bereits 2024 das erste bemannte Raumschiff losschicken, um nach achtmonatiger lebensgefährlicher Reise durchs All dann 2025 zur Eroberung Landung ansetzen zu können. Die US-Weltraumbehörde glaubt zwar, erst um 2040 so weit sein zu können, hat inzwischen jedoch schon einmal zahlreiche Technologien erfunden, die zur extraterrestrischen Fernreise und Exkursion notwendig sind: vom CO2-Sauerstoffwandler bis hin zum Ziegelstein aus der Mikrowelle. Und erfinderische Köpfe wie die MIT-Forscherin Dava Newman arbeiten seit geraumer Zeit an Outfits, die uns selbst unter den widrigsten Bedingungen (über)leben lassen.

Ohne all diese gewaltigen technischen Errungenschaften und nicht nur für kommende Generationen inspirierenden Herausforderungen zu schmälern, drängt sich eine Frage vor alle anderen: Sind wir Menschen überhaupt physisch und psychisch in der Lage, Marsianer zu werden? Die kurze Antwort vorweg: Wenn dies gelingt, wird es keine schmerzfreie Geburt. Oder, wie der US-Schriftsteller und Spacejunkie Stephen Petranek es formuliert: ""Es ist möglich, dass es eines Tages eine menschliche Spezies auf der Erde geben wird, die ein wenig anders ist als die menschliche Spezies auf dem Mars.""
Ein ungemütlicher Ort

Um zu verstehen, welche körperlichen Herausforderungen der Mars uns stellt, muss man sich die Rahmenbedingungen ansehen. Er ist rund sechsmal kleiner und verzeichnet an der Oberfläche 62 Prozent weniger Schwerkraft als die Erde. Die Atmosphäre ist toxisch und tödlich – zumindest für die allermeisten Lebewesen, wie wir sie kennen. Die Temperaturen schwanken zwischen frischen -5 und -125 Grad Celsius. Wasser gibt es reichlich, allerdings ist dieses gefroren. Der atmosphärische Druck ist so niedrig, dass weder Wasser fließen noch Menschen überleben könnten. Die dünne Atmosphäre und die geringe Stärke des planetaren Magnetfelds sorgen weiters dafür, dass man an der Oberfläche hundertfach mehr kosmische Strahlung abbekommt als auf der Erde. Und ebenfalls eher ungemütlich: Regelmäßige Sandstürme neigen dazu, das Sonnenlicht wochenlang zu verdrängen. Auf der positiven Seite: Es gibt ansonsten reichlich Sonne, und ein Tag auf dem Mars dauert in etwa so lange wie auf der Erde. Nur ein Marsjahr zählt etwa doppelt so viele Tage.
Ist der Mars bewohnbar? Ein Überblick von Geomorphologin Mari Foroutan.
TED-Ed
Nichts für Muskelprotze

Diese planetarischen Umstände sind kein Zuckerschlecken für unseren Körper. Der Mensch entstand nach Millionen Jahren der Evolution, geformt durch die Lebensbedingungen auf der Erde. Unsere Muskeln, unsere Knochen, unser gesamtes Erscheinungsbild ist von der Schwerkraft geprägt. Wie sich die geringere Schwerkraft des Mars dauerhaft auf unsere Physis auswirken würde, ist noch nicht belegt. Die Raumfahrt liefert jedoch zahlreiche, zumindest vorsichtig stimmende Erkenntnisse bezüglich des Aufenthalts in Schwerelosigkeit: Die Muskeln schrumpfen, die Knochendichte nimmt ab, das Herz-Kreislauf-System wird beeinträchtigt und auch die Sehleistung beeinflusst. Das Immunsystem wird geschwächt, und die vielfach höhere kosmische Strahlung auf dem Mars könnte zur DNA-Instabilität führen und das Krebsrisiko erhöhen.
Halluzinationen

Von Raumfahrern, die längere Zeit im All verbracht haben, wissen wir auch, dass die Distanz zur Erde Auswirkungen auf die Psyche hat. Kosmonaut Michail Kornijenko, der sich mit dem Astronauten John Kelly fast ein Jahr lang auf der Internationalen Raumstation befand, erklärte nach seiner Rückkehr, vor allem die Erde selbst und ganz einfache Dinge stark vermisst zu haben: ""Ich vermisste Gerüche. Ich vermisste Bäume, ich habe sogar von ihnen geträumt. Ich habe sogar halluziniert. Ich dachte, ich rieche echtes Feuer, und dass etwas gegrillt wird. Ich habe schließlich Bäume an die Wand gehängt, um mich aufzumuntern. Du vermisst die Erde dort oben wirklich.""

Die Nasa veröffentlichte 2016 eine Studie zu den psychischen Auswirkungen der Raumfahrt. Darin werden häufige Probleme wie Einsamkeit, vorübergehende Angstzustände und Depressionen – teils mit psychosomatischen Symptomen – vor allem zu Beginn von Missionen im All beschrieben. Starke Stimmungsschwankungen wie bei manischer Depression oder Schizophrenie wurden hingegen nicht verzeichnet. Das Leben auf einem fremden Planeten, abgeschnitten von den irdischen Wurzeln, könnte jedenfalls negative Gefühle wie Isolation und Beklemmung verstärken.
Astronaut John Kelly spricht über seinen 340-tägigen Aufenthalt auf der Raumstation ISS und seine Rückkehr.
NASA
Rückkehr auf die Erde

Eine Erkenntnis sollte ebenfalls nicht verachtet werden: Was in der Raumkapsel passiert, bleibt nicht in der Raumkapsel. Nach seinem einjährigen Aufenthalt auf der ISS litt Astronaut Scott Kelly noch sechs Monate lang an geistigen Einbußen. Nicht nur das: Seine Haut fühlte sich bei Berührungen zeitweise an, als würde sie brennen, und der ganze Körper war steif und ausgelaugt.
Warum ist das alles wichtig für den Mars?

Die Umstände im All sind nicht eins zu eins mit den Bedingungen auf dem Mars zu vergleichen, das Wissen darüber dürfte jedoch noch entscheidend für künftige Missionen zum roten Planeten und dessen Kolonisierung sein. Menschen, die von der Erde zum Mars aufbrechen, müssen nicht nur eine achtmonatige Reise in einer Blechbüchse überstehen, sondern auch mit den dortigen widrigen Lebensbedingungen zurechtkommen. Sprich: in geringer Schwerkraft existieren und sich vor der starken Strahlung, der dünnen Atmosphäre, der extremen Kälte und den desaströsen Stürmen schützen.
Höhlenmenschen

Das bedeutet: Sollte die ""Eroberung"" des Mars glücken, würden die menschlichen Siedler viele Jahrzehnte oder vielleicht gar Jahrhunderte in geschlossenen Behausungen verbringen, bevor die Technologie fortgeschritten genug für Terraforming ist. Zunächst in sehr kompakten Einheiten, die einer Raumstation ähneln, und später in (teilweise) unterirdischen Gebäuden. Wobei klar sein muss, dass die Architektur bis zum Aufbau und der Realisierung einer vitalen Industrie wohl nicht den traumhaften Konzepten so mancher Vordenker ähneln wird. Weniger Glaspalast, mehr Bunker. Mit Gewächshäusern statt Gemüsegärten, Solarpanelen zur Stromerzeugung, Systemen zur Produktion von Sauerstoff und Rückgewinnung von Wasser und vielen weiteren technischen Erfindungen, die vieles dessen ermöglichen, was auf der Erde selbstverständlich ist.
Marsha ist ein von der Nasa mit 500.000 Dollar gefördertes Behausungsprojekt für eine 3D-gedruckte Unterkunft auf dem Mars.
SpaceFactory
Vegane Ernährung

Für die ersten Marsianer wird die Ernährung eine der größten Herausforderungen sein. Trockennahrung, die von der Erde mitgeschickt wird, kann keine dauerhafte Lösung sein. Die Eigenerzeugung von Vitaminen und Proteinen aus Pflanzen wie Kartoffeln, Bohnen und Tomaten wird neben Sauerstoff und Wasser der Schlüssel zum Überleben sein. Die Nasa hat aktuell rund 200 Speisen auf dem Plan der ISS-Besucher, für langfristige oder dauerhafte Aufenthalte auf einem Planeten sei Abwechslung nicht nur aus gesundheitlichen Gründen essenziell, erklärt Vickie Kloeris, Ernährungsexpertin der US-Weltraumbehörde. Abstriche müssen so oder so hingenommen werden: Aufgrund des hohen ökologischen Aufwands dürfte Fleisch zumindest für längere Zeit nicht auf dem Speiseplan stehen. Bis dahin wird es vielleicht aber schon echt gute unechte Steaks und Burger geben.
Geboren auf dem Mars

Bevor all diese Probleme gelöst sind, muss man sich natürlich die Frage stellen, ob eine Kolonisierung im Sinne der Schaffung einer eigenständigen Marsbevölkerung auf elementarer Ebene überhaupt möglich ist. Sprich Fortpflanzung. Hier gibt es bisher kaum aussagekräftige Studien. Noch hat sich kein Menschen und kein größeres Säugetier im All oder bei verminderter Schwerkraft vermehrt. Studien dazu sind zumindest geplant, und es konnte bereits gezeigt werden, dass gefrorenes menschliches Sperma seine Aktivität im All behält.

Darüber hinaus wird es spekulativ, sowohl was den Akt der Reproduktion selbst als auch das Kinderkriegen betrifft. Sex allein könnte zum ""Akt"" werden. Körperflüssigkeiten verhalten sich bei verminderter Schwerkraft anders, und die üblichen Mechaniken der Kopulation müssten an die neuen Umstände angepasst werden. Am meisten beschäftigt Wissenschafter aber, welche Auswirkungen die vielfach höhere kosmische Strahlung und die verminderte Schwerkraft auf den Fötus haben. Mutationen während der Entwicklung könnten zu Fehlgeburten oder Krankheiten führen.
Schlaksige Körper, orangefarbene Haut, große Augen: Wie würden Menschen aussehen, die auf dem Mars leben? Eine Visualisierung von ""Science Insider"".
Science Insider
Veränderungen des Körpers

Sollte eine Geburt auf dem Mars gelingen, muss man zudem gefasst darauf sein, was dabei herauskommt. Mit Sicherheit ein entzückendes Baby, dessen Körper sich dem britischen Astrophysiker Chris Impey zufolge rasch an die Mars-Gegebenheiten anpassen dürfte. Schon die Physis erwachsener Erdauswanderer würde sich mit den Jahren der geringeren Schwerkraft fügen, Muskeln, Knochen und Weichteile anders wachsen. ""Nach nur ein paar Generationen könnten diese Menschen ein Ableger der Menschheit werden. Sie werden sich wahrscheinlich zu etwas anderem entwickeln"", so Impey. ""Und psychologisch werden sie sich wie ein neues Volk fühlen.""

Annahmen, die auch der Anthropologe Cameron Smith teilt. Zwar seien genetische Mutationen willkürlich, doch anhand der äußeren Lebensumstände ließen sich so manche physischen Anpassungen wenigstens grob prognostizieren – wenngleich gravierende Veränderungen des menschlichen Körpers wohl nur über eine große Zeitspanne von hunderten oder tausenden Jahren geschehen würden. Müssten Marsianer etwa konstant mit einem geringeren Sauerstoffgehalt auskommen als Erdbewohner, würden ihre Lungen wachsen, um mehr aufnehmen zu können. Das könnte zu einem mächtigeren Brustkorb führen. Wie sich die unterschiedliche Zusammensetzung der Luft im Detail auswirken würde, sei aber aus heutiger Sicht nicht abschätzbar.
Schlaksige Figur, großer Kopf

Aufgrund der geringeren Schwerkraft würden Menschen auf dem Mars dünnere Knochen und weniger Muskelmasse entwickeln. Der Körper hätte weniger zu kämpfen, um in die Höhe zu wachsen. Das Herz müsste weniger kräftig pumpen, um Blut zirkulieren zu lassen. Gleichzeitig könnte aufgrund des geringeren Widerstands der Hirndruck steigen, was bei Marsianern zu größeren Schädeln führte. Über Jahrtausende gerechnet könnte das auch zu breiteren Becken bei Frauen führen.

Dem Biologen Scott Solomon zufolge dürfte sich überdies die vermehrte kosmische Strahlung sichtbar machen. Während das verminderte Sonnenlicht und der Aufenthalt in Bunkern oder unter der Oberfläche die Haut der Marsbewohner zunächst wohl bleicher machen würden, könnte die höhere Strahlung dazu führen, dass Carotin stärker an die Oberfläche dringt und die Haut orange färbt. Die geringere Verfügbarkeit von natürlichem Licht könnte wiederum zu einer Anpassung und vielleicht Vergrößerung der Augen führen.
Menschen sind voller Keime, die sie auch ins All mitschleppen. Der Besuch von Erdbewohnern könnte für gebürtige Marsianer eine große Gefahr darstellen.
SciShow Space
Keimfreie Zone

Da auf dem Mars keine Organismen leben, die dem Menschen ähneln, wären Marsianer deutlich weniger Viren und Bakterien ausgesetzt als Erdbewohner. Einerseits würde sich das positiv auf das Bevölkerungswachstum auswirken. Andererseits bestünde so immerzu die Gefahr einer ernsthaften Epidemie, sobald ein neuer Besucher von der Erde ankommt. Hat sich der Mars eines Tages als eigenständige Kolonie etabliert, in der native Marsianer die Mehrheit bilden, würde das jedes Aufeinandertreffen mit Erdmenschen zu einem Risiko für beide Seiten machen. Jedes Volk würde seine eigenen Keime mit sich bringen und so eine Gefahr für sein Gegenüber darstellen. Die gute Nachricht: Wie die Nasa-Zwillingsstudie zeigte, wirken Impfstoffe wie jene gegen die Grippe auch bei verminderter Schwerkraft. Also ist vielleicht eine gegenseitige Immunisierung möglich.
Riskante Heimreise

Wirklich interessant ist gewiss die Frage, ob Erdmenschen und Marsmenschen auf lange Sicht eine Verbundenheit werden aufrechterhalten können. Zum einen würden die Marsianer laut Astrophysiker Impey früher oder später eine eigene Identität finden. Andererseits treiben die Distanz und später auch die gesundheitlichen Gefahren einen Keil zwischen die Völker. Wer auf dem Mars bei einem Drittel der irdischen Schwerkraft aufwächst, könnte sich ohne technische Hilfsmittel nicht mehr auf der Erde aufhalten. Der Blutkreislauf, die Muskulatur und das Skelett würden unter der Erdgravitation wortwörtlich eingehen.
Elon Musks Raumfahrtunternehmen will bereits 2024 erste bemannte Raketen zum Mars schicken. Für die Passagiere wird es kein Zuckerschlecken.
Tech Insider
Wollen wir Marsianer werden?

Der romantische Ausblick auf die Kolonisierung des Mars verspricht den Fortbestand unserer Spezies, sollten wir die Erde zerstören, sollte der Erde etwas geschehen. Und der Aufbau einer Marskolonie würde nicht nur neue Rohstoffquellen für künftige Megaprojekte liefern, sondern auch ein Sprungbrett in bislang unerreichte Dimensionen des Weltalls sein. Und hat man das nötige Kleingeld, könnte man seine Freunde mit einem Feriensitz an einem wirklich exotischen Örtchen beeindrucken. Sofern sie die Anreise überleben.

Es gibt also gute Gründe, die Reise zum roten Planeten anzutreten und heute noch verrückt klingende Ideen zu prüfen. Gleichzeitig besteht die Gefahr, mit den falschen Erwartungen an dieses Jahrtausendprojekt heranzugehen. Wer den Mars vor allem als Chance zur Rettung unserer Spezies sieht, sollte sich nicht wundern, wenn letzten Endes, nach hunderten Generationen, nicht der eigene Arterhalt gesichert, sondern eine neue Spezies geschaffen wurde, der die Erdlinge nicht wie die engsten Verwandten am Herzen liegen.";https://www.derstandard.at/story/2000106030310/mutierte-orange-menschen-was-passiert-wenn-wir-marsianer-werden;Standard;Zsolt Wilhelm
21.07.2018;Wie die Medizin der Zukunft funktioniert;"Die Stimmung im Kreissaal war angespannt. Das Baby schien im Geburtskanal festzustecken, für einen kurzen Moment. Doch dann hörten alle erleichtert den ersten Schrei des Neugeborenen. Während die Ärztin das Kind abnabelt, wird von einer Assistentin bereits eine digitale Gesundheitsakte erstellt, eine Art Mutter-Kind-Pass, der jedoch eine lebenslange Angelegenheit sein wird. Willkommen in der digitalisierten Medizin. Wir wagen einen Blick in die Zukunft.

Ein paar Bluttropfen des Babys reichen aus, um sein Genom zu sequenzieren und es in die digitale Akte zu spielen. Künftige Untersuchungen, Erkrankungen und Impfungen werden alle dort eingetragen, stehen Ärzten, Patienten, Versicherungen und Forschern zu jeder Zeit zur Verfügung.

Alle Lebensjahre im Kalkül

Gene, das weiß man, schalten sich im Laufe eines Lebens an und aus. Wie und wann genau, darüber könnten diese Datenfiles Auskunft geben und insofern auch Krankheitsrisiken genauer definieren. Und wenn dieser neugeborene Mensch 50 Jahre später tatsächlich starke Bauchschmerzen bekäme, würde die Diagnose viel schneller gestellt, der Krebs viel genauer als bisher typisiert und behandelt werden.

""Daten werden ein medizinischer Rohstoff, aus dem wir maßgeschneiderte Handlungsanweisungen ableiten"", sagt Michael Krainer, Onkologe an der Universitätsklinik für Innere Medizin an der Med-Uni Wien. Ein Programm entscheidet, welche Therapie medizinisch Sinn macht.

""Die neue Medizin der Zukunft wird personalisiert sein, also individuell zugeschnitten auf unsere genetisch ererbten Eigenschaften, unsere genetischen Erfahrungswerte und unsere immunologische Geschichte"", fasst es der Molekularbiologe Giulio Superti-Furga vom Zentrum für Molekulare Medizin an der Med-Uni Wien zusammen.

Therapiert wird nicht mehr wie heute nach dem Gießkannenprinzip, sondern maßgeschneidert. Krebs könnte, rechtzeitig erkannt und behandelt, zunehmend eine vorübergehende Krankheitsepisode werden – und seinen todbringenden Nimbus verlieren.
Evolution steuern

Oder eines Tages vielleicht sogar vollständig zerstört werden. Denn Krebsspezialisten setzen große Hoffnungen gerade deshalb auf die Immunonkologie. In diesem therapeutischen Ansatz werden nämlich nicht die kranken Zellen selbst angegriffen, sondern die vom Krebs lahmgelegten Immunzellen reaktiviert. Der Kampf über die Bande findet auch heute schon statt.

Die Wissenschafter haben parallel dazu auch gelernt, wie sich die DNA des Menschen mit eingeschleusten Viren manipulieren lässt. Mit der Genschere, dem CRISPR/Cas9-Verfahren, lassen sich kranke Teile des menschlichen Bauplans ersetzen. Mit dieser biochemischen Methode können Krankheiten möglicherweise sogar geheilt werden.

Das ist einstweilen aber nur eine Idee mit einem enormen Risiko: Die biomolekulare Manipulation hat das Potenzial, die gesamte Menschheit zu verändern. Dann nämlich, wenn solche Verfahren auf die Keimzellen der Menschen angewendet werden, etwa um Erbkrankheiten zu heilen. Damit könnten unter Umständen schwere genetische Erkrankungen bereits im Kindesalter tatsächlich ausradiert werden, doch wenn diese genmanipulierten Individuen eines Tages selbst Kinder bekommen, können wir ""die Folgen derzeit überhaupt nicht abschätzen"", so Superti-Furga, und das sei der Grund, warum sich die Wissenschaftscommunity heute in diesem Bereich Selbstbeschränkungen auferlegt hat. Durch ein Mehr an Wissen könnte diese moralische Schranke eines Tages jedoch fallen.

Ersatzteile finden

Abgesehen davon werden sich Therapien für den Menschen im ausgehenden 21. Jahrhundert noch in einem ganz anderen Gebiet verändern. ""Durch die Fortschritte in der Transplantationsmedizin eröffnen sich noch ungeahnte Möglichkeiten"", prognostiziert Patrick Schöggl, Direktor bei Deloitte Österreich und dort für Healthcare zuständig.

Die Zukunft, so der Experte, habe bereits begonnen. Organe werden von Mensch zu Mensch verpflanzt, könnten aber schon bald auch im Labor gezüchtet werden. Wenn es um die menschliche Hardware geht, also Knochen zum Beispiel, werden schon heute ganze Schädeldecken mit speziellen Programmen auf Basis von CT-Bildern konstruiert und in 3D-Druckern ausgedruckt – für Patienten nach Unfällen zum Beispiel.

Auch an der Rekonstruktion von Blutgefäßen wie der zentralen Aorta arbeiten Forscher wie Utz Kappert am Herzzentrum in Dresden. ""Als Operateur kann ich vor einem Eingriff sämtliche Schritte am Computer planen, simulieren, durchspielen und schon im Vorhinein Alternativen überlegen"", erläutert er die Vorteile. Die kühnste Vision sind wohl Organdrucker, die im Operationssaal ausdrucken, was gerade gebraucht wird.
Fit ohne Ende

Denn klar ist: Wenn die Menschen immer älter werden, wird auch der Bedarf an Ersatzteilen steigen. Bei jedem medizinischen Fortschritt wird der Erhalt der Lebensqualität von Patienten ein zunehmend wichtiges Kriterium sein. Laut EU-Aging-Report werden 13 Prozent der Europäerinnen im Jahr 2070 älter als 80 Jahre sein. Zum Vergleich: Heute sind es nur fünf Prozent.

""Im Alter länger gesund bleiben, weil die Menschen Gesundheitsvorsorge betreiben"", nennt Manfred Anderle, Obmann der österreichischen Pensionsversicherungsanstalt, eines der vorrangigen Ziele. Aus seiner Sicht bedeutet das, das Pflegegeld erst so spät wie möglich bewilligen zu müssen. Für dieses Ziel werden in der PVA die Maßnahmen zur ""Gesundheitsvorsorge aktiv"" auch für Pensionisten forciert.

Denn Menschen sollen so lange wie möglich selbstständig leben können. Auch dabei wird die elektronische Akte wichtig sein. Möglicherweise findet eines Tages sogar eine automatische Fernwartung des Menschen statt. Intern und extern. Klar ist, dass im Zeitalter der Digitalisierung Berufsgruppen enger zusammenarbeiten müssen.
Werkzeuge gegen Demenz

Wie das geht, erleben Neurologen, IT-Experten und Designer gerade im Rahmen des EU-Projektes Memento, das Menschen mit Demenz das Wohnen zu Hause länger als bisher ermöglichen wird. Die zentrale Frage ist, welche digitalen Tools die schwindende Gedächtnisleistung im Alltag kompensieren können beziehungsweise Hilfestellungen in Notsituationen sein könnten. Eine GPS-Uhr zum Beispiel könnte einspringen, wenn Demenzkranke ihre Orientierung verloren haben und umherirren. Sensoren zu Hause könnten gewährleisten, dass der Herd immer abgeschaltet ist.

""Mediziner verstehen nicht, wie Informatiker denken, und Informatiker haben keine Ahnung, was Demenz ist"", sagt Neurologin Elisabeth Stögmann, die im Rahmen der Memento-Workshops erlebt, was technisch machbar ist. Die unterschiedlichen Denkweisen empfindet sie als Herausforderung, davon sollen zukünftige Patienten profitieren.

Was Demenz verursacht, ist bis heute unklar. Klar ist lediglich, dass die Lebensbedingungen eine Rolle spielen. Prävention, also die Vermeidung von Krankheiten, hat deshalb in nahezu allen Bereichen Priorität. Wer weiß, dass er ein erhöhtes Schlaganfallrisiko hat, könnte weniger Fleisch essen oder regelmäßig Sport treiben.
Einflüsse von außen

""Wir entdecken gerade erst den Einfluss, den die Umwelt auf die DNA des Menschen hat"", betont Superti-Furga und sieht Chancen, dass die Vorzeichen von Erkrankungen früher erkannt und damit besser behandelt werden können. Eines Tages könnte es sein, dass winzige Nanoroboter im Organismus patrouillieren, um Risiken zu orten.

Klingt alles nach Science-Fiction? Die Marktforscher von Frost & Sullivan haben kürzlich eine Studie präsentiert, in der sie dem Geschäftsfeld der digitalen Pathologie in Verbindung mit künstlich intelligenten Systemen hohe Wachstumsraten prognostizieren. Bis 2021 wird der Markt um 13,2 Prozent wachsen.";https://www.derstandard.at/story/2000077082135/wie-die-medizin-der-zukunft-funktioniert;Standard;Karin Pollack
07.01.2017;Stephen Hawking: Geburtstagsständchen für den Popstar der Physik;"Albert Einstein, Isaac Newton, ""Star Trek""-Held Data und Stephen Hawking sitzen in einer Holodecksimulation und pokern. Als Hawking den Einsatz erneut erhöht, schmeißt Newton die Karten hin, sichtlich genervt vom nutzlosen Gambeln. Data entscheidet ebenfalls, nicht mehr mitzuziehen. Nur Einstein, vor dem sich der größte Stapel an Chips türmt, will Hawking das Pokerface nicht abnehmen: ""Alle Quantenfluktuationen im Universum werden die Karten in Ihrer Hand nicht auswechseln."" Mit breitem Grinsen legt Hawking die Karten auf den Tisch: ein Vierling – die Runde geht an ihn.

Mit zahlreichen Gastauftritten bei den ""Simpsons"", ""Futurama"" und der ""Big Bang Theory"" ist der britische Physiker Stephen Hawking zwar äußerst routiniert, was Fernsehproduktionen angeht. Der Auftritt in ""Star Trek"", seiner ""liebsten Science-Fiction-Serie"", hat ihm allerdings ein besonderes Vergnügen bereitet, wie er gerne erzählt. Die Szene scheint Hawking wie auf den Leib geschrieben: Er tritt gegen die Gesetze der Wahrscheinlichkeit und die Größen der Physik an – und gewinnt.

Dass diese Losung in gewissem Sinn für Hawkings Leben steht, wird am 8. Jänner besonders deutlich, wenn er seinen 75. Geburtstag feiert. Mit Anfang 20 war Hawking PhD-Student in theoretischer Astronomie und Kosmologie an der Universität Cambridge, an der er bis heute tätig ist. Hawking galt als talentiert, wenn auch nicht gerade fleißig. Die Krankheit meldete sich zunächst mit unvermittelten Stürzen. Meist blieb nur ein Kratzer, bei einem schweren Sturz über Stiegen verlor er aber das Bewusstsein. Als er 21 Jahre alt war, wurde schließlich Amyotrophe Lateralsklerose (ALS) diagnostiziert. Bei dieser degenerativen Erkrankung des motorischen Nervensystems gaben ihm die Ärzte noch ""zwei bis drei"" Jahre zu leben.
Drohung des Todes

Hawking verfiel in eine Depression, ging aber an die Uni zurück, entdeckte seine Liebe zu Wagner und identifizierte sich mit der Tragik. Die Krankheit schritt langsamer voran als erwartet. Die medizinische Erklärung dafür: Er leidet vermutlich an einer chronisch juvenilen ALS, die einen längeren Krankheitsverlauf hat. Hawking hat eine andere Erklärung: ""Was den Unterschied machte, war, dass ich mich verliebt habe.""

Trotz der kurzen Lebenserwartung heiratete die drei Jahre jüngere Romanistikstudentin Jane Wilde Hawking im Jahr 1965. Drei Kinder gingen aus der Beziehung hervor. Für Hawking bedeutete die Ehe einen Motivationsschub – nun wollte er einen Job finden, und dazu musste er das Doktorat abschließen. ""Damals begann ich zum ersten Mal in meinem Leben hart zu arbeiten. Und zu meiner Überraschung mochte ich es"", erzählte er 2012 in einem Dokumentarfilm. Aus der Drohung des Todes schöpfte er Motivation für sein Leben: ""Da jeder Tag mein letzter sein könnte, habe ich das Begehren entwickelt, aus jeder Minute das Beste zu machen.""
Universum ohne Gott

Wer meint, dass die erschwerten Umstände Hawkings wissenschaftlichen Ehrgeiz einbremsten, der irrt – im Gegenteil. ""Da ich wusste, dass ich nicht viel Zeit haben würde, wollte ich die großen Fragen der Kosmologie angehen."" Der Doktorand wandte sich keiner geringeren Frage zu als der nach dem Ursprung des Universums.

Dazu muss man wissen, dass zu dieser Zeit zwei Theorien gegeneinander antraten, um die Entstehung des Universums zu erklären: einerseits die Steady State Theory, nach der das Universum schon immer existiert hat. Damals war das die bequemere Theorie für viele Physiker, ersparte sie ihnen doch die unangenehme Frage, wie das Universum begonnen hat und ob es dazu einen Schöpfer bräuchte. Diese pikante Frage stellte sich Hawking, indem er sich der Konkurrenztheorie annahm: der Urknalltheorie. ""Ich fragte mich, könnte der Big Bang von sich aus passieren, ohne die Existenz eines Gottes?"" Seine monatelangen Berechnungen ergaben: ja.
Es gibt immer einen Ausweg

Seine Frau und Studienkollegen verbrachten Wochen an der Schreibmaschine, um Hawkings Dissertation abzutippen – er selbst war dazu nicht mehr in der Lage. Der Inhalt seiner Arbeit brachte ihm in der Fachwelt rasch ein gewisses Renommee ein, auch wenn er der breiten Öffentlichkeit damals noch nicht bekannt war.

Seine bekannteste Theorie stellte Hawking 1974 mit der nach ihm benannten Hawking-Strahlung auf. Während man zuvor dachte, Schwarze Löcher würden eine derart starke Gravitationskraft ausüben, dass sie keine Strahlung aussenden, konnte Hawking das Gegenteil zeigen: Schwarze Löcher zerstrahlen – je nach Masse mehr oder weniger schnell. Was schließt Hawking daraus? ""Wenn Sie sich wie in einem Schwarzen Loch fühlen, geben Sie nicht auf, es gibt einen Weg hinaus.""

Die Arbeit, in der es ihm erstmals gelang, Relativitätstheorie, Quantentheorie und Thermodynamik zusammenzuführen, brachte ihm breite Anerkennung ein. So folgte eine Einladung an das California Institute of Technology in die Gruppe von Kip Thorne. Während des einjährigen Aufenthalts verlor Hawking die Fähigkeit, mit seinen Händen zu schreiben. ""Gleichzeitig entwickelte er außergewöhnliche Wege zu denken"", erinnert sich Thorne an diese Zeit. ""Er konnte gedanklich zu den Grenzen des Wissens reisen und Dinge sehen, die sonst niemand sehen konnte.""
Im eigenen Körper gefangen

Den Tiefpunkt seiner Krankheit erlitt Hawking 1985, als er infolge einer Lungenentzündung nicht mehr sprechen konnte. 2012 blickte er zurück: ""Ich fühlte mich in meinem Körper gefangen."" Der überraschende Lichtblick kam durch einen Programmierer aus Kalifornien, der Hawkings ersten Sprachcomputer entwickelte. Seitdem spricht Hawking mit amerikanischem Akzent, was die Queen nicht unbedingt goutiert.

Durch den Sprachcomputer gelang es Hawking auch, sein erstes populärwissenschaftliches Buch fertigzustellen, das 1988 unter dem Titel ""A Brief History of Time"" (""Eine kurze Geschichte der Zeit"") erschien. Zunächst wurde er für sein Vorhaben belächelt, ein Buch über den Urknall, Schwarze Löcher und Quantenphysik schreiben zu wollen, das sich ""an jedem Flughafen"" verkaufen sollte. Doch die ""Kurze Geschichte der Zeit"" erreichte millionenfache Verkäufe und hielt sich vier Jahre in den Bestsellerlisten. Hawking war damit endgültig zum Popstar der Physik geworden.
Kehrseite des Erfolgs

Der Ruhm hatte aber auch seine Schattenseiten – seine Frau und er entfremdeten sich und trennten sich 1990, 1995 folgte die Scheidung. Wenig später heiratete Hawking seine Pflegerin. Mittlerweile sind sie geschieden.

Da Geburtstage immer auch die Endlichkeit des Lebens ins Sichtfeld rücken, interessiert uns hier noch, wie Hawking über den Tod denkt: ""Ich habe keine Angst vor dem Tod, aber es gibt so vieles, was ich vorher noch herausfinden will."" In diesem Sinne: Happy birthday, Stephen Hawking!";https://www.derstandard.at/story/2000050190141/stephen-hawking-geburtstagsstaendchen-fuer-den-popstar-der-physik;Standard;Tanja Traxler
06.04.2019;Zukunftsforscher Gatterer, können Sie mir die Angst vor KI nehmen?;"Eine Welt ohne künstliche Intelligenz (KI) wird es nicht mehr geben. Bis vor wenigen Jahren galt KI noch als wissenschaftliche Spezialdisziplin und inspirierte vor allem als Stoff der Science-Fiction-Literatur. Heute ist die Technologie längst fester Bestandteil des Alltags. Selbstlernende Systeme steuern die Spracherkennung in Smartphones, lassen Autos autonom fahren und helfen bei maschinellen Übersetzungen, bei der Identifikation von Objekten oder Personen, bei Kreditvergaben und Vorhersagen aller Art. Erstmals ändert sich die Beziehung zwischen Menschen und Maschinen – sie sind nicht mehr Werkzeuge, sondern lernen selbst weiter und treffen eigenständige Entscheidungen. Je nach ideologischem Hintergrund sind die Szenarien der Auswirkungen auf den Job- und Arbeitsmarkt mehr oder weniger düster. Welche Arbeit wird es nicht mehr geben? Wie werde ich mit Roboterkollegen zusammenarbeiten?

Big Data, billigere Technologie und immer bessere Algorithmen erschaffen allerdings in allen Bereichen eine neue Normalität. Die allgemeine Verunsicherung ist groß. Furcht ist da. Polarisierung hat den Diskurs über Mensch und Maschine ergriffen, die Informationen über manipulativen, überwachenden und kriminellen Einsatz der Technologien haben zu Vertrauensverlust und Ablehnung geführt. Auf der anderen Seite fungiert KI quasi als Deus ex Machina, als wunderbares Allheilmittel für alle Probleme und Schieflagen. Organisationen halten die größten Stücke auf Vorhersage- und Auswahlinstrumente – vom Strategischen bis zur Bewerbung und Eignung der Kandidaten – und versehen dies mit dem Wohlgeschmack der größeren ""Objektivität"".

Gleichzeitig fließen enorme Milliardenbeträge in die Forschung, wächst ein KI-Start-up nach dem anderen aus dem Boden. Bis 2022 soll Schätzungen zufolge allein der europäische KI-Markt zehn Milliarden Euro schwer sein, die jährlichen Zuwachsraten sollen bei fast 40 Prozent liegen, hat die Gartner-Group publiziert. Es wird investiert auf Teufel komm raus.

Diese Investments wollen natürlich wieder verdient werden. Es geht also um Marktmacht.

STANDARD: Herr Gatterer, ich fürchte mich vor künstlicher Intelligenz und ihrem von Macht- und Wirtschaftlichkeitskriterien getriebenen Einsatz. Bin ich eine Fortschrittsverweigerin?

Gatterer: Kein Wunder, dass Sie sich fürchten. Es ist ja auch so, dass wir zu KI beides haben: Überschätzung und zugleich Unterschätzung in den systemischen Auswirkungen. Die kollektive Verunsicherung, die den Prozess der digitalen Transformation schon generell begleitet, wird durch diese neue technologische Qualität und Komplexität auf eine neue Stufe gehoben. Das Resultat sind simplifizierende und polarisierende Mensch-Maschine-Erzählungen, die den aktuellen KI-Diskurs dominieren: auf der einen Seite euphorische KI-Utopien und eine quasireligiöse Hoffnung auf maschinelle Superintelligenz – auf der anderen Seite die dystopische Angst vor einer Unterwerfung der Menschheit durch intelligente Maschinen, Roboter und Algorithmen. Beide Narrative trivialisieren Technologie, indem sie die komplexe Dynamik soziotechnischer Fortschritte auf relativ simple und tendenziell lineare Szenarien reduzieren. Und sie überschätzen KI im Sinne eines magischen Denkens als Alleskönner, positiv wie negativ. Insbesondere in ökonomischen Kontexten ist KI deshalb zum Hype-Phänomen des digitalen Disruptionsdiskurses avanciert, zu einem Buzzword, mit dem sich jedes Unternehmen gern schmückt.

STANDARD: Entkommen wird man ihr jedenfalls bald nirgends mehr ...

Gatterer: Alles mit Strom wird versuchen, KI zu haben. Weil KI in nahezu jedem Produkt und jeder Lebenslage eingesetzt werden kann, belegt sie zugleich die Pole-Position in der Disziplin ""Lösungen auf der Suche nach Problemen"": KI wird heute in alle möglichen Gadgets gesteckt – in vielen Fällen ohne Sinn und Verstand, sondern schlicht, um zu zeigen, dass es geht.

STANDARD: Was alles geht, ist ja zu sehen: Verhaltensmanipulation durch Apps oder Social Media, der Missbrauch durch autoritäre und kriminelle Mächte oder die Erosion des öffentlichen Diskurses, die eine Beschädigung demokratischer Prozesse und den Aufstieg des Populismus vorantreibt ...

Gatterer: Viele Konsumenten machen die Erfahrung, dass KI-getriebene Smartifizierungen fragwürdig sind (was ist ""intelligent"" an einem sprachkontrollierten WC?) und KI in vielen Fällen ein Mehr an Unzuverlässigkeit und lästigem Aufgefordertwerden bedeutet, etwa in der Interaktion mit Sprachassistenten. Und Unternehmen stellen fest, dass KI nicht wie ein ""magic dust"" funktioniert, mit dem man eine Organisation von heute auf morgen ""smart"" macht, sondern dass die Implementierung von KI ein komplizierter und komplexer Prozess ist. Aber natürlich: Hinter jeder KI steckt das Weltbild – oder die Absicht – der Programmierer. Und das sind häufig verhaltensökonomisch getriebene Weltbilder.

STANDARD: Die zumindest auf Abhängigkeit ausgerichtet sind ...

Gatterer: Ja, im Prinzip. Und kriminelles Verhalten ist auch da. Der Einsatz für Überwachung, die in China bis zu Kreditpunkten für oder gegen die Teilhabe an der Gesellschaft reicht, ist da. Und wir sehen jetzt, dass genau das, nämlich die Möglichkeit des Missbrauchs, uns fordert. Es ist jetzt an der Zeit, Grenzlinien zu ziehen. Gerade weil KI eine so mächtige Technologie ist, gilt es, sie klug und reflektiert einzusetzen, um die Wirtschaft und Gesellschaft von morgen konstruktiv zu gestalten. Die Zeit ist jetzt reif für eine neue, aufgeklärte und pragmatische Perspektive auf KI. Dafür brauchen Unternehmen nicht nur ein klares Verständnis dessen, was KI tatsächlich ist und leisten kann, sondern vor allem ein neues Mindset: einen konstruktiven und zukunftsoffenen Blick auf KI-basierte Gestaltungspotenziale – und den Mut, aktiv mit lernenden Maschinen zu operieren, zu kooperieren. Da ist ja auch vieles im Gange in der Ausarbeitung von Ethikregeln, etwa die globale Vereinigung der Programmierer, die an einem globalen Ethikregelwerk arbeitet.

STANDARD: Wird da nicht der Bock zum Gärtner? Die stehen ja vermutlich überwiegend bei Konzernen in Lohn ...

Gatterer: Es ist ein freier Zusammenschluss außerhalb der Unternehmen – und einige der globalen Unternehmen haben schon gesagt, dass sie sich solchen Ethikregeln unterwerfen werden.

STANDARD: Zur Notwendigkeit der Grenzziehung und zum persönlichen Ohnmachtsgefühl: Wer kann denn Grenzen ziehen? Ich kann in Teilbereichen entscheiden, anzuwenden oder nicht anzuwenden. Da ist aber bald Schluss, weil KI kein isoliertes Ding ist – irgendwann kicke ich mich aus dem Spiel, wenn ich verweigere. So wie vor einigen Jahren in Bewerbungsprozessen, wenn gar keine elektronische Spur zu finden war ... Da fällt mir der Kabarettist Josef Hader ein: ""Topfpflanzen, gehts spazieren"" ...

Gatterer: Ich glaube, dass die Entwicklerszene ein wirklich großes Momentum hat. Diese Initiative ist kein Greenwashing-Verfahren! Es sind natürlich alle Wissenschaften gefordert, und an den Unternehmen wird es auch liegen. Sogar Elon Musk bemüht sich um Transparenz, weil er weiß, dass er sonst keine Akzeptanz erhält. Ich schreibe der kollektiven Kraft der Grenzziehung offenbar viel mehr Macht zu als Sie. Zwischen Technotopia und Retropia liegt sehr viel, und in den kommenden Jahren geht es darum, dass wir KI neu verstehen.

STANDARD: Das mit dem Angstnehmen hat nicht ganz funktioniert. Was werden wir alle in den kommenden fünf, zehn Jahren sehen?

Gatterer: Wir werden erleben, dass KI nicht perfekt ist, dass sie vielleicht nur zu 80 Prozent ihre Jobs erfüllt und nicht alles ersetzen kann. Wir werden sehen, dass mehr Daten auch ein Mehr an Erfahrung braucht. Wir werden sehen, dass mehr Maschineneinsatz ein Mehr an Mitmenschlichkeit braucht. Wir werden erleben, dass die menschliche Resonanzerfahrung, das, was sich hier und jetzt im wirklichen Raum abspielt und emotional entsteht, der zentrale Ausgangspunkt für unsere Interaktionen ist. Wir werden auch mehr Nützliches und Hilfreiches erleben, es gibt ja bereits unheimlich tolle Fortschritte auf den verschiedensten Gebieten – denken wir einmal an die Medizin beispielsweise. Es werden sich sehr viele neue Fragestellungen auftun – genau an diese Öffnungen glaube ich.

STANDARD: Also eine Hinführung zur Frage aller Fragen: Wer sind wir als Menschen, wer oder was wollen und können wir sein?

Gatterer: Ja. Mehr als acht Milliarden Menschen müssen sich intensiv mit ihrer Zukunft und ihrem Planeten auseinandersetzen. Solche Weiterentwicklungsprozesse sind – das wissen wir ja – nicht immer angenehm. In den Fokus rückt dabei das Thema Human Computation: die Frage, wie ein kooperatives Miteinander von Mensch und Maschine aussieht. KI wird die menschliche Intelligenz nicht ersetzen. Aber sie kann sie komplementär und kreativ erweitern, etwa im Rahmen nichtautonomer Systeme, in denen Maschinen unterstützen, aber der Mensch final entscheidet. Dieser Shift hin zu diesen Mensch-plus-Maschine-Umwelten ermöglicht – und erfordert – auch ein Upgrade der menschlichen Intelligenz und Empathie. Die nächste Gesellschaft wird eine ""kognifizierte"" Gesellschaft sein: eine Welt, in der kognitive Technologie zum ubiquitären Gebrauchsgegenstand wird. Um diese Gesellschaft mitzugestalten, müssen wir uns heute selbstbewusst der Frage zuwenden: Wie wollen wir in Zukunft leben, und auf Basis welcher Werte wollen wir KI nutzen?

So wie KI heute tendenziell überschätzt wird, wird ihr potenzieller Nutzen, insbesondere in der Verbindung mit Robotics und dem Internet der Dinge, zugleich stark unterschätzt. Die Rahmenbedingungen für die KI-Welt von morgen werden heute geschaffen – und Unternehmen spielen dabei eine tonangebende Rolle. Richtig angewandt kann KI uns nicht nur helfen, den Alltag zu vereinfachen und Produktionsverfahren ressourcenschonender zu organisieren, sondern auch den öffentlichen Raum sicherer zu machen, gesünder zu leben und globale Krisen zu lösen. Schon heute rettet KI im Gesundheitsbereich Leben – künftig könnte sie dazu beitragen, sämtliche Domänen menschlicher, nachhaltiger und gerechter zu gestalten, von Energie über Mobilität bis zu Bildung. Damit eröffnet KI zugleich eine neue Ära der Rehumanisierung und der Sinnarbeit. Der Einsatz von KI verändert unser Selbstverständnis, macht den Menschen wieder menschlicher und bietet die Chance, die Gesellschaft sozialer und humaner zu gestalten.";https://www.derstandard.at/story/2000100873194/zukunftsforscher-gatterer-koennen-sie-mir-die-angst-vor-ki-nehmen;Standard;Karin Bauer
04.09.2014;"Komplexitätsforscher: ""Das sind Aha-Erlebnisse bei der Datenanalyse""";"STANDARD: Welchen Ansatz verfolgen Komplexitätsforscher bei der Analyse von Krankheitsverläufen?

Klimek: Wir wollen verstehen, wie Krankheiten entstehen und wie sie sich ausbreiten können. Dabei gehen wir von mehreren Netzwerken aus, die ihre Funktion nicht erfüllen. Nehmen wir das Beispiel Ebola: Das Virus schleust seine DNA in das Netzwerk der Zellen, in ein System von 25.000 Genen beim Menschen, und verändert die Abläufe. Menschen werden krank. Da die Krankheit ansteckend ist, ist es wichtig, auch das soziale Netzwerk der Patienten zu verstehen. Wen kennen sie, mit wem sind sie in Kontakt? Und dann muss man auch noch begreifen, wie die Transportnetze funktionieren. Durch welche Flugverbindungen kann sich die Krankheit weltweit ausbreiten? Wie lange dauert es, bis auch außerhalb der Seuchenregion Krankheitsfälle auftreten? Wenn man für eine Computersimulation dazu Daten von Epidemien wie Sars oder H1N1 heranzieht, ergibt sich ein Muster.

STANDARD: Geben Komplexitätsforscher auch Empfehlungen ab, um diese Szenarien zu verhindern?

Klimek: In besonderen Fällen ja. Wir arbeiten mit den Daten und ziehen unsere Schlussfolgerungen daraus. Die Interpretationsmöglichkeiten für Präventionsstrategien liegen bei den Auftraggebern solcher Datenanalysen. Im Fall der Ebola-Seuche analysiert die Weltgesundheitsorganisation WHO laufend derartige Szenarien mittels Computersimulationen und trifft sicher auch die Maßnahmen, um das zu verhindern. Ebola ist aber nur ein Beispiel. Man kann derartige Analysen auch auf andere Krankheiten anwenden, zum Beispiel auf die Entstehung von Diabetes oder Krebs.

STANDARD: Ist das Institut für die Wissenschaft komplexer Systeme, dem Sie angehören, an ähnlichen Analysen beteiligt?

Klimek: Ja. Wir haben Studien über epigenetische Einflüsse bei Diabetes durchgeführt - zum Beispiel über die Häufigkeit von Diabetes bei den Geburtsjahrgängen 1940, 1944 und 1945. Da gibt es deutliche Unterschiede. Der Jahrgang 1945 hat ein doppelt so hohes Risiko, krank zu werden.

STANDARD: Warum?

Klimek: 1945 herrschte hierzulande eine Hungersnot - wie auch am Ende des Ersten Weltkriegs 1917/1918 und in den Zeiten der großen Wirtschaftsdepression in den 1930er-Jahren. Auch schwangere Frauen hatten Hunger, weshalb die ungeborenen Kinder daran gewöhnt wurden, wenig Nährstoffe zu bekommen. Man spricht hier von fetaler Programmierung der Ungeborenen. Als sie auf die Welt kamen, war die Hungersnot vorbei, aber ihr Stoffwechsel in vielen Fällen nicht in der Lage, dieses überraschende Angebot zu verarbeiten. Wir analysieren aber auch, wie Diabetes mit anderen Krankheiten zusammenhängt - zum Beispiel mit Depressionen. Da gibt es deutliche Unterschiede zwischen Männern und Frauen, aber auch zwischen Typ-1- und Typ-2-Diabetikern. Frauen sind da statistisch anfälliger.

STANDARD: Wie kommen Sie zu den Daten?

Klimek: Es handelt sich dabei um einen Forschungsdatensatz des österreichischen Hauptverbandes. Man kann über das Abrechnungsdatum von Krankenkassen den Gesundheitszustand einer Bevölkerung ablesen. Wann immer Sie zum Arzt gehen, in die Apotheke oder ins Krankenhaus kommen: Es wird verbucht. Der österreichische Hauptverband wacht über diese Forschungsdaten, wir nützen unsere Modelle, um sie interpretieren zu können.

STANDARD: Beschäftigt Sie dabei die Kritik von Datenschützern und die Fragen, wem eigentlich die Daten über Patienten gehören?

Klimek: Natürlich ist das ein wesentliches Thema. Die Daten sind aber vollkommen anonymisiert. Wir wollen mit unseren Arbeiten aufzeigen, dass die Ergebnisse dieser Forschung der gesamten Bevölkerung zugutekommen. Man kann ja so auch Risiken abschätzen und in der Bevölkerung mit Vorsorge gegensteuern. Wenn man weiß, dass Frauen mit Diabetes nach zwanzig Jahren der Erkrankung eher zu Depressionen neigen, kann das auch helfen, diese Patienten davor zu bewahren. Damit wollen wir dazu beitragen, einen Grundkonsens in der Bevölkerung herzustellen, inwiefern solche Daten zum Wohle aller verwendet werden können.

STANDARD: Aber reichen Daten aus, um zu einem derartigen Ergebnis in der Analyse zu kommen?

Klimek: Ich zitiere da gerne einen Satz des ehemaligen Präsidenten des Santa Fe Institutes, Geoffrey West: ""Big data withouth big theory is bullshit."" Das heißt: Man muss schon von einer Annahme ausgehen, sonst weiß man selbstverständlich nicht, wo man bei der Simulation von Daten ansetzen soll. Ein Kollege aus Russland hat uns einmal gebeten, die Wahlergebnisse zu analysieren. Er war besorgt um die Demokratie und nahm an, dass in vielen Bezirken gefälschte Stimmen für das Vereinte Russland zugefügt wurden. Wir entwickelten ein mathematisches Modell und kamen zum Schluss, dass die Wahlbeteiligung dort besonders hoch war, wo besonders viele Stimmen für Wladimir Putin abgegeben wurden. Das lässt die Vermutung zu, dass seine Annahme stimmte. Wir haben dann das Modell auf aller Herren Länder angewandt und fanden für Uganda ganz ähnliche Ergebnisse. Das sind dann schon Momente, in denen wir Aha-Erlebnisse haben. ";https://www.derstandard.at/story/2000005112222/das-sind-die-aha-erlebnisse-bei-der-datenanalyse;Standard;Peter Illetschko
23.08.2018;"Forscher: ""Sexroboter sind ein Anwendungsfeld meiner Arbeit""";"Es gibt einen Begriff in der Robotik, der das unbehagliche Gefühl zum Ausdruck bringt, das menschenähnliche Roboter bei uns Menschen hinterlassen können: das ""uncanny valley"". Es ist ein Tal des Unbehagens, das auf beiden Seiten in ein ansteigendes Wohlbefinden mündet. Letzteres stellt sich ein, wenn ein Roboter, den wir vor uns haben, so abstrakt ist, dass er kaum etwas Menschliches an sich hat. Es entsteht aber auch, wenn er uns so perfekt ähnelt, dass er kaum noch als Maschine erscheint. Unheimlich wird es hingegen, wenn ein Roboter fast wie ein Mensch aussieht, aber doch nicht ganz – genau dann befinden wir uns in der Talsohle des ""uncanny valley"".

Als Paradebeispiele dieser Berg-und-Tal-Fahrt werden gerne die Fabrikate des japanischen Ingenieurs Hiroshi Ishiguro herangezogen. Wie kein anderer Mensch auf der Welt hat er sich damit einen Namen gemacht, Roboter zu entwickeln, die Menschen zum Verwechseln ähnlich sehen. Von sich selbst hat er fünf derartige Kopien hergestellt, die ihn bei seinen zahlreichen Vorträgen in aller Welt begleiten. Sein doppeltes Konterfei ist nicht nur ein vielfach abgelichtetes Motiv für Tech-Portale wie ""Wired"", sondern diente auch schon als Covermotiv für das wissenschaftliche Fachblatt ""Science"".

Eine Roboterphilosophiekonferenz des Technikphilosophieprofessors Mark Coeckelbergh an der Universität Wien hat Ishiguro kürzlich nach Österreich geführt. Wie er da so im Hörsaal C1 auf dem Uni-Campus vor dem Auditorium steht, wirkt er viel kleiner als die coverfüllende Person, die man schon so oft gesehen hat. Er wirkt schüchtern, beinahe zerbrechlich. Seine schwarze Lederjacke, die er den ganzen Tag über anbehalten wird, mutet wie ein Schutzschild an. Mit einer umständlichen Handbewegung richtet er alle paar Minuten die vorderen Strähnen seiner buschigen, schwarzen Haare. Bis er schließlich zu sprechen anfängt, wagt man kaum, sicher zu sein: Ist er es oder einer seiner androiden Doppelgänger?

Doch, es dürfte schon der ""richtige"" Ishiguro sein – wir schreiben das Jahr 2018, und die maschinellen Sprachfähigkeiten sind noch nicht derart entwickelt, dass ein ganzer Hörsaal voller Philosophen auf eine Täuschung hereinfallen könnte, oder?

Während man sich also zunehmend sicher glaubt, dass es schon ein Mensch ist, dem man hier beim Sprechen zuhört, bekommt man immer mehr Fotos und Videos von androiden Robotern präsentiert, quasi in allen Lebenslagen: Roboter als Lehrer, Roboter in der Altenbetreuung, Roboter als Restaurantservicepersonal und so weiter. Geht es nach Ishiguro, gibt es keinen Lebensbereich, in dem Roboter keinen Platz haben könnten und schon gar nicht sollten. Was uns zu einer Frage führt, die wir Ishiguro später im Interview gemeinsam mit einem belgischen Journalisten von der Wirtschaftszeitung ""De Tijd"" stellen werden, als sich die Möglichkeit ergibt, ihn zu zweit eine Stunde lang zu seiner Forschung zu befragen:

STANDARD: Was ist die Mission, die Sie in Ihrer Arbeit verfolgen?

Ishiguro: Ich bin Wissenschafter und Ingenieur. Ich will etwas Neues entwickeln – das ist für einen Wissenschafter und Ingenieur wirklich wichtig. Ich tue das, indem ich menschenähnliche Roboter entwickle.
Die größte Herausforderung bei der Konstruktion von menschenähnlichen Robotern ist die Haut. Bei den Androiden von Hiroshi Ishiguro wird sie alle paar Jahre erneuert.
Foto: Hiroshi Ishiguro Laboratory

STANDARD: Warum liegt Ihnen so viel daran, Roboter zu entwickeln, die den Menschen möglichst ähnlich sehen?

Ishiguro: Der Grund dafür ist, dass ich die Interaktion zwischen Menschen und Robotern erforschen will. Der humanoide Roboter ist also das Testobjekt, um Funktionen von Menschen zu studieren.

STANDARD: Momentan existieren von Ihnen fünf androide Kopien. Welche davon mögen Sie am liebsten?

Ishiguro: Die letzte ist immer die beste.

STANDARD: Und warum?

Ishiguro: Die alten sind meine alten Gesichter. Die neueste hat die größte Ähnlichkeit mit mir, und sie hat bessere Funktionen und bessere Mechanismen als frühere Versionen. Sie hat auch natürlichere Gesichtszüge – wir verbessern die Technologie immer weiter.

STANDARD: Warum wollten Sie einen Roboter entwickeln, der genauso wie Sie selbst aussieht?

Ishiguro: Um zu verstehen, was der Mensch ist. Mir ist der Mensch wichtiger als der Roboter. Was ist der Mensch? Das ist die Frage, der ich nachgehe.

STANDARD: Sie arbeiten seit zwei Jahrzehnten mit Robotern – was haben Sie denn bisher über Menschen gelernt?

Ishiguro: Sehr viele Dinge, aber vor allem technischer Natur – wie man Gesichtszüge imitiert, wie körperliche Bewegungen ablaufen. Was die Basis des Menschen angeht, sind es recht kleine Einsichten. Wir sind dabei, zu untersuchen, wie man gute Konversationen führen kann – zu zweit und zu dritt. Doch das menschliche Verhalten ist wirklich sehr kompliziert.
Hiroshi Ishiguros bislang perfektester Android ist Erica – hier vor der Kamera.
The Guardian

STANDARD: Als Sie begonnen haben, an Androiden zu arbeiten, war das noch ein recht unbekanntes Forschungsgebiet – wie ist das heute?

Ishiguro: Androiden sind immer noch ein sehr spezielles Forschungsgebiet. Es gibt kein anderes Team, das Androiden herstellen kann, die so perfekt sind wie unsere.

STANDARD: Was ist das Schwierigste dabei?

Ishiguro: Der Gesichtsausdruck und die Haut sind sehr schwer nachzubauen, es gibt dabei viele komplizierte Herausforderungen. Daher eignen sich Androiden nicht gut für die Massenproduktion.

STANDARD: Woraus besteht die Haut Ihrer Androiden?

Ishiguro: Aus Silikon. Das Problem ist, dass es nicht sehr robust ist. Wir müssen die Haut daher alle paar Jahre erneuern.

STANDARD: Und wie erzeugen Sie die Gesichtsausdrücke?

Ishiguro: Dafür gibt es viele kleine Motoren im Gesicht. Darin besteht unser wichtigstes Know-how.

STANDARD: Wie viel kostet es, so einen Androiden herzustellen?

Ishiguro: Das hängt stark von den eingebauten Funktionen ab, liegt aber mindestens bei 200.000 US-Dollar (umgerechnet rund 160.000 Euro, Anm.). Das ist das Minimum.

STANDARD: Ist es auch notwendig, einen Prozess des Alterns zu erzeugen, damit Ihr Android Ihnen auch künftig ähnlich sieht, wenn Sie gealtert sind?

Ishiguro: Es ist besser, das Design zu verändern. Alle Androiden älter zu machen ist sehr aufwendig, deswegen erzeuge ich alle paar Jahre einen neuen Androiden. Außerdem nutze ich medizinische Technologien, um mich zu verjüngen.

STANDARD: Sie arbeiten derzeit daran, die Konversationsfähigkeiten von Robotern zu verbessern – was sind die Herausforderungen dabei?

Ishiguro: Es geht um das Begehren – wir versuchen, es in Robotern zu installieren. Die Frage ist: Was ist eine gute Konversation? Ein Roboter erfüllt einfach die Aufträge der Menschen. Wenn ein Roboter aber Begehren oder Absichten hätte, könnte er viel authentischere Konversationen führen. Die Idee ist also: Wir installieren im Roboter Begehren, und der Roboter agiert dann basierend darauf. An dieser Stelle wird Ishiguro still, klappt seinen Laptop auf und klopft in die Tasten. Die ungewöhnlich abrupte Unterbrechung des Gesprächs ist merkwürdig, bietet aber immerhin eine willkommene Möglichkeit, über das nachzudenken, was Ishiguro soeben gesagt hat: Begehren installieren – im Ernst?

Doch bevor wir ihn das fragen, interessiert uns, was hier gerade vor sich geht:

STANDARD: Was schreiben Sie eigentlich auf dem Computer?

Ishiguro: Ich muss kurz einen Gedanken notieren. Das ist der Grund, warum ich Interviews mache: Journalisten stellen andere Fragen als Wissenschafter. Deswegen können Sie mir Inspirationen aus einer ganz anderen Perspektive geben. Ich genieße Interviews, weil sie mich in der Forschung weiterbringen. Was ich jetzt gerade notiert habe, war: Das Problem ist, dass wir das Verhältnis von Begehren, Erfahrungen und Gefühlen herausfinden müssen.

STANDARD: Was meinen Sie damit?

Ishiguro: Wir haben es geschafft, künstliche Intelligenz zu erzeugen, es ist uns auch gelungen, Gefühle in Form von emotionalen Ausdrücken bei Robotern zu erwecken. Die Herausforderung für das nächste Jahrzehnt ist, Bewusstsein und Begehren zu erzeugen.

STANDARD: Glauben Sie tatsächlich, dass das möglich ist?

Ishiguro: Natürlich! Wenn wir als Wissenschafter denken, dass es unmöglich ist, würden wir nicht daran forschen.

STANDARD: Liegt die Erzeugung von Bewusstsein nicht jenseits mechanischer und elektronischer Fertigkeiten?

Ishiguro: Nein. Glauben Sie, dass ich ein Bewusstsein habe?

STANDARD: Ja, ich denke schon.

Ishiguro: Warum?

STANDARD: Das sagt mir mein Gefühl.

Ishiguro: Eben! Die Menschen fühlen etwas, das man nicht erklären kann – das ist die Schwierigkeit. Logisch verstehen Sie, dass ich ein Mensch bin. Ich habe einen lebendigen Körper aus Fleisch und Blut, also nehmen Sie an, dass ich ein Bewusstsein habe. Wenn ich aber meinen Körper öffne und Sie überall mechanische Bestandteile sehen, würden Sie wahrscheinlich Ihre Meinung ändern.

STANDARD: Wahrscheinlich schon.

Ishiguro: Sie können Ihre Meinung über das Bewusstsein also sehr schnell ändern. Das ist die Schwierigkeit: Bewusstsein ist nicht objektiv feststellbar. STANDARD: Sie haben zuvor gesagt, dass Sie an Robotern arbeiten, weil Sie etwas über den Menschen lernen wollen. Was zeichnet den Menschen aus?

Ishiguro: Es ist die Technik. Der Nutzen von Technik ist das, was den Menschen vom Tier unterscheidet. Letztlich trägt auch die Entwicklung von Robotern dazu bei, was es heißt, ein Mensch zu sein.

STANDARD: Sind Sie schon einmal als moderner Frankenstein bezeichnet worden?

Ishiguro: Es hat mich schon einmal jemand als verrückten Professor bezeichnet. Ich finde das aber nicht, ich mache ehrliche Forschung. Auch Sie haben Fragen an Menschen, deswegen interviewen Sie mich. Ich mache genau dasselbe, nur stelle ich meine Fragen im Labor.

STANDARD: Wie würden Sie Ihre Beziehung zu Ihren Androiden beschreiben?

Ishiguro: Für mich gilt: Ein Roboter ist ein Werkzeug, um Forschung zu betreiben. In Japan gehen wir davon aus, dass viele Dinge so etwas wie eine Seele haben – auch Roboter. Meine Beziehung zu ihnen ist aber vergleichbar mit der zwischen einem Arzt und seinen Patienten. Ich habe keine persönliche Beziehung zum Roboter. Meine Rolle ist es, zu beobachten.

STANDARD: Wie wir gelesen haben, hat aber einer Ihrer Studenten für einen Roboter Gefühle entwickelt – wie ist das passiert?

Ishiguro: Das ist eine wirklich peinliche Geschichte. Dieser Student war nicht richtig krank, aber er war sehr einsam.

STANDARD: Können Roboter denn keinen Ausweg aus der Einsamkeit bieten?

Ishiguro: Das hängt von den jeweiligen Menschen ab. Autistische Kinder etwa lieben es, mit Robotern zu sprechen, aber sie hassen die Kommunikation mit Menschen. Auch alten Menschen könnte der Kontakt mit Androiden sehr helfen.

STANDARD: Wie sieht es mit physischer Intimität aus – denken Sie, dass es gang und gäbe sein wird, körperlich mit Robotern intim zu sein?

Ishiguro: Warum nicht? Der Roboter ist eine physische Entität. Sexroboter sind ein mögliches Anwendungsfeld meiner Forschung. Bei Menschen, die kein normales Sozialverhalten haben, können Roboter eine Lösung sein. Ich spreche nicht von normalen Menschen, sondern von solchen, die spezielle Probleme oder Bedürfnisse haben.

STANDARD: Denken Sie, dass es gut wäre, wenn immer mehr Menschen Beziehungen mit Robotern eingehen?

Ishiguro: Warum sollte es schlecht sein? Manche Menschen können keine normale Beziehung mit Menschen eingehen. Durch Roboter könnte sich unsere Gesellschaft verbessern. Letztlich wollen wir jeden Menschen durch den Einsatz von Technologie unterstützen. STANDARD: An welche Bereiche denken Sie?

Ishiguro: Zum Beispiel an die Erziehung: Es gibt so viele schlechte Eltern auf dieser Welt – Eltern, die ihre Kinder schlagen. Roboter behandeln die Menschen aber immer sehr behutsam. Androiden wären besser als schlechte Eltern.

STANDARD: Woran könnte es liegen, dass viele Menschen die Vorstellung, von Androiden erzogen zu werden oder ihnen auch nur zu begegnen, gruselig finden?

Ishiguro: Gruselig? Nein!

STANDARD: Was sagen Sie zum ""Uncanny valley""-Effekt?

Ishiguro: Diesen Effekt gibt es, aber bei Robotern, die wie Zombies aussehen. Wir haben diesen gruseligen Effekt mit unseren Androiden bereits überwunden.

STANDARD: Denken Sie, dass sich humanoide Roboter künftig in großer Zahl in unsere Gesellschaft vollständig integrieren werden?

Ishiguro: Ja, selbstverständlich! Jetzt ist der Roboter noch eine Art Maschine, ein Werkzeug für uns, aber das menschliche Gehirn ist ja eigentlich auch nichts anderes als eine sehr komplexe Maschine. Wenn Roboter erst einmal autonom agieren, wird unsere Beziehung zu Ihnen viel komplizierter und menschenähnlicher werden. Der Kern Ihrer Frage ist aber nicht uninteressant: Die menschliche und die androide Gesellschaft werden sich verweben – das muss ich mir gleich notieren.";https://www.derstandard.at/story/2000077322736/roboterforscher-sexroboter-sind-ein-anwendungsfeld-meiner-arbeit;Standard;Tanja Traxler
13.09.2017;"Esa-Direktor Aschbacher: ""Erdbeobachtung rettet Menschenleben""";"Noch nie ist ein Österreicher höher in der Hierarchie der europäischen Weltraumorganisation (Esa) aufgestiegen: Seit 2016 ist Josef Aschbacher einer der Direktoren der Esa und als solcher zuständig für das Erdbeobachtungsprogramm, das größte Direktorat der Organisation mit einem Jahresbudget von 1,5 Milliarden Euro. Sein Dienststandort ist das European Space Research Institut im italienischen Frascati, doch er hat auch ein Büro im Esa-Direktorium in Paris. Diese Woche hält er auf Einladung der Forschungsförderungsgesellschaft (FFG) einen Vortrag in Wien.

STANDARD: Sie sind Meteorologe und Geophysiker. Was sind aus Ihrer Sicht die wichtigsten Entwicklungen der Erdbeobachtung der vergangenen Jahre?

Aschbacher: Nach dem Studium der Meteorologie und Geophysik an der Uni Innsbruck bin ich 1990 zur Esa gegangen. Seither hat sich wahnsinnig viel getan. Damals hatte die Esa noch keinen eigenen europäischen Erdbeobachtungssatelliten. Der erste solche Satellit, ERS1, wurde 1991 gestartet – das war ein Riesenschritt, nicht nur, weil er der erste europäische war, sondern auch in technischer Hinsicht: Er hatte ein aktives Radar an Bord, das hat es damals nirgends sonst gegeben, nicht einmal bei der Nasa. Ich bin zur Esa gekommen, um mit den Daten dieses Satelliten zu arbeiten und neue Projekte zu entwickeln.

STANDARD: Was hat sich seither verändert?

Aschbacher: Europa hat heute das beste Erdbeobachtungssystem der Welt: Copernicus. Es greift auf die Technologie von damals zurück, hat sie aber verfeinert und ergänzt. Die heutigen Sentinel-Satelliten (so nennen sich die Copernicus-Satelliten, Anm.) sind ein abgerundetes Konzept. Wir haben sechs Familien, die alle möglichen Bereiche mit diversen Instrumenten messen. Diese Daten sind für viele Anwendungen hilfreich – Landwirtschaft, Ozeanografie, Tourismus, Wettervorhersagen, Katastrophenschutz. Wir produzieren mit den Sentinels ein größeres Datenvolumen als alle Videos und Fotos, die täglich auf Facebook geladen werden – damit sind wir mitten im Big-Data-Bereich.

STANDARD: In welcher Dimension spielt sich das ab?

Aschbacher: Wir produzieren täglich zehn bis zwölf Terabyte an Daten, in unserem Archiv sind 35 Petabyte an Produkten heruntergeladen worden, und es ist ein massiver Anstieg, der sich durch Copernicus ergibt. Das System ist erst knapp drei Jahre in Operation, die Kurve geht exponentiell nach oben. Wir sind heute in einer sehr komfortablen Situation, von der ich, als ich Student war, nur träumen konnte. Ich habe meine Dissertation mit Daten von der Nasa gemacht, heute sind amerikanische Studenten interessiert, unsere Daten zu bekommen, weil wir die beste Datenquelle anbieten.

STANDARD: Welche künftigen Systeme würden Sie sich wünschen?

Aschbacher: Wir haben heute in der Esa elf Satelliten in Operation und 28 Satelliten im Bau. Das heißt, die unmittelbare Zukunft ist der Bau und Start dieser neuen Satelliten, aber parallel läuft die Entwicklung neuer Programme und Beobachtungssysteme, da sind wir sehr ambitioniert.

STANDARD: Welche Projekte sind bereits in Planung?

Aschbacher: Im wissenschaftlichen Bereich haben wir derzeit sechs sogenannte Earth-Explorer-Satelliten in Betrieb, die wissenschaftliche Daten liefern, etwa zur globalen Eisbedeckung oder zum Salzgehalt der Ozeane. Zwei weitere Earth Explorer befinden sich im Bau. Dann haben wir eine meteorologische Serie: Hier gibt es eine Weiterentwicklung des Systems aus geostationären und polarumlaufenden Satelliten für die Wetterbeobachtung. Der dritte Bereich ist Copernicus.

STANDARD: Welche Pläne gibt es für den Ausbau dieses Programms?

Aschbacher: Wir haben fünf Copernicus-Satelliten im Orbit und werden den sechsten, Sentinel-5P, am 13. Oktober starten. Die bisherigen Sentinels konzentrieren sich auf Messungen des Bodens und der Wasseroberfläche, Sentinel 5P wird die Atmosphäre untersuchen. Weitere 15 Sentinels, die wir bereits entwickeln, sollen in den nächsten Jahren starten. Darüber hinaus wird die Zukunft des Copernicus-Systems gerade dieser Tage definiert. Dafür planen wir ein Multi-Milliarden-Euro-Programm, das von der Esa und den EU-Staaten kofinanziert wird.

STANDARD: Für welchen Zeitraum ist das Budget veranschlagt?

Aschbacher: Etwa von 2019 bis 2027 – ab 2019 finanziert von der Esa, von 2021 bis 2027 von der EU. Allerdings ist das der Finanzierungszeitraum. Der Zeitraum für Entwicklung und Betrieb geht natürlich darüber hinaus.

STANDARD: Sie sagen, dass Europa das beste Erdbeobachtungsprogramm der Welt hat. Wie wichtig ist die Kooperation mit der US-Weltraumbehörde Nasa in diesem Bereich?

Aschbacher: Mit der Nasa arbeiten wir sehr gut zusammen, wir haben auch einige gemeinsame Missionen. Ein Beispiel ist Sentinel 6, das ist eine topografische Mission, um sehr genau die Änderungen der Höhe des Meeresspiegels zu vermessen. Die Nasa trägt wichtige Komponenten zum Satelliten bei, wir arbeiten aber auch in Wissenschaftsfragen eng zusammen.

STANDARD: Der Rhetorik nach sind Klimaforschung und Erdbeobachtung keine Prioritäten des US-Präsidenten Donald Trump. Wie könnte sich die Weltraumpolitik dieser Administration und des wahrscheinlichen neuen Nasa-Direktors James Bridenstine auf das Erdbeobachtungsprogramm auswirken?

Aschbacher: Der neue Administrator Bridenstine muss erst bestätigt werden, aber das wird wohl passieren. Er ist von der Trump-Administration eingesetzt und wird deren Politik implementieren. Wie wir wissen, hat der US-Präsident kein großes Interesse an Klimaforschung. Im Gegenteil: Er ist dabei, aus dem Pariser Klimaabkommen auszusteigen, und ist sehr skeptisch gegenüber der Klimaforschung und dem Klimawandel eingestellt. Das schlägt sich teilweise auf die Budgets der Nasa nieder. Der Präsident hat vorgeschlagen, das Earth-Science-Programm um 250 bis 300 Millionen Dollar pro Jahr zu kürzen. Es gibt noch einen Vorschlag des Hauses und des Senats, die das Budget verhandeln und dem Präsidenten zur Unterzeichnung vorlegen. Der Senat ist sehr positiv gegenüber der Erdbeobachtung eingestellt und hat das Budget in seinem Vorschlag wieder erhöht auf den derzeitigen Betrag. Jetzt kommt es darauf an, wie die Verhandlungen laufen. Ich glaube, es wird nicht ganz so trist sein, wie befürchtet.

STANDARD: Wie würde sich die Partnerschaft zwischen Nasa und Esa verschieben, wenn es doch zu den Budgetkürzungen kommt?

Aschbacher: Die Kernaufgaben der Nasa im Bereich Earth Science sind durch die Budgetkürzungen nicht in Gefahr. Dazu zählt der Betrieb der Satelliten und die wissenschaftliche Untersuchung des Systems Erde. Es gibt aber Bereiche – Präsident Trump hat fünf Missionen vorgeschlagen -, die gekürzt oder verzögert werden könnten. Dazu zählen etwa atmosphärische Messungen von CO2 – da kann es passieren, dass Europa verstärkt eintreten muss, um die Messungen sicherzustellen.

STANDARD: Kommen wir zur Rolle Österreichs: Welchen Beitrag leistet das Land zum Erdbeobachtungsprogramm und zur Esa?

Aschbacher: Österreich ist ein sehr wichtiger Partner in der Erdbeobachtung, und in der österreichischen Weltraumforschung ist die Erdbeobachtung die erste Priorität, wo das meiste Geld investiert wird. Das freut mich als österreichischen Direktor sehr, und es macht gesellschaftlich Sinn: Die Daten der Erdbeobachtung können sehr direkt zum Nutzen der Bevölkerung verwendet werden. Externe Berater haben errechnet, dass ein Euro investiert in das Erdbeobachtungssystem Copernicus einen Nutzen von zehn Euro für die Gesellschaft bringt.

STANDARD: Wie profitieren die Menschen von der Erdbeobachtung?

Aschbacher: Aktuell zeigt sich beim Hurrikan Irma: Durch die Erdbeobachtung können Schäden durch Naturkatastrophen minimiert und auch Menschenleben gerettet werden. Durch die Erdbeobachtungsdaten können Frühwarnsysteme für Unwetter und Extremereignisse aufgebaut und verbessert werden. Erdbeobachtungsdaten können aber auch zur Schadensbestimmung nach dem Ereignis verwendet werden, etwa von Versicherungen. Durch Daten zu Klimafragen lässt sich der Klimawandel besser messen und studieren. Auch liefern wir Daten zur Änderung der Landnutzung, was für die Land- und Fortwirtschaft hilfreich sein kann. Für Tourismusgebiete sind die Daten zu Wettervorhersagen und Änderungen der Schneelage entscheidend – die Erdbeobachtung hat also einen sehr direkten sozialen Nutzen.

STANDARD: Sie halten morgen, Donnerstag, in Wien einen Vortrag zu Weltraumforschung und Digitalisierung – worum geht es dabei?

Aschbacher: Durch die Digitalisierung ist, was vor 30 Jahren komplett unvorstellbar war, heute Standard. Die Frage ist, wie sich die Weltraumforschung in weiteren zehn Jahren mit neuen Methoden wie künstlicher Intelligenz entwickeln wird. Eine der größten Herausforderungen ist, wie man sehr viele und diverse Daten schnell prozessieren und die wichtigen Informationen herausfiltern kann. Mozart hat einmal gesagt, es gibt so viele Noten und all diese Noten ergeben keinen Sinn, wenn man nicht die wenigen guten herauspickt, die eine harmonische Melodie ergeben. Wir haben in etwa dasselbe Problem: Wir haben so viele Daten, und die Kunst ist, herauszufinden, welche Daten wesentlich sind für die Fragen, die wir uns stellen.";https://www.derstandard.at/story/2000063965627/esa-direktor-aschbacher-erdbeobachtung-rettet-menschenleben;Standard;David Rennert, Tanja Traxler
13.02.2018;Die Vermessung der Gletscher;"Als Quantennanophysiker beschäftigt man sich nicht unbedingt mit etwas Greifbarem. Da kann man schon, so wie ich, in eine Sinnkrise stürzen. Photonen und Nanopartikel sind zwar faszinierend, aber um einige Größenordnungen weniger tastbar als Gletscher, die sich erwandern und erfahren lassen, und ein ganz konkretes Gebilde im Kopf erzeugen, wenn man über seine Forschung nachdenkt. Allerdings lassen sich auch Gletscher und die Atmosphäre physikalisch modellieren, und damit in weiterer Folge die Auswirkungen des Klimawandels besser verstehen.

So war die Idee geboren, mich beruflich neu zu orientieren – von Quanten zu Lasern zu Gletschern sozusagen. Natürlich könnte man sich auf den Standpunkt stellen: Warum um Gletscher kümmern, warum diese überhaupt erforschen? Liegen sie doch weit entfernt von dem, wo sich der Alltag für einen großen Teil der Menschheit abspielt. Das ist jedoch nur ein Teil der Wahrheit.

Was sind Gletscher eigentlich?

Intuitiv hat man natürlich eine gute Idee davon, was einen Gletscher ausmacht. Man kann das Ganze allerdings etwas genauer festmachen. Ein Gletscher ist eine Ansammlung von mehrjährigem Schnee oder Eis, das aufgrund des Einflusses von Gravitation den Hang hinab fließt. Vielleicht etwas schwer vorstellbar, aber es ist tatsächlich so. Das Eis eines Gletschers ist ständig im Fluss. Recht eindrucksvoll zu sehen ist dies in folgendem Zeitraffer, für welchen täglich Aufnahmen von Oktober 2015 bis August 2017 gemacht wurden – die Aufnahmen bei schlechter Sicht wurden entfernt. Dieser wurde von der ZAMG veröffentlicht und zeigt den Hufeisenbruch der Pasterze:
ZAMG - Wetter, Klima, Umwelt, Geophysik

Auch wenn er unveränderlich erscheint, ein Gletscher ist ständig im Wandel. Abgesehen vom Fließen finden noch andere Prozesse statt. Er kann Masse gewinnen (Akkumulation) oder aber Masse verlieren (Ablation).

Akkumulation besteht im wesentlichen aus Schnee, eventuell Lawinen und, wenn es kalt genug ist, Regen, der gefriert. Der Schnee kann hierbei als fester Niederschlag fallen oder per Wind auf den oft in einer Senke liegenden Gletscher verfrachtet werden. Über die Jahre hinweg verdichtet er sich dann immer weiter, wird zu Firn und letztlich zu Eis. Hinter der Ablation stecken hingegen Schmelze, Verdunstung und Sublimation. Letztere ist der direkte Übergang von Eis in Wasserdampf, was vor allem bei tropischen Gletschern eine wichtige Rolle spielt. Aber auch das ""Kalben"" von Gletschern, die in Seen oder das Meer münden, trägt zum Massenverlust bei, wie folgendes Video eindrucksvoll zeigt:
Exposure Labs

So gesehen lässt sich ein Gletscher in zwei Zonen einteilen: Die Akkumulationszone liegt in höheren Lagen, in denen der Massengewinn gegenüber dem Massenverlust überwiegt. Die Ablationszone hingegen liegt tiefer und stellt jenen Teil des Gletschers dar, wo der Massenverlust größer ist als der Massengewinn. Die beiden Zonen werden durch die sogenannte Gleichgewichtslinie getrennt.

Eis aus dem oberen Bereich eines Gletschers fließt also langsam aber stetig Richtung Tal und wird so in die Ablationszone transportiert, wo es schließlich irgendwann abschmilzt. Ob sich ein Gletscher ausdehnt oder ob er sich zurückzieht, hängt nun im Wesentlichen davon ab, ob Massengewinn oder -verlust überwiegt. Kommt von oben nicht genug Eis nach oder schmilzt in den Sommermonaten unten besonders viel ab, so kann der Verlust nicht ausgeglichen werden. Der Gletscher verliert an Dicke und die Zunge zieht sich zurück.

Warum sind Gletscher überhaupt wichtig?

Gletschern kommt in verschiedenen Regionen der Welt eine wichtige Rolle zu. Während sie in Österreich zumeist in touristischem Zusammenhang erwähnt werden, trägt ihr Schmelzwasser in Südamerika zum Beispiel zur Versorgung von Städten und Dörfern mit Trinkwasser oder Elektrizität aus Wasserkraftwerken bei. Entsprechend ist dort auch die Landwirtschaft auf dieses Wassers angewiesen. Dabei wird während der niederschlagsreichen Zeit das Wasser von Gletschern gespeichert und während trockenerer Phasen wieder abgegeben. Ohne Gletscher, die als Puffer fungieren, fehlt dieses zusätzliche Wasser, da es bereits kurz nach dem Abregnen wieder in die Flüsse gelangt. Besonders akut ist diese Problematik zum Beispiel in den tropischen Anden, wo die Trinkwasserversorgung zum Teil von nahen Gletschern abhängt. Dies betrifft nicht nur kleine Dörfer sondern durchaus große Städte wie Quito, wo 2,2 Millionen Einwohner unter anderem auch mit Wasser vom Gletscher des nahen Antisana versorgt werden.

Abhängig von der Topographie des Gletschers kann es durch dessen Abschmelzen auch unmittelbarere Risiken für menschliche Siedlungen geben. Durch dieses können sich vermehrt Eisstauseen bilden, die, wenn sie ausbrechen, ihren Wasserinhalt talwärts ergießen – mit ernsthaften Auswirkungen für in ihrer Bahn liegende besiedelte Gebiete und Infrastruktur. Auch dies ist kein hypothetisches Szenario. So barst am 4. August 1985 der Gig-Tsho-Gletschersee in der Khumbu-Region des Himalayas. Die Folgen waren katastrophal: Es gab vier Todesfälle, ein neu errichtetes Wasserkraftwerk wurde zerstört, 14 Brücken und 30 Häuser fortgerissen sowie Kulturland vernichtet.

All das sind Gründe, warum es unabdingbar ist zu verstehen, wie sich Gletscher in Zukunft entwickeln werden. Dies liefert einerseits eine Entscheidungsgrundlage für den eventuellen Bau künstlicher Wasserreservoirs, die dazu dienen sollen, potentiellen Knappheiten entgegenzusteuern – oder kann helfen, Katastrophen durch Eisstauseen zu antizipieren oder deren Auswirkungen gering zu halten.

Letztlich sind Gletscher aber noch zu einem Symbol für etwas Anderes geworden: Sie zeigen besonders anschaulich den Einfluss von Klimaveränderungen auf unsere Erde. Diese Veränderungen können natürliche Ursachen haben, wie das Ende der Kleinen Eiszeit Anfang des 20. Jahrhunderts, und menschengemachte, die sich für einen Großteil des Abschmelzens in der jüngeren Vergangenheit verantwortlich zeichnen. Besonders eindrucksvoll ist ein Bildvergleich – siehe unten – im Fall der Pasterze im Jahr 1920 (links) und 2012 (rechts). Die Zunge hat sich seitdem um mehr als einen Kilometer zurückgezogen, auch die Dicke des Eises ist geringer geworden. Die Position der Gletscherzunge

Das führt auch zu einer ersten Messgröße, die vergleichsweise einfach zu bestimmen ist – die Position der Gletscherzunge. Daten dazu werden seit dem Ende des 19. Jahrhunderts systematisch gesammelt. Aber auch aus historischen Aufzeichnungen, alten Fotografien oder Gemälden lässt sich rekonstruieren, wie sich die Zunge im Lauf der Jahre verändert hat. Die Datenreihen reichen in einigen Fällen mehrere hundert Jahre zurück. Über markante Punkte im Gelände und Kartenmaterial kann auf diese Weise relativ genau nachvollzogen werden, wo sich die Gletscherzunge zum Zeitpunkt der Bilder befand. Regelmäßige Aufnahmen von derselben Stelle liefern somit eine gute Grundlage für Positionsbestimmungen.

Am Hintereisferner in den Ötztaler Alpen wird Wanderern noch auf andere Art und Weise versucht zu vermitteln, wie weit sich dessen Zunge bereits zurückgezogen hat. Kurz nachdem man das Hochjochhospitz passiert hat, findet man am Wegrand in unregelmäßigen Abständen Tafeln, die anzeigen, in welchem Jahr der Gletscher noch bis dorthin reichte. Von der Tafel mit der Aufschrift 1894 aus ist er gar nicht mehr sichtbar, erst ab jener mit dem markierten Jahr 1938 lässt sich der Beginn des Hintereisferners hinter der nächsten Talbiegung erahnen. Gletscher ziehen sich weltweit zurück

Es gibt noch andere Methoden, um eine etwas konkretere Idee von der Entwicklung des Gletschers zu erhalten. Die Position und Form der Gletscherzunge können auch mittels Maßband, GPS oder einer Kombination aus beidem bestimmt werden.

Letztlich ergibt sich aus dem Vergleich zwischen den Messungen in verschiedenen Jahren ein simples Bild davon, ob sich die Zunge zurückzieht, ausbreitet oder an derselben Stelle bleibt. Alles in allem zeigt sich weltweit – bis auf wenige Ausnahmen – ein sehr eindeutiges Bild: Die Gletscher ziehen sich zurück. Dieser Trend zieht sich quer durch alle Klimazonen. Speziell in Österreich haben die Gletscher seit ihrer letzten Maximalausdehnung in der ersten Hälfte des 19. Jahrhunderts etwa 60 Prozent ihrer Fläche verloren.

In der folgenden Grafik ist die Entwicklung von drei ausgewählten Gletschern in Österreich zu sehen. Abgesehen von kurzen Phasen, in denen sich die Gletscher wieder weiter ausbreiteten, ziehen sie sich seit über hundert Jahren weiter und weiter zurück. Die Geschwindigkeit des Rückzuges ist allerdings von Gletscher zu Gletscher verschieden und hängt von lokalen Gegebenheiten ab.
Die Massenbilanz eines Gletschers

Nur aus der Längenänderung ergibt sich aber zunächst ein unvollständiges Bild. Auch der wissenschaftliche Wert mit Hinblick auf das Prozessverständnis ist eher gering. Ein direktes Signal dafür, wie ein Gletscher auf die Wetterverhältnisse innerhalb eines Jahres reagiert, lässt sich erst aus der Massenbilanz ableiten.

Die Massenbilanz eines Gletschers ist quasi Buchhaltung: Bilanz = Massenzunahme – Massenabnahme. Positive Werte bedeuten eine Zunahme an Masse, negative eine Abnahme. Hier können satellitengestützte Messmethoden bereits einen guten Eindruck verschaffen, diese alleine sind aber zur Zeit noch nicht ausreichend. Die Genauigkeit, mit der ein Satellit die Höhe der Gletscheroberfläche bestimmen kann, ist ein limitierender Faktor, sowie der Umstand, dass die Dichte der oberflächennahen Schichten nur auf direktem Weg bestimmt werden kann.

Die direkteste und genaueste Methode ist also, zu vermessen wie viel ""Dicke"" verloren oder gewonnen wird. Aus diesem Grund wandern zweimal jährlich Glaziologen und Freiwillige auf etwa 13 ausgewählte Gletscher in Österreich – circa 170 weltweit –, bohren dort Löcher, versenken Stäbe und dokumentieren, wie weit bereits zuvor ""verpflanzte"" Pegelstäbe wieder aus dem Eis ragen – dazu gibt es hier eine Fotogalerie.

Zeitlich hängen diese Messungen mit dem hydrologischen Jahr zusammen, welches von 1. Oktober bis 30. September dauert. Zwischen Herbst und Frühling liegt dabei die Akkumulationsperiode, also die Phase in der der Gletscher an Masse zunimmt, während die Ablationsperiode von Frühling über den Sommer bis in den Herbst dauert. Quasi ein umgekehrter Winterschlaf in dem der Gletscher versucht, sich im Winterhalbjahr genug Masse anzuessen, um den Sommer zu überdauern.

Die gebohrten Löcher können verschieden tief sein, üblich sind zehn Meter. In diese werden der Länge nach miteinander verbundene und markierte Stäbe eingebracht, auch Ablationspegel genannt, die zusammen bis zum Boden des Lochs reichen – dort lässt man diese schließlich festfrieren. Die Idee ist, zu einem späteren Zeitpunkt wieder zu der Bohrung zurückzukehren und zu messen, wie weit der Stab nun aus dem Eis herausragt – dies zeigt direkt wie viel Eis abgeschmolzen ist. Im folgenden Bild ist rechts im Vordergrund ein solcher Stab zu sehen, im Hintergrund wird ein frisches Loch gebohrt. Aus den freiliegenden Segmenten lässt sich ablesen, dass an dieser Stelle etwa zwei Meter Eis abgeschmolzen sind. Aus früheren Messungen weiß man auch, ob zum Beispiel beim letzten Besuch bereits die Hälfte eines Stabes frei lag, dies wird anhand der geführten Aufzeichnungen entsprechend korrigiert. Hintereisferner: Keine Gletscherzunahme seit den 80ern

In der Akkumulationszone verkompliziert sich das Prozedere etwas. Um festzustellen, wie viel die Massenzunahme beträgt, ist es nötig, die Dicke und Dichte(n) der Schnee und Firnschicht über dem Eis zu bestimmen. Praktisch gesehen bedeutet dies einfach, dass man eine Schneesonde verwendet oder einen Schneeschacht gräbt, bis man auf Eis oder den Oberflächenhorizont des Vorjahres stößt. Eine schweißtreibende Angelegenheit, denn die notwendige Tiefe des Schachts kann mitunter mehrere Meter betragen. Die Dichte wird durch das Abwägen einer Schnee- oder Firnprobe eines definierten Volumens ermittelt.

Nach jeder Messkampagne werden die Messungen an verschiedenen Punkten am Gletscher auf den gesamten Gletscher hochgerechnet. Letztlich lässt sich aus den gewonnenen Daten die über die Gletscherfläche ermittelte Massenänderung über Jahre hinweg für jedes Jahr darstellen. In der Abbildung weiter unten ist eine solche Massenbilanzreihe für den Hintereisferner in den Ötztaler Alpen zu sehen. Besonders bemerkenswert: Seit 1984 gab es kein einziges Jahr mehr, in dem der Gletscher Masse gewonnen hätte.

Das ist, wie bereits eingangs erwähnt, ein Trend der sich durch alle Klimazonen zieht. Die Gletscherzungen ziehen sich zurück und die Massenbilanzen haben sich ins Negative verschoben. Die Gletscher verschwinden, Jahr für Jahr bleibt ein bisschen weniger von ihnen übrig. In manchen Regionen der Welt werden die Konsequenzen stärker spürbar sein als in anderen, aber sie sind da. Um ihnen vorbeugen zu können, ist es unerlässlich Gletscher und ihre Umgebung besser zu verstehen. Da sich jedes Modell an der Realität messen muss, ist die Vermessung von Gletschern ein wichtiges Unterfangen. Wissenschaftlich an einem für die Gesellschaft wichtigen Thema zu arbeiten, und zwar nicht nur im Büro oder in dunklen Labors, sondern hin und wieder auch an Feldarbeiten auf Gletschern teilzunehmen, ergibt insgesamt ein schönes Jobprofil. Damit verschwand auch still und leise die Ursache für meine Neuorientierung – die Sinnkrise.";https://www.derstandard.at/story/2000073445558/die-vermessung-der-gletscher;Standard;Johannes Horak
02.08.2017;"""Star Trek""-Tricorder wird Realität: Prototypen geschaffen";"Bisher gab es das nur in der Science-Fiction-Serie ""Star Trek"": kleine Geräte, die kontaktlos Krankheiten erkennen. Tüftler haben solche Tricorder gebaut – und damit millionenschwere Preise gewonnen. Bis zum Patienten-Alltag ist es aber noch weit.

Die Serie lief erstmals in den 1960er-Jahren, aber die Technologie aus ""Raumschiff Enterprise"" war ihrer Zeit schon immer weit voraus. Doktor Leonard McCoy untersucht seine Patienten in der Serie beispielsweise mit einem Tricorder. Kontaktlos und sekundenschnell erkennt das kleine Gerät Krankheiten. Unter ""Star Trek""-Fans legendär – aber eben auch nicht von dieser Welt. Bis jetzt.
Wettbewerb 2012 ausgeschrieben

Zehn Millionen US-Dollar (8,53 Mio. Euro) hatte die X-Prize-Stiftung 2012 demjenigen versprochen, der den legendären Tricorder nachbaut. Das Gerät solle genau wie das Vorbild aus der Fernsehserie funktionieren und möglichst genau eine Auswahl von 15 verschiedenen Krankheiten bei 30 Patienten ermitteln. Zudem dürfe es nicht mehr als rund zwei Kilogramm wiegen. Mehr als 300 Teams bewarben sich, wie die Stiftung mitteilte.

X-Prize-Wettbewerbe haben schon mehrfach Schlagzeilen gemacht. 1996 hatte die Stiftung aus den USA zehn Millionen Dollar für einen privat finanzierten Flug bis an den Rand des Orbits ausgelobt. Das Preisgeld hatten sich acht Jahre später die Entwickler des Raketenflugzeugs ""SpaceShipOne"" gesichert.

Und auch die Tricorder-Herausforderung ist jetzt geknackt. ""Wir wollten ein wirkliches Produkt für den Konsumenten, das die Menschen gerne benutzen würden"", sagte Jessica Ching von der X-Prize-Stiftung bei einer live im Internet übertragenen Diskussionsveranstaltung während einer Medizintechnik-Konferenz in San Diego in Kalifornien in der Nacht auf Dienstag. ""Man muss sich das vorstellen: Als wir den Wettbewerb starteten, gab es noch kein Uber, kein Yelp, Facebook war noch sehr früh und Elon Musk noch nicht zu sehen.""
Mehrere Teams ausgezeichnet

An mehrere Teams wurden schließlich Geldpreise ausgegeben, als Sieger ausgezeichnet – und mit 2,6 Millionen Dollar (2,22 Mio. Euro) belohnt – wurden die Brüder Basil und George Harris samt ihres Teams. Den zweiten Platz und eine Million Dollar (852.733,01 Euro) bekam ein Team aus Taiwan, geleitet vom Harvard-Professor Chung-Kang Peng.

""Unser Team war wirklich ein Küchentisch-Team"", sagt Philip Charron von der Sieger-Gruppe. ""Wir haben aus Basils Haus in Pennsylvania heraus gearbeitet und dachten, es wäre einfach schön, irgendwo vorne zu landen. An das Gewinnen dachten wir gar nicht."" Unter anderem half die inzwischen 16 Jahre alte Tochter von Basil Harris beim Testen.

Der von dem Team entwickelte ""Dxter"" ist ein halbrundes weißes Gerät, das beispielsweise an ein iPad angeschlossen werden kann. Zunächst wird der Patient über eine Software befragt. Dann können über Sensoren, die unter anderem an Brust und Handgelenk angebracht werden, verschiedene Vitalfunktionen gemessen werden.
XPRIZE

Das zweitplatzierte Gerät ist eine kleine Box, die an ein Smartphone angeschlossen wird. Darüber wird der Patient befragt und angeleitet, einige Instrumente zu benutzen, die in der Box stecken. So gibt es beispielsweise ein Gerät, das sich der Patient ins Ohr stecken kann. Das Messinstrument überträgt live ein Video aus dem Inneren des Ohres und macht ein Foto, das von der Box ausgewertet wird. Auch der Schlaf des Patienten kann untersucht werden – und das weitaus günstiger als in vielen Schlaflaboren. ""Die Kosten für Gesundheitspflege steigen und der Zugang ist schwierig – und das ist ein Problem auf der ganzen Welt"", sagt Teamleiter Chung.
Vom Alltag noch weiter entfernt

Noch handelt es sich bei den Geräten um Prototypen, vom Patienten-Alltag sind sie weit entfernt. Das Preisgeld würde sofort in neue Tests gesteckt, sagt dann auch Charron vom Sieger-Team. Das Ziel sei aber, sobald wie möglich den Patienten direkt zu erreichen. Die Anwendungsmöglichkeiten seien vielseitig – von abgelegenen Gebieten über Flüchtlingscamps bis hin vielleicht auch irgendwann dann doch wieder zum Weltall, wie einst bei ""Star Trek"". ""Wir versuchen hier in diesem Land ja jemanden zum Mars zu schicken und es wäre doch großartig, wenn der Tricorder es auch wieder ins Weltall schaffen würde."" ";https://www.derstandard.at/story/2000062143444/star-trek-technologie-tricorder-wird-realitaet;Standard;Christina Horsten
28.08.2016;"Forschungsexperte: ""Man braucht eine Kultur des Zulassens""";"Das Austrian Institute of Technology (AIT) hat zuletzt einige Großinvestitionen in Infrastruktur getätigt. Was ist der Hintergrund dieser Initiative?

Plimon: Wir haben seit der Gründung laufend Gewinne erwirtschaftet. Deshalb fiel uns die strategische Entscheidung leicht, heuer nicht wie üblich sechs Millionen Euro, sondern 12,5 Millionen Euro in die Erneuerung der Infrastruktur zu investieren. Damit werden bestehende Einheiten und Labors ausgebaut. Wir sind ja angetreten, um in Europa sichtbar zu werden. Das gelingt einerseits durch Industriekooperationen, andererseits durch einen wissenschaftlichen Output, der sich im Vergleich sehen lassen kann. Wir wollten von Anfang an in bestimmten wissenschaftlichen Indikatoren über dem Durchschnitt europäischer Vorbilder liegen: der Fraunhofer-Gesellschaft, der niederländischen TNO und der finnischen VTT. In den vergangenen beiden Jahren ist es uns gelungen, vor jedem einzelnen dieser Institute zu liegen. In keinem großen Abstand – aber immerhin. Das heißt: Wir sind in dieser Liga etabliert. Und wir haben, wissenschaftlich betrachtet, die richtige Flughöhe. Um all das beizubehalten, muss man nach Jahren der Konsolidierung auch investieren.

STANDARD: Können Sie Beispiele nennen?

Plimon: Wir haben bereits ein gutes Smart-Grids-Labor, das aber im Bereich des Gleichstroms ausbaufähig war. Auch mit dem Batterielabor können wir sehr zufrieden sein – hier kann man viele chemische Analysen umsetzen, aber eben nur für kleinere Batterien, für Knopfzellen. In Zukunft werden wir dort größere Einheiten analysieren. Auch im Leichtmetallbereich in Ranshofen haben wir investiert, um wieder vor der Industrie zu sein, das waren wir zuletzt nicht mehr. Es ging da wie dort um eine Besserstellung für den Wettbewerb.

STANDARD: Ist der so bestimmend? In Österreich gibt es bezüglich der Größe ohnehin kaum ein vergleichbares Forschungsinstitut.

Plimon: In der Strategie heißt es eindeutig: Der Heimmarkt für uns als Forschungsunternehmen ist Europa. Und wir haben einige Kooperationen, die auch darüber hinausgehen. Die aus der Bildverarbeitung kommenden Fahrerassistenzsysteme für Bombardier zum Beispiel. Oder im Bereich Quantenkryptographie. Das haben unsere Vorgänger lanciert, danach gab es ein langes Wellental, was die Wirtschaftlichkeit dieser Entwicklungen betrifft. Nun wird es eine Kooperation mit dem chinesischen Telekommunikationsriesen Huawei geben.

STANDARD: Gibt es Bereiche, wo es wissenschaftlich und wirtschaftlich nicht nach Wunsch läuft?

Plimon: Sicher. Wir haben einige Themen, die wir nachschärfen müssen. Es gibt auch Bereiche, die wissenschaftlich hervorragend laufen, die wirtschaftlich aber noch Zeit brauchen. Die Forschung an Speichelproben zum Beispiel im Department ""Health and Environment"". Diese Wissenschafter haben große EU-Projekte an Land gezogen. Die Erlöse werden auch irgendwann kommen.

STANDARD: Wie ist in all diesen anfangs erwähnten strategischen Überlegungen das Engagement des Wissenschafters Andreas Kugi einzuordnen, der ja an der TU Wien Vorstand des Instituts für Automatisierungs- und Regelungstechnik bleibt und nicht in die AIT-Departmentstruktur eingegliedert wurde?

Plimon: Mit Kugi haben wir jemanden im Team, der in seinem Bereich auf der Wissenschaftslandkarte gut sichtbar ist. Er ist selbst mit der Idee gekommen, bei uns die Grundlagenarbeit von der TU Wien zur Anwendung zu bringen, also die Wertschöpfungskette damit abzuschließen. Das ist für uns ein Experiment und natürlich strategisch interessant, weil die Entwicklung von Automatisierungsprozessen, seine Arbeit, uns im Bereich Industrie 4.0 sicher besser positioniert. Wir denken da zunächst einmal an Inhalte – und erst später an Strukturen, an Departmenteinteilungen, deswegen ist seine Tätigkeit auch nirgendwo eingegliedert. Auch die enge Zusammenarbeit mit einer Universität ist ein Experiment. Ich bin mir sicher, dass wir, wenn es weiterhin gut läuft und wir gelernt haben, worauf es ankommt, weitere folgen lassen.

STANDARD: Wie sehen Sie in diesem Zusammenhang das AIT-Engagement in der Komplexitätsforschung?

Plimon: Auch der Complexity-Science-Hub ist ein Experiment. Kein eigenes Zentrum, sondern eine Kooperationsplattform. Sechs Institute und Unis zahlen ein, alle profitieren von den wissenschaftlichen Ergebnissen. Das geschieht, weil Netzwerkanalysen mit Big Data für viele Bereiche wichtig sind. Für die Medizin, aber auch für die Stadtentwicklung, um nur zwei zu nennen.

STANDARD: Die finanziellen Mittel für derartige Engagements scheint es zu geben. Was braucht man sonst noch dafür?

Plimon: Man braucht Menschen, die sich für ein Thema begeistern können, die etwas bewegen wollen. Und man braucht eine Kultur des Zulassens.

STANDARD: Wie essenziell sind denn gut arbeitende Unis für ein Anwendungsforschungszentrum wie das AIT?

Plimon: Enorm wichtig. Hier entsteht die Basis für viele Forschungsarbeiten. Wenn die Unis in einem Land nicht funktionieren, dann haben wir ein Problem. Ein anderes positives Beispiel ist die TU Graz mit ihrer Computer-Vision-Abteilung. Wir haben in Österreich ganz hervorragende Unis. Ihre Tragik besteht vor allem im Betreuungsverhältnis zwischen Studenten und Professoren. Das können wir aber von unserer Seite nicht ändern, da braucht es den politischen Willen dazu.";https://www.derstandard.at/story/2000043385428/forschungsexperte-man-braucht-eine-kultur-des-zulassens;Standard;Peter Illetschko
06.05.2018;Das Büro der Zukunft passt sich an;"Klammer auf, Platte raus, Platte hoch, Klammer zu, Bremsen lösen, und dann mit dem ganzen Ding quer durch den Raum, wohin auch immer einen die räumliche Sehnsucht gerade treibt. ""Ich bin kein Freund von fix eingebauten, schweren Büromöbeln"", sagt der Amsterdamer Designer Dave Keune. ""Mit flexiblen, beweglichen Einrichtungsgegenständen ist die Gestaltungsvielfalt um ein Vielfaches größer. Und die Lust an der Nutzung ebenso! So kann der Raum jeden Tag neu erlebt und erkundet werden.""

Seine bislang radikalste Bürospielwiese ist der sogenannte Design Innovation Space (DIS) in Strijp-S im Nordwesten von Eindhoven. Einst wurden in der denkmalgeschützten Fabrikhalle Geräte für Philips hergestellt, heute finden hier kreative Prozesse statt. Einmal im Jahr, wenn im Oktober rund eine Viertelmillion Besucher zur Dutch Design Week anreisen, wird der 20 auf 15 Meter große Raum als öffentlich zugänglicher Thinktank genutzt. Die restliche Zeit ist er Coworking-Space und Event-Location. ""Aufgrund des Denkmalschutzes war es theoretisch nicht einmal möglich, eine Schraube in die Wand zu bohren"", sagt Keune. ""Allein deshalb schon habe ich mich für ein modulares, frei bewegliches Mobiliar entschieden, das man in jedem Büroraum implementieren kann.""
Kreativität braucht Freiraum

Der DIS in Eindhoven folgt einem jungen Trend (einem von vielen), der im Fachjargon als Activity Based Working bezeichnet wird. Im Gegensatz zum klassischen Büro wählt der Mitarbeiter hier für jede Tätigkeit die dafür am besten geeignete Arbeitssituation selbstverantwortlich aus. Das Spektrum umfasst nicht nur unterschiedliche Orte im Büro, sondern auch entsprechend variantenreiche und bewegungsorientierte Positionen – ob das nun Sitzen, Stehen oder Liegen, ob das nun Drehstuhl, Samtsofa oder Hängematte ist. Erlaubt ist, was gefällt. Fachleute und Arbeitgeber preisen den Trend als das Zukunftsmodell schlechthin. ""Kreativität braucht Freiraum, und zwar nicht nur im Kopf, sondern auch im Körper"", sagt Bernhard Herzog, verantwortlich für Forschung und Entwicklung beim Wiener Beratungsunternehmen M.O.O.CON. ""Daher halte ich diese Entwicklung in der zeitgenössischen Büroplanung grundsätzlich für sehr positiv. Allerdings gebe ich zu bedenken: Activity-based Working muss zu mir als Person und zur Kultur des Unternehmens passen. Nicht jeder fühlt sich in dieser räumlichen Wahlfreiheit beglückt."" Besonders warnt Herzog davor, neue Bürokonzepte einzig und allein aus Gründen der Flächeneffizienz und Wirtschaftlichkeit einzuführen. ""Das geht immer schief.""
Flexibilität, Individualität, Mitbestimmung

Experten sind sich einig: Das Arbeiten befindet sich im Umbruch. Vorbei sind die Zeiten der klassischen Einzel- und Großraumbüros sowie der Cubicles, der kleinen, abgegrenzten Schreibtischzellen, wie man sie aus US-amerikanischen Sitcoms kennt. Flexibilität, Individualität, Mitbestimmung sind nun die Buzzwords. Denn durch Technologien wie künstliche Intelligenz oder Big Data werden sich die Büros künftig an die jeweiligen Bedürfnisse der Mitarbeiter anpassen. ""Dem einen ist zu kalt, dem anderen zu heiß. Mit zentralen Systemen kann man es niemandem recht machen"", sagt Sabine Hoffmann, Professorin für Gebäudesysteme und -technik an der Technischen Uni Kaiserslautern. Sie erforscht mit dem Deutschen Forschungszentrum für Künstliche Intelligenz im sogenannten Living Lab Smart Office Space, wie man solche Streits künftig vermeiden kann.

Etwa haben sie und ihre Kollegen mit einer Wärmebildkamera Personen, die das Labor als Arbeitsplatz nutzen, aufgezeichnet und die Umgebungstemperatur eingestellt, bevor diese erkannt haben, dass ihnen zu warm ist. Oder einen Bürosessel entwickelt, der individuell temperiert werden kann. In die Sitzfläche wurden Sensoren eingebaut, um zu erkennen, welche Tätigkeit die Person ausübt. Je nachdem, ob sie am Computer liest, von Hand schreibt oder mit Kollegen spricht, könnte sich das Licht am Arbeitsplatz der Tätigkeit anpassen.

Auch Lichtsysteme, die sich der Sonneneinstrahlung angleichen und so auf den Biorhythmus der Person eingehen, oder Lampen, die ein konstantes Geräusch verbreiten, um ablenkenden Lärm zu reduzieren, zählen zu Hoffmanns Forschung. Die gesammelten Daten, so die Idee, würden zentral gespeichert, die vernetzten Möbel so programmiert, dass sie aus den Daten lernen. So würde der Schreibtisch in die ideale Höhe fahren, sobald ein Mitarbeiter das Büro betritt und sich einloggt.
Positiver Einfluss auf Gesundheit

Was nach Science-Fiction klingt, wird laut Hoffmann in ""fünf bis zehn Jahren häufig in Büros zu finden sein, vorausgesetzt, der Datenschutz ist geklärt"". Neben der Flexibilität haben solche Systeme weitere Vorteile. Sie wirken sich positiv auf Wohlbefinden und Produktivität aus, sowie auf Gesundheit, Emotionen und Schlaf, wenn Temperatur, Licht und Lärmpegel passen. Denn: ""In Großraumbüros sind Mitarbeiter am anfälligsten für Temperatur, gefolgt von Lärm und zu wenig Tageslicht"", sagt Brent Bauer, medizinischer Leiter der US-amerikanischen Mayo Clinic. Er forscht im zugehörigen Well Living Lab, wie sich Räume auf Menschen auswirken und wie man sie gesünder gestalten kann.

Doch haben Büros überhaupt Zukunft, wenn viele eigentlich nur mehr Laptop und WLAN brauchen? Einerseits ist das Home-Office die Antwort auf das Bedürfnis nach Individualität, erst im Juli 2017 ließ das US-amerikanische Tech-Unternehmen Automattic sein Büro in San Francisco auf. Andererseits gewinnt das Büro als Ort wieder an Bedeutung. Ebenfalls 2017 stellte IBM seinen Mitarbeitern ein Ultimatum, entweder ins Büro zu kommen oder die Firma zu verlassen. Die Idee dahinter: Persönlicher Austausch fördere Kreativität und Innovation. Gleichzeitig verbessere das Arbeiten außerhalb des Büros die Ideenfindung, wie Forschungen des Fraunhofer-Instituts zeigen. Die Antwort liegt also letzt- lich in der Architektur der Büros: kein starres Interieur, sondern flexible Formen wie etwa im Design Innovation Space.";https://www.derstandard.at/story/2000078780400/das-buero-der-zukunft-passt-sich-an;Standard;Wojciech Czaja, Selina Thaler
25.07.2015;Carl Hart: Auf dem Campus ein Star;"Professor Carl Hart betritt den Hörsaal 614 des Psychologie-Instituts der Columbia-Universität in New York, füttert den Computer mit Power-Point-Seiten und beginnt seine Vorlesung. ""Wie Drogenkonsum gemessen wird"" steht für heute auf dem Plan, eine scheinbar trockene Routineangelegenheit. Fast 60 Studenten sind in Harts Kurs ""PSYC 2460, Drugs and Behavior"" inskribiert, und obwohl es noch nicht einmal neun Uhr morgens ist, folgen sie hellwach seinen Ausführungen, tippen mit, diskutieren, stellen Fragen.

Denn worum es hier geht, ist alles andere als Routine. Hart hat es sich zu einer seiner Aufgaben gemacht, zu hinterfragen, was an wissenschaftlichen Studien über Drogen an die Öffentlichkeit gelangt und für Angst und Missverständnisse sorgt; mehr noch: wie diese Ergebnisse überhaupt zustande kommen. Wenn wir hören ""X ist gefährlich / macht süchtig / bringt einen um"" – wobei X von Marihuana bis Crack alles Mögliche sein kann -, dann hakt Hart nach und fragt: Wie ist diese Schlussfolgerung zustande gekommen? Ist sie von Tieren auf Menschen verallgemeinerbar? Wie hoch war die Dosierung? Und vor allem: Wurden soziale und ökonomische Faktoren berücksichtigt?

Konkret präsentiert Hart an diesem Dienstagmorgen Studien über die Neurotoxizität von Methamphetamin, also Forschung, die untersucht, ob und wie Crystal Meth und verwandte Substanzen das Nervensystem ruinieren. Indem er ein Forschungsdetail nach dem anderen überprüft, animiert er die Studenten zu sokratischen Gesprächen und weckt ihr Misstrauen gegen leichtfertige Schlüsse. ""Wie stark war die Dosierung, die man den an keine Substanzen gewöhnten Ratten verabreicht hat?"" Aha, das Zwanzigfache des normal Zuträglichen. ""Wie groß wäre die Wirkung, wenn man die Tiere langsam an den Konsum gewöhnte?"" Das sei in dieser Studie nicht untersucht worden, aber andere hätten ergeben, dass die schädlichen Effekte signifikant geringer ausfallen würden, wie übrigens bei den meisten Drogen.

""Warum wird so etwas an Tieren getestet?"" Wahrscheinlich, so eine Studentin, damit man überhaupt eine große Wirkung nachweisen kann, ohne Menschen zu gefährden: ""Ist das nun schlechte Wissenschaft, 'bad science'?"", will Hart wissen. Nun, man wolle eben die Grenzen abstecken, die Extremfälle testen. Das sei schon in Ordnung, aber wenn man sich darauf beschränkt, ergibt das viele schiefe Bilder vom Drogenkonsum.

Hart zeigt den Studenten, wie solche Bilder in die Drogenpolitik einfließen, wie sich die Öffentlichkeit auf Horrorszenarios, auf Drogendealer und Bandenkriege konzentriert und dabei den Unterschied zu den Substanzen, um die es geht, aus den Augen verliert.
Professor für Psychologie und Psychiatrie

Wie wenige ist Carl Hart prädestiniert, in Fragen von Drogenkonsum und -politik seine Stimme zu erheben. Er hat Psychologie und Neurowissenschaften studiert und mit einem Ph.D. in Neuroscience abgeschlossen. An der Columbia ist er Professor für Psychologie und Psychiatrie, an der Abteilung für Drogenmissbrauch des Psychiatrischen Instituts des Staates New York arbeitet er als Forscher. Sein offizieller Titel ist Neuropsychopharmakologe. Diese Bezeichnung ist schon kompliziert genug – sie signalisiert, dass es um die Zusammenhänge zwischen dem Ner-vensystem, dem mentalen Bereich und ""Drugs"" im weiteren Sinn, also Drogen und Medikamente, geht. Doch, so Hart, ""eigentlich möchte ich noch ein ,Kultur-' oder ,Sozio-' vor meine Berufsbezeichnung hängen"".

Denn alle Diskussionen über legale und illegale Drogen und Medikamente würden zu kurz greifen, wenn sie ignorieren, in welcher sozialen und ökonomischen Umgebung, unter wie viel Stress und Paranoia, mit wie viel oder wenig Aussicht auf eine lebenswerte Zukunft Drogenkonsum bzw. -missbrauch stattfinden.

Genau das aber ist Harts eigentliches Anliegen, und er weiß, wovon er spricht. 1966 geboren, ist er im schwarzen Ghetto von Miami in einer dysfunktionalen Familie aufgewachsen. Er hat Marihuana und Meth konsumiert, Heroin und Kokain. Er hat mit Drogen gehandelt, illegal Waffen besessen, in Geschäften geklaut, ist mehrmals vor der Polizei geflüchtet.

Wie er es aus diesem Eck auf einen Lehrstuhl geschafft hat – als erster afroamerikanischer Professor in Sciences an der Columbia-Universität -, das beschreibt er in dem beeindruckenden Buch High Price.

""Intellektuelle"" – dieses Zitat des marokkanischen Schriftstellers Tahar Ben Jelloun ist dem Buch vorangestellt – ""die den Mut gehabt haben, ihre Opposition zu äußern, haben oft einen sehr hohen Preis bezahlt."" Mit High Price hat Hart teils eine schonungslose Autobiografie geschrieben, teils eine Bildungsgeschichte, teils ein Sachbuch über den Stand der Drogenforschung und, als Resultat aus alldem, ein Plädoyer für einen anderen Blick auf ""das Drogenproblem"".

Hart lässt den Leser an der Erfahrung teilhaben, arm und schwarz abseits des amerikanischen Mainstreams aufzuwachsen: was es heißt, wenn man ""weißes"" Englisch als sozusagen zweite Sprache erlernen muss, andererseits sehr genau lernt, die Körpersprache anderer Menschen zu lesen, weil man das buchstäblich zum Überleben braucht; was es bedeutet, wenn Wünsche nach Zugehörigkeit oder wenigstens nach so etwas wie Sicherheit unerwidert bleiben, immer und immer wieder – sogar Ratten würden weniger an frei verfügbaren Drogen konsumieren, wenn sie sich stressfrei bewegen können, statt in einem Käfig zusammengepfercht zu sein. Das habe eine Studie bereits 1978 ergeben.
Glück, Zufälle und Mentoren

Noch früher aber, vor fast 45 Jahren, begann unter der Regierung Nixon der bis heute sinn- und erfolglose ""War on drugs"". Er ist die Folie, vor der sich Erfahrungen wie die des jungen Carl abheben. Auch bis dahin als Freizeitbeschäftigung bzw. seit Jahrtausenden als Heilmittel verwendete Substanzen wurden nun als ""Schedule 1 drugs"" (äußerst gefährlich und ohne medizinischen Wert) mit Heroin gleichgestellt und werden mit der gleichen Härte verfolgt. Die Folgen sind inzwischen allgemein bekannt: noch mehr Bandenkriege, überfüllte Gefängnisse und die zunehmende Stigmatisierung einer schwarzen ""underclass"".

Diese allgemeinen Erkenntnisse ergänzt Hart durch seine eigene Geschichte. Lange habe auch er geglaubt, dass die Drogen an der Misere seiner Hood, seiner Freunde schuld seien. Die offizielle Lesart, dass sie per se das soziale Geflecht des Landes zerstören, wurde ja auch von schwarzen Kongressabgeordneten unterschrieben – als ob es hier ein Geflecht gebe, dem alle gleichermaßen verbunden sind.

Glück, Zufällen und Mentoren habe er es zu verdanken, schreibt Hart, dass er dieser Ideologie entkommen und sie dann noch dazu kritisch hinterfragen konnte. Nach der Highschool hatte er zunächst keine Ahnung, wie es weitergehen sollte, entschloss sich dann kurzfristig, zur Air Force zu gehen. Zum ersten Mal in seinem Leben erfuhr er, wie sich das anfühlt, wenn man nicht nach seiner Hautfarbe oder Sprache beurteilt wird, sondern nach seiner Leistung, und wenn man mehr oder weniger gleich wie alle anderen behandelt wird. Die Erfahrung sollte sich in Übersee wiederholen. In England wurde er einmal von der Polizei angehalten, aber bloß, weil ein Rücklicht des Wagens nicht funktionierte (""Wir wollten Sie das nur wissen lassen, Sir""). ""Eine Begegnung mit der Polizei ohne Anspannung oder Angst: Das war merkwürdig.""

In seinem Inneren stellte sich eine Weiche nach der anderen.Er hörte Musik, die ihm fremd gewesen war, Ella Fitzgerald, Bob Marley, Gil Scott-Heron vor allem. Er diskutierte nächtelang, las Unmengen. ""Ich hatte viel kulturelles Kapital"", fasst er zusammen. College war der nächste, fast logische Schritt, ein großer in seiner Familie, in der es kaum jemand auch nur bis zum Ende der Highschool geschafft hatte.

Die Ausbildung an der Uni lenkte sein Interesse an Drogen in die Richtung, die er bis heute verfolgt: evidenzbasiertes Forschen statt Emotionen, Misstrauen gegen vorgefasste Meinungen und der Drang, vorgeblichen Gründen auf den wahren Grund zu gehen. So untersuchte er am Psychiatrischen Institut von Columbia, wie regelmäßige Kokainkonsumenten reagieren, wenn ihnen statt einer weiteren Dosis eine geringe Menge Geld angeboten wurde. Zu einem guten Prozentsatz zogen sie das Geld vor.

Es war keine perfekte Studie – die Situation der vorübergehend im Institut wohnenden Versuchspersonen war künstlich, sie kamen aus einer funktionierenden sozialen Umgebung -, doch Hart konnte zumindest zeigen, dass ihr Kokskonsum keine alles andere verdrängende Sucht darstellte. Studien dieser Art machten ihn über die wissenschaftliche Gemeinschaft hinaus bekannt und auf dem Campus zu einem Star.
Bob Marley an der Wand

An der Wand von Carl Harts Büro in der Uni hängt ein Poster von Bob Marley. Harts eigene Dreadlocks sind fast noch aufwendiger als die des Reggae-Stars. Silberne Ringe und ein Stein blitzen an seinen Ohren auf und, wenn er lacht, ein Goldzahn in seinem Mund. Professoren einer ehrwürdigen Ivy-League-Uni stellt man sich anders vor. Doch gerade darin liegt vielleicht der Grund für sein Äußeres: Ich habe es geschafft, mag er signalisieren, trotz aller Merkmale, die dagegen sprechen. Mehr Street-Credibility? Er lacht. ""Wenn ich meine Forschung mache, bin ich ein weißer Labormantel wie die anderen.""

Bei öffentlichen Auftritten allerdings kann sein Habitus schon helfen, etwa wenn er in Interviews sagt, dass die meisten Drogenforscher von Drogen keine Ahnung haben. ""Das Problem ist, es stimmt"", sagt Hart. Er bemühe sich, in seinen Publikationen die Fakten für sich sprechen zu lassen.

Wenn er hingegen in öffentlichen Foren auftrete, bei einem TED-Talk etwa oder im Gespräch mit dem konservativen TV-Talkshow-Master Bill O'Reilly, dann stelle er die Schlussfolgerungen aus seiner Arbeit in allen Konsequenzen dar und bette sie in gesellschaftliche Befunde ein. Insbesondere seit der Legalisierung von Marihuana in einigen US-Bundesstaaten wird er häufiger gefragt, wie er es mit den diesbezüglichen Gesetzen und ihren Folgen hält.

""Es sind mehrere Dinge"", sagt Hart, ""die sich relativ klar sagen lassen: 80 Prozent der Konsumenten von Drogen, auch der ,harten', sind nicht süchtig, sondern können mit ihrer Gewohnheit umgehen. Armut und Kriminalität insbesondere in der schwarzen Bevölkerung gab es auch vor den Drogenwellen der letzten Jahrzehnte; die Drogen sind vor allem ein Vorwand, um Feindbilder zu schaffen. Mehr als 80 Prozent der wegen Drogenbesitz oder -handel Inhaftierten sind Schwarze, obwohl die Mehrheit der Konsumenten weiß ist. Die Antwort auf das Drogenproblem liegt nicht in noch mehr Strafen, sondern in attraktiven sozialen, beruflichen, ökonomischen Alternativen. Alle Drogen sollten entkriminalisiert und kontrolliert verabreicht werden; das wäre der erste Schritt einer vernünftigen Drogenpolitik. Und die Wissenschaft sollte so eine Politik anleiten. Die größten Probleme sind nicht die Drogen selbst, sondern Armut, Arbeitslosigkeit, Ignoranz – und die bestehenden Drogengesetze.""

Zu den Entwicklungen in Colorado, Washington und anderen Staaten, in denen Marihuana freigegeben worden ist, meint er, dass man abwarten und sich die ersten Studien genau ansehen müsse – ""the drug data pusher"" nennt ihn das Magazin Wired. ""Es wird wieder Untersuchungen geben"", sagt er, ""die nachweisen wollen, dass Pot rauchen dumm macht, dass es ein Einstieg für andere Drogen ist usw."" Und wieder werde man Schlussfolgerungen auseinanderklauben müssen, die die Gefahren aufbauschen – ""die Forschungsinstitute wollen schließlich weiter finanziert werden"" – oder methodisch unsauber argumentieren. Beliebt sei etwa immer gewesen, aufeinanderfolgende Phänomene mit Kausalitäten zu verwechseln. ""Heroin-User haben früher Gras geraucht? Sie haben früher auch Milch getrunken!""
Therapie statt Strafen

Harts Arbeiten haben ihn über die Staaten hinaus bekanntgemacht. Neben dem Marley-Poster hängen in seinem Büro die Ankündigung der brasilianischen Ausgabe seines Buches – ""Um Preço Muito Alto"" – und Ausschnitte unter anderem aus schweizerischen und polnischen Zeitungen, die sich mit seiner Forschung beschäftigen (außerdem das Faksimile eines Aufmachers der New York Times von 1914 (!), dem zufolge afroamerikanische ""Kokain-Unholde"" eine neue Bedrohung aus dem Süden seien – ""Negro Cocaine ,Fiends' Are a New Southern Menace"").

Die Drogenpolitik in den Niederlanden, in der Tschechischen Re-publik und in der Schweiz verfolgt er mit Interesse. Vor allem findet er beeindruckend, wie Portugal auf den stark angewachsenen Heroinkonsum reagiert hat: ""Therapie statt Strafen, und was die Gefängnisse gekostet haben, geht jetzt in entsprechende Einrichtungen.""

Das begrüßt auch die Psychiaterin Gabriele Fischer von der Med-Uni Wien, Leiterin der Drogenambulanz. Sie kennt Carl Hart beruflich seit Jahrzehnten, beide haben für das National Institute on Drug Abuse in den USA geforscht. Fischer ist sich mit ihrem Kollegen einig, dass die soziale Komponente und eine psychische wie physische Gesundheit entscheidende Rollen spielen, wenn es darum geht, nicht abhängig zu werden. Allerdings findet sie, dass Hart genetische Faktoren für Suchtverhalten nicht genügend berücksichtige. ""Er sagt, es gebe keine Sucht. Doch es gibt eine mehr oder weniger vorhandene Empfindlichkeit."" Und sie zitiert gerade amerikanische Studien, die nachweisen, dass intakte Familien, sozialer Zusammenhalt und das Fehlen einer genetischen Belastung schützende Faktoren sind; manchmal auch, wie Harts Biografie zeige, Glück und Mentoren zum richtigen Zeitpunkt.
Erkundung von Risikogruppen

Dem widerspricht Hart nicht. Er würde auch unterschreiben, dass, wie Fischer sagt, die USA vorbildlich in der empirischen Erkundung von Risikogruppen und in der Entwicklung von Medikamenten sind. Österreich habe dafür, so Fischer, wie andere europäische Staaten ein Niveau an Grundsicherung, ""wie man es sich in Amerika gar nicht vorstellen kann"". Dafür wiederum erschöpfe sich die Drogen betreffende Kommunikationsstrategie hierzulande in Kampagnen, die an den Zielgruppen vorbeigehen und von Entscheidungsträgern willkürlich lanciert würden: ""Eminenzbasiert"" nennt Fischer sie, im Gegensatz zu den immerhin evidenzbasierten Entscheidungen jenseits des Atlantiks.

Da allerdings ist sich Hart nicht so sicher, und gegenüber der lobend erwähnten Pharmaforschung bleibt er ebenfalls skeptisch. Zwar hat sich seit Beginn des sogenannten Kriegs gegen Drogen einiges geändert. Die öffentliche Meinung kippt immer mehr in Richtung Legalisierung oder Entkriminalisierung, in mehr als zwei Dutzend US-Staaten ist zumindest das Letztere erreicht. In den Medien werden die therapeutischen Wirkungen von psychoaktiven Pflanzen heute ebenso diskutiert wie die Sinnlosigkeit von Gesetzen, die immer noch unzählige Menschen wegen Drogen hinter Gitter bringen. Das der Drogenpropaganda unverdäch-tige National Geographic-Magazin hebt eine Cannabis-Pflanze aufs Cover und berichtet sachlich über ihre kulturelle und medizinische Bedeutung.

Doch wer was konsumieren darf und mit welchen Konsequenzen, gibt Hart zu bedenken, sei immer noch eine Frage willkürlicher, oft sozial diskriminierender Entscheidungen und nicht rationaler Überlegungen. ""Nehmen wir das Medikament Adderall"", sagt er. ""Das ist ein Psychostimulans, das millionenfach gegen Hyperaktivität und Narkolepsie verschrieben wird. Es ist chemisch gesehen ein Amphetamin und mit Meth verwandt. Das eine ist als Medikament im Umlauf, das andere gilt als gefährliche Droge – schauen Sie sich nur das Montana Meth Project (www.montanameth. org) an.""

In der Klasse nimmt er das Beispiel auf. Er zitiert eine Untersuchung der US-Armee, die die beiden Substanzen verglichen hat. Adderall, stellte sich heraus, war therapeutisch weniger wirkungsvoll. ""Sie sehen"" , sagt er, ""was bei uns ,Drogen' sind und was normal ist, das ist oft willkürlich. Und wenn es darum geht, was gut und was schlecht für unseren Körper ist? Betrachten wir unser täglich Brot – Fett, Zucker, industrielle Nahrung! Gibt es bessere Prädiktoren für einen Herzinfarkt?""

Carl Hart geht gerne vom konkreten Einzelfall zur großen Sicht über, zum ""big picture"". Auch das macht ihn zu einem ungewöhnlichen Vertreter seiner Zunft. Doch kommen wir noch einmal, wenn er es schon gerade im Unterricht behandelt, zum Thema Meth zurück: Wie realistisch findet er die TV-Serie Breaking Bad?

Hart lacht. ""Ich habe mir viele Folgen angesehen. Ich finde sie sehr spannend und amüsant. Aber mit der Wirklichkeit von Meth haben sie sehr wenig zu tun."" ";https://www.derstandard.at/story/2000019677510/carl-hart-auf-dem-campus-ein-star;Standard;Michael Freund
27.10.2015;Interstellarer Staub gewährt neue Einblicke in die Lokale Flocke;"Die Weltraumsonde Ulysses brach 1990 zu einer der herausragendsten Missionen der europäischen Forschungsgeschichte auf: Das Kooperationsprojekt zwischen Esa und Nasa hatte in erster Linie die Erforschung der Sonne zum Ziel. Dafür wurde Ulysses auf eine polare Sonnenumlaufbahn geschickt, für die sie sich beim Jupiter ordentlich Schwung holte. Es war die erste Sonde überhaupt, die unser Zentralgestirn in einem zur Ekliptik um rund 90 Grad verschobenen Orbit umkreiste.

Die zweite Aufgabe von Ulysses galt interstellaren Staubpartikeln, die die Sonde auf ihrer 19 Jahre dauernden Mission einfangen und analysieren sollte. Mehr als 900 von ihnen spürte Ulysses auf. Nun legten Wissenschafter erstmals eine umfassende Analyse dieses bisher größten Datensatzes interstellarer Staubteilchen vor. Ihre Bilanz: Im Einflussbereich der Sonne können sich Flugrichtung und -geschwindigkeit der Teilchen stärker ändern als bisher gedacht.

Seit etwa 100.000 Jahren durchquert unser Sonnensystem mit einer Geschwindigkeit von etwa 80.000 Kilometer pro Stunde die Lokale Flocke – eine Wolke aus interstellarer Materie mit einem Durchmesser von 30 Lichtjahren. Mikroskopisch kleine Staubteilchen aus dieser Wolke bahnen sich ihren Weg bis ins innere Sonnensystems. Für Forscher sind sie eine Art Botschafter aus den Tiefen des Alls und enthalten grundlegende Informationen über unsere entferntere kosmische Heimat.
Mehrere Staubfänger

Mehrere Raumsonden haben die ""zugereisten"" Teilchen in der Vergangenheit aufgespürt und charakterisiert. Zu ihnen zählen Galileo und Cassini, welche die Gasplaneten Jupiter und Saturn zum Ziel hatten, sowie die Mission Stardust, die im Jahr 2006 eingefangene interstellare Staubteilchen zur Erde brachte. ""Die Daten von Ulysses, die wir jetzt erstmals in ihrer Gesamtheit ausgewertet haben, sind einzigartig"", sagt Harald Krüger vom Göttinger Max-Planck-Institut für Sonnensystemforschung. 16 Jahre lang untersuchte das Instrument an Bord von Ulysses fast ohne Unterbrechungen den Teilchenstrom von außerhalb unseres Sonnensystems. Im Vergleich dazu lieferten andere Missionen nur Momentaufnahmen.

Den Daten der mehr als 900 Teilchen, die das Staubinstrument von Ulysses detektierte, haben die Forscher die bisher detailliertesten Informationen über Masse, Größe und Flugrichtung der interstellaren Wanderer entnommen. Computersimulationen halfen dabei, die verschiedenen Einflüsse der Sonne zu verstehen und voneinander zu trennen.

So bestätigten sich frühere Analysen, wonach der interstellare Staub stets in ungefähr derselben Richtung das Sonnensystem durchquert. Sie entspricht der Richtung, in der sich das Sonnensystem und die Lokale Flocke relativ zueinander bewegen. ""Kleinere Abweichungen von dieser Hauptrichtung hängen von der Masse der Teilchen und vom Einfluss der Sonne ab"", sagt Peter Strub vom Göttinger Max-Planck-Institut.
Richtungsänderung bei interstellaren Teilchen

Im Jahr 2005 allerdings zeigte sich ein anderes Bild: Die weitgereisten Teilchen erreichten den Staubdetektor aus einer verschobenen Richtung. ""Unsere Simulationen legen nun nahe, dass auch dieser Effekt auf die Schwankungen des Sonnenmagnetfelds zurückzuführen ist"", erklärt Veerle Sterken vom International Space Science Institute in Bern. ""Veränderte Ausgangsbedingungen in der Lokalen Flocke sind vermutlich nicht der Grund.""

Auch Größe und Beschaffenheit der Teilchen nahmen die Forscher unter die Lupe. Während die meisten der Staubpartikel im Durchmesser zwischen einem halben und 0,05 Mikrometern (Tausendstel Millimeter) messen, gibt es auch einige auffallend große Exemplare von mehreren Mikrometern Größe. ""Bemühungen, die Staubteilchen außerhalb unseres Sonnensystems von der Erde aus zu beobachten und zu charakterisieren, liefern keine derart großen Teilchen"", sagt Krüger.

Im Gegenzug finden sich die sehr kleinen Teilchen, die Astronomen mit Teleskopen typischerweise nachweisen, nicht in den Ulysses-Messungen. Wie Computersimulationen zeigen, laden sich diese Winzlinge im Vergleich zu ihren Massen im Einflussbereich der Sonne stark elektrisch auf, werden abgelenkt und so aus dem Hauptteilchenstrom herausgefiltert.
Poröse Partikel

Die Simulationen deuten zudem darauf hin, dass der exotische Staub eine geringe Dichte aufweist und somit porös ist. ""Die innere Struktur der Teilchen kann der Ulysses-Staubdetektor zwar nicht messen"", so Sterken. ""Am Computer können wir jedoch verschiedene Dichten ausprobieren. Mit porösen Teilchen lassen sich die Messdaten von Ulysses am besten rekonstruieren.""

Die Zusammensetzung der interstellaren Partikel konnten die Forscher mit dem Staubinstrument auf Ulysses nicht untersuchen. Dies ist jedoch mit dem am Max-Planck-Institut für Kernphysik in Heidelberg entwickelten Nachfolgeinstrument auf der Cassini-Sonde möglich. Diese Messungen werden ganz neue Einblicke in die Entstehungsbedingungen und die Entwicklung der interstellaren Teilchen gewähren.

Die Messungen interstellarer Staubteilchen im Sonnensystem erlauben somit einen Blick in die Lokale Flocke, die sich sonst nur durch Beobachtungen von der Erde aus untersuchen lässt. Bei zukünftigen Ausschreibungen der europäischen Weltraumagentur ESA wollen sich Staubforscher mit eigenen Vorschlägen für Missionen zur Untersuchung von interstellarem Staub beteiligen.";https://www.derstandard.at/story/2000024197015/interstellarer-staub-gewaehrt-neue-einblicke-in-die-lokale-flocke;Standard;red
06.05.2016;Herbert W. Franke: Eingebungen durch Eingaben;"""Maschinen eignen sich nicht, um Kunst zu schaffen!"" So lautete der Tenor, in den Philosophen wie Kunstkritiker einstimmten, als ein paar Künstler in den 1950er-Jahren begannen, ihre Computerexperimente auszustellen: Bilder, die auf technischen Verfahren beruhten und etwa Visualisierungen mathematischer Funktionen waren. Man konnte sich darin allenfalls mittelbar ausdrücken: hier eine Formel eingeben, dort Ausgangszahlen ändern. Von der zeitgeistigen ausdrucks- oder körperzentrierten Kunst, die die Eingebung der Eingabe vorzog, waren diese halb wissenschaftlichen Versuche weit entfernt.

Einer dieser Künstler – heute würde man vielleicht sagen: Nerds – war der 1927 in Wien geborene Herbert W. Franke, der ebendort Physik, Chemie, Psychologie, Mathematik und Philosophie studiert und 1950 einen Doktor in Theoretischer Physik erlangt hatte. Als er einmal an der Ulmer Hochschule für Gestaltung einen Gastvortrag hielt, um den Studenten die neuen Methoden schmackhaft zu machen, trat der Grafikdesigner Otl Aicher, Mitbegründer der Hochschule, vor, um zu befinden: Dies alles sei nicht uninteressant, nur habe es leider mit Kunst nichts zu tun. ""Für Kunst braucht man Material! Was Sie verwenden, sind aber nur Elektronen."" Um deren Ungreifbarkeit zu illustrieren, so erinnert sich Franke im STANDARD-Gespräch, habe Aicher ""so in der Luft herumgefuchtelt"".
Einsichten in die Schönheit

Und vom Tisch war somit die Idee, in Ulm einen Fachbereich für Computerkunst einzurichten. Dies nämlich war – und ist – eine große Überzeugung Frankes: dass die Kunst aus dem Rechner neue, grundlegende Einsichten in die Ästhetik geben könne. Als Basis diente ihm die sogenannte ""Informationsästhetik"", mit der Max Bense und Abraham Moles seit den 1950ern eine Systematisierung der Schönheit versucht hatten: Man brach Bilder auf Zahlenverhältnisse herunter, um Kunsteffekte messbar zu machen. Und um rasch zu sehen, wie Parameterveränderungen die Wirkung eines Bildes beeinflussen, war der Computer freilich das optimale Werkzeug.

Seine Faszination für vom Zauber der Mathematik durchdrungene Ornamentik oder unerhörte digitale Topografien weiterzugeben – das war Frankes großes Anliegen als Ausstellungsmacher und Lehrender, etwa für Wahrnehmungspsychologie. Er schrieb aber auch unzählige Sachbücher und Essays, in deren glasklarem Stil ein Vermittler mit Herzblut spürbar wird.
Das Unentdeckte im Inneren

Leben konnte Franke von der Computerkunst, zu der er über die experimentelle Fotografie gefunden hatte, nicht. Mehr ""um sich zu ernähren"" denn – wie man meinen könnte – als Reflexion seines Computerinteresses, begann er daher, Science-Fiction zu schreiben – ein Feld, in dem er ebenfalls zum Doyen aufsteigen sollte (zu Franke als Sci-Fi-Autor, siehe den Artikel Mit der Schreibmaschine in die Matrix). Daneben betätigte er sich zudem nicht nur als Zukunfts-, sondern auch als Höhlenforscher: Seit er eines Tages in der Besatzungszeit, da Auslandsreisen schwierig zu bewerkstelligen waren, beschlossen hatte, dann eben das unentdeckte Innere zu erkunden.

Unterdessen wandelten sich die Zeiten. 1954 war Franke noch auf ein ""Brauchen wir nicht!"" gestoßen, als er als PR-Mitarbeiter bei Siemens vorschlug, Computergrafiken ins Firmendesign einzubauen. Zehn Jahre später kam man auf ihn zu, weil die Münchner Staatsoper für ihre ""Experimentierbühne"" einen Sonderwunsch hatte: Es entstand ein Ballett, bei dem ein Tänzer auf sein digital verfremdetes Abbild trifft.
Ein glücklicher Zufall

Das Stück war ein Erfolg, was aber wohl auch daran lag, dass die Kunst hier auch der Demonstration technologischer Potenz dienen durfte. In Kunstkreisen hatten es die Grafiken aus dem Kastl, die auch an die geometrisch-kalkulierten Bilder des Konstruktivismus und der Konkreten Kunst anknüpften, weiterhin schwer. Als Franke in den 1970ern im Wiener Künstlerhaus ein Computerkunstzentrum aufbauen wollte, landeten auch diese Pläne in der Schublade. Bis sie dort 1979 der Intendant des ORF Oberösterreich, Hannes Leopoldseder, fand: Er stutzte das ""Ars ex Machina"" getaufte Projekt ein wenig zurecht und schenkte ihm als Ars Electronica eine bis heute andauernde Zukunft.

In den 1980ern musste Franke noch mit der Deutschen Luft- und Raumfahrtbehörde kooperieren, um an starke Computer zu kommen. In der Ära der Kleincomputer sollte hohe Rechenleistung bald marktgängig werden. Während sich mit Big Data und Reizüberflutung die Medienkunst vielfach darauf verlagerte, soziopolitische Auswirkungen zu thematisieren, ist Franke indes vor allem der informationsästhetischen Forschung treu geblieben, die er vorwiegend mit der Technikern und vertrauten Software Mathematica betreibt. Er sieht darin einen Weg, jenes Denken zu schulen, das es brauche, um die Maschinen ""in die richtige Richtung zu weisen"".
Unbändige Vermittlerlust

Die Vermittlerlust hat Franke, der heute bei München lebt, nie verlassen. Aktuell konzipiert der 88-Jährige eine Ausstellung, die die – längst anerkannten – Computerkunstklassiker dank aktueller Technologien neu erfahrbar machen soll. Ein weiteres Projekt befasst sich mit in Echtzeit veränderbaren Grafiken. In den letzten Jahren hat Franke mit seiner Ehefrau Susanne Päch aber auch viele Reisen unternommen. Ein Streben weg von bloß computergenerierten Landschaften? Weg vom Bildschirm? ""Nein, das hat es bei mir nie gegeben. Aber eine gute Abwechslung ist es.""";https://www.derstandard.at/story/2000036424556/herbert-w-franke-eingebungen-durch-eingaben;Standard;Roman Gerold
11.12.2020;Forschung im Zeichen der Digitalisierung;"Trotz erschwerter Rahmenbedingungen wird an den österreichischen Fachhochschulen nach wie vor intensiv geforscht. Über Josef-Ressel-Zentren wird anwendungsorientierte Forschung, bei der Wissenschafter mit Unternehmen kooperieren, durch die Christian-Doppler-Forschungsgesellschaft gefördert. Jedes dieser Zentren entsteht aus einer besonderen Fragestellung aus der Wirtschaft und braucht auch eine spezifische wissenschaftliche Expertise. Die Laufzeit ist mit maximal fünf Jahren begrenzt.

Seit Oktober gibt es auch erstmals ein Josef-Ressel-Zentrum an der FH Wien der WKW. Geforscht wird zum Thema ""Collective Action & Responsible Partnerships"". Konkret wird nach Antworten zu folgenden Fragen gesucht: Was bewegt Unternehmen zur Kooperation? Wie können sie große und komplexe Herausforderungen erfolgreich meistern? Die Covid-19-Krise habe gezeigt, dass zur Bewältigung solcher Herausforderungen ein kollektives Handeln unterschiedlicher Akteure essenziell ist. Welche Faktoren unterstützen Unternehmen bei kollektiven Projekten? Wie muss die Zusammenarbeit gestaltet werden, um die gewünschten Ergebnisse zu erbringen? Wie kann der Erfolg von kollektiven Projekten gemessen werden? Diesen Fragen möchte die Forschung auf den Grund gehen, heißt es dazu in einer Aussendung.

Die Produktion von Pulveraktivkohle aus kommunalen Reststoffen wird seit November in einem Josef-Ressel-Zentrum am MCI erforscht. Der Fokus liegt auf nachhaltige und wirtschaftlich erfolgversprechenden Lösungen zur Abwasserreinigung. Aktuell wird an den österreichischen Fachhochschulen in 23 Josef-Ressel-Zentren geforscht, die meisten davon gibt es an der FH Oberösterreich. Thematisch spiegeln sich darin die wirtschaftlichen Entwicklungen wider. Fast die Hälfte, nämlich zwölf der Zentren sind im Bereich Mathematik, Informatik, Elektronik angesiedelt, gefolgt von drei Zentren im Bereich der Life-Sciences.
Informatik und Gesundheit

Aber auch außerhalb dieser Josef-Ressel-Zentren wird Forschung betrieben. So wird beispielsweise seit Februar 2019 im Studiengang ""Allgemeine Gesundheits- und Krankenpflege"" und im Institut für Informatik der Fachhochschule Wiener Neustadt am EU-geförderten Projekt ""AgeWell"" gearbeitet. Dabei soll ein digitaler Coaching-Roboter, der den Namen Helga trägt, entwickelt werden, der Seniorinnen und Senioren bedürfnisorientiert im Alltag mittels App unterstützt. In der zweiten Projektphase wurde in Fokusgruppen getestet. Insgesamt nahmen 34 Personen aus der Altersgruppe teil. Und obwohl die meisten zugaben, zu Beginn eher skeptisch gewesen zu sein, fanden sie es schlussendlich spannend, mit Helga zu interagieren. Als nächster Schritt folgen nun Interviews mit Gesundheitsexperten, die in die Weiterenwicklung des Roboters einfließen werden, heißt es dazu aus der Fachhochschule.

Nach wie vor ein großes Forschungsgebiet ist der Bereich Artificial Intelligence (AI). Vieles kann aber auch schon angewendet werden. Weiterbildungen zur Anwendung von künstlicher Intelligenz im Unternehmen gibt es an der FH Technikum Wien. Dafür wurde eigens eine AI Academy in der berufsorientierten Weiterbildungsakademie (Technikum Wien Academy) eingerichtet. Das Bildungsangebot orientiert sich am Lernniveau und der gewünschten Unterrichtsintensität und reicht von Kurzseminaren über maßgeschneiderte Inhouse-Schulungen bis hin zu einem eigenen Master-Lehrgang. Inhaltlich setzen sich die Programme mit den Bereichen Business-Intelligence, Big Data, künstliche Intelligenz, Machine-Learning oder Deep Learning auseinander.";https://www.derstandard.at/story/2000122245328/forschung-im-zeichen-der-digitalisierung;Standard;Gudrun Ostermann
15.10.2020;"""Few shot learning"": KI-Systeme sollen aus ganz wenigen Daten lernen";"Maschinen lernen derzeit primär durch die Analyse großer Datenmengen. Weil das aufwendig ist, wird in der Forschung der Ansatz des ""few shot learning"" verfolgt. Ähnlich wie Menschen sollen Systeme Künstlicher Intelligenz (KI) aufbauend auf vorhandenem Wissen nur anhand weniger Beispiele neue Fähigkeiten lernen. In diese Richtung zielt eine neue Forschungspartnerschaft zwischen Österreichs KI-Aushängeschild Sepp Hochreiter von der Uni Linz und der Wiener Softwarefirma Anyline.
""Ewig nachtrainieren""

""Wenn ich zum ersten Mal in Italien Auto fahre, merke ich, dass die Leute anders reagieren und die Verkehrsschilder vielleicht ein bisschen anders aussehen, aber ich kann mich schnell darauf einstellen"", erklärte Hochreiter, der das Institut für Machine Learning der Universität Linz und das Artificial Intelligence Laboratory des Linz Institute of Technology (LIT) leitet, gegenüber der APA. KI-Systeme seien bisher auf diesem Gebiet sehr schwach gewesen, ""man hat immer ewig nachtrainieren müssen"", wenn sie sich auf eine neue Aufgabe einstellen mussten.

Ein großer Forschungsbereich widme sich daher der Frage, wie man die für das Lernen notwendige Datenmenge reduzieren kann, sagte Anyline-Mitbegründer Daniel Albertini zur APA. Denn es gebe Anwendungsfälle, wo man nicht so viele Daten sammeln könne oder auch wolle, etwa aus datenschutzrechtlichen Gründen. Das 2013 gegründete Unternehmen, das mit seinen Anwendungen KI für mobile optische Texterkennung nutzt, habe in den vergangenen Jahren schon viel in diese Richtung gearbeitet.
Grundlagenforschung

Nun will man sich gemeinsam mit der Uni Linz des Themas annehmen. Anyline finanziert im Rahmen der Forschungspartnerschaft in den kommenden drei Jahren eine Doktorandenstelle bei Hochreiter. ""Wir wollen damit die Grundlagenforschung vorantreiben, Ziel ist, gemeinsam zu publizieren und auf Konferenzen zu veröffentlichen"", sagte Albertini, der sich von der Zusammenarbeit nicht nur einen regen Wissensaustausch mit der Uni Linz erhofft, sondern auch mehr Marktkraft für sein Unternehmen.

Inhaltlich gehe es darum, dass ""neuronale Netze lernen, effizienter zu lernen"", so Albertini. Er vergleicht das Prinzip des ""few shot learning"" mit einer Person, die schon fünf Sprachen spricht und sich leicht tut, eine sechste Sprache zu lernen, weil sie auf vorhandenem Wissen aufbauen kann.
Gelerntes anpassen

Ähnlich wie der Mensch soll ein neuronales Netz bei einer neuen Aufgabe versuchen, das viele bereits Gelernte anzupassen, um nicht alles von Grund auf neu lernen zu müssen, so Hochreiter. ""Das kann man so weit vorantreiben, dass ein neuronales Netz nur mehr ein Beispiel braucht, damit es von einem komplett anderen Anwendungsfall das Wissen auf einen neuen Fall transferieren kann"", erklärte Albertini.

Das Software-Unternehmen will das Lernen neuronaler Netze aber auch auf einem anderen Gebiet verbessern. Im Rahmen eines von der Forschungsförderungsgesellschaft FFG geförderten Projekts soll eine sogenannte Trainer-Plattform entwickelt werden. ""Dabei geht es darum, wie neuronale Netze während der Anwendung automatisiert weiterlernen und dadurch besser werden können"", so Albertini.
Schriftzeichen ohne Internet erkennen

Die Anwendungen von Anyline ermöglichen es, mittels KI Schriftzeichen auf mobilen Geräten auch ohne Internet zu erkennen und in Echtzeit zu verarbeiten. Eingesetzt wird das beispielsweise von der Polizei in Österreich und anderen europäischen Ländern, die Ausweise oder Auto-Kennzeichen scannen und so rasch zu Informationen kommen, oder von zahlreichen Firmen. Das Unternehmen hat im vergangenen Jahr seine Mitarbeiterzahl auf aktuell 75 verdoppelt, einen Vertriebsstandort in Boston (USA) eröffnet und im Jänner eine Investitionsrunde mit 12 Mio. Euro abgeschlossen, ""die wir in ein weiteres Wachstum stecken"", so Albertini.";https://www.derstandard.at/story/2000120938520/few-shot-learning-ki-systeme-sollen-aus-ganz-wenigen-daten;Standard;APA
21.09.2020;"Mathematiker: ""Wie kann ein Computer lernen, ohne zu vergessen?";"Aktuelle künstliche Intelligenz (KI) ist keineswegs der Weisheit letzter Schluss. Christoph Lampert vom IST Austria erklärt, wie in der Grundlagenforschung die Fähigkeiten der Systeme erweitert werden. Er kritisiert zudem, dass in Sachen KI zwar viel Meinung, aber wenig tatsächliches Wissen im Umlauf ist.

STANDARD: Wie vermitteln Sie jemandem, der sich im Begriffsdickicht aus Machine-Learning, Deep Learning und künstlicher Intelligenz nicht so richtig zurechtfindet, eine Ahnung von Ihrer Forschungstätigkeit?

Christoph Lampert: Künstliche Intelligenz ist keine Magie. Es sind Techniken, die in der Informatik über viele Jahrzehnte entwickelt wurden und mittlerweile so gut funktionieren, dass man sie in praktischen Anwendungen einsetzen kann. Maschinelles Lernen ist eine Art, einem Computer zu erklären, was er tun soll. Man gibt ihm Beispiele von dem, was man erreichen will – etwa deutsche Sätze mit ihrer englischen Übersetzung. Der Computer findet durch einen Suchprozess selbst Regeln, wie man diese Sätze übersetzt, und macht daraus eine Software. Man erspart sich, dass ein Programmierer das alles erledigen muss. Deep Learning ist ein Begriff, der in den letzten Jahren für eine spezielle Art des maschinellen Lernens populär wurde. Dabei werden künstliche neuronale Netze benutzt, die stark dem menschlichen Gehirn nachempfunden sind – allerdings in sehr abstrakter Weise.

STANDARD: Ein großer Teil der Forschungsarbeit im KI-Bereich scheint die Suche nach neuen Anwendungsbereichen zu sein.

Lampert: Da würde ich widersprechen. Natürlich, man liest in den Medien meist über Anwendungen. Man liest, dass der Computer Musik komponieren, Auto fahren oder die Covid-Ausbreitung berechnen kann. Hinter solchen Nachrichten stehen oft Unternehmen mit ihren Produkten. Es tut sich aber auch viel im Bereich der Grundlagenforschung. Ursprünglich wurde maschinelles Lernen entwickelt, ohne eine spezifische Anwendung vor Augen zu haben. Allerdings ist der Weg von der Grundlagenforschung zu Anwendungen in diesem Bereich relativ kurz. Man kann als Forscher eine Idee haben, wie sich maschinelles Lernen in gewissen Aspekten verbessern lässt, und ein halbes Jahr später kann diese Idee bereits Teil einer App sein.

STANDARD: Wie kann man also abseits der Anwendungsentwicklung die grundlegenden Prinzipien maschinellen Lernens noch erweitern und verbessern?

Lampert: Wir stellen beispielsweise die Frage: Wie kann maschinelles Lernen davon profitieren, dass mehrere Dinge gleichzeitig trainiert werden? Oder wie kann ein Computer inkrementell – also kontinuierlich über die Zeit – neue Dinge lernen, ohne alte, schon gelernte zu vergessen? Diese Fähigkeiten haben aktuelle Modelle nicht. Wenn ein Computer trainiert wird, Katzenfotos im Internet zu finden, erinnert er sich nicht daran, dass er davor nach Hunden gesucht hat. Er fängt jedes Mal bei null an. Im Vergleich zum menschlichen Lernen wirkt das unnatürlich. Jeder Mensch nutzt beim Lernen das, was er früher gelernt hat, als Grundlage. Dieses Prinzip auf das maschinelle Lernen zu übertragen ist eine unserer Herausforderungen. Wenn meine kleine Tochter im Zoo lernt, was ein Zebra ist, benötigt sie nur ein Bild, um zu wissen, wie dieses Tier aussieht. Der Computer benötigt im Moment tausende Bilder von Zebras. Er hat völlig vergessen, was er sonst schon über Tiere gelernt hat.

STANDARD: Ist man weit davon entfernt, dass dieses sogenannte Lifelong Machine-Learning funktioniert?

Lampert: Die Frage ist nicht zu beantworten. Die Medien zeichnen gerne schwarz-weiß – nach dem Motto: Der Computer konnte eine Sache bisher nicht, jetzt kann er sie. Aber so ist es nicht. Im Lauf ihrer Entwicklung werden die Systeme besser und besser. Bisher braucht der Computer vielleicht 1000 Bilder eines Zebras, um gut zu wissen, wie dieses Tier aussieht. Mit unseren Techniken soll diese Zahl reduziert werden. Er braucht dann vielleicht nur noch 100 oder zehn oder drei. Den Zeitpunkt, zu dem wir sagen ""Wir sind fertig"" wird es nicht geben. Aber irgendwann wird man sagen: Es ist gut genug für die momentanen Umstände.

STANDARD: Welche Anwendungen werden möglich?

Lampert: Einerseits ist das schwer zu sagen. Vielleicht gibt es Ideen, die wir jetzt noch gar nicht absehen können. Andererseits könnte mit einer Verringerung der benötigten Daten alles, was jetzt schon mit maschinellem Lernen passiert, besser gemacht werden. Im Moment kommen von großen Konzernen wie Google, Facebook, Amazon viele Anwendungen für die Endnutzer. Sie haben die meisten Daten sowie die meiste Rechenkraft und damit das Potenzial, Anwendungen herzustellen, die nützlich sind. Wenn man die Anzahl der notwendigen Daten um den Faktor 100 reduzieren könnte, wäre es deutlich einfacher, dass auch kleinere Unternehmen oder Privatpersonen das hinbekommen. Eine konkrete Anwendung liegt im Bereich der Sprachen: Es ist heute kein Problem, eine Übersetzung vom Deutschen ins Englische zu trainieren. Es gibt Milliarden Dokumente im Netz, anhand derer das gemacht wird. Es gibt aber Sprachen, zu denen es online viel weniger Dokumente gibt. Würde man ein Übersetzungsprogramm für Maltesisch trainieren wollen, wäre wahrscheinlich nicht genug Material vorhanden. Wenn man aber Vorwissen mitintegrieren kann – etwa dass Maltesisch verwandt mit dem Arabischen ist, aber viele italienische Wortübernahmen enthält –, wird es einfacher.

STANDARD: Sie beschäftigen sich auch mit der Frage, wie man die Verlässlichkeit künstlicher Intelligenz erhöhen kann. Wie gehen Sie an diese Sache heran?

Lampert: Ich als Experte für maschinelles Lernen würde im Moment nicht mit einem Flugzeug fliegen, das von einer künstlichen Intelligenz gebaut wurde. Die Systeme funktionieren in 99 Prozent der Fälle wirklich gut, machen aber auch Fehler – und das macht sie weniger vertrauenswürdig. Wir fragen uns nun, wie wir Machine-Learning-Systeme entwerfen können, die zu 100 Prozent vertrauenswürdig sind. Der Algorithmus soll die Garantie abgeben können, dass innerhalb einer gewissen Bandbreite kein Fehler gemacht wird – so wie auf einer Brücke, vor der ein Schild anzeigt: Es ist okay, mit Fahrzeugen drüberzufahren, solange sie nicht mehr als drei Tonnen wiegen. Solche Aussagen gibt es für KI-Systeme im Moment nicht.

STANDARD: Dass nur mit Mühe nachvollziehbar ist, wie KI überhaupt zu ihren Ergebnissen kommt, trägt nicht gerade zum Vertrauen der Menschen in diese Systeme bei.

Lampert: Auch zur Frage der Interpretierbarkeit gibt es viele Ansätze. In gewisser Weise wissen wir exakt, wie die Systeme zu einem Ergebnis kommen – immerhin haben wir das Computerprogramm selbst gebaut. Es ist aber so komplex, dass uns die Intuition dazu fehlt. Hier ist die Herausforderung, die richtige Sprache, das richtige Level an Abstraktion zu finden. Zu sagen, dieses Neuron hat nach drei Sekunden gefeuert, also siehst du eine Katze auf dem Bildschirm, ist vielleicht nicht die Erklärung, die man haben will. Aber wenn der Computer sagt: Ich denke, auf dem Foto ist eine Katze, weil da ein Objekt ist, das aussieht wie ein Tier, spitze Ohren hat und ein flauschiges Fell, wäre das vielleicht eine Erklärung, die akzeptabel ist.

STANDARD: Sie haben vor kurzem ein Buch herausgegeben, das sich mit künstlicher Intelligenz beschäftigt. Was war die Motivation?

Lampert: Das Problem beim Thema künstliche Intelligenz ist, dass mittlerweile alle eine Meinung dazu haben, kaum jemand aber weiß, was tatsächlich dahintersteckt. Es wirkt wie Magie. Dabei ist es einfach eine Software, die gewisse Schritte ausführt, die zu einem Ergebnis führen. Manche Dinge werden damit leichter, es gibt aber auch nachteilige Effekte. Als Gesellschaft muss man darüber reden. Es muss einen Diskurs geben, welche Dinge automatisiert werden sollen, welche nicht; welche Daten bereitgestellt werden, welche privat bleiben sollen. Dafür braucht es aber Wissen, nicht nur Meinung. Wir haben uns die Aufgabe gestellt, die Konzepte zu erklären, ohne hinter einer Fachsprache zu verschwinden. Geschrieben ist das Buch von Studierenden in ihren 20ern, die noch verstehen, welche Probleme man hat, wenn man sich dem Thema neu nähert.";"Mathematiker: ""Wie kann ein Computer lernen, ohne zu vergessen?""";Standard;Alois Pumhösel
20.08.2020;Sprung in die Welt der künstlichen Intelligenz;"Seit Jahren ist Artificial Intelligence (AI) das zentrale Schlagwort in den Informationstechnologien. Zumeist ist damit eine automatische Mustererkennung in Daten auf Basis neuronaler Netzwerke und von Machine-Learning gemeint. Die Methoden gaben Computersystemen viele neue Fähigkeiten. AI-Systeme erkennen Gesichter auf Handyfotos und Tumoren auf Röntgenaufnahmen, sie helfen autonomen Autos bei der Navigation oder in Industrieanlagen bei der Qualitätsprüfung von Werkstücken. Vielleicht gerade wegen der vielfältigen Einsetzbarkeit der neuen Technologie werden ihre Grenzen oft auch falsch wahrgenommen. ""Das Problem, dass AI überschätzt wird, begleitet die Technologie seit ihrem Anbeginn"", sagt Wilfried Wöber vom Kompetenzfeld Digital Manufacturing & Robotics der FH Technikum Wien. ""Potenzielle Anwender gehen oft davon aus, dass AI eine Herausforderung selbstständig erkennen und lösen kann – was natürlich nicht der Fall ist.""
Sinnvolle Anwendungen

Wöber leitet das Projekt ""AI anwenden und verstehen"", gefördert von der für Wirtschaft, Arbeit und Statistik zuständigen MA 23 der Stadt Wien. Dabei geht es darum, in den Unternehmen Aufklärungsarbeit zu leisten und den Verantwortlichen dort ""zu zeigen, was künstliche Intelligenz wirklich ist und was man damit machen kann"", sagt der Forscher. Derzeit kooperiert er im Projekt mit etwa 50 Unternehmen.

Einerseits sollen sinnvolle Anwendungen der neuen Technologien ""für reale Probleme"" in den Unternehmen gefunden werden – am besten ""in Bereichen, in denen man heute noch gar nicht an AI denkt"", betont Wöber. Auf der anderen Seite soll das Thema aber auch entmystifiziert werden. Die Unternehmen sollen auf Basis ihres neu erworbenen Know-hows selbst entscheiden können, ob die Technologie für sie von Bedeutung ist. ""Der Bäcker von nebenan kann Sensoren installieren, um zu zählen, wann er wie viel Kundenfrequenz hat. Die Frage ist, ob sich diese Information für seine Geschäftstätigkeit tatsächlich auszahlt"", gibt der Forscher ein Beispiel.
Biodiversitätsforschung

Die Hard- und Softwareindustrie, die ihre Produkte absetzen möchte, finde oft auch dort Anwendungen, wo sie nicht unbedingt Mehrwert bieten, sagt Wöber: ""Viele Probleme in der Produktion kann man sehr gut auch mit klassischen Methoden der Automatisierungstechnik lösen.""

Neben seiner Arbeit an der FH Technikum schreibt der 1987 in Wien geborene Forscher mit Studienabschluss in Mechatronik und Robotik seine Dissertation an der Wiener Boku. Dort geht es darum, AI im Dienste der Biodiversitätsforschung einzusetzen. ""Ich nutze die Technologie, um Tierfotos zu analysieren. Konkret geht es um Fische in Afrikas Seen"", sagt Wöber. ""Biologen sollen dank der resultierenden Daten ergründen können, warum Populationen derselben Spezies an verschiedenen Ort anders aussehen.""

Neben jener als Forscher macht Wöber noch eine Karriere ganz anderer Art: Er trainiert Trampolinturner, die bei ihren bis zu zwölf Meter hohen Sprüngen Salti und Schrauben vollführen. Bis zum Alter von 14 Jahren war er selbst aktiv, seit 19 ist er Lehrwart und führt junge Athleten zu Weltmeisterschaften und Olympischen Jugendspielen – vielfach mit Topergebnissen.";https://www.derstandard.at/story/2000119436109/sprung-in-die-welt-der-kuenstlichen-intelligenz;Standard;Alois Pumhösel
10.07.2020;KI verbraucht enorm viel Strom – ein unlösbares Problem?;"Künstliche Intelligenz (KI) wird oft als das Öl der Zukunft bezeichnet. In der abgegriffenen Metapher steckt tatsächlich ein Funken Wahrheit. Ähnlich wie bisher bei Ölvorkommen gilt nun: Wer die KI erst einmal erschlossen und nutzbar gemacht hat, für den fließt auch das Geld.

KI-Modelle stecken nicht nur hinter Social-Media-Algorithmen, sie suchen auch die richtigen Personen für Bewerbungsgespräche aus, in Zukunft könnten sie selbstfahrende Autos steuern. Bevor Machine-Learning-Modelle aber wirklich intelligent werden, muss man sie mit vielen Daten füttern und trainieren. Es sind teilweise Millionen an Zyklen notwendig, bis ein Modell brauchbar wird.
So viel CO2 wie zehn Autos

Das kostet viel Rechenpower – und damit auch viel Strom, der nicht immer aus erneuerbaren Quellen kommt. Um ein einziges Modell zu berechnen, könnten bis zu 283 Tonnen CO2 emittiert werden, haben Wissenschafter der Universität von Massachusetts errechnet. Das ist mehr als zehn amerikanische Autos während ihrer gesamten Lebenszeit in die Luft blasen. Für den Versuch ließen die Forscher bekannte KI-Modelle auf einem einzelnen Grafikprozessor laufen und multiplizierten den Energieverbrauch mit der Gesamtzeit, die für ein vollständiges Anlernen notwendig wäre. Ausgegangen sind die Forscher aber vom Energiemix, der in den USA üblich ist. Dort wird noch immer der Großteil des Stroms aus Kohle gewonnen. In Österreich ist der Energiemix grüner: Hierzulande stammen mehr als 80 Prozent aus erneuerbaren Energien. Doch nicht immer werden KI-Modelle dort trainiert, wo sie erforscht werden. Unternehmen und Forscher greifen für rechenintensive Aufgaben oft auf Cloud-Computing-Anbieter zurück. Wenig überraschend dominieren die IT-Riesen Amazon, Google und Microsoft das Geschäft mit der Rechenleistung in der Wolke.

Zwar haben die Unternehmen angekündigt, verstärkt auf erneuerbare Energien zu setzen, vor allem Marktführer Amazon lässt sich aber ungern in die Karten schauen, heißt es in einem Bericht von Greenpeace aus dem Jahr 2017. Mindestens die Hälfte von Amazons Strom soll aus Kohle und Gas kommen. Doch auch die meisten Konkurrenten sind von einer Green Cloud weit entfernt.

In Northern Virginia, dem Gebiet mit der höchsten Dichte an Rechenzentren weltweit, liegt der Anteil an erneuerbaren Energien jedenfalls im einstelligen Bereich. Inzwischen ist das Internet für rund zehn Prozent des globalen Energieverbrauchs verantwortlich. Weil KI in immer mehr Bereichen Einzug hält, wird ihr Anteil am Stromverbrauch steigen.
Licht in Sicht

In Zukunft sollen Quantencomputer das Energieproblem von KI lösen. Anstatt Nullen und Einsen können diese Zustände darstellen, die mit klassischer Physik nicht erklärbar sind. Von den Quantencomputern, die derzeit noch am Anfang stehen, erhofft man sich gewaltige Leistungssprünge. Die gleiche Aufgabe wäre dann mit viel weniger Rechenpower lösbar, was letztlich weniger Energieverbrauch und Emissionen bedeutet. Eine andere Hoffnung sind optische Prozessoren, die statt mit Transistoren mit Lichtsignalen arbeiten und weniger Abwärme verursachen.

Ein großer Teil des Strombedarfs geht ohnehin in die Kühlung der Prozessoren, die abgeleitete Wärme wird oft ungenutzt nach draußen geblasen. Bei Google erfand ausgerechnet eine KI ein System, mit dem sich Rechenzentren energiesparender kühlen lassen. Mithilfe von Daten tausender Sensoren prognostiziert ein Algorithmus die Wärmeentwicklung an verschiedenen Stellen der Serverfarm und stimmt die Kühlleistung darauf ab.

Und Amazon nutzt die verbleibende Wärme einfach, um seinen neuen Campus zu heizen.";https://www.derstandard.at/story/2000117876198/wie-nachhaltig-ist-ki;Standard;red
02.03.2020;TU Wien: Neues Labor will lernende eingebettete IT-Systeme entwickeln;"Wie man maschinelles Lernen auf möglichst effiziente und ressourcenschonende Weise in eingebetteten IT-Systemen (""Embedded Systems"") nutzen kann, will ein neues Christian Doppler(CD)-Labor an der Technischen Universität (TU) Wien erforschen. Die Forscher gehen davon aus, dass damit Künstliche Intelligenz (KI) Schritt für Schritt im Alltag Einzug halten wird. Das Labor wurde am Montag eröffnet.

Maschinelles Lernen ist ein Teilaspekt der KI. Dabei arbeitet ein Computer nicht einfach vorgefertigte Befehle ab. Vielmehr werden Systeme entwickelt, die aus Daten lernen und damit Abläufe optimieren können. Üblicherweise werden dafür Hochleistungscomputer benötigt. Doch die stehen vielfach für Anwendungen in der Praxis nicht zur Verfügung und Systeme müssen mit begrenzter Energie, Speicherplatz und Rechenzeit auskommen.
Ziele

Als Beispiel nennen die TU-Forscher in einer Aussendung selbstfahrende Autos. Von Sensoren erfasste Bilddaten etwa müssen im Auto innerhalb kurzer Zeit und mit hoher Verlässlichkeit verarbeitet werden – und das mit begrenzter Rechenkapazität und limitiertem Stromverbrauch.

Das soll in ""Embedded Systems"" passieren, kleinen Computern, die kompakt und kaum sichtbar in verschiedene Geräte eingebaut sind. Die Wissenschafter um Laborleiter Axel Jantsch wollen sich dabei vor allem darauf konzentrieren, ""wie man Hardware am besten konfigurieren kann"", also etwa welche Chips und Rechnerarchitektur sich für bestimmte Anwendungen eignen und welche Plattformen und Werkzeuge die höchste Effizienz ermöglichen.
Erkennung

Im Zentrum der Forschung am ""CD-Labor für Embedded Machine Learning"" stehen lernende Systeme im Bereich der Bild- und Videoerkennung. Dazu zählen Anwendungen für den Außenbereich, ohne Zugang zu großen Speicherkapazitäten und Energiequellen, beispielsweise automatische Ampelsteuerungen. Solche Systeme könnten aber auch bei selbstfahrenden Autos, Qualitätskontrollen bei Produktionsanlagen oder personalisierter Sensorik zur Analyse von Körpersignalen in der Medizin und Sport eingesetzt werden.

In den von der Christian Doppler Gesellschaft (CDG) für jeweils sieben Jahre genehmigten CD-Laboren kooperieren Wissenschafter mit Unternehmen im Bereich anwendungsorientierte Grundlagenforschung. Das Budget kommt dabei jeweils zur Hälfte von der öffentlichen Hand und den Industriepartnern. Im neuen CD-Labor sind das die Mission Embedded GmbH, Siemens Österreich und AVL List. Auch die TU Graz ist als wissenschaftlicher Partner am Labor beteiligt.";https://www.derstandard.at/story/2000115224790/tu-wien-neues-labor-will-lernende-eingebettete-it-systeme-entwickeln;Standard;APA
08.09.2020;Skillmanagement: Was kannst du noch?;"Die Idee, eine Software zu entwickeln, die Fähigkeiten von Mitarbeitern abseits der Jobbeschreibung im Unternehmen sichtbar macht, hatte das Gründungsquartett von Skilltree am Ende ihres Informationsmanagement-Studiums an der Fachhochschule Joanneum. Sie waren bereits in unterschiedlichen IT-Unternehmen tätig und haben ähnliche Probleme geortet: ""Man sieht die Person immer nur im Kontext ihrer Arbeit, aber der Pool an anderen Fähigkeiten und ihr zusätzliches Wissen ist nicht sichtbar"", sagt der Finanzchef des Grazer Start-ups, Markus Skergeth. Der Gedanke sei gewesen, diese Skills sichtbar zu machen. Systeme, die Fähigkeiten verwalten, gebe es bereits, aber diese seien in ihrer Nutzung nur auf Personalabteilungen (HR) fokussiert. Damit Kompetenzmanagement im Unternehmen funktioniere, müssten Mitarbeiter das System selbst verwenden. Keiner möchte aber ein System nutzen, das nur Arbeit macht.

Die Software wird deshalb mit Machine-Learning und künstlicher Intelligenz unterstützt, um den Aufwand für den Nutzer zu minimieren. Gearbeitet wird mit Texterkennung, um Lebensläufe auszuwerten, sowie mit Modellen zur Erkennung komplexer Zusammenhänge, damit Verbindungen zwischen den einzelnen Mitarbeitern erkannt werden können. Für die Praxis bedeute das: Man müsse nicht mehr alle Kollegen im Unternehmen persönlich kennen, um rasch ein agiles Team zusammenzustellen.

Die Software schlägt anhand der benötigten Skills Mitarbeiter vor, die auch über das System kontaktiert werden können. Die Jungunternehmer forschen gerade mit der FH Joanneum, wie sich auch noch anhand der Persönlichkeit ein Team zusammenstellen ließe.
Spielerischer Ansatz

Besonders stolz sei man darauf, dass sich die Software an Gamification orientiere. Jeder Mitarbeiter hat wie in einem Computerspiel einen Fähigkeitenbaum und entwickle seinen ""Charakter"" im Unternehmen. Der Mitarbeiter loggt sich dafür in eine Webapplikation ein und bekommt ein Profil, das im Idealfall bereits mit Skills seines CV hinterlegt ist.

Zusätzlich findet man Fähigkeiten vor, die für das Unternehmen relevant sind. Zutreffende Skills können aktiviert werden, aber auch Fähigkeiten angegeben werden, die man sich noch aneignen möchte. Dies sei auch ein Feedbacksystem für die HR-Abteilung, um zu erkennen, wo Interesse an Weiterbildungen besteht, sagt Skergeth.
Diskriminierung vorbeugen

Die Gründer von Skilltree haben das Thema Diskriminierung ebenfalls bei der Programmierung berücksichtigt: Alle Daten, die zu einer Schlechterstellung beitragen könnten, seien dem System vorenthalten. ""Auf die Algorithmen hat auch kein Unternehmen Zugriff, den haben nur wir."" Und in puncto Datenschutz: Die Daten werden in der Europäischen Union gespeichert. Es werden auch nur die notwendigsten Daten der Mitarbeiter an Skilltree übermittelt. Passwörter sehe man gar nicht ein.";https://www.derstandard.at/story/2000119792668/skillmanagement-was-kannst-du-noch;Standard;Stefanie Leschnik
23.11.2020;Linzer Drohne findet dank KI-Unterstützung Verunglückte auch im dichten Wald;"Bei Rettungseinsätzen werden vermisste oder verunglückte Personen mit Wärmebildkameras vom Helikopter aus gesucht. In dicht bewaldeten Gebieten sind sie so aber kaum zu finden. Ein neues Verfahren ermöglicht nun, mithilfe künstlicher Intelligenz (KI) zahlreiche Bilder einer auf einer Drohne montierten Wärmebildkamera zu kombinieren und dadurch auch bei starker Vegetation eine Person zu orten, berichten Forscher der Uni Linz im Fachjournal ""Nature Machine Intelligence"".

In Österreich ist der ÖAMTC 2018 knapp 2300 alpine Such- und Rettungseinsätze mit Helikoptern geflogen, in den 165 Nationalparks der USA waren es im selben Jahr mehr als 2700 derartige Einsätze, schreiben die Wissenschafter um Oliver Bimber vom Institut für Computergrafik der Universität Linz in ihrer Arbeit. In Zukunft werden dabei wohl autonome Drohnen vermehrt eingesetzt, um Suchgebiete zu vergrößern oder Suchzeiten zu verkürzen.
KI kombiniert Bilder

Das Problem ist, dass dabei Wärmebildkameras zum Einsatz kommen, die Bilder aus der Differenz von Körperwärme und Umgebungstemperatur erzeugen – ein Verfahren, das in dicht bewaldeten Gebieten keine Ergebnisse liefert. Denn entweder verdeckt die Vegetation den Untergrund zu stark, oder die sonnenbestrahlten Bäume haben eine ähnliche Temperatur wie die vermisste Person.

Bimber und sein Team haben nun einen Drohnenprototyp entwickelt, der diese Aufgabe meistern kann. Die Drohne versucht dabei nicht wie üblich, Personen in einzelnen Wärmebildern zu detektieren, sondern kombiniert mehrere Einzelaufnahmen zu einem Integralbild. Und erst dieses wird mithilfe von Deep-Learning-Verfahren klassifiziert, also von der KI ausgewertet.

""Das Ganze funktioniert nach dem Messprinzip synthetischer Aperturen, wie es schon in anderen Bereichen genutzt wird, etwa bei Radioteleskopen"", erklärte Bimber gegenüber der APA. Ist etwa der Durchmesser eines Radioteleskops nicht groß genug für eine gute Signalqualität, werden einfach mehrere Teleskope vernetzt, um rechnerisch ein Signal zu erzeugen, das jenem eines viel größeren Teleskops entspricht.

Dieses Prinzip nutzen die Linzer Forscher, indem die Drohne Bilder über eine große Fläche aufnimmt. ""Die Bilder werden dann so kombiniert, dass sich daraus ein Integralbild ergibt. Dieses entspricht einem Bild, das man mit einer Linse von mehreren Hundert Metern Durchmesser aufgenommen hat"", so Bimber.

Während eine Einzelaufnahme mit einer normalen, nur wenige Millimeter großen Linse eine so hohe Tiefenschärfe hat, dass eine vermisste Person fast vollständig verdeckt ist, hat das Integralbild der künstlich erzeugten riesigen Linse nur eine ganz geringe Tiefenschärfe. ""Indem wir diese Linse auf den Waldboden fokussieren, wird alles über dem Boden, also etwa die ganzen Bäume, so unscharf, dass es im Integralbild verschwindet, und die vermisste Person wird erkennbar"", erklärte der Wissenschafter.
Hohe Erkennungsrate

""Die Prototypen funktionieren wirklich gut, wir haben eine Erkennungsrate von 87 bis 95 Prozent im Praxiseinsatz, trotz starker Verdeckung"", sagte Bimber. Mit herkömmlichen Einzelbildern würde man dagegen lediglich eine Erkennungsrate von unter 25 Prozent erreichen.

Weil es für diesen Einsatzzweck keine Daten zum Trainieren des neuronalen Netzes gegeben hat, haben die Forscher eine eigene Datenbasis erstellt, die nun frei zur Verfügung steht. ""Wir haben dafür ein Jahr lang Bilddaten von Menschen in allen möglichen Posen von oben aufgenommen, stehend, gehend, sitzend, liegend"", so Bimber.

Dabei konnten die Forscher zeigen, dass es gar nicht notwendig ist, diese Bilddaten in allen möglichen Waldtypen aufzunehmen, ""weil unser Verfahren die Verdeckung so gut wegrechnet und die Klassifizierung völlig unabhängig von der Verdeckung gelingt"". Daher konnten sie die Trainingsdaten auf einem Testgebiet am Campus der Uni Linz aufnehmen.
Drohne mit Verbrennungsmotor geplant

Die Testmessungen mit den Prototypen wurden dann in ganz unterschiedlichen Wäldern, zu unterschiedlichen Jahreszeiten und bei unterschiedlichen Bedingungen durchgeführt, nur in der Nacht darf nicht geflogen werden, weil immer Sichtkontakt zur Drohne bestehen muss. Zum Zeitpunkt der Einreichung des ""Nature""-Papers sei die Integration und Klassifikation der Bilder noch nicht in Echtzeit erfolgt, dies sei aber mittlerweile möglich.

Die einzige Einschränkung für einen Einsatz in der Praxis sieht Bimber derzeit in der batteriebedingt limitierten Flugzeit der Drohnen von maximal 30 Minuten. Die Forscher planen aber bereits ein neues Projekt, im Zuge dessen eine Drohne mit Verbrennungsmotor verwendet werden soll, ""und dann wird das Ganze praktikabel"".

Das Verfahren eignet sich den Forschern zufolge aber nicht nur für zivile Such- und Rettungseinsätze, sondern auch in anderen Bereichen wie für Überwachungsaufgaben der Polizei oder des Militärs, bei autonomen Fahrzeugen oder für Wildbeobachtungen.";https://www.derstandard.at/story/2000121921305/linzer-drohne-findet-dank-ki-unterstuetzung-verunglueckte-auch-im-dichten;Standard;APA
24.09.2020;ACR-Innovationspreis für Fake Shop-Detektor;"Für die Entwicklung eines ""Fake Shop-Detektors"" wurde das Österreichische Institut für angewandte Telekommunikation (ÖIAT) mit einem von drei Innovationspreisen des mittelständischen Forschungsnetzwerks ACR (Austrian Cooperative Research) ausgezeichnet. Ein weiterer Preis ging an das Forschungsinstitut für Chemie und Technik (OFI) für eine auch Viren und Bakterien zurückhaltende Filtertechnologie für die Innenbelüftung von Bussen, teilte ACR am Dienstag mit.

Der Fake Shop-Detektor wurde vom ÖIAT in einem Forschungsprojekt unter der Leitung des Austrian Institute of Technology (AIT) entwickelt. Er basiert auf der ""Watchlist Internet"" des ÖIAT, in der seit 2013 mehr als 7.700 betrügerische Online-Shops gesammelt wurden. In dem Projekt ""MAL2"" (MAchine Learning detection of MALicious content) sollen mit Hilfe Künstlicher Intelligenz (KI) Quellcodes von Internet-Shops analysiert und rückgemeldet werden, ob diese vertrauenswürdig oder nicht. Der Detektor soll Ende des Jahres als Plugin veröffentlicht werden.
Saubere Luft

Dem OFI ist es gemeinsam mit dem Busunternehmen Hehle Reisen aus Lochau (Vorarlberg) und weiteren Partnern nach dem ersten Lockdown im Frühjahr gelungen, eine Filtertechnologie für die Innenbelüftung von Bussen so weiterzuentwickeln, dass nicht nur Allergene aus der Raumluft zurückgehalten werden, sondern auch Viren und Bakterien.

Der dritte ACR-Innovationspreis ging an die Holzforschung Austria und die Waldviertler Firma Willibald Longin für die Entwicklung neuer Deckenkonstruktionen mit großer Spannweite, die Holz und Beton auf eine neuartige Weise verbinden und dabei ohne Stahlbewehrung und Klebefugen auskommen. Die Gewinner wurden durch ein öffentliches Online-Voting und eine Fachjury ermittelt.";https://www.derstandard.at/story/2000121946502/acr-innovationspreis-fuer-fake-shop-detektor;Standard;APA
21.09.2020;Twitter schneidet Fotos automatisch zu und Schwarze weg;"Es ist eines dieser vielen Mini-Features, die den Alltag praktischer machen, kaum bewusst auffallen – und die deshalb auch keiner hinterfragt: der automatische Zuschnitt von Fotos, etwa auf Twitter. Die Plattform erkennt das inhaltliche ""Zentrum"" von hochgeladenen Bildern und schneidet diese so zu, dass User in ihren Twitter-Feeds bei besonders hohen und breiten Bildern den angeblich interessantesten Ausschnitt angezeigt bekommen. Das Problem dabei: Der Algorithmus scheint systematisch nichtweiße Personen wegzuschneiden.

Ein Twitter-User stellte den Zuschnittsalgorithmus auf die Probe: Er postete zwei schmale weiße Streifen, an deren oberem und unterem Ende jeweils gegengleich die Porträts des früheren US-Präsidenten Barack Obama und des republikanischen Politikers Mitch McConnell platziert waren. Twitter entschied sich in beiden Fällen für McConnell, in der nichtexpandierten Ansicht ist Obama weggeschnitten.
Lenny statt Carl

Andere Twitter-User versuchten das Verhalten zu replizieren: Auch Michael Jackson scheint Twitter mit heller Haut besser zu gefallen, sogar bei Zeichentrickfiguren bevorzugt die Plattform weiße Personen: Den schwarze Carl von den ""Simpsons"" schneidet Twitter weg, stattdessen fällt der Bildausschnitt auf seinen gelben Kumpanen Lenny.
Racial Bias in KI-Systemen wird zum Problem

Twitter arbeitet beim Zuschnitt von Fotos mit einem Machine-Learning-Modell, wie das Unternehmen in einem Blogpost von 2018 schrieb. Zuvor habe man sich auf klassische Gesichtserkennung verlassen, die jedoch fehleranfällig war – schließlich befindet sich nicht in jedem Bild ein Gesicht. Der neue Algorithmus hingegen stelle die ""interessantesten"" Merkmale eine Bildes ins Zentrum.

Der sogenannte ""Racial Bias"" von künstlicher Intelligenz kommt immer wieder zum Vorschein, beim Twitter-Algorithmus handelt es sich aber noch um ein vergleichsweise harmloses Beispiel. So wurde etwa bekannt, dass autonome Autos schwarze Menschen eher überfahren und dass ein System, das viele US-Krankenhäuser einsetzen, Schwarzen schlechtere Behandlung beschert. Auch Gesichtserkennung, die weltweit zunehmend eingesetzt wird, ist bei Schwarzen fehleranfälliger.
Twitter will untersuchen

Manche führen das Problem auf die mangelnde Diversität im Technologiesektor zurück, ein Problem könnten auch die Datensätze sein, mit denen KI-Modelle trainiert werden. Die Modelle würden dann menschliche Fehler und Vorurteile reproduzieren. Auch der Zuschnittsalgorithmus von Twitter wurden laut dem Unternehmen mit Material von externen Dienstleistern trainiert.

Allerdings scheint die Hautfarbe nicht das einzige Entscheidungskriterium für den Zuschnitt von Twitter-Bildern zu sein, wie Experimente von anderen Twitter-Usern zeigen. Möglicherweise spielt etwa der Kontrast eine Rolle. Dantley Davis, Chef für Design und Entwicklung bei Twitter, tweete etwa ein Bild, bei dem sich der Algorithmus für eine schwarze Person ""entschied"".

Davis werde das Problem jedenfalls ""untersuchen"", schrieb er auf Twitter. Auch der Twitter-Technologiechef Parag Agrawal twitterte, dass das Modell ""laufende Anpassung"" benötigt. Er sei aber froh über die ""öffentlichen, offenen und rigorosen"" Experimente der Twittter-Nutzer, von denen er lernen wolle.";https://www.derstandard.at/story/2000120146773/twitter-schneidet-fotos-automatisch-zu-und-schwarze-weg;Standard;pp
05.04.2020;Computer analysieren historische Reiseliteratur;"""Eine offene Barke wiegt mich über die silbernen Fluthen von Triest zu Istriens Küsten. Berge und Thäler fliegen vorüber, immer neue Ansichten steigen aus dem Schooss des Meeres, und das ahnungsvolle Blau der Ferne durchzittern weiße Segel, gleich Geistern künftiger Genüsse."" Was sich anhört wie schwülstige Träume von der Ferne in Zeiten, in denen Urlaubsreisen mit Selbstisolation in den eigenen vier Wänden getauscht werden müssen, sind in Wirklichkeit die ersten Worte von Joseph Georg Wiedemanns Streifzüge an Istriens Küsten, erschienen 1810.

Das Buch ist eines von hunderten Reiseberichten, die bisher in den Beständen der Österreichischen Nationalbibliothek (ÖNB) schlummerten, ohne dass sie als solche im Katalog aufschienen. Bis ein Algorithmus, der die digitalisierten ÖNB-Bestände abgraste, befand, das Werk sei zu 99 Prozent dem Genre ""Reisebericht"" zuzuordnen. Was zu 100 Prozent richtig ist.

""Mit den Methoden der Digital Humanities, der digitalen Geisteswissenschaften, eröffnen sich ganz neue Möglichkeiten für die Auswertung und Analyse von großen Textmengen"", sagt Martin Krickl, Data Librarian an der ÖNB. Krickl ist Teil des Teams des Projekts ""Travelogues"", in dem Expertise aus Geschichts-, Bibliotheks- und Computerwissenschaften zusammengeführt wird, um deutschsprachige Reiseberichte von 1500 bis 1876 semiautomatisiert und mithilfe von Machine-Learning ausfindig zu machen und zu analysieren.
Algorithmus für Stereotype

Das Projekt, das von Historikern des Instituts für die Erforschung der Habsburgermonarchie und des Balkanraumes der Akademie der Wissenschaften geleitet und vom Wissenschaftsfonds FWF sowie der Deutschen Forschungsgesellschaft (DFG) gefördert wird, läuft noch bis 2021. Es hat zum Ziel, dass eigens entwickelte Algorithmen speziell Berichte über Orientreisen abklopfen, und zwar dahingehend, wie sie die Konzepte ""Fremdheit"" und ""Orient"" darstellen. ""Wir haben im Projekt Austrian Books Online in den vergangenen Jahren in Kooperation mit Google 600.000 Bücher mit 200 Millionen Seiten digitalisiert"", sagt Max Kaiser, Leiter der Abteilung Forschung und Entwicklung der ÖNB. ""Wir machen diese Volltexte nicht nur online der Öffentlichkeit zugänglich, sondern stellen sie auch als Daten für Forschungsprojekte zur Verfügung."" Mithilfe von computerunterstütztem ""Distant Reading"" können im Unterschied zum herkömmlichen ""Close Reading"", in dem Wissenschafter Bündel von Texten sozusagen eigenhändig lesen, Forschungsfragen auf Basis großer Datenmengen bearbeitet werden, wie Kaiser betont – wie eben zum Beispiel die Entwicklung von Orient-Stereotypen anhand von Reiseliteratur.
KI-Verfahren für alte Drucke

Mit dem Travelogues-Projekt werden historische deutschsprachige Reiseberichte nun erstmals systematisch gesammelt und nicht nur qualitativ, sondern auch mit quantifizierenden, der künstlichen Intelligenz entlehnten Verfahren analysiert. Dazu wurde zunächst die Texterkennung mithilfe von Machine-Learning-Techniken verbessert. ""Gerade deutsche historische Druckschriften sind extrem heterogen"", sagt Krickl. ""Jede Druckwerkstatt hatte ein eigenes Typenmaterial. Auch wenn sich Buchstaben oft nur leicht unterscheiden, ein Schnörkel da und ein Strich dort ist für die automatische Texterkennung eine Herausforderung."" Auch die Erkennung von unterschiedlichen Segmentierungen, also der Leerräume zwischen den Wörtern, bedurfte einigen Trainings – das vom Forschungsinstitut L3S der Universität Hannover übernommen wurde.

Das Austrian Institute of Technology (AIT) wiederum entwickelte auf Basis einer von den Historikern festgelegten Definition einen sogenannten Classifier. Das ist ein Algorithmus, der aus dem gesamten digitalisierten Bestand historische Reiseberichte eruiert. ""Bereits eine verifizierte Grundmenge von 30 Werken reichte als Basis für die Erkennung"", sagt Krickl. Insgesamt steht nun ein Korpus von rund 3500 Reiseberichten zum Text-Mining bereit.
Lernen von Machine Learning

Um diese Werkzeuge auf andere Genres und Sprachen umzulegen, brauche es vor allem klare Definitionen des gesuchten Themas, betonen Kaiser und Krickl. Ein Testlauf, bei dem der Algorithmus mit einer Grundmenge an Kochbüchern gefüttert wurde, zeigte, dass das System alle möglichen Bücher mit Rezepten vorschlug, seien es Apothekerrezepte oder Anleitungen, wie man Schießpulver herstellt, schildert Krickl. ""Machine-Learning zeigt auch die Lücken im eigenen Denken auf, wodurch auch die Forschung ihr Selbstverständnis hinterfragen muss.""

Weil sich viele Reiseberichte auf die Wahrnehmung anderer Autoren stützten, gerade was die Auffassungen über das ""Fremde"" in orientalischen Ländern betrifft, erarbeiten die Travelogues-Forscher Modelle für Intertextualität, um automatisiert nach Abhängigkeiten zwischen den Reiseberichten suchen zu können. ""Wir experimentieren sowohl mit linguistischen Ansätzen, wie man sie aus der Plagiatssoftware kennt, als auch mit Methoden, die etwa Ortsnamen vergleichen, um Bezüge herzustellen"", erklärt Krickl. ""In einem weiteren Schritt müssen die Historiker das diffuse Thema der Fremdheit in metrisierbare Einheiten zerlegen und dann mithilfe von Algorithmen feinjustieren, damit letztlich verlässlich wiederkehrende Muster erkannt werden können.""

Sämtliche Informationen fließen dann auch in die Datenbanken der ÖNB ein – wodurch der Katalog aufgewertet und Werke mitsamt ihren Inhalten und Bezügen zu anderen Autoren besser auffindbar werden. ""Wir lernen so unseren Bestand besser kennen"", sagt Max Kaiser. Die Forschungsdaten, Metadaten und Softwarecodes werden übrigens frei zugänglich gemacht für weitere Erforschungen – die geneigten Leser und Leserinnen können sich derweil anhand der historischen Berichte auf eigene Faust auf eine imaginäre Reise begeben.";https://www.derstandard.at/story/2000116459111/computer-analysieren-historische-reiseliteratur;Standard;Karin Krichmayr
12.06.2020;Algorithmen für den optimierten Kran;"Krane kauft man nicht von der Stange. Je nach den individuellen Einsatzsituationen in Häfen, Logistikzentren oder auf Baustellen müssen sie dementsprechend konfiguriert werden. Für die Hersteller ist es sinnvoll, nicht bei jedem Kran bei null zu starten, sondern die Planungen zumindest teilweise automatisieren zu können.

Doris Entner hilft ihnen dabei. Als Expertin im Bereich Digital Engineering des Vorarlberger Forschungsinstituts V-Research – ein Teil des KMU-Forschungsnetzwerks Austrian Cooperative Research (ACR) – arbeitet sie viel mit Kranherstellern zusammen und unterstützt die Entwicklung von Produkten und Prozessen mit klugen Algorithmen. ""Bei uns verbindet sich das Fachwissen der Ingenieure aus der Praxis mit Know-how aus Informatik und Mathematik"", sagt die 1983 geborene Vorarlbergerin.

Diese Schnittstellenfunktion führte etwa zu einem Werkzeug, das die Modellbildung der Krane automatisiert und als Verkaufskonfigurator eingesetzt werden kann. ""Das ist ein System, das eine grafische Oberfläche mit einem allgemeinen Grundmodell verbindet"", sagt Entner. ""Man gibt Maße und Parameter ein, und das Tool errechnet und visualisiert auf dieser Basis ein CAD-Modell eines Krans, der für eine konkrete Situation maßgeschneidert ist.""
Statikproblem

Besonders aufwendig kann sich die Optimierung der Statik eines Krans gestalten. Blechdicken sollen beispielsweise angesichts der erforderlichen Statik so dünn wie möglich sein, um die Konstruktion leicht und günstig zu halten. Die Berechnungen müssen dabei für tausende Konstellationen durchgespielt werden.

Entner und Kollegen haben das Statikproblem mit verschiedenen Algorithmen gelöst und die Resultate anhand der Kundenanforderungen verglichen. ""Wir haben zum Beispiel mithilfe von Machine-Learning-Algorithmen eine Annäherungsfunktion entwickelt, die die Optimierung beschleunigt"", sagt die Forscherin. ""Die Anwendung konnte dann in kürzerer Rechenzeit ein sehr gutes Ergebnis erzielen.""

Bei vielen Optimierungsaufgaben in der Industrie benötigt man aber gar keine Neuentwicklung, sondern man kann auf die vielen in einschlägigen Datenbanken vorhandenen Algorithmen zurückgreifen. Dabei ist es allerdings oft recht schwierig herauszufinden, welches der Programme nun genau für den jeweils individuellen Fall passt.
Entscheidungsmatrix

In einem weiteren Projekt helfen Entner und ihre Kollegen bei dieser Frage: ""Wir haben eine Entscheidungsmatrix entwickelt, die aufgrund der konkreten Anforderungen die passenden Programme auswählt"", sagt die Forscherin.

Mathematik war schon in der Schule Entners liebstes Fach. Ihre Studienwahl fiel auf Technische Mathematik. Während des Studiums an der Uni Innsbruck absolvierte sie ein Austauschjahr in Helsinki – einem Ort, an den sie für ihr Doktorat, das sie 2013 abschloss, zurückkehrte. Bereits hier beschäftigte sie sich mit dem Einsatz von maschinellem Lernen für die Datenanalyse.

2018 wurde sie bei der Vergabe des Wissenschaftspreises des Landes Vorarlberg mit einem Spezialpreis geehrt. Entner hat mittlerweile zwei Kinder, die ein und fünf Jahre alt sind. ""Der Große rechnet auch schon gern"", sagt sie.";https://www.derstandard.at/story/2000117981341/algorithmen-fuer-den-optimierten-kran;Standard;Alois Pumhösel
25.09.2020;Roboter sollen fehlende Erntehelfer nach Brexit ersetzen;"Erntearbeit ist hart und wird oft von unterbezahlten Arbeitern aus dem Ausland erledigt. Automatisierbar waren bestimmte Arbeiten bisher kaum. Während etwa Getreide und Kartoffeln schon seit langem von Maschinen abgeerntet werden, ist menschliche Hilfe beim Pflücken vieler Sorten Obst und Gemüse auch heute noch unerlässlich.

Diese Hilfe könnte Großbritannien künftig fehlen. Ein neues Einwanderungsgesetz droht die Einreise von ungelernten Arbeitskräften zu erschweren. Bisher kamen hunderttausende Menschen aus vor allem östlichen EU-Ländern nach Großbritannien, um einfache Arbeiten, etwa auf dem Feld, zu erledigen. Mit dem Austritt aus der EU und dem Ende der Übergangsperiode ist damit nun Schluss. Bereits vor dem Brexit bangten Branchenvertreter vor dem ""Ende der britischen Erdbeere"". Unternehmen arbeiten deshalb daran, auch feinmotorisch schwierige Erntearbeiten zu automatisieren. Fieldwork Robotics, ein Ableger der Universität Plymouth in England, kooperiert etwa mit dem französischen Gemüseverarbeiter Bonduelle, um Pflückroboter auf die Felder zu bringen. Ein kürzlich vorgestellter Prototyp kann schon jetzt innerhalb weniger Sekunden Karfiolköpfe ernten.
25.000 Himbeeren pro Tag

Der Roboter ist mit einem Greifarm und einem Messer ausgestattet, um die Karfiolköpfe sauber abzuschneiden. Mithilfe von 3D-Kameras und anderen Sensoren orientiert sich die Maschine im Raum, ein Machine-Learning-Algorithmus lernt mit jedem gepflückten Karfiol dazu. Feldversuche sollen voraussichtlich 2022 beginnen.

Bereits 2019 hat das Unternehmen den weltweit ersten Himbeerpflückroboter vorgestellt, der eine Himbeere pro 2,8 Sekunden pflücken kann, eine menschliche Arbeitskraft benötigt dazu etwa zwei Sekunden. Der Roboter könne aber bis zu 20 Stunden am Stück arbeiten, auch bei Nacht. So seien 25.000 Himbeeren pro Tag schaffbar. Ein Mensch pflückt in einer Acht-Stunden-Schicht etwa 15.000. Auch auf Tomaten wurde der Roboter bereits erfolgreich losgelassen.

Fieldwork Robotics hat im Jänner bereits knapp 300.000 Pfund von Unterstützern eingesammelt, auch die britische Regierung finanziert das Projekt mit einer halben Million Pfund, eine weitere halbe Million soll von Investoren kommen.

Die Plückroboter werden in Großbritannien jedenfalls dringend benötigt: Bereits in den vergangenen Jahren verrotteten auf britischen Feldern Tonnen von Früchten, da die Erntearbeiter aus Osteuropa ausblieben. Mit der Covid-Pandemie und den damit einhergehenden Reisebeschränkungen hat sich die Lage zusätzlich verschärft.";https://www.derstandard.at/story/2000120213911/roboter-sollen-fehlende-erntehelfer-nach-brexit-ersetzen;Standard;pp
05.02.2020;Entwickler lässt Bot für ihn auf Tinder swipen und flirten;"Der Entwickler Robert Winters verbringt viel Zeit auf Tinder, könnte man meinen – zumindest suggerierte dies sein Profil, führte er doch nach eigenen Angaben zeitweise 200 Gespräche gleichzeitig auf der Plattform. Dabei saß er aber gar nicht selbst stundenlang vor seinem Handy, sondern ließ einen Bot die Arbeit erledigen.

Der Belgier hat ein Programm entwickelt, das sich mithilfe von Machine Learning merkt, welche Personen er attraktiv findet und welche nicht. Trainiert wurde das System mit Aufnahmen aus der Google-Bildersuche. Dabei hat er auf den Code des Entwicklers Jeffrey Li zurückgegriffen, einem Datenwissenschafter, der auf Github erstmals ein derartiges Programm angeboten hat.
Bot chattet auch

Lis Software nutzt deswegen die Google-Suche, weil sie sonst Profil für Profil mit den Fotos der anderen Tinder-Nutzer hätte gefüttert werden müssen. Dort allerdings stufte Li wesentlich mehr Personen als unattraktiv denn attraktiv ein. So hätte das System nicht nur viel weniger Daten, um Kriterien für Attraktivität zu errechnen, sondern das Training würde auch wesentlich mehr Zeit in Anspruch nehmen.

Winters führte diese Idee weiter. Seine Version der Software beurteilt nicht nur die Attraktivität der Personen auf den Profilfotos, sondern übernimmt bei einem gegenseitigen Treffer auch das Chatten mit dem Gegenüber. Und während das für den Belgier sehr gut funktioniert haben dürfte, gefiel es Tinder selbst eher weniger – das Unternehmen hat ihn kurzerhand gesperrt.
Kritik in den letzten Wochen

Dating-Apps wie Tinder und Co sind in den letzten Wochen zunehmend in Kritik geraten: So würden sie laut einer Untersuchung des norwegischen Verbraucherschutzverbands NCC gegen die seit Mai 2018 geltende EU-Datenschutzgrundverordnung (DSGVO) verstoßen. Insgesamt hat NCC neben Tinder weitere beliebte Apps wie die Grindr, die Make-up-App Perfect 365 oder die Menstruations-App Mydays untersucht.

Es wurde jeweils erhoben, welche Daten an welche Drittanbieter weitergegeben werden. Das Ergebnis: Die zehn Apps lieferten sensible Daten an 135 Unternehmen, unter anderem die IP-Adresse und GPS-Standorte, Daten über sexuelle Ausrichtung, politische Einstellung und eingenommene Medikamente. Empfänger sind zum Teil bekannte Technologieriesen – so bekommt Googles Marketingfirma Doubleclick von acht der zehn untersuchten Apps Daten übermittelt, Facebook sogar von neun.";https://www.derstandard.at/story/2000114161741/entwickler-laesst-bot-fuer-ihn-auf-tinder-swipen-und-flirten;Standard;red
09.09.2019;Wer entscheidet in einer Welt autonomer Systeme?;"Bei Künstlicher Intelligenz handelt es sich weniger um menschliche Intelligenz – also etwa die Fähigkeit abstrakt, vernünftig und kreativ zu denken und daraus zweckvolles Handeln abzuleiten – als vielmehr um ""Prediction Machines"" – also Vorhersagemaschinen – jedenfalls im Kontext von Entscheidungen. Das wahrscheinlichste Szenario wird zur Vorhersage und die Vorhersage zur Entscheidungsgrundlage. Doch wie wird das wahrscheinlichste Szenario errechnet, wer kann sich dieser Entscheidungsgrundlage bedienen und wessen Interessen werden damit abgebildet, welche Ziele verfolgt? Werden wir alle unseren individuellen Algorithmus haben, der mit unseren Präferenzen gefüttert wird, sodass das für uns beste Produkt, der für uns passendste Job, das für unsere Arbeit angemessenste Gehalt gefunden wird? Oder werden große Konzerne unsere Daten für banale Gewinnmaximierung und Staaten, unter dem Deckmantel der Sicherheit oder Verwaltungsoptimierung, zur Kontrolle ihrer Bürger einsetzen? Wer kann sich der Daten bedienen und welche Auswirkungen haben automatisierte Entscheidungen?

Ein Algorithmus ist eine Folge mathematischer Anweisungen und damit immer auch eine mathematische Abbildung persönlicher Weltanschauung, denn welche Daten wie gewichtet werden, um ein Problem zu lösen, sind keine objektiv zu beantwortenden Fragen. Sinn und Zweck eines Algorithmus sind der Umgang und die Strukturierung von Daten zur Lösungsfindung, zum Beispiel ""Wenn-Dann""- bzw. ""Wahr-Falsch""-Gleichungen. Im Fall von ""Machine Learning"" entwickelt das System weniger auf Basis vorgegebener Regeln als vielmehr auf Basis vorliegender strukturierter und unstrukturierter Daten selbst einen Ansatz zur Lösungsfindung. Dabei ist es im Nachhinein oft nicht mehr nachvollziehbar, wie das autonome System zu einem Ergebnis gekommen ist. Hinzu kommt, dass die Funktionsweise autonomer Systeme häufig hinter Schutzrechten wie dem Betriebsgeheimnis versteckt wird. Auch die historischen Daten, auf Basis derer das System trainiert wurde, können vorurteilsbehaftet oder fehlerhaft sein. Ein Algorithmus für Jobempfehlungen sollte also nicht auf Basis historischer Daten oder genereller Präferenzen davon absehen, etwa einer Frau eine KFZ-Mechanikerinnenstelle anzubieten – nur, weil es zu wenig Evidenz für die Wahrscheinlichkeit dieser Handlungsvariante gibt.
Digitalisierung und Machtkonzentration

Entscheidungsfindung wird zentralisiert – von der Bankbeamtin zur Kreditsoftware, von lokal zu global – und damit auch zunehmend monopolisiert. Immer mehr Macht in immer weniger Händen. Damit stellen sich Machtfragen: Wer kann Lebens-, Wirtschafts- und Arbeitsbedingungen gestalten, was lernen Maschinen und wem nützen welche Algorithmen? Darin begründet liegt auch eine weitere Herausforderung. Je größer die Datenmenge, desto größer die Wahrscheinlichkeit, dass das System umfassend lernen kann und eben auch KFZ-Mechanikerinnen kennt. Je größer die Anwendergruppe, desto umfangreicher das Feedback für das System und damit das Verbesserungspotential. Auch in dem Zusammenschluss staatlicher und privater Anwendungen liegt ein demokratiegefährdendes Potential. Dieser Zusammenschluss passiert häufig im Bereich des Militärs und im Namen der Sicherheit. Insbesondere unter dem Aspekt der ""Sicherheit"" gibt es zunehmende Bestrebungen, auch auf staatlicher Ebene Daten zu sammeln und Methoden der Vorhersage anzuwenden. Nicht selten kommt es dabei auch zu einem Teilen von Informationen zwischen privaten und staatlichen Einrichtungen, auf Basis dessen Entscheidungen getroffen werden, ohne transparente, demokratische und für die Allgemeinheit nachvollziehbarer Prozesse – weswegen auch von einer sich entwickelnden ""Black Box Society"" die Rede ist. Mit Hinblick auf die nötige Regulierung der Datenauswertungspraktiken ist es jedoch höchst problematisch, wenn jene die reguliert werden sollen und jene die regulieren sollen zu eng, miteinander verflochten sind.

Ein Wettrennen um die KI-Vorherrschaft hat längst begonnen und wird insbesondere zwischen China und den USA aber auch Russland und der EU ausgetragen. Dabei resultiert die Gefahr weniger aus einem Verlieren des Rennens, als vielmehr aus der schnellen Verbreitung halbgarer oder gar potentiell gefährlicher Lösungen. Auch in der Datenerhebung fallen zunehmend alle Grenzen. Die Logik dahinter: Jene KI-Systeme, die auf Basis einer größeren Datenmenge lernen können, haben die Nase weiterhin vorne. Halbgare Lösungen auf Basis fraglicher Datenerhebungen werden schnell skaliert. Dadurch birgt eine sehr vielversprechende Technologie eine Gefahr zum Nachteil für sehr viele Menschen und eine gerechte gesellschaftliche Entwicklung schlechthin zu werden. Je weniger demokratisch die Prozesse, desto schwächer die Rechte Einzelner sowie jene marginalisierter Gruppen und umso gefährlicher die vorstellbaren Szenarien.
Wozu werden KI-Entscheidungen herangezogen?

Der datengestützten Entscheidungsfindung liegt also eine gewisse Zentralisierungstendenz zugrunde. Von Personen im Kundenkontakt zu ""Prediction Machines"" des Managements. Dies funktioniert analog zu dem Kreditrahmen, der etwa nicht mehr von den Bankbetreuern festgelegt wird, sondern von einer Software. Die Beispiele sind vielfältig: HR-Software von Firmen wie ""Recorded Future"", teilweise finanziert von Google und der CIA oder Workday, an der Amazons CEO Jeff Bezos beteiligt ist, erheben umfangreiche Daten ihrer Mitarbeiter. Daten rund um den Sprachgebrauch beim Formulieren von E-Mails, den Umgang mit KollegInnen, die Termindichte und Vielfalt genauso wie die persönlichen Netzwerke führen dort direkt zu Vorschlägen von Bonuszahlungen, Beförderungen oder Kündigungen.

Weniger problematische, die Lebensqualität positiv beeinflussende KI-Beispiele werden immer wieder im Gesundheitsbereich angeführt, darunter auch eine besonders vielversprechende Anwendungsmethode – jene der Arbeitsteilung zwischen Mensch und Maschine. So sind die quantifizierbaren Stärken von Mensch und Maschine zum Beispiel in der Krebsdiagnose komplementär. Die menschliche Pathologin ist fehlerfreier, wenn sie Krebs feststellt (weniger ""false negatives""), das KI-System, wenn es keinen Krebs feststellt (weniger ""false positives""). Was im Fall der Brustkrebsdiagnosen eine Reduktion der menschlichen Fehler um 85 Prozent von 3,4 auf 0,5 Prozent bedeuten kann. Auch die Auslagerung sich wiederholender Entscheidungen an autonome Systeme wirkt vielversprechend, der Mensch greift nur noch im Falle ungewöhnlicher Ausnahmen ein.
Diskriminierung und Ungleichheiten werden automatisiert

Ein Erfolg oder Misserfolg mathematischer Systeme liegt also nicht zuletzt auch an der Definition der Mensch-Maschine-Schnittstellen. Gibt es diese nicht oder kaum, kann sich das System schnell verselbständigen. Mathematikerin Cathy O’Neil schreibt im Falle opaker (undurchsichtiger), unfairer und breit eingesetzter Systeme sogar von ""mathematischen Vernichtungswaffen"" (""Weapons of Math Destruction""). Wenn Menschen mit einer bestimmten Wohnadresse vom System für strafrückfälliger eingeschätzt werden als andere, oder die Missbrauchswahrscheinlichkeit neugeborener Babys eingeschätzt wird, oder Lehrer aufgrund intransparenter Kriterien zur Kündigung vorgeschlagen werden, oder die falschen sozialen Kontakte einen Kredit verhindern – Chancen werden überall dort mathematisch verhindert. Insbesondere Mitglieder ärmerer Gesellschaftsschichten sind von derlei automatisierten Ungerechtigkeiten betroffen.

Politikwissenschafterin Virginia Eubanks schreibt in ihrem Buch ""Automating Inequality"" eindrucksvoll wie automatisierte Entscheidungsfindungen systematisch zu Benachteiligung armer Bevölkerungsgruppen führen können. Die Verweigerung öffentlicher Dienstleistungen auf Basis falscher oder mit diskriminierenden Annahmen behafteter Einstufungen des Systems, können dabei weitreichende Konsequenzen haben. Eine intransparente Wahrscheinlichkeitsrechnung mit schlechten Datensätzen, mit dem Ziel der effizienteren Ressourcenzuteilung, kann auf diese Weise Menschen völlig abschneiden, ohne Rücksicht auf erklärende Besonderheiten und häufig ohne Möglichkeit zur Richtigstellung. Mathematische Unzulänglichkeiten, diskriminierende oder schlicht fehlerhafte Datensätze und mangelnde Verantwortung können ohne jede menschliche Interaktion so zu einem Verlust von Chancen oder gar zu lebensbedrohlichen Situationen führen. So können autonome Entscheidungssysteme Ungleichheit nicht nur fort- und festschreiben, sondern sie im wahrsten Sinne des Wortes automatisieren.
Licht ins Dunkel

Transparenz zu der grundlegenden Funktionsweise des technischen Systems, die das eigene Leben betreffen, sollte gewährleistet sein. Welche Daten werden verwendet, woher kommen diese, welches Ziel verfolgt die Anwendung und wie kann gegen falsche Entscheidungen vorgegangen werden? Der Versuchung, diese Anwendungen als ""mathematischen Hokuspokus"" zu mystifizieren, sollte widerstanden werden – dies könnte auch verpflichtend eingefordert werden. Es geht neben dem Hinweis, wann welche Daten von mir erfasst werden, in Zukunft auch stärker um die Entscheidungshoheit jener Anwendungen, die mit diesen Daten ausgeführt werden.

Darüber hinaus gilt, dass Technologie kein Ersatz für Gerechtigkeit ist. Der gerechte Zugang zu grundlegenden Ressourcen und damit auch die Frage der Verteilungsgerechtigkeit kann nicht technologisch werden, sondern bedarf grundlegender gesellschaftspolitischer Diskurse sowie Entscheidungs- und Aushandlungsprozesse. So gilt es zum Beispiel Armut durch verbesserten Zugang zu Ressourcen und Chancen zu bekämpfen und nicht mittels autonomer Systeme effizienter zu organisieren. Technik kann diese Vorhaben in der Umsetzung unterstützen, kann diese jedoch nicht ersetzen oder ein Fehlen dieser kompensieren.";https://www.derstandard.at/story/2000108183643/wer-entscheidet-in-einer-welt-autonomer-systeme;Standard;Fridolin Herkommner
14.10.2020;Der Fadenwurm als Vorbild: Simples neuronales Netz kann Spur halten;"Der Fadenwurm ""C. elegans"" kann mit seinem nur 302 Neuronen zählenden Nervensystem recht komplexe Aufgaben meistern. Nach dem Vorbild des kleinen Wurms haben nun österreichische Forscher künstliche neuronale Netze verbessert. Mit einem einfacheren, kleineren Netz bestehend aus nur 19 Zellen lassen sich Aufgaben wie das Spurhalten eines autonomen Fahrzeugs effizienter und zuverlässiger lösen lassen als bisher, berichten sie im Fachjournal ""Nature Machine Intelligence"".

Lern- und anpassungsfähige Programme, die man als ""Künstliche Intelligenz"" (KI) bezeichnet, können durch Analyse großer Datenmengen und Erfahrung lernen. Das ist etwa bei der Sprachsteuerung am Handy, Suchmaschinen, Übersetzungsprogrammen oder selbstfahrenden Autos bereits Realität. Angetrieben wird die Entwicklung vor allem durch große vorhandene Datenmengen und immer höhere Rechenleistung.
MITCSAIL
Neues Modell

Das Forscherteam der Technischen Universität (TU) Wien, des Institute of Science and Technology (IST) Austria in Klosterneuburg (NÖ) und des Massachusetts Institute of Technology (MIT) hat sich in seiner Arbeit überlegt, wie man die Komplexität künstlicher neuronaler Netze reduzieren kann. Orientiert haben sie sich dabei an der Natur – konkret am Fadenwurm. Der zeigt mit einer verblüffend kleinen Zahl von Nervenzellen interessante Verhaltensmuster. ""Das liegt an der effizienten und harmonischen Art, wie sein Nervensystem Information verarbeitet"", erklärte Radu Grosu, Leiter der Forschungsgruppe ""Cyber-Physical Systems"" an der TU Wien, in einer Aussendung.

""Inspiriert von der Natur haben wir neue mathematische Modelle für Neuronen und Synapsen entwickelt"", so der Informatiker Thomas Henzinger, Präsident des IST Austria. Das Netz des neuen KI-Modells ist viel einfacher, weil nicht jede Zelle mit jeder anderen verbunden wurde. Zudem gehorcht die Verarbeitung der Signale innerhalb der einzelnen Zellen anderen mathematischen Regeln als bei bisherigen Deep Learning Modellen.
Foto: TU Wien
Stark im Spurhalten

Nach Ansicht des Forscherteams hat das System entscheidende Vorteile gegenüber bisherigen Modellen: Es kommt viel besser mit gestörten Eingaben zurecht und aufgrund seiner Einfachheit kann seine Funktionsweise im Detail erklärt werden.

Getestet haben die Forscher ihr Modell beim Spurhalten eines autonomen Fahrzeugs. Für solche Aufgaben würden heute oft Deep Learning-Modelle mit Millionen an Parametern, verwendet, ""unser System kommt mit 75.000 trainierbaren Parametern aus"", so Mathias Lechner, PhD-Student am IST Austria.

Das neue System besteht aus zwei Teilen: Ein Netzwerk verarbeitet den Kamera-Input und entscheidet, welche Teile des Bildes wichtig sind. Es gibt dann Signale an das Kontrollsystem des neuronalen Netzwerks weiter, welches das Fahrzeug lenkt. Das Kontrollsystem besteht dabei nur aus 19 Zellen, das sei ""um drei Größenordnungen kleiner als es mit bisherigen State-of-the-art-Modellen möglich wäre"", so Lechner.
Dem Bildrauschen widerstehen

Um zu testen, wie robust das neue System im Vergleich zu bisherigen Deep Learning-Modellen ist, haben die Forscher die Bilder künstlich verschlechtert. Während andere Netzwerke mit einem solchen Bildrauschen nur schwer zurechtkommen, ""ist unser System sehr widerstandsfähig gegenüber Artefakten beim Input. Diese Eigenschaft ist eine direkte Folge des neuartigen Modells und seiner Architektur"", erklärte Lechner.

Das neue Modell hat noch weitere Vorteile: Bei jeder einzelnen Entscheidung des Systems lässt sich die Rolle jeder einzelnen Zelle identifizieren. So lässt sich die Funktion der Zellen verstehen und ihr Verhalten erklären, während größere Deep Learning-Modelle dagegen eine ""Black Box"" darstellen. Zudem ermöglicht die Methode, die Dauer des Trainings zu reduzieren, und schafft die Möglichkeit, Künstliche Intelligenz in relativ einfachen Systemen zu implementieren, betonen die Forscher.";https://www.derstandard.at/story/2000120902792/der-fadenwurm-als-vorbild-simples-neuronales-netz-kann-spur-halten;Standard;APA
21.07.2020;GPS-Tracker soll Übergewicht von Haustieren vorbeugen;"Allein der Gedanke treibt den meisten Tierbesitzern den kalten Angstschweiß auf die Stirn: Der Hund oder die Katze ist verschwunden. Was, wenn etwas passiert ist? Was, wenn sich das Tierchen verlaufen hat?

Das Paschinger Unternehmen Tractive hat aus dieser Problematik eine Geschäftsidee entwickelt. Die Oberösterreicher bauen GPS-Tracker für Haustiere, die es Besitzern ermöglichen, via App am Smartphone den eigenen Vierbeiner zu lokalisieren. ""Ich habe mit einem Freund stundenlang seinen entlaufenen Hund gesucht"", erzählt Tractive-Gründer Michael Hurnaus im Gespräch mit dem STANDARD.

""So entstand die Idee, ich konnte es mir nicht erklären, warum es so etwas nicht gab."" Das war im Jahr 2012. Gemeinsam mit Florian Gschwandtner, dem mit ihm befreundeten Runtastic-Gründer, stampfte er daraufhin das Konzept aus dem Boden. Hurnaus kündigte seinen Job bei Amazon und kehrte von Kalifornien ins Valley Linz-Land zurück. Im Jänner kam auch Gschwandtner wieder ins Team und übernahm das Wachstumsmanagement.
Tractive will anhand der gesammelten Daten herausfinden, wie sich das ""typische"" Tier verhält. Bei Unregelmäßigkeiten soll die App Alarm schlagen.
Foto: Reuters/DAVID W CERNY
275.000 zahlende Kunden

Knappe acht Jahre später zählt der Betrieb eigenen Angaben zufolge 275.000 zahlende Kunden in 150 Ländern und sieht sich damit als Weltmarktführer. Der Tracker, der am Halsband des Tieres befestigt wird, funktioniert wie ein Handy, hat eine SIM-Karte und GSM- sowie GPS-Modul eingebaut. 49,99 Euro kostet das Gerät, 4,99 Euro das notwendige Monatsabo (bei einjähriger Bindung), damit die Ortung funktioniert.

Hurnaus kennt die Zweifel, ob es denn sein Produkt braucht: ""Den Hund, der nicht weglaufen kann, gibt es nicht. Einmal erschreckt, und er ist weg."" Er zieht deshalb eine Parallele zum Sicherheitsgurt: ""Man hofft, ihn nie zu brauchen, aber ist froh, wenn er da ist.""
Viele Beagles tragen bereits den Halsbandzusatz, um ihren Besitzern nicht mehr so leicht entwischen zu können.
Foto: Tractive
Fitnesstracker

Was Katzen den lieben langen Tag so machen, wissen viele Besitzer nicht. Deshalb brachte Tractive vor rund einem Jahr einen Katzentracker auf den Markt. ""Kunden sehen, wo sich ihre Katzen herumtreiben. Es geht dabei weniger um den Verlust als um das Interesse am Leben des Tiers.""

Gleichzeitig mit dem Schritt nach Amerika stellte Tractive eine neue Funktion des Geräts vor: der Fitnesstracker. Die App analysiert, wie viel sich das Haustier bewegt. Zwei Ziele verfolgen die Paschinger. Einerseits sollen Besitzer aufmerksam werden, wenn Tiere übergewichtig sind und mehr Bewegung brauchen. Tractive zufolge haben 40 Prozent der mitteleuropäischen Hunde und Katzen zu viel auf den Rippen.
Unregelmäßigkeiten erkennen

Andererseits möchte man ein vordiagnostisches Tool zur Verfügung stellen: ""Wir sammeln Daten, um rauszufinden, wie viel sich zum Beispiel ein ‚typischer‘ Labrador bewegt. Via Machine-Learning soll die App Bescheid geben, wenn sich Unregelmäßigkeiten abzeichnen"", so Hurnaus. Die Daten würden anonymisiert und nicht weitergegeben, man wolle den Vorsprung nicht einbüßen.
Viele Katzenbesitzer haben keine Ahnung, wo sich der oft eigenwillige Vierbeiner so herumtreibt.
Foto: Tractive

Den ""Start-up-Schuhen"" ist Tractive mit 94 Beschäftigten aus 27 Nationen zwar schon entwachsen, wie in der Branche üblich gibt es über den Umsatz jedoch keine Angaben. Auf Basis des Abo-Modells und der Kundenzahl lässt sich jedoch auf einen monatlichen Umsatz von rund 1,5 Millionen Euro schließen. Vor rund zwei Wochen expandierte Tractive in die USA und will den Markt erschließen. Aktuell kommen fast alle Einnahmen aus Europa. Am ""beliebtesten"" ist der Tracker aktuell bei Beagles.";https://www.derstandard.at/story/2000118871295/gps-tracker-soll-uebergewicht-von-haustieren-vorbeugen;Standard;Andreas Danzer
17.01.2020;Was soll den Erschaffern einer künftigen KI beigebracht werden?;"Künstliche Intelligenz (KI) ist aus unseren Lebens- und Berufswelten nicht mehr wegzudenken. Damit verbindet sich viel Emotion – von der gehypten Lösung für ""alle"" Probleme über die differenzierte Betrachtung der Chancen und Risiken bis hin zu mal mehr, mal weniger seriösen Warnungen, die Mehrheit menschlicher Arbeitsplätze sei durch intelligente Maschinen gefährdet.

Hier beginnt oft schon das Missverständnis: Die KI gibt es nicht. Wer über KI spricht, muss präzisieren, wovon die Rede ist: von datengetriebenen Ansätzen wie Machine Learning? Oder von wenig datenabhängigen Verfahren, die aufgrund der Rückmeldungen ihrer Sensoren lernen? Von autonom navigierenden Fahrzeugen? Von Roboterarmen, die mittels Bilderkennung lernen, Objekte oder Menschen zu erkennen, und entsprechend eigenständig agieren, zum Beispiel als kollaborative Roboter? Von der Mustersuche in den Betriebsdaten einer Fabrik, die helfen soll, zu prognostizieren, ob eine Maschine demnächst ausfallen wird? Von der Dialogfähigkeit eines Pflege- oder Spielroboters, der Empathie emulieren kann? Die Liste der technischen Verfahren und ihrer Anwendungen ist lang, und manche sind bereits gängige Praxis. Andere liefern im Entwicklungsbetrieb gute Ergebnisse, sind aber erst eingeschränkt zertifiziert, da ihr Verhalten und ihre Sicherheit auf Wahrscheinlichkeiten beruhen. Hinzu kommen Probleme mit der Transparenz der Entscheidungsfindung: Sobald eine Maschine autonom handelt, muss zweifelsfrei nachvollziehbar sein, warum sie eine Handlung gesetzt oder nicht gesetzt hat (sogenannte ""explainable"" AI).

Gerade bei der Anwendung neuronaler Netze – eine der derzeit vielgenannten Technologien – hat sich gezeigt, dass maschinelles Lernen dieser Überprüfung unbedingt bedarf: Nur, weil eine KI Objekte oder Menschen mit attraktiver Trefferquote unterschiedlichen Kategorien zuordnet, heißt das nicht, dass dies auf den richtigen Kriterien beruht. Wenn zum Beispiel eine KI im Internet drei wesentliche Lebensformen auszumachen glaubt – Frauen, Männer und Katzen – muss das nicht bedeuten, dass dies dem realen Zustand entspricht. Noch weniger muss es den gewünschten Zustand repräsentieren, der vielleicht andere selten vertretene Lebensformen zu stärken sucht. Und wer weiß darüber hinaus, ob die besagte KI anhand tatsächlich weiblicher, männlicher oder katzentypischer Eigenschaften unterschieden hat, oder auf Basis anderer Merkmale, die ohne kausalen Zusammenhang so gut mit unserer Unterscheidung von Frauen, Männern und Katzen korreliert, dass wir geneigt sind, der KI zu vertrauen, sie habe ""gelernt, wesentliche Lebensformen zu unterscheiden""?
Von Robotern belehrt werden?
Foto: APA/dpa/Axel Heimken
Maximen für die Aus- und Weiterbildung zum Thema KI

Soweit ein (vereinfachter) Auszug der aktuellen Diskussion um KI-Systeme. Nicht nur für Gesellschaft, Politik und Wirtschaft stellt sich die Frage, wie der Umgang mit den neuen Systemen zu erlernen ist, sondern, spezieller noch für Schulen, Hochschulen öffentliche und gewerbliche Weiterbildungsanbieter oder Schulungsabteilungen im Unternehmen. Zunächst würde man dem Curriculum oder Weiterbildungsprogramm generelle Maximen für den Umgang mit KI zugrunde legen, wie sie zum Beispiel der österreichische Rat für Robotik in seinem White Paper umrissen hat. Dies betrifft ethische und gesellschaftliche Fragen, berührt aber auch handfeste technische Themen wie zum Beispiel Maschinen- und Datensicherheit. Ein weiteres Prinzip erschließt sich aus der Grundordnung der sozialen Marktwirtschaft: Im Bereich der Produktion von Gütern und Dienstleistungen gilt es, wettbewerbsfähig zu bleiben – zu Bedingungen für die arbeitenden Menschen, die unserem hohen Wertestandard entsprechen. Geht es im zweiten Schritt um die Verteilung des erwirtschafteten Wohlstands (Subventionen, Steuern), wird man stärkeres Gewicht auf soziale Kriterien legen.

Der gedankliche Sprung zur Frage dieses Beitrags, was Aus- und Weiterbildenden unterrichten sollen, scheint zunächst weit: Sollte nicht das Ingenieurwesen vordringlich über Mathematik, Statistik und Programmierkenntnisse sprechen? Über Sensoren und die intelligente Verarbeitung der Daten die sie liefern? Sollten nicht die Arbeitssoziologie vor allem über arbeitsorganisatorische Fragen diskutieren, und die Rechtswissenschaften Fragen wie die Zurechenbarkeit der Folgen des Handelns autonomer Roboter auf den Speisezettel der Lernenden setzen? Beides wird wichtig sein: Fokussiert ein Bildungsprogramm rein auf Lernziele des eigenen Faches, degradiert das die Absolventinnen und Absolventen zu blassen Fachidioten, die (hoffentlich) ihre (anderswo erworbene) Sensibilität für die oben angerissenen Fragen in die Anwendung dieses Wissens einbringen werden. Beschränkt man sich dagegen auf ethische Imperative und Willenserklärungen, ohne konkrete Techniken in angemessener Tiefe zu unterrichten, wird das Handeln der Absolventinnen und Absolventen kaum über Absichtserklärungen hinauskommen.
Herausforderungen für Bildungsprogramme

Für Aus- und Weiterbildende ergeben sich diverse Herausforderungen: Zunächst müssen Lernziele und -inhalte das jeweilige Fachwissen in angemessener Dosis mit dem Wissen darüber verbinden, was übergreifend im Umgang mit KI gewollt und nicht gewollt sein kann. Zum zweiten ist zu bedenken, dass angesichts der raschen technologischen Veränderung heutige Bildungsprogramme auf Basis heute verfügbarer Technologien Menschen unterrichten, die zukünftig Kompetenzen benötigen, die wir heute noch nicht kennen. Das erweitert den Aus- und Weiterbildungsbedarf um Themen im Zusammenhang mit Technologieentwicklung, -assimilation und -folgenabschätzung.

Wie in vielen anderen Bereichen der Digitalisierung, wird man sich iterativ nach vorne arbeiten müssen. Zum Beispiel betreibt die Fachhochschule Technikum Wien eine Digitalen Fabrik als Lernort für Engineeringstudiengänge und Unternehmen. Mit hochmodernem industriellen Equipment sind verschiedenste Konzepte der intelligenten Produktion realisiert. Gemäß dem Ansatz der flexiblen Fertigung erledigen mobile Roboter Transportaufgaben zur Herstellung individualisierter Produkte. KI unterstützt die Roboter bei der Navigation und Lokalisierung. Mobile Roboter agieren als Schwarm und besitzen Manipulatoren zur Interaktion mit den Bearbeitungsstationen. Darüber stehen den Lernenden zahlreiche Use Cases zur Verfügung, die zum Beispiel die Integration vielfältiger Sensoren und Aktuatoren oder die Möglichkeiten der Bildverarbeitung zur Realisierung automatisierter robotergestützter Applikationen mit nicht konstanten und dynamischen Eingangsgrößen praktisch demonstrieren.
KI-Anwendung in der Digitalen Fabrik der FH Technikum Wien.
Foto: FH Technikum Wien

Wenn hier von KI (präziser: Von maschinellem Lernen und statistischem Schlussfolgern) die Rede ist, betrifft das vor allem nicht klar definierbare Aufgaben, wie zum Beispiel das Greifen von Objekten in unbekannter Lage oder die Reaktion auf unerwartete Stöße. Klassische Methoden wie tiefe faltende neuronale Netze zur Objektdetektion beziehungsweise zur Lagebestimmung für Greifprozesse werden um moderne parameterfreie Methoden zur Navigation erweitert. Dabei erlernt der mobile Roboter basierend auf historischen Daten selbstständig die Fahrzeugdynamik, was sowohl in der Lokalisierung als auch in der Pfadplanung einsetzbar ist. Dank immenser Unterstützung der Industrie stehen reale industrielle Systeme zur Verfügung. Die technischen Curricula insbesondere der Studiengänge Mechatronik/Robotik, Maschinenbau, internationales Wirtschaftsingenieurwesen und Innovations- und Technologiemanagement können modernste Technologie unterrichten, die noch dazu durch die intensive Forschungstätigkeit der Fakultät am Puls der Zeit bleibt, zum Beispiel wenn es darum geht, erlernte Fahrzeugmodelle für die Navigation in Echtzeitanwendungen zugänglich zu machen.

Technologisch bleiben also kaum Wünsche offen. Die dritte Herausforderung für die Gestalter der Studiengänge, Laborpraktika und industriellen Lehrgänge liegt darin, die übergreifende und interdisziplinäre Klammer bewusst zu machen, die die Technologie betreffend die Auswirkungen ihrer Anwendung reflektiert, in der Fabrik zum Beispiel die Gestaltung von Produktionsarbeitsplätzen in der intelligenten Umgebung, von der eigeninitiativ autonome Signale an den Menschen ausgehen. In Fächern wie Innovations- und Technologiemanagement fällt das leicht: Die Lernenden sind mit dem Motiv angetreten, eben diese Fragen zu stellen. Disziplinbedingt liegen ihre Schwierigkeiten eher darin, die KI in ihrer technischen Komplexität zu erfassen. Genau umgekehrt erleben es rein technische Studiengänge wie zum Beispiel Mechatronik oder Maschinenbau: Das Motiv, diese Fächer zu studieren ist in der Regel die Faszination an der Technik; ""philosophische Abschweifungen"" werden oft als irrrelevante Pflichtübung abgetan. Bei gleichem Lernziel betreffend den förderlichen Umgang mit KI erfordert allein diese Unterschiedlichkeit der Lernenden andere didaktische Zugänge.
Ein vorläufiges Fazit – Aufruf zum interdisziplinären Diskurs

Diese hier exemplarisch für zwei Zielgruppen angestellte Überlegung lässt sich für andere Adressaten analog fortsetzen. Weitere Unterscheidungen kommen hinzu. Soll zum Beispiel jemand die Technologie ""nur"" verwenden können, oder ist es Ziel, selbst KI-Systeme entwickeln zu können? Wie erschließt man dem Informatiker die physikalische Komplexität mechanischer Systeme, und umgekehrt dem Maschinenbauer die Risiken einer schlecht reflektierten Algorithmenanwendung? Hier wird der interdisziplinäre Diskurs zur Schlüsselqualifikation. Den Bildungsgestaltern gehen die Fragen nicht aus. Uns bleibt, eine letzte Frage zu stellen: Wer sind sie, die Gestalter dieser Lernprogramme und Curricula? Kann man bei technisch hochspezialisierten Aus- und Weiterbildnern voraussetzen, dass das Bewusstsein betreffend nicht-technologische Fragen schon hinreichend weit entwickelt ist? Woher bekommen umgekehrt die nicht-technischen AusbildnerInnen die technische Überblicksperspektive in der für ihre Zwecke angemessen, und sachlich korrekt reduzierten Komplexität? Anders gesagt: Who is teaching the teachers? An diesem wichtigen Multiplikatorpunkt bleibt viel zu tun, und auch dazu ist der interdisziplinäre Dialog unverzichtbar!";https://www.derstandard.at/story/2000113111302/was-soll-den-erschaffern-einer-kuenftigen-ki-beigebracht-werden;Standard;Corinna Engelhardt-Nowitzki
05.03.2020;Python erstmals unter den drei populärsten Programmiersprachen;"

Die aktuellen Top 20 der Programmiersprachen:

1 JavaScript
2 Python
2 Java
4 PHP
5 C#
6 C++
7 Ruby
7 CSS [keine Programmiersprache]
9 TypeScript
9 C
11 Swift
12 Objective-C
13 Scala
13 R
15 Go
15 Shell
17 PowerShell
18 Perl
19 Kotlin
20 Haskell
Foto: APA/AFP/KIRILL KUDRYAVTSEV

Was ist derzeit auf Github und Stackoverflow die beliebteste Programmiersprache? Die Analysefirma Redmonk hat die Frage mit ihrem aktuellen Ranking beantwortet. Und das zeigt, dass sich in den vergangenen Jahren einiges getan hat und die Dominanz von JavaScript und Java ins Wackeln geraten könnte.
Siegszug oder Ausreißer?

Allmählich nach oben kletterte in den Rankings der vergangenen Jahre Python, das nun erstmals hinter Javascript den zweiten Platz gemeinsam mit Java eingenommen hat. Redmonk erklärt das unter vor allem mit der Vielseitigkeit, guten Verfügbarkeit und einfachen Anwendung der Programmiersprache. Fraglich sei nur, ob es sich im aktuellen Ranking um einen Ausreißer nach oben handelt, oder sich Python längerfristig einen Platz unter den drei populärsten Programmiersprachen sichern kann.

Ebenfalls bemerkenswert sind die Entwicklungen von C++, dessen Nutzung zurückgegangen ist, sowie der allmähliche Aufstieg von R. R ist wie Python unter anderem bei Wissenschaftlern beliebt, die beiden Programmiersprachen werden hier etwa für Machine Learning bzw. statistische Anwendungen eingesetzt. Auch Kotlin, Groovy, Dart und Typescript konnten zulegen.";https://www.derstandard.at/story/2000115365427/python-erstmals-unter-drei-populaersten-programmiersprachen;Standard;
10.12.2019;Wissen, wann das Hochwasser droht;"Wetterextreme werden mit dem Klimawandel häufiger. Umso wichtiger wird ihre möglichst exakte Vorhersage. In Europa oder Nordamerika, wo lange Datenzeitreihen aus vergleichsweise dichten Messnetzen in die Modelle einfließen, ist man dafür gut gewappnet. In anderen Weltgegenden, wo etwa nicht einmal aktuelle Pegelstände vorliegen, gestalten sich gute Prognosen dagegen schwieriger.

Google plant einen Service, der trotz dieses Mankos gute Hochwasserprognosen aus aller Welt zugänglich machen will. Ein Schlüssel dazu ist künstliche Intelligenz (KI). Lernende Algorithmen sollen trainiert werden, Hochwassergefahr anhand aktueller meteorologischer Prognosen zu erkennen. Die Grundlagenforschung dahinter erfolgt nicht nur im Silicon Valley, sondern zumindest zum Teil auch am Institut für Machine Learning der Johannes Kepler Universität (JKU).

In der Wiener Außenstelle der Linzer Universität sitzen Frederik Kratzert und Daniel Klotz vor ihren Bildschirmen. Die beiden Doktoranden entwickeln gemeinsam mit den JKU-Informatikern Günter Klambauer und Sepp Hochreiter in einer von Google finanzierten Forschungsarbeit einen Algorithmus, der die künftige Abflussmenge von Flüssen vorhersagen soll, ohne dabei auf die klassischen hydrologischen Modelle zurückzugreifen. Die Erkenntnisse wurden zuletzt im Fachjournal ""Hydrology and Earth System Sciences"" vorgestellt.
Mangelnde Genauigkeit

Diese bisher gängigen Modelle packen die wichtigsten Komponenten des Wasserabflusses – von der Speicherung als Schnee bis zur Versickerung im Boden – in mathematische Gleichungen. Für Kratzert sind diese Berechnungen zwangsläufig eher einfach gehalten.

""Zum einen sind noch gar nicht alle Prozesse vollständig verstanden. Zum anderen sind die Daten in einer hohen Genauigkeit gar nicht vorhanden"", erklärt der Forscher, der sich vor seinem Doktorat im Bereich Machine-Learning bereits an der Wiener Boku mit Hydrologie beschäftigt hat. ""Diese Methoden funktionieren dort gut, wo es Abflussmessungen gibt, um ein Modell speziell für einen Fluss kalibrieren zu können."" Und das sei selbst in Österreich nicht bei allen Fließgewässern der Fall.

Neuronale Netze als Basis

Die Alternative, an der Kratzert arbeitet, funktioniert nach ganz anderen Prinzipien. Nicht lange Zeitreihen von Messdaten und hydrologische Gleichungen geben hier die Basis, sondern Satellitendaten und neuronale Netze. ""Wir trainieren die künstliche Intelligenz anhand vieler Flüsse zusammen, sodass sie ein ganz allgemeines Verständnis der zugrunde liegenden Zusammenhänge gewinnt"", erläutert Kratzert.

Zu den Trainingsdaten gehören neben meteorologischen Daten wie Niederschlag, Temperatur, Luftfeuchte und Sonneneinstrahlung auch Kennwerte zu Klima, Böden, Vegetation und Topografie einer Region.

Die Werte sind von meteorologischen Diensten beziehbar oder können aus Satellitendaten extrahiert werden und sind also vergleichsweise einfach zugänglich. Das Trainingsziel ist, den Abfluss an bestimmten Punkten im Flussnetz bestimmen zu können, der dann in Pegelstände übersetzt werden kann. Ein Datensatz an Pegelmesswerten ist zwar für das Training der KI weiterhin erforderlich, aber nicht mehr für ihre Anwendung zur Erstellung von Hochwasserprognosen.
Schnee erkennen

Die Forscher konnten auch Einblicke gewinnen, wie das KI-Modell letztendlich aus den Daten Schlüsse zieht. ""Ohne dass ihm das als eigene Information mitgegeben wurde, hat das System aus den zugrunde liegenden Daten ein Konzept von Schnee entwickelt"", gibt Günter Klambauer ein Beispiel.

""Es hat gelernt, dass bei bestimmten Temperaturen Niederschlag als Schnee akkumuliert wird und nicht unmittelbar zum Abfluss in den Flüssen beiträgt. Wir haben eine Stelle, ein Neuron im neuronalen Netz, gefunden, dessen Wert anstieg, wenn Schnee eine Rolle spielt.""

Kratzert verweist darauf, dass ein KI-Modell, das mit Satellitendaten aus Nordamerika trainiert wurde, ""signifikant bessere"" Ergebnisse brachte als das aktuelle nationale Wassermodell der USA. Bis aber ein weltweites hydrologisches Modell auf KI-Basis vorliegt, werde es noch länger dauern. Google hat derzeit ""Flood Forecast""-Pilotprojekte in Indien und Bangladesch laufen.

Einfache Übertragung läuft nicht

Die KI-Komponenten der JKU-Forscher sollen darin Eingang finden. Allerdings: Einfach das bereits trainierte neuronale Netz auf diese Weltregionen zu übertragen, ist nicht möglich. Das Modell könne nicht auf Daten eines anderen Satellitenprodukts angewendet werden.

Damit die tatsächliche Ausbreitung von Hochwasser vorhergesagt werden kann, ist noch ein weiterer Schritt notwendig, der allerdings nicht mehr Aufgabe der JKU-Forscher ist. Im Google-Projekt werden die Abflussdaten mit detaillierten 3D-Landschaftsmodellen kombiniert.

Mit Fluid-Dynamics-Gleichungen kann errechnet werden, wie sich das Wasser in der Topografie verteilt. Heraus kommt schließlich eine – in die Zukunft gerechnete – Überflutungskarte einer Region.";https://www.derstandard.at/story/2000111843674/wissen-wann-das-hochwasser-droht;Standard;Alois Pumhösel
27.10.2018;Des Menschen Angst vor der Maschine;"So viel Pessimismus hätte man von den traditionell alles Negative zur Seite schiebenden US-Amerikanern gar nicht erwartet: Nicht weniger als 75 Prozent fürchten laut einer Gallup-Umfrage aus dem vergangenen Frühjahr, dass die fortschreitende Automatisierung der Arbeitswelt mehr traditionelle Job vernichten wird, als sie neue für höher qualifizierte Arbeitskräfte schafft.

Eine Einschätzung, die Unternehmern wahrscheinlich zu negativ ist, weil sie mit künstlicher Intelligenz (KI) wirtschaftliche Ziele verfolgen. Schneller, mehr und billiger produzieren ist ihr Plan. Sie fordern von Politikern gleichzeitig mehr Maßnahmen zur besseren Qualifizierung, als diese bisher gesetzt haben. Irgendjemand muss ja die Maschinen bedienen. Der Großindustrielle Hannes Androsch urgiert etwa in seiner Funktion als Vorsitzender des Forschungsrats eine zeitgemäße Bildungs- und Innovationsinitiative und einen Ausbau der flächendeckenden Digitalisierung auf hohem Niveau, um die Automatisierung erst möglich zu machen. International gilt der nächste Mobilfunkstandard 5G als Voraussetzung für die intelligente Fabrik. Davon sei man, so Androsch, noch sehr weit entfernt.

Auch Studien, die vor allem die Chancen dieser laufenden industriellen Revolution sehen, drängen auf raschestmögliche Qualifizierungsmaßnahmen. Das IT-Beratungsunternehmen Gartner publizierte vor etwas mehr als einem halben Jahr die Prognose, dass es schon 2020 nicht weniger als 2,3 Millionen neue Jobs geben wird; 1,8 Millionen traditionelle Arbeitsplätze würden allerdings wegfallen.
Kooperation von Mensch und Maschine

Svetlana Sicular, Vizepräsidentin für Forschung bei Gartner, betonte, der besondere Nutzen von künstlicher Intelligenz liege in der Kooperation von Mensch und Maschine. Unternehmer müssten sich darauf konzentrieren und vor allem überlegen, wie sie ihre Mitarbeiter auf diese so notwendige wie neuartige Zusammenarbeit vorbereiten. Die Unternehmensleitungen müssten bei jeder Investition in Automatisierung analysieren, welche Arbeitskräfte dadurch wegfallen könnten.
DER STANDARD

Eine andere Studienauswertung macht das auch deutlich: Laut International Federation of Robotics werden 2020 etwa drei Millionen Roboter vor allem in der Fertigung im Einsatz sein – und hier vor allem in Japan und Deutschland, zwei schon heute führenden Ländern in Sachen Automatisierung. Die Tatsache, dass in beiden Ländern die Arbeitslosigkeit relativ niedrig ist, stimmt Beobachter optimistisch.

Dennoch: Wird die Industrie diese Aufgabe langfristig wahrnehmen? Auch der Computerwissenschafter Sepp Hochreiter von der Johannes-Kepler-Universität Linz ist überzeugt, dass ""ganz viele Jobs wegfallen werden"", vor allem mechanische Arbeiten, die aus einfachen, klar begrenzten Handgriffen bestehen: am Fließband, im Callcenter, am Lenkrad eines Pkws oder Lkws.

Es werde aufgrund der raschen Entwicklung des autonomen Fahrens in relativ naher Zukunft keine Taxifahrer mehr geben, dafür aber KI-Experten, die das Netz der fahrenden Maschinen überwachen. Mechaniker müssten dann wohl auch in der Wartung dieser Fahrzeuge geschult werden, wären also höher qualifiziert als jetzt. Statistische Auswertungen müssten keine Menschen machen, sagt Hochreiter. Erste Diagnosen von Brust- oder Hautkrebs sind von KI aufgrund des raschen Zugriffs auf Daten und Vergleichsmöglichkeiten schneller und präziser als von Ärzten möglich. Ihre Interpretation müssten dann selbstverständlich wieder Menschen übernehmen. Im Journalismus könnten auch Bots auswerten, das wievielte Tor ein Fußballer nun insgesamt geschossen habe. Sportergebnisse und Börsenkurse werden schon jetzt in einigen Redaktionen über Algorithmen eingespielt .

Justus Piater, Informatiker an der Universität Innsbruck, argumentiert ähnlich und stellt die Frage, ob wirklich genug für die Weiterbildung der Arbeitskräfte getan wird, denn die Entwicklungen im Bereich künstliche Intelligenz und Robotik seien rasant und würden in kürzester Zeit zu tiefgreifenden Änderungen führen. Er selbst forscht mit seinem Team an Haushaltsrobotern und daran, wie man ihre Fähigkeiten optimieren könnte, wie also aus dem System, das dem Staubsaugerroboter zugrunde liegt, eines wird, das mehr Aufgaben im Heim übernehmen kann.
Keine Allmachtsfantasien

Davon dürfte kaum Gefahr für den Arbeitsplatz des Menschen ausgehen. Die berufliche Konkurrenz durch KI wird auf einer anderen technologischen Basis stärker – vor allem durch die schon seit einigen Jahren stark gehypte Methode Deep Learning. Die intelligenten Systeme werden dabei mittels neuronaler Netze, und ohne auf den Menschen angewiesen zu sein, aus Erfahrungen lernen.

Ein autonomes Fahrzeug, das lernt, in den Hauptverkehrszeiten Ausweichrouten zu nehmen, um schneller ans Ziel zu kommen, wäre ja eigentlich wünschenswert. Aber wie sicher ist die Nutzung für den Menschen wirklich? Wie sehr muss man sich vor Systemen fürchten, die nicht nur selbstlernend, sondern auch selbstständig sind?

Man muss nicht gleich an Maschinen mit Allmachtsfantasien denken, wie es etwa der Bordcomputer Hal in Stanley Kubricks Meisterwerk ""2001: Odyssee im Weltraum"" ist. Systeme könnten auch ohne derartige Übernahmegedanken irregeleitet sein, etwa im Verkehr folgenschwere Fehler begehen, weil diese Handlung im neuronalen Netz als richtig ausgelegt wird.
Drohnen als Transportmittel

Mit verkehrstauglichen Flugobjekten hat sich zuletzt die Stadtforscherin Katja Schechtner von der OECD beschäftigt – und zwar im konkreten Fall mit Drohnen. Die Studie ""Uncertain Skies: Drones in the World of Tomorrow"" analysiert Chancen und Risiken für den Transport von Gütern und Personen durch derartige Flugobjekte.

""Da sprechen wir von fliegenden Maschinen, die Nahrungsmittel liefern, aber auch von solchen, die vielleicht einmal die Last eines großen Passagierflugzeugs tragen können"", sagt sie. ""Es wird Drohnen geben, die das bestehende Verkehrssystem besser überwachen, aber auch solche, die zum Beispiel an Unfallorte Rettungsbojen transportieren werden.""

Es werde unterschiedliche Typen der Flugkörper je nach Nutzungszweck geben. Die Drohne, die ausschließlich mittels künstlicher Intelligenz gesteuert wird, sei eine letzte Stufe der Entwicklungen. Der Status quo: Der Flugkörper muss in Sichtweite des Operators am Boden gesteuert werden, Ausnahmen gibt es in manchen Ländern für klar begrenzte Lufträume: In Ruanda etwa können Drohnen ohne Sichtkontakt mit der Bodenstation innerhalb eines Luftkorridors Blutkonserven liefern, was nur ein paar Stunden dauert. Auf dem Landweg würde es Tage in Anspruch nehmen.

Schechtner: ""Bis Drohnen mit KI fliegen können, wird es noch fünf bis zehn Jahre dauern."" Und auch danach wird es nicht gleich möglich sein, die autonome Drohne von A nach B zu schicken. Man wird einen rechtlichen Rahmen brauchen – und die Akzeptanz der Gesellschaft. Und die ist nur durch größtmögliche Transparenz erreichbar.

Bild nicht mehr verfügbar.
Autos werden ohne menschlichen Roboter, aber auch ohne Fahrer auskommen.
Foto: Getty Images

Wie immer bei der Einführung neuen Technologien hängen die gesellschaftlichen Auswirkungen davon ab, wer sie wie nützt und welche Regeln damit verbunden sind, ob Staat, Unternehmertum oder Wissenschaft Macht über das Wissen haben und wie dessen Anwendung kontrolliert wird.

Deep Learning dürfte dabei als größtes Risiko eingeschätzt werden. Die Europäische Kommission veröffentliche im vergangenen Frühjahr eine ""Erklärung zu künstlicher Intelligenz, Robotik und autonomen Systemen"", die genau diese Fragen behandelt: Vor allem wird beklagt, dass die besonders leistungsstarken kognitiven Technologien völlig undurchsichtig in ihrer Herstellung seien. ""Ihr Handeln wird nicht mehr linear von Menschen programmiert. Google Brain entwickelt KI-Lösungen, die künstliche Intelligenz angeblich besser und schneller entwickeln können als Menschen. AlphaZero kann sich ohne Kenntnis der Schachregeln innerhalb von vier Stunden eigenständig zu einem Weltklassespieler entwickeln."" Und weiter: ""Es ist unmöglich, zu verstehen, wie AlphaGo fähig war, den menschlichen Go-Weltmeister zu schlagen.
Grenzen der Nutzung von KI

Durch Deep Learning und sogenannte ""Generative Adversarial Network""-Konzepte sind Maschinen in der Lage, sich selbst neue Strategien beizubringen und eigenständig nach neuen analysierbaren Informationen zu suchen."" Das Papier wurde von der ""Europäischen Gruppe für Ethik der Naturwissenschaften und der Neuen Technologien"" publiziert und schloss mit der Forderung nach einer umfassenden Diskussion über Grenzen der Nutzung von KI – und ein entsprechendes Regelwerk, an das sich alle Staaten halten müssten.

Kommentare dazu zeigen bereits jetzt: Es sind immer Unternehmen, die mit großen medialen Erfolgen aufwarten, die es damit in die Schlagzeilen der Medien schaffen. Der Zweck heiligt die Mittel. KI-Unternehmern scheint dabei kein Marketinggag zu billig. Sophia zum Beispiel, die Maschine von Hanson Robotics, kam zu Berühmtheit, als ihr von Saudi-Arabien die Staatsbürgerschaft verliehen wurde, wo Frauen normalerweise keine Staatsbürgerschaft erhalten. Die These vom austauschbaren Menschen wurde hier zynisch in die Realität umgesetzt. Dabei waren Sophias Fähigkeiten laut Beobachtern nicht einmal außergewöhnlich.

Von jener ""Superintelligenz"", die der Philosoph Nick Bostrom im gleichnamigen Buch beschwört, kann da noch keine Rede sein. Ihre Entwicklung sei ein existenzielles Risiko, orakelt er. Der Physiker Max Tegmark meinte in ""Leben 3.0"" sogar: Eine Superintelligenz könnte viel schneller Menschen ausrotten, als wir es mit acht von elf Elefantenarten geschafft haben. Diese Superintelligenz ist zwar noch nicht da, und es ist auch nicht sicher, ob es sie jemals geben wird. Aktuelle Entwicklungen sind aber so weitreichend, dass man es nicht ausschließen kann.

Bild nicht mehr verfügbar.
Der Roboter als Helfer: eine Vision, die schon lange verfolgt wird und die mit zunehmend leistungsstarken Systemen von der Angst vor zu mächtigen Maschinen begleitet wird.
Foto: Getty Images

Aber vielleicht liegt die Gefahr ja gar nicht in der Zukunft, sondern in der Gegenwart, meint Justus Piater aus Innsbruck. Mit der Demokratisierung von Information über Social Media ging auch die Demokratisierung von Fehlinformation einher. Da aufrüttelnde Inhalte durch die künstliche Intelligenz von Facebook & Co vorrangig befördert werden, stellen diese Plattformen höchst effektive Mechanismen zur Verbreitung von Propaganda dar.

Damit lässt sich Wählerverhalten gezielt beeinflussen, Vertrauen in die Relevanz faktischer Informationen überhaupt zerstören – und damit auch der Stellenwert von gutem Journalismus vernichten. Das stelle eine Bedrohung für die freie, demokratische Gesellschaftsordnung dar.
";https://www.derstandard.at/story/2000085351600/des-menschen-angst-vor-der-maschine;Standard;Peter Illetschko
30.03.2020;"Fan schafft mit KI 4K-Version von ""Star Trek: Voyager""";"Die Möglichkeiten von Maschinenlernen haben in den vergangenen Jahren deutliche Fortschritte gemacht. Und dies wird zunehmend auch dazu genützt, um alte Filme und Serien aufzuarbeiten. Welch beeindruckende Ergebnisse sich damit bereits erzielen lassen, zeigt nun ein Hobbyprojekt.
Exemplarisch

Ein Fan von ""Star Trek: Voyager"" hat mehrere Folgen der Serie mithilfe von künstlicher Intelligenz aus dem damals gebräuchlichen SD-Ausgangsmaterial auf 4K hochrechnen lassen. Als Beleg dafür hat er einen Ausschnitt aus Episode 4 der sechsten Staffel vorgenommen, die auf Deutsch unter dem Namen ""Dame, Doktor, As, Spion"" firmiert. Für die Berechnungen verwendete er die Software ""Gigapixel AI"".
Billy Reichard
Probleme

Perfekt ist die 4K-Version natürlich nicht. Zwar zeigt sich ein deutlich schärferes und weniger verrauschtes Bild, vor allem bei Kameraschwenks sind aber immer wieder Artefakte zu erkennen. Für einen echten ""Remaster"" müssten diese Probleme noch behoben werden, zudem wäre dann natürlich auch noch der Ton aufzubereiten.
Geringe Hoffnungen

Es ist nicht das erste Mal, dass sich Fans eine ""Star Trek""-Serie für eine solche Überarbeitung vornehmen, auch von ""Deep Space Nine"" gibt es schon eine inoffizielle HD-Version. Beide eint dasselbe Problem: Es erscheint reichlich unwahrscheinlich, dass sie jemals offiziell freigegeben werden, immerhin würden sie damit gegen das Copyright verstoßen. Insofern müssten sich für solch eine Variante schon die Rechteinhaber hinter ""Star Trek"" bewegen, und diese scheinen bisher kein Interesse an dem doch recht signifikanten Aufwand für solch eine 4K-Neuauflage zu haben.";https://www.derstandard.at/story/2000116328825/star-trek-voyager-fan-schafft-4k-version-dank-ki;Standard;red
11.04.2020;Lernbegleiterin für Programmier-Pros und Anfänger;"Wie unterrichtet man Softwareentwicklung, wenn das Thema für die Studierenden vollkommen neu ist? Welche Maßnahmen helfen, speziell Frauen an das Programmieren heranzuführen? Und wie unterrichtet man in ein und derselben Lehrveranstaltung Studierende, deren einschlägige Kenntnisse sehr unterschiedlich sind?

Das sind Fragen, mit denen sich Sigrid Schefer-Wenzl beschäftigt. Die Wirtschaftsinformatikerin, die an der FH Campus Wien forscht und lehrt, konzentrierte sich hier vor allem auf die Entwicklung didaktischer Konzepte für sehr diverse Gruppen. ""In meinen Lehrveranstaltungen sitzen Leute, die noch nie mit dem Programmieren zu tun hatten, neben professionellen Softwareentwicklern"", erzählt Schefer-Wenzl.

Der klassische Vortragsunterricht würde sich hier ohnehin nicht gut eignen, immerhin ist eine Programmiersprache auch tatsächlich eine Sprache, die es zu beherrschen gilt, betont die Informatikerin: ""Anwendungsorientierte Lehre steht im Vordergrund. Es geht dabei nicht um langweiliges Syntaxlernen, sondern darum, schnell Lernerfolge zu schaffen.""

Auch Anfänger beschäftigen sich gleich mit richtigen Apps, die etwa Bestellungen in Restaurants managen. Man arbeitet zuerst mit vorgefertigten Elementen, passt Programmierungen an, ergänzt sie.
Verschiedene ""Lernpfade""

Die Didaktikerin definiert verschiedene ""Lernpfade"" für verschiedene Niveaus, die die Studierenden danach durchlaufen können. ""Experten"" unter den Studierenden kommen in Gruppenarbeiten mit Anfängern zusammen und übernehmen Tutorien.

Lerntagebücher sind ein Element, um die Studierenden über die Lernerfolge reflektieren zu lassen. ""Natürlich erkläre ich den Studierenden inhaltliche Dinge. Dennoch sehe ich meine Rolle aber eher als Coach oder Lernbegleiterin, die die Studierenden motiviert"", sagt die FH-Professorin.

Schefer-Wenzl ist eine der wenigen weiblichen Lektorinnen im Informatikbereich. Der Frauenanteil unter ihren Studierenden ist aber relativ hoch. ""Fast alle Studentinnen sind bei mir"", sagt sie. Trotz des geringen Frauenanteils, der ihre Disziplin prägt, habe sie sich aber nie ausgeschlossen gefühlt.
Mit Computern sprechen

Die 1984 geborene Wienerin entschied sich für Wirtschaftsinformatik, weil das Fach verschiedenste Berufsbilder über alle Branchen hinweg gestattet. Ein Ursprung des Interesses könnte bei ihrem Vater liegen, der ebenfalls Informatiker ist. Er habe ihr als Kind erklärt, dass er ""mit Computern spricht"". Schefer-Wenzl: ""Ich fand das sehr mysteriös und faszinierend.""

Ihr Studium führte sie während der Doktorarbeit zu der Frage, wie man Security möglichst früh in Softwareprojekte einbringen kann. Heute forscht sie vor allem im Smart-City-Bereich und geht dort der Frage nach, wie Netze mit sehr vielen Teilnehmern effizient organisiert werden können – etwa bei einer Smart-Parking-Anwendung, die zeigt, wo Parkplätze frei sind.

Ein Gegenpol zur Informatik ist für Schefer-Wenzl die klassische Musik. ""Ich habe in der Schulzeit intensiv Klavier gespielt und vor kurzem wieder neu begonnen"", sagt die Mutter eines fünfjährigen Buben. ""Für meinen Sohn war mein Wiedereinstieg aber eher demotivierend. Er hat mich plötzlich spielen gesehen und dachte, bei ihm geht das auch so schnell.""";https://www.derstandard.at/story/2000116619203/lernbegleiterin-fuer-programmier-pros-und-anfaenger;Standard;Alois Pumhösel
14.02.2020;Incels: Frauenhasser im Netz werden toxischer – und immer radikaler;"Immer mehr geraten sogenannte Incels in den Fokus der Öffentlichkeit. Die relativ junge Community der ""involuntary celibates"", der unfreiwillig Enthaltsamen, hat sich vor allem im Netz gebildet. Ihre radikalen Mitglieder verachten Frauen, weil sie denken, aufgrund ihres Aussehens keine sexuelle Beziehung zu ihnen aufbauen zu können. Schuld daran seien weibliche Personen selbst. In der Vergangenheit fiel die Gruppierung vor allem durch schwere Straftaten einzelner Personen, die sich selbst als ""Incels"" bezeichneten, auf: etwa bei einem Amoklauf in Toronto, bei der die Motivation des Täters war, so viele Frauen wie möglich umzubringen.

In einem aktuellen Papier illustrieren nun Informatiker und Forscher aus Deutschland, der Schweiz und den USA die bisher umfassendste Untersuchung der ""Mannosphäre"", also dem Phänomen der antifeministischen Bewegungen im Netz. Dafür wurden sieben Foren, mehrere spezialisierte Wikis und 57 Subreddits durchforstet. Insgesamt beinhalten die Datensätze die Konten von 138.000 Nutzern und insgesamt 7,5 Millionen Beiträge.
Mehrere Gruppierungen

Die ""Mannosphäre"" wird von den Forschern in vier große Gruppierungen eingeteilt: Die extremste Gruppierung stellen die besonders radikalen Incels, die einen tiefen Frauenhass hegen und die steigende soziale Stellung der Frau als ""Plage"" betrachten. Heirateten Frauen früher, um finanzielle Stabilität zu erlangen, würden sie nun nur attraktive Männer suchen, um mit ihnen zu schlafen, so die Ideologie dahinter. Frauen seien mittlerweile privilegiert – und dafür will sich die Gruppierung rächen. Sie gilt als die potenziell gewaltbereiteste Gruppe.

Men’s Right Activists (MRA) – Männerrechtsaktivisten – glauben, dass soziale Institutionen tendenziell Männer diskriminieren. Während ihnen in der Vergangenheit legitime Bedenken zugesprochen wurden, kritisiert beispielsweise die antirassistische NGO Southern Poverty Law Center den enorm misogynen Ton, in dem diese Argumentation häufig kommuniziert wird.
System ""gebrochen""

Men Going Their Own Way (MGTOW) glauben, dass die Gesellschaft sich gegen Männer verschworen hat. Im Gegensatz zu der MRA fordern sie keinen gesellschaftlichen oder juristischen Wandel, sondern glauben, dass das System so gebrochen sei, dass es nicht gerichtet werden könne – und sehen die Lösung daher darin, den ""eigenen Weg zu gehen"". Das heißt für viele, seriöse Beziehungen zu Frauen zu meiden. Oft tritt das Phänomen mit extremem Antifeminismus und Misogynie gepaart in Erscheinung, so die Untersuchung.

Pick Up Artists (PUA) ist hingegen eine Community, die eine ""Kunst"" daraus gemacht hat, Frauen zu verführen. Influencer der Szene bezeichnen sich selbst als PUA und bringen anderen Männern, oft entgeltlich, bei, wie man eine Frau zum eigenen, häufig sexuellen Zweck manipulieren kann.
Große Migration – und Radikalisierung

Die Untersuchung kommt zu dem Ergebnis, dass ältere Gruppierungen immer mehr aussterben. Es herrsche eine große Migration innerhalb der Communitys, immer mehr würden sich die Nutzer aber radikalisieren und sich etwa den extremeren, hasserfüllteren Incels anschließen. So würden jährlich rund acht Prozent der beobachteten MRA- und MGTOW-Mitglieder zu der Incel-Community migrieren.

Für die Untersuchung wurde die Machine-Learning-gesteuerte Software Perspective von Google eingesetzt, die Beiträge wurden in Foren nach bestimmten Keywords durchforstet. Das Team verwendete diese, um eine Art Toxizitätswert zu bestimmen. Durch diese Methode kam man zu dem Schluss, dass die Wortwahl der Gruppierungen auf dem sozialen Medium Reddit weitaus hasserfüllter ist als die eines durchschnittlichen Nutzers – eher nähere die Rhetorik sich an rechtsextreme Communitys, beispielsweise auf der Plattform Gab.
Probleme

Eigens eingerichtete Foren seien hingegen noch toxischer als die Subreddits der Mannosphäre. In der Vergangenheit ist Reddit gegen die radikalen Gruppierungen vorgegangen – 2017 wurde etwa der Subreddit ""Incels"" gesperrt, vergangenes Jahr sein Nachfolger ""Braincels"".

Die Messung funktioniere den Forschern zufolge allerdings nicht ideal – so wurde Perspective in der Vergangenheit selbst für eine mögliche rassistische Voreingenommenheit kritisiert. Außerdem werden einige Codes, die von den Gruppierungen verwendet werden, übersehen – so sprechen die Nutzer teilweise in einer für Außenseiter kryptisch wirkende Sprache.";https://www.derstandard.at/story/2000114568813/incels-frauenhasser-im-netz-werden-toxischer-und-immer-radikaler;Standard;muz
23.09.2019;Warum das AMS keine KI auf österreichische Bürger loslassen sollte;"Es wird eine Zeit geben, vielleicht in 100 Jahren oder schon früher, wo man auf die heutige Zeit zurückschauen und sich darüber an den Kopf greifen wird, wie irrsinnig wir sein konnten, die Macht der Computer völlig falsch zu nutzen. Einige Ereignisse und ihre Entscheidungsträger werden dabei ganz besonders in die Geschichte eingehen. So für Österreich vielleicht das Arbeitsmarktservice (AMS), das letzte Woche endgültig entschieden hat, ein Computerprogramm ausrechnen zu lassen, ob man einem arbeitsuchenden Menschen noch hilft oder nicht.
Warum ist das AMS-Programm ethisch fragwürdig?

Unser modernes Denken glaubt, dass man alles, was lebt, vermessen und mathematisch modellieren kann; ja, das Schicksal vielleicht sogar ausrechnen kann. Ein Traum menschlicher Macht. Und zugleich ein Albtraum, wenn Menschen mit Computermacht dieser Tage dabei sind, ihn in die Tat umzusetzen, und anfangen, die Digitalisierung dafür zu nutzen, das Schicksal anderer auszurechnen.

Wie gut unsere heutigen Big-Data-basierten Computersysteme in der Lage sind, auf den einzelnen Menschen bezogene ""Fakten"" zu errechnen, das sieht man ganz banal, wenn man als Internetnutzer Werbung erhält. Oft ist man erstaunt, wie zielgenau die Werbung ist, die einem angezeigt wird. Computersysteme wissen heute wirklich viel über uns. Aber nicht selten lächelt man auch darüber, wie falsch die Werbung sein kann: Gerade wurde zum zehnten Mal das Auto angeboten, das man scheußlich findet, und außerdem hat man sich den neuen Wagen doch schon vor drei Monaten gekauft.

Wenn man sich diese Genauigkeit der heutigen Werbesysteme ansieht, erhält man einen Eindruck davon, was die mächtigsten Maschinen der Welt heute leisten können. Oft liegen sie falsch. Aber wen stört das schon? Für uns Nutzer hat es keine persönlich negativen Folgen, wenn Werbung falsch liegt. Aber wenn man als Arbeitsuchender falsch klassifiziert wird? Dann hat das durchaus Konsequenzen. Dann sind Fehlerraten kein Spaß mehr.

Hat das AMS also bessere Computersysteme oder eine Art ""Super-Künstliche-Intelligenz"", die besser rechnet als die mächtigsten Werbenetze unserer Zeit? Ich hoffe doch sehr, denn wenn die Einschätzung über das Schicksal von einzelnen Arbeitssuchenden so zielgenau danebenliegen kann wie die Einschätzung davon, welches Produkt uns als nächstes interessiert, dann gute Nacht, Marie. Der gegenwärtig mir bekannte Stand der Technik gibt es nicht her, verlässlich über einzelne Personen mit solcher Präzision zu urteilen, dass man darauf die weitere Förderung durch den Staat basieren dürfte.1
Die Würde des Menschen und menschlicher Ausschuss

Neben der fragwürdigen technischen Reife sollte uns noch eine weitere Frage beschäftigen: dass nämlich die AMS-KI und überhaupt jeder Einsatz heutiger KI gegen einzelne Menschen sich nicht mit unserem Verständnis von Menschenwürde vereinbaren lassen. Warum? Weil Würde das ist, was wir seit der Aufklärung jedem einzelnen menschlichen Individuum zusprechen. Jeder Einzelne, selbst ein Verbrecher, hat das Recht auf einen Prozess. Jeder Mensch wird als individuelles, respektables Wesen gesehen. Jeder Bürger legt heute das Vertrauen in den Staat, als Person gesehen und fair behandelt zu werden. Diese Bedeutung des Einzelschicksals wird jedoch durch KI-Systeme aufgehoben. Warum? Weil KIs auf Basis von Mustern arbeiten. Sie basieren ihre Entscheidungen auf Regeln. Sie fassen also Menschen in Gruppen zusammen; ordnen sie Clustern zu.

Böse gesprochen könnte man sagen: KIs stecken Menschen in Schubladen auf Basis von Regeln, die sie aus einer großen Grundgesamtheit von Daten abgeleitet haben. Zum Beispiel die Regel des AMS-Systems, dass Frauen mit Betreuungspflichten eher in die Gruppe der Nichtvermittelbaren gehören. Frauen sind im AMS-System dieser allgemeinen Regel unterworfen. Aber was, wenn die einzelne junge Frau mit ihren sieben Kindern eine ganz vitale Frau ist, die es durchaus verdienen würde, vermittelt zu werden? Man denke nur an Frau van der Leyen mit ihren sieben Kindern! Sie wäre vom AMS-Algorithmus wahrscheinlich als Erste benachteiligt worden. Die Würde des Menschen bedeutet, dass wir jeden Einzelnen ansehen; dass wir ihre oder seine individuelle Vitalität, Emotionalität, Lebenslage und Gesundheit ansehen, würdigen und dann entscheiden, wie wir uns ihm oder ihr gegenüber verhalten.
Wie gut ist die AMS-KI?
Foto: APA/HERBERT PFARRHOFER

Wenn nun der AMS-Algorithmus bei x Personen eine falsche Zuordnung zu einer schlechten Gruppe vornimmt, was passiert dann mit genau diesen Einzelschicksalen, die in dieser Gruppe falsch gelandet sind? Betrachtet man diese Menschen dann als ""Ausschuss""? Als hinzunehmenden Kollateralschaden von etwas, das man neblig als ""Fortschritt"" bezeichnet? Wie müssen sich diese Menschen fühlen, über die ein Algorithmus sagt: Du da, du bist nichts wert! Würde das AMS pauschal dem Urteil seiner magischen KI-Maschine vertrauen und den ""Ausschuss"" an falsch berechneten Schicksalen als ""Kollateralschaden"" hinnehmen, dann würde die Institution wirklich diametral der Menschenwürde zuwiderhandeln.
Das AMS sieht seine Sachbearbeiter in der Verantwortung

Das AMS hält dem zu Recht entgegen, dass Kollateralschaden nicht entsteht. Man habe immer noch einen AMS-Sachbearbeiter dazwischen, der die schlechte Nachricht überbringt. Aber hier ist nun die Gretchenfragen: Welche Informationen und Freiheitsgrade bekommt der AMS-Sachbearbeiter, um die Entscheidung der Maschine zu missachten – also selbst das Urteil zu fällen? Aus Experimenten wissen wir seit langem, dass Menschen Maschinenurteile meistens für die objektiveren und richtigeren halten.

Fast jeder, der sich nicht mit Informatik auskennt, denkt, dass man Maschinen voll vertrauen darf. Aber das ist, wie am Werbebeispiel gezeigt, sehr oft nicht weise. Maschinen machen ihre eigenen Fehler! Und das nicht zu knapp, wie ich in meinem Buch ""Digitale Ethik"" sehr genau beschrieben habe. Was hat das AMS also vorbereitet, um diese Fehlerwahrscheinlichkeit seines KI-Algorithmus den eigenen Sachbearbeitern anzuzeigen? Wie gut verstehen die AMS-Sachbearbeiter die Einzelheiten der neuen KI-Logik, um deren Vorschläge zu hinterfragen? Werden AMS-Mitarbeiter dazu ermutigt, dass sie der neuen hauseigenen ""Super-KI"" widersprechen und der jungen Mutter vor ihnen doch helfen, selbst wenn der Algorithmus das nicht empfiehlt?

Ich hoffe, das AMS hat sehr gute Antworten auf diese Fragen, ebenso wie eine astreine Datenqualität legitim gesammelter Daten, weil es sonst nämlich in der Tat Gefahr läuft, sich einer würdelosen Entwicklung zu unterwerfen, die die amerikanische Mathematikerin Cathy O'Neil in ihrem kürzlich erschienen Bestseller als das beschrieben hat, um was es sich hier handelt: einen ""Angriff der Algorithmen"" durch falsche Digitalisierung.
Was heißt ""falsche"" Digitalisierung?

Beim Lesen dieser Kritik am AMS denken Sie, liebe Leser, sicherlich sofort, dass ich, Sarah Spiekermann, eine Digitalisierungsgegnerin sein muss. Stimmt nicht. Ich wünsche mir nur, dass Digitalisierung für die richtigen Zwecke eingesetzt wird und nicht für die falschen. Das AMS besitzt sicherlich einen sehr reichen, über die Zeit gewachsenen Schatz an Daten über arbeitsuchende Menschen. Diese Daten sind eine Fundgrube an potenziellem Wissen über unsere Sozialsysteme. Würde das AMS diese Daten erkunden, verfeinern und ausbauen, könnte ein sicherlich ungeheuer reiches Wissen geschaffen werden. Wissen darüber, was Menschen zu Arbeit bewegt, in Arbeit hält, zu Arbeit bringt. Auch wäre für das AMS wichtig zu wissen, ob es bei den eigenen Sachbearbeitern Vorurteile und Unterschiede gibt. Aggregiert könnte solches Wissen (zum Beispiel über Vorurteile) den AMS-Mitarbeitern in Schulungen nahegebracht werden, um deren Leistung zu verbessern und Intuition zu schulen.

Die digitale Transformation könnte also zutiefst positiv im Sinne des Wissens, des Lernens und Schulens des eigenen Personals genutzt werden. Stattdessen tut man das, was ich ""falsche"" Digitalisierung nenne. Man delegiert das Denken und Lernen und eigentliche Entscheiden an eine Maschine. Man betreibt ""machine learning"" statt ""human learning"". Am Ende steht ein komplexes Datenmodell, das vielleicht nur noch der Drittdienstleister der AMS-KI-Synthesisforschung versteht, während das AMS und seine Mitarbeiter nur noch ""Durchreicher"" von Maschinenentscheidungen sind. Solch eine Entwicklung sehe ich als falsch an, denn nicht nur sind heutige KIs technisch zu fehlerbehaftet für solche Jobs. Auch fehlt ihnen die Fähigkeit zu Intuition, Sympathie, Empathie, Vertrauen und wahrem Lernen durch emotionales und kulturelles Gedächtnis, die nur wir Menschen haben – und in solch sensiblen Lebensbereichen auch brauchen.";https://www.derstandard.at/story/2000108890110/warum-das-ams-keine-ki-auf-oesterreichische-buerger-loslassen-sollte;Standard;Sarak Spiekermann
16.08.2019;Universitätenkonferenz fordert Einrichtung eines Instituts für künstliche Intelligenz;"Wien/Alpbach – Um die Kompetenz Österreichs im Bereich Künstlicher Intelligenz (KI) zu stärken, schlägt die Universitätenkonferenz (uniko) in einem Positionspapier die Einrichtung eines KI-Instituts im Rahmen des europäischen KI-Netzwerks ELLIS sowie die Schaffung von Rechen-Infrastruktur vor. Für ersteres wären jährlich rund 30 Millionen Euro erforderlich, für letztere Anschaffungskosten in Höhe von 40 Millionen. In dem Positionspapier begrüßen die Universitäten die Initiative zur Erstellung einer österreichischen KI-Strategie. Diese sollte eigentlich bei den diesjährigen Alpbacher Technologiegesprächen präsentiert werden. Daraus wird aufgrund der innenpolitischen Situation aber nichts, ein Beschluss bleibt wohl der nächsten Regierung vorbehalten.
Stichwort ELLIS

Mit den von der uniko vorgeschlagenen Maßnahmen soll auch der Forschungsstandort Österreich in der internationalen KI-Community, speziell in den beiden europäischen KI-Netzwerken ELLIS und CLAIRE verankert werden. ""Österreich muss hier eine maßgebliche Rolle spielen"", heißt es in dem Papier.

Das Netzwerk ELLIS (European Laboratory for Learning and Intelligent Systems) plant laut dem Papier, die wichtigsten Standorte für die KI-Bereiche Deep Learning und Machine Learning zu vernetzen. Österreich habe ""ausgezeichnete Chancen"", neben der ETH Zürich, der Universitäten Tübingen und Cambridge einer der wenigen begehrten Standorte eines ELLIS-Instituts zu werden. Dies würde eine jährliche nationale Finanzierung in der Größenordnung von rund 30 Mio. Euro für zehn bis 15 Forschungsgruppen für die nächsten zehn Jahre erfordern.
Was erforderlich ist

Weil in Österreich sowohl die universitäre KI-Forschung als auch die Unternehmensstruktur kleinteilig gestaltet sei, sollte auch ein nationales KI-Netzwerk geschaffen werden, meint die uniko. Für ein solches Programm wären jährlich 15 Mio. Euro zur Finanzierung von rund 100 Doktoranden und Postdocs notwendig.

Schließlich wird in dem Papier die Schaffung von Rechenkapazität, insbesondere eines Graphikkarten-Clusters (GPU-Cluster), für notwendig erachtet, um im internationalen Wettbewerb der KI-Forschung mithalten zu können. Vorgeschlagen wird die Einrichtung eines GPU-Clusters mit rund 10.000 GPUs, der exklusiv der akademischen Forschung im Bereich des Deep Learnings zur Verfügung steht. Die Anschaffungskosten dafür werden mit rund 40 Mio. Euro beziffert.

Als ""essenziell"" wird in dem Papier zudem die Verfügbarkeit von Daten für die Forschung bezeichnet. Dazu sollten geeignete Datenpools bzw. Zugriffsmöglichkeiten mit entsprechenden Rechtsgrundlagen für deren Nutzung geschaffen werden.";https://www.derstandard.at/story/2000107464613/universitaetenkonferenz-fordert-einrichtung-eines-instituts-fuer-kuenstliche-intelligenz;Standard;APA
29.11.2019;"Forscher und Studenten zeigen in neuem Buch, ""wie Maschinen lernen""";"Künstliche Intelligenz (KI) ist eines der großen Themen unserer Zeit. Zwar wird das Buzzwort oft ventiliert, doch nicht jeder, der es mehr oder weniger gelassen ausspricht, könnte es auch erklären. Forscher und Studenten tun das nun gemeinsam unter dem Titel ""Wie Maschinen lernen"" in Buchform. Ko-Herausgeber Christoph Lampert ortet viel Bedarf für ein ""alltagsverständliches Einführungsbuch"".

Von der Verheißung der automatischen Erledigung unliebsamer Tätigkeiten durch ""intelligente"" Systeme, über das freihändige Fahren und die ultimativ passende musikalische Playlist bis zur Angst vorm Obsoletwerden des eigenen Jobs oder gar vor dem ums Eck wartenden ""Terminator"" reichen Bilder, die mit KI, Maschinellem Lernen oder Deep Learning verbunden sind. Mehr Wissen könnte den mitunter recht emotional geführten Debatten durchaus gut tun, so vielfach der Eindruck.
Mechanismen

Für den Einstieg müsse man auch bei weitem kein Experte sein, denn die grundlegenden Mechanismen seien gar nicht schwer zu verstehen, so der am Institute of Science and Technology (IST) Austria in Klosterneuburg (NÖ) tätige Lampert vor Journalisten in Wien. Auch bei weitem nicht alle der 25 Studenten, die im Rahmen des von der Studienstiftung des deutschen Volkes unterstützten Buchprojekts im Tandem mit Wissenschaftern zu Autoren wurden, hatten davor umfassendes Vorwissen über KI und Co.

Die Herausforderung für das Buch war, möglichst auf Fachjargon und Formeln zu verzichten und die Erklärungen trotzdem nicht zu verwässern. ""Wir haben versucht, einen Mittelweg zu finden"", sagte Lampert, der darauf hinwies, dass es bisher kein derartiges Sachbuch auf Deutsch gebe. Als Hauptperson in ""Wie Maschinen lernen"" fungiert die Studentin Lisa, die mit verschiedenen mehr oder weniger alltäglichen Situationen und Problemen konfrontiert ist, die mit dem Themenkreis zu tun haben.

So geht es von grundlegenden Themen wie ""Algorithmen"", ""Daten"" oder ""Maschinelles Lernen"" langsam in Richtung mathematischer Methoden oder um die momentan so präsenten ""Künstlichen neuronalen Netzwerke"" bis zu ethischen und gesellschaftlichen Fragen. Auch wenn all das in eher jugendlicher Sprache ausgeführt wird, ""ist die Zielgruppe im Prinzip jeder, der sich auf einem gewissen Niveau von logischem Denken damit beschäftigen will"", sagte Lampert, der am IST Professor für Computer Vision und Machine Learning ist.
Künstliche Intelligenz

Auch abseits der größeren philosophischen und gesellschaftspolitischen Fragen, die alleine schon der Terminus ""Künstliche Intelligenz"" mit sich bringt, täte eine Auseinandersetzung mit dem gut, was sich schon seit geraumer Zeit etwa im Bereich der personalisierten Werbung tut. Hier handelt es sich um den kommerziell mit Abstand größten Anwendungsbereich mit weltweiten Jahresumsätzen rund um 350 Milliarden Dollar. Alleine in Österreich geht man laut Lampert von einem derzeitigen Marktvolumen von rund einer Milliarde Euro aus. Der Rohstoff dafür sind Daten über das Verhalten von realen Menschen im Internet, anhand derer Systeme maschinell lernen.

Mehr Mündigkeit im Umgang mit Daten und eine ungefähre Vorstellung davon, ""was vor und hinter den Kulissen passiert"", wäre laut Lampert daher wünschenswert. Sehr viele Menschen benützten das Internet momentan ein wenig nach dem Motto ""das ist mein Gerät"", aber die Mechanismen der Vernetzung dahinter seien meistens unbekannt, auch weil man sie nicht sieht. Ein gewisses Verständnis dafür, wie Algorithmen funktionieren und wie sie aus diesen ungeheuren Datenmengen sehr spezifische Schlüsse ziehen, würde hier helfen. Ein Stück weit brauche es einfach Bewusstsein dafür, ""dass ein Computer keine magische Kiste ist"".

Davon ist auch KI noch weit entfernt, so der Experte. Ein System, das tatsächlich so weit entwickelt und flexibel ist, um sich autonom neuen Problemen zuzuwenden und vielleicht sogar eine Art Bewusstsein entwickelt, ist für Lampert noch tief im Bereich der Science Fiction anzusiedeln. Auch Visionen in Richtung einer künstlichen ""generellen Intelligenz"" oder ""starken KI"", die mitunter etwa von den Schöpfern der ob ihrer Go- und Schachfähigkeiten aufsehenerregenden Google-Software AlphaZero auch in wissenschaftlichen Arbeiten in den näheren Zukunftsraum gestellt werden, sieht der KI-Experte skeptisch. Hier werde momentan vieles versprochen – auch mit eindeutigen Marketing-Hintergedanken.";https://www.derstandard.at/story/2000111663760/forscher-und-studenten-zeigen-in-neuem-buch-wie-maschinen-lernen;Standard;
21.03.2018;"""Sonnenblicke auf der Flucht"": Künstliche Intelligenz schreibt Gedicht";"Tunnel23 schickte ein Machine Learning-Projekt zum Gedichtwettbewerb ""Frankfurter Bibliothek"", eine von der Werbeagentur trainierte Künstliche Intelligenz erschuf das Gedicht ""Sonnenblicke auf der Flucht"". Dichtungen von Goethe und Schiller waren die Basis, um der KI Poesie zu lehren.
TUNNEL23

Das KI-basierte Gedicht wurde in den Gedichtband ""Frankfurter Bibliothek"" der Bretano-Gesellschaft aufgenommen. ""Kreativität wurde bis dato ausschließlich dem Menschen zugeschrieben – ein wesentliches Merkmal, das ihn so einzigartig macht. Doch die KI perfektioniert das Nachahmen des Menschen und zwingt uns die Definition von Kreativität zu überdenken,"" sagt dazu Michael Katzlberger, Geschäftsführer von Tunnel23.";https://www.derstandard.at/story/2000076538946/sonnenblicke-auf-der-flucht-kuenstliche-intelligenz-schreibt-gedicht;Standard;red
22.06.2020;Das große Zittern um die Studierenden;"Jetzt hat sich also auch noch Chinas Regierung eingemischt. Peking hat vergangene Woche in einer Aussendung der staatlichen Agentur Xinhua deutlich gemacht, was man davon halten würde, wenn chinesische Studentinnen und Studenten im Wintersemester wieder an ihre Unis im englischsprachigen Ausland zurückkehren würden: nicht viel. Konkret ist die Meldung auf Unis in Australien gemünzt, sie spricht von der Corona-Ansteckungsgefahr und warnt vor Rassismus. Und sie hat wohl auch aktuelle politische Konflikte mit Canberra zum Hintergrund. Und doch gibt sie eine Sorge wieder, die Bildungsinstitutionen, vor allem in der englischsprachigen Welt, seit Beginn der Pandemie umtreibt: Was, wenn nach der Pandemie die Studierenden aus dem Ausland wegbleiben, die sich in den vergangenen Jahrzehnten vielerorts zur Gruppe der größten Uni-Finanziers entwickelt haben?

In den USA, Großbritannien und Australien schnüren die Unis schon unter Maßgabe knapper Mittel ungewohnte neue Budgets und bereiten sich auch für die kommenden Semester auf Fernlehre vor. In Österreich tut man, jedenfalls im zweiten Punkt, dasselbe, sagt Elisabeth Brunner-Sobanski, Leiterin des Ausschusses für internationale Angelegenheiten der Fachhochschulkonferenz (FHK). Sie spricht von ""sehr herausfordernden und ressourcenintensiven"" Planungen, die derzeit in den FHs angestellt würden. Immerhin würden manche Studiengänge zwischen 20 und 40 Prozent Regelstudierende aus dem Ausland zählen. Im Schnitt kommen rund 15 Prozent der 55.200 Studierenden an den FHs aus dem Ausland.
Meist aus Deutschland

An den öffentlichen Unis liegt der Anteil gemessen an der Gesamtstudierendenzahl von 265.000 bei 27 Prozent. Prozentuell den höchsten Anteil an ausländischen Studierenden gibt es an den österreichischen Privatunis. Knapp zwei Fünftel aller Studierenden – im Wintersemester 2018/19 waren es rund 13.650 – sind ausländische Staatsangehörige. Allerdings kommen die meisten ausländischen Studierenden an allen Hochschulen aus Deutschland.

Brunner-Sobanski rechnet damit, dass zumindest der Einstieg ins kommende Semester auch in Österreich vielerorts virtuell erfolgen werde. Danach werde man sehen, ob es weitere Verbesserungen bei der Reisefreiheit gebe. Immerhin, so Brunner-Sobanski vorsichtig positiv: Bisher gebe es ""keine großen Mobilitätseinbrüche"", was die geplante Teilnahme an Studien betreffe.

Eine weitere positive Nachricht präsentiert Sabine Seidler, Präsidentin der Österreichischen Universitätenkonferenz. Weil das Uni-System in Österreich – abseits der im internationalen Vergleich geringen Studienbeiträge (726,72 Euro pro Semester für Studierende aus Drittstaaten) – öffentlich finanziert ist, ""halten sich die finanziellen Verluste in Grenzen"".
Britische Sorgen

Schwer getroffen sind in Österreich einige Institute, die ihr Geld bisher mit Deutschkursen für fremdsprachige Studierende verdient haben. Sorgen wie etwa in Großbritannien gibt es hierzulande bisher aber nicht. Dort veröffentlichte der Guardian bereits im Mai ein Meinungsstück des Oxford-Historikers Glen O’Hara, in dem ganz konkret über das mögliche Ende für eine beträchtliche Zahl britischer Bildungsinstitutionen nachgedacht wird. Dass Unis im angelsächsischen Raum besonders betroffen sind, liegt auf der Hand: Sie finanzieren sich über Studiengebühren und vermarkten auf dem Campus Gastronomie und Logis.

Weil zudem der Anteil Studierender aus anderen Ländern besonders hoch ist, sind sie sehr empfindlich im Hinblick auf Drohungen aus dem Ausland. Was auch China weiß, das mit seiner Warnung vor Australien – aber auch anderen Staaten, die Peking kritisiert haben – diplomatisch einen Schuss vor den Bug gesetzt hat.

Dazu kommen neue Argumente. Gar nicht so schlecht finden es etwa einige Mitglieder der Black-Lives-Matter-Bewegung, dass Menschen aus früheren Kolonien nun nicht mehr unreflektiert in ihren Studien in die ""britische Lebensart"" eingeführt werden.
Campus künftig leer?

Und dann bleibt noch die wichtigste Frage offen: Wie geht es weiter, wenn Covid-19 einmal besiegt sein sollte? In den USA, Großbritannien und Australien rechnen die Unis nicht damit, dass sich die Zahl der Studierenden aus dem Ausland so schnell wieder auf den alten Stand bringen lässt. Und auch nicht damit, dass alle Lernenden ihre Daueranwesenheit auf dem Campus so einfach wieder fortsetzen wollen. Zwar sei der Appetit auf ein Studium zu Hause bei den Eltern gering, so O’Hara im Guardian, doch gebe es viele finanzielle Gründe dafür.

Ähnlich, aber in optimistischem Tonfall formulierte das auch schon kurz zuvor in der New York Times der NYU-Professor Hans Taparia. ""Die Zukunft des College ist online, und sie ist billiger"" heißt sein Text, in dem er vorrechnet, wie eine Konzentration auf Telelearning aussehen kann. Statt Studiengebühren von 42.000 Dollar wären dann etwa für einen zweijährigen Lehrgang der Computerwissenschaft nur noch 7000 Dollar fällig. Eine These, die sich nicht zuletzt auch im US-Wahlkampf, wiederfinden könnte, in dem astronomische College-Kreditschulden stets ein Thema sind.

Derart dramatische Umwälzungen der Studienrealität, die statt von Treffen in Hörsaal, Aula und Bibliothek dann von Videostreams und Chats mit den Lehrenden geprägt wäre, mögen sich so schnell nicht verwirklichen lassen. Doch auch in Österreich dürfte die Studienwelt nach Corona etwas anders aussehen. Man werde ""über alternative Internationalisierungsangebote abseits physischer Mobilität"" nachdenken, sagt Brunner-Sobanski.";https://www.derstandard.at/story/2000118180761/das-grosse-zittern-um-die-studierenden;Standard;Karin Bauer, Manuel Escher, Gudrun Ostermann
24.10.2019;Roboterjournalismus: Dieser Text ist (noch) KI-frei;"Alle Texte, auch die kürzeste Meldung, die Sie heute auf der STANDARD-Website gelesen haben, wurden von Menschen verfasst. Ehrenwort. Bald wird das aber keine Selbstverständlichkeit mehr sein. Einer BBC-Studie zufolge werden im Jahr 2026 schon 90 Prozent aller Nachrichtentexte von Computern verfasst. Bereits heute erstellen Maschinen Börsen- und Sportberichte, meist befüllen sie Tabellen. Man nennt das etwas irreführend Roboterjournalismus. Andere Textformen wie Essays, Reportagen und Kommentare verlangen mehr Sprachgespür und nicht bloß ein System, das Textbausteine aneinanderreiht. Ist künstliche Intelligenz dazu in der Lage? Ein Faktencheck.

    Wie sieht es mit kreativem Schreiben aus? 

Anfang des Jahres machte die amerikanische Open-AI-Stiftung, die sich mit Potenzialen und Gefahren künstlicher Intelligenz befasst, Schlagzeilen: Die Forscher entwickelten eine Sprach-KI namens GPT-2, die selbstständig journalistische und und literarische Texte verfassen kann. Obwohl Open AI die eigenen Entwicklungen stets als Open Source freigibt, blieb GPT-2 unter Verschluss, lediglich eine stark reduzierte Version wurde bislang veröffentlicht. Die Macher bezeichnen die KI als ""Deep Fake für Texte"", als in der Lage, Artikel zu erstellen, die nicht mehr zweifelsfrei von den von Menschen geschriebenen zu unterscheiden sind. So könne die KI etwa zum Thema Brexit schreiben – erfundene Zitate inklusive. Oder journalistische Kommentare mit plausiblen Argumentationsketten liefern.

    Wird die nächste Edelfeder ein Algorithmus sein? 

Nein, sagt der Sprachwissenschafter und KI-Forscher Aljoscha Burchardt vom Deutschen Forschungszentrum für künstliche Intelligenz. Es handelt sich bei den derzeitigen KI-Anwendungen um simple ""Sprachmodelle"", die in der Lage sind, Sätze auf Basis von Erfahrungswerten weiterzuführen. Eine Anwendung ist zum Beispiel die Google-Suche, die dem Nutzer das nächste Wort vorschlägt. Burchardt: ""Die Systeme sind statistisch-mathematisch und können aus vorhandenen Daten Modelle bauen; man kann sie verwenden, um Schach zu spielen oder mit Kamerabildern ein Auto zu steuern, oder eben auch, um einen Text zu generieren.""

    Wie funktionieren Sprach-KI-Anwendungen im Detail? 

Wetterdaten oder Sportergebnisse brauchen nicht unbedingt eine blumige oder originelle Sprache, der Algorithmus ist dafür da, zuverlässig einen Text aus Daten und Ergebnissen zu generieren. Der Roboterjournalist braucht eine Datenbank mit Sportergebnissen und ein paar vorgegebene Textbausteine. Daraus entstehen dann gut verständliche Texte. Hier geht es nicht um wortgewaltige Formulierungen (wie sie GPT-2 offenbar beherrschen soll), sondern um Präzision.

Die sogenannten neuronalen Netze hingegen können aus dem vorhandenen Kontext einen Text weiterspinnen. Allerdings hat man keine Kontrolle über die Inhalte, sagt Burchardt. Das, was diese Systeme machen, ist eigentlich genau das Gegenteil dessen, was man unter klassischem Roboterjournalismus versteht: Sprachmodelle wie GPT-2 werden mit riesigen Textmengen gefüttert.
OpenAI GPT-2: An Almost Too Good Text Generator
Two Minute Papers

Open AI bediente sich dabei bei Reddit, einer Plattform, auf der Nachrichtentexte verlinkt werden. Was passiert aber, wenn man dieses Modell mit anderen Textformaten wie etwa Essays speist? Möglicherweise könnte es dann das Verfassen variantenreicherer Texte erlernen.

    Kann die Maschine Texte für den ""New Yorker"" verfassen? 

Was wäre etwa, würde Open AI das System GPT-2 mit dem kompletten Archiv des renommierten US-Magazins The New Yorker füttern? Hier findet die Maschine ""Millionen von polierten und geprüften Wörtern, viele davon von Meistern der Literaturwissenschaft geschrieben"", meint der New Yorker-Autor John Seabrook. Er stellte sich die Frage, ob GPT-2 jemals in der Lage sein könnte, einen Text für den New Yorker zu schreiben. Open AI ließ sich auf das Experiment ein: Alle nichtfiktionalen Texte, die seit den 1960ern in dem Magazin veröffentlicht wurden, las die Software in weniger als einer Stunde ein. Anschließend erhielt sie die Aufgabe, einen Absatz aus einem Ernest-Hemingway-Porträt sinnvoll und stilistisch herausragend fortzusetzen.

Das Ergebnis wirkt zunächst beeindruckend: Das System verfasste eine Textpassage im Stil der Porträtautorin, die sich zumindest oberflächlich durchaus wie ein Artikel des New Yorker liest. Erst bei genauerer Betrachtung erkennt man die Fehler. So ist von ""winzigen Kühen"" oder von ""Lacken aus roter Bratensauce im Vorgarten"" die Rede. Die künstliche Intelligenz begreift offensichtlich nicht vollständig, wie unsere Welt beschaffen ist. Noch.

    Wissen die Maschinen also doch nicht alles? 

Das Ergebnis des Experiments ist für Journalisten und Autoren zunächst beruhigend: Der Algorithmus sagt das nächste Wort oder den nächsten Absatz voraus, kann aber nach wie vor nicht kausal denken. Er kann schlicht nicht ""wissen"", dass es kaum ""winzige Kühe"" gibt und Lacken aus Bratensauce eher selten in den Vorgärten berühmter Schriftsteller auftauchen. ""Man kann die Maschinen Bücher lesen lassen, aber sie haben nicht die kognitiven Fähigkeiten, diese Fakten weiterzuverarbeiten und das Wissen zu strukturieren"", sagt Burchardt.

    Wird es weiter menschlichen Journalismus geben? 

Ein Sprachmodell, das kontrolliert journalistische Texte verfassen kann, wäre wohl irgendwo zwischen einer wortgewaltigen KI wie GPT-2 und dem vergleichsweise simplen Algorithmus, der Sportergebnisse zusammenstellt, gelagert. Doch davon sind wir noch weit entfernt.

Viel früher werden lernende Maschinen als Recherchehelfer für Journalisten zum Einsatz kommen. Aljoscha Burchardt sieht in den Text- und Datenmengen, die in den sozialen Medien entstehen, tatsächlich großes Potenzial: ""Mit maschineller Hilfe könnte man zum Beispiel Argumentationslinien vorsortieren oder Meinungsführer zu gewissen Themen ausfindig machen."" Die These von Mensch gegen Maschine findet der KI-Forscher ""polarisierend"". Die Frage, die man sich eher stellen müsse: Mithilfe welcher Maschinen wird der Journalist in Zukunft seinen Job machen – und wie wird sich seine Arbeit dadurch verändern?";https://www.derstandard.at/story/2000110230471/roboter-journalismus-dieser-text-ist-noch-ki-frei;Standard;Oliver Stajic
27.07.2019;Start-Preis: Die Logik im Finanzdschungel;"Im Gegensatz zu Physik oder Chemie werden im Finanzsektor selten allgemeingültige Regeln formuliert. Das Verhalten der einzelnen Akteure ist vielfältig und oft unvorhersehbar, Versuche können nicht wiederholt werden, und generell scheint die Logik der Börse keinen Naturgesetzen zu gehorchen. Doch es gibt sehr wohl Konstanten, wie das Projekt von Christa Cuchiero zeigt. Sie forscht am Institut für Statistik und Mathematik der Wirtschaftsuniversität Wien (WU) am Schnittpunkt von Mathematik und Finanzwissenschaft. Erst kürzlich bekam sie als einzige Frau einen von den sechs begehrten Start-Preisen, die jährlich vom Wissenschaftsfonds FWF und dem Wissenschaftsministerium verliehen werden.
Finanzmathematikerin Christa Cuchiero ist heuer die einzige weibliche Start-Preisträgerin.
Foto: Privat

Cuchiero erklärt die überraschende Universalität an einigen Beispielen: Da wären einerseits die Kurven der Marktkapitalisierungen. Ein Maß, für das die Börsenkurse mit der Anzahl der im Umlauf befindlichen Aktien eines Unternehmens multipliziert werden. Die Verteilung dieser Werte sehe über die letzten 90 Jahre immer gleich aus – unbeeindruckt von einer Finanzkrise oder einer florierenden Wirtschaft. Und auch bei der sogenannten Volatilität, die Cuchiero nach den Preisen als zweitwichtigste Größe auf dem Finanzmarkt bezeichnet, ist solch ein allgemeingültiges Phänomen zu beobachten: ""Volatilität beschreibt die Schwankungen von Preisen. Und wir haben gesehen, dass sie als mathematische Größe ein besonderes, nämlich raues Verhalten hat."" Wird eine solche stabile Größe erstmals statistisch ermittelt, kann man versuchen, ihr Verhalten zu modellieren und so Voraussagen zu treffen.
Preissetzung und Risikoeinschätzung

Auf mathematischer Seite geschieht das mit sogenannten stochastischen Prozessen: ""Das individuelle Verhalten am Aktienmarkt, also wie einzelne Händler kaufen und verkaufen, ist schwer abzuschätzen"", erklärt die gebürtige Oberösterreicherin. Stochastische Modelle können die Dynamiken der Preise abbilden und berechnen, wie sich ihre Wahrscheinlichkeit verteilt.

Dieses Wissen fließt daraufhin wieder in den Finanzsektor ein und wird dort etwa für Preissetzungen und Risikoeinschätzungen eingesetzt. ""Gerade nach der Finanzkrise ist das wichtig. Wir können so zum Beispiel einschätzen, ob es ein Event geben könnte, an dem das ganze System bankrottgeht."" Universelle Modellierungsansätze spielen auch im Bereich von Maschine Learning und künstlicher Intelligenz eine große Rolle. Ihre Forschungsgruppe an der WU kooperiert deshalb auch eng mit der ETH Zürich. Cuchiero: ""Finanzmathematik und Machine Learning inspirieren sich gegenseitig.""

Schon in der Volksschule habe sie lieber gerechnet als Aufsätze geschrieben, meint die Mathematikerin. Während des Studiums an der Technischen Universität Wien und an der ETH Zürich konzentrierte sie sich auf die Bereiche Stochastik und Wahrscheinlichkeitstheorie. Auch heute fasziniere es sie, dass man Fragestellungen aus der Praxis auf beweisbare, mathematische Aussagen zurückführen könne. ""Das ist für mich, wie Rätsel zu lösen.""

Das Leben jenseits der Wissenschaft verbringt Cuchiero jedoch nicht mit Spekulationen auf dem Aktienmarkt. Viel lieber investiert sie ihre Zeit und Muße in Wintersport wie Skitouren und Tiefschneefahren und in Kinobesuche.";https://www.derstandard.at/story/2000106607592/start-preis-die-logik-im-finanzdschungel;Standard;Katharina Krapshofer
28.08.2019;"Uni-Wien-Rektor Engl: ""Dringend mehr Geld für den FWF""";"Es kommt selten vor, dass ein Rektor einer Universität sagt, dass die Unis nicht mehr Geld benötigen. Noch seltener kommt es vor, dass ein Rektor einer österreichischen Universität sagt, dass die österreichischen Unis nicht mehr Geld benötigen. Am Rande der Technologiegespräche beim Europäischen Forum Alpbach überraschte der Mathematiker Heinz Engl, seit 2011 Rektor der Universität Wien, im Gespräch mit dem STANDARD mit der Aussage, dass momentan nicht die Universitäten mehr Geld benötigen. Das liegt nicht daran, dass bereits eine finanzielle Ausstattung auf internationalem Spitzenniveau gegeben wäre. Doch Engl sieht ein noch dringlicheres finanzielles Sorgenkind: den Wissenschaftsfonds FWF. Um die Grundlagenforschung ausreichend zu dotieren, fordert Engl von der nächsten Regierung eine Erhöhung des FWF-Budgets um zehn Prozent pro Jahr für die nächsten Jahre plus einen zweistelligen Millionenbetrag für die geplante Exzellenzinitiative.

STANDARD: Eines der großen forschungspolitischen Themen, die bei den diesjährigen Technologiegesprächen in Alpbach diskutiert wurden, ist das Forschungsrahmengesetz, das nun in Begutachtung ging. Wie stehen Sie dazu?

Engl: Rahmengesetz – das sagt ja eigentlich schon alles. Es hieß ursprünglich Forschungsfinanzierungsgesetz. Was wir jetzt brauchen – und ich verstehe, dass das erst die nächste Bundesregierung machen kann -, ist ein nächster Schritt in der Finanzierung mit kräftigem Sprung nach vorne. Die Strukturen kann man punktuell sicherlich auch verändern, aber das Entscheidende ist die Finanzierung, denn Konkurrenzfähigkeit – auch in der Industrie – kommt aus der Grundlagenforschung.

STANDARD: Brauchen die Unis mehr Geld?

Engl: In Alpbach wurde heuer ein Vergleich gezogen zwischen dem Spitzenforschungsland Schweiz und Österreich. Sowohl bei der Hochschulfinanzierung wie auch bei der Forschungsfinanzierung liegt Österreich deutlich hinter der Schweiz. Ich erlaube mir jetzt einen plakativen Vergleich: Die Schweiz hat 150.000 Studierende. Die Uni Wien hat 90.000, also 60 Prozent der Schweizer Studierenden. Das Budget der Uni Wien beträgt aber nur zehn Prozent des Schweizer Hochschulbudgets. Wir hinken also deutlich nach, aber was man positiv sehen muss, ist, dass es in der laufenden Periode der Leistungsvereinbarungen einen deutlichen Sprung nach vorne gibt. Die Uni Wien hat jetzt einen Zuwachs von 17 Prozent des Unibudgets. Man kann nicht erwarten, dass so ein großer Abstand auf einmal aufgeholt wird, das würden wir auch gar nicht verkraften. Wir investieren diesen Zuwachs sehr strategisch.

STANDARD: Wie steht es um die Forschungsfinanzierung?

Engl: Auch dazu ist in Alpbach eine Zahl präsentiert worden: Die Förderung von neuen Projekten in der Grundlagenforschung beträgt in Österreich durch den Wissenschaftsfonds FWF unter 300 Millionen Euro. Der Schweizer Nationalfonds ist hingegen mit umgerechnet fast einer Milliarde Euro dotiert. Beim FWF ist jetzt wirklich ein großer finanzieller Schub notwendig. Wir berufen jetzt an der Uni Wien 70 neue Professorinnen und Professoren. Sie alle sind Forscher auf Spitzenniveau, die nun hoffnungsfroh Anträge beim FWF stellen werden. Auch andere Unis expandieren, und die Central European University übersiedelt nach Wien. Allein durch den Zuwachs von hochkarätigen Wissenschafterinnen und Wissenschaftern steigen die Anträge an den FWF. Schon jetzt sind die Genehmigungsraten von Projekten viel zu gering. Deswegen brauchen wir als nächsten Schritt nicht wieder 17 Prozent mehr für die Unis – später gerne wieder. Jetzt brauchen wir dringend eine Aufstockung des FWF.

STANDARD: Was ist Ihre Forderung an die nächste Bundesregierung für das FWF-Budget?

Engl: Es braucht einerseits einen zweistelligen Millionenbetrag pro Jahr, damit man die vielzitierte Exzellenzinitiative sinnvoll umsetzen kann. Dazu werden viele sehr gute Anträge von den österreichischen Unis und außeruniversitären Institutionen kommen, und man braucht genug Geld, um für die wirklich hervorragenden Anträge genügend Spielraum zu haben. Doch auch die Grundfinanzierung der Einzelprojekte ist wichtig. Für diese Basisfinanzierung des FWF braucht es in den nächsten paar Jahren eine Steigerung des Budgets von zehn Prozent pro Jahr. Dann gibt es auch noch andere Programme: Die Forschungsförderungsgesellschaft ist an sich ganz gut ausgestattet, aber auch hier kann man zusätzlich investieren. Wichtig ist auch die Christian-Doppler-Gesellschaft, die auch international ein gutes Modell dafür ist, wie man die Grundlagenforschung und industrielle Forschung verbindet. Das Zentrale ist jetzt aber wirklich, den FWF so zu finanzieren, dass wir mit Ländern wie Deutschland mithalten können. Die Deutsche Forschungsgemeinschaft hat doppelt so viel Geld pro Einwohner wie der FWF. Die Schweiz ist natürlich ein Traumziel, das wir nicht so schnell erreichen werden.

STANDARD: Welche Vorhaben kann die Uni Wien durch die jüngste Budgetsteigerung umsetzen?

Engl: Viele! Durch die Steigerung des Budgets der Uni Wien um 17 Prozent haben wir rund 200 Millionen Euro mehr zur Verfügung. Diese Mittel dienen einerseits dazu – das ist im Gesetz auch festgeschrieben -, die Betreuungsverhältnisse zu verbessern. Wir haben Studienrichtungen, die zum Teil aus allen Nähten platzen. Durch mehr wissenschaftliches Personal werden wir hier die Betreuung verbessern. Weiters verfolgen wir einen sehr strategischen Plan, in Felder zu investieren, in denen es eine intensive Nachfrage nach den besten Wissenschafterinnen und Wissenschaftern in diesem Feld gibt. Konkret sind wir dabei, 70 neue Professorinnen und Professoren zu berufen. Mit circa 20 Personen sind wir bereits in Berufungsverhandlungen, die weiteren 50 werden bis Ende des nächsten Jahres ihre Stelle angetreten haben.

STANDARD: In welchen Fächern wird es weitere Lehrstühle geben?

Engl: Ein Gebiet wurde auch in Alpbach intensiv diskutiert: künstliche Intelligenz und Machine-Learning. Für diesen Bereich sind knapp zehn neue Professuren ausgeschrieben. Weiters ist es uns wichtig, die Digitalisierung disziplinenübergreifend zu verstehen. Sie ist für uns nicht nur ein Thema der Mathematik und Informatik, sondern auch der Geistes- und Sozialwissenschaften, Stichwort Digital Humanities. An der Publizistik wird es beispielsweise eine neue Professur für Computational Communication Science geben. Mit der Medizinischen Universität Wien schreiben wir gemeinsame Professuren im Bereich Gesundheit und Mikrobiom aus. Weiters haben wir eine Professur für Klimaforschung ausgeschrieben und viele andere gesellschaftliche Zukunftsthemen. Diesen großen Schritt haben wir von langer Hand strategisch vorbereitet und er wurde uns nun durch die Budgetsteigerung ermöglicht.

STANDARD: Haben 70 neue Professorinnen und Professoren samt Laboren überhaupt Platz an der Universität Wien?

Engl: Wir sind in gutem Gespräch, ein neues Gebäude zu bekommen. Die Besiedlung dieses neuen Gebäudes muss natürlich strategisch organisiert werden. Es wird in der Innenstadt sein, nahe an unseren anderen Standorten. Das gibt uns die Möglichkeit, zum Teil kleinere Standorte zusammenzuführen.

STANDARD: Die Central European University (CEU) nimmt im Herbst ihren Betrieb in Wien auf, da der Fortbestand in Budapest nicht länger möglich war. Sind Kooperationen mit der Uni Wien geplant?

Engl: Wir sind seit Beginn dieser Diskussion in sehr gutem Gespräch mit der CEU. Wir werden am 16. September eine gemeinsame Tagung an der Universität Wien machen über die Freiheit der Wissenschaft – ein Thema, das besonders für die CEU sehr wichtig ist. Wir haben auch Forschungskooperationen geplant, etwa in der Kognitionsforschung. Wir heißen die CEU jedenfalls herzlich willkommen in Wien.";https://www.derstandard.at/story/2000107854941/uni-wien-rektor-engl-dringend-mehr-geld-fuer-den-fwf;Standard;Tanja Traxler
02.09.2018;"Forscher Hochreiter: ""Künstliche Intelligenz kann jeden Blödsinn lernen""";"Er ist Professor an der Johannes-Kepler-Universität und eine absolute Koryphäe im Bereich der künstlichen Intelligenz: Sepp Hochreiter. Alle großen IT-Firmen wie Google, Apple, Amazon oder Facebook benutzen die von ihm erfundene LSTM-Technologie (Long Short-Term Memory). Beispielsweise beruht die Sprachsteuerung von Alexa darauf. Er spricht darüber, wie die künstliche Intelligenz (KI) viele Wirtschaftsbereiche verändern wird – beispielsweise beim autonomen Fahren oder der Medikamentenentwicklung. Mithilfe der KI werden Tierversuche fast überflüssig, und in der Krebsdiagnostik ist die Maschine bereits besser als der Mensch. Außerdem erzählt er, wie Google ihm indirekt verboten hat, an seiner Technologie weiterzuforschen, welche Gefahren die KI mit sich bringt und warum Europa die ganze Entwicklung verschläft.

STANDARD: Sie haben als Forscher den technologischen Fortschritt bei künstlicher Intelligenz stark geprägt. Müssen wir uns Sorgen machen, dass die KI eines Tages die Weltherrschaft übernimmt?

Hochreiter: Das ist absoluter Schwachsinn. Sollte die KI irgendwann tatsächlich intelligenter sein als der Mensch, warum sollte sie sich mit uns beschäftigen? Siebenjährige Mädchen befassen sich auch am liebsten mit siebenjährigen Mädchen und Fußballfans mit Fußballfans. KIs rosten in unserer Biosphäre. Sie würden die Erde Richtung Weltraum verlassen, wo sie Energieressourcen finden, mit denen sie etwas anfangen kann. Außerdem kontrolliert der Mensch die Maschine – sollte sie feindlich gesinnt sein, wird sie abgedreht.
Die chinesische Roboterdame Jiajia verliert bei Gesprächen mit Menschen nie die Geduld. Eine künstliche Intelligenz verleiht ihr die Gabe, sich zu unterhalten.
Foto: AFP/JOHANNES EISELE

STANDARD: In welchen Branchen gewinnt Ihre Forschung an Bedeutung?

Hochreiter: Wir kooperieren mit VW und Audi bei der Entwicklung selbstfahrender Autos. Diese großen Unternehmen sind allerdings starr und bürokratisch aufgebaut. Der Informationsaustausch dauert zu lang. Deshalb überlegen wir, an der Johannes-Kepler-Universität mit kleineren Partnern ein eigenes selbstfahrendes Auto zu bauen. Das bedarf aber noch viel Planung. Und auch im Gesundheitswesen stehen große Veränderungen bevor.

STANDARD: Inwiefern?

Hochreiter: Eine KI kann Brustkrebs oder Gehirntumore besser diagnostizieren als Menschen. Auch in der Dermatologie werden die Maschinen Menschen bald überholen. Es geht hier rein um die Diagnostik, nicht um die Behandlung. Die KI analysiert und lernt aus Millionen Datensätzen von der ganzen Welt – so viele Fälle kann ein Arzt nie zu Gesicht bekommen.

STANDARD: Betrifft das auch die Pharmazie?

Hochreiter: Pharmafirmen zeigen großes Interesse an KI-basierten Entwicklungsmethoden. Die Maschine kann unerwartete Nebenwirkungen äußerst präzise vorhersagen. Um in der Medikamentenentwicklung alle möglichen Nebenwirkungen zu prophezeien, müssen Millionen Moleküle auf zehntausende biologische Effekte getestet werden. Das wäre für Experimente im Labor viel zu zeit- und kostenintensiv. Die Maschine ist dabei schnell und effizient.

STANDARD: Lassen sich Tierversuche dadurch abschaffen?

Hochreiter: Zur Gänze abschaffen wohl nicht, aber sie lassen sich stark minimieren. Vieles lässt sich präzise ausrechnen, wofür man bisher tausende Versuchsmäuse brauchte. Das macht die Entwicklung auch klar kostengünstiger.

STANDARD: Alle großen IT-Firmen verwenden Ihre LSTM-Technologie. Wie wirkt sich das auf Ihre Arbeit aus?

Hochreiter: Long Short-Term Memory (LSTM) ist eine Technik zur Entwicklung von KI und der Wissensgenerierung durch Erfahrung. Sie ist in jedem Smartphone und sehr vielen Autos verarbeitet. Die Sprachsteuerung von Amazons Alexa basiert darauf. Google hat vor Jahren einmal versucht, mich einzuschüchtern. Sie meinten, jedes Mal, wenn ich eine Idee zum Thema LSTM veröffentliche, setzen sie weltweit 200 Mitarbeiter darauf an, die das Vorhaben schneller umsetzen als ich. Das war anfangs ein Schock, wenn dir so ein Gigant indirekt verbietet, an deiner Erfindung weiterzuarbeiten. Ich habe ihnen daraufhin allerdings erklärt, sie hätten bestimmt mehr Leute, wir in Linz jedoch mehr Kreativität und Hirn.

STANDARD: Dabei wäre Ihre Technologie fast unbemerkt geblieben?

Hochreiter: Ich habe LSTM erstmals in meiner Diplomarbeit 1991 beschrieben, damals hat aber nicht einmal mein Betreuer das Potenzial erkannt. Auch vier Jahre später bei einer Konferenz hat noch kaum jemand verstanden, worum es geht, und auch die Rechenleistungen fehlten. 1997 hat es meine Technologie dann doch in ein Journal geschafft. Patent habe ich dennoch keines darauf.

STANDARD: Welche Gefahren birgt dieser technische Fortschritt?

Hochreiter: Kürzlich hat eine KI anhand von Gesichtszügen die sexuelle Orientierung von Menschen vorhergesagt. Sie lag praktisch immer richtig und war besser als der Mensch. Damit haben wir nicht gerechnet. Das wirft sowohl rechtlich als auch ethisch spannende Fragen auf, wie weit man gehen darf. Soll künftig bei einem Vorstellungsgespräch eine KI analysieren, ob man fleißig ist, verschläft oder kriminell ist?

STANDARD: Gibt es andere bedenkliche Entwicklungen?

Hochreiter: Die Gefahr wächst, dass über soziale Medien die öffentliche Meinung manipuliert wird. KIs und Chatbots produzieren Texte und Kommentare, und niemand merkt, dass Maschinen dahinterstecken. Man kennt das bereits von US-Wahlen. Anderes Beispiel: In den USA wurde für einen Test eine KI angelernt, um Gerichtsurteile zu sprechen. Das hat an sich gut funktioniert, bis eine ernüchternde Erkenntnis kam. Wurde beim Angeklagten die Hautfarbe von Weiß auf Schwarz geändert – sonst nichts -, fiel das Strafmaß deutlich höher aus. Das steckte in den Daten, und die KI hat es den Menschen nachgemacht.

STANDARD: Wer entscheidet, welche Daten gelernt werden sollen?

Hochreiter: Es werden sich neue Jobs entwickeln – Datenkurator wird einer davon sein. Dieser überprüft, ob genügend und die korrekten Daten eingespielt wurden. Beispielsweise ob für selbstfahrende Autos ausreichend Daten über Land- und Stadtfahrten und Witterungen vorhanden sind. Eine KI ist immer nur so gut wie die Daten, die sie bekommt, denn sie lernt auch Fehler von Menschen, die in den Daten stecken. Im Prinzip ist die KI wie ein Kind, man kann ihr jeden Blödsinn beibringen.
Google wollte Sepp Hochreiter indirekt verbieten, an seiner eigenen Technologie weiterzuforschen. Das ließ er jedoch nicht zu.
Foto: JKU

STANDARD: Was heißt das für andere Branchen?

Hochreiter: Natürlich werden viele Jobs verlorengehen, vor allem jene mit einfachen repetitiven Tätigkeiten. Im Marketing eröffnet das viele Möglichkeiten. Produktanpreisungen oder Kundenstromanalysen lassen sich hervorragend von einer KI durchführen. Eine KI lässt sich bei Beschwerdeanrufen bedingungslos den ganzen Tag beschimpfen und beschwichtigt den Kunden weiterhin. Für einen Mensch ist das eine große Belastung. Und auch Liveticker von Sportereignissen funktionieren schon sehr gut mit Bots. Mobilität ist wie eingangs erwähnt ein zentrales Thema. Taxifahrer sind bereits nervös – in Los Angeles hat vor kurzem ein Taxler meine Kollegen mit der Begründung aus dem Auto geworfen, dass sie ihm den Job stehlen. Er hatte rausgefunden, dass sie eine KI-Konferenz besuchen.

STANDARD: Sie haben mehrere Regierungen zu künstlicher Intelligenz beraten. Wie entwickelt sich Europa auf diesem Gebiet?

Hochreiter: Es gibt hier massiven Aufholbedarf. Es mangelt an Infrastruktur. Sowohl die Politik als auch die Industrie drohen hier eine wichtige Entwicklung zu verschlafen. In politischen Gremien – jenen, die Entscheidungen treffen – sitzen meist nur Philosophen und Ethiker. Sprich: in erster Linie Menschen, die nur vor der ""bösen KI"" warnen. Es müssten auch Forscher eingebaut werden, die die Architektur bauen und sich in der Materie auskennen.

STANDARD: Können Sie ein Beispiel für den Aufholbedarf nennen?

Hochreiter: Deutschland und Österreich sind gut im Maschinen- oder Anlagenbau, auf kurz oder lang werden die Technologiefirmen allerdings draufkommen, dass man die Ingenieursleistung leicht zukaufen kann. Google und Facebook nutzen Daten und passen Angebote an den Kunden an. Zum Beispiel könnte man mit Daten von einer Bohrmaschine viel machen. Was macht der Nutzer falsch? Wann geht der Bohrer kaputt? Bei welcher Drehzahl gibt es Probleme etc. – Firmen könnten sich besser auf den Kunden einstellen. Dasselbe gilt für Kühlschränke oder sonstige Haushaltsgeräte. Ich rate den europäischen Firmen, sich nicht aus der Hand nehmen zu lassen, worin man gut ist. Auf einmal baut Google Bohrmaschinen, das muss nicht sein.";https://www.derstandard.at/story/2000086470899/forscher-sepp-hochreiter-kuenstliche-intelligenz-kann-jeden-bloedsinn-lernen;Standard;Andreas Danzer
09.04.2017;Augmented Reality als neue Medienkultur;"Anprobieren, dann in den Spiegel schauen. Die Rituale, die bisher dem Kauf eines Pullovers vorausgingen, könnten bald auf den Kopf gestellt werden. Künftig wird man sich im Geschäft zuerst vor den Spiegel – vulgo Riesendisplay – stellen, wo das Bild des Kunden nach Belieben und entsprechend der individuellen Statur mit virtuellen Kleidungsstücken überblendet wird. So werden flugs dutzende Kombinationen durchprobiert. Nur mehr in die besten Stücke schlüpft man dann auch wirklich hinein.

Stefan Hauswiesner arbeitet mit seinem Grazer Unternehmen Reactive Reality an einem derartigen System. ""Jeder kann am Smartphone Outfits zusammenstellen, sie virtuell anprobieren oder Stars damit ankleiden. Man kann sich vor beliebige Hintergründe versetzen und die Kombinationen mit anderen teilen"", so die Vision des Gründers, dessen Start-up unter anderem die Förderagentur AWS unterstützt.
""Vermischte Realitäten""

Natürlich ist das Prinzip nicht auf Kleidung beschränkt. ""Man wird sich im Strandoutfit an einen Urlaubsort platzieren oder sich in den Lieblingsfilm hineinprojizieren"", so Hauswiesner. Die sozialen Medien werden von ganz neuen Inhalten geprägt sein. Und die PR-Branche wird ein neues Werkzeug haben, um Kleidung, Filme oder Hotels zu bewerben.

Techniken, die Aufnahmen oder Blickfelder mit virtuellen Inhalten anreichern, fasst der Begriff Augmented Reality zusammen. Dieter Schmalstieg, Leiter des Instituts für Maschinelles Sehen und Darstellen der TU Graz – hier forschte auch Reactive-Reality-Gründer Hauswiesner -, sieht mit den Produkten der ""vermischten Realitäten"" ein neues Medium entstehen, ähnlich wie einst die Fotografie oder den Film.

""Die ersten Filme waren verfilmte Theaterstücke, weil noch keine eigenen künstlerischen Konventionen vorhanden waren. Auch in der Augmented Reality muss sich eine eigene Medienkultur erst etablieren"", sagt Schmalstieg, der auch am Wiener Zentrum für Virtual Reality und Visualisierung (VRVis) tätig ist. Fix ist: Die neuen Bildanalysetools werden es einfach machen, diese Medien zu erschaffen. Jeder User wird potenziell zum Mixed-Reality-Künstler.
Maschinelles Lernen

Die Werkzeuge selbst zu erschaffen, ist dagegen weniger einfach. Die Algorithmen, die etwa Hauswiesner und Kollegen für die virtuelle Umkleidekabine mithilfe von Machine-Learning-Techniken entwickeln, extrahieren die individuellen Geometrien einer Person aus dem Bild und schneiden sie pixelgenau aus dem Hintergrund. Die Darstellungen von Kleidungsstücken werden auf ähnliche Art analysiert, verformt und auf die Körpergeometrien übertragen.

Schon jetzt deutet eine Reihe einfacher Anwendungen auf die kommende Ära der ""angereicherten Realitäten"" hin: In TV-Übertragungen werden bei Sportlern entsprechend positionierte Zusatzinfos eingeblendet. Autoparkassistenten reichern Videobilder mit hilfreichen Anweisungen an. Smartphone-Apps bieten Übersetzungen, wenn man sie auf fremdsprachige Texte richtet, oder sie verfremden dank Gesichtserkennungsalgorithmen das aufgenommene Antlitz in comichafter Weise.
Microsofts Hololens

Einen nächsten Evolutionsschritt soll das Nutzbarmachen von Datenbrillen für alltägliche Anwendungen bringen. Microsofts Hololens, die Entwicklern bereits zur Verfügung steht, ist etwa mit Sensorik und Displaytechnik ausgestattet, um 3-D-Darstellungen mit hoher Genauigkeit ins Blickfeld einzublenden. Bis Echtzeitnavigationsdaten punktgenau und alltagstauglich im Brillenglas erscheinen, bis die Waschmaschine durch virtuelle Datenbrillenmenüs gesteuert werden kann, ist allerdings noch eine lange Liste von Problemen zu meistern, betont Schmalstieg.

Das Gesichtsfeld der Datenbrillen ist eng, die unterbringbare Rechenleistung begrenzt. Die Sensorik kann zwar ein 3-D-Bild der Umgebung erfassen, um sie interpretieren zu können, wird aber eine schnelle Datenverbindung zu Onlinediensten – oder deutlich mehr Rechenleistung – benötigt, zählt Schmalstieg Probleme auf.

Nicht zuletzt ist auch der hohe Preis der Technologie noch ein Hemmschuh für Heimanwendungen. Im Kontext der vernetzten Produktion einer Industrie 4.0 gewinnen Augmented-Reality-Applikationen dagegen bereits an Relevanz und geben etwa kontextbezogen Anlagen- oder Wartungsinformationen auf Tablets aus. Schmalstieg und Kollegen starten gerade ein Projekt für virtuelle Zusammenarbeit, die im Industrieumfeld nutzbar ist. ""Dabei kann etwa ein Experte aus der Distanz durch die Augen eines Facharbeiters blicken und mit einem übermittelten 3-D-Modell einer Anlage interagieren. Für den Facharbeiter könnte der Experte durch einen Avatar oder auch nur eine virtuelle Hand repräsentiert werden"", erläutert der Wissenschafter.
Fotos von kritischen Infrastrukturen

Augmented-Reality-Anwendungen, die auch ohne Server- oder Cloudverbindung auskommen, ermöglicht dagegen Ar4.io, wie Reactive Reality ein Unternehmens-Spin-off der TU Graz. Gründer Clemens Arth entwickelt mit seinem Team Algorithmen, die Bildstrukturen auf Basis einer platzsparenden Datenbank erkennen, die auf einem mobilen Gerät vorhanden ist. Im industriellen Umfeld könnte das System etwa von Wartungsarbeitern eingesetzt werden, um gezielt Informationen zu kritischen Infrastrukturen abzufragen, von denen man keine Fotos durchs Netz schicken will.

Augmented Reality wird im Zentrum vieler Spielarten von Mensch-Maschinen-Kommunikation am Arbeitsplatz stehen. Am Software Competence Center Hagenberg (SCCH) in Oberösterreich arbeiten Thomas Ziebermayr und Kollegen beispielsweise an einem Assistenzsystem, das einem Facharbeiter Informationen für komplexe Schweißarbeiten zur Verfügung stellt. Daten zu verwendeten Blechen oder Einstellungen am Schweißgerät können eingeblendet und die passenden Parameter durch den Arbeiter auch gleich über die Datenbrille eingestellt werden.
Röntgenblick

Eine besondere Idee, die schon lange mit Augmented Reality verbunden ist, wurde von Schmalstiegs Gruppe auf spezielle Weise umgesetzt: ein ""Röntgenblick"", der Menschen durch Wände hindurchschauen lässt. Die Forscher statteten dabei eine Drohne mit 3-D-Sensorik aus, die ihre Daten an eine Datenbrille übermittelt. Schmalstieg: ""Die Drohne scannt ihre Umgebung. Aus den Daten wird errechnet, wie die Raumgeometrie aus der Perspektive des Datenbrillenträgers aussieht, und dann auf diese Weise dargestellt. Es sieht dann aus, als würde man durch ein Loch in der Wand schauen.""";https://www.derstandard.at/story/2000055390721/augmented-reality-als-neue-medienkultur;Standard;Alois Pumhösel
09.04.2019;"""Wir haben keine Ahnung"": MIT-Forscherin sieht menschliche KI noch Jahrzehnte entfernt";"Die großen Sprüche klopfen andere: Im Forschungszentrum des renommierten Massachusetts Institute of Technology (MIT) ist Zurückhaltung angesagt, wenn es um Prognosen im Bereich der künstlichen Intelligenz (KI) geht. Während die Branche boomt und IT-Unternehmer wie Elon Musk und Ray Kurzweil sowie Technologie-Philosophen wie Nick Bostrom bereits von digitaler Neuzeit sprechen oder auch vor dem Aufkommen superintelligenter Maschinen warnen, wirkt die Direktorin des neuen KI-Forschungsarms MIT Quest for Intelligence ungleich geerdeter. ""Wir sind gerade erst am Anfang"", sagte Professorin Aude Oliva im Rahmen der MIT Europe Conference in Wien in einem Vortrag über den gegenwärtigen Forschungsstand. ""Von künstlicher Intelligenz auf menschlichem Niveau sind wir noch Jahrzehnte entfernt.""

Gleiches gelte für Unternehmungen, die auf die Verbindung von Gehirnen und Computersystemen abzielen. Unter anderem arbeiten die Firmen Neuralink und Kernel an Schnittstellen, die die menschliche Schaltzentrale mit KI verschmelzen lassen sollen. ""Wir verstehen gerade erst einmal zwei bis drei Prozent des Gehirns"", sagt die Neurowissenschafterin.
Laut Professorin Aude Oliva steht die Menschheit erst am Anfang dessen, was eines Tages als künstliche Intelligenz bezeichnet werden könnte.
Foto: DER STANDARD
Selbstlernend, aber nicht intelligent

Anstatt in Sci-Fi-Visionen zu schwelgen, müsse man sich zunächst vor Augen halten, wo die Forschung aktuell steht. Im Bereich der KI sei man gegenwärtig dabei, eingeschränktes Maschinenlernen auf mehrere gleichzeitige Anwendungen auszuweiten. Wenn man heute von KI spricht, ist damit nicht Intelligenz oder Kreativität im menschlichen Sinn gemeint, sondern Mustererkennung auf Basis riesiger Datenmengen und Prognosen auf Basis von Wahrscheinlichkeitsrechnungen.

So gelingt es, mit Computern Tumore in Röntgenbildern zu identifizieren, Sprachen zu übersetzen oder auch selbstlernende Algorithmen zum Go-Spielen zu trainieren. Intuition, Hausverstand, eine moralische Ebene, kognitive Flexibilität und letztendlich Bewusstsein bleiben für KI laut Oliva noch sehr lang außer Reichweite. Dazu müssten Menschen zunächst herausfinden, wie das Gehirn genau funktioniert und wie man intelligente Systeme bauen kann.
Einsatzfelder und Anwendungsbeispiele von KI-Systemen.
Foto: DER STANDARD
Endlos viele Ideen, kaum Fachkräfte

Dass heutige ""KI"" nicht wirklich intelligent ist, tut dem Run auf Ausbildungsplätze und den Milliardeninvestitionen in Machine-Learning-Anwendungen aber keinen Abbruch. Im Gegenteil: Heutige Entwicklungen zeigen, wie viel bereits mit eingeschränkten Lösungen erreicht werden kann. Die Erfolge selbstlernender Software in Bereichen wie Medizin, Mobilität oder Industrie hätten daher zu Recht einen regelrechten Branchenboom ausgelöst. Der Bedarf an Fachkräften sei aber weiter enorm: Weltweit gibt es MIT-Schätzungen zufolge erst rund 20.000 KI-Spezialisten. Während es an Ideen aus der Forschung nicht mangle, kämen Unternehmen aufgrund mangelnder Expertise kaum mit den Implementierungen nach.
Schon gehört? Edition Zukunft, der Podcast über das Leben und die Welt von morgen.
Es kann schnell gehen

Wie schnell die Entwicklung in spezifischen Bereichen voranschreitet, zeigt sich laut Oliva bei visuellen Erkennungssystemen. Während Algorithmen zur Bilderfassung trotz Millionen Vergleichsbildern gerade einmal einzelne Elemente klassifizieren können, sollen in rund fünf Jahren bereits Systeme verfügbar werden, die kausale Zusammenhänge und Intentionen erkennen. Ein Beispiel: Kann ein heutiges System einen Wanderer aufgrund der Bildgestaltung (blauer Himmel, grüner Berg et cetere) erkennen, soll es künftig möglich sein, anhand des Settings und der Dynamik den Ausgang einer Videoszene zu prognostizieren. Oliva veranschaulicht das mit der Aufnahme einer Talkshow, in der ein Gast anläuft und in die Arme des Moderators springt.
Video: Was selbst für Kleinkinder selbstverständlich ist, ist für KI heute noch unerreichbar.
johnnyk427
Moonshot Kleinkindintelligenz

Auf der anderen Seite verdeutlichen Oliva zufolge für Menschen ganz selbstverständliche Fähigkeiten, wie wenig die Wissenschaft gegenwärtig über Intelligenz weiß. Dazu zieht sie ein bekanntes Experiment des Max Planck Institute for Evolutionary Anthropology heran, das zeigt, dass selbst zweijährige Kinder und sogar Schimpansen selbstlos handeln können. Im obigen Video ist gleich zu Beginn zu sehen, wie ein mit Büchern bepackter Mann wiederholt gegen eine verschlossene Schranktür stößt. Das zweijährige Kind, das den Mann beobachtet, reagiert innerhalb kürzester Zeit und kommt ihm zu Hilfe. ""Wir haben keine Ahnung, wie das funktioniert"", sagt Oliva. Und so groß die Bemühungen sind, Intelligenz zu rekonstruieren und kluge Computersysteme zu entwickeln: Aus Sicht der KI-Forscher müssen die kognitiven Fähigkeiten eines Kleinkinds heute noch als ""Moonshot"" bezeichnet werden. Noch jedenfalls.";https://www.derstandard.at/story/2000100755909/wir-haben-keine-ahnung-mit-forscherin-sieht-menschliche-ki-noch;Standard;Zsolt Wilhelm
17.12.2019;Wie gut ist die Batterie deines Elektroautos?;"Im vergangenen Jahr waren weltweit rund 5,6 Millionen Elektroautos zugelassen, auf Österreich entfielen davon etwa 26.700. Die Tendenz ist weiterhin steigend, Experten rechnen mit einer Verdopplung des Absatzes im kommenden Jahr. Die steigende Anzahl an Neukäufen schafft auch einen florierenden Markt für Gebrauchtwägen. Einziger Haken: Vor allem die Batterie sollte bei gebrauchten Elektroautos unbedingt überprüft werden. Das war bislang nicht so einfach möglich, dabei stellt die Batterie einen erheblichen Teil des Fahrzeugwertes dar. ""Der Wert der Batterie macht ungefähr 50 Prozent des Gesamtwerts des Autos aus"", weiß Wolfgang Berger, wie teuer die Komponente ist. Eine Tauschbatterie kostet zwischen 10.000 Euro und 12.000 Euro.
Eine Fahrt, ein Test

Sein Zugang ist also ein anderer: Wolfgang Berger und sein Partner Nicolaus Mayerhofer wollen es ermöglichen, die Batterie mit nur einer einzelnen Probefahrt komplett durchzutesten. Entwickelt haben die beiden also ein Testverfahren für Li-Batterien in Elektrofahrzeugen. Der Clou: Das Verfahren erfordert lediglich eine kurze Testfahrt, für die Berechnungen werden ML-Daten herangezogen. ""Wir zeichnen große Datenmengen der Batterie auf, beispielsweise Spannung und Temperatur. Diese riesigen Datenmengen werden per Mobilfunk an unser Backend übertragen und in die Cloud übertragen. Letztlich erlauben uns dann Machine Learning-Algorithmen Rückschluss auf den ""State of Health"", also den Gesundheitszustand der Batterie.""
Wolfgang Berger und sein Partner Nicolaus Mayerhofer, Gründer von Aviloo.
Foto: aviloo
Schlechte eigene Erfahrungen

Entstanden ist die Idee ein wenig aus der Not heraus: ""Mein erstes Elektroauto haben ich vor rund drei Jahren aus steuerlichen Gründen angeschafft – und bin dann recht schnell zum E-Auto-Fahrer konvertiert. Also habe ich beschlossen, einen zweiten Wagen für meine Frau anzuschaffen, allerdings gebraucht zu kaufen"", erklärt Wolfgang Berger, wie er auf die Thematik aufmerksam wurde. Das Problem laut Berger: ""Niemand kann dir sagen, wie gut die Batterie ist"". Er habe Händler gefragt, der das allerdings nicht gewusst hätte, und auch der ÖAMTC habe nicht helfen können.

Ein klassischer Reichweitentest war für Wolfgang Berger auch keine Option, spielen dabei doch einige Variablen eine Rolle und lassen damit wieder kein eindeutiges und vor allem vergleichbares Ergebnis zu. ""Ein unabhängiger, neutraler Test war bislang nicht möglich"", schließt Aviloo laut Berger eine Lücke.

Der Kunde muss jetzt nur zu einem Aviloo-Partner fahren, bekommt dort ein kleines Hardware-Kästchen für die OBD-Schnittstelle des Wagens, macht eine Testfahrt und erhält am Ende sein Zertifikat mit seinem Aviloo-Rating. Dieses Aviloo-Rating soll unabhängige und herstellerübergreifend vergleichbare Bewertungen der Batterie ermöglichen. Neun Autos sind derzeit mit dem System kompatibel, darunter der eGolf von VW, der BMW i3 und Modelle von Renault, Nissan, Hyundai und Kia. Pro Test fallen 95 Euro für den Fahrzeughalter an.
Partnersuche läuft

Derzeit gibt es einen Standort in Wiener Neudorf, weitere Standorte sollen aber folgen, wie Wolfgang Berger erklärt: ""Im Q1 2020 möchten wir die ersten Kundentests mit Kundenfahrzeugen gemeinsam mit dem ÖAMTC durchführen. Das Geschäftsmodell sehe außerdem weitere Koop-Partner vor, beispielsweise den ADAC, die Autohändler oder Werkstätten. ""Wir sind der Meinung, dass gebrauchte Elektroautos mit geprüfter Batterie von Aviloo für einen höheren wert verkaufbar sind – immerhin definiert der Batteriezustand den Wert des Autos."" Gut möglich also, dass das Aviloo-Rating in Zukunft den Kauf- beziehungsweise Verkaufspreis von Elektroautos entscheidend mitbestimmt.";https://www.derstandard.at/story/2000112382870/wie-gut-ist-die-batterie-deines-elektroautos;Standard;Oliver Janko
24.06.2019;23 Beispiele, wie künstliche Intelligenz unser Leben verbessert;"Von Robotern, die über die generelle Intelligenz eines menschlichen Gehirns verfügen, mögen wir noch ein paar Jahrzehnte entfernt sein. Doch schon heute nehmen uns lernende Algorithmen Arbeit ab, sorgen für sicherere Diagnosen in der Medizin und beweisen, dass Maschinen sogar künstlerisch tätig sein können. Ein Einblick in die gegenwärtigen und absehbaren Kompetenzen der künstlichen Intelligenz (KI):
Selbstfahrende Autos sollen den Verkehr künftig sicherer machen. Autonome Züge sind schon seit Jahren im Einsatz.
Foto: APA/AFP/Courtesy of Nuro/HO
Robotaxis

Selbstfahrende Autos sind nur ein Teil des autonomen Verkehrs von morgen, an dem seit geraumer Zeit intensiv gearbeitet wird. 2015 verriet der Luftfahrtkonzern Boeing, dass Piloten bei Linienflügen im Schnitt nur noch sieben Minuten manuell steuern würden. Und seit einigen Jahren sind Züge im Einsatz, die ohne Zugführer auskommen. Auch die Wiener U5 wird dazugehören.

Selbstfahrende Autos und Lkws sind eine größere Herausforderung, weil Straßenverkehr chaotisch ist. Aktuelle Autopiloten unterstützen Fahrer dabei, sicherer zu navigieren, sie ersetzen sie aber nicht. Vollautonome Vehikel würden laut McKinsey-Schätzung 90 Prozent weniger Verkehrsunfälle verursachen. Und effiziente Carsharingmodelle könnten einer MIT-Sudie nach für 75 Prozent weniger Taxis auf den Straßen sorgen und gleichzeitig die Wartezeit verkürzen.

Wie bald das alles geschehen wird, darüber streiten sich die Erfinder. E-Auto-Vorreiter Tesla will bereits 2020 eine Roboterflotte haben, und Andrew Ng, Chefforscher von Baidu, glaubt, dass es 2021 so weit sein wird. Pessimistischere Experten sehen diese Zukunft jedoch noch Jahrzehnte entfernt.
Messenger wie jene von Google erstellen bereits selbst passende Antworten inklusive Smileys.
Foto: Google
Digitale Heinzelmännchen

Maschinen werden uns vor allem Arbeit abnehmen, die uns wenig Freude bereitet. Das beginnt im Kleinen: Der lernende Spamfilter von Gmail etwa sortiert bereits 99 Prozent aller unerwünschten E-Mails aus, und Messenger wie jene von Google erstellen bereits selbst passende Antworten inklusive Smileys, um uns das Tippen üblicher Phrasen zu ersparen.

Die Beratungsfirma Capgemini bietet Software, die Finanzberichte lesen und analysieren kann, um relevante Inhalte zu extrahieren. Die Universität von Chicago testete bereits ähnliche Systeme für den Schuleinsatz, um Lehrer bei der Bewertung von Arbeiten zu unterstützen und früh Schwächen und Stärken von Schülern identifizieren zu können.

Und die Chancen stehen gut, dass man trockene Faktenreports künftig nicht mehr selbst schreiben muss: Schon heute fassen Programme Sportereignisse und Börsenberichte für uns zusammen. Das Ziel: weniger Routinetasks und mehr Zeit zum Denken.
Die KI-Lösung ""Let there be color"" kann auf Basis riesiger Bilddatenbanken besser als jeder Historiker alte Schwarz-Weiß-Fotos und -Videos realitätsnah einfärben.

Künstelligenz

Heutigen KIs fehlt das, was wir als Bewusstsein erachten. Trotzdem sind sie in der Lage, Kunstwerke herzustellen. Seit fast zehn Jahren etwa inspirieren uns die Gedichte von Ray Kurzweils Cybernetic Poet, der basierend auf den Ergüssen menschlicher Autoren ins Dichten kommt.

Mit den Plug & Play Generative Networks hat ein Forscherteam um Anh Nguyen einen Generator entwickelt, der selbstständig fotorealistische Bilder erzeugen kann. Aus dem Nichts entstehen so Störche, Vulkane oder auch Klöster. Ebenso spannend ist die KI-Lösung Let there be color (Video), die nachweislich besser als jeder Historiker alte Schwarz-Weiß-Fotos und -Videos realitätsnah einfärben kann.

KI-Komponisten verwöhnen wiederum unsere Ohren. Die Open-Source-Lösung Musenet etwa kann wohlklingende Musikstücke jedes Genres komponieren und dafür zehn unterschiedliche Instrumente gleichzeitig bedienen.
Intelligente Systeme können massiv dazu beitragen, präzisere medizinische Diagnosen zu treffen.
nature video
Maschinen in Weiß

Der erste Computerassistent stand Ärzten bereits in den 1970ern zur Seite. Erfinder Edward Shortliffe konnte zeigen, dass Mediziner mit seiner Software Infektionskrankheiten um bis zu 20 Prozent besser in den Griff bekamen. Seither hat sich viel getan.

2016 demonstrierten Forscher um Dayong Wang, dass Fehldiagnosen bei bösartigem Brustkrebs dank Bilderkennungssoftware um 85 Prozent reduziert werden können. Im Jahr darauf veröffentlichte die Universität Stanford einen Algorithmus, der Hautkrebs besser erkennt als Dermatologen. Ebenso erfolgreich verlief ein Projekt der Uni, um anhand von Röntgenbildern maschinell Erkrankungen im Brustbereich zu identifizieren. Forscher aus Nottingham entwickelten unterdessen eine KI, die genauer als Ärzte prognostizieren kann, ob ein Patient in den nächsten zehn Jahren einen Herzinfarkt oder Schlaganfall erleiden wird.

Basierend auf Millionen Krankendaten können spezialisierte KIs also schon heute die Qualität von Behandlungen steigern und zur Prävention beitragen. Firmen wie Autonomous Healthcare arbeiten nun daran, die vielen Diagnose- und Überwachungsgeräte über ein intelligentes System zu verknüpfen. Davon sollen langfristig nicht nur Patienten auf Intensivstationen profitieren.
KI-Assistenten werden uns in Zukunft immer mehr unter die Arme greifen. Die dahinterstehenden Konzerne sammeln dabei kräftig Informationen über uns.
Foto: Temi
Blechbutler zu Diensten

Bis Siri und Alexa wirklich den Haushalt schupfen können, werden noch viele Daten über den Äther strömen. An humanoiden Assistenten wird allerdings bereits kräftig geschliffen.

Mit der Einführung von neuralen Netzen 2016 wurden große Sprünge bei der maschinellen Erfassung von Sprache gemacht. Digitale Butler erkennen nicht nur Schrift und Stimmen, dank Erfindungen wie Lipnet könnten sie heute auch schon präziser als Menschen Lippen lesen. Mittels der digitalen Stimmreproduktion Wavenet nehmen Roboterstimmen menschliche Züge an, und Suchalgorithmen wie jene von Google und Amazon wissen beängstigend gut über unsere Wünsche Bescheid.

Ein Versuch, diese vielen Skills in ein alltagstaugliches Gefäß zu kippen, ist der autonome Roboter Temi. Er kann dank einer Vielzahl von Sensoren und seines Softwarehirns seinen Herrchen und Frauchen folgen, Notizen, Anrufe und Termine entgegennehmen, Momente als Video festhalten und ein Auge auf die Kinder werfen oder die Katze suchen.";https://www.derstandard.at/story/2000105201099/hilfe-ki-23-beispiele-wie-kuenstliche-intelligenz-unser-leben-verbessert;Standard;Zsolt Wilhelm
16.11.2018;Arbeitsleben: Flexibilität bereits Standard;"Flexibilität, Mobilität, das Verschwinden von Hierarchien – die Arbeitswelt ändert sich rasant. Was die Veränderungen für Unternehmen bedeuten und was von Berufseinsteiger erwartet wird, war Thema beim Jobtalk von Uniport am Dienstag in Wien.

Flexibilisierung und Mobilität sei für Axel Helmert, Geschäftsführer des Softwareentwicklers MSG Life, nur eine Ebene von Arbeit 4.0. ""Die radikaleren Veränderungen werden durch den Einsatz von künstlicher Intelligenz und Machine Learning ausgelöst werden"", sagt er. ""Wir müssen damit rechnen, dass sich die Arbeit in den nächsten 15 Jahren stärker verändern wird als bisher.""

Dass Mitarbeiter für die neuen Anforderungen auch das notwendige Equipment bekommen, sei für Harald Kröger, Head of Financial Institutions, Country & Portfolio Risk Management bei der Raiffeisenbank International (RBI), eine Selbstverständlichkeit. ""Wir können Mitarbeiter nicht in Rahmen zwängen und dann erwarten, dass sie mobile Lösungen entwickeln"", sagt er. Flexible Arbeitszeiten und Remote-Work-Möglichkeiten sind daher selbstverständlich.

Auch bei der KPMG sind flexible Arbeitszeiten und Arbeitsplätze im Bewerbungsgespräch kein Thema. ""Wichtiger werden Sabbatical oder Bildungskarenz, aber auch die Möglichkeit, Teilzeit arbeiten zu können"", sagt Thomas Schmutzer, Director Management Consulting bei KPMG.

Große Veränderungen stehen Wien Energie im nächsten Jahr bevor, da bezieht das Unternehmen ein neues Büro mit offenem Raumkonzept. Für Anja Soffa, Personalentwicklerin bei Wien Energie, werde dadurch die Idee von Arbeit 4.0 verwirklicht. Der unmittelbare Austausch mit den Kollegen werde verbessert, vernetztes Arbeiten erleichtert. ""Außerdem werden wir im Laufe des nächsten Jahres auf Vertrauensarbeitszeit umstellen"", ergänzt sie. Derzeit werde noch getestet.
Reine Vertrauensarbeitszeit

Bereits Erfahrung mit Vertrauensarbeitszeit hat man bei MSG Life. ""Beim Führen über Inhalte braucht es Führungskräfte und Mitarbeiter, die das auch können"", sagt Helmert. Es gebe zwar auch hier Negativbeispiele, aber unterm Strich habe das Unternehmen davon profitiert.

Cordelia Rudolph ist Softwareentwicklerin bei MSG Life, und auch wenn sie von überall aus und zu jeder Zeit arbeiten könnte, versucht sie, so gegen acht im Büro zu sein, dafür hat sie dann auch kein schlechtes Gewissen, wenn der Laptop im Büro bleibt und sie dadurch nicht verleitet ist, am Abend noch schnell die Mails zu checken.

""Die Selbstdisziplin, am Abend nicht ins Postfach zu schauen, muss man erst lernen, sagt Stefanie Podpera, Trainee bei Wien Energie. Erwartet werde es von den Führungskräften jedenfalls nicht, sagen die Anwesenden unisono. Und als Jobeinsteiger dürfe man sich hier nicht unter Druck setzen lassen, ergänzt Podpera. Auch für Julia Fischer, Assistent Manager bei KPMG, sei der Umgang mit den mobilen Devices außerhalb der Bürozeiten ein Lernprozess. ""Führungskräfte sind hier Vorbilder. Und eine kurze Nachfrage, wie dringend das sei, kann auch helfen.""

Für Suhrud Athavale, Junior Credit Risk Analyst bei der RBI, sei der Druck, ständig on zu sein, hauptsächlich selbstgemacht. Und obwohl es von den Vorgesetzten nicht erwartet wird, checkt er dennoch im Urlaub regelmäßig seine E-Mails. ""Einfach um auf dem Laufenden zu bleiben und nach der Rückkehr gleich weiterarbeiten zu können"", ergänzt er.";https://www.derstandard.at/story/2000091141707/arbeitsleben-flexibilitaet-bereits-standard;Standard;Gudrun Ostermann
13.09.2019;"Belästigung durch Penisfotos: Entwicklerin sagt ""Dick Pics"" den Kampf an";"Zahlreiche Frauen mussten im Netz bereits die Bekanntschaft mit ""Dick Pics"" machen. Sie haben per Mail, Messenger oder Direktnachricht unerwünscht das Foto eines Penis zugeschickt bekommen. Manchmal von Bekannten, manchmal von fremden Männern. Das Phänomen der ""Dick Pics"" existiert seit Jahren und wurde bereits in Studien untersucht. Einer US-amerikanischen Entwicklerin reicht es nun. Sie arbeitet an einer Software, mit der die belästigenden Aufnahmen ausgefiltert werden sollen.
""Send nudes for science""

Die Software von Kelsey Bressler kann mittels Machine Learning Penisse auf Fotos erkennen und soll sie künftig aus Online-Konversationen und Direktnachrichten filtern. Um möglichst viel Material für ihre Software zu bekommen, startete die Amerikanerin den Twitter-Account @showYoDiq mit dem Aufruf ""Send nudes for science"" – Nutzer sollen ihr Penisfotos schicken, damit das Programm lernen kann. Voraussetzung: Die Teilnehmer müssen älter als 18 Jahre sowie Urheber der Fotos sein.

Bressler bekam vor einiger Zeit selbst so eine Aufnahme eines fremden Mannes zugeschickt. Ihr Partner habe dann die Idee gehabt, dass sie doch eine Software dagegen schreiben könnte, wie sie nun der BBC erzählte. Ihr Wunsch ist, dass soziale Netzwerke, Dienste wie Twitter und Messenger ihre Software integrieren. Alternativ könnte es auch einen eigenständigen Dienst geben.

Bis der Penisfilter auf die ""Dick Pics"" der Netzwelt losgelassen werden kann, dauert es noch. Bei bisherigen Tests sei aber bereits eine hohe Trefferquote erzielt worden. Damit die Software noch besser wird, startete Bressler im September den ungewöhnlichen Aufruf via Twitter. Das führt auch zu kuriosen Erlebnissen. ""Leute schicken mir ziemlich viele Fotos von Trumps Gesicht. Aber sie versuchen auch, den Filter zu testen oder auszutricksen"", sagt die Entwicklerin zu ""Golem"". Etwa indem Penisse mit Glitter ""getarnt"" oder einem Hotdog-Brötchen ""verkleidet"" werden.
Belästigung

Dass ""Dick Pics"" ein echtes Problem sind, wird in Diskussionen im Netz oft verharmlost. Für Bressler sind die unaufgefordert und ungewünscht zugeschickten Aufnahmen aber nur das Online-Äquivalent zu Personen, die sich auf der Straße entblößen. Exhibitionismus in der Öffentlichkeit ist strafbar und kann in Österreich mit einer Freiheitsstrafe bis zu sechs Monaten belegt werden. Bei ungefragt zugeschickten Penisfotos raten Experten, zur Polizei zu gehen und den Absender der jeweiligen Plattform zu melden. Was man jedenfalls nicht tun sollte, ist, das Foto weiterzuleiten oder öffentlich zu posten. Denn damit kann man sich selbst strafbar machen.";https://www.derstandard.at/story/2000108591298/belaestigung-durch-penisfotos-entwicklerin-sagt-dick-pics-den-kampf-an;Standard;red
05.03.2018;700 Gbit/s: Bislang größte DDoS-Attacke auf Österreich gemessen;"Mit bis zu 700 Gigabit pro Sekunde lief in den vergangenen Tagen eine Distributed Denial of Service-Attacke auf österreichische Server. Es handelt sich laut dem Kärntner IT-Dienstleister Anexia damit um den bisher größten Cyberangriff auf heimische IT-Systeme. Gegolten hat er einem Kunden des Unternehmens, einem ""großen internationalen Serviceprovider"", der eigene Server in Österreich betreibt, erklärt Firmenchef Alexander Windbichler gegenüber dem STANDARD. Drei Tage Daten-Bombardement

Erfasst wurde der erste Angriff am Donnerstag (1. März), als auch das US-Codehosting-Portal Github unter massiven Datenbeschuss kam. Es dauerte laut Anexia einige Minuten, ehe die eigenen Systeme die Traffic-Last entsprechend verteilen konnten. Folgeangriffe, die in hoher Intensität noch bis zum Samstag gemessen wurden, seien ""binnen einer Sekunde"" abgewehrt worden. Die Angriffe würden weiter laufen, aber mittlerweile mit deutlich verringerter Intensität

Das Unternehmen verfügt über ein Infrastruktursystem mit dem Namen ""Backbone Europe"", das über eine Bandbreite von einem Terabit verfügt. Das Traffic-Management bei Cyberangriffen laufe mit einem selbst entwickelten Machine-Learning-Algorithmus, der wie ein ""vollautomatisiertes Bundesheer"" vorstellbar sei. Das System soll auch selbständig von neuen Angriffen lernen.
""Memcached""-Leck erlaubt billige Angriffe

Wer hinter dem Angriff steckt, ist völlig unklar, zumal die Datenflut von Servern rund um die Welt – inklusive Österreich – kam. Es ist davon auszugehen, dass die Verantwortlichen im Dunklen bleiben werden. Ausgenutzt wurde jedenfalls die sogenannte ""Memcached""-Lücke. Dabei werden entsprechende Caching-Server dank einer Schwachstelle als eine Art ""Verstärker"" eingesetzt. Viele Server dürften also ohne Wissen ihrer Betreiber Teil der Angriffe gewesen sein.

Das Leck ermöglicht es derzeit, im Darknet günstig Attacken einzukaufen. Schon im Gegenwert von 50 Euro ließen sich DDoS-Kapazitäten buchen, mit denen dank des ""Memcached""-Bugs größere Server in die Knie gezwungen werden können. Die Schwachstelle bleibt eine Gefahr, solange viele Caching-Server von den Betreibern noch nicht per Aktualisierung abgesichert wurden.";https://www.derstandard.at/story/2000075492832/700-gbits-bislang-groesste-ddos-attacke-auf-oesterreich-gemessen;Standard;gpi
28.05.2019;Erwärmung der Arktis führt in unseren Breiten zu Wetterextremen;"Vor etwa eineinhalb Jahren bestätigten Forscher unter der Leitung der TU Wien erstmals, was ohnehin bereits vermutet wurde: Extreme Wetterlagen und insbesondere häufigere Starkregenereignisse mit entsprechenden Folgen wie Überschwemmungen und Vermurungen hängen zu einem Gutteil mit dem Klimawandel zusammen. Im vergangenen April stellten Wissenschafter vom Potsdam-Institut für Klimafolgenforschung überdies eine Verbindung zwischen bestimmten Wellenmustern im Jetstream und anhaltenden Hitzeperioden auf der Nordhalbkugel her.

Auch in diesem Fall vermuteten die Wissenschafter, dass die globale Erwärmung das Auftreten dieser Wellenmuster beeinflussen würde. Die Annahme konnte nun in einer aktuellen Studie untermauert werden: Der wellenförmige Verlauf des Jetstreams im Winter und die damit verbundenen Extremwetterlagen wie Kälteeinbrüche in Mitteleuropa und Nordamerika stehen laut Atmosphärenforschern des Alfred-Wegener-Instituts (AWI) in direktem Zusammenhang mit dem Klimawandel.
Schwächelnde Winde in zehn Kilometern Höhe

Als Jetstream werden starke Westwindbänder über den mittleren Breiten bezeichnet, die die großen Wettersysteme von West nach Ost dynamisch schieben. Der Wind weht im Bereich der oberen Troposphäre bis zur Stratosphäre in etwa zehn Kilometern Höhe rund um die Erde, wird von den Temperaturunterschieden zwischen Tropen und Arktis angetrieben und erreichte früher Spitzengeschwindigkeiten von bis zu 500 Kilometern pro Stunde.

Allerdings zeigten Beobachtungen in jüngerer Zeit, dass sich der Jetstream mittlerweile immer wieder abschwächt. Er weht dann seltener auf einem geradlinigen Kurs parallel zum Äquator, sondern schlängelt sich häufiger in Form riesiger Wellen über die Nordhalbkugel.
Kaltlufteinbrüche aus der Arktis

Diese Wellen wiederum führen den Wissenschaftern zufolge im Winter zu ungewöhnlichen Kaltlufteinbrüchen aus der Arktis in die mittleren Breiten – zuletzt im vergangenen Jänner, als der Mittlere Westen der USA von extremer Kälte heimgesucht wurde. Im Sommer dagegen verursacht ein schwächelnder Jetstream lang anhaltende Hitzewellen und Trockenheit, wie sie in Europa unter anderem 2003, 2006, 2015 und 2018 zu erleben waren.

Diese grundsätzlichen Zusammenhänge sind zwar dem AWI zufolge seit einiger Zeit bekannt. Forschern gelang es aber bisher nicht, den Schlängelkurs des Jetstreams in Klimamodellen realistisch zu reproduzieren und einen Zusammenhang zwischen dem schwächelnden Jetstream und den globalen Klimaänderungen herzustellen. Diese Hürde nahmen die Wissenschafter nun, indem sie ihr globales Klimamodell um einen innovativen Baustein der Ozonchemie ergänzten.
Algorithmus hilft Wechselwirkungen zu verstehen

""Wir haben einen Machine-Learning-Algorithmus entwickelt, welcher es uns erlaubt, die Ozonschicht als interaktives Element im Modell darzustellen und daher die Wechselwirkungen aus der Stratosphäre und der Ozonschicht mit zu berücksichtigen"", erläutert der Erstautor der Studie und AWI-Atmosphärenforscher Erik Romanowsky. ""Mit diesem Modellsystem sind wir jetzt in der Lage, die beobachteten Veränderungen im Jetstream realistisch zu reproduzieren.""

Mithilfe des neuen Kombimodells können die Forscher nun auch die Ursachen des Jetstreamschlängelns genauer untersuchen, wie sie im Fachjournal ""Scientific Reports"" berichten. ""Unsere Studie zeigt, dass die Veränderungen im Jetstream zumindest teilweise vom Rückgang des arktischen Meereises verursacht werden"", erklärt der Leiter der AWI-Atmosphärenforschung, Markus Rex. ""Sollte die Eisdecke weiter schrumpfen, gehen wir davon aus, dass die bisher beobachteten Extremwetterereignisse in den mittleren Breiten in ihrer Häufigkeit und Intensität zunehmen werden.""

""Unsere Ergebnisse untermauern zudem, dass die häufiger auftretenden winterlichen Kaltphasen in den USA, Europa und Asien der Klimaerwärmung nicht widersprechen, sondern vielmehr Teil des menschengemachten Klimawandels sind"", fügt Rex hinzu.";https://www.derstandard.at/story/2000103956334/erwaermung-der-arktis-fuehrt-in-unseren-breiten-zu-wetterextremen;Standard;red,APA
11.02.2018;Wer forscht, bestimmt die Zukunft;"Österreichs Universitäten bekommen mehr Geld. Das ist vorrangig nicht unserer derzeitigen Regierung, sondern dem ehemaligen Wissenschaftsminister Reinhold Mitterlehner zu verdanken. Ehre, wem Ehre gebührt. Die zentralen Argumente für die Finanzspritze bilden die Lehre und die teilweise katastrophalen Betreuungsverhältnisse an den heimischen Universitäten. Im Vergleich zur ETH Zürich, die pro Professur 40 Studierende zählt, verzeichnet die TU Wien beispielsweise ein Verhältnis von 120 Studierenden pro Professorin oder Professor.

Was in den Wortmeldungen und Diskussionen der letzten Tage allerdings fehlte, war die Rolle der Universitäten als Forschungseinrichtungen. Sie sind dazu verpflichtet, Wissen zu generieren, Neues zu schaffen und Bestehendes zu hinterfragen. Abseits des Themas Studienplatzfinanzierung werden damit drängende Fragen offengelassen. Es bleibt Hannes Androsch überlassen, quasi als einziger öffentlich auf diese Fehlentwicklung hinzuweisen.
Zwei Milliarden fehlen

Bisherige Regierungsprogramme haben das nie erreichte Ziel von zwei Prozent des Bruttoinlandseinkommens (BIP) für Universitäten zumindest niedergeschrieben (für das Jahr 2020). Heute wird dieses Ziel nicht einmal mehr erwähnt.

Zurzeit befinden wir uns leicht unter 1,5 Prozent des BIP. Die aktuelle Erhöhung von 1,35 Milliarden für den Zeitrahmen 2019 bis 2021 mitgerechnet, fehlen uns immer noch rund zwei Milliarden Euro pro Jahr, um die zwei Prozent zu erreichen. Abgesehen davon ist von einer Exzellenzinitiative für Universitäten wie in Deutschland keine Rede mehr.

In einem Standard-Kommentar im Februar des Vorjahres habe ich auf die dringende Notwendigkeit für Investitionen in die Forschung hingewiesen – insbesondere der Informatik und Digitalisierung. Nach einem Jahr bleibt festzuhalten, dass sich wenig geändert hat. Ganz im Gegenteil.
Blick aus der Röhre

Wenn wir uns in der Welt und in Europa umsehen, wird unser Rückstand immer größer. Die Schweiz prescht mit ihrem auf zwei Milliarden Franken dotierten ""digitalen Manifest"" für Wissenschaft und Bildung nach vorne. Mit Mitteln der Industrie schafft auch Deutschland 100 neue zusätzliche Professuren in der Informatik und an der Schnittstelle zur Informatik.

Berlin reagiert auf die Zeichen der Zeit mit etwa 50, Bayern immerhin mit 20 Informatikprofessuren. Ähnliches geschieht in England oder auch China, wo gerade massiv in Forschung im Bereich Artificial Intelligence investiert wird. Der Blick aus der Röhre zeigt, dass wir Gefahr laufen, unsere besten Forscherinnen und Forscher zu verlieren.

Uns könnte ein Braindrain der IT-Spitze bevorstehen. Ähnliche Tendenzen gibt es in anderen Disziplinen, wie der jüngste Abgang des Genetikers Josef Penninger nach Kanada zeigt. Dabei verfügt Österreich – positiv vermerkt – in der Informatik über wirkliches Innovationspotenzial.

Wer forscht, bestimmt die Zukunft. Und die Zukunft braucht mehr Informatik. Das zeigen Entwicklungen in den Bereichen ""Internet of Things"", ""Cyber Physical Systems"", ""Artificial Intelligence"" (AI) und ""Machine Learning"", die schon jetzt massiv alle Bereiche unseres Lebens beeinflussen.

Die Forschung in diesen Bereichen bestimmt die Art und Anzahl unserer Arbeitsplätze, unser Innovationspotenzial, aber auch unsere sozialen und ökonomischen Abhängigkeiten, etwa die Abhängigkeit Europas (und der Welt) von wenigen US-amerikanischen Internet-Plattformen.
Grundlagenforschung

Damit Österreich ein Innovation-Leader wird – dieser Anspruch wurde vielfach geäußert – und nicht nur Follower bleibt, benötigen wir zuallererst Grundlagenforschung. Einen empirischen Beleg dafür bietet das sogenannte Tire Tracks Diagram des Computing Community Consortium (CCC) in den USA.

Es veranschaulicht, dass in Bereichen wie Networking, AI/Robotics, Software-Technologies oder Personal Computing universitäre und teilweise private Forschung ungefähr 20 bis 30 Jahre dem Entstehen von Multimilliarden-Märkten vorangeht. Es braucht also einen langen Investitionsatem, um große Sprünge zu machen. No high risk, no fun!

Der ehemalige Wissenschaftsminister Harald Mahrer hat es vorigen Sommer in Alpbach schön formuliert, als er ein Ende der Schwimmbeckenrandpolitik gefordert hat.

Forschung ist Work in Progress. Dabei geht es nicht nur um das bessere Nutzen von Systemen oder um leichte Adaptierungen. Mehr noch geht es darum, mit eigenständigen Forschungsleistungen gestaltend einzugreifen und im internationalen Wettlauf mitzumachen. Hier gibt es wichtige Forschungsfragen und Entwicklungsaufgaben in der Informatik und an ihren Schnittstellen. Etwa Sicherheits- und Datenschutzfragen beim Design von Systemen, die auf Machine-Learning basieren.

Dahinter steht die Entwicklung in Richtung informationeller Selbstbestimmung und Partizipation, um das verantwortungsvolle Verhalten von Bürgerinnen und Bürgern in dieser digitalen Welt zu ermöglichen.

Wo werden wir in zehn Jahren stehen? Wo wollen wir stehen? Werden wir den Entwicklungen nur folgen oder die Welt mitgestalten? Sind wir bereit, technische Innovation mit sozialer zu verbinden? Veränderung geschieht nicht automatisch, wir müssen uns dafür einsetzen.

Dieser Appell gilt deswegen nicht nur unserer Bundesregierung, sondern auch den Ländern und den Universitäten selbst. Lassen wir den Beckenrand hinter uns und springen wir ins tiefe Wasser.";https://www.derstandard.at/story/2000074048697/wer-forscht-bestimmt-die-zukunft;Standard;Hannes Werthner
06.11.2019;Wiener Entwicklungshilfe für Legal-Tech-Start-ups;"Unzählige Tätigkeiten in der Rechtsbranche können durch Digitalisierung schneller und effizienter werden. Das gibt Start-ups im Legal-Tech-Bereich ein weites Feld für Innovationen und Ideen. Der vor einem Jahr von sieben Wiener Anwaltskanzleien gegründete Legal Tech Hub Vienna (LTHV), dessen Accelerator solche Start-ups fördern und an die Bedürfnisse der heimischen Rechtsbranche heranführen will, hat sich in seinem zweiten Durchgang dennoch zu einer Fokussierung entschieden.

Die fünf Technologieunternehmen, die für den zweiten Batch ausgewählt wurden, beschäftigen sich mit Prozessoptimierung innerhalb von Kanzleien, verbesserte Mandatsarbeit und die Umsetzung des ""Know Your Customer (KYC)""-Prinzips in den europäischen Geldwäscherichtlinien, sagt Gudrun Stangl, Partnerin bei Schönherr und Vorstandsmitglied des LTHV.
Tool für Geldwäscheprüfung

Eines von den Unternehmen, die im Rahmen des Hubs von Coaches begleitet wurden und mit Juristen die Praxistauglichkeit ihrer Lösungen getestet haben, ist die österreichische Firma 360Kompany, die Zugang zu elektronischen Firmenbüchern und Handelsregistern sowie KYC-Prüfungen für Anwälte, Wirtschaftstreuhänder und Banken anbietet.

Auch der Luxemburger Start-up Smart Oversight hilft mit seiner Software bei der Anwendung von Antigeldwäschebestimmungen.

Die drei anderen Firmen – Juralio aus den Niederlanden, Closed aus Frankreich und Bigle Legal aus Spanien – wollen die tägliche Arbeit von Anwälten erleichtern. ""Wir suchen Lösungen zur Prozessoptimierung, mit denen wir tatsächlich unsere Mandatsarbeit verbessern, schneller werden und weiterlernen können"", sagt Stangl.
Musterverträge

Bigle Legal wurde von zwei spanischen Brüdern, einem Anwalt und einem Manager, gegründet und bietet ein Programm, das die Erstellung von Musterverträgen innerhalb von Kanzleien automatisiert. Statt dass der Anwalt sich aus zahlreichen Vorlagen einen neuen Vertrag zusammenkopiert, wird hier ein Grundmuster erstellt und dann mit gezielten Fragen erweitert und angepasst.

Per Link kann auch der Mandant in diesen Vorgang eingebunden werden. Die Software sei benutzerfreundlicher als vergleichbare Produkte auf dem Markt und helfe, die Fehleranfälligkeit zu verringern, sagt Lukas Schmidt, Rechtsanwalt bei Dorda, der in der LTHV-Jury sitzt.

Das Produkt sei bereits in Spanien und den Niederlanden auf dem Markt und werde nun für Österreich angepasst. Als nächsten Schritt wollen die Entwickler Machine-Learning-Elemente in die Plattform integrieren.
Projektmanagement

Closed wurde von ehemaligen französischen Anwälten gegründet und soll den mühsamen Prozess bei Unternehmenstransaktionen – etwa Übernahmen, Joint Ventures oder Finanzierungen – mit ihren oft seitenlangen Listen an Dokumenten vereinfachen.

""Derzeit gibt es meist eine lange Liste im Wordformat mit Verantwortlichkeiten, Deadlines und Remindern, die zwischen den Anwälten hin und her geschickt wird"", sagt Schmidt. ""Es ist gescheiter, dies über eine volldigitalisierte Projektmanagementplattform zu machen.""

Es gebe zwar zahlreiche Projektmanagementtools auf dem Markt, doch die seien weniger gut zugeschnitten für die besonderen Bedürfnisse der Anwaltsbranche, sagt Schmidt. ""Es gefällt Anwälten, wenn sie in digitaler Form etwas erhalten, was so aussieht wie die Listen, die sie in den letzten 30 Jahren verwendet haben.""
Digitale Plattformen

Ein Ziel im Accelerator sei es gewesen, eine Lösung für Closed auszuarbeiten, mit dem der Signiertermin beim Notar völlig digital ablaufen kann, sagt Schmidt. Dann müssten ausländische Partner dafür nicht mehr nach Österreich reisen. Notwendig sei dafür allerdings noch eine gesetzliche Anpassung; bisher ist nur die digitale Gründung einer GmbH möglich.

Eine ähnliche digitale Plattform, allerdings nur für die Phase zwischen Signing und Closing einer Transaktion, hat Schönherr kanzleiintern entwickelt und wurde dafür im Wettbewerb ""Promoting the Best"" der Vereinigung Österreichischer Unternehmensjuristen (VUJ) und Women in Law ausgezeichnet. Noch habe keine in den LTHV involvierte Kanzlei einen Vertrag mit einem der Start-ups abgeschlossen, ""aber die Gespräche laufen sehr gut"", sagt Stangl. Die intensive Arbeit habe sich bereits ausgezahlt, auch weil sich Wien und die angrenzende CEE-Region als Standort für Legal-Tech-Entwicklungen dadurch etabliere.

Längerfristiges Ziel sei es, Produkte mit echter künstlicher Intelligenz zu entwickeln, doch vorerst bleibe man im semiautomatisierten Bereich zur Prozess- und Back-Office-Optimierung.

Konkret ist die Internationalisierung des Hubs, betont Stangl. ""Wir würden ihn gerne europaweit sehen, nur dann können wir gegenüber Hubs in Asien und Großbritannien eine gute Figur machen."" Als erster Schritt sei 2020 eine Ausweitung ins Baltikum vorgesehen, wo vor allem Estland als Vorreiter der Digitalisierung gilt.

Dazu kommt politische Lobbyarbeit: Ein White Paper wurde ausgearbeitet, das die bestehenden rechtlichen Hürden für Anwälte beim Cloud-Computing anspricht. Hier wolle man mit der Rechtsanwaltskammer Wien zusammenarbeiten, sagt Stangl.";https://www.derstandard.at/story/2000110701180/wiener-entwicklungshilfe-fuer-legal-tech-start-ups;Standard;Eric Frey
24.03.2019;IT-Kräfte gesucht: Das Griss um Diven und Prinzessinnen;"Die heimische Betriebsansiedlungsagentur ABA wirbt neuerdings um IT-Fachkräfte im Ausland. Man hat sich dafür extra Ex-Ditech-Co-Gründerin Aleksandra Izdebska geholt, die EU-weit nach den begehrten Spezialisten angeln soll. Es ist noch nicht all zu lange her, da hat Wirtschaftsministerin Margarete Schramböck (ÖVP) Brieflein an Unternehmen verschickt, sie mögen doch Coding-Lehrlinge ausbilden. Doch ist es um die Fachleute hierzulande tatsächlich so schlecht bestellt, wie Branchenvertreter klagen? Laut Wirtschaftskammer können derzeit zumindest 10.000 IT-Stellen nicht besetzt werden.

Eine aktuelle Analyse des Jobportals Stepstone legt nahe, dass das Ächzen nicht unbegründet ist. ""Kreativer Kopf für innovatives Team, leidenschaftliche Entwicklerinnen mit Weitblick, ambitionierte Entwicklerinnen mit fundierten Kenntnissen"", wer einschlägige Plattformen durchforstet, stößt schnell auf Anzeigen wie diese. Stepstone hat eine halbe Million Stellenanzeigen ausgewertet. Experten aus IT und technischen Berufen sind demnach besonders begehrt: Sie haben mit 28 Prozent bundesweit den größten Anteil an Ausschreibungen. In Wien (29 Prozent) und Oberösterreich (38 Prozent) liegt der Anteil noch deutlich höher.
Digitalisierung kommt an

Dabei gehen IT-Kräfte selten aktiv auf Suche, sagt Rudi Bauer, Geschäftsführer von Stepstone Österreich. Nicht länger als drei bis vier Tage seien sie am Markt. Wundern kann er sich darüber nicht: ""Die Digitalisierung kommt bei kleinen und mittleren Unternehmen in ländlichen Regionen an. Dafür suchen sie verstärkt Experten."" Auch aktuelle Zahlen des Arbeitsmarktservice legen nahe, dass das Klagen nicht unbegründet ist. 628 offenen Stellen für akademisch ausgebildete IT-Kräfte standen im Februar 243 Arbeitslose gegenüber. 539 offene Stellen auf Maturaniveau trafen auf 216 Jobsuchende. Insgesamt ist die Lücke beim AMS aber überschaubar. In Summe gab es im IT-Bereich 2.596 offene Stellen – gegenüber 2.239 Arbeitslosen.

Für Peter Lieber ist auch das ein Teil des Problems. Lieber hat als Präsident des Verbands Österreichischer Software-Industrie (Vösi) einen guten Überblick, was in der Branche läuft. Auf eine Zahl will er sich gar nicht festlegen ""sie wird jedes Jahr höher werden, denn vermutlich ist jede Fachkraft in Zukunft IT-affin"" sagt er. 10.000 Fachkräfte oder mehr, diesen Zahlen stünden im Endeffekt sogar 6800 arbeitslose IT-Fachkräfte gegenüber. Über 50-Jährige seien von den Betrieben aussortiert worden, sagt er. Was mit den Kosten zu tun habe, aber nicht nur. ""Viele sind auch nicht mehr bereit, sich entsprechend weiterzubilden.""
Gute Voraussetzungen

Was er etwas sarkastisch anfügt: Gesucht werden am besten 19-Jährige mit einer Erfahrung von fünf Jahren. Lieber nimmt vor allem die Betriebe in die Pflicht: ""Unternehmen sind in immer mehr Branchen immer weniger bereit, die Leute auszubilden."" Dem Argument so mancher Chefs, das hinter vorgehaltener Hand zu hören ist, ""da bilde ich die Jungen aus, und dann laufen sie davon"", kontert er: ""Man muss die Leute dann auch fair bezahlen."" Es gelte eben, über einen längeren Zeitraum zu investieren.

Die Voraussetzungen im Bildungssystem im Sinne technischer Bildung seien in Österreich jedenfalls geradezu paradiesisch. Nicht nur große IT-Unternehmen, sondern etwa auch Handelsriesen oder andere Branchen müssten in größerem Ausmaß ausbilden, findet er. Denn die Crux in der heimischen IT-Struktur: 70 Prozent sind EPUs und damit kein Ausbildungsbetrieb. 68.000 Betriebe im Bereich Unternehmensberatung/IT bilden insgesamt keine 900 Lehrlinge aus. Die meisten Lehrstellen gibt es in der Industrie.
Konkurrenz mit anderen Betrieben

Man bilde ohnehin aus, kontert Nicole Berkmann vom Handelsriesen Spar. Wenn auch nicht unbedingt die gefragten IT-Spezialisten. Den Mangel als solchen bestätigt auch sie. In Salzburg konkurriere man mit großen Betrieben wie Porsche Informatik oder Palfinger um IT-Kräfte. Spar hat reagiert und nahe Villach ein ausgelagertes IT-Zentrum errichtet, weil dort die Konkurrenz um die Kräfte geringer ist und mehr ITler verfügbar sind. Während bei Spar derzeit von 2.400 Lehrlingen zwei mit der der neuen Lehre zum E-Commerce-Kaufmann begonnen haben (50 E-Commerce-Lehrlinge bildet der Handel bisher aus, Anm.), ist die neue Coding-Lehre bei Spar gar nicht gefragt. Peter Lieber hält von dieser Art der Ausbildung ohnehin nichts: ""Coding ist die unterste Schublade"", ein Entwickler denke größer: ""Das sind Prinzessinnen, Divas, Künstler.""

Alfred Harl, Obmann des Fachverbands Unternehmensberatung und IT (Ubit), sieht das anders: 386 Coding-Lehrlinge gebe es bereits. Durchaus ein Erfolg, der sich der Digitalministerin verdanke. Die Regierung mache tatsächlich ""überraschend viel richtig"", sagt auch Lieber. Sie rufe auf, zu kooperieren und Netzwerke zu bilden. Am Ende bleibe aber nichts anderes übrig: ""Die letzten Meter müssen die Firmen gehen."" Man müsse es nun schaffen, endlich Frauen für die Branche zu gewinnen und zusehen, dass die vielen Informatik-Studienabbrecher vielleicht ihr Glück in der Coding-Lehre suchen, findet Harl.";https://www.derstandard.at/story/2000100060786/it-kraefte-gesucht-das-griss-um-diven-und-prinzessinnen;Standard;rebu
24.02.2019;Ein Auge für Datenberge;"Im Zuge der Digitalisierung werden in den Anlagen der Fertigungsbetriebe immer mehr Sensoren verbaut. Sie überwachen Umweltbedingungen, Zustand und Funktion der Maschinen.

Aus den Datenbergen, die sie anhäufen, lassen sich vielfältige Erkenntnisse ziehen. Unter welchen Bedingungen wird die beste Produktqualität erzielt? Wie lässt sich der Ausschuss verringern? Wann muss ein Gerät gewartet werden, bevor ein Schaden eintritt?

Der Weg zur Beantwortung dieser Fragen ist oft nicht einfach, auch wenn Datenmaterial in Hülle und Fülle vorhanden sind. ""Man muss die notwendigen Algorithmen richtig einsetzen, um Muster und Zusammenhänge zu erkennen. Daran scheitern Fachexperten in Betrieben aber oft, weil sie mit solchen Methoden unzureichend vertraut sind"", sagt Thomas Mühlbacher vom Wiener Zentrum für Virtual Reality und Visualisierung (VRVis).

Der 1987 geborene Wissenschafter hilft dabei, Werkzeuge zu entwickeln, die die Daten visuell aufbereiten, um sie auf diese Art für Menschen zugänglicher und verständlicher zu machen.

Die Visual-Analytics-Ansätze, die die Daten in interaktive Grafiken, Diagramme und Modelle verwandeln, helfen den Anwendern zuerst einmal dabei, Probleme in geeigneter Art zu formulieren und die richtigen Fragen zu stellen. Danach können sie helfen, die richtigen Antwort zu geben, etwa indem sie die Entwicklung, Validierung, Anwendung und Verbesserung einer Machine-Learning-Lösung begleiten.

""Beispielsweise können tausende Sensorkurven übereinandergelegt werden, um Abnützungserscheinungen einer Maschine genau abzuschätzen"", erläutert Mühlbacher.
Lernfähige Systeme

Der Forscher, der an der TU Wien Computergrafik studierte, arbeitet seit 2012 am VRVis, das als K1-Zentrum im Rahmen des Comet-Programms von der Förderagentur FFG mit Mitteln aus dem Wirtschafts- und dem Verkehrsministerium finanziert wird. Im Zuge seiner Arbeit am Forschungszentrum schrieb er auch seine Dissertation zur Anwendung von Visual Analytics im Bereich lernfähiger Systeme.

Lange Zeit lag der Anwendungsschwerpunkt seiner Arbeit im Energiebereich. Dazu gehörten etwa Fragen, wie man die wetterabhängige Energieversorgung durch Solar- und Windkraftanlagen besser planbar macht.

Das Visual-Analytics-Werkzeug Visplore, an dem Mühlbacher aktuell arbeitet, soll künftig auch mithilfe eines Spin-off-Unternehmens vermarktet werden. Es soll Industriebetrieben dabei helfen, ihre Datenberge besser in den Griff zu bekommen.

Dem Weg in die Computergrafik ging Mühlbachers Begeisterung für Videospiele voraus. Ausgleich zur Computerarbeit fand der in Kaisermühlen aufgewachsene Wiener bis vor kurzem als Mitglied einer Heavy-Metal-Band ""mit Plattenvertrag und allem Drum und Dran"".

Steht die Entwicklung eines Computerspiels, wie er es mit Studienkollegen einst vorhatte, auch heute noch auf dem Plan? ""Vielleicht in der Pension"", antwortet Mühlbacher. Immerhin: Manche Aspekte dieses Faibles würden sich in der dynamischen und oft experimentellen Interaktion mit den visuell aufbereiteten Daten, die seine Forschungsarbeit ausmachen, durchaus wiederfinden.";https://www.derstandard.at/story/2000098233213/ein-auge-fuer-datenberge;Standard;Alois Pumhösel
05.11.2018;Menschengerechte Roboter bauen;"Seit April baut Martina Mara ihr In stitut für Roboterpsychologie an der Linzer Johannes-Kepler-Universität (JKU) auf. Im Gespräch mit der APA sagt sie, dass sie sich neue Bilder von Robotern wünscht, weg vom Terminator hin zu einem sympathischen Werkzeug, das mit Menschen zusammenarbeitet.

Die 37-Jährige promovierte 2014 in Psychologie über die Nutzerwahrnehmung menschenähnlicher Maschinen und arbeitete in den vergangenen zehn Jahren im Futurelab, der Forschungseinrichtung des Ars Electronica Center in Linz, das sich intensiv mit Zukunftsfragen beschäftigt. Ein Zusammentreffen mit Robotikprofessor Hiroshi Ishiguro aus Osaka, der mit seinen Geminoids Roboter so menschenähnlich wie möglich bauen will, weckte das Interesse an der Reaktion der Menschen auf die ihnen ähnlichsehenden Maschinen. Den Ansatz, den Menschen nachzubauen, findet Mara aber absurd. ""Die Forschung zeigt, dass sich viele Menschen damit schwertun, wenn man Mensch und Maschine kaum noch unterscheiden kann. Dann machen wir es uns doch leichter und gestalten Roboter so, dass sie klar als Maschinen erkennbar bleiben.""

Künstliche Intelligenz humanoid?

Generell sieht sie die Repräsentation von künstlicher Intelligenz (KI) und Robotik im öffentlichen Raum fast ausschließlich als Humanoiden und Androiden kritisch. ""Das befeuert Ängste, den Menschen als Gesamtes, also in kognitiven, sozialen wie emotionalen Aspekten, zu ersetzen."" Diese sind für Mara unbegründet, da müsse man weg vom Ersetzen hin zum Ergänzen. Natürlich haben Roboter und KI den Menschen in bestimmten Domänen etwas voraus, ""gleichzeitig gibt es sehr viele Bereiche, in denen ein Roboter dem Menschen niemals das Wasser reichen können wird"".

Ihrer Ansicht nach solle man eher die Synergien nutzen und mit Unterstützung der Maschinen wichtige Zukunftsfragen angehen, etwa in der medizinischen Diagnostik oder in Fragen des Klimawandels. Nicht der Terminator, wie sich viele Menschen die künstliche Intelligenz vor Augen führen, sondern das sympathische Werkzeug ist für Mara ein wünschenswertes Bild. Etwa in der Pflege, wo es nicht um ""scheinempathische Humanoiden am Bett von der Oma"" gehe, sondern um echte Unterstützung des Pflegepersonals mit Transportrobotern, die im Hintergrund schon Bettwäsche und Medikamente bringen.

""Mein Ansatz ist auch, dass Roboter so intuitiv werden, dass selbst wenig geschulte Menschen mit ihnen zusammenarbeiten können"", sagt Mara und denkt dabei an ältere oder wenig technikaffine Leute.

Kollaborative Robotik

In der Industrie sind Roboter bereits gang und gäbe – allerdings als teilweise riesige Maschinen und aus Sicherheitsgründen streng getrennt von Menschen. Mit der kollaborativen Robotik solle sich das ändern. Diese sogenannten Cobots sollen so sicher gestaltet sein, dass sie physisch nahe mit Menschen arbeiten, gemeinsam Teile bauen können. Mara will wissen, wie die Cobots sich verhalten sollen, damit Menschen sich bei der Zusammenarbeit wohlfühlen. ""Wichtig ist, dass man den Menschen ermöglicht, die Maschine zu verstehen, und sie kommunizieren kann, was ihre nächsten Schritte sind."" Lichtsignale sind ein Ansatz, ein anderer ist das Bewegungsdesign.

Wenn der Roboter in einer kurvigen statt linearen Bewegung nach etwas greift, wird er zwar ineffizienter, aber der Mensch versteht schneller, was der Roboter vorhat und kann mit seinem Arbeitsschritt früher beginnen, wodurch das Team insgesamt schneller wird. ""Das finde ich total spannend, da sehe ich noch großes Forschungspotenzial.""Die Chancen, aber auch Risiken der Robotik und KI sowie mögliche Handlungsfelder zeigt Mara als Mitglied des österreichischen Rats für Robotik der Politik auf.

Ethikfragen

Dabei geht es darum, soziale Aspekte, ethische Richtlinien und Werte zu beachten. Im Herbst wird ein erstes White Paper veröffentlicht. Wichtig ist es der Technikpsychologin auch, auf Stereotype etwa im Bereich des Machine Learning hinzuweisen, wobei KI-Systeme aus einem großen Pool an Trainingsdaten lernen und die Erkenntnisse auf neue Daten anwenden. Dass diese Daten von Menschen gemacht und dabei auch Fehler und Klischees mitübertragen werden, sei vielen nicht bewusst.So verwende ein Programm für das türkische Personalpronomen ""o"", welches ""er"", ""sie"" oder ""es"" heißen kann, aufgrund seiner Erfahrungen mit den Lerndaten die männliche Übersetzung in ""er ist fleißig, er ist Arzt"", aber die weibliche in ""sie ist faul, sie ist Krankenschwester"", obwohl es nicht wissen konnte, was gemeint war. ""Das sind ganz aktuelle Probleme, um die man sich kümmern müsste"", betont die Professorin.

Denn auch Personalabteilungen in Firmen würden KI-unterstützte Entscheidungshilfen zur Vorselektion von Jobbewerbern anwenden. ""Wenn zum Beispiel ein Stationsarzt gesucht wird und das System sortiert überproportional weibliche Kandidaten aus, weil es gelernt hat, dass so einen Job eher Männer machen, ist das schon ein Problem"", ergänzt Mara.

An der Universität möchte die Wissenschafterin, die am 5. November ihre Antrittsvorlesung hält, den interdisziplinären ""Spirit"" aus dem Ars Electronica Futurelab mitnehmen. Die JKU sei ohnehin offen für interdisziplinäre Kooperationen, ""es ist ein Nexus der künstlichen Intelligenz rund um mich mit den Instituten von Sepp Hochreiter, Gerhard Widmer und Armin Biere"", am Linz Institute of Technology sollen verschiedene Perspektiven auf digitale Transformation zusammenkommen.

Alle Disziplinen zusammen

""Mein großes Ziel ist es, dass Sozialwissenschaften und Psychologie schon früh in der Entwicklung von neuen Technologien, die in unsere Gesellschaft kommen, mitwirken, dass der ganze Prozess interdisziplinär vonstattengeht, und natürlich, dass die Roboter- und KI-Systeme menschengerecht gestaltet werden.""Außerdem hofft Mara, die heuer den Bawag-Frauenpreis für herausragende Leistungen und besonderes Engagement von Frauen für die Gesellschaft erhält, dass sie mehr Studentinnen für technische Fächer an der JKU motivieren kann. Sie persönlich würde ungern im Medizinbereich auf KI verzichten, wenn es darum geht, ""dass eine KI Millionen von Daten und Bildern mit meinen vergleichen kann und gemeinsam mit Ärzten zu viel akkurateren Diagnosen kommt"". Auch die autonome Mobilität ist für die Oberösterreicherin in vielen Fragen sinnvoll. ""Richtig gern hätte ich einen Bot, der meine Termine gut organisiert und der diese ganzen Privacy Policies für mich übersetzt, der mir beim Management meiner Privatsphäreneinstellungen im Internet und bei Apps hilft"", wüsste die Mara noch eine gute Verwendung für künstliche Intelligenz.";https://www.derstandard.at/story/2000089991011/menschengerechte-roboter-bauen;Standard;APA
20.11.2018;Eine alte Zigarettenfabrik, eine 230 Meter lange Straße und viele Start-ups;"Der Name Strada del Start-up lässt eine Anlehnung an Rainhard Fendrichs Hit ""Strada del Sole"" vermuten. Die ersten Worte des Songs stehen auch mit großen Lettern an einer Wand im neuen Gründercampus in der Linzer Tabakfabrik. Die besungene Hitz' fehlte bei der offiziellen Eröffnung am Dienstag. Fünf Grad und Wind. Aus den ""fehlenden Lire"" lässt sich jedoch eine recht annehmbare Analogie bilden. Viele der frisch eingemieteten Jungunternehmer suchen klarerweise immer wieder nach Investoren und deren metaphorischen Lire. Tatsächlich war der Name nur ein Arbeitstitel für das Projekt, erzählt Christian Forsterleitner von Start-up 300. Man sei aber dann darauf ""picken geblieben"".

Die Strada geht vom Business-Angel-Netzwerk Start-up 300 aus. Erst kürzlich ließ es mit der Übernahme der Crowdfunding-Plattform Conda aufhorchen. Auf 230 Metern bietet die Strada Platz für 300 Arbeitsplätze. Von den 57 zur Verfügung stehenden Kojen sind lediglich noch acht zu haben. Junge Unternehmen wie das Modelabel Vresh, die Kryptofirma Blockpit oder Event-Start-up Triply reihen sich neben Corporates wie FACC, Doka, KPMG oder der Wiener Städtischen. Die erfahrenen Unternehmen nutzen die Tabakfabrik zur Auslagerung von Projekten, aber teilweise auch als fixen Arbeitsplatz für Mitarbeiter. Ein Start-up kann ein privates Büro für vier Personen um 600 Euro im Monat mieten, ein etabliertes Unternehmen für 2.400 Euro. Der Zugang zum Coworking-Space ohne fixen Platz kostet 100 Euro. Thematisch soll der Straßenzug in die digitale Zukunft führen und um Technologien wie Virtual Reality, Blockchain, Machine Learning oder künstliche Intelligenz kreisen.
Gschwantner setzt auf autonome Mobilität

Die Eröffnungsrede hielt passenderweise das heimische Start-up-Aushängeschild Florian Gschwandtner von Runtastic. Er sieht in Linz und in der Strada viel Potenzial, sogar so viel, dass er meint, Linz könnte irgendwann zur Innovationsstadt Nummer eins in Österreich werden. In seiner gewohnt schwunghaften Art sprach er über Unternehmertum und forderte eine Änderung der Rahmenbedingungen. Es müssten gewisse Änderungen passieren, damit Österreich nicht den Anschluss verliert. Unter anderem sollte Risikokapital steuerlich absetzbar werden oder Programmieren als dritte Fremdsprache etabliert. Große Hoffnungen hegt er für die Mobilfunkindustrie und autonome Mobilität.
""Zum Scheitern verurteilt""

Begeistert gibt sich der Linzer Bürgermeister Klaus Luger (SPÖ): ""Die Strada ist ein Triple-A-Projekt."" Und das obwohl er die Tabakfabrik an sich aufgegeben hatte. ""Der Standort war zum Scheitern verurteilt. Wenn das Projekt nicht funktioniert hätte, wäre die Welt nicht untergegangen. Dass es so gut funktioniert, freut mich umso mehr"", sagt Luger im Gespräch mit dem STANDARD.

Von etwaigen Schwierigkeiten in der Aufbauphase will Luger nichts wissen. Es sei alles problemlos verlaufen. Dem Vernehmen nach hätte sich allerdings auch der Wiener Start-up-Hub Wexelerate in der alten Zigarettenproduktionshalle einmieten wollen. Dagegen hat sich Start-up 300 jedoch erfolgreich gewehrt.
Strom aus dem Seil

Einrichtungstechnisch hat man sich an einer tatsächlichen Straße orientiert – jedoch eher einer hippen Gasse im Silicon Valley als einem Weg in einem Industriegebiet. Straßenschilder, Hollywoodschaukeln, Ampeln, Graffitis und natürlich eine Bar zieren die Strada. Auch kleine Spielereien haben sich die Innenarchitekten überlegt. Beispielsweise kommt der Strom nicht aus der Steckdose, sondern aus Seilen, wie man sie ursprünglich aus dem Turnunterricht kennt. Pitching-Wettbewerb für Schüler

Im Vorfeld zur offiziellen Eröffnung wurde in der Strada ein Pitching-Wettbewerb der etwas anderen Art veranstaltet. Die Schüler der Handelsakademie Traun bekamen die Möglichkeit, ihre eigenen Geschäftsideen zu präsentieren. Fünf Teams stellten sich den rund 130 anwesenden Mitschülern und der Jury, in der unter anderem Start-up-300-Vorstand Bernhard Lehner und die Präsidentin der Wirtschaftskammer Oberösterreich, Doris Hummer, saßen.

Der Sieg ging an das Team Seaorbis. Die drei 17-jährigen Schülerinnen Elmedina Hodzic, Hava Dukaeva und Aira Selimovic haben sich ein System für smarte Mülltonnen überlegt. Ein Sensor registriert, wie viel Müll man in die Tonne wirft, eine App zeichnet es auf. Das Ganze ist an ein Bonuspunktesystem gekoppelt. Je mehr Müll man wegwirft, desto mehr Bonuspunkte gibt es. Sponsoren und Partner sollen die Punkte in weiterer Folge vergüten. Neben Sachgutscheinen hat Seaorbis eine sechsmonatige Mitgliedschaft in der Tabakfabrik gewonnen. ""Wir werden in unserer Freizeit an der Idee arbeiten und uns nach der Matura voll darauf konzentrieren"", sagt Elmedina Hodzic begeistert. Lehner kann der Idee einiges abgewinnen: ""Der Ansatz ist gut. Mit ein bisschen Glück finden wir in einer kleinen Gemeinde in Oberösterreich einen aufgeschlossenen Bürgermeister und können einen Testlauf mit dem System starten.""";https://www.derstandard.at/story/2000091830992/eine-alte-zigarettenfabrik-eine-230-meter-lange-strasse-und-viele;Standard;Andreas Danzer
21.03.2018;Künstliche Intelligenz als Poetin: Als die Maschine lernte, Klassiker zu sein;"Noch die künstlichste aller Intelligenzen schweigt, wenn ihr nicht eine Kreativagentur sagt, was sie zu tun hat. Rechtzeitig zum Welttag der Poesie wurde ein Gedicht besonders ehrfürchtig ins Licht der Öffentlichkeit gerückt: Sonnenblicke auf der Flucht, ein poetisches Gebilde in 13 Versen, angeordnet in vier unterschiedlich langen Strophen.

Auf den ersten, flüchtigen Blick erregt das untenstehende Opusculum keine besondere Aufmerksamkeit. Ein Geheimnis hüllt gleich die Anfangsstrophe in ein diffus verschwimmendes Licht. ""Auf der Flucht gezimmert in einer Schauernacht"": Es fällt schwer, diese Aussage nicht für selbstbezüglich zu halten. Häufig genug machen moderne Texte von der Möglichkeit Gebrauch, von sich selbst zu sagen, was es mit ihnen auf sich hat. Um den Leser nur umso gezielter hinters Licht zu führen.

Es ist daher gut vorstellbar und möglich, dass jemand das Gedicht Sonnenblicke auf der Flucht zu unbehaglicher Nachtzeit ""gezimmert"" hat. So wie man zu anderer Gelegenheit vielleicht sagt, man hätte etwas ""zusammengeschustert"". Damit wäre angedeutet, dass der Enthusiasmus überall dort einspringt, wo es an Muße und Können fehlt. Um wie viel eher gilt das für die saure, durch tausend Rücksichten erschwerte Kunst der Poesie!

Aber das Gedicht spricht ja ausdrücklich von ""Flucht"". Wer, ob sinnbildlich oder nicht, um sein Leben hat rennen müssen, der wird vielleicht das von ihm in einer Schauernacht Gezimmerte im Nachhinein, bei Tageslicht, nicht darum schief anschauen, weil es wackelt. Der Leser, die Leserin soll die nämliche Nachsicht üben. Aber kann ein Gedicht unter den verzwickten Entstehungsbedingungen der x-ten Moderne überhaupt noch ""wackeln""?
Schritte einer Software

Das Poem Sonnenblicke auf der Flucht wurde von künstlicher Intelligenz (KI) geschrieben. Darin liegt sein höherer Sinn – insofern jedes poetische Sprechen sich von bloßer Umgangssprachlichkeit unterscheidet. Die Digitalkreativagentur TUNNEL23 (echt nur in Versalien) hat dabei ein künstliches Programm ausdrücklich zum Lernen angehalten. Was Hänschen an aktivem wie passivem Spracherwerb nicht leistet, das erlernt eine Software noch allemal.
TUNNEL23

Das KI-basierte Gedicht hat – wohl auch, weil es mit starken Substantiva renommiert – einen regelrechten Siegeszug angetreten. Die Kraftmeierei einer Poesie, die mit ""Göttern"", ""Glocken"" oder ""goldenen Gliedern"" nur so um sich wirft, macht zweifellos Eindruck in einer Epoche, die es vorzieht, ihren geringen Appetit auf Lyrik mit viel Hörensagen zu zügeln. ""Du erklirrende, entheilende Gestalt"": Wer Präfixe gebraucht, um die Aussage (s)eines Gedichts mutwillig zu verunstalten, muss sich weniger um die Künstlichkeit seines Denkens scheren als um dessen Verseuchung mit Kitsch. Unser Gedicht fand jetzt aktuell Aufnahme in den Gedichtband Frankfurter Bibliothek der Brentano-Gesellschaft. ""Es ist schon faszinierend, was künstliche Intelligenz alles so vermag"", schrieb ein Vertreter des honorigen Vereins prompt an den STANDARD. Doch ist es das wirklich?
Aneinanderreihung willkürlicher Effekte

Effekt macht das Gedicht Sonnenblicke auf der Flucht auch nach mehrmaliger Lektüre vornehmlich dann, wenn man lyrisches Schreiben als Aneinanderreihung willkürlicher Effekte versteht. Als Erzeugen von Wirkungen, die man sich als umso befremdlicher vorzustellen hat, als ihr Sinn ""dunkel"" zu sein hat. Jedes Kind weiß, dass Poesie sich seit alters her – warum auch immer – uneigentlich ausdrückt, geschraubt und verblasen. Selbst ihre Besten, Goethe und Schiller, konnten es einfach nicht einfacher sagen. Zum banausischen Aspekt des Experiments gehört übrigens, dass der künstliche Dichter mit Wortmaterial der beiden Weimarer Klassiker gedopt worden ist.

""Kreativität wurde bis dato ausschließlich dem Menschen zugeschrieben – ein wesentliches Merkmal, das ihn so einzigartig macht. Doch die KI perfektioniert das Nachahmen des Menschen und zwingt uns, die Definition von Kreativität zu überdenken."" Das betont Michael Katzlberger, Geschäftsführer von TUNNEL23. Seinen Optimismus will man nicht ohne weiteres teilen. Noch hat die KI einen langen Weg vor sich, ehe Dichterlorbeer ihre Hardware kränzt. Bis dahin wird auch geklärt sein, wer dieser unterzeichnende ""Hirosaki"" ist.";https://www.derstandard.at/story/2000076567516/kuenstliche-intelligenz-schreibt-gedicht-als-die-maschine-lernte-klassiker-zu;Standard;Ronald Pohl
28.02.2019;Österreichs Bankensektor im Umbruch: digital statt regional;"Gut zehn Jahre ist die Finanzkrise nun vorbei und auf den Finanzmärkten hat sich seither einiges geändert. Nicht nur strengere regulatorische Vorgaben und damit einhergehende Bürokratie machten den Banken anfangs zu schaffen, auch völlig neue Konkurrenz-Konstellationen haben sich über die letzten Jahre entwickelt.

Die großen Player der in Österreich aktiven Banken haben sich nach der Krise solide stabilisiert. Die Volksbank etwa hat weitreichende Zusammenlegungen vollzogen. Die Erste Bank/Sparkasse hat sich mit dem Online-Banking-System ""George"" vor allem online weiterentwickelt. ""George"" soll Sparkassen-Kunden vor allem eine übersichtlichere Kontodarstellung ermöglichen. Raiffeisen zog mit ""Mein ELBA"" nach, das auch eine individuelle Nutzeroberfläche im Online-Banking ermöglicht.
Raiffeisen

Doch reicht einfach ein neuer Anstrich oder eine nette Benutzeroberfläche aus? Niedrige Zinsen und geringere Margen, auch Finanzierungen machen den Banken zu schaffen, doch zumindest oberflächlich scheint sich wieder eine gewisse Zufriedenheit mit dem Status quo einzustellen, um nicht sogar von österreichischer Gemütlichkeit zu sprechen. Stresstests werden überstanden und im Großen und Ganzen läuft alles ganz okay. Nur: sind das Zeichen einer Entspannung, einer Stabilisierung in einem schwierigen Umfeld oder ist es einfach nur die Ruhe vor dem nächsten Sturm?
Foto: REUTERS/Leonhard Foeger

David gegen Goliath im Finanzsektor

Wer einen Ausblick, weg von der bequemen Ist-Situation, Richtung Zukunft des Bankings versucht, stößt auf einige Punkte, die heimische Großbanken zumindest nachdenklich und achtsam stimmen sollten. Die Digitalisierung ist nicht nur ein Schlagwort, sondern gehen gerade im Bankensektor mit ganz konkreten möglichen Veränderungen einher. Einige Folgen des digitalen Wandels sind jetzt schon ersichtlich, wobei die Fortschritte in diesem Bereich einerseits bei klassischen Fintechs und andererseits bei Banken, die in Österreich vergleichsweise noch kleinere Marktanteile halten.
Fintechs in der Veranlagung und im Lending-Segment

Das Berliner Start-Up N26, das Kontoführung am Smartphone anbietet, kann auf über 1,5 Millionen Kundinnen und Kunden verweisen. Erst im Jänner stieg das Unternehmen zum Einhorn auf; also Unternehmen mit einem Marktwert von über 100 Millionen Euro. Damit ist N26 als reine Online-Bank längst ähnlich beliebt wie manch etablierte Bank und macht diesen durch niedrige oder erst gar keine Kosten in der Kundengunst zu schaffen. Andere junge Unternehmen decken auch an das klassische Banking angrenzende Themen ab, wie beispielsweise Wikifolio für Investments und die stärker werdenden Immobilien-Crowdinvestmentplattformen wie beispielsweise Dagobertinvest, an dessen Spitze ein ehemaliger Bankenvorstand steht.

Wer nicht Kapital veranlagen, sondern ausleihen möchte, wird ebenfalls von Fintechs bedient. Mikrokredite und Peer-To-Peer-Lending sind hier die Schlagworte der Stunde. Kleinkredite werden in Österreich online beispielsweise über Cashpresso und Cashper vergeben. Diese Kredit-Startups, von denen Cashpresso erst Mitte 2018 3,5 Millionen Euro in Wagniskapital einsammelte, werden von einigen als deutliche Konkurrenz zu en traditionellen Banken betrachtet, aufgrund dynamischerer Quoten und als weniger konservativ betrachteter Kreditmodelle.

Alle Fintechs verbindet der Kampf gegen die großen, alteingesessenen Player am Markt und auch die Tatsache, dass der eigene Erfolg schlichtweg vom Erreichen einer gewissen kritischen Masse am Markt abhängig ist. Erst dann werden Margen interessanter, Geschäftsmodelle tragfähig und nachhaltige Gewinne erreichbar. Wachstum um jeden Preis scheint einerseits riskant und führt anderseits potentiell dazu, dass bei geringer Profitabilität und hohen Kosten für die Neukundenakquise irgendwann nur noch ein Unternehmensverkauf als gewinnbringende Option vorhanden ist.
DW Deutsch
Vergleichsweise unbekannte Banken als Vorreiter

Bei kleineren Banken oder Banken, die zumindest in Österreich eher auf Nischen fokussiert sind, finden sich ebenso spannende Marktteilnehmer. Ein Beispiel in diesem Bereich ist die Renault Bank, die bei Zinsvergleichen immer wieder mit höheren Sparzinsen auftrumpft und hier klassischen Banken wohl den ein oder anderen Kunden durch geringfügig bessere Konditionen abnimmt.

Es zeigt sich auch, dass diese Institute beim digitalen Wandel teils die Nase gegenüber Großbanken doch weit vorne haben. Die Santander Consumer Bank bietet etwa die Möglichkeit, einen Kredit direkt online zu beantragen – mit der Unterstützung eines Bankberaters, der per Telefon und Screen-Sharing zugeschalten ist. Der gesamte Antragsprozess wurde hier also bereits erfolgreich digitalisiert, ein Besuch (oder gar mehrere Termine) bei einem Berater vor Ort ist nicht nötig.
Digitalisierung: Chance oder Bedrohung?

Der Vorsprung, den Start-Ups und einzelne Banken gegenüber den Großbanken in Österreich haben, kann nun auf unterschiedliche Weise gedeutet werden. Jene, die im digitalen Umfeld den Takt vorgeben, zeigen was alles bereits möglich wäre, obwohl sie vergleichsweise weniger Kapital zur Verfügung haben.

Dieser Vorsprung kann entweder als Bedrohung wahrgenommen werden, oder als Ansporn. Wenn traditioneller ausgerichtete Banken von digitalem Wandel sprechen, denken sie meist an Online-Banking, aber Dinge wie Online-Beratung in Echtzeit, Machine-Learning für Investmentstrategien oder schlichtweg vollständig digitalisierte Prozesse, die Bankbesuche ersetzen können, klingen hier noch wie Zukunftsmusik. Machen Sie den Selbsttest und probieren Sie aus, ob und mit wie vielen Klicks Sie Ihren Kontostand von vor drei Jahren herausfinden können. 
Digitalen Wandel aktiv gestalten

Digitalisierung scheint für manche Bank eher wie eine Bedrohung zu wirken, durch die Filialen geschlossen und Teammitglieder gekündigt werden könnten. Doch wer angsterfüllt in die Zukunft blickt, wird sie nicht proaktiv gestalten können.

Vielleicht ist dieser Vorsprung all jener, die sich der Digitalisierung verschrieben haben, ein Weckruf, der den heimischen Großbanken zeigt, dass nicht die Digitalisierung eine Gefahr ist, sondern die nach der Krise neu etablierte Gemütlichkeit das eigentliche Risiko für ihre zukünftige Entwicklung darstellt. Augen zu und durch funktioniert vielleicht in einer schlechten Konjunkturphase, vielleicht auch noch in einer echten Krise, aber sicher nicht bei einer Veränderung, die so nachhaltig ist, dass sie die gesamte Branche dauerhaft in neue Bahnen lenken wird.

Klar ist, dass die Digitalisierung – und dabei ist bitte nicht einfach nur an Online-Banking zu denken – nicht aufzuhalten ist. Maschinen werden Investmentstrategien emotionslos besser entwickeln als es Menschen heute tun, Gespräche mit Mitarbeiterinnen und Mitarbeitern werden per Chat erledigt, Kreditanträge online eingereicht und auch die Schwarmintelligenz kann durchaus eine Rolle spielen.
Gemeinsames Leid von Banken und Pharma-Konzernen

Das naheliegende Fazit ist, dass sich Geschäftsmodelle in der Finanzbranche nachhaltig verändern werden. Die gesamte Wertschöpfungskette von Banken kann in einigen Jahren anders aussehen als heute und solange etablierte Institute den digitalen Fortschritt nicht ausreichend selbst in die Hand nehmen, werden laufend neue Start-Ups heranwachsen, die früher oder später so erfolgreich sein werden, dass alteingesessenen Banken kaum eine andere Möglichkeit bleiben wird, als diese zu übernehmen.

Ein teurer Weg, Innovationen extern zuzukaufen, nachdem man sie selbst verschlafen hat. Gewissermaßen ist die Situation dadurch mit der Pharmabranche vergleichbar, in der klassische Pharma-Unternehmen regelmäßig kleinere Biotech-Unternehmen aufkaufen, um an innovative Forschungsergebnisse zu kommen. Doch noch ist es nicht zu spät, dass klassische Banken diese Parallelen erkennen, den digitalen Wandel endlich als Tatsache akzeptieren und selbst voranschreiten, um dieses neue Zeitalter des Bankings aktiv mitzugestalten. ";https://www.derstandard.at/story/2000097971013/oesterreichs-bankensektor-im-umbruch-digital-statt-regional;Standard;Christian Allner
10.02.2016;Medien: Von Frauen bleiben Brüste und Beine;"""Die Ergebnisse sind erschütternd"", sagt Martina Thiele, Professorin für Kommunikationswissenschaft an der Universität Salzburg. Eine kürzlich erschienene britische Studie, die bisher größte ihrer Art, zeigt, wie oft und in welcher Weise Frauen und Männer in Medien vorkommen. Untersucht wurden mithilfe von AI (Artificial Intelligence) mehr als 2,3 Millionen Artikel in 950 Onlinepublikationen über einen Zeitraum von sechs Monaten. 77 Prozent aller in Texten erwähnten Personen sind Männer. Bildlich dargestellt sind 30 Prozent Frauen, hier ist der Unterschied also etwas geringer.

""Ich bin sehr beeindruckt von dem großen Sample, also der Quantität der Studie"", ergänzt Thiele, ""aber die Ergebnisse bestätigen das, was in klassischen Studien seit den 1970er-Jahren herauskommt: Männer sind in den Medien dominant. Daran hat sich nichts geändert."" Sie lobt, dass in der Studie, die ein Team vom Intelligent Systems Laboratory an der Universität Bristol unter Nello Christianini mit der Journalismusforscherin Cynthia Carter von der Universität Cardiff verfasst hat, auch qualitative Aspekte erfasst werden. Es wird also untersucht, in welchem Zusammenhang Frauen beziehungsweise Männer repräsentiert sind. Auch hier sind die Ergebnisse wenig überraschend: Frauen kommen, wenn überhaupt, am ehesten in den Bereichen Mode, Unterhaltung und Kunst vor. In Sport und Politik sind sie am wenigsten vertreten.
Wohin mit Conchita?

""Das zeigt einmal mehr die sogenannte Ambivalenz der Sichtbarkeit"", erklärt Thiele, ""weil mehr Sichtbarkeit nicht automatisch mehr Ansehen bedeutet."" Sie verweist außerdem auf den ""face-ism-/bodyism-Index"", mit dem in der Medienforschung seit den 1980er-Jahren operiert wird: ""Er zeigt, dass Männer sehr viel öfter im Porträt dargestellt werden, von Frauen werden oft nur einzelne Körperteile wie Beine oder Brüste gezeigt."" Dies habe sich in den letzten Jahrzehnten auch in deutschsprachigen Medien nur sehr wenig verändert. ""Es gibt einige wenige Frauen in Spitzenpostionen, wir sprechen hier vom Merkeleffekt. Durch Politikerinnen wie Merkel ändert sich quantitativ ein kleines bisschen, qualitativ aber kaum etwas.""

""Grundsätzlich finde ich die Studie gut"", betont Thiele, die aktuell zu Stereotypen in den Medien forscht, ""aber sie reproduziert Geschlechterdualismen"". Sie basiert auf automatischer Gesichtserkennung, die erst die Bearbeitung von derartigen Datenmengen in kurzer Zeit erlaubt. ""Die Gesichtserkennung funktioniert schon recht gut"", sagt Monika Henzinger, Professorin am Institut für Informatik der Universität Wien, und verweist auf eine diesbezügliche Studie aus dem deutschsprachigen Raum (Machine Learning Applied to Perception: Decision-Images for Gender Classification) Für Uneindeutigkeiten ist dabei aber kein Platz: ""Ich frage mich, was die Gesichtserkennung zum Beispiel mit einem Foto von Conchita Wurst anfängt"", sagt Thiele.
Gegensteuern

Auch für FJUM-Geschäftsführerin Daniela Kraus sind die Ergebnisse der neuen Studie nicht überraschend: ""Die Studie bestätigt Befunde wie sie – leider – seit Jahrzehnten gezogen werden müssen. Frauen sind in Medien unterrepräsentiert, wenn es um Wirtschaft, Politik und Hard News geht. Ich denke, es ist wichtig, einen Schritt weiter zu gehen und zu fragen, wie konkret gegengesteuert werden kann.""

Dafür führt sie ein ökonomisches Argument ins Treffen: ""Medien haben schon längst Frauen als interessante Zielgruppe im Blick. Wäre es da nicht überlegenswert, wie Redaktionen diese Frauen noch besser erreichen können? Eine vielfältige Berichterstattung, die Frauen nicht marginalisiert, hat für weibliche Rezipientinnen höhere Anschlussfähigkeit, ist glaubwürdiger, interessanter und attraktiver.""";https://www.derstandard.at/story/2000030648328/medienvon-frauen-bleiben-brueste-und-beine;Standard;Tanja Paar
09.06.2018;USA stellen leistungsstärksten Supercomputer vor;"Das Energieministerium der USA hat am Freitag den aktuell leistungsstärksten Supercomputer, genannt Summit, vorgestellt. Bei maximaler Kapazität erreicht er eine Rechenleistung von 200 Petaflops. Das sind 200 Billiarden Berechnungen pro Sekunde. Den vorherigen Rekord hielt der chinesische Computer Sunway Taihulight mit einer Rechenleistung von 93 Petaflops. Der neue Computer ist sieben Mal schneller als der letzte Rekordhalter aus den USA, Titan. Summit kann innerhalb von einer Stunde ein Problem lösen, für welches ein durchschnittlicher Heimcomputer 30 Jahre brauchen würde.
Enormer Energieverbrauch

Das Gerät wird im Oak Ridge National Laboratory im US-Bundesstaat Tennessee aufbewahrt. Für diese extreme Leistung sind 4.608 Server mit über 9.000 IBM-Power9-Prozessoren mit 22 Kernen und über 27.000 Nvidia-Tesla-V100-Grafikkarten notwendig. Pro Minute sind umgerechnet rund 15.000 Liter (4.000 Gallonen) Wasser notwendig, um das System zu kühlen. Mit dem Energieverbrauch pro Minute könnte man 8.100 Haushalte mit Strom versorgen. Insgesamt verbraucht Summit so viel Platz wie zwei Tennisplätze.
Künstliche Intelligenz im Fokus

Fokus von Summit ist es, künstliche Intelligenz anzuwenden. Mittels Machine und Deep Learning soll die Forschung unter anderen in den Bereichen Gesundheit, Physik und Klimamodellen vorangetrieben werden. Der Supercomputer wurde bereits genutzt, um die nach eigenen Aussagen erste Exascale-Berechnung durchzuführen – das sind eine Trillionen Rechenoperationen.

Geplante Projekte sind, Supernovas zu analysieren und herauszufinden, wie Elemente wie etwa Gold sich durch das Universum bewegen, sowie auch die Krebsforschung voranzutreiben. Grundsätzlich sind aber nicht nur Gesundheits- und Klimaforschung der Fokus solcher Supercomputer – oft werden sie auch genutzt, um Kriegswaffen zu erforschen.";https://www.derstandard.at/story/2000081268400/usa-stellt-leistungsstaerksten-supercomputer-vor;Standard;red
18.12.2017;"Psychologin: ""Roboter kopieren unsere Vorurteile""";"STANDARD: Wie sieht die Arbeit einer Roboterpsychologin aus?

Mara: Keine Angst – bei mir liegen keine Roboter auf der Couch, die über ihre Traumata reden. Mir geht es um das Wohlbefinden der menschlichen Interaktionspartner. Meine Fragestellungen drehen sich darum, wie Menschen Roboter und künstliche Intelligenz erleben und wie Roboter gestaltet werden können, sodass die Menschen sie akzeptieren. Es gibt große Ängste davor, von der Technologie dominiert zu werden. Ein wichtiger Teil meiner Arbeit ist die Frage, wie Roboter nicht als Bedrohung, sondern als Bereicherung erlebt werden können.

STANDARD: Worin unterscheidet sich Ihr Ansatz von der Forschung an optimalen Schnittstellen zwischen Mensch und Maschine?

Mara: Die Roboterpsychologie geht weit über klassische Usability-Forschung hinaus, in der es etwa darum geht, wie man Bedienungsoberflächen intuitiv gestalten kann. Meine Fragestellungen reichen von gesellschaftlichen Ängsten gegenüber bestimmten Einsatzgebieten von Robotern über die Rolle der User-Persönlichkeit bis zum subjektiven Sicherheitsgefühl in der Kommunikation von autonomen Fahrzeugen und Fußgängern.

STANDARD: Sie beschreiben in Ihrer Arbeit ein Gefühl der Unheimlichkeit, das sich beim Kontakt mit Robotern bei den Menschen einstellt. Können Sie das näher erläutern?

Mara: Für das Phänomen wurde bereits im Jahr 1970 vom japanischen Robotiker Masahiro Mori der Begriff des ""Uncanny Valley"", also des ""unheimlichen Tals"", geprägt. Von ihm stammt eine hypothetische Grafik, die den Zusammenhang zwischen Menschenähnlichkeit eines Roboters und emotionaler Reaktion des Menschen zeigt. Ihm zufolge wird die Menschenähnlichkeit nur bis zu einem gewissen Grad positiv wahrgenommen. Der Roboter kann Kopf und Augen haben, muss aber klar als Maschine erkennbar sein. Wenn etwa Silikonhaut und künstliches Haar die Unterscheidung erschweren, wirkt der Roboter unheimlich. Ganz neue Studien deuten darauf hin, dass es bei körperloser künstlicher Intelligenz ähnlich sein könnte: Vom sogenannten ""Uncanny Valley of the Mind"" spricht man etwa dann, wenn Online-Chatbots in ihrem Verhalten zu menschenähnlich werden, weil sie Emotionalität simulieren oder selbstständig Entscheidungen treffen.

STANDARD: Gewöhnen sich die Menschen mit der Zeit an menschenähnliche Roboter?

Mara: Dass es Gewöhnungseffekte gibt, ist anzunehmen. Man würde aber mehr Langzeitstudien benötigen, was noch schwer durchführbar ist. Es stellt sich aber die grundsätzliche Frage, ob Menschen überhaupt mit menschenähnlichen Robotern zusammenarbeiten und -leben wollen. Umfragen zeigen hier große Ablehnung. Wenn man sich vorstellt, dass Roboter und Menschen nicht mehr unterscheidbar sind, würde das eine dauerhafte Unsicherheit hervorrufen und unsere Erwartungshaltungen durcheinanderbringen. Das ist natürlich noch weit weg von der heutigen Realität. Bei Bots, die bereits jetzt mit Menschen online kommunizieren und menschliche Interaktion simulieren, ist das Problem aber viel naheliegender.

STANDARD: Die ersten autonom agierenden Maschinen haben ohnehin keine menschliche Form, sondern sind beispielsweise Fahrzeuge. Wie wird sich die Interaktion mit den Autos der Zukunft verändern?

Mara: Wir untersuchen am Futurelab, wie künftige Transportsysteme mit ihrer Umwelt kommunizieren. Die Frage ist wichtig, weil die Angst vor Kontrollverlust durch vernetzte Technologie auch Fußgänger ergreifen könnte. Es geht darum, transparent zu machen, welche Schritte eine Maschine als Nächstes ergreifen wird. Gemeinsam mit einem deutschen Fahrzeughersteller simulieren wir in einem Testlabor mithilfe von Projektionen, Drohnen und kleinen Bodenrobotern entsprechende Verkehrsszenarien. Ist es beispielsweise für Menschen, die die Fahrbahn eines autonomen Fahrzeugs kreuzen müssen, wichtig, dass der Roboter ihnen mitteilt, wenn er sie gesehen hat? Traditionell stellen wir in diesen Situationen oft Blickkontakt mit menschlichen Fahrern her. Wir haben untersucht, ob es wichtig ist, diese Verhaltensweisen auf die Technologie zu übertragen, und wie man das machen könnte.

STANDARD: Was haben Sie herausgefunden?

Mara: Wir konnten zeigen, dass man etwa mit Lichtsignalen die Vorhersehbarkeit der Aktionen eines Fahrzeugs steigern kann. Fußgänger wurden durch sie deutlich schneller und bewegten sich sicherer. Ideal sind zurückhaltende visuelle Signale, die nur peripher wahrgenommen werden, aber dennoch zur Erhöhung des Sicherheitsgefühls beitragen. Ein Ansatz ist beispielsweise ein Display am Kühlergrill des Autos, das die Signale in Richtung der gemeinten Person aussendet. Die Möglichkeiten, Signale zu entsenden, werden sich weiterentwickeln. In 20 Jahren wird die Karosserie vielleicht als großes Display gedacht.

STANDARD: Auch in der Zusammenarbeit mit Robotern in der Industrie ist die Vorhersehbarkeit wichtig. Wie könnte das dort aussehen?

Mara: Auch hier gilt: Die Maschinen müssen sich den Menschen erklären. Für uns ist es wichtig, möglichst früh antizipieren zu können, wo ein Roboter etwa hingreifen möchte. Da zeigt sich, dass die Maschinenbewegung nicht den kürzesten Weg von A nach B nehmen darf. Mit kurvenartigen Bewegungen kann man schon früh andeuten, wo es hingeht. Aus technischer Perspektive ist das ineffizient, die Lesbarkeit von Roboterbewegungen wird damit aber erhöht und das Team aus Mensch und Maschine insgesamt schneller.

STANDARD: Ist die Bereitschaft, Akzeptanz für Robotik aufzubringen, auch kulturell bedingt? Man sagt den Japanern beispielsweise nach, dass sie hier besser zurechtkommen.

Mara: Die Daten der Forschung sind nicht eindeutig. Manche Studien weisen darauf hin, dass die Menschen in Japan offener gegenüber autonom agierenden Maschinen sind, andere sehen keinen Unterschied oder zeigen sogar das Gegenteil. Meine persönliche Erfahrung würde die These schon stützen. Ein Erklärungsansatz liegt in der religiösen Tradition von Animismus und Shinto, in der das Konzept eines beseelten Objekts gängig ist. In der christlichen Tradition gibt es diesen Gedanken nicht. Auch die Mediensozialisation wird oft ins Treffen geführt. In den verbreiteten Anime und Mangas sind Roboter Kooperationspartner oder sogar Retter der Menschheit. Das steht in starkem Kontrast zur westlichen Vision eines ""Terminators"", der den Endbereiter der Menschheit gibt.

STANDARD: Die meisten Entwickler von Robotern sind Männer. Verbauen sie auch ihre männliche Perspektive in die Technologie?

Mara: Die gesamte Tech-Branche wird von jüngeren weißen Männern dominiert. Es ist schwierig zu untersuchen, welche Verzerrungen, welche Bias dadurch entstehen. Es gibt ein Video, das in Social-Media-Kanälen kursiert, das einen schwarzen Mann zeigt, der es nicht schafft, Seife aus einem Seifenspender zu bekommen. Erst als er ein weißes Blatt Papier darunterhält, kommt Seife heraus. Das verbildlicht, was schiefgehen kann, wenn Teams zu wenig divers sind. Man kann etwa beobachten, dass Serviceroboter nach traditionell weiblichen Klischees designt werden – bis hin zum Putzroboter mit angedeutet weiblicher Figur und Schürzchen. Klar belegt ist auch, dass künstliche Intelligenz, die ja auch Teil von Robotern sein kann, Vorurteile und Stereotype von Menschen übernimmt.

STANDARD: Wie kann man sich das vorstellen?

Mara: Künstliche Intelligenz ist in ihrer derzeitigen Umsetzung vor allem maschinelles Lernen. Die Programme werden mit von Menschen gemachten Inhalten gefüttert und ziehen ihre Schlüsse daraus. Wir dürfen also nicht davon ausgehen, dass das Ergebnis wertneutral ist – was aber oft intuitiv vermutet wird. Studien zeigen, dass Machine-Learning-Systeme Genderstereotype übernehmen und etwa Frauen begrifflich näher bei der Familie verortet werden und Männer näher bei der Karriere.

STANDARD: Inwiefern prägt die Popkultur technologische Entwicklungen vor?

Mara: Bei vielen Menschen haben Science-Fiction-Filme und -Bücher dafür gesorgt, dass sie den aktuellen Stand der Technik überschätzen. Viele haben beim Begriff ""Roboter"" sofort menschenähnliche Androiden im Kopf, wie man sie vielleicht aus Star Trek oder I, Robot kennt. Die Entwickler, die gerade hart daran arbeiten, dass Roboter stabil auf zwei Beinen bleiben, können darüber nur lachen. Es würde nicht schaden, öfter Bilder von aktuellen Robotern herzuzeigen – beispielsweise autonome Transport- oder Reinigungssysteme.";https://www.derstandard.at/story/2000067810717/psychologin-mara-roboter-kopieren-unsere-vorurteile;Standard;Alois Pumhösel
09.01.2017;Big Data: Sind wir bereits gläserne Menschen?;"Annas Profil auf Linkedin liest sich beeindruckend. Die studierte Betriebswirtin ist derzeit Senior Human Resources Manager bei einer großen Unternehmensberatung und auf der Suche nach neuen Herausforderungen. Verhandlungssicheres Englisch, kommunikatives und kundenorientiertes Auftreten zeichnen sie aus.

Anna hat schon lange das Restless-Leg-Syndrom. Die neurologische Erkrankung verursacht Kribbeln und Brennen in den Beinen. Durch Medikamente hat sie ihr Problem gut im Griff. Gelegentlich nimmt sie an klinischen Studien teil. Als Moderatorin des Onlineforums der Selbsthilfegruppe berät sie andere Betroffene.

Auf Annas Jobsuche-Postings in beruflichen Netzwerken und Stellenmärkten hat sich niemand gemeldet. Warum? Im Rahmen einer Studie über das Restless-Leg-Syndrom wurde eine Magnetresonanztomografie-Untersuchung von Annas Gehirn gemacht. Vor kurzem wurden die Daten anonymisiert auf einer Onlineplattform Forschern weltweit zur Verfügung gestellt.
Der Supercomputer auf dem Schreibtisch

Open Data, also freier Zugriff auf Daten, ist für mich als Data-Mining-Forscherin unverzichtbar. Wir arbeiten an Techniken, die verständliche Muster und Zusammenhänge in sehr großen Datenmengen finden.

Ein Algorithmus ist häufig eine komplexe Folge von Anweisungen, die der Computer abarbeiten kann, um ein bestimmtes Ziel zu erreichen. Durch die rasante technische Entwicklung steckt schon in Ihrem Notebook oder Desktop-Computer daheim mehr Rechenleistung als in einem Supercomputer der 90er-Jahre. Wir in der Wissenschaft, aber vor allem Unternehmen wie Google haben natürlich noch viel mehr Möglichkeiten. Die Kombination aus Open Data und Rechenleistung ermöglicht es uns, mit Algorithmen immer mehr von den vielfältigen Informationen in großen Datensätzen zu verstehen.

Mit meiner Forschungsgruppe entwickle ich Clustering-Algorithmen, die die Datenflut automatisch in sinnvolle und verständliche Gruppen einteilen. Schon kleine Kinder beherrschen das perfekt. Wenn wir ihnen zum Beispiel verschiedene Gegenstände zeigen, können sie diese schon sehr früh sortieren, selbst wenn sie die Wörter dafür noch nicht kennen. Für Algorithmen ist das automatische Erkennen von Clustern eine Herausforderung. Für erste Tests verwenden wir synthetische Daten. Wir erzeugen einen Datensatz, der genau solche Muster enthält, die unser Algorithmus finden kann. Ist dieser erste Test geglückt, brauchen wir reale Daten, um zu sehen, ob unsere Idee auch in der Praxis funktionieren kann.

In Kooperation mit Medizinern und Neurowissenschaftern evaluieren wir Algorithmen auf Daten aus medizinischen Studien und Online-Datenbanken wie ""ADNI"" (Alzheimers Disease Neuroimaging Initiative) oder ""INDI"" (International Neuroimaging Data Sharing Initiative). Wir haben hierfür beispielsweise eine Clustering-Technik entwickelt, die Personen aufgrund des Zusammenspiels ihrer Gehirnregionen in Gruppen einteilt. Die Ergebnisse helfen dabei, komplexe psychosomatische und neurodegenerative Krankheitsbilder besser zu verstehen, und gehen in die Entwicklung neuer Behandlungsansätze ein.
Bessere Algorithmen brauchen bessere Köche

Jede Art von künstlicher Intelligenz basiert auf Algorithmen. So kompliziert sie auch sein mögen, Algorithmen sind prinzipiell vom Menschen verfasste Kochrezepte. Wir entscheiden, was sie tun und wozu wir sie einsetzen. Wir brauchen politische, rechtliche und finanzielle Rahmenbedingungen, damit diese Techniken auch in Zukunft dem Wohl des Menschen dienen.

Die Schule sollte eine Informatikgrundausbildung bieten, damit jeder Chancen und Risiken einschätzen und mit seinen Daten verantwortungsbewusst umgehen kann. Wir brauchen genug Geld für unabhängige Forschung im Bereich Data-Mining, Machine-Learning und verwandten Disziplinen, wenn wir das Feld nicht finanzkräftigen Firmen und militärischer Forschung überlassen wollen. Wir brauchen mehr gut ausgebildete Data-Scientists, das sind Informatiker mit fundierten Kenntnissen in mindestens einem Anwendungsgebiet wie Biomedizin oder Betriebswirtschaft. Sie können gemeinsam mit anderen Wissenschaftsdisziplinen, Juristen und Politikern die technologische Entwicklung ethisch vorantreiben.
Sie sind die Summe Ihrer Daten

Warum hat Anna, die fiktive Person aus dem Beispiel zu Beginn, keine Jobangebote gekommen? Eine zukünftige Personensuchmaschine hat die unterschiedlichen Facetten ihrer digitalen Identität zusammengeführt, sodass Gesundheitsinformationen und alle weiteren privaten Details für potenzielle Arbeitgeber sichtbar sind. Ist das wirklich Zukunftsmusik, oder gibt es schon heute den gläsernen Menschen?

Es gibt tatsächlich bereits Algorithmen zur Gesichtsrekonstruktion aus medizinischen Bildern. Daher kann das MRT-Bild prinzipiell mit dem Profilfoto in Linkedin abgeglichen werden. Textmining-Algorithmen erkennen aus Wortwahl, Satzbau und anderen Eigenheiten, dass die Beiträge aus dem Selbsthilfeforum und den beruflichen Netzwerken von derselben Person stammen. Diese Techniken sind noch nicht ausgereift, aber das ist nur eine Frage der Zeit";https://www.derstandard.at/story/2000049611201/big-data-sind-wir-bereits-glaeserne-menschen;Standard;Claudia Plant
06.08.2017;Bierbrauen mit dem Raspberry Pi und andere herausragende Projekte;"Als der Raspberry Pi im Jahr 2012 erstmals vorgestellt wurde, wagten selbst dessen Erfinder nicht davon zu träumen, zu welchem einem riesigen Erfolg sich der kostengünstige Minirechner in den Folgejahren entwickeln sollte. Millionenfach verkauft gibt es mittlerweile eine Fülle an wohldokumentierten Projekten, die interessiert Nutzer nachbauen können. Einige der Spannendsten davon sind auf Opensource.com versammelt.
Pi in the Sky

Waren Aufnahme der Erde aus großen Höhen bis vor wenigen Jahren noch weitgehend großen Unternehmen vorbehalten, sorgt die Verfügbarkeit günstiger Hardware dafür, dass sich dieses Feld längst auch Hobbybastlern erschlossen hat. Ein besonders ambitioniertes Unterfangen nennt sich dabei ""Pi in the Sky"". Aus einer langen Reihe von Experimenten hat Dave Ackerman eine eigene Hardwareerweiterung für den Minirechner entwickelt, und ebenso als Open Source veröffentlicht wie die zugehörige Software. Zusätzlich kann die Pi-in-the-Sky-Platine aber auch käuflich erworben werden. Was sich damit alles so anstellen lässt, hat Ackerman in den letzten Jahren eindrucksvoll dokumentiert. So hat er etwa einen Plüschbären aus einer Höhe von 39.000 Metern in Richtung Erde springen lassen – und damit noch ein paar Meter mehr erzielt, als es Felix Baumgartner geschafft hat. Ein anderes Mal hat eine Frau die Dienste des Bastlers in Anspruch genommen, um ihr Brautkleid in die Stratosphäre zu schicken. Bierbrauerei

Irdischere Ziele verfolgt Christoph Aedo: Dieser verwendet den Raspberry Pi, um damit sein eigenes Bier zu brauen. Konkret kümmert sich der Pi hier um die Erfassung von Temperaturinformationen, um dann anhand dieser Daten mehrere Heizelemente passend zu schalten. Als Software nutzt er CraftBeerPi, ein Python-Programm, das von einer Gruppe anderer Pi-Entwickler erstellt wurde. Künstliche Intelligenz

Auch für erste Experimente mit dem Bereich Maschinenlernen eignet sich der Raspberry Pi bestens. In einem Blogeintrag dokumentiert Red-Hat-Mitarbeiterin Rikki Endsley eine Reihe von Experimenten mit Tensorflow, der von Google als Open Source veröffentlichten Software zur Erstellung neuronaler Netzwerke. Dazu gehört etwa eine Projekt, das mithilfe von Maschinenlernen einschätzen kann, um wie viele Minuten ein Zug vermutlich verspätet sein wird. Tensorflow wird hier unter anderem zur Bilderkennung eingesetzt, damit auch wirklich nur Personenzüge erfasst werden und keine Frachtentransporte.
Dashboard

Eine eigenes Dashboard, das alle wichtigen Informationen auf den ersten Blick bietet: Dieses Projekt hat sich Conor O'Callaghan vorgenommen. Seine Entwicklung basiert dabei auf einem Raspberry Pi 2B, den er mit einem 7-Zoll-Bildschirm kombiniert hat. Die passende Software hat er auf Basis von Python 3 und OpenShift entwickelt, sie liefert ihm unter anderem aktuelle Nachrichten aber auch Informationen zu Wetter oder den nächsten Busabfahrten bei der nahegelegensten Station.
Webserver

Für Anfänger mögen aber andere Projekte zunächst realistischer sein. Ist es doch mit dem Pi auch recht einfach möglich einen eigenen Web-Server zu betreiben. Wie das etwa ablaufen kann, und worauf dabei zu achten ist, dokumentiert Mitchell McLaughlin in einem weiteren Artikel.";https://www.derstandard.at/story/2000062208065/bierbrauen-mit-dem-raspberry-pi-und-andere-herausragende-projekte;Standard;red
04.10.2019;Digitalisierung im Stall: Die überwachte Kuh;"Der Bauer von heute kann seine Kühe lückenlos überwachen. Möglich macht das eine maßgeschneiderte Vernetzungstechnologie: Sensorchips an den Beinen erfassen Bewegungsmuster, die Krankheit oder Brunst verraten können. Als Ohrmarke oder Halsband verpackt können Chips über das Wiederkäuen Auskunft geben – auch das eine Datenquelle zum Gesundheitszustand. Eine Sensorkapsel, verschluckt, im Pansen der Tiere, dokumentiert PH-Werte, Temperatur und Trinkverhalten; ein Sensor am Schwanz einer trächtigen Kuh schlägt Alarm, sobald die Geburt einsetzt.
Totalüberwachung

Die Digitalisierung hat Stall und Nutztiere erreicht – die Schlagworte dazu lauten Smart Farming oder Precision Farming. Einerseits bringt die Technologie Vorteile für die Tiergesundheit: Man erkennt auch in unübersichtlichen Herden schneller, wenn ein Tier krank ist, nicht mehr richtig frisst, sich agressiv verhält. Das lässt sich wirtschaftlich darstellen. Auf der anderen Seite markiert die Technologie den nächsten Schritt in der Industrialisierung der Landwirtschaft: Die Verfügbarkeit von biologischen Funktionsdaten zu jedem einzelnen Tier zieht die Effizienzschraube, in der die Tiere stecken, noch mehr an. Manchen Beobachter beschleicht angesichts der Totalüberwachung ein ungutes Gefühl. Ist die Technologie auch für die Tiere eine gute Entwicklung? Was heißt all das aus tierethischer Sicht? Sensoren an und in den Tieren schicken unterschiedliche Daten zu ihren Basisstationen: Der Aufenthaltsort wird per Satellitennavigation oder durch die Laufzeitmessung von Funksignalen eruiert. Beschleunigungssensoren geben Auskunft über Bewegungsmuster des Tiers und einzelner Körperteile wie Ohren, Kopf oder Schwanz. Aus dem Tiermagen kommen neben Bewegungsdaten Zeitreihen zu Temperatur und PH-Wert der Magensäure.Mit der Messung ist es nicht getan: Es braucht ausgeklügelte Auswertungsstrategien, um aus den Rohdaten relevante Information über das Befinden des Tiers abzuleiten. Machine-Learning-Algorithmen lernen, aus den Daten Muster zu extrahieren – Muster, die auf Brunst, bevorstehende Kalbung oder Krankheit hindeuten, ermöglichen es, die individuelle Behandlung und Fütterung der Tiere zu verbessern.
Foto: Getty Images/Marek Wykowski

Wenige Wissenschafter beschäftigen sich mit der Frage, was die Digitalisierung für Tiere bedeutet. Einer von ihnen ist Christian Dürnberger. Er arbeitet an der Abteilung Ethik der Mensch-Tier-Beziehung des Messerli-Forschungsinstituts der Vetmeduni Wien; auf Fragen der Ethik in Landwirtschaft und Veterinärmedizin ist er spezialisiert.

""In der Ethik wird seit Jahrtausenden danach gefragt, welchen moralischen Umgang wir unseren Mitmenschen schulden. Tiere galten die längste Zeit nur so viel wie Gegenstände"", sagt Dürnberger. Erst im 19. Jahrhundert schlug mit dem Philosophen Jeremy Bentham das Denken eine neue Richtung ein. Bentham machte die Frage, ob wir einem Wesen moralisches Handeln schulden, nicht mehr davon abhängig, ob dieses Wesen zu Vernunft fähig ist. Sondern davon, ob es leidensfähig ist.
Haben Tiere Rechte?

Was wir Tieren schulden – zudieser Frage gibt es heute drei grundlegende Positionen, sagt Dürnberger. Die erste Position geht davon aus, dass Tiere Rechte haben und dass es unmoralisch ist, Tiere zu nutzen. In dieser Postion erübrigt sich die Frage, wie sich die Nutzung von Tieren durch Technologie verbessern lässt. Sehr wohl relevant für die Frage, was eine digitalisierte Landwirtschaft in Zukunft bringt, sind die beiden anderen Positionen, die von der Unterscheidung zwischen Tierschutz und Tierwohl ausgehen.

Kein leidensfähige Wesen will leiden. Die zentrale Forderung des Tierschutzes ist also seit Bentham, dass man Tieren das Leiden ersparen solle. ""Wenn eine Technologie für bessere Tiergesundheit sorgt, und das ist bei den Smart-Farming-Anwendungen sicher der Fall, ist das aus der Sicht des Tierschutzes sehr interessant"", sagt Dürnberger. ""Denn in dieser Hinsicht lässt sich eine wesentliche Verbesserung erzielen. Es kommt zum Beispiel noch oft vor, dass eine Erkrankung bei Nutztieren übersehen wird, Geld für eine Behandlung fehlt und es geschlachtet werden muss.""

Die Forderung nach Tierwohl geht weit über Tierschutz hinaus. Nach Definition des ""Farm Animal Welfare Councils"" soll das Tier frei von Hunger, Durst, haltungsbedingten Beschwerden, Angst und Stress sein. Es soll die Freiheit zum Ausleben angeborener Verhaltensweisen haben. ""Auch Tiere haben individuelle Vorlieben. Kühe fressen etw gerne gemeinsam mit bestimmten Artgenossen, Freunden sozusagen. Hat man mehr Daten über die Tiere, könnte man theoretisch auch hier etwas tun"", sagt Dürnberger. ""Ich bin aber sehr skeptisch, dass die Digitalisierung im Stall tatsächlich zu größerem Tierwohl führen wird.""

Ist das ungute Gefühl angesichts der ""gläsernen"" und totalüberwachten Kuh verständlich? ""Das Gefühl mag greifbarer werden, wenn man an einen Hund denkt, den man als Clown schminkt. Der Hund mag kein negatives Empfinden dazu haben. Dem Menschen sagt eine moralische Intuition aber vielleicht, etwas Falsches zu machen"", so Dürnberger. ""Es ist die absolute Verdinglichung des Tieres, die Unbehagen auslöst."" Die Tierwürde – ein umstrittener Begriff – liegt immer im Empfinden des Menschen begründet.
Im Stall der Zukunft

Sind wir also mit der Technologisierung der Landwirtschaft erneut drauf und dran, Tiere wie Gegenstände zu behandeln? Dürnberger: ""Die permanente Kontrolle bringt in Sachen Tiergesundheit viel. Zugleich zeigt sie, dass das Tier als bloße Ressource wahrgenommen wird."" Das Problem wird klar, wenn man die Sache in die Zukunft weiterdenkt. ""Was ist, wenn in einem automatisierten Stall der Zukunft nicht mehr der Bauer, sondern Algorithmen entscheiden, ob sich die Haltung eines Tiers noch lohnt, oder ob es geschlachtet werden soll?""";https://www.derstandard.at/story/2000109401814/digitalisierung-im-stall-die-ueberwachte-kuh;Standard;Alois Pumhösel
08.03.2019;Warum so wenige Frauen den Code knacken wollen;"Jedes Kind kann mit einem Smartphone umgehen. Sämtliche Lebensbereiche sind durchdigitalisiert. Wir sind umgeben von Informatik, leben tagtäglich mit Algorithmen und deren Auswirkungen. Am Horizont dräut der großflächige Einsatz künstlicher Intelligenz, ein Produkt ausgefeilter Programmierung von Informatikern – hauptsächlich männlichen wohlgemerkt.

Denn so facettenreich das Forschungsgebiet heute ist, so einseitig sind die Geschlechterverhältnisse. Trotz langjähriger Initiativen, Mädchen für die IT zu begeistern, dümpelt in Österreich der Anteil an Frauen unter Informatikabsolventinnen ohne große Veränderungen bei gerade einmal 15 Prozent dahin – und das, obwohl die Wirtschaft händeringend nach IT-Fachkräften jeden Geschlechts sucht, um den Anschluss an die internationale Entwicklung in Bezug auf Industrie 4.0, Machine Learning und künstlicher Intelligenz nicht ganz zu verpassen.

Das Phänomen ist auch anderswo bekannt, doch nicht überall so eklatant wie im deutschsprachigen Raum. In Bulgarien waren 2017 laut einer Eurostat-Untersuchung 26,5 Prozent der IT-Fachkräfte weiblich. Dem Spitzenreiter folgten vor allem östliche und nördliche EU-Staaten. Österreich lag mit 15,6 Prozent unter dem EU-Schnitt von 17,2 Prozent. Bei den US-Tech-Riesen sieht es im Übrigen nicht viel anders aus. Bei Apple waren im Jahr 2017 23 Prozent der IT-Jobs von Frauen besetzt, bei Facebook 19 Prozent.
Konservative Strukturen Doch was ist los? Hat die weibliche Hälfte der Generation Smartphone kein Interesse an technischen Details? Hat der Klischeetyp des tendenziell asozialen, jungen, männlichen Computernerds das Fach so fest im Griff, dass Frauen es sich dreimal überlegen, bevor sie sich das antun? ""Wir leben in Österreich und in Mitteleuropa in einer konservativen Gesellschaft mit konservativen Familien- und Denkstrukturen"", sagt Gertrude Kappel. Die Professorin am Institut für Softwaretechnik und interaktive Systeme ist im Dekanat der Fakultät für Informatik der TU Wien unter anderem für Diversity zuständig.

""Informatik ist eindeutig technisch konnotiert"", sagt Kappel. Daher rufe sie uralte Stereotypen auf den Plan, die Buben eher für technische Fächer befähigt sehen als Mädchen. Stereotype, die sich immer wieder reproduzieren, wenn Uni-Stellen, Forschungsinstitute genauso wie die Chefetagen der Techbranche zum größten Teil männlich besetzt werden.

Die Auswirkungen dieser Rollenbilder sind nach wie vor desaströs. In einer Studie der Fachhochschule Oberösterreich, in der mehr als hundert Schülerinnen befragt wurden, gaben neun von zehn Mädchen, die über ein Informatikstudium nachdachten, an, dass ihnen nahegelegt wurde, doch etwas Soziales, Kommunikatives oder Frauenspezifisches zu studieren. Wenig verwunderlich die daraus resultierenden Selbstzweifel: 75 Prozent der befragten Schülerinnen trauten sich ein Informatikstudium nicht zu.

Noch immer herrsche die Vorstellung von weltfremden Techies vor, berichten viele Expertinnen – ein falsches Bild, wie Kappel betont. ""20 Prozent des Informatikstudiums haben mit logischem Denken zu tun, manchmal auch mit nächtelangem Tüfteln an einem Problem. 80 Prozent bestehen darin, Probleme zu verstehen, über Lösungsansätze zu kommunizieren und mit der Gesellschaft zu interagieren.""

Und, der Vollständigkeit halber: ""Logisch denken können Frauen genauso gut wie Männer."" Neurobiologische Erklärungen, die Unterschiede im Gehirn für den Gender Gap verantwortlich machen wollen, halten sich hartnäckig, sind aber nachweislich Humbug. Schließlich war das Handwerk des Programmierens in den Anfängen des Computerzeitalters in den 50er- und 60er-Jahren eine absolute Frauendomäne (siehe Artikel hier).
IT ist in Indien und Malaysia viel weiblicher

Auch heute ist Informatik nicht überall ein ""Männerfach"": In Indien beträgt der Frauenanteil in den Computerwissenschaften rund 40 Prozent, in Malaysia bis zu 50 Prozent, ähnlich ist es in arabischen Ländern. Studien haben gezeigt, dass gerade in Ländern, in denen wenig Gleichberechtigung und soziale Absicherung herrschen, Frauen in der IT einen sicheren Job und ein gutes Einkommen sehen. In Ländern, wo die Gleichstellung weiter fortgeschritten ist, hätten Frauen mehr Wahlmöglichkeiten und würden sich daher öfter für eine andere Ausbildung entscheiden, so die These.

Doch auch wenn hierzulande Sexismus und Ungleichbehandlung kaum mehr so offensichtlich gepflegt würden wie zu früheren Zeiten, sei mitunter ein ""latentes Unbehagen Frauen gegenüber"" zu spüren, sagt Kappel. Und es könne immer noch vorkommen, dass ein männlicher Kollege ein Gespräch unter Informatikerinnen mit folgendem Satz quittiert: ""Na habt ihr eine wissenschaftliche Tupperparty gehabt?"", wie Kappel von einer Begebenheit vor nicht allzu langer Zeit berichtet. Sie könne so etwas ignorieren, aber ""eine Studienanfängerin läuft davon, wenn ihr so etwas passiert"".

Auch wenn solche Vorfälle nur eine Minderheit betreffen, Tatsache ist, dass viele Frauen aus verschiedensten Gründen im Lauf ihrer Karriere durch ""Lecks in der Pipeline"" verlorengehen (unter anderem durch die Geburt eines Kindes) – auch wenn sich die Tendenz langsam zu ändern scheint, zumindest an den technischen Universitäten insgesamt. Daten speziell für das Fach Informatik liegen diesbezüglich nicht vor.

Doch was tun angesichts dieses tiefen Grabens zwischen den Geschlechtern? Bei den Kleinsten anfangen, ist Laura Kovacs, Informatikerin an der TU Wien, überzeugt. ""So wie unsere Kinder schreiben und lesen lernen, sollten sie von Beginn an lernen, mit Computern umzugehen. Mit spielerischen Mathematikübungen kann man schon im Kindergarten anfangen.""

Agata Ciabattoni, ebenso Informatikerin an der TU Wien, sieht das Problem vor allem in Österreichs geteiltem Schulsystem: ""Kinder müssen zu früh entscheiden, was sie einmal machen wollen. Eltern schicken Töchter eher an sprachorientierte als an technische Schulen.""

Die vielzitierten Role Models seien nicht primär ausschlaggebend dafür, dass Frauen in einem männerdominierten Feld reüssieren, hat die Sozialwissenschafterin Marita Haas in einer FWF-Studie herausgefunden. Vielmehr gehe es um eine gewisse Offenheit in Bezug auf Lebens- und Karriereplanung, ein ermutigendes Umfeld und strukturelle Maßnahmen wie Quoten und Diversitätsinitiativen.

Allein die Bezeichnung eines Studiengangs ist entscheidend dafür, wie viele Frauen sich dafür anmelden, wie Isabel Roessler vom deutschen Centrum für Hochschulentwicklung in der groß angelegten Studie ""Fruit: Frauen in IT"" herausgefunden hat. Je mehr die praktischen Anwendungen in der ""echten Welt"" durchklingen, desto höher der Frauenanteil. Dafür sprechen auch die Zahlen: So waren in Österreich 2017 24,6 Prozent Frauen im Informatik-Zweig Wirtschaftsinformatik inskribiert, aber nur 7,9 Prozent im Fach Telematik.
Programmierkurse nicht nur für Nerds

Was Kursgestaltung und neue Lehrpläne bewirken können, zeigt das Beispiel der renommierten US-Universität Carnegie Mellon: Um den Frauenanteil im Fach Informatik zu steigern, wurden umfassende Interviews mit Studierenden geführt, Highschool-Lehrer geschult und neue Curricula entwickelt. Zwischen 1995 und 2000 stieg der Anteil an Studienanfängerinnen von sieben auf mehr als 40 Prozent.

Fest steht: Den Unis ist das Problem bewusst. Und es wird weiter versucht, mit allen Mitteln die Männerdomäne Informatik zu knacken: Maturanten und Maturantinnen, die das Gefühl haben, sie könnten nicht mit den Nerds mithalten, können sich mit speziellen Online-Programmierkursen auf das Studium vorbereiten. Mentoring-Programme begleiten Erstsemestrige von Anfang an, interdisziplinäre Brücken werden geschaffen, um das Informatikstudium attraktiver zu gestalten. In Doktoratsprogrammen gibt es häufig eine Frauenquote. Davon profitieren vielfach weibliche Studierende aus Ost- und Südeuropa sowie aus Ländern des Vorderen Orients, die nach Österreich strömen.

""Es besteht die begründete Hoffnung, dass wir in 20 Jahren nicht mehr über dieses Thema reden müssen"", ist Gertrude Kappel überzeugt. ""Heute ist ein anderer Druck von Frauen vorhanden, die langsam in alle Bereiche vordringen."" Es wird sich zeigen, ob die vielen Maßnahmen letztlich greifen, um die starren Geschlechterverhältnisse dauerhaft aufzubrechen – damit mehr Frauen die Ausgestaltung der digitalen Zukunft in die Hand nehmen.";https://www.derstandard.at/story/2000099099551/warum-so-wenige-frauen-den-code-knacken-wollen;Standard;Karin Krichmayr
25.01.2011;"""Freie Formen fordern neue geometrische Modelle""";"STANDARD: Wo begegnet uns geometrisches Modellieren im Alltag?

Pottmann: Heutzutage steckt beinahe hinter jedem Produkt ein Computermodell - von der Cola-Dose bis zum Essbesteck - und damit auch Methoden der geometrischen Modellierung. Ein prominentes Beispiel dafür ist der Karosseriebau in der Autoindustrie; ein Einsatzgebiet, das die Anfänge dieser Wissenschaft massiv geprägt hat. Denn dort gibt es Formen, die man nur mit dem Computer beherrschen kann.

STANDARD: Wo kommt es noch zum Einsatz?

Pottmann: Auch Filmen wie Shrek oder Findet Nemo liegen dreidimensionale Computermodelle zugrunde. Dazu gehören auch physikalische Simulationen, um etwa das Verhalten von Wasser oder die Lichtbrechung mit der gewünschten Realitätsnähe abzubilden. Geometrische Modellierung stellt in diesem Zusammenhang letztlich die Werkzeuge, die Software bereit, mit der die Animateure arbeiten. Führende Animationsstudios wie Pixar beschäftigen eine Reihe von Topforschern auf diesem Gebiet, und Disney hat an der ETH Zürich ein einschlägiges Forschungszentrum eingerichtet.

STANDARD: Sie sprechen sich vehement für das Unterrichtsfach darstellende Geometrie aus. Warum?

Pottmann: Das Fach wurde fast überall in Europa bereits abgeschafft. Das hat negative Effekte auf das Bildungsniveau in der Geometrie. Dass wir in Österreich so stark sind auf dem Gebiet des Visual Computing, hat auch viel mit der Schulausbildung zu tun. Das ist ein Alleinstellungsmerkmal im internationalen Vergleich.

STANDARD: Wie kommen Sie zu dieser Annahme?

Pottmann: Ich hatte vergangenes Jahr eine Vorlesung über angewandte Geometrie an der King Abdullah University of Science and Technology (Kaust) und diese so gehalten, wie ich sie auch an der TU Wien halten würde. Dabei habe ich festgestellt, dass die Studenten das großteils nicht verstanden haben - und das sind immerhin handverlesene Studenten aus aller Herren Länder. Das hat mir den Niveauunterschied verdeutlicht. Die darstellende Geometrie ist es wert, als eigenes Fach erhalten zu bleiben, denn sie hat einen positiven Einfluss auf die Schulung der Raumvorstellung. Ich denke dabei an das Fach geometrisches Zeichnen in der Unterstufe.

STANDARD: Hat dieses Fach nicht ein sehr angestaubtes Image?

Pottmann: Das Fach ist heute viel moderner. Es wird professionelle 3-D-Modellisierungssoftware eingesetzt und auch fächerübergreifend gelehrt. 3-D-Modelle können nur dann verstanden werden, wenn man eine gewisse geometrische Schulung hat. Pläne und Skizzen sind sozusagen Kommunikationsmittel des Ingenieurs, und man muss sie verstehen lernen. Mit CAD (Computer Aided Design, Anm.) ist heute mehr machbar als früher, das heißt, es sind auch viel komplexere Modelle möglich. Man muss also noch mehr Verständnis für die Geometrie mitbringen.

STANDARD: Demnach müsste die Wirtschaft Interesse haben, dass diese Ausbildung erhalten bleibt.

Pottmann: Hat sie auch. Wenn ich mir das Büro von Zaha Hadid in London vor Augen führe: Dort sitzen mehrere hundert graduierte Architekten und arbeiten mit CAD. Ich komme deshalb darauf, weil mir die Architektur ein besonderes Anliegen ist. Mit dem Einsatz von mehr Mathematik, insbesondere computergestützter Geometrie in der Architektur, haben wir fast einen Trend in der Forschung entwickelt: die Architekturgeometrie.

STANDARD: Was kann man sich darunter vorstellen?

Pottmann: In der zeitgenössischen Architektur gibt es die Tendenz zu sogenannten freien Formen. Die bekanntesten Vertreter sind Architekten wie Frank Gehry und Zaha Hadid. Sie verwenden für ihre Entwürfe Software, die aus einem anderen Gebiet, der Autoindustrie stammen. In der Architektur ist die Skala naturgemäß viel größer. Dies erforderte andere geometrische Modelle. Wir befassen uns also mit Verfahren, um freie Formen in kleinere, einfachere Teile wie Paneele oder Träger zu zerlegen, sodass die Ästhetik und die Intention des Entwurfs erhalten bleiben. Es gibt nebenbei auch einen Rückfluss in die reine Mathematik.

STANDARD: Welche Trends können Sie im Zusammenhang mit Visual Computing erkennen?

Pottmann: Visual Computing ist ein Konglomerat aus unterschiedlichen Forschungsgebieten, deren Grenzen aber immer mehr verschwimmen: Rendering, Simulation und Animation, Visualisierung, Virtuelle Realität, geometrische Modellierung, Bildverarbeitung, Mustererkennung, maschinelles Sehen, im Grunde auch Fernerkundung und noch einiges mehr. Die Methoden kommen aus der Informatik und der Mathematik und je nach Anwendung aus verschiedenen Ingenieurswissenschaften. Ein genereller Trend, der überhaupt erst zur Begriffsbildung Visual Computing geführt hat, ist, dass diese Gebiete zunehmend verschmelzen. Es gibt einen Pool von Methoden, den man in fast allen Teilbereichen findet.

STANDARD: Klingt kompliziert.

Pottmann: Ist es auch: Zur Lösung eines Problems oder um einen Fortschritt auf einem Gebiet zu erzielen, muss man auch den Überblick über die anderen Bereiche haben. Ein weiterer Trend ist der Einfluss des Internets. Es gibt viele Datenbanken von 3-D-Modellen im Netz. Es macht keinen Sinn, diese Modelle stets von Grund auf neu zu erstellen. Wir brauchen neue Methoden, um aus der Vielfalt der Modelle, die bereits vorhanden sind, die jeweils wichtigsten schnell zu finden.

STANDARD: Das heißt, eine Suchmaschine für geometrische Modelle?

Pottmann: Ganz genau. Dazu gibt es bereits interdisziplinäre Forschung. Außerdem lässt sich beobachten, dass gewisse Entwicklungen in der Mathematik von einem Teilgebiet aufgegriffen werden und dort eine Art Mode ausbilden, die dann in der Folge die Top-Tagungen prägt und dann wieder in einen anderen Bereich des Visual Computing schwappt. Momentan ist das so mit dem sogenannten Machine Learning, also einem Teilgebiet der künstlichen Intelligenz. Ich sehe gewisse Chancen für hochdimensionale mathematische Modelle, die über das Machine Learning noch hinausgehen.

STANDARD: Sie halten heute Nachmittag einen Vortrag auf dem Visual-Computing-Symposium des Zentrums für Virtual Reality und Visualisierung, VRVIS. Worüber werden Sie sprechen?

Pottmann: Ich habe geplant, bei meinem Vortrag ein Beispiel aus meiner Arbeit bei Kaust zu zeigen, wo die Motivation aus der Architektur kommt. Architektur ist der Renner, weil es hier noch nicht so viel Konkurrenz gibt: Wir waren die Ersten, die sich anhand von mathematischen Methoden mit dem Thema freie Formen in der Architektur befasst haben.";https://www.derstandard.at/story/1295570821131/interview-freie-formen-fordern-neue-geometrische-modelle;Standard;DER STANDARD
10.08.2017;"Googles künstliche Intelligenz erklärt Strategiespiel ""Starcraft 2"" den Krieg";"Vor nicht allzu langer Zeit schien es noch undenkbar, dass ein Computer die besten menschlichen Spieler im Brettspiel Go schlagen kann. Ist doch Go erheblich komplexer als etwa Schach, das ""simple"" Ausrechnen sämtlicher möglicher Züge stellt hier also keine Option dar. Doch dank der Hilfe von künstlicher Intelligenz ist mittlerweile auch diese Hürde genommen. Nun wollen sich jene Forscher, deren Software schon die stärksten Go-Spieler besiegt hat, neuen Herausforderungen stellen.
Starcraft 2

Googles Maschinenlernabteilung Deepmind hat den Spieleklassiker ""Starcraft 2"" als nächstes Ziel für seine neuronalen Netzwerk auserkoren. Ziel ist es dabei auch hier früher oder später besser als alle menschlichen Spieler zu werden. Dabei kann man auf die Unterstützung von Spieleentwickler Blizzard setzen, der passend dazu diverse Tools veröffentlicht hat, mit deren Hilfe AI-Entwicklern die Bot-Entwicklung erleichtert wird. Allerdings wurde Wert darauf gelegt, dass die künstliche Intelligenz hierdurch keinerlei unfairen Vorteil erhält, versichert Blizzard.

Zudem haben die beiden Unternehmen Daten zu 65.000 vergangenen ""Starcraft 2""-Spielen veröffentlicht, wie Wired berichtet. Diese sollen als eine Art Starthilfe dienen, und so auch andere Forscher außerhalb von Google anziehen. Bereits jetzt ist bekannt, dass sich Facebook derzeit ebenfalls an ""Starcraft"" versucht, auch wenn man dabei – zumindest derzeit – den ersten Teil des Spiels gewählt hat.
Herausforderung

Etwaige Erwartungen, dass die Deepmind AI ""Starcraft 2"" recht bald beherrschen wird, dämpft Google rasch. Ein Echtzeitstrategiespiel sei für künstliche Intelligenz wesentlich schwerer zu beherrschen als etwa Go. Nicht nur, dass die Zahl der möglichen Spielkombinationen ungleich höher ist, hat man hier auch keinen perfekten Überblick, wie es bei einem Brettspiel der Fall ist. Die AI muss zum Teil also oft schätzen, was der Gegner gerade tut.
Mit Minispielen will Deepmind seiner AI die Grundlagen von ""Starcraft 2"" beibringen.
DeepMind

Zudem müsse die Künstliche Intelligenz lernen, mit Unsicherheiten umzugehen und einen eigenen Spielstil zu entwickeln, was ganz neue Herausforderungen an die Entwicklung stellt. Aber genau das ist schlussendlich auch der Grund, warum man ""Starcraft 2"" und nicht ein einfacheres Ziel für die Forschung gewählt hat, betonen die Entwickler.
Konkreter Nutzen

All das mag nach Spielereien mit wenig praktischem Nutzen klingen, doch diese Annahme wäre verfehlt. Nutzt doch Google die aus solchen Forschungen gewonnen Erkenntnisse längst, um Verbesserungen an den eigenen Produkten und Services vorzunehmen. So helfen neuronale Netze Google mittlerweile etwa dabei die Kühlsysteme in den eigenen Rechenzentren zu optimieren – und damit ganz konkret den Stromverbrauch zu reduzieren und auch Geld zu sparen.";https://www.derstandard.at/story/2000062499557/googles-kuenstliche-intelligenz-erklaert-starcraft-2-den-krieg;Standard;apo
23.05.2019;Hauszustellung von Paketen wird zum Luxus;"Adresse laut Routenplaner ansteuern, Abstellmöglichkeit für den Lieferwagen finden, im Laderaum das richtige Paket suchen; Türnummer finden, läuten, warten; Empfänger unterschreiben lassen, einen Zettel hinterlegen oder eine Abstellmöglichkeit nutzen; dann geht's auf zur nächsten Adresse. Das Spiel beginnt von vorn, dutzende Male am Tag. Paketzustellung zur Haustür ist eine zeit- und kostenintensive Dienstleistung.

In Zeiten vor dem E-Commerce-Boom war sie noch leicht bewältigbar. Mit dem Aufstieg der Onlinehändler geraten die an der Zustellung beteiligten Infrastrukturen aber schwer unter Druck. Lieferwagen verstopfen Innenstädte und lassen CO2-Emissionen ansteigen.

Logistiker leiden unter immensem Kostendruck. Fahrer werden zur Mangelware. Die ""letzte Meile"" wurde zur großen offenen Wunde des Logistikgeschäfts. Offen bleibt, wie sie sich nachhaltig schließen lässt.

Fix ist: Die Paketlawine schwillt weiter an. Und zwar massiv. Das Beratungsunternehmen Oliver Wyman geht für Deutschland von einer annähernden Verdreifachung der auszuliefernden Pakete bis 2028 aus. Waren es dort im Jahr 2018 etwa 3,5 Milliarden Pakete, sollen es eine Dekade später neun Milliarden sein.

Nicht nur bestellen Kunden immer mehr Elektronik, Medien und Kleidung im Netz, auch Aspekte wie zunehmender Direktvertrieb von Herstellern oder neues Paketaufkommen im Business-to-Business-Bereich – beispielsweise beim Ersatzteilhandel – tragen dazu bei. ""Unsere Zahlen bilden ein mittleres Wachstumsszenario für die nächsten zehn Jahre ab. Es gibt durchaus auch Schätzungen, die deutlich darüber hinausgehen"", ordnet Michael Lierow von Oliver Wyman die Entwicklungen ein.
Neue Lösungen gesucht

Das Wachstum kann nicht ohne Folgen bleiben. Sprich: Die Zustellung kann nicht weiterhin gleichzeitig flächendeckend und so kostengünstig wie bisher erfolgen. Die wachsenden Paketberge verlangen nach neuen Lösungen auf der letzten Meile, betonen die Studienautoren von Oliver Wyman.

Die bisher in Deutschland und Österreich vollkommen selbstverständliche Lieferung an die Haustür könnte dabei zum ""Luxusgut"" werden. ""Die Preissteigerungen werden von den Zustellunternehmen jetzt noch zurückgehalten. Langfristig werden sie aber kommen"", prognostiziert Lierow.

Wesentlicher Kostentreiber werden die Personalkosten für die Zusteller sein. ""Das Problem ist, dass für die großen Paketmengen wesentlich mehr Fahrer benötigt werden. Der Arbeitsmarkt ist hier aber bereits jetzt vielerorts leergefegt"", sagt der Experte. Also müssten die Löhne steigen. ""In Gebieten mit geringer Arbeitslosigkeit – etwa im Raum München – sehen wir heute schon Stundenlöhne von Fahrern, die bis zu 35 Prozent über dem Mittel liegen"", sagt Lierow. ""In den kommenden Jahren ist zu erwarten, dass die Logistiker noch einmal 30 bis 40 Prozent drauflegen müssen.""
Höhere Löhne, höhere Preise

In ihrer Analyse gehen Lierow und Kollegen davon aus, dass der Bedarf an Lieferfahrern 2028 in Deutschland bei etwa 200.000 liegen wird. Zum Vergleich: 2018 waren hier etwa 90.000 Zusteller unterwegs. Um den Job entsprechend attraktiv zu gestalten, wäre eine Verdoppelung der Stundenlöhne von etwa 15 auf 30 Euro nötig, rechnen die Logistikexperten vor.

Das würde wiederum die Kosten pro Paket von heute 2,50 Euro auf 4,50 Euro ansteigen lassen. ""Nicht alle werden bereit sein, einen derart hohen Preis für die Zustellung an der Haustür zu bezahlen"", sagt Lierow voraus.

Doch ließe sich das Problem nicht mit neuen Technologien lösen? Immerhin wird viel über künftige Einsätze von Drohnen, Robotern und autonomen Fahrzeugen auf der letzten Meile spekuliert. In Spezialfällen könnten manche dieser Technologien auch durchaus zur Anwendung kommen – etwa eilige Medikamentenlieferungen durch Drohnen. Die große Masse an Paketen werden diese Spielarten zumindest in naher Zukunft aber nicht bewegen.

""An vollständig autonome Zustellfahrzeuge bis 2028 glaube ich nicht"", sagt Lierow. ""Was bis dahin aber möglich wäre, ist etwa sogenanntes Platooning auf Langstrecken."" Dabei sitzt nur noch im ersten Lkw eines Konvois ein menschlicher Fahrer, weitere folgen autonom.

Doch selbst diese Technologie bringt im Lieferverkehr entscheidende Nachteile mit sich. Lierow: ""Um etwa die Sortierkapazitäten eines Verteilerzentrums zu nutzen, ist es vorteilhaft, die Paketlieferungen nach und nach einlangen zu lassen und nicht auf einmal in einem großen Konvoi.""
Packstationen

Andere Mittel der Digitalisierung werden durchaus intensiver zum Zug kommen: etwa Routenplanungssysteme, die die Fahrzeuge vorausschauend und automatisch durch den Verkehr lotsen. ""Diese Technologien werden in den nächsten Jahren noch stärker eingesetzt werden, um effizienter zu planen und Kosten zu senken"", erklärt Lierow. ""Der Einsatz von Machine-Learning-Algorithmen macht die vorhandenen Daten dabei wesentlich besser nutzbar.""

Die größte absehbare Veränderung wird aber darin liegen, dass zumindest ein Teil der letzten Meile an den Paketempfänger selbst delegiert wird: Man wird sein Paket bei Filialen, Packstationen oder Paketautomaten abholen müssen. ""Die kostenintensive Filiallösung wird für die Logistiker eher die Ausnahme bleiben"", so die Einschätzung Lierows. ""Wesentlich sinnvoller sind Packstationen, in denen die Pakete auf kleiner Fläche verwahrt und vom Kunden jederzeit entnommen werden können.""

Eine Station mit beispielsweise 200 Fächern könnte, mehrmals nachgefüllt, den Kunden pro Tag gut 500 Pakete übergeben. Dementsprechend viele Fahrwege würden eingespart. ""In Polen werden heute bereits viele Pakete auf diese Art zugestellt. In Großbritannien kooperieren Hermes und die polnische Inpost, in Deutschland DHL und Amazon beim Ausbau der Packstationen"", gibt Lierow Beispiele für die Entwicklung.

Laut der Analyse würden sich die Kosten dieser ""Multidrop-Zustellung"" im Jahr 2028 auf 2,80 bis 3,30 Euro pro Paket belaufen – ein Drittel günstiger als die erwarteten Kosten für die Hauszustellung.
Städte in der Pflicht

Vermisst wird in dieser Entwicklung oftmals ein entschiedenes und innovatives Auftreten der Städte. ""City-Logistik-Konzepte gibt es in der Theorie schon seit mehreren Jahrzehnten, allerdings konnten sie in der Praxis selten richtig abheben.

Dabei würden Regularien, die etwa keine Fahrten halbvoller Lkws in die Innenstädte zulassen, der Verkehrsplanung sehr helfen"", sagt Lierow. ""Mit dem Trend Richtung Packstationen geht es nun aber auch ohne Zutun der Städte in die richtige Richtung.""

Wenn Hauszustellung nun zum Luxus wird, könnte dann eigentlich der stationäre Handel wieder an Bedeutung gewinnen? Immerhin fällt ein großer Vorteil der Internetbestellung weg, wenn man ohnehin zur Packstation muss.

Lierow ist in dieser Sache vorsichtig: ""Abhängig von den Standorten von Geschäften und Paketstationen kann das ein kleines Stück weit der Fall sein. Der Onlinehandel kann aber auch bei Preis, Auswahl und Verfügbarkeit auf Vorteile pochen. Eine generelle Trendumkehr sehe ich hier also nicht.""";https://www.derstandard.at/story/2000103558912/hauszustellung-von-paketen-wird-zum-luxus;Standard;Alois Pumhösel
03.04.2017;Google Translate soll mehr Textverständnis lernen;"Wenn Maschinen Texte übersetzen, gehen sie meist Satz für Satz vor. Dabei stehen wichtige Informationen oft an anderer Stelle im Text. Schweizer Forscher verfolgen den Ansatz, Übersetzungsprogrammen mehr Textverständnis beizubringen.
Probleme mit Pronomen

Programme wie Google Translate verwenden Statistik, um die wahrscheinlichste Übersetzung von Wortgruppen in Sätzen zu liefern. Hinter menschlichen Übersetzerinnen liegen die Maschinen jedoch noch meilenweit zurück. Einer der Gründe: Die Algorithmen schauen nicht über die Grenzen eines Satzes hinaus. Dadurch haben sie etwa Mühe mit Pronomen, wie ""sie"" oder ""diese"", da das, worauf sie sich beziehen, in einem anderen Satz steht.

Forscher um Andrei Popescu-Belis vom Forschungsinstitut Idiap in Martigny (Kanton Wallis) wollen das im Rahmen eines vom Schweizerischen Nationalfonds SNF unterstützten Projekts ändern, indem sie den Algorithmus auch angrenzende Sätze analysieren lassen. Am Montag stellten sie ihre neuesten Ergebnisse bei einer Konferenz der ""Association for Computational Linguistics"" im spanischen Valencia vor.

Ein Beispiel, wie das ""Satz für Satz"" Vorgehen von Übersetzungstools Probleme mit Pronomen verursacht: ""Meine Tante hat eine tolle Limousine gekauft. Sie ist aber nicht so schön."" Google Translate übersetzt dies so ins Englische: ""My aunt has bought a great sedan. But she is not so beautiful."" Der englischsprachige Leser liest also, dass die Tante nicht so schön sei, weil ""sie"" in Zusammenhang mit ""schön"" öfter mit ""she"" übersetzt wird als mit ""it"".
Schwierige Übersetzungen zwischen Französisch und Englisch

Solche Probleme bestehen insbesondere bei Übersetzungen zwischen Französisch und Englisch sowie Englisch und Spanisch. Programme wie Google Translate irren sich beim Übersetzen von Pronomen bei diesen Sprachpaaren in rund der Hälfte der Fälle.

Das von Popescu-Belis' Team gemeinsam mit Kollegen von den Universitäten Genf, Zürich und Utrecht entwickelte Tool senkt diese Fehlerrate auf 30 Prozent. Der Trick: Die Wissenschafter brachten dem Übersetzungsalgorithmus mittels maschinellem Lernen bei, auch angrenzende Sätze zu berücksichtigen. ""Im Prinzip geben wir dem System an, wie viele der voranstehenden Sätze es in welcher Weise analysieren muss. Dann testen wir es unter realen Bedingungen"", sagte Popescu-Belis.

Noch sei die Technik zwar nicht ausgereift für die breite Anwendung, allerdings hat das Projekt schon die Aufmerksamkeit von Anbietern von Übersetzungsprogrammen auf sich gezogen. Das Forschungsteam sieht in der Lösung des Pronomen-Problems indes nur den Anfang. Mit der gleichen Idee, Maschinen mit mehr Textverständnis übersetzen zu lassen, wollen sie auch Aspekte wie die korrekte Abfolge der Zeiten oder die zum Kontext passende Terminologie verbessern.";https://www.derstandard.at/story/2000055292936/google-translate-soll-mehr-textverstaendnis-lernen;Standard;APA
12.11.2019;Erstmals Klage gegen Gesichtserkennungssoftware in China;"Professor Guo Bing zeichnet seit Anfang November für den ersten Gerichtsprozess zum Thema Gesichtserkennung in China verantwortlich. Der Wissenschafter der Universität Zhejiang Sci-Tech im Osten Chinas wollte nicht hinnehmen, dass der Safaripark Hangzhou mit 17. Oktober seine Jahreskartenbesitzer plötzlich zu einem verpflichtenden Gesichtsscan aufforderte. Wer sich weigert, dem soll künftig der Zutritt zu dem Zoo verwehrt bleiben, hieß es in der Textnachricht an die Jahreskartenbesitzer. Bis dahin mussten bereits jedes Mal die Fingerabdrücke gescannt und die nationale Identitätskarte vorgewiesen werden. Alle Maßnahmen würden der Sicherheit und dem Komfort der Gäste dienen, hieß es von den Parkbetreibern.

Guo wollte daraufhin seine Jahreskarte kündigen und forderte eine volle Rückerstattung des Kaufpreises, was ihm jedoch verwehrt wurde, da er den Park bereits mehrmals im laufenden Jahr besucht hatte. Als Guo daraufhin eine Klage wegen Konsumentenschutzverstößen aufgrund des ""verpflichtenden Sammelns individueller Charaktereigenschaften der Besucher"" einbrachte, die Behörden die Klage akzeptierten und der Fall Aufmerksamkeit in Chinas sozialen Medien erlangte, lenkte der Zoo ein. Man bot dem Professor an, zu den bisherigen Bedingungen den Park besuchen zu können.

""Ich habe diese Klage nicht aus wirtschaftlichen Gründen eingebracht"", sagte Guo einer Regionalzeitung. Er habe deutlich sein Unbehagen und seine Sorge über das Sammeln biometrischer Daten zum Ausdruck gebracht. Dabei bestünden zu viele Unsicherheiten und Sicherheitsrisiken, weshalb es hier mehr Regulierung brauche. Ein Zoosprecher beteuerte gegenüber dem Onlinemedium ""Sixth Tone"", dass man selbstverständlich Maßnahmen treffe, um die biometrischen Daten sicher aufzubewahren, wollte dabei aber nicht ins Detail gehen.
Schlechte Chancen für Klage

Juristen wie Mimi Zou von der Universität Oxford räumen der Klage jedoch nur geringe Chancen ein. Es gebe derzeit keine verbindlichen Gesetze, die die Argumentation des Klägers untermauern. Da der private Zoo die Gesichtserkennung zur Bedingung für den Einlass mache, müsse es laut derzeitiger Rechtslage keine optionale Zustimmung durch die Kunden geben, sagte sie der BBC. Dennoch sei in China in den vergangenen Jahren ein loses Netzwerk aus vereinzelten Datenschutzgesetzen sowie eine freiwillige nationale Richtlinie für die Verwendung privater Daten entstanden. Diese sind rechtlich noch eher zahnlos, ""bilden aber eine normative Grundlage für ein rechtlich bindendes Rahmenwerk"", das künftig entstehen könnte. Sogar die Finanzriesen Alipay und Wechat Pay sollen Teile des strengeren freiwilligen Datenschutzes testweise bereits umgesetzt haben.
In China ersetzt das Gesicht immer öfter die Geldbörse.

Zou sieht diese Entwicklung in Zusammenhang mit einer steigenden Unzufriedenheit der Bevölkerung über den Umgang von Privatunternehmen mit privaten Daten. Die staatliche Gesichtserkennung ist trotz aller mahnenden Stimmen aus dem Ausland aufgrund ihrer Erfolge in der Verbrechensaufklärung weiterhin sehr beliebt bei den Chinesen. Bis zu 60.000 mutmaßliche Verbrecher seien alleine dank der Gesichtserkennung bereits aufgegriffen worden, heißt es. Mehrere Privatsphäreverletzungen durch private Unternehmen in den vergangenen Monaten – etwa der Weiterverkauf biometrischer Daten durch Handy-Apps oder der einfache Hack von Schließfächern mit Fotos – sensibilisierten aber zumindest einen Teil der Bevölkerung.
Komfort versus Überwachung

""Neue Technologien bringen Komfort in das Leben der Menschen"", aber man müsse stets vor Missbrauch gewarnt sein, sagte etwa ein Nutzer des chinesischen Mikrobloggingdiensts Weibo in Anlehnung an die Klage des Uni-Professors. ""Schauen die Menschen noch die Tiere an, oder ist es umgekehrt?"", fragte ein anderer User zynisch. Tatsächlich schwingt die staatliche Überwachung im Kern immer mit. ""In diesem Bereich gibt es keine Privatsphäre"", sagt Zou.

Tatsächlich haben in China in den vergangenen Jahren nicht nur private Firmen, sondern auch die öffentliche Hand die Gesichtserkennung und damit das Sammeln biometrischer Daten massiv vorangetrieben. Immer öfter ersetzt das Gesicht dabei den QR-Code am Handy, sowohl an Bahnsteigen auch als an Flughafengates – ja, sogar an Schulen. Bis Ende des Jahres sollen zudem bis zu eine halbe Milliarde Sicherheitskameras, über das ganze Land verteilt, fertig aufgestellt sein.
Einchecken per Gesichtsscan – an der Uni.

Das Urteil im Zooprozess – das noch aussteht – wird eher keine Datenschutzrevolution in China auslösen. Es könnte aber die öffentliche Debatte über das Thema befeuern, vor allem wenn auch Ausländer mit höheren Ansprüchen an Datenschutz und Privatsphäre zunehmend von Gesichtsscans betroffen sind.
Erleichtertes Bezahlen für Touristen

Vergangene Woche ermöglichten etwa Chinas Finanzriesen Alipay und Wechat Pay Touristen fortan das bargeldlose Bezahlen mit ausländischen Kreditkarten. War bis vor kurzem noch ein chinesisches Bankkonto nötig, so können die mehr als 140 Millionen Touristen, die China jedes Jahr besuchen, nun per App ihre Kredikarten um eine virtuelle chinesische Karte ergänzen, mit der das oftmals mühsame Bezahlen in China erleichtert werden sollte. Das Bezahlen per Gesichtserkennung, das sich in China bereits großer Beliebtheit erfreut, dürfte wohl bald auch schon für China-Besucher möglich sein.";https://www.derstandard.at/story/2000110927617/erstmals-klage-gegen-gesichtserkennungssoftware-in-china;Standard;Fabian Sommavilla
14.09.2018;Künstliche Intelligenz als Gitarrenlehrer;"Beim Erlernen eines Instruments wie der Gitarre können auch Erkenntnisse aus der Sportwissenschaft behilflich sein. Immerhin geht es zuerst darum, Koordination und Feinmotorik der Finger zu verbessern, Bewegungsabläufe zu wiederholen und entsprechendes Muskelgedächtnis aufzubauen. Bei diesem Lernvorgang, so dachten sich Florian Lettner und Wolfgang Damm, könnten Sensorik und Rechenkraft eines Smartphones gute Dienste leisten.

Das Angebot an Apps, die das Gitarrenspiel lehren, ist nicht gerade knapp. Dennoch konnte sich Fretello, wie die beiden Absolventen der FH Oberösterreich in Hagenberg ihre Anwendung nannten, durchsetzen, was – laut Lettner – derzeit 200.000 Downloads und über 100.000 registrierte Benutzer bezeugen. Ein Argument für ihre Lernplattform sind ausgeklügelte Artificial-Intelligence-Algorithmen, die zielgerichtete Hilfestellung beim Üben von Pop- und Rockmusikstücken ermöglichen.

Musik spielte für die Gründer immer eine wichtige Rolle. Lettner spielte bereits mit 14 in einer Punkband. Schlagzeug lernte er an der Musikschule, Gitarre brachte er sich selbst bei. Der berufliche Werdegang führte über ein Mobile-Computing-Studium in Hagenberg zur ersten eigenen Firma, in der er mit Damm im Auftrag eines US-Anbieters Software für 360-Grad-Kameras entwickelte. 2016 war klar, dass man etwas Neues machen wollte – da lag die Musik nahe. Lettner: ""Wir hatten das technische Know-how und die Erfahrung in der Forschung.""
Trainingspläne

Möchte man Solieren, Technikaufbau oder das Nachspielen von Songs lernen? Darstellung in Noten oder Tabulatur? Kann man bereits Sechzehntelnoten oder Triolen spielen? Nachdem Vorlieben und Kenntnisse abgefragt sind, erstellt Fretello – auch auf Basis des Musikgeschmacks, der sich in Social-Media-Vorlieben spiegelt – einen Trainingsplan. Der Algorithmus wählt für die 20-minütigen Einheiten, die mehrmals pro Woche zu absolvieren sind, aus 20.000 Übungen und 7000 Musikstücken, erläutert Lettner.

Während des Übens ""hört"" die App nun dem Musikschüler zu. Kontrolliert wird nicht nur, ob der richtige Ton gespielt wird. Auch das Spieltempo wird vermessen. Die Artificial Intelligence, die mithilfe eines großen Datensatzes trainiert wurde, um Audiosignale klassifizieren zu können, erkennt Abweichungen aller Art und schließt auf ihre Fehlerquelle – etwa wenn Störgeräusche entstehen, weil eine der Saiten nicht richtig gedrückt wurde.

""Unser System erkennt, welche leichten oder schweren Fehler häufig vorkommen und gibt Feedback – beispielsweise in Form eines Videos, das zeigt, wie man die Spielweise anpassen kann"", sagt Lettner.
Zusammenklang

Das funktioniere unabhängig vom Musikstück und beim Zusammenklang von Akkorden genauso wie bei Improvisationen, bei denen kontrolliert wird, ob die gespielten Töne zusammenpassen. Zu den Forschungspartnern des Start-ups gehören neben der FH Hagenberg und der JKU Linz aktuell vor allem das Austrian Research Institute for Artificial Intelligence (OFAI) in Wien.

Die App könne den Unterricht durch einen Musiklehrer und die Erarbeitung einer korrekten Spielweise unterstützen. Schwieriger sei es dagegen, mit diesen Mitteln an Artikulation oder eigenem Stil zu arbeiten, räumt Lettner ein. ""Die emotionale Komponente ist aktuell nicht unser Ziel.""

Die Gründer, beide Anfang 30, konnten den finanziellen Grundstein zu Fretello, an dem zurzeit elf Personen arbeiten, durch den Erfolg ihrer ersten Firma legen. Die Prototypumsetzung gelang durch zwei Projekte, die von der Förderagentur FFG unterstützt wurden. 2016 war eine erste Version online, Anfang 2018 konnte durch die Puls-4-Show ""2 Minuten, 2 Millionen"" ein Investment von 300.000 Euro lukriert werden. Für Nutzer kostet die Anwendung nach einer Testphase zehn Euro pro Monat oder 100 Euro pro Jahr.

Im Moment verhandeln die Gründer mit großen Musikpublishern und Rechteinhabern über Kooperationen. Lettner: ""Künftig soll es möglich sein, dass die Nutzer nicht nur Übungen absolvieren, sondern auch ihre Lieblingssongs nachzuspielen lernen.""";https://www.derstandard.at/story/2000087120831/kuenstliche-intelligenz-als-gitarrenlehrer;Standard;Alois Pumhösel
20.11.2015;Jeder Vierte startet mit Sinnkrise ins Berufsleben;"Das Berufsleben beginnt bei jedem vierten Absolventen bereits mit einer Sinn- und Identitätskrise: Das ist das Ergebnis einer Studie von Wissenschaftern der Privatuni Schloss Seeburg. Dafür wurden 2640 Studierende im deutschsprachigen Raum online befragt.

""Die sogenannte Quarterlife-Crisis beginnt, wie wir nachweisen konnten, etwa mit Abschluss des Studiums"", sagt Thomas Schneidhofer, Professor für Management, der die Studie gemeinsam mit Rafaela Artner durchführte. ""Die Betroffenen wissen dann nicht mehr wirklich, wie es für sie weitergeht.""
Frauen eher gefährdet

Was sich zeigte: Besonders Frauen dürften in eine solche Krise schlittern. ""Was insofern beachtlich ist, als dass Frauen in der psychologischen Literatur als das emotional stabilere Geschlecht gelten.""

Ebenfalls gefährdet sind laut Schneidhofer sozial Schwächere. ""Man könnte vermuten, dass das damit zusammenhängt, dass sie einen Habitus haben, der nicht unbedingt zu den Erfordernissen passt. Was nicht heißt, dass sie diesen Anforderungen nicht gewachsen wären. Sie haben nur das Gefühl, dass ihr Mindset, all ihre Fähigkeiten nicht für die Arbeitswelt genügen.""
Impostor-Syndrom

Dieses Gefühl nennt sich im Fachjargon ""Impostor""- oder zu Deutsch: Hochstapler-Syndrom. ""Obwohl die Betroffenen kompetent wären, eine Position auszufüllen zweifeln sie daran, dass sie das auch können"", sagt Schneidhofer. ""Nach dem Motto: Irgendwann wird jemand herausfinden, dass ich gar nicht so gut bin, wie alle glauben.""

Auch Absolventen, die noch zu Hause wohnen dürfte die Quarterlife-Crisis eher betreffen. ""Sie konnten noch nicht richtig mit dem Leben beginnen und dann wird auch noch ständig in den Medien gesagt, dass ihre Chancen am Arbeitsmarkt nicht besonders rosig sind. Das verursacht eine Krise.""

Im Gegensatz dazu dürften Studienrichtung und akademischer Erfolg keinen Einfluss darauf haben, ob ein Akademiker oder eine Akademikern gefährdet ist, in Sinnzweifel zu verfallen oder nicht.
Erklärung für Perspektivenlosigkeit

Abgefragt wurde für die Studie einerseits, ob die Teilnehmer von sich selbst glauben, in der Krise zu sein. Andererseits prüften die Studienautoren die Betroffenheit anhand psychologischer und soziologischer Faktoren, darunter: Angst, Überforderung oder Orientierungslosigkeit (beispielsweise: ""Ich fühle mich nur in gewissen Bereichen den Anforderungen gewachsen"").

Schneidhofer sieht in den Studienergebnissen ""eine Erklärung für die Perspektivenlosigkeit der Generation Y, die ihr immer wieder angedichtet wird.""";https://www.derstandard.at/story/2000025584144/jeder-vierte-startet-mit-sinnkrise-ins-berufleben;Standard;Lisa Breit
05.06.2019;"""KI im Militär sollte uns Sorgen machen""";"Europa darf nicht den Anschluss bei der Entwicklung von künstlicher Intelligenz verlieren – Bernhard Schölkopf, einer der führenden Forscher im Bereich des maschinellen Lernens, lässt kaum eine Gelegenheit verstreichen, die Öffentlichkeit in Sachen künstlicher Intelligenz ""wachzurütteln"".

Anlässlich des zehnjährigen Bestehens des Institute of Science and Technology (IST) Austria hält er diese Woche einen Vortrag in Klosterneuburg, wo er sich mit den Chancen und Herausforderungen von künstlicher Intelligenz beschäftigt und damit, welchen Beitrag Europa dabei leisten kann.

STANDARD: Herr Schölkopf, Sie forschen zu maschinellem Lernen. Wie lernen Maschinen?

Schölkopf: Bei maschinellem Lernen geht es darum, dass sich Maschinen durch Erfahrungen verbessern. Maschinen sollen durch Beobachtungen lernen, bestimmte Aufgaben zu erfüllen. Diese Aufgaben könnte man ihnen nur mit großen Schwierigkeiten einprogrammieren. Man hat früher versucht, Computern Regeln einzuprogrammieren, wie man Objekte erkennt, aber das hat nicht funktioniert. Die Unterschiede durch Beobachtungsrichtung oder Beleuchtung sind zu groß. Man hat daher einen anderen Weg gewählt und orientiert sich daran, wie Menschen oder Tiere lernen. Wir bekommen ja auch keine Regeln einprogrammiert, sondern lernen aus Beobachtungen.
Wie maschinelles Lernen die Informationsverarbeitung revolutionieren kann, erklärte Bernhard Schölkopf in einem Vortrag bei der Berliner Konferenz Falling Walls im November 2018. Video: Falling Walls
Falling Walls Foundation

STANDARD: Ihr Spezialgebiet ist Kausalität bei maschinellem Lernen – worum geht es dabei?

Schölkopf: Maschinelles Lernen basiert auf statistischen Abhängigkeiten. Ein Beispiel: Wenn man sich die Daten zur Häufigkeit von Klapperstörchen in europäischen Ländern und der Geburtenrate bei Menschen ansieht, zeigt sich, dass diese beiden Größen korrelieren. Wir wissen natürlich, dass es keinen direkten kausalen Zusammenhang gibt, aber das ist dem maschinellen Lernen egal, solange es eine Korrelation gibt. Wir gehen davon aus, dass diese Abhängigkeit fundamental durch eine kausale Struktur bedingt ist, nur ist diese oft sehr komplex. Im maschinellen Lernen verschließen wir die Augen vor den kausalen Strukturen und versuchen, Korrelationen zu finden. Aber das Potenzial davon ist begrenzt. Intelligente Systeme sollen nicht nur dasitzen und beobachten, sondern sie sollten Modelle bauen, um mit der Umgebung zu interagieren und vorhersagen zu können, wie die Welt auf ihre Interaktionen reagieren wird.

STANDARD: Welche Rolle könnte maschinelles Lernen bei den großen globalen Herausforderungen wie dem Klimawandel spielen?

Schölkopf: Eine wesentliche Voraussetzung ist zu verstehen, welche Auswirkungen Eingriffe in das Klima haben. Methoden des maschinellen Lernens könnten eine Rolle dabei spielen, diese Kausalzusammenhänge richtig zu verstehen. Es gibt natürlich auch viele kleinere Anwendungsbereiche. Wenn man etwa bedenkt, wie viele Lastwägen leer durch die Gegend fahren, gibt es ein großes Potenzial, viel Energie zu sparen, und künstliche Intelligenz wird eine Rolle bei solchen Effizienzsteigerungen spielen.

STANDARD: Welchen Einfluss könnte maschinelles Lernen auf die Wissenschaft haben?

Schölkopf: In vielen Wissenschaftsbereichen werden zunehmend Datenmengen erhoben, die so groß sind, dass kein Mensch sie sich je ansehen kann. Beispielsweise in der Astronomie, aber auch in der Biologie und Medizin. Diese Daten werden immer mehr durch maschinelles Lernen untersucht werden. Ein Problem dabei ist die Generalisierung. Das Ziel ist, aus Datenmengen Hypothesen zu bilden, die auch über diese Datenmenge hinaus Gültigkeit haben. Das ist das gleiche Problem, vor dem auch Wissenschafter stehen: Sie machen Beobachtungen und versuchen, daraus ein allgemeines Gesetz abzuleiten. Ich glaube nicht, dass Wissenschafter in näherer Zukunft durch maschinelles Lernen ersetzt werden, denn wir wissen nicht, wo die Kreativität dann herkommen sollte, die in der Wissenschaft eine wesentliche Rolle spielt. Ich denke aber, wir sind hier am Anfang einer revolutionären Entwicklung. In absehbarer Zukunft wird bei jedem Krebspatienten das Erbgut der Tumorzellen sequenziert werden, und intelligente Verfahren werden individualisierte Therapieempfehlungen geben.

STANDARD: Einige Intellektuelle argumentieren, man sollte sich schon jetzt Gedanken machen, wie mit einer künstlichen Intelligenz umzugehen ist, die menschliche Intelligenz ganz allgemein übertrifft. Teilen Sie diese Meinung oder sind wir davon so weit entfernt, dass solche Gedanken müßig sind?

Schölkopf: Meine Meinung ist: sowohl als auch. Ich denke, wir sind noch extrem weit davon entfernt. Ich will aber kein Denkverbot aussprechen. Es ist ein interessantes Problem, über so etwas philosophisch oder soziologisch nach zudenken. Mich interessiert das auch und ich lese gerne Science-Fiction. Aber wenn wir ehrlich sind, ist es völlig unrealistisch, dass es eine allgemeine künstliche Intelligenz zu unseren Lebzeiten geben wird – natürlich mit dem Vorbehalt, dass Vorhersagen immer schwierig sind. Wenn man sich ansieht, welche Dinge jetzt gut funktionieren, ist das in den allermeisten Fällen Mustererkennung aus großen Datenmengen. Zu einer allgemeinen Intelligenz in vielen verschiedenen Bereichen gehört schon noch viel mehr. Darüber würde ich mir keine Sorgen machen. Es gibt viele andere Probleme, über die man sich viel mehr Sorgen machen sollte, zum Beispiel den Klimawandel. Wenn man sich im Bereich der künstlichen Intelligenz Sorgen machen will, dann sollte man eher die Frage stellen: Welche militärischen Anwendungen von künstlicher Intelligenz sind möglich, die potenziell kommen werden?

STANDARD: Warum ist es Ihrer Meinung nach wichtig, dass sich Europa bei künstlicher Intelligenz verstärkt einbringt?

Schölkopf: Ich versuche immer, die Menschen in Europa wachzurütteln, dass wir uns zusammentun sollten, damit diese Art der Forschung, die die Welt verändern wird, auch verstärkt in Europa passiert. Diese Revolution wird Auswirkungen auf sehr viele Bereiche haben. Ich habe kein Patentrezept dafür, wie wir sie gestalten sollten. Aber ich habe das Gefühl, dass es gut ist, wenn diese Revolution nicht nur in Amerika und in China passiert, sondern auch in den europäischen Gesellschaften gestaltet wird. Viele europäische Wissenschafter haben sich in der Initiative ""European Lab for Learning and Intelligent Systems"" (ELLIS) zusammengetan, um gemeinsame Forschungsaktivitäten zu starten, aber wir brauchen noch mehr Rückenwind von der Politik.";https://www.derstandard.at/story/2000104338938/ki-im-militaer-sollte-uns-sorgen-machen;Standard;Tanja Traxler
;;;;Standard;
;;;;Standard;
;;;;Standard;
;;;;Standard;
;;;;Standard;
;;;;Standard;
;;;;Standard;
;;;;Standard;
;;;;Standard;
;;;;Standard;
;;;;Standard;
;;;;Standard;
;;;;Standard;
;;;;Standard;
;;;;Standard;
;;;;Standard;
;;;;Standard;
;;;;Standard;
;;;;Standard;
;;;;Standard;
;;;;Standard;
;;;;Standard;
;;;;Standard;
;;;;Standard;
;;;;Standard;
;;;;Standard;
;;;;Standard;
;;;;Standard;
;;;;Standard;
;;;;Standard;
;;;;Standard;
