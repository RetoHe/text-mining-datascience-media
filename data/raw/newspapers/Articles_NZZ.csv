Datum;Titel;Text;Link;Quelle;Autor ;;;Data Science
08.09.2017;Wie Roboter Häuser bauen;"Holz ist heimelig: Das denken viele Schweizer. Und doch verbauen sie in ihren Wohnungen und Häusern immer weniger Holz, das aus Schweizer Wäldern stammt. Gemäss den neuesten Zahlen des Bundesamts für Statistik sind in der Schweiz 2016 4,46 Millionen Kubikmeter Holz geerntet worden - so wenig wie seit zehn Jahren nicht mehr. Und das, obwohl sich der Wald zumindest in den Alpen ausdehnt. Die Gründe liegen auf der Hand: Die Konkurrenz im Ausland produziert um einiges günstiger, und die Rohholzpreise sinken.

«In der Schweiz ist die Eigentümerschaft des Waldes sehr kleingliedrig», sagt Martin Riediker, Präsident der Leitungsgruppe des Nationalen Forschungsprogramms «Ressource Holz» (NFP 66), das untersucht, wie sich Holz in der Schweiz besser verwerten lässt.

«Um Holz effizient nutzen zu können, brauchte es grössere Bewirtschaftungseinheiten, aber über diese verfügt das Land nun einmal nicht.» Riediker setzt deshalb auf eine andere Strategie: «Wir müssen den Sog auf heimisches Holz vergrössern, indem wir in der holzverwertenden Industrie Innovationen fördern.»
Digital entworfen, geplant und gebaut

Viel Schub in diese Richtung kommt von Robotern, die Holz auf neue Art bearbeiten und fügen. Im Robotic Fabrication Laboratory des Instituts für Technologie in der Architektur (ITA) an der ETH Zürich konstruieren derzeit zwei Roboter Raumzellen aus Holzbalken in Fertigbauweise. Diese dienen als Wohneinheiten und beherbergen später die Zimmer in den oberen Stockwerken im dreigeschossigen DFAB House, dem laut ETH «weltweit ersten Haus, das weitgehend mit digitalen Prozessen entworfen, geplant und gebaut wird».

Das DFAB House wird im Frühling 2018 in das Versuchsgebäude «Nest» auf dem Areal der Eidgenössischen Materialprüfungs- und Forschungsanstalt (Empa) in Dübendorf eingebaut. Dabei handelt es sich um ein fassadenloses Bauwerk mit Plattformen, in dem bautechnologische Innovationen in Form austauschbarer Module unter echten Bedingungen getestet werden können.

    «Das Bauen mit Robotern stellt einen Gegenentwurf
    zu den Prozessen dar, die beim klassischen Bauen
    zur Anwendung kommen.»

Für den Bau der 9 Meter langen, 3,5 Meter breiten und ebenso hohen Raumzellen wurden die zwei Industrieroboter mit Greifarmen und weiteren Holzbearbeitungswerkzeugen ausgerüstet. Stehen Roboter üblicherweise auf dem Boden, so hängen sie im Robotic Fabrication Laboratory an einem Portal frei beweglich von der Decke. In perfektem Zusammenspiel errichten die beiden Roboter Wände, Ecken und Dach der Raumzellen für das DFAB House: Der erste placiert einen Balken, der zweite fügt einen stabilisierenden hinzu, so entsteht in einem autonomen Prozess die gesamte Struktur.

«Statt wie früher flächig und gegen die Schwerkraft, bauen wir direkt räumlich», erklärt Andreas Thoma, der als Projektleiter für die Konstruktionsarbeiten verantwortlich ist. Die «roboterassemblierte» Bauweise führe zu einem «neuen architektonischen Ausdruck». Ob ein Bauteil einfach oder kompliziert angelegt ist, spielt für den Roboter - anders als für den Menschen - keine Rolle. «Das erlaubt uns, andere Geometrien zu bauen als lediglich planare Flächen», sagt Thoma. «So entstehen komplexe und zugleich leistungsfähige Strukturen.»

Entwickelt haben das Robotic Fabrication Laboratory die beiden Professoren Matthias Kohler und Fabio Gramazio, die seit 2005 die Professur für Architektur und Digitale Fabrikation unter dem Namen Gramazio Kohler Research zu einer weltweit beachteten Forschungsgruppe aufgebaut haben.

«Das Bauen mit Robotern ist integrativ gedacht und stellt einen Gegenentwurf zu den Prozessen dar, die beim klassischen Bauen zur Anwendung kommen», erklärt Matthias Kohler. Heute zeichnet ein Architekt seine Pläne mithilfe von computergestützten Konstruktionssystemen und CAD-Programmen, aber sobald der Bau einer Einheit oder eines Gebäudes beginnt, hat die Digitalisierung ein Ende. Dann übernimmt der Bauführer die Umsetzung und die Handwerker die Konstruktion.  Demgegenüber führen die an der ETH Zürich erprobten Bauprozesse von der digitalen Datenbasis des Entwurfs direkt in die Ausführung. Das bedeutet: Bei der Planung wird nicht nur der Bauplan, sondern der ganze Bauprozess digital angelegt und programmiert, Roboter führen ihn dann gemäss den Befehlen präzise, schnell und sicher aus. «Mit diesem Ansatz rückt der Architekt wieder näher an die Konstruktion des von ihm geplanten Bauwerks heran», sagt Matthias Kohler.
Erste Erfahrungen vor fast zehn Jahren

Bis man am Lehrstuhl für Architektur und Digitale Fabrikation so weit war, brauchte es lange Vorarbeiten. Erste Erfahrungen mit dem Einsatz von Robotern im Holzbau machten die ETH-Forscher 2008 mit dem Projekt «Sequenzielle Wand». Hier baute ein Industrieroboter aus Holzlatten Schritt für Schritt geometrisch vielschichtige Wände. Nach dem gleichen Prinzip entstanden später komplexe Holztragwerke.

Erste Praxistauglichkeit bewies das roboterassemblierte Bauen beim Planen und Erstellen des Dachs des Neubaus für das Institut für Technologie in der Architektur. Das elegante, mehrfach geschwungene Holzdach wurde nach einem Entwurf von Gramazio Kohler Research komplett von einem Portalroboter in den Produktionshallen der Firma Erne in Laufenburg vorgefertigt. Aus exakt 48 624 kleinen Kanthölzern baute die Maschine 168 Fachwerkträger von bis zu 15 Metern Länge, die vor Ort an der ETH mithilfe eines Lastenkrans zum 2300 Quadratmeter grossen Dach zusammengesetzt wurden.

«Die Entwicklung des Dachs setzt für Architekten und Ingenieure sowie für die Bauwirtschaft neue Impulse», erklärte 2016 der für die Konzeption des ganzen Gebäudes verantwortliche ETH-Professor Sacha Menz. Tatsächlich gilt die Baubranche als nicht besonders risikofreudig, wie der Roboterspezialist Guang-Zhong Yang vom Imperial College London kürzlich im Fachblatt «Science Robotics» beklagt hat: «Die meisten Konstruktionsverfahren bedienen sich immer noch derselben manuellen Methoden und konventionellen Materialien, die vor hundert Jahren entwickelt worden sind.»

«Architekten haben bisher Berührungsängste gegenüber dem Holz gehabt», sagt Martin Riediker, der Präsident der Leitungsgruppe des NFP 66. Er ist überzeugt, dass das Bauen mit Robotern den Architekten «nie da gewesene Design-Möglichkeiten» eröffnet und einen «neuen Zugang zum Holz als Baustoff» ermöglicht. «Was Matthias Kohler und Fabio Gramazio an der ETH machen, ist der Anfang einer Vision. Ich würde sogar von einer Revolution reden.»

Die Chancen stehen gut, dass Holz als Baustoff eine vielversprechende Zukunft vor sich hat: Weltweit entstehen immer grössere, sicherere und stärkere Gebäude aus Holz, zum Beispiel in Wien ein Holzhochhaus mit 24 Stöcken. Holz hilft auch, die Erde zu kühlen: Wenn mit Holz gebaut wird, entstehen jene Emissionen des Treibhausgases Kohlendioxid nicht, die bei der Produktion von Beton und Stahl anfallen. Zudem speichert Holz das von den Bäumen aus der Luft aufgenommene CO2 (siehe unten).

Roboter werden an der ETH Zürich nicht nur für Holzkonstruktionen eingesetzt, sondern auch im Betonbau. So kann ein im Rahmen des Nationalen Forschungsschwerpunkts Digitale Fabrikation entwickelter Bauroboter namens «In situ Fabricator» direkt auf der Baustelle ein dreidimensionales Bewehrungsgitter aus Metall herstellen, in das der Beton eingegossen wird. Das Gewebe ist so dicht, dass der Beton nicht herausfliesst. Das Metallgeflecht stellt die Stabilität der Struktur sicher und macht die althergebrachte und wenig effiziente Technik des Verschalens im Betonbau unnötig.

Mit diesem Verfahren sind der Formsprache im Umgang mit Beton kaum mehr Grenzen gesetzt. Mit der «Mesh Mould»-Technologie wird dieses Jahr im DFAB House des «Nest»-Gebäudes eine doppelt gekrümmte, tragende Wand errichtet, welche die Architektur des Arbeits- und Wohnbereichs im Basisgeschoss prägen wird.

Auch im Betonbau gilt: Das Schwierige ist, einem Roboter beizubringen, wie er eine Aufgabe zu erledigen hat. Hat man das geschafft, ist der Bau komplexer Strukturen ein Kinderspiel.
Holz ist erneuerbar und steht zumindest in der Schweiz in grossen Mengen zur Verfügung. Laut Studien könnten hierzulande jährlich bis zu 8,5 Millionen Kubikmeter nachhaltig geerntet werden, heute wird lediglich etwas mehr als die Hälfte davon geschlagen. Das Verbauen von Holz nützt auch dem Klima. Laut Martin Riediker, dem Präsidenten der Leitungsgruppe des Schweizer Holz-Forschungsprogramms NFP 66, werden in der Schweiz pro Jahr 2,2 Millionen Tonnen des Treibhausgases CO2 weniger emittiert, wenn Holz statt Beton verbaut wird. Zudem werden im langjährigen Durchschnitt rund 2,5 Millionen Tonnen CO2 jährlich in Produkten aus Holz gespeichert.

Viele Holzarten haben eine ausserordentliche Festigkeit und eignen sich deshalb hervorragend für den Bau tragender Konstruktionen. Untersuchungen in den USA haben zum Beispiel auch gezeigt, dass Holzbauten Erdbeben besser widerstehen als solche aus Beton oder Stahl. Und anders als man denken würde, behalten Konstruktionen aus Holz bei einem Feuer ihre strukturelle Integrität über längere Zeit: Stahl und Beton brennen zwar nicht, dafür schmelzen sie ab einer gewissen Temperatur.

Holz als Baustoff zu verwenden, hat aber auch Nachteile. Holz ist immer noch relativ teuer: «Ein Liter Beton kostet weniger als ein Liter Mineralwasser», sagt Martin Riediker. Dazu kommt, dass Fassaden aus Holz verwittern und sich schnell verfärben, was nicht jedem Hausbesitzer gefällt. Gerade in der Schweiz häufige Holzarten wie Buchenholz sind sehr anfällig auf Feuchtigkeit und verziehen sich, wenn sie nass werden. Mit speziellen Oberflächenbehandlungen kann die Buche inzwischen aber so wasserabweisend gemacht werden, dass sich ihr Holz selbst in Duschwänden verbauen lässt. ";https://nzzas.nzz.ch/wissen/der-verlaengerte-arm-des-architekten-ld.1315207?reduced=true;NZZ;Patrick Imhasly;;;Datenwissenschaft
08.07.2015;Online-Lexikografen wühlen im Staub der Archive;"Das Bild im Wikipedia-Artikel zum Thema Schweiz im Ersten Weltkrieg aus Bonfol im Kanton Jura hat das Zeug zu einer Ikone: Der unbekannte Fotograf blickt in einen Kindergarten, der gleichzeitig als Soldatenstube dient. Dicht gedrängt sitzen im Vordergrund etwa zwanzig Kinder. Hinten sind vier Soldaten in ein Schachspiel vertieft. Ein Kanonenofen spendet Wärme, an der Wand hängen ein Kreuz und ein Werbeplakat für alkoholfreien Apfelwein. In den Begleitinformationen erfahren wir, dass das Bild nicht nur aus dem Schweizer Bundesarchiv stammt, sondern auch von dieser Institution selber hochgeladen wurde.
Verborgene Schätze

2013 hat das Bundesarchiv während sechs Monaten einen sogenannten Wikipedian-in-Residence beschäftigt. Er wurde gemeinsam von Wikimedia Schweiz und dem Archiv bezahlt und hat in dieser Zeit über 5000 Fotos aus dem Ersten Weltkrieg auf Wikimedia Commons hochgeladen. Wikimedia Commons ist die Mediendatenbank der Wikipedia-Welt, Grundlage für Illustration der Artikel in den über 280 Sprachversionen der Online-Enzyklopädie.

Das gleiche Verfahren hat auch die Schweizerische Nationalbibliothek angewandt. Zu den Schätzen, die hier gehoben werden konnten, gehört die sogenannte Gugelmann-Sammlung mit Grafiken von Schweizer Kleinmeistern aus dem 17. und 18. Jahrhundert: Landschafts- und Stadtansichten, Alltagsszenen und Trachten. Die Schenkung von 1982 ist eine der wertvollsten, die der Institution je gemacht wurden, 2300 Grafiken aus dieser Sammlung hat die Nationalbibliothek auf Wikipedia geladen.

Bei beiden Institutionen hat die Zusammenarbeit mit Wikimedia eine lange Vorgeschichte. Kontakte bestanden seit vielen Jahren ebenso wie der Wunsch nach Zusammenarbeit. Für Matthias Nepfer von der Nationalbibliothek liegen die Vorteile auf der Hand: «Suchmaschinen zeigen die Einträge bei Wikipedia an erster Stelle. Unsere Inhalte müssen da zu finden sein, wo die Leute sie suchen.»

Beide Institutionen haben sich bei der Zusammenarbeit für einen Wikipedian-in-Residence entschieden: Der Wikipedian-in-Residence bei beiden Institutionen war der Zürcher Ingenieur Micha L. Rieser. Er ist selber Anhänger der Open-Data-Bewegung und vertritt die Überzeugung, dass Wissen öffentlich und kostenlos für jedermann zugänglich sein muss.

Rieser ist einer der wenigen Schweizer Administratoren bei Wikipedia. Das heisst, er betreut Autorinnen und Autoren von Wikipedia und verhindert Wildwuchs in der beliebten Online-Enzyklopädie. «Am Anfang steht immer die gegenseitige Information, in Workshops vermittle ich Grundlagen der Wikipedia-Arbeit.» Im Moment setzt er seine Arbeit bei der Universitätsbibliothek Basel fort, wo er Mitarbeiter und Studenten schult und nach verborgenen Schätzen sucht. Der erste Wikipedian-in-Residence wurde übrigens 2010 am British Museum in London beschäftigt.
Fruchtbare Zusammenarbeit

Sowohl das Bundesarchiv als auch die Schweizerische Nationalbibliothek sind mit den Resultaten der Zusammenarbeit mehr als zufrieden. Wikipedia verfügt über ausgeklügelte Instrumente zur Analyse des Publikums, aber auch zur Benutzung der eigenen Ressourcen: So wurden die Bilder aus dem Bundesarchiv bis Ende Mai 3,5 Millionen Mal betrachtet. Der Katalog des Bundesarchivs zählt pro Monat lediglich 12 000 Aufrufe. Rund 160 Bilder aus der Bundesarchiv-Sammlung wurden bisher in Wikipedia-Artikel eingebaut.

Ähnlich tönt es bei der Nationalbibliothek: «Die Zusammenarbeit hat zu einer höheren Sichtbarkeit und Bekanntheit unserer Institution beigetragen und wertvolle Kontakte zur Wikipedia-Community ermöglicht», erklärt Matthias Nepfer. Die Community habe die Bilder sehr schnell weiterverarbeitet, so Nepfer. So wurden Kategorien gebildet und Beschreibungen ergänzt – beides verbessert die Auffindbarkeit der Bilder.
Geschichte im Netz

Kooperation mit Wikipedia, genauer gesagt mit der Wikimedia Foundation, ist auch für andere Institutionen ein Thema: So gibt es etwa bei der Zentralbibliothek Solothurn Kontakte zur Wikipedia-Gemeinschaft, wie uns Direktorin Verena Bider erklärt. Bereits 2013 hat man hier ein Konvolut von 2000 Grafiken aus dem Kanton Solothurn auf Wikimedia Commons geladen. Beim Schweizerischen Sozialarchiv in Zürich hat man – mit Unterstützung des Wikipedianers Micha L. Rieser – etwa 200 Artikel mit Hinweisen auf eigene Sammlungen ergänzt, das gilt etwa für Artikel über soziale Bewegungen wie den Landesstreik von 1918 oder die Jugendunruhen von 1980 oder über Personen wie Hermann Greulich (1842 bis 1925), den Gründer der SP Schweiz, oder den Zürcher Arzt und Anarchisten Fritz Brupbacher (1874 bis 1945).

Auch die ETH-Bibliothek in Zürich macht sich Gedanken über eine vertiefte Zusammenarbeit. Sie bietet heute 250 000 digitalisierte Bilder auf einer eigenen Plattform an: «Wir prüfen, ob diese Digitalisate vollständig auf der Plattform Wikimedia Commons zur Verfügung gestellt werden sollen», erklärt Franziska Regner von der ETH-Bibliothek.

Ebenfalls im Jahr 2013 hat auch die Zentralbibliothek Zürich ein Pilotprojekt mit Wikimedia unternommen, musste die Kooperation allerdings dann aus Kapazitätsgründen wieder beenden. Zudem erhoffte man sich zusätzliche Informationen über Werke, bei denen die Beschreibungen (Metadaten) fehlten. Es habe aber kaum Rückmeldungen gegeben, sagt Jochen Hesse, Leiter der Grafischen Sammlung der Zentralbibliothek Zürich. – Einen anderen Weg hat man bei der Kantonsbibliothek Thurgau eingeschlagen. Dort hat vor kurzem der erste von drei öffentlichen Workshops stattgefunden. Ziel: die Verbesserung von bestehenden Einträgen zu Themen aus dem Kanton Thurgau oder das Schreiben von neuen Beiträgen.

Für Kantonsbibliothekar Bernhard Bertelmann geht es aber um mehr: «Wir sind sehr inspiriert von der Bewegung der Citizen-Science. Die Bürger produzieren selber Wissen über Themen, die für sie wichtig sind, und hier haben wir als Bibliothek eine wichtige Vermittlungsaufgabe.»
Explosionsartiges Wachstum

Die 2001 gegründete Wikipedia ist explosionsartig gewachsen und ist auch heute eines der interessantesten Internet-Projekte. Der Wikipedia-Gründer Jimmy Wales wurde 2011 in der Schweiz mit dem renommierten Gottlieb-Duttweiler-Preis ausgezeichnet. Die 288 Sprachausgaben der Online-Enzyklopädie bieten über 35 Millionen Artikel und gehören zu den meistbesuchten Internetseiten weltweit. Da bleibt auch Kritik nicht aus.

Wikiwatch, ein Projekt der Viadrina-Universität Frankfurt an der Oder, etwa unterhält eine eigene Seite mit Hintergrundwissen. Das Schweizer Pendant Wikibu scheint seit 2011 nicht mehr aktualisiert worden zu sein. Beide bieten eine Anzahl von analytischen Instrumenten für einen kritischen Umgang mit der Online-Enzyklopädie an, die allerdings alle aus der Wikipedia-Welt stammen.

Wie steht es um Glaubwürdigkeit und Qualität? «Wikipedia ist einfach ein Fakt, und wir müssen uns damit beschäftigen», sagt etwa der Leiter des Zürcher Sozialarchivs, der Geschichtsprofessor Christian Koller. Bernhard Bertelmann von der Kantonsbibliothek Thurgau pflichtet ihm bei: «Entscheidend ist, ob über die Online-Enzyklopädie die wichtigen wissenschaftlichen Quellen gefunden werden können.» Die Kooperation hat auch Grenzen: Wikimedia ist kein Mittel zur Langzeitarchivierung. «Die Langzeitarchivierung der Daten ist unsere ureigene bibliothekarische Aufgabe», sagt Verena Bider.
Heftige Diskussionen

Wikipedia wird gemeinhin als «Mitmach-Enzyklopädie» bezeichnet. Das führt oft zu Missverständnissen. Tatsächlich ist die Wikipedia-Community stark strukturiert und verfügt über klare Hierarchien. Für die Verwaltung sind die sogenannten Administratoren zuständig, sie versuchen die schlimmsten Auswüchse zu verhindern und können mitunter schnell eingreifen. Das musste etwa die netzpolitische Organisation Digitale Allmend Schweiz erleben, deren Eintrag mit einem Löschantrag versehen wurde. Begründung: mangelnde Relevanz. Gerade dieses Kriterium führt oft zu Diskussionen, die nicht ohne eine gewisse Gehässigkeit geführt werden und mitunter zu unverständlichen Entscheiden führen.

Das musste auch Mario Purkathofer, der Leiter des Dock18-Instituts für Medienkulturen der Welt, erfahren, als er einen Artikel über seine Einrichtung schrieb: «Am Ende hat einfach ein deutscher Gymnasiast entschieden, den Eintrag zu löschen.» Rieser als Wikipedia-Administrator beobachtet diese Diskussionen scharf und nicht ohne Kritik an der Wikipedia-Bewegung. Er konstatiert einen Hang zur «Regelhuberei». Was kann man dagegen tun? «Man soll sich als Institution genau überlegen, wie man bei diesem Projekt einsteigt. Es braucht manchmal auch einen Lotsen dazu. Den Relevanz-Diskussionen muss man sich aber mit einem gewissen Selbstbewusstsein stellen.»";https://www.nzz.ch/digital/online-lexikografen-wuehlen-im-staub-der-archive-1.18576706;NZZ;Dominik Landwehr;;;Machine Learning
13.10.2015;Besser als gedacht;"Es ist so sicher wie das Amen in der Kirche: Wenn die nationale Statistikbehörde Chinas ( National Bureau of Statistics , NBS) am kommenden Montag die Zahlen über die Wirtschaftsleistung der zweitgrössten Volkswirtschaft der Welt im dritten Quartal veröffentlicht, werden postwendend Diskussionen über die Verlässlichkeit dieser Daten beginnen. Mit gefälschten Statistiken hatte das Reich der Mitte schon von jeher Probleme. Während des «grossen Sprungs» unter Mao Zedong hätten sich die Volkskommunen im Wettbewerb um Produktionsrekorde mit falschen Statistiken gegenseitig übertroffen, heisst es in der Mao-Biografie von Felix Wemheuer . An dieser Skepsis hat sich nichts geändert. Mehr als ein halbes Jahrhundert später zweifeln viele Ökonomen immer noch an der Aussagekraft offizieller chinesischer Daten.
Wikileaks sei Dank

Ironischerweise dient ausgerechnet Regierungschef Li Keqiang der Anklage als Zeuge. Während seiner Amtszeit als Parteisekretär der im Nordosten Chinas gelegenen Provinz Liaoning hatte er sich am Rande des Nationalen Volkskongresses in Peking im März 2007 mit dem damaligen US-Botschafter Clark T. Randt getroffen und sich auch über die Daten aus seiner Provinz geäussert. Es dauerte nicht lange, bis das Protokoll über das Gespräch im Internet bei Wikileaks zu finden war und das britische Wochenmagazin «The Economist» daraus den « Li-Keqiang-Index » ableitete. Li mass den gesamtwirtschaftlichen Statistiken aus seiner Provinz wenig Bedeutung bei. Diese seien von Menschen gemacht und damit unzuverlässig. Vielmehr vertraue er drei Grössen, um einen Überblick über die Wirtschaft der Provinz zu bekommen, sagte er damals zu Randt: erstens dem Energieverbrauch, zweitens dem Güterverkehr auf der Schiene und drittens der Kreditvergabe durch die Geschäftsbanken. Andere Daten wie das Bruttoinlandprodukt (BIP) dienten lediglich als Referenzgrössen, wird Li in dem Protokoll zitiert.

Die Aussagen Lis befremden auch Jahre später noch, weil er als Parteisekretär eigentlich hätte darauf dringen können, verlässliche Zahlen über die Wirtschaftsleistung zu erhalten. Allerdings findet dieser widersprüchliche Punkt in den Diskussionen keine Beachtung. Vielmehr muss Li seitdem als Kronzeuge dafür herhalten, dass die Chinesen es mit ihren offiziellen Statistiken nicht so genau nehmen. Es gibt jedoch ernstzunehmende Stimmen, die betonen, den Zahlen der nationalen Statistikbehörde Chinas sei weitestgehend zu trauen. Die Ökonomen von Capital Economics hatten jüngst in ihrer Publikation «Making sense of the official GDP data» geschrieben, diese seien nicht «fabriziert», also nicht gefälscht. Da es keine Belge dafür gebe, dass unter der Hand noch alternative, «wahre» Statistiken existierten, würden die Politiker zwangsläufig den offiziellen Daten trauen – sonst würden sie den wirtschaftspolitischen Prozess im Blindflug steuern, heisst die Schlussfolgerung von Capital Economics.

Auch die Ökonomen Johan Fernald, Eric Hsu und Mark M. Spiegel kommen in ihrer Studie « Is China Fudging its Figures? Evidence from Trading Partner Data » zu dem Ergebnis, dass die Qualität der Zahlen über die chinesische Wirtschaftsleistung in den vergangenen Jahren deutlich zugenommen habe. Da China einen starken Strukturwandel durchlaufe, wodurch die Messung gesamtwirtschaftlicher Daten erschwert werde, lohne sich auch ein Blick auf weitere Indikatoren, lautet die Empfehlung der drei Ökonomen von der Federal Reserve Bank of San Francisco.
Daten von 10 000 Unternehmen

Die Statistiken seien besser als gedacht, sagt auch der an der Hongkong University of Science and Technology lehrende Carsten A. Holz , der im Rahmen seiner Forschung u. a. die chinesische Datenwelt unter die Lupe nimmt. Laut dem Ökonomen ziehen die Statistiker des NBS für die Berechnungen der vierteljährlichen Wirtschaftsleistung vor allem die Daten von rund 10 000 Grossunternehmen aus der Industrie, dem Bau- und Gastgewerbe, dem Handel sowie der Immobilienwirtschaft heran. Damit ist rund ein Drittel der chinesischen Wirtschaftsleistung abgedeckt. Zudem berichten noch 350 000 Firmen, die jährlich mehr als 20 Mio. Y (umgerechnet 3,1 Mio. Fr.) Umsatz erzielen, an das NSB. Schliesslich fliessen Stichproben von Kleinstbetrieben ein.
Kein Beleg für Manipulationen

Holz geht davon aus, dass die Daten der 10 000 Grossunternehmen den Statistikern als verlässliche Basis für die Berechnung des BIP-Wachstums dienen. Dieser Wert werde unter Berücksichtigung der Zahlen der kleineren Firmen unter Umständen noch leicht korrigiert. Bei der Festlegung der Quartalswerte für das BIP gebe es zwar politischen Spielraum, um die Zahlen in etwas besserem Licht erscheinen zu lassen, betont Holz. Die Abweichung – nach oben – betrage maximal jedoch nur einen halben Prozentpunkt.

Der Wirtschaftswissenschafter Holz stützt seine Aussage auf ein statistisches Verfahren (« Benford's Law »), mit dem er untersuchte, wie verlässlich die offiziellen BIP-Statistiken Chinas sind. Er könne nicht ausschliessen, dass einzelne Datenpunkte gefälscht seien, doch seine Untersuchungen zeigten, dass es keinen Beleg für systematische Manipulationen gebe, sagt Holz.

Seit der Li-Keqiang-Index vor fünf Jahren im «Economist» erstmals veröffentlicht wurde, gibt es diverse Erweiterungen. In seiner Urform hat er jedoch Schwächen, weil die Messung des Energieverbrauchs schwierig ist und dieser vor allem von der Schwerindustrie getrieben wird. Auch der Güterverkehr auf der Schiene wird vorrangig von Schwergewichten wie der Stahlindustrie in Anspruch genommen. Der in China weitaus wichtigere Transport auf der Strasse fällt unter den Tisch. Und schliesslich spiegelt der Index nicht mehr die sich wandelnde wirtschaftliche Realität Chinas (vgl. linke Grafik). Seit wenigen Jahren sind die Dienstleistungsbetriebe die treibende Kraft hinter dem Wirtschaftswachstum im Reich der Mitte. Die einst dominierende Schwerindustrie hat an Bedeutung verloren (vgl. rechte Grafik). Dienstleistungen brauchen jedoch deutlich weniger Energie und werden nicht auf der Schiene transportiert. Diese in den ursprünglichen Li-Keqiang-Index einfliessenden Grössen haben wegen des Strukturwandels der chinesischen Wirtschaft als Richtwert für die gesamtwirtschaftliche Entwicklung also an Aussagekraft verloren. Ein Blick auf sie lohnt sich jedoch, um ein umfassendes Bild von der gesamten chinesischen Wirtschaft zu erhalten, wie auch die drei Ökonomen von der Federal Reserve Bank of San Francisco aufzeigen.
US-Präsident auf Maos Spuren

Allerdings hat die nationale Statistikbehörde Chinas im Gegensatz zur Messung der industriellen Wirtschaftsleistung noch erheblichen Nachholbedarf bei der Bestimmung der Wertschöpfung der oft vergleichsweise kleinen Dienstleister. In diesem Punkt könnte es gar sein, dass der gesamtwirtschaftliche Beitrag des tertiären Sektors eher unter- denn überschätzt wird. Bei der Berechnung der Wirtschaftsleistung eines Landes handelt es sich also nicht um eine exakte Wissenschaft – und mögliche politische Einflussnahmen, um in besserem Licht dazustehen, sind auch westlichen Demokratien nicht fremd.

Nur wenige Jahre nach Maos «grossem Sprung» mit vielen gefälschten Statistiken erwies sich ein amerikanischer Machthaber als kreativer Manipulator, wie ein Blick auf eine Seite von « Shadow Government Statistics » zeigt. Von dem 36. Präsidenten der Vereinigten Staaten, Lyndon B. Johnson, der zwischen 1963 und 1969 im Weissen Haus war, ist bekannt, dass er sich die Berichte über die BIP-Quartalszahlen immer vorab geben liess. Wenn sie ihm nicht passten, mussten sie von der statistischen Behörde «korrigiert» werden.";https://www.nzz.ch/wirtschaft/wirtschaftspolitik/besser-als-gedacht-1.18628840?reduced=true;NZZ;Matthias Müller;;;
23.09.2020;Der Wettbewerb unter den Fachhochschulen verschärft sich;"Es war eine kleine Provokation: Anfang September hingen im Winterthurer Hauptbahnhof Plakate, die für die neue Fachhochschule Ost warben. Eine Werbeoffensive für ein Studium in St. Gallen, Rapperswil-Jona oder Buchs, nur wenige Schritte von der Zürcher Hochschule für Angewandte Wissenschaften (ZHAW) entfernt.

Die St. Galler belassen es aber nicht bei ein paar Plakaten. Ab Herbst 2021 bietet die FH Ost in Rapperswil-Jona ein Bachelorstudium in Wirtschaftsinformatik an – ein Angebot, das sich indirekt auch gegen die ZHAW richtet, wie der St. Galler Bildungsdirektor Stefan Kölliker (svp.) durchblicken liess. Im April 2019 erklärte er im «St. Galler Tagblatt»: «Wenn es gelingt, auch Zürcher Wirtschaftsstudierende zu holen, wird Rapperswil zahlenmässig einen gewaltigen Sprung machen.»

Diese Rivalität ist ein Symptom für die jüngsten Umwälzungen der Fachhochschullandschaft. Der forcierte Wettbewerb zeigt sich auch daran, dass sich die Fachhochschulen mit neuen Studiengängen überbieten – vorzugsweise mit Titeln wie «Artificial Intelligence and Machine Learning» (Luzern), «Computational and Data Science» (Chur) oder «Data Science» (Brugg-Windisch).

Wer interessant bleiben will, muss die neuesten Studiengänge im Programm haben. Und je früher eine Fachhochschule auf einen Trend wie die Digitalisierung reagiert, desto grösser die Chance, dass sich der Ausbildungsgang etabliert.

So bietet etwa die Fachhochschule Graubünden (FHGR) ab Herbst 2021 den Studiengang «Digital Supply Chain» an. Neu ist das Fach nicht, aber die FHGR ist die Erste, die es in einen eigenen Studiengang verpackt. Schon in diesem Sommer hat die FHGR eine Informationsoffensive lanciert, um das Angebot unter die Leute zu bringen.
Churer Alleingang

Dass Chur sich mit neuen Studiengängen profilieren will, hat auch mit der neuen Konkurrenz in St. Gallen zu tun. Bis im vergangenen Jahr war die FH Ostschweiz ein loser Verbund der Fachhochschulen Buchs, St. Gallen, Rapperswil-Jona und Chur – vier Schulen, vier Rektorate, vier Kulturen. Als 2015 das Hochschulförderungs- und -koordinationsgesetz in Kraft trat, war diese Struktur nicht mehr gesetzeskonform. Die vier Schulen mussten sich neu organisieren. Chur wählte den Alleingang, der Kanton St. Gallen schloss die eigenen drei Hochschulen zu einer einzigen FH mit drei Standorten zusammen.

Die FH Ost bleibt trotz ihrer neu gewonnenen Stärke die drittkleinste Fachhochschule der Schweiz – ein Handicap im Wettbewerb um Studenten, Personal und Geld. Die Ostschweiz spürt zudem die Sogwirkung der zur FH Zürich gehörenden ZHAW mit ihrem bestens ausgebauten Angebot in Winterthur. St. Gallen versucht entsprechend Gegensteuer zu geben. Mit Daniel Seelhofer haben die St. Galler einen ZHAW-Exponenten abgeworben und als Gründungsrektor eingesetzt. Ein weiteres Mittel ist der Transfer von Fachrichtungen: Einzelne Studiengänge werden künftig nicht nur in Rapperswil-Jona oder St. Gallen angeboten, sondern an beiden Orten. In der Ostschweiz geistert zudem die Idee herum, in Wil einen FH-Ableger aufzubauen, um für die Thurgauer attraktiver zu werden (die heute notabene mit Vorliebe an der ZHAW studieren).
Engmaschiger Flickenteppich

Solche Expansionsstrategien waren vor ein paar Jahren noch undenkbar.
Zu Beginn der 1990er Jahre war die tertiäre Berufsbildung ein engmaschiger Flickenteppich aus Dutzenden von dezentral geführten Fachschulen, die kaum gemeinsame Standards hatten – Ingenieurschulen, höhere Wirtschafts- und Verwaltungsschulen (HWV), Schulen für soziale Arbeit und andere Institutionen. Die einzelnen Schulen rekrutierten ihren Nachwuchs hauptsächlich in ihren scharf umrissenen Territorien.

Die bildungspolitische Debatte war damals geprägt von der schwächelnden Konjunktur sowie den Sorgen um die Wettbewerbsfähigkeit und die Innovationskraft. Die Schweiz hatte Angst, international den Anschluss zu verlieren. Das Nein zum EWR-Beitritt vom Dezember 1992 befeuerte diese Sorge weiter.

In der Berufsbildungs- und Hochschulpolitik gab die fehlende «Europafähigkeit» des höheren Bildungsbereichs bereits ab 1988 Anlass zur Sorge. Was, wenn der Zugang für ausländische Arbeitskräfte liberalisiert wird? Sind dann die Abgänger deutscher Fachhochschulen gegenüber Schweizern mit ihren HTL- und HWV-Titeln im Vorteil? Umgekehrt fehlte den Abgängern des ausseruniversitären Tertiärbereichs die Sicherheit, dass ihre Titel im Ausland anerkannt würden. Ähnliche Sorgen gab es bei der Lehrerausbildung und im Bereich der Gesundheits-, Sozial- und Kunstberufe (GSK).
Eindrückliche Zahlen

Eine Initiative der Ingenieurschulen mündete schliesslich 1993 in der Einführung der technischen Berufsmatura und 1995 im Fachhochschulgesetz. In einem politischen Kraftakt gelang es Bund und Kantonen, innert weniger Jahre aus den gut siebzig höheren Fachschulen sieben regionale Fachhochschulen zu formen und den GSK-Bereich in diese zu integrieren.

Allerdings bestehen alte Rivalitäten zwischen ehemals autonomen Schulen auch in den neuen Konstrukten weiter, wie Christian Leder vom Kompetenzzentrum für Hochschul- und Wissenschaftsforschung der Universität Zürich sagt. Die Fachhochschule Nordwestschweiz umfasst neun Hochschulen, die sich auf Standorte in vier Kantonen verteilen. Insbesondere zwischen den Standorten im Aargau und in den beiden Basel würden die Gegensätze sichtbar, etwa wenn es um die Verteilung von Geldern gehe, sagt Leder. «Die Konkurrenzkämpfe sind zwar eine Tatsache, aber keine einzige der Schweizer Fachhochschulen ist gefährdet. Sie bedienen eine Nachfrage.»

Tatsächlich wirkte sich die Reform eindrücklich auf die Statistiken aus: Seit 1990 hat sich die Zahl der Studierenden an den schweizerischen Hochschulen verdreifacht. 2019 erreichte sie den Stand von 258 000 Studierenden, von ihnen waren knapp 80 000 an einer Fachhochschule eingeschrieben. «Die Etablierung der Fachhochschulen war insgesamt eine gelungene Reform», sagt Stefan Wolter, Professor für Bildungsökonomie an der Universität Bern. «Damit wurde klar ein wirtschaftliches Bedürfnis erfüllt. Es ist nicht nur gelungen, die berufliche Grundbildung zu retten und den Lernenden den Weg in die Hochschulen zu öffnen. Es war auch eine Tertiarisierung ohne Akademisierung – also ohne Ausbau der Gymnasialquote und der Universitäten.»
Potenzial längst nicht ausgeschöpft

Eine Tatsache sei zudem, dass die Fachhochschulen ihr Potenzial längst nicht ausgeschöpft hätten, sagt Wolter. «Die Universitätsquote hat sich in den vergangenen zwanzig Jahren kaum verändert, die Unis schöpften das Potenzial an gymnasialen Maturanden vollumfänglich aus. Aber bis heute gehen bei weitem nicht alle Berufsmaturanden an eine Fachhochschule.»

Die Zahl der Fachhochschulanmeldungen ist allerdings auch stark konjunkturabhängig, wie Wolter sagt. «Wenn die Wirtschaft nicht läuft und das Jobangebot knapp wird, entscheiden sich mehr Berufsmaturanden für ein FH-Studium.» Die Fachhochschulen wirken damit wie ein Krisenpuffer – Absolventen in der höheren Berufsbildung retten sich vor schlechtbezahlten Jobs oder vor Arbeitslosigkeit in ein Studium.

Für die Absolventen zahlt sich das Studium dann häufig doppelt aus: Sie gewinnen nicht nur Zeit bei der Jobsuche, sondern können sich auch über höhere Löhne freuen. 2018 lag der Medianlohn von FH-Absolventen bei knapp 107 000 Franken pro Jahr. Das ist deutlich näher an der Marke der Uni-Absolventen (119 770 Franken) als an jener von Berufslehre-Absolventen (71 900 Franken).

Die jüngsten Neupositionierungen einzelner Schulen mit weiteren Ausbildungsgängen, überarbeitetem Branding und neuen Gebietsansprüchen und -aufteilungen dürften den Fachhochschulen zusätzlich Auftrieb verleihen. Die Schulen gewinnen dadurch an Profil und Attraktivität. Das wird sich auch auf die Studierendenzahlen auswirken.";https://www.nzz.ch/schweiz/der-wettbewerb-unter-den-fachhochschulen-verschaerft-sich-ld.1575639;NZZ;Andri Rostetter;;;
23.06.2020;Bei diesen Projekten werden Bürger zu Forschern;"«Rettet den Regen.» Mit diesen Worten wandte sich der britische Klimaforscher Ed Hawkins im März an seine Landsleute. Gerade hatte der Corona-Lockdown begonnen: Die Briten hatten viel Zeit und wenig zu tun. Hawkins lud sie ein, 65 000 Seiten digitalisierter Regenfall-Aufzeichnungen aus den Jahren 1820 bis 1960 zu transkribieren, damit er und seine Kollegen die Daten für ihre Klimastudien einsetzen können.

«Rainfall Rescue» ist eines von Tausenden sogenannten Citizen-Science-Projekten, bei denen Freiwillige zur Forschung beitragen. Allein die amerikanische Website «SciStarter» bündelt rund 3000 solcher Projekte. Jene, die sich der Transkription historischer Witterungs- und Klimaaufzeichnungen widmen, tragen unter anderem dazu bei, die Wirtschaft zu stabilisieren, gefährdete Kulturgüter zu schützen und Menschenleben zu retten. Denn je mehr Daten von gestern die Forscher zur Verfügung haben, desto besser können sie das Wetter und das Klima von heute verstehen – und desto zuverlässiger Extremwetterereignisse wie Hochwasser und Hitzewellen prognostizieren.

Citizen-Science-Projekte rund um Klima und Wetter sind derzeit sehr populär. Zwei Wochen nach Ed Hawkins’ Aufruf war «Rainfall Rescue» beendet: In Windeseile hatten 16 000 Helfer über 5 Millionen Messungen transkribiert.

Zu den beliebtesten und längsten Citizen-Science-Projekten gehört «Old Weather»: Hier entnehmen die Freiwilligen die Wetterdaten historischen Schiffslogbüchern. Der Klimaforscher Philip Brohan, Mitarbeiter des britischen meteorologischen Dienstes Met Office, rief das Projekt im Jahr 2010 ins Leben. Seither transkribieren die Freiwilligen Angaben zu Luftdruck, Windgeschwindigkeit, Lufttemperatur, Meereisausdehnung und weiteren Faktoren, unter anderem aus den Logbüchern von Walfangschiffen.
Mitfiebern mit Walfängern

«Man baut eine starke Bindung zu den Walfängern auf», sagt Joan Arthur, Büroangestellte an der Oxford University, die seit acht Jahren bei «Old Weather» aktiv ist. «Ich mache mir während des Transkribierens immer Sorgen und hoffe, dass die Besatzung sicher zurückkehrt.» Die Logbücher beschreiben in knappen Worten oftmals tapfere Menschen in tragischen Umständen. Stürme und Skurrilitäten kommen hier auf einer Seite zusammen – etwa die Tatsache, dass auf dem Walfänger «Fleetwing» anno 1882 ein 15-jähriges Mädchen das Logbuch führte.

Die Daten von «Old Weather» werden in die Datenbank der amerikanischen National Oceanic and Atmospheric Administration eingespeist, mit der hauptsächlich Witterungs- und Klimaforscher arbeiten. So haben Joan Arthur und ihre Mitstreiter in den letzten zehn Jahren zu mindestens 500 wissenschaftlichen Studien beigetragen. Bisher haben sie rund 13 Millionen Messungen transkribiert – aber nicht alle Daten sind gleichermassen nützlich, und ihr Nutzen ändert sich immer wieder.

Seit es beispielsweise möglich ist, anhand von zwei- bis dreihundert Luftdruckmessungen das globale Wetter zu rekonstruieren, haben historische Luftdruckmessungen an Bedeutung gewonnen. Unter den «Old Weather»-Daten befinden sich 2,5 Millionen solcher Luftdruckangaben – so trägt das Projekt unter anderem zur Klimaüberwachung bei.

«Das Transkribieren führt die Ausmasse des Klimawandels vor Augen», sagt der Rentner Michael Purves, der wie Joan Arthur bei «Old Weather» mitmacht. Der studierte Meteorologe lebt im kanadischen Victoria. In einem Logbuch las er, dass ein Schiff namens «Bear» im Juni 1918 in der Nähe seines Wohnortes im Eis feststeckte. «Ich schaute mir Grafiken zur Meereisausdehnung an – und war schockiert, als ich sah, dass im Juni 2019 das Eis rund tausend Kilometer nördlicher war.» Im Laufe der letzten hundert Jahre habe die Eisausdehnung zwar fluktuiert. Doch an dem nunmehr stark schwindenden Meereseis lasse sich der gegenwärtige Klimawandel gut beobachten.

Der typische Citizen-Scientist ähnelt Purves: Er ist männlich und verfügt über eine akademische Ausbildung. Weibliche Freiwillige wie Arthur scheinen rar, laut Studien machen sie zwischen 8 und 18 Prozent der Citizen-Scientists aus. Dagegen sind alle Altersgruppen vertreten, von Jugendlichen bis zu Menschen weit über 80.
Digitalisierung in Usbekistan

Bevor es Citizen-Science-Projekte gab, transkribierten hauptsächlich Doktoranden und studentische Hilfskräfte die historischen Wetterangaben. Aber mit Freiwilligen lässt sich das, was früher Jahre gedauert hat, binnen Monaten erledigen – und die Geschwindigkeit ist für Klimaforscher ein wichtiger Faktor.

«Die Welt verliert täglich Hunderttausende Wetterdaten, insbesondere in wenig entwickelten Ländern», sagt Rick Crouthamel. Er ist Direktor der International Environmental Data Rescue Organization (Iedro). Die Nichtregierungsorganisation unterstützt arme Staaten sowie Entwicklungsländer bei der Digitalisierung alter Klima- und Wetterberichte und lädt Freiwillige ein, die Daten auf «Weather Wizzard» zu transkribieren.

Während die Freiwilligen bei «Old Weather» und anderen Projekten mit bereits digitalisierten Quellen arbeiten, bauen sie bei Iedro in Zusammenarbeit mit der Weltorganisation für Meteorologie (WMO) die entsprechende Digitalisierungsstruktur vor Ort auf. Derzeit unterstützen sie den Wetterdienst in Usbekistan bei der Digitalisierung von rund 17 Millionen Seiten historischer Wetterdaten. Bis jetzt sind knapp 9 Millionen digital. Auch Guatemala und Angola wenden sich an Iedro und die WMO.

«Die Länder profitieren von den Projekten», sagt Crouthamel. «Die historischen Daten liefern Hinweise darauf, wie die vorhandene Infrastruktur gegen Wetterbedingungen verstärkt werden sollte.» Für die Planung neuer Brücken und Gebäude seien sie ebenfalls wichtig. Ausserdem spielen sie eine Rolle bei der Organisation bestimmter öffentlicher Dienste, etwa des Katastrophenschutzes.

Auch in der Schweiz sind Freiwillige für die Klima-und Wetterforschung im Einsatz – unter anderem im Rahmen des Projekts «Kollektives Überschwemmungsgedächtnis», das seit 2013 an der Universität Bern geführt wird. Jedes siebte Gebäude hierzulande gilt als hochwassergefährdet. Freiwillige laden auf der Projektwebsite Fotos von Überschwemmungsschäden hoch, die den Wissenschaftern zur Einschätzung von Gefahren dienen.

Gerade in der Erforschung von Wetter und Klima ist die Beteiligung von Freiwilligen aber keineswegs neu. So hat der britische Forscher George Symons, Leiter des «British Rainfall Survey», im Jahre 1860 mit 500 und später gar mit 3400 Freiwilligen zusammengearbeitet. Sie schickten ihm regelmässig Niederschlagsangaben aus allen Gegenden des Königreichs. Wie sich herausstellt, haben wetterinteressierte Briten bereits vor über 150 Jahren den Regen gerettet.";https://www.nzz.ch/feuilleton/citizen-science-bei-diesen-projekten-werden-buerger-zu-forschern-ld.1561802;NZZ;Anna Gielas;;;
07.02.2017;«Heute sind wir gewissermassen blind»;"Herr Verscheure, was haben die Forschenden der beiden ETH vom neuen Swiss Data Science Center (SDSC)?

Wir werden Wissenschaftern aus allen möglichen Fachbereichen helfen, besser mit ihren Forschungsdaten umzugehen. Wir sehen, dass die Datensätze immer grösser werden, weil sie zunehmend von automatisierten Sensoren erfasst werden. Stellen Sie sich beispielsweise einen Umweltwissenschafter vor, der Tausende von Sensoren in Schweizer Gewässern ausbringt und damit kontinuierlich die Schadstoffkonzentrationen erfasst. Diese Datenfülle bietet gigantische Möglichkeiten, aber sie macht es auch schwierig, aus dem Aufgezeichneten noch sinnvolle Schlüsse abzuleiten. Hier kommt das SDSC ins Spiel: Wir stellen Werkzeuge zur Verfügung, mit denen solche Datenberge nicht nur gespeichert, sondern auch bearbeitet, bereinigt und analysiert werden können. Daten sammeln, aufbereiten und analysieren – das gehört seit Jahrhunderten zum Kerngeschäft der exakten Wissenschaften. Warum braucht es dafür neuerdings extra ein Institut?

In den letzten Jahrhunderten haben die Wissenschaften im Wesentlichen beobachtend gearbeitet. Datengetriebene Wissenschaft funktioniert anders; sie bedeutet, dass die Beobachtungen gewaltigen Datenbergen entspringen, die zumindest in ihrer Rohform keinen unmittelbaren Sinn ergeben. Wenn man auf diese Rohdaten aber die Werkzeuge der Datenwissenschaft anwendet – maschinelles Lernen, Data-Mining, Statistik –, stösst man auf Strukturen, die dem menschlichen Beobachter verborgen bleiben. Diese gilt es zu finden und zu verstehen. Was macht Sie so gewiss, dass sich mit den Methoden der Datenwissenschaft Erkenntnisse gewinnen lassen, die den Forschenden andernfalls durch die Lappen gingen?

Der Grossteil der Datenwissenschaft ist heute gewissermassen blind. Da werden einfach grosse Datenmengen in eine «Blackbox» geworfen. Dem stehen viele Wissenschafter sehr skeptisch gegenüber, denn sie wollen intuitiv erfassen, was mit den Daten geschieht. Unser Ziel ist deshalb der Brückenschlag zwischen der Datenwissenschaft und den verschiedenen Fachbereichen von der Umweltwissenschaft bis zur Medizin. Wir arbeiten mit Experten verschiedener Richtungen zusammen, um die für eine bestimmte Frage jeweils besten Methoden zu entwickeln.

Einige Forschungsbereiche, etwa die Astronomie und die Teilchenphysik, arbeiten traditionell mit riesigen Datenmengen und haben sich längst entsprechend organisiert. Werden auch sie die Dienste des SDSC nutzen?

Wir haben Anfragen von Astronomen bekommen. Die beobachten zwar alle den gleichen Himmel, aber die Instrumente, mit denen sie das tun – Satelliten, Teleskope, Radioantennen –, speichern Daten so unterschiedlich, dass sich diese nur schwer miteinander vergleichen lassen. Das würden die Forschenden gern mit unserer Hilfe verbessern. ";https://www.nzz.ch/schweiz/gruendung-des-swiss-data-science-center-heute-sind-wir-gewissermassen-blind-ld.144045#back-order;NZZ;Helga Rietz;;;
22.11.2017;So entstehen personalisierte Leseempfehlungen der «Neuen Zürcher Zeitung»;"Im neuen Bereich «Meine NZZ» finden Sie – neben dem Briefing und den durch Sie selbst gesteuerten Funktionen «Merkliste» und «Kürzlich gelesen» – individuelle Leseempfehlungen.

Das Besondere an diesen Leseempfehlungen ist, dass sie automatisch und auf Ihr bisheriges Leseverhalten abgestimmt zusammengestellt werden. Wie genau das geschieht und was unsere Leitlinien für die algorithmische Zusammenstellung dieser Listen sind, möchten wir Ihnen im Folgenden gern genauer erläutern.
Die Filterblase

Von Personalisierung ist derzeit häufig die Rede. Im selben Atemzug wird nicht selten auch von der sogenannten Filterblase gesprochen. Dahinter steckt die Vermutung, dass Ihnen Personalisierungsalgorithmen tendenziell «mehr vom selben» anzeigen und so Quellen- und Themenvielfalt reduzieren. Diese Problematik ist tatsächlich vielen Empfehlungsalgorithmen inhärent. Diesem Effekt kann man jedoch aktiv entgegenwirken, und zwar durch entsprechendes, der Zielstellung angepasstes «algorithm design».

Im Fall der NZZ lautet die Zielstellung für die Ausgestaltung der Algorithmen: «Wir wollen mittels algorithmischer Produkte unsere Leser individuell, umfassend und mit Relevanz informieren.» Relevanz hat dabei zwei Ausprägungen: persönliche Relevanz und generelle Relevanz.
Persönliche Relevanz

Persönliche Relevanz bedeutet, dass ein Artikel für Sie persönlich von Interesse ist. Um diese Relevanz bei einem neu veröffentlichten Artikel abschätzen zu können, ziehen wir Ihre bisher gelesenen Artikel zu Rate – und ausschliesslich diese.

Andere Algorithmen würden zum Beispiel Ihre demografischen Merkmale verwenden, um Ihnen Artikel anzuzeigen, die Leser mit ähnlichen demografischen Merkmalen gelesen haben. Wir tun dies absichtlich nicht, da wir der Überzeugung sind, dass nicht Ihr Alter, Ihr Einkommen oder Ihre Herkunft relevant sind für Ihre Artikel-Präferenzen, sondern ausschliesslich Ihre tatsächlichen Interessen.

Dieses Vorgehen bedingt jedoch, dass wir Sie erst ein wenig «kennenlernen». Es kann also etwas dauern, bis Sie die Empfehlungen als für Sie besonders relevant einschätzen. Das Gute an der Sache: Sie müssen nichts weiter dafür tun, ausser regelmässig angemeldet NZZ-Artikel entweder in der App oder auf NZZ.ch zu lesen.
Generelle Relevanz

Die zweite Dimension, die generelle Relevanz, bedeutet, dass ein Thema aufgrund der aktuellen Nachrichtenlage von besonderem Interesse ist.

Diese Einschätzung geschieht durch die Journalistinnen und Journalisten nach den redaktionellen Kriterien der NZZ und fliesst als Signal der Relevanz direkt in den Algorithmus ein. Mehr noch: Je nach Situation ist diese «redaktionelle Relevanz» das ausschlaggebende Kriterium für den Empfehlungsalgorithmus. Dies ist besonders dann der Fall, wenn wir Sie noch nicht gut genug kennen, um die persönliche Relevanz abschätzen zu können. Auch empfiehlt der Algorithmus unter Umständen Artikel mit hoher genereller Relevanz selbst dann, wenn Ihre persönliche Relevanz dagegen sprechen würde. Das ist die direkte Konsequenz unseres Auftrags, Sie umfassend zu informieren.

Ganz konkret finden Sie momentan drei in dieser Art für Sie individuell zusammengestellte Listen vor:

    Aktuell relevante Artikel: Hier zeigen wir Ihnen bis zu drei mit hoher Wahrscheinlichkeit für Sie persönlich besonders interessante Artikel an, die seit Ihrem letzten Besuch publiziert wurden.
    Artikel für den Abend: Hier zeigen wir Ihnen jeden Abend die fünf für Sie relevantesten Artikel des Tages, die Sie noch nicht gelesen haben.
    Artikel, die Sie möglicherweise verpasst haben: Hier zeigen wir Ihnen jeden Freitag die zehn für Sie relevantesten, noch nicht gelesenen Artikel der vergangenen Woche. Dabei legen wir besonderes Gewicht auf längere Artikel, für die Ihnen unter der Woche vielleicht die Zeit zum Lesen gefehlt hat.";https://www.nzz.ch/information/meine-nzz-so-entstehen-die-leseempfehlungen-ld.1331030;NZZ;Rene Pfitzner;;;
10.12.2019;Wie Algorithmen das Online-Shopping aufmischen – und wo sie an Grenzen stossen;"Nicht jeder Kunde von Zalando sieht auf der Website des Modehändlers dasselbe Angebot. Der als Schnäppchenjäger bekannte Konsument bekommt vor allem Produkte zu reduzierten Preisen zu sehen. Der Kundin, die sich in der Vergangenheit Stilberatungen geleistet hat, werden dagegen viele zusammengestellte Outfits präsentiert. Auch das Wetter hat einen Einfluss auf die personalisierte Benutzeroberfläche. Fängt es in einer Gegend an zu schneien, finden die Kunden auf der Website vermehrt warme Kleider wie Wollmützen oder Handschuhe. Welche Angebote ausgespielt werden, entscheiden Algorithmen.

Künstliche Intelligenz kommt bei Zalando aber auch bei der Auswahl von weiteren Kaufangeboten zum Einsatz, unterstützt die Stilberater oder bestimmt, welche Artikel an welchem Ort gelagert werden.
Die Rabatte sind entscheidend

Der Berliner Technologiekonzern zählt im Detailhandel zu den Vorreitern im Sachen künstlicher Intelligenz. Laut Andreas Liedtke, Detailhandelsexperte bei der Beratungsfirma Boston Consulting Group (BCG), haben die Modehändler in diesem Bereich generell die Nase vorne. Das grösste Thema seien dabei die durch Algorithmen gesteuerten Rabatte im Online-Handel. Denn die Modetrends seien sehr schnelllebig, und daher spielten Preisnachlässe auf die alte Kollektion eine wichtige Rolle für den Verkaufserfolg.

Erst am Anfang der Entwicklung stünden dagegen die stark im stationären Geschäft verankerten Lebensmittelhändler, die mit strukturellen Problemen kämpften. BCG hat in Zusammenarbeit mit Google 100 Detailhändler der Bereiche Mode, Lebensmittel, Elektronik und Wohnen aus dem deutschsprachigen Raum zum Thema künstliche Intelligenz und Advanced Analytics befragt. Demnach haben drei Viertel von ihnen in diesem Bereich mindestens ein Pilotprojekt gestartet. Sie rechnen mit Kostenersparnissen und einer besseren Preisgestaltung. Die Firmen investieren laut der Studie durchschnittlich rund 1% des Umsatzes in künstliche Intelligenz und Advanced Analytics und erhoffen sich dadurch eine Umsatzsteigerung von 7%.
Reicher Datenschatz von Migros und Coop

Wie lohnend der Einsatz von Algorithmen tatsächlich für die Detailhändler ist, hängt allerdings vor allem von den verfügbaren Daten ab, mit denen die Modelle «trainiert» werden. Gerade die stark auf das stationäre Geschäft ausgerichteten Händler verfügen dabei über viele Kundendaten. Hierzulande etwa haben die Bonusprogramme Cumulus und Supercard den Grossverteilern Migros und Coop einen grossen Datenschatz beschert.

«Wir verwenden die Kundendaten sehr zurückhaltend», sagt Patrick Frauchiger, Leiter Direktion Data Strategy and Science beim Migros-Genossenschafts-Bund. Bisher sei etwa der Einsatz von personalisierten Preisen bei der Migros kein Thema. Im Rahmen des Cumulus-Programms setze man einzig zielgruppenspezifische Angebote zur Kundenbindung ein. Dies bedeutet, dass der Joghurt-Liebhaber etwa mit Rabatten auf sein favorisiertes Joghurt belohnt wird. Auch Konkurrentin Coop schreckt vor personalisierten Preisen zurück. Die beiden Detailhändler haben Angst vor den Reaktionen in der Öffentlichkeit.

Die Personalisierung des Angebots wird aber auch bei der Migros vorangetrieben. Unter dem Namen My Migros hat die regionale Genossenschaft Migros Aare einen Online-Shop kreiert, der den Kunden aufgrund des bisherigen Einkaufsverhaltens individuell Produkte vorschlägt, die diese sehr wahrscheinlich demnächst wieder benötigen. Der Algorithmus berechnet die Wahrscheinlichkeit, dass die Nutzer ein Produkt bald kaufen wollen, und dieses erscheint dann zuoberst auf der Einkaufsliste.
Der stationäre Handel zieht nach

Laut Frauchiger tauscht man sich mit dem Datenteam der Migros-Online-Tochter Digitec Galaxus regelmässig aus. Doch nicht nur im E-Commerce kommt künstliche Intelligenz zum Einsatz, sondern auch im stationären Geschäft. In einem Pilotversuch wird beispielsweise mittels eines Algorithmus von Migros aufgrund von Wetterdaten und dem Verbrauch in den Vorjahren prognostiziert, wie viel Frischprodukte im Regal stehen sollen.

Auch bei der Planung von neuen Standorten setzt der Detailhändler auf künstliche Intelligenz. Zudem wäre es theoretisch möglich, damit das Sortiment stärker auf die Präferenzen der Kundschaft auszurichten. Doch die Migros hat sich zum Ziel gesetzt, in allen Filialen ein breites Sortiment anzubieten.

Für Coop geht es laut einer Sprecherin beim Einsatz von künstlicher Intelligenz und Advanced Analytics vor allem darum, die internen Prozesse zu unterstützen. Warenwirtschaftssysteme, die dabei helfen, die gelieferte Menge der Nachfrage anzupassen, sind ein Beispiel dafür.
Der Einsatz ist begrenzt

Auch die Convenience-Food-Anbieterin Valora optimiert mithilfe von künstlicher Intelligenz ihre Warensortimente. Zudem spielt der Einsatz von Algorithmen bei der Entwicklung von automatisierten Läden eine Rolle. Sie ermöglichen die Personalisierung von Angeboten für die Kunden. So werden etwa in der kassenlosen Avec-Box von Valora bereits einfache Personalisierungen verwendet. Der Tabak- und Kaffeeautomat erkennt die letzten Bestellungen der Kunden und schlägt ihnen das am meisten gekaufte Produkt wieder vor.

Den Detailhändlern bietet der Einsatz von künstlicher Intelligenz grundsätzlich verschiedene Vorteile, doch sie stossen dabei auch an Grenzen. Der ausgeklügeltste Algorithmus bringt nichts, wenn die Qualität der Daten mangelhaft ist. Zudem müssen die Kunden die Preisgestaltung als fair und transparent empfinden. Nicht zuletzt spielen auch ethische Richtlinien im Unternehmen sowie regulatorische Restriktionen wie Datenschutzgesetze eine wichtige Rolle. ";https://www.nzz.ch/wirtschaft/wie-algorithmen-das-einkaufen-aufmischen-ld.1524728;NZZ;Natalie Gratwohl;;;
05.04.2020;Der Methangehalt der Luft nimmt weiter zu – das liegt nicht nur am Permafrost;"Das bekannteste Methan-Schreckensszenario geht so: Die Erde erwärmt sich immer mehr. Die Permafrostböden in Sibirien, Kanada und im Hochgebirge tauen auf. Aus den Schmelzwassertümpeln blubbert Methan heraus. Das hochpotente Treibhausgas reichert sich in der Atmosphäre an und verstärkt die Erderwärmung. So wird noch mehr Methan freigesetzt – ein Teufelskreis.

Vor ein paar Jahren war in Medienberichten sogar von einer «Methanbombe» die Rede. Doch es ist ähnlich wie bei vielen Schreckensszenarien, die im Zeitalter des Klimawandels kursieren: Bei genauer Betrachtung liegen die Dinge oft ein bisschen anders.
Unerwartet rascher Anstieg

Richtig ist, dass der Gehalt von Methan in der Luft seit 2007 – nach einer mehrjährigen Pause – wieder rasant angestiegen ist. Die Konzentration hat bereits einen Wert von über 1870 Anteilen Methan pro Milliarde Luftanteile erreicht. Diese Zunahme auf mehr als das Zweieinhalbfache des vorindustriellen Werts bereitet Fachleuten in steigendem Masse Sorgen.

Methan ist als Treibhausgas wesentlich potenter als CO2. Dass es trotzdem nur für ein Fünftel der globalen Erwärmung verantwortlich gemacht wird, liegt an seiner deutlich niedrigeren Konzentration. Ein grosser Vorteil im Vergleich mit CO2 ist darüber hinaus die Tatsache, dass Methan im Durchschnitt nur ungefähr neun Jahre in der Atmosphäre verbleibt (CO2 hält sich Jahrzehnte bis Jahrhunderte).

Richtig ist auch: Ein kleiner Teil des Methans stammt aus Permafrostböden, und diese Freisetzung dürfte in der Tat im Zuge der Erwärmung weiter zunehmen. Die meisten Forscher rechnen aber keineswegs mit einer katastrophalen Entwicklung, die als «Methanbombe» bezeichnet werden könnte. Das wird klar, wenn man sich zum Vergleich den Klimawandel der Vergangenheit ansieht.
Lehren aus der Eiszeit

Vor 14 000 Jahren ging die letzte Eiszeit zu Ende. Mit der Temperatur stieg auch der Methangehalt der Luft. Doch Methan aus auftauenden Permafrostböden spielte damals wahrscheinlich keine grosse Rolle. Das hat ein Team um Michael Dyonisius von der University of Rochester im amerikanischen Gliedstaat New York kürzlich anhand von Eisproben aus der Antarktis herausgefunden.

Eine Analyse von Kohlenstoffisotopen brachte die entscheidende Erkenntnis. Methan aus sehr alten Kohlenstoffspeichern, die lange von der Atmosphäre abgeschlossen waren, enthält praktisch nichts von dem verräterischen Isotop C-14: Wäre damals in rauen Mengen gasförmiges Methan aus Permafrostböden frei geworden, hätte man in den Eisbohrkernen eine entsprechende Veränderung der Isotopenanteile finden müssen. Doch dem war nicht so. Also müsse der damalige Methananstieg vorwiegend auf andere Quellen zurückgehen, argumentieren die Forscher im Wissenschaftsmagazin «Science».1
Permafrost ist keine Bombe

Für die künftige Erwärmung kann eine ähnliche Entwicklung erwartet werden. «Wir rechnen nicht damit, dass es in Zukunft zu einer katastrophalen Freisetzung von Methan aus Permafrostböden kommen wird», erklärt Vasilii Petrenko, der Leiters des Labors, in dem die Studie entstand. Petrenko nimmt vielmehr an, dass Emissionen durch die Nutzung fossiler Brennstoffe viel bedeutsamer für den künftigen Methangehalt der Luft sein werden.

Methan gelangt nicht nur bei der Produktion und Nutzung von Erdgas in die Luft, sondern auch bei der Förderung von Erdöl und Kohle. Beispielsweise entweicht das Treibhausgas aus vielen offenen Kohleflözen – ganz besonders in China, aber gemäss einem Bericht der Internationalen Energieagentur auch in Russland, den USA, Indien, Australien, Südafrika und Indonesien.
Fossile Quellen

Wahrscheinlich stossen die fossilen Industrien deutlich mehr Methan aus als gedacht. Diese Einschätzung wird jedenfalls durch eine weitere Studie aus Petrenkos Labor befeuert, die vor kurzem im Magazin «Nature» erschien.2 Die Autoren ermittelten indirekte Hinweise darauf, dass die Freisetzung von Methan aus geologischen Quellen im Laufe der Industrialisierung stark angestiegen ist. Das Ergebnis verblüffte die Fachwelt, denn bis anhin hatten die Schätzungen der Methanemissionen aus fossilen Industrien wesentlich niedriger gelegen.

Nun wächst der politische Druck, Massnahmen zur Verringerung der Methanfreisetzung zu ergreifen. Mehrere Kommentatoren weisen darauf hin, dass der Anreiz, sich von der Energieerzeugung aus fossilen Brennstoffen zu lösen, angesichts der neuen Resultate noch einmal gewachsen sei: Denn wenn diese Branche einen grösseren Anteil der Treibhausgase erzeugt als gedacht, haben Massnahmen in diesem Bereich auch eine grössere Wirkung auf das Klima. Die Öl- und Gasproduktion weltweit biete ein erhebliches Potenzial, um die Methanemissionen in den nächsten Jahrzehnten zu senken, sagt Lena Höglund-Isaksson vom International Institute for Applied Systems Analysis in Wien, die sich in einer Studie den technischen Reduktionsmöglichkeiten gewidmet hat.
Zersetzung in Sümpfen

Die andere wichtige Methanquelle – neben den geologischen Quellen – sind Feuchtgebiete, vor allem solche in den Tropen. Das geht etwa aus einer Übersichtsstudie von einem hochrangig besetzten Team um Marielle Saunois an der Université Versailles hervor, die zurzeit noch fachlich begutachtet wird.3

Feuchtgebiete produzieren Methan vor allem durch bakterielle Zersetzung, in Sümpfen zum Beispiel. Aber nicht alle Bakterien produzieren Methan, ein Teil von ihnen verzehrt es stattdessen. Ob ein Feuchtgebiet das Gas emittiert, wird erst aus der Bilanz klar. Welche Bakterienarten begünstigt werden, diktieren die Umgebungsbedingungen, und eine davon ist die Temperatur. Allgemein wird erwartet, dass mit der fortschreitenden globalen Erwärmung die Emission von Methan aus Feuchtgebieten zunehmen wird.
Methanquelle Reisfeld

Auch viele vom Menschen geprägte Feuchtgebiete setzen Methan frei, Reisfelder zum Beispiel. Da die Weltbevölkerung weiter wächst, wird voraussichtlich immer mehr Reis angebaut. Man muss also damit rechnen, dass diese Emissionen noch zunehmen werden, wie indische Forscher von der Universität BBAU in Lucknow, Uttar Pradesh, in dem Buch «Restoration of Wetland Ecosystem: A Trajectory Towards a Sustainable Environment» schreiben. Nachhaltige Formen des Reisanbaus müssten nun erforscht werden. Mehrere Studien haben bereits gezeigt, dass sich die Methanemissionen verringern lassen, wenn die Felder auf adäquate Weise bewässert, gepflügt oder besät werden.

Gemäss Saunois gehen zwischen 50 und 70 Prozent der gegenwärtigen Methanemissionen auf den Menschen zurück. Neben den fossilen Industrien und dem Anbau von Reis und anderen Nutzpflanzen ist hier noch eine weitere wichtige Quelle zu nennen: die Viehhaltung, die global ungefähr ein Drittel der anthropogenen Methanfreisetzung hervorruft. Die Angaben zu den Emissionen sind allerdings noch sehr ungenau – das haben die neuen überraschenden Studien aus Petrenkos Labor erneut gezeigt. Oft müssen Punktmessungen auf weite Gebiete extrapoliert werden. Das führt schon aus mathematischen Gründen zu grossen Fehlern.
Messen per Satellit

Auch darum ist immer noch unklar, was den raschen Methananstieg seit 2007 hervorgerufen hat. Als heisser Kandidat gelten die Feuchtgebiete in den Tropen. Gemäss Höglund-Isaksson stehen aber auch die Förderung von Schiefergas, die Kohleförderung sowie Müll und Abwässer unter Verdacht, zu dem Anstieg beigetragen zu haben.

Um die Ungewissheit bei den Quellen zu verringern, braucht es noch präzisere Messungen der Methankonzentration als bisher. Forscher setzen grosse Hoffnungen in ein Instrument an Bord des europäischen Satelliten Sentinel-5P, der seit dem Jahr 2017 die Erde umkreist. Neben vielen anderen Grössen misst das «Tropospheric Monitoring Instrument» auch die Methankonzentration in der Luft. Noch ist die Auswertung der Messungen nicht ausgereift. Künftig soll es dank diesen Daten aber gelingen, die Quellen des Gases genauer festzustellen und bei der Eindämmung der Emissionen mitzuhelfen.";https://www.nzz.ch/wissenschaft/treibhausgas-methan-auftauender-permafrost-nicht-die-hauptquelle-ld.1546899;NZZ;Sven Titz;;;
24.10.2018;Die Universität Zürich schafft 18 neue Professuren im Bereich der Digitalisierung;"«Die Digitalisierung verändert unsere Arbeitswelt, unsere ganze Gesellschaft fundamental», sagt Michael Hengartner. Um den Wandel mitzugestalten, brauche es nicht nur Informatikerinnen und Naturwissenschafter, sondern auch Ökonomen, Juristinnen, Psychologen, Ethikerinnen. Mit eindringlicher Stimme fährt der Rektor der Universität Zürich (UZH) fort: «Die Digitalisierung bringt nicht nur Herausforderungen, sondern auch viele Chancen.» Nach einer Kunstpause beendet er seinen kurzen Appell: «Nur wenn Expertinnen und Experten aus den unterschiedlichen Bereichen zusammenarbeiten, werden wir die Chancen für die Gesellschaft packen.»

Diesen strategischen Leitgedanken verkündete Hengartner vor einem knappen Jahr – passenderweise – per Videobotschaft auf Youtube. Jetzt hat die Universität einen historischen Schritt angekündigt: Sie schafft achtzehn neue Professuren, die sich in einer interdisziplinären Herangehensweise mit der Digitalisierung befassen. Seit der Gründung der philosophischen Fakultät im Jahr 1833 habe es an der Universität Zürich nie mehr eine solch grosse Aufstockung gegeben, sagt Hengartner im persönlichen Gespräch mit der NZZ.

Bereits vor zwei Jahren wurde mit der Digital Society Initiative (DSI) ein informelles Netzwerk für eine fächerübergreifende Zusammenarbeit geschaffen. Der Zulauf war gross: Rund 170 der insgesamt 675 Professorinnen und Professoren schlossen sich an. Im Zentrum standen die Themenbereiche Gesundheit, Demokratie, Arbeit, Mobilität und Kommunikation. Das ist auch bei den achtzehn neuen Professuren der Fall. Die acht Lehrstühle und zehn Assistenzprofessuren befassen sich mit Digital and Mobile Health, Big Data Science, Digital Entrepreneurship, künstlicher Intelligenz und Machine-Learning oder digitaler Sprachwissenschaft. Das Berufungsverfahren ist im Gang, die Kandidaten werden weltweit rekrutiert. Bereits im kommenden Jahr werden die ersten Professoren ihre Arbeit aufnehmen.
Szenarien erarbeiten

Abraham Bernstein vom Direktorium der DSI erklärt, welche Fragestellungen in den verschiedenen Themenbereichen entstehen können. Wie kann man beispielsweise von medizinischer, technologischer, sozialer, aber auch juristischer Seite auf die explodierenden Gesundheitskosten reagieren? Was für Gesetze und Sicherheitsvorkehrungen bestehen für digitale Krankenakten? Oder: Wie lässt sich die Schweizer Demokratie mit technologischen Hilfsmitteln weiterentwickeln? Wie kann man mit Tausenden Leuten konstruktive Diskussionen führen und Entscheidungen treffen? Im Bereich der Lehre gehe es darum, Bildungstechnologie förderlich einzusetzen, damit die Studierenden beispielsweise Theorien selbständig erlernten und die Vorlesungen für Diskussionen genutzt werden könnten. Als Volluniversität könne die UZH bei diesen Themen einen wichtigen Beitrag leisten, sagt Hengartner. «Wir wissen nicht, wie sich die Digitalisierung entwickelt, aber wir können Szenarien erarbeiten und Optionen auf den Tisch legen», sagt der Rektor. Das sei für ein direktdemokratisches Land wie die Schweiz zentral. «Am Ende entscheiden die Bürgerinnen und Bürger, wohin die Reise geht.»
«Doppelbürger» gefordert

In der Lehre werden für die Studierenden neue Nebenfächer geschaffen, die in die bestehenden Curricula integriert werden können. Auf Master- und Doktorandenstufe sollen Spezialprogramme angeboten werden. In der Forschung streicht Bernstein die interdisziplinäre Zusammenarbeit hervor. Die Digitalisierung erfordert, dass die Grenzen der Fächer und Fakultäten überschritten werden. Der Informatikprofessor nennt die neuen Professoren «Brückenbauer zwischen etablierten Disziplinen». Hengartner beschreibt sie als «Doppelbürger», die ihr Fachwissen aus der eigenen Fakultät nach aussen tragen. «Sie müssen in zwei Welten zu Hause sein, die beide ihre interne Logik haben.»

    «Die Digitalisierung rast so schnell auf uns zu, dass wir nicht warten können, bis diese Professuren frei werden. Sonst ist der Zug schon längst abgefahren.»

    Michael Hengartner, UZH-Rektor

Mit der Fokussierung auf die Digitalisierung ist die Universität Zürich nicht allein. So hat das Massachusetts Institute of Technology (MIT) kürzlich bekanntgegeben, eine Milliarde Dollar in ein neues College für künstliche Intelligenz zu investieren. Auch an chinesischen Universitäten gibt es vergleichbare Aufwendungen. Die ETH Zürich hat im Zuge des Projekts ETH plus rund hundert neue Professuren in zukunftsträchtigen Wissensgebieten angekündigt, viele davon mit interdisziplinärem Ansatz und im Bereich der Digitalisierung.

Mit den achtzehn neuen Professuren im Bereich der Digitalisierung sei man im europäischen Vergleich vorne mit dabei, sagt Hengartner. Er spricht von einer Investition in dreistelliger Millionenhöhe über die Jahre, die aus Reserven, Fundraising und Umschichtungen generiert werden soll. Das unterstreiche die gesamtgesellschaftliche Relevanz des Themas. «Früher hiess es, man müsse lesen, schreiben und rechnen können. Jetzt kommen die digitalen Fähigkeiten hinzu», sagt der Rektor. Diese Fähigkeiten seien grundlegend für das Verständnis dafür, wie die heutige Welt funktioniert. «Eigentlich hätten wir den Schritt bereits vor Jahren machen sollen», sagt der Rektor. Deshalb müsse man nun möglichst rasch handeln. Der normale Turnus von Professoren, die nach der Pensionierung ersetzt würden, reiche dabei nicht aus. «Die Digitalisierung rast so schnell auf uns zu, dass wir nicht warten können, bis diese Professuren frei werden. Sonst ist der Zug schon längst abgefahren.»";https://www.nzz.ch/zuerich/uzh-18-neue-professuren-im-bereich-der-digitalisierung-ld.1430445;NZZ;Nils Pfändler;;;
04.03.2020;Wiederholt sich die Geschichte? Und können wir die Zukunft berechnen, wenn wir über genügend Daten verfügen?;"Wenn man die Nachrichten in der Zeitung liest, beschleicht einen das Gefühl, dass alles schon einmal da war: Gewalt im Nahen Osten, Drohgebärden autoritärer Herrscher, Kriege, humanitäre Katastrophen, vorhersehbare Empörungs- und Erregungswellen nach jedem Skandal, die Themen, die immer wieder einmal Konjunktur haben, der Aufstieg und der Fall von Helden – so als würden die Zeitläufte einem Muster folgen.

Wiederholt sich die Geschichte? Drohen mit der Zersplitterung des Parteiensystems Weimarer Verhältnisse? Sind die Konfessionskriege im Nahen und Mittleren Osten eine Wiederkehr des Dreissigjährigen Kriegs? Welche statistischen Lehren lassen sich aus der Vergangenheit ziehen? Und kann man aus der Auswertung historischer Daten künftige Krisen und Konflikte vorhersagen?

Seit Jahrhunderten schwelt in der Philosophiegeschichte ein Streit darüber, ob Geschichte eine Naturwissenschaft wie Physik oder Biologie ist. Der britische Geschichtsphilosoph Arnold Toynbee vertrat die Ansicht, dass Geschichte «Naturgesetzen» unterliege. Auch marxistische Theorien gehen von einem deterministischen Verlauf der Geschichte aus.
Wie Herrschaft entsteht

Andere Theoretiker wie Karl Popper lehnten die Gleichsetzung von Geschichte und Naturwissenschaften kategorisch ab: Die Zeitläufte seien zu komplex, als dass man sie mit Naturgesetzen beschreiben könne. Menschen hätten einen freien Willen, Atome nicht. Kliometrie, also die quantitative Erfassung historischer Prozesse, gehe von falschen Voraussetzungen aus.

Der US-Evolutionsbiologe Peter Turchin versucht, die kliometrische Periodisierung zu rehabilitieren und zu aktualisieren: Er hat ein mathematisches Modell entwickelt, das erklären soll, wie politische Herrschaft entsteht und zusammenbricht. Turchin nimmt an, dass Geschichte zyklisch verläuft. So hat er zum Beispiel einen Zusammenhang zwischen Steppengrenzen und Grossreichen identifiziert.

Dieser Befund ist nicht neu. Dass es klimatische Voraussetzungen gibt, welche die Entstehung von Hochkulturen begünstigen, hat die Geschichtswissenschaft immer wieder betont. Historisch belegbar ist, dass fast alle Hochkulturen – das Reich von Akkad, das Römische Reich, das alte Ägypten, die Inka, die Azteken – ihre Ursprünge an Flüssen in den Subtropen haben.
Immer wieder: Gewaltausbrüche

Neu ist, dass Turchin diese Dynamiken mit quantitativen Daten belegt und daraus «makrohistorische Regelmässigkeiten» ableitet. So tritt nach seinen Berechnungen soziopolitische Instabilität in der historischen Betrachtung immer nach einem Peak der Bevölkerungsentwicklung auf. Rasantes Bevölkerungswachstum in Verbindung mit wachsender Ungleichheit und einer «Elitenüberproduktion» durch die Expansion von Universitätsabgängern, das sind für Turchin die soziodemografischen Ingredienzien für politische Instabilität.

Das ist natürlich sehr modellhaft, und der Biologe bestreitet auch gar nicht, dass exogene Faktoren wie das Klima oder Individuen auf der Mikroebene Einfluss auf die Zeitläufte haben und zu «erratischen Dynamiken» führen können. Trotzdem ist er überzeugt, dass es in der Geschichte Regelmässigkeiten gibt und man anhand dieser Regelmässigkeiten Prognosen über zukünftige Ereignisse erstellen kann. «Wenn aus dem Mikrochaos molekularer Bewegungen die Gesetze der Thermodynamik erwachsen (. . .), dann gibt es möglicherweise allgemeine Regelmässigkeiten, welche die Dynamiken menschlicher Gesellschaften charakterisieren, auch wenn das Verhalten einzelner Personen nicht vorhersagbar ist.»

Turchins Anspruch ist nicht weniger, als eine mathematische Theorie der Geschichte zu formulieren. Als empirisches Beispiel führt er die periodisch auftretenden Gewaltausbrüche in den USA an: Alle fünfzig Jahre, so zeigen seine kompilierten Daten, werden die USA von einer Welle der Gewalt erschüttert. 1870, nach dem blutigen Bürgerkrieg (1861 bis 1865), kam es in den Südstaaten erneut zu Unruhen. Knapp fünfzig Jahre später, während des «Red Summer» 1919, zogen Lynchmobs durch die Städte und machten Jagd auf Afroamerikaner. 1968 eskalierte mit der Ermordung von Martin Luther King die rassistische Gewalt erneut. Und 2020? Nach Turchin müsste wieder eine Welle der Gewalt ausbrechen.
Das Gehirn stösst an Grenzen

Der Biologe argumentiert, dass sich mit mathematischen Modellen der allgemeine Verlauf und «Aggregatszustand» der Gesellschaft prognostizieren lasse: soziale, konjunkturelle und politische Krisen, Kriege, Konflikte, Katastrophen. Es mutet ein wenig an wie in Isaac Asimovs Science-Fiction-Roman «Foundation», wo der Mathematiker Hari Seldon eine Grosstheorie entwickelt, die die Gesellschaft nach den Regeln der Chemie modelliert. Nur, dass diese Wissenschaft bei Turchin nicht Psychohistorik, sondern Kliodynamik heisst.

Den Einwand, soziale Systeme seien zu komplex, als dass sie mit mathematischen Modellen beschrieben werden könnten, entkräftet Turchin mit dem Gegenargument, gerade die Komplexität mache mathematische Modelle notwendig. Das blosse menschliche Gehirn sei «kein schlechtes Werkzeug, um lineare Trends zu extrapolieren», doch bei Systemen mit nichtlinearen Feedbackschleifen stosse der Denkapparat an seine Grenzen. Es bedürfe daher eines «mathematischen Formalismus», um unsere Ideen auszudrücken.

Big History feiert dank Big Data und Supercomputern ein Comeback. Es gibt eine Reihe von Projekten, die mit der Auswertung massenhafter Daten den Ausbruch humanitärer Krisen prognostizieren wollen. So versucht das «United Nations Global Pulse», ein Programm der Vereinten Nationen, anhand von Mobilfunkdaten den Verlauf des Zika-Virus vorherzusagen.
Nicht jedes Scharmützel ist ein Krieg

Das von Google finanzierte Projekt GDELT registriert Ereignisse aus der ganzen Welt, um auf dieser Grundlage Prognosen über Konflikte zu erstellen. Eine Software analysiert in Echtzeit Nachrichtenquellen aus Print, Fernsehen und Radio in über hundert Sprachen und bildet daraus einen Ereignisdatensatz, der mittlerweile auf 3,2 Billionen Datenpunkte aus den letzten zweihundert Jahren Geschichte angewachsen ist. Mithilfe von Algorithmen werden diese Daten kategorisiert und mit Datenbanken wie etwa historischen Archiven abgeglichen.

Aus diesen Daten werden für jedes Land sogenannte Unruheintensitäten – ein Mass für Aufruhr – destilliert, die auf einer Zeitachse abgetragen werden. Die Proteste in Kairo im Januar 2011, die den Beginn des Arabischen Frühlings markierten und zum Sturz des Machthabers Mubarak führten, wiesen beispielsweise Parallelen mit dem Selbstmordanschlag in Stockholm im Dezember 2010 auf, wo zwei Sprengsätze in einer belebten Einkaufszone detonierten.

Nun muss man bei historischen Analogien vorsichtig sein. So grobschlächtige Vergleiche laufen Gefahr, Ereignisse zu relativieren oder zu historisieren. Nicht jede Migrationsbewegung ist eine Völkerwanderung, und nicht jedes Scharmützel ist ein grosser Krieg, zumal Gewalt vielgestaltig und methodisch schwer zu fassen ist. Trotzdem lassen sich historische Ereignisse ganz wertneutral in Beziehung setzen.
Das Ende der Geschichte?

Die Frage ist ja nicht nur, was es für die Geschichtsschreibung bedeutet, wenn Geschichte als Datensatz erscheint, sondern was es für die Selbstbeschreibung der modernen Gesellschaft bedeutet, die ja davon ausgeht, dass sich das Zeitgeschehen gestalten lässt. Was heisst das für unser Fortschrittsnarrativ? Kann es noch Utopien geben, wenn sich die Geschichte wiederholt? Kann es sein, dass die Verdopplung der Welt in Binärcodes unser eigenes Handeln simulatorisch erscheinen lässt? Wie soll das Projekt der Moderne weitergehen, wenn wir im Spiegel der Daten einen fatalen Determinismus erblicken? Ist es das Ende der Geschichte, wenn sie nicht mehr ist als eine Abfolge von Programmen?

Vielleicht stehen wir erst am Anfang der Geschichte, weil uns die Beschreibungskategorien fehlen. Vielleicht ist es aber auch nur ein epistemologisches Problem: ein Muster des Sozialen, dass der Mensch dort Zusammenhänge erblickt, wo gar keine sind, um sich seiner eigenen Rationalität zu versichern. Womöglich haben wir auch gar nicht erkannt, dass die Algorithmen, die aus historischen Daten Wahrscheinlichkeiten für die Zukunft ableiten, selbst Geschichte schreiben, und wir trotz der Gegenwartsfixiertheit unserer Zeit diese Chronistenrolle im Nachhinein gar nicht wahrnehmen. Wie formulierte Victor Hugo in seinem Roman «Der lachende Mann» (1869) so schön: «Was ist Geschichte? Ein Echo der Vergangenheit in der Zukunft, ein Widerschein der Zukunft in der Vergangenheit.»";https://www.nzz.ch/feuilleton/big-history-koennen-wir-die-zukunft-der-weltgeschichte-berechnen-ld.1543750;NZZ;Adrian Lobe;;;
16.07.2020;Das Lauterkeitsverfahren gegen den Kardiologen Frank Ruschitzka endet mit einer Ermahnung;"Die Universität Zürich hat einen Schlussstrich unter die Causa Ruschitzka gezogen. Sie ermahnt den Leiter des Universitären Herzzentrums, bei wissenschaftlichen Publikationen in Zukunft sorgfältiger zu verfahren und sämtliche Aspekte rigoros auf ihre wissenschaftliche Integrität zu prüfen. Mit diesem Schiedsspruch stellt die Universität Zürich das Lauterkeitsverfahren ein, das nach Kritik an einer Studie von Ruschitzka eingeleitet worden war.

Frank Ruschitzka werde in den nächsten Monaten 30 Stunden unentgeltlich Einsatz am Center for Reproducible Science leisten, heisst es in einer Mitteilung der Universität. Ziel sei es, den Begutachtungs- und Publikationsprozess von Manuskripten der Big-Data-Forschung in der Medizin an der UZH zu verbessern, um solche Fälle zukünftig zu vermeiden.

Stein des Anstosses ist eine Studie, die Ruschitzka Mitte Mai zusammen mit drei Mitautoren in der Fachzeitschrift «Lancet» veröffentlicht hatte. Darin kamen die Forscher zu dem Schluss, das Malariamedikament Hydroxychloroquin schade bei der Behandlung von Covid-19 mehr, als es nütze. Die Autoren stützten sich dabei auf Daten, die von einer privaten Firma zusammengetragen und aufbereitet worden waren. Schon bald wurde allerdings von verschiedenen Seiten der Verdacht geäussert, die Daten seien fabriziert. Doch die Firma weigerte sich, die Daten offenzulegen. Daraufhin zogen Ruschitzka und zwei seiner Mitautoren ihre Publikation zurück.

Schon vor einigen Wochen hatte Ruschitzka darauf hingewiesen, dass er weder an der Erhebung der Rohdaten noch an deren Weiterverarbeitung beteiligt gewesen sei. Er sei erst zum Verfassen des Manuskripts beigezogen worden. Die Universität Zürich schliesst sich dieser Sichtweise an, moniert jedoch, Ruschitzka sei bei der Beurteilung der Korrektheit und der Aussagekraft der Patientendaten zu unkritisch gewesen. Den gleichen Vorwurf müssten sich auch die Gutachter der Fachzeitschrift «Lancet» gefallen lassen.";https://www.nzz.ch/wissenschaft/ruschitzka-universitaet-stellt-verfahren-wegen-lancet-studie-ein-ld.1566783;NZZ;Christian Speicher;;;
22.10.2020;«Testen, testen, testen»? – Kaum ein anderes europäisches Land hat das Testing so schleppend ausgebaut wie die Schweiz;"Testing kostet Geld, viel Geld. Den Coronavirus-Test vergütet der Bund mit einer Pauschale von 169 Fr. Bei derzeit durchschnittlich 17 000 Tests pro Tag kommen so täglich fast 3 Mio. Fr. zusammen. Allerdings haben renommierte Ökonomen wie der Wirtschaftsnobelpreisträger Paul Romer immer wieder darauf hingewiesen, dass es sich lohnt, noch weitaus mehr Geld dafür in die Hand zu nehmen. Schliesslich sind alle Alternativen wie Betriebsschliessungen und Quarantänemassnahmen mittelfristig mit sehr viel höheren ökonomischen und sozialen Kosten verbunden. Ökonomen der Universität Zürich schätzten, dass es sich lohnen würde, 13 Mio. Tests durchzuführen, um eine Woche Lockdown zu verhindern.

So hatte auch der Zürcher Verhaltensökonom Ernst Fehr schon im März in einem Video für die NZZ dazu aufgerufen, regelmässig repräsentative Stichproben der Bevölkerung zu testen, um endlich verlässliche Zahlen zur Verbreitung des Virus in unterschiedlichen Bevölkerungsgruppen und Regionen zu bekommen.

Allerdings hätte dies zweierlei vorausgesetzt: Zum einen hätten auch Personen ohne Symptome in grossem Ausmass getestet werden müssen. Zum anderen hätte man eine solche Strategie zunächst einmal entwickeln und gemeinsam mit den Kantonen und Gesundheitsbehörden in Gang setzen müssen.

Die Teststrategie der Schweiz basiert aber laut dem Bundesamt für Gesundheit (BAG) darauf, nur Personen mit Symptomen zu testen. Die internationalen Testdaten zeigen darüber hinaus, dass die Schweiz die Tests offenbar nicht kontinuierlich ausgeweitet hat, wie das in anderen Ländern geschehen ist. Getestet wird vor allem dann viel, wenn die Infektionszahlen gerade stark steigen.
Nach einem Höhepunkt im Sommer hat die Schweiz das Testing wieder heruntergefahren

Die Ausgangslage im Frühjahr war gar nicht schlecht: Anfang April gehörte die Schweiz bei der Anzahl Tests pro Kopf noch zur Spitzengruppe in Europa. Damals wurden zirka 0,7 Tests auf 1000 Einwohner durchgeführt. Bis Anfang Juli stieg die Zahl der Tests deutlich, und die Schweiz lag vor Ländern wie Deutschland, Schweden und Italien. Der Bund übernahm ab Ende Juni auf Empfehlung der Ökonomen in der Swiss National Covid-19 Science Task Force die Testkosten, was wohl ein entscheidender Faktor war, um das Testing voranzutreiben.

Was ist danach passiert? Die Schweiz hat in den restlichen Sommermonaten das Testing vernachlässigt – offenbar ohne epidemiologische Rechtfertigung, denn die Fallzahlen stiegen bereits langsam, aber stetig. Nach dem Ende der Sommerferien wurde dann wieder mehr getestet – die tägliche Anzahl Tests wuchs im September deutlich schneller als die Zahl der neuen Fälle. Kurz bevor die Fallzahlen Ende September laut offiziellen Meldungen etwas zurückgingen, wurde das Testing aber wieder heruntergefahren. Erst als die Infektionszahlen im Oktober deutlich anstiegen, wurde die Anzahl Tests stark erhöht. (Hinweis: Die folgende Grafik wurde Ende November nochmals aktualisiert, der Text bezieht sich aber auf die Situation Mitte Oktober). Mit dem wellenförmigen Verlauf steht die Schweiz ziemlich alleine da. Zwar gab es auch in anderen europäischen Ländern Phasen, in denen das Testing kurzzeitig zurückgefahren wurde. Allerdings zeigt sich sehr deutlich, dass die meisten Länder spätestens ab Juli die Anzahl Tests kontinuierlich gesteigert haben – noch bevor die Fallzahlen im Herbst überall in die Höhe schnellten. So wuchs in Deutschland und Schweden die Zahl der Tests früher als die Fallzahlen – damit dürften diese Länder in einer komfortableren Position sein, wenn es darum geht, rasch Infizierte in der zweiten Welle zu identifizieren. Bei der Anzahl Tests pro Kopf ist die Schweiz immer noch abgeschlagen

Besonders Dänemark hat die Tests ab dem Sommer sehr stark erhöht. Dort wurden Anfang Oktober täglich mehr als 8 Tests pro 1000 Einwohner durchgeführt, etwa siebenmal so viele wie in der Schweiz. Auch Belgien, Irland, Finnland, Norwegen sowie Österreich haben die Anzahl Tests sehr stark erhöht und zwei- bis dreimal so viel getestet wie die Schweiz.
Kein anderes europäisches Land hat die Tests seit dem Frühjahr so wenig ausgebaut wie die Schweiz Seit Anfang Oktober haben viele Länder die Zahl der täglichen Tests nochmals deutlich erhöht – so auch die Schweiz als Reaktion auf den starken Anstieg der Krankheitsfälle. Sie kommt nun auf etwa doppelt so viele Tests pro Kopf als noch Anfang des Monats. Dänemark hat die Testintensität hingegen etwas heruntergefahren, hatte aber auch weniger Neuinfektionen zu verzeichnen. Trotzdem ist Dänemark mit über 5 Tests pro 1000 Einwohner immer noch der Spitzenreiter, vor Belgien und Grossbritannien. Die Schweiz konnte zwar im Vergleich etwas aufholen, sie liegt aber immer noch im hinteren Mittelfeld, bezogen auf die Anzahl Tests pro Kopf. (Hinweis: Die folgenden Grafiken wurden Ende November nochmals aktualisiert, der Text bezieht sich aber auf die Situation Mitte Oktober).
Bei den Tests pro Kopf befindet sich die Schweiz aktuell im Mittelfeld Aufschlussreich ist auch ein Vergleich der Länder bei der Anzahl Tests im Verhältnis zu ihrer Wirtschaftsleistung. Die Schweiz ist eines der reichsten Länder der Welt und liegt mit einem Bruttoinlandprodukt pro Kopf von zirka 85 000 Fr. (2019) vor den skandinavischen Ländern sowie meilenweit vor Deutschland, Frankreich und Grossbritannien. Die Schweiz sollte sich die teuren Tests am ehesten leisten können, sie testet aber im Verhältnis zum Bruttoinlandprodukt bei weitem am wenigsten. Andere kleine Länder wie Litauen, Zypern und Lettland nehmen dagegen im Verhältnis sehr viel mehr Ressourcen in die Hand.
Gemessen an der Wirtschaftsleistung testet die Schweiz sehr wenig Hohe Positivitätsrate und unkontrollierte Ausbreitung

Die gegenwärtig sehr hohe Positivitätsrate bzw. das niedrige Verhältnis von Tests und bestätigten Fällen dürfte unter anderem auch mit einer zu geringen Anzahl Tests während der Sommermonate und Ende September zusammenhängen. Nur osteuropäische Länder und Belgien testen pro Fall derzeit noch weniger. Viele Fälle werden dadurch nicht mehr erkannt. Damit sind die Pforten für eine unkontrollierte Ausbreitung des Virus weit geöffnet. Gerade Osteuropa und Belgien haben wie die Schweiz jetzt mit einem besonders starken Anstieg der Neuinfektionen zu kämpfen.
Nur einige osteuropäische Länder testen pro Fall weniger als die Schweiz Eine weitere Folge des Testregimes: Die von Epidemiologen und Statistikern benötigte Datenbasis ist weiterhin stark verzerrt – die täglich publizierten Fallzahlen zeigen kein realistisches Bild der wirklichen Situation.

Ob mehr Tests den Verlauf der Pandemie verändert hätten, ist ungewiss: Auf Anfrage teilte das BAG mit, dass es darauf keine Hinweise gebe, allerdings könne auch nicht ausgeschlossen werden, dass die Angst vor der Quarantäne Menschen davon abgehalten habe, sich testen zu lassen.

Der Ökonomieprofessor David Dorn von der Universität Zürich und Mitglied der Swiss National Covid-19 Science Task Force hält radikale Lösungen wie Zwangstesten oder das Testen der gesamten Bevölkerung in einem freiheitlichen Staat zwar für nicht durchführbar, aber auch er plädiert dafür, lieber etwas zu viele Tests durchzuführen als zu wenige. Wichtig sei im derzeitigen Regime vor allem, die Botschaft zu vermitteln, dass man sich auch bei nur geringen Symptomen testen lasse, und den Zugang zu Tests weiter zu vereinfachen.

Laut seinem Kollegen Marius Brülhart von der Universität Lausanne, ebenfalls Mitglied der Task-Force, sind in der Schweiz Kostenerwägungen wohl der Hauptgrund, warum das Testing nicht grossflächiger ausgebaut wurde. Es entstehe der Eindruck, dass gewisse Entscheidungsträger im Bereich der öffentlichen Gesundheit Mühe hätten, sich an die finanziellen Dimensionen der Jahrhundert-Pandemie zu gewöhnen. «Wenn die Wirtschaft Milliarden verliert, sollte im Gesundheitswesen nicht um Millionen gerungen werden», so Brülhart.";https://www.nzz.ch/wirtschaft/schweiz-das-corona-testing-wurde-zu-kleinflaechig-ausgebaut-ld.1582227;NZZ;Florian Seliger;;;
04.12.2020;Fast so steil wie die Infektionskurve: Die Twitter-Aktivitäten der Experten stiegen im Herbst stark an;"«Die Tendenz vieler Politiker, nun zu behaupten, man habe diese Entwicklung nicht kommen sehen und man sei von Wissenschaftern nicht bereits seit Monaten gewarnt worden, ist äusserst bedenklich und der Schweiz unwürdig.» Mit solchen und ähnlichen Tweets meldeten sich der Epidemiologe Christian Althaus von der Universität Bern und seine Kollegen der Covid-19 Science Task Force im Herbst zu Wort.

Die Angriffe richteten sich meist gegen die langsame Entscheidungsfindung im föderalen System der Schweiz. Oder gegen die Tendenz von Kantonen und Bundesrat, sich gegenseitig die Verantwortung zuzuschieben. In vielen Tweets wurde der Satz «Jeder Tag zählt» verwendet – eine Anspielung auf das exponentielle Wachstum bei der Ausbreitung des Virus, das sich bekanntlich nicht für die Regeln im Politikbetrieb interessiert.

Einige Mitglieder fühlten sich im Verlauf der Pandemie von der Politik regelrecht übergangen, jedenfalls waren sie ganz und gar nicht mit dem Lockerungskurs des Bundesrates während der Sommermonate einverstanden. Christian Althaus ärgerte sich im Oktober besonders, als öffentlich wurde, dass im Kanton Schwyz ein Jodlerfestival stattfand und sich einige Wochen später ein Schwyzer Spital mit einem Hilferuf an die Öffentlichkeit wandte.

In einem weiteren Tweet sprach er gar von einem Maulkorb, den man den Experten gerne verpasst hätte.
Die Zahl der Experten-Tweets stieg zusammen mit den Corona-Fällen stark an

Die Swiss National Covid-19 Science Task Force besteht aus fast 70 Wissenschaftern aus unterschiedlichen Fachrichtungen, die zehn Expertengruppen bilden. Ihr Auftrag besteht darin, die Behörden in der jetzigen Krise zu beraten und die Öffentlichkeit über die aktuellsten wissenschaftlichen Erkenntnisse zum Virus zu informieren.

Den Informationsauftrag nahmen die Mitglieder sehr ernst. Dies zeigt eine Auszählung aller von den Mitgliedern der Task-Force versendeten Tweets. Im Herbst stiegen die Experten-Tweets stark an. Waren es im Frühjahr noch maximal um die 71 Tweets pro Tag, kletterte die Zahl Ende Oktober bis auf über 90 Tweets pro Tag an. Die Höchstzahl wurde kurz nach der Einführung von verschärften Massnahmen durch den Bundesrat erreicht. Nicht eingerechnet sind die zahllosen Retweets, in denen Beiträge anderer Twitter-Nutzer oder Zeitungsartikel weitergeleitet werden.
Die Experten verschickten Ende Oktober über 90 Tweets pro Tag Die Twitter-Nutzung der Experten ist allerdings sehr ungleichmässig verteilt. So ist der Spitzenreiter @FLAHAULT mit ungefähr 7400 Tweets seit März ein extremer Ausreisser. Hinter diesem Account steht Antoine Flahault, Direktor am Institut de santé globale an der Universität Genf, der täglich oft Dutzende Tweets verfasst, um die epidemiologische Lage verschiedener Länder darzustellen.
Ein Experte ist für über 45% der Tweets verantwortlich In den deutschsprachigen Medien wesentlich bekannter und mit grösserem Schweiz-Bezug sind @C_Althaus mit zirka 950 Tweets sowie zwei weitere Forscher von der Universität Bern, nämlich @nicolamlow (Nicola Low, 1900 Tweets) und «Matthias #WashYourHands Egger» @eggersnsf (700 Tweets). Zu erwähnen sind auch Samia Hurst (Universität Genf), Thierry Fumeaux (Schweizerische Gesellschaft für Intensivmedizin) und die St. Galler Ökonomin Monika Bütler. Die anderen Experten twittern wesentlich weniger – einige wichtige Exponenten wie der derzeitige Chef der Task-Force, Martin Ackermann, und der von Medien häufig befragte Epidemiologe Marcel Tanner glänzen gar mit völliger Abwesenheit von Twitter.

Lässt man nun den Extremfall @FLAHAULT weg, so nähert sich die Kurve dem Pandemieverlauf noch stärker an. Der Anstieg der Twitter-Meldungen im Herbst fällt deutlich mit dem starken Anstieg der Fallzahlen Anfang Oktober zusammen.
Die Tweets folgen dem Verlauf der Pandemie Die allermeisten Tweets drehten sich dabei um das eine Thema. Viele Inhalte verwenden sogenannte Hashtags, das sind durch ein Rautezeichen gekennzeichnete Schlagwörter, die das Auffinden von Tweets zu bestimmten Themen erleichtern. Der Begriff #COVID19 wurde in fast 5600 Tweets verwendet, #SARSCoV2 in 500. Alle anderen Hashtags werden sehr viel seltener von den Experten verwendet oder sind Schreibvarianten von #COVID19. Zwar finden sich in den Tweets auch andere Themen – diese fallen aber zahlenmässig kaum ins Gewicht.
So gut wie alle Tweets drehen sich um das eine Thema Die Experten erwähnen sich in ihren Tweets sehr oft gegenseitig. Besonders viele Erwähnungen bekommen Matthias Egger, Marcel Salathé und Christian Althaus. Erwähnungen bekommen auch das BAG und akademische Institutionen. Politiker und Journalisten werden hingegen kaum genannt.
Die Experten erwähnen sich in den Tweets oft gegenseitig Zwei Frauen sind auf Twitter besonders erfolgreich

Besonders eifrige Twitter-Benutzerinnen sind ausserdem @EckerleIsabella von der Universität Genf (über 1300 Tweets seit März) und @firefoxx66 (3500 Tweets). Hinter letzterem Pseudonym versteckt sich Emma Hodcroft, die Postdoktorandin am Biozentrum der Universität Basel ist und sich auf Twitter auch als «Die Virenjägerin!» bezeichnet. Beide sind zwar keine Mitglieder der Task-Force, prägen aber die Debatte mit ihren Beiträgen stark.

Isabella Eckerle ist Spitzenreiterin bei den sogenannten Retweets und den Followern. Retweets sind Erwähnungen in den Tweets anderer User; Follower die Personen, die jemandem auf Twitter folgen. Retweets sind praktisch das Pendant zu Zitationen in der Wissenschaft und messen, wie stark die eigenen Tweets die Debatte prägen. Eckerle erreicht dabei mit Abstand den höchsten Wert. Sie hat mehrere Tweets verfasst, die beinahe 2000 Retweets generiert haben. Seit September kommt sie auf durchschnittlich fast 60 Retweets pro Tweet und liegt damit weit vor den Task-Force-Mitgliedern.
Durchschnittliche Anzahl Retweets pro Tweet Mit je zirka 30 000 Followern liegen Eckerle und Hodcroft deutlich vor Marcel Salathé (18 000), Christian Althaus (zirka 17 000) und auch vor national bekannten Schweizer Politikern wie SVP-Nationalrat Roger Köppel (24 000) oder SP-Nationalrätin Jacqueline Badran (21 000).
Nicht alle Tweets haben grossen wissenschaftlichen Gehalt

Auf Twitter ist es besonders schwierig, Fakten von Meinungen und reinen Emotionen zu trennen. Während emotionale Äusserungen in wissenschaftlichen Veröffentlichungen tabu sind und Fakten und Meinungen normalerweise von Journalisten kanalisiert werden, werden sie auf Twitter beliebig durchmischt – auch durch die Wissenschafter.

Einige Experten, wie Antoine Flahault, beschränken sich tatsächlich meist darauf, Fakten und Statistiken zu verbreiten oder Ergebnisse wissenschaftlicher Studien zusammenzufassen. Auch Althaus versucht häufig in seinen Tweets, schwerverständliche Resultate einem breiteren Publikum zu erklären.

Doch oftmals handelt es sich um Statistiken aus bereits in den Medien ubiquitären Quellen wie «Our World in Data» und dem «Oxford Covid-19 Government Response Tracker»; Statistiken also, die dem informierten Publikum aus der Zeitungslektüre bereits wohlbekannt sein dürften.

Nicht selten werden Statistiken zudem ad hoc ausgewählt, um die eigenen Vermutungen zu unterstützen oder Zusammenhänge zu suggerieren.

Allerdings machen sich die Experten nicht immer die Mühe, noch ergänzendes Zahlenmaterial zu veröffentlichen, um die eigene Meinung zu untermauern. So forderte Isabella Eckerle Ende Oktober einen Lockdown für die Schweiz, ohne jedoch den Nutzen mit den gesellschaftlichen und wirtschaftlichen Kosten abzuwägen.

Andere Tweets sind eher dem Reich der persönlichen Gefühlsäusserungen zuzuordnen. So beklagte sich wiederum Isabella Eckerle in einem Tweet bitterlich über die mangelnde Bereitschaft der Gesellschaft, wissenschaftliche Erkenntnisse anzuerkennen, und zog Parallelen zur Klimadebatte.

Die Grenzen zwischen der Verbreitung wissenschaftlicher Erkenntnisse und politischem Aktivismus sind denn auch in beiden Diskussionen fliessend. Und da Experten auch nur Menschen sind, gewähren manche von ihnen auch tiefere Einblicke ins Privatleben. Emma Hodcroft klärt uns unter anderem über ihre Pläne für den Weihnachtsrummel auf, und sie lässt uns am Schicksal ihres durch zu häufigen Gebrauch verschlissenen Laptops teilhaben.

Um herauszufinden, wie emotional die Experten zur Sache gehen, haben wir alle im Herbst abgesonderten, englischsprachigen Experten-Tweets einer sogenannten Sentiment-Analyse unterzogen. Der dafür nötige Algorithmus wurde 2014 von amerikanischen Wissenschaftern entwickelt. Der Anteil der besonders emotionsgeladenen Tweets ist bei Hodcroft relativ hoch. Möglicherweise liegt darin auch eine Erklärung für die grosse Aufmerksamkeit, die sie auf Twitter geniesst. Bei Althaus zeigt die Analyse nur einen geringen Anteil an sehr emotionsgeladenen Tweets. Er verzichtet meistens auf emotionales Vokabular, auf das der Algorithmus stark reagiert, und wählt eher neutrale Formulierungen. Das erstaunt zunächst, zumal der Berner Epidemiologe einige Beiträge mit drastischen Meinungsäusserungen verfasst hat. So erinnert seine Aussage, wonach ihm die bösen Behörden einen «Maulkorb» verpassen wollen, in diesem Moment eher an Empörungssprech als an wissenschaftlichen Duktus. Solche Aussagen machen aber nur einen sehr kleinen Anteil seiner Tweets aus. Erklärbar ist die Diskrepanz zwischen Wahrnehmung und Datenauswertung unter anderem auch damit, dass mit der Sentiment-Analyse nur die englischsprachigen Tweets untersucht werden können. Seine Warnungen an die Politik verfasst Althaus jedoch meistens auf Deutsch.
Wie emotionsgeladen die Tweets sind Wissenschafter wurden zu wichtigen Meinungsführern

Gemäss Mike Schäfer, Professor für Wissenschaftskommunikation an der Universität Zürich, ist Twitter bis anhin noch kein wissenschaftliches Massenmedium. Oft kann zwar nicht getrennt werden zwischen Tweets von Privatpersonen und solchen im Namen eines öffentlichen Gremiums oder Berufsstandes. Die Wissenschafter, die Twitter nutzen, täten dies jedoch vor allem aus beruflichen Gründen, um Kollegen und «Meinungsführer» aus Medien und Politik unspezifisch zu adressieren.

In der Corona-Krise stieg die Nachfrage nach zeitnaher Information jedoch schlagartig stark an. Sie hat dazu geführt, dass Twitter bei Wissenschaftern zum unersetzlichen Kommunikationsinstrument geworden ist, um sich praktisch in Echtzeit mit Informationen und Warnungen an die Politik und die Öffentlichkeit zu wenden. Einige Wissenschafter haben die Aufmerksamkeit auf Twitter dazu genutzt, um selbst zu wichtigen Meinungsführern zu werden.

Bei den vielen Studien und Statistiken, die auf Twitter verbreitet werden, ist es unmöglich zu beurteilen, ob sie wissenschaftlichen Qualitätskriterien genügen. Auf Twitter sind bekanntlich auch Verschwörungstheoretiker und Anhänger anderer kruder Ansichten unterwegs, und die Identifizierung von Falschinformationen ist manchmal gar nicht so einfach. Das kritische Hinterfragen und die Einordnung der Informationen durch die Medien bleiben daher unabdingbar – darauf weist auch ein Beitrag in «Qualität der Medien – Jahrbuch 2020» der Universität Zürich deutlich hin. In Zeiten, in denen die Debatte auch durch manche Wissenschafter sehr emotionsreich geführt wird, ist das wohl umso wichtiger.

Hinweis: Für Marcel Salathé konnten wir keine älteren Tweets finden. Wir sind daher fälschlicherweise davon ausgegangen, dass Marcel Salathé mittlerweile einen neuen Twitter-Account benutzt. Wie wir mittlerweile aber über Twitter erfahren haben, lässt er alte Tweets regelmässig löschen. Wir haben die Fussnote in der entsprechenden Grafik angepasst.";https://www.nzz.ch/visuals/fast-so-steil-wie-die-infektionskurve-die-twitter-aktivitaeten-der-experten-stiegen-im-herbst-stark-an-ld.1588595;NZZ;Florian Seliger;;;
09.03.2019;Die Zürcher Hochschulen lancieren gemeinsam eine 300 Millionen Franken teure Digitalisierungsinitiative;"Zürich und seine Hochschulen rüsten sich für den absehbaren Digitalisierungswettbewerb. Bereits im Herbst gab etwa die Universität Zürich bekannt, 18 neue Professuren im Bereich der Digitalisierung zu schaffen. Nun sollen die Anstrengungen aller Zürcher Hochschulen intensiviert und gebündelt werden.

Erstmals lancieren die Universität Zürich (UZH), die Zürcher Hochschule für Angewandte Wissenschaften (ZHAW), die Zürcher Hochschule der Künste (ZHdK) sowie die Pädagogische Hochschule Zürich (PHZH) eine gemeinsame Digitalisierungsinitiative, wie der Zürcher Regierungsrat am Freitag bekanntgegeben hat.
Interdisziplinärer Ansatz

Die Digitalisierung stelle hohe Anforderungen an die wirtschaftliche, technologische und soziale Innovationskraft des Kantons Zürich, heisst es in der entsprechenden Mitteilung. Die Initiative solle dem Kanton eine «führende Rolle» in diesem Bereich sichern. Man wolle die Wettbewerbsfähigkeit von Zürich als Forschungs- und Entwicklungsstandort national und international stärken.

«Wir wollen den digitalen Wandel aktiv mitgestalten», sagte die Zürcher Bildungsdirektorin Silvia Steiner (cvp.) vor den Medien. Die Initiative besteht aus drei Teilen: einem Forschungscluster, einem Innovationsprogramm sowie der Bildungsförderung. Dadurch sollen Synergien und neue Impulse zwischen den Hochschulen entstehen.

Konkret soll die Initiative Brückenprofessuren mit interdisziplinären Schwerpunkten ermöglichen, die Zusammenarbeit mit dem privaten und dem öffentlichen Sektor unterstützen und digitale Lerninhalte sowie innovative Formen der Berufsbildung entwickeln helfen. Damit soll sie letztlich Wirtschaft und Gesellschaft dabei unterstützen, die Chancen der Digitalisierung wahrzunehmen.
Doppelt so viele Universitätsprofessuren

Welche Vorhaben konkret geplant oder ausgebaut werden, erläuterten darauf die Rektoren der Zürcher Hochschulen. Der UZH verleiht die Initiative laut Michael Hengartner einen «zweiten Schub». Unter anderem sei eine Verdopplung der geplanten Professuren auf 36 möglich, wovon 20 befristete Assistenzprofessuren wären. Gestärkt würden die Bereiche Big Data, digitale Gesundheit sowie künstliche Intelligenz und maschinelles Lernen. Brückenprofessuren sind etwa mit der ZHAW in der KMU-Datenwirtschaft oder mit der ZHdK für Virtualität als Methode in der Forschung vorgesehen. Schliesslich soll es ein hochschulübergreifendes Doktoratsprogramm geben.

Der Rektor der ZHAW, Jean-Marc Piveteau, spricht von einer «Hebelwirkung», welche durch die Zusammenarbeit mit den anderen Hochschulen entstehe. Die ZHAW möchte mit den zusätzlichen Mitteln zum Beispiel ihr Data Science Lab ausbauen und Projekte für Open Labs oder Spin-offs unterstützen sowie Fellowship-Programme aufbauen. Ausserdem will sie mithelfen, digitale Lehr- und Lerninhalte zu entwickeln. Dadurch soll nicht zuletzt eine Brücke zwischen der Berufsmaturität und einem Fachhochschulstudium geschlagen werden, womit die Schule bereits begonnen hat.
Personalisiertes Lernen

Die ZHdK will sich im geplanten Forschungscluster mit zwei Schwerpunkten einbringen: Das sind zum einen Creative Economies, mit denen die wirtschaftlichen Entwicklungen im Kreativbereich untersucht werden. Die Digitalisierung schaffe neue Distributionskanäle, die andere Geschäftsmodelle erforderten, erläuterte Rektor Thomas D. Meier. Zum anderen möchte man die sogenannten Immersive Arts stärken. Beispiele für diesbezügliche Pilotprojekte sind weiterentwickelte 3-D-Modelle oder Motion-Capture-Techniken für Filmaufnahmen.

Die Pädagogische Hochschule widmet sich den neuen Formen des digitalen Lehrens und Lernens. Als Beispiel nennt Rektor Heinz Rhyn adaptive Lernfördersysteme, die sich dem Bedürfnis und dem Kenntnisstand der Lernenden anpassen und einen individualisierten Unterricht ermöglichen. Daneben möchte man die digitalen Fähigkeiten («digital skills») von Lehrkräften und Dozierenden etwa mittels digitaler Selbstlernangebote stärken.
Kreditantrag von 108,3 Millionen Franken

Die Hochschulen investierten bereits heute namhafte Beträge, um sich für die Herausforderungen der Digitalisierung zu rüsten, sagte Bildungsdirektorin Steiner. Trotzdem seien sie auf die zusätzliche Finanzierung durch die öffentliche Hand angewiesen. Es brauche diesen «einmaligen finanziellen Sondereffort», um andere Bereiche der Hochschulen nicht zu schwächen.

«Wir kommen nun langsam an die Grenzen diesbezüglich, was wir allein stemmen können», bestätigte Universitätsrektor Hengartner. «Die Initiative kommt genau zum richtigen Zeitpunkt», pflichtete der Rektor der Pädagogischen Hochschule, Heinz Rhyn, bei. Die Hochschulen betonten, dass die Zusammenarbeit – auch mit der eidgenössisch getragenen ETH – aber bereits ein Fakt sei.

Zur Umsetzung der Digitalisierungsinitiative wird gesamthaft ein Kostenrahmen von 300 Millionen Franken für die Dauer von zehn Jahren veranschlagt. 191,7 Millionen Franken tragen die Hochschulen selbst, der Kanton soll zusätzlich 108,3 Millionen Franken beisteuern. Diesen Betrag beantragt der Regierungsrat dem Kantonsrat über einen entsprechenden Rahmenkredit. Sofern das Parlament diesem zustimmt, soll die Initiative bereits im nächsten Jahr starten.";https://www.nzz.ch/zuerich/digitalisierung-zuerich-lanciert-initiative-fuer-hochschulen-ld.1465575;NZZ;Lena Schenkel;;;
27.09.2019;Eine Schweizer Initiative will der digitalen Welt ein Gewissen verpassen;"Neue Technologien bringen viele Vorteile. Doch die Digitalisierung kann auch eine Gefahr darstellen: etwa dann, wenn Algorithmen diskriminierende Entscheidungen treffen, wenn Tech-Konzerne mit den Daten von Nutzern leichtfertig umgehen oder Unternehmen Überwachungstechnik an autoritäre Regime liefern, die sie zur Kontrolle der Bürger nutzen. Schon länger beschäftigen sich Staaten, Wissenschaft, Industrie und Zivilgesellschaft daher mit der Frage, wie verhindert werden kann, dass Big Data und Algorithmen zum Schaden anstatt zum Nutzen der Menschen eingesetzt werden.

Nun will eine neue Schweizer Initiative mitmischen und Unternehmen zu mehr Ethik in der digitalen Welt bewegen. Die Swiss Digital Initiative (SDI) ist Anfang September in Genf gestartet. Lanciert hat das Projekt die Standort-Initiative Digitalswitzerland. Den Vorsitz der dahinterstehenden Stiftung übernimmt die ehemalige Bundesrätin Doris Leuthard. Weitere Mitglieder sind die Präsidenten der ETH Zürich und der Universität Genf sowie der Ringier-CEO und Gründer von Digitalswitzerland, Marc Walder. Mit Bundeskanzler Walter Thurnherr soll auch der Bund im Stiftungsrat vertreten sein.

Die Initiatoren richten sich an Firmen weltweit und wollen diese dazu bringen, beim Umgang mit künstlicher Intelligenz (KI) oder bei der Ansammlung und Verwendung persönlicher Daten ethische Standards einzuhalten. Schlagworte sind Transparenz, Verantwortung, Nichtdiskriminierung oder Erklärbarkeit. Vertreter aus den Chefetagen von Google, Facebook, Microsoft oder Huawei waren bei der Lancierung der SDI denn auch anwesend. Ob und inwiefern die Unternehmen mitmachen werden, ist aber noch offen. Ein erster Entwurf eines Grundsatzpapiers soll in den kommenden Monaten überarbeitet werden. «Dann werden wir mal sehen, wer das unterschreibt», sagt Doris Leuthard im Gespräch mit der NZZ. Sie hoffe, im Januar am Weltwirtschaftsforum (WEF) in Davos erste Projekte vorstellen zu können.
Noch viele offene Fragen

Denn noch ist wenig bekannt dazu, wie konkret Unternehmen zur Einhaltung ethischer Standards bewegt werden sollen oder was die SDI von bereits existierenden Projekten unterscheiden wird. Man stehe noch am Anfang, sagt Leuthard. Die Idee sei aber, auf bestehenden Richtlinien und Ethik-Initiativen aufzubauen. «Es gibt relativ viele Prinzipien, was meistens fehlt, ist die konkrete Umsetzung», betont die ehemalige Bundesrätin.

Seit einiger Zeit häufen sich die Versuche, mehr Ethik in die digitale Welt zu bringen. Allein im Bereich der künstlichen Intelligenz zählte die Organisation AlgorithmWatch im Juni, ohne Anspruch auf Vollständigkeit, mehr als 80 Ethik-Initiativen. Die Mehrheit ist 2018 oder 2019 entstanden. Im Mai haben sich die Mitgliedsstaaten der Organisation für wirtschaftliche Zusammenarbeit und Entwicklung (OECD) auf nicht bindende Grundsätze für den Umgang mit KI geeinigt. Auf europäischer Ebene hat eine von der EU-Kommission eingesetzte Expertengruppe im April Ethik-Leitlinien für eine vertrauenswürdige KI veröffentlicht. Bis 2020 werden diese in einer Pilotphase von verschiedenen Interessenvertretern getestet.

Auch die Forschung und der Privatsektor befassen sich mit dem Thema, Konzerne setzen auf eigene Ethik-Komitees oder Ähnliches. Microsoft hat seine KI-Prinzipien, genau wie IBM. Amazon unterstützt gemeinsam mit der amerikanischen National Science Foundation Recherche im Bereich Fairness bei künstlicher Intelligenz; und Facebook hat an der Technischen Universität München in ein Institut für KI-Ethiker investiert.

Die Grundsätze, auf die sich die zahlreichen Ethik-Bestrebungen beziehen, unterscheiden sich dabei kaum. Meist ist von Transparenz, Nichtdiskriminierung, Rechenschaftspflicht und Sicherheit die Rede. Laut der Organisation AlgorithmWatch, die die Auswirkung von Algorithmen auf die Gesellschaft kritisch untersucht, gehen die wenigsten der Initiativen aber über eine erklärte Selbstverpflichtung hinaus. Nur bei drei oder vier der von der Organisation verzeichneten Fälle gebe es Hinweise darauf, dass auch ein Aufsichts- oder Durchsetzungsmechanismus zur Anwendung komme.
Gefahr des «ethics washing»

Häufig handle es sich bei dem Vorgehen um nichts anderes als um sogenanntes «ethics washing», kritisiert Thomas Metzinger, Professor für theoretische Philosophie an der Universität Mainz. «Das bedeutet, dass die Industrie ethische Debatten organisiert und kultiviert, um sich Zeit zu kaufen – um die Öffentlichkeit abzulenken, um wirksame Regulation und echte Politikgestaltung zu unterbinden oder zumindest zu verschleppen», schrieb Metzinger im April in einem Gastbeitrag im deutschen «Tagesspiegel».

Der Universitätsprofessor war Mitglied jener Expertengruppe, die die ethischen Leitlinien für KI im Auftrag der EU-Kommission erarbeitet hat. Er hat dabei mit einer zweiten Person rote Linien definiert, also Prinzipien, die festlegen, was nicht mit KI gemacht werden darf. Dazu zählten sie unter anderem den Einsatz von tödlichen autonomen Waffensystemen oder die KI-gestützte Bewertung von Bürgern durch den Staat, die etwa bei Chinas anlaufendem Sozialkreditsystem zur Anwendung kommt.

Diese roten Linien seien auf Druck von Industrievertretern jedoch verwässert, andere gar aus der Endfassung der Leitlinien gelöscht worden, beklagte Metzinger nach der Ausarbeitung des Textes. Erst Handlungsempfehlungen an die Kommission, die von der Expertengruppe zwei Monate später veröffentlicht wurden, waren schärfer formuliert. «Die Frage bei Ethik-Initiativen ist immer, ob sie ernst gemeint sind oder ob es sich lediglich um eine Betrugs-Nummer vonseiten der Industrie handelt», sagt Metzinger im Gespräch.
Ein Gütesiegel für Ethik?

Dieser Frage wird sich auch die SDI stellen müssen. Um es nicht bei blossen Willensbekundungen der Unternehmen zu belassen, erwägen die Träger der Initiative den Einsatz eines Label-Systems. Dieses könnte jene Firmen zertifizieren, die sich an gewisse Standards halten. Wie wirksam solch ein freiwilliges Label sein kann, muss sich allerdings erst zeigen. Ungewiss ist zudem, ob sich dieses international als Qualitätsmerkmal durchsetzen könnte. Die SDI steht mit der Idee auch nicht alleine da: Die Fraunhofer-Gesellschaft etwa arbeitet ebenfalls an einer KI-Zertifizierung.

Grossen Einfluss erhoffen sich die Initiatoren der SDI unter anderem durch den Stiftungsstandort Genf, mit seinen internationalen Organisationen. «Die Stadt bietet das optimale Umfeld, um rasch mit internationalen Akteuren in Kontakt zu treten», sagt Valentin Zellweger, der als Schweizer Botschafter bei der Uno vor Ort ist und die Initiatoren bei der Lancierung unterstützt hat. Umgekehrt könne die Stadt auch von der SDI profitieren, denkt Daniel Stauffacher, ehemaliger Schweizer Botschafter, Präsident der Stiftung ICT4Peace und Gründer des Zurich Hub for Ethics and Technology, der sich mit Chancen und Risiken der neuen Technologien befasst: «Dies ist ein wichtiger Schritt dafür, dass Genf für die Gouvernanz-Bildung in der neuen digitalisierten Welt mit an der Spitze bleibt.»

Noch bleiben jedenfalls viele Fragen offen. Etwa, wie kontrolliert würde, dass sich Unternehmen an von einem Label vorgegebene Kriterien halten, oder ob die Zertifizierung auch wieder entzogen werden könnte. «Letztlich hängt viel davon ab, wer ein solches Label herausgeben würde», sagt Cornelia Diethelm, Gründerin des Centre for Digital Responsibility, eines Think-Tanks, der sich mit Fragen der Ethik im digitalen Bereich befasst. Sie hält es für die Glaubwürdigkeit der neuen Initiative für wichtig, dass auch die Zivilgesellschaft ins Boot geholt werde. Laut den Initiatoren ist das geplant.";https://www.nzz.ch/schweiz/eine-schweizer-initiative-will-der-digitalen-welt-ein-gewissen-verpassen-ld.1508955;NZZ;Judith Kormann;;;
09.11.2020;Die Nachricht von 90 Prozent Wirksamkeit eines Corona-Impfstoffs gibt Anlass zur Hoffnung – trotzdem sind noch Fragen offen;"Das Rennen um den ersten ordentlich zugelassenen Impfstoff gegen das neue Coronavirus, Sars-CoV-2, biegt in die Zielgerade ein. In einer Pressemeldung haben das amerikanische Unternehmen Pfizer und Biontech aus dem deutschen Mainz am Montag erstmals Wirksamkeitsdaten aus einer klinischen Zulassungsstudie der Phase 3 mitgeteilt. Laut der Meldung erreichte ihr Impfstoffkandidat BNT162b2 bei einer Zwischenanalyse eine Wirksamkeit von über 90 Prozent.

Die internationale Untersuchung mit der mRNA-basierten Vakzine – dabei handelt es sich um einen genetischen Impfstoff, der auf Ribonukleinsäure (RNA) beruht – startete Ende Juli und rekrutierte bis heute mehr als 43 000 Probanden. Die Mehrheit von ihnen bekam im Abstand von drei Wochen zwei Dosen des Impfstoffs, ein kleinerer Teil eine Placebo-Impfung. Weder die Probanden noch die Ärzte wussten, bei wem der richtige Impfstoff eingesetzt wurde. Diese «Verblindung» der Angaben sollte erst nach 94 bestätigten Fällen von Infektionen mit Sars-CoV-2 bei den betroffenen Probanden aufgehoben werden, so sah es das Studienprotokoll vor.
Schutzwirkung nach einem Monat etabliert

Die Analyse der «entblindeten» Daten zeigt nun laut Firmenangaben, dass die untersuchte Vakzine neun von zehn geimpften Personen Schutz vor Covid-19 bot. Diese Schutzwirkung war bereits eine Woche nach der zweiten Impfdosis erreicht – also einen Monat nach Impfbeginn. Neben den Informationen zur Wirksamkeit hat das externe und firmenunabhängige Data Monitoring Committee in seiner Interimsanalyse auch die Daten zur Sicherheit der Vakzine angeschaut. Bisher seien keine ernsthaften Sicherheitsprobleme beobachtet worden, heisst es dazu in der Firmenmitteilung für Investoren und Medien.

Die Phase-3-Studie soll auch nach der erfolgreichen Zwischenanalyse weitergeführt werden. Damit wollen die Firmen noch mehr Daten zu Wirksamkeit und Sicherheit der Impfung sammeln. Auch sollen damit noch weitere Fragen beantwortet werden: Schützt die Impfung auch vor einer zweiten Ansteckung mit dem Virus? Und verhindert sie schwere Krankheitsverläufe?

Sobald bei den Probanden 164 bestätigte Corona-Infektionen aufgetreten sind, soll die endgültige Analyse stattfinden. Diese soll dann in einer Fachzeitschrift publiziert werden. Zudem wollen Pfizer und Biontech, sobald die vorgeschriebenen Meilensteine bezüglich Sicherheit in der Studie erreicht sind, bei der amerikanischen Arzneimittelbehörde FDA eine notfallmässige Zulassung beantragen. Das sollte laut den Firmen bereits in der nächsten Woche der Fall sein.

Auch was die Produktion und Verteilung des Impfstoffs anbelangt, geben sich Pfizer und Biontech zuversichtlich und ehrgeizig. Noch in diesem Jahr könnten weltweit 50 Millionen Impfdosen ausgeliefert werden; und 2021 lasse sich die produzierte Menge auf bis zu 1,3 Milliarden Dosen steigern.
Auch unabhängige Wissenschafter sind begeistert

«Heute ist ein guter Tag für die Wissenschaft und die Menschheit», wird Albert Bourla, Verwaltungsratspräsident und CEO von Pfizer, in der Medienmitteilung zitiert. Die präsentierten Zwischenergebnisse seien ein wichtiger Schritt auf dem Weg, die globale Gesundheitskrise zu beenden.

Die Interimsanalyse zeige, dass eine Schutzimpfung Covid-19 verhindern könne, sagt Ugur Sahin, Mitgründer und CEO von Biontech laut der Mitteilung. «Das ist ein Sieg für Innovation, Wissenschaft und globale Zusammenarbeit.»

Ähnlich sehen es offenbar viele Wissenschafter. Entgegen ihrer sonst eher nüchternen Art benützen auch sie in ihren Einschätzungen viele Superlative. Von «grossartigen und vielversprechenden Daten» ist ebenso die Rede wie von «phantastischen» und «hervorragenden» Resultaten. Für einen Forscher ist die Information von Pfizer und Biontech die beste Nachricht, die er seit Beginn der Pandemie gehört habe. Das werde unseren Umgang mit der Pandemie entscheidend beeinflussen. Viele erwarten eine baldige Zulassung des mRNA-Impfstoffs.

So auch der Münchner Infektiologe und Covid-19-Forscher Clemens Wendtner. Er beschreibt die kommunizierten Wirksamkeitsdaten als «Silberstreifen» an dem sonst so düsteren Corona-Horizont. In einem Statement gegenüber dem deutschen Science Media Center (SMC) bezeichnet er die 90 Prozent Wirksamkeit als umso bemerkenswerter, als viele der laufenden Impfstudien zu Covid-19 nur eine Erfolgsquote von 50 Prozent voraussetzten.
Kein Einblick in die Primärdaten

Etwas kritischer beurteilt Marylyn Addo, Tropenmedizinerin am Universitätsklinikum Eppendorf in Hamburg, die präsentierten Zwischenergebnisse: «Das sind interessante erste Signale», sagt sie gegenüber dem SMC. Sie bemängelt, dass die Resultate erst in einer Pressemitteilung kommuniziert worden seien und keine Primärdaten zur Verfügung ständen. Addo würde auch gerne wissen, wie gut die Schutzwirkung der Impfung in den verschiedenen Altersgruppen ausgefallen ist.

Für alle Fachleute ist klar: Bei den jüngsten Studienresultaten handelt es sich um vorläufige Wirksamkeitsdaten. Diese können sich nach einer längeren Beobachtungszeit der Probanden noch ändern. Es ist deshalb wichtig, wie Wendtner betont, dass auch nach einer möglichen Zulassung des Impfstoffs die Langzeitwirkungen und die Langzeitnebenwirkungen auf dem Radarschirm bleiben.";https://www.nzz.ch/wissenschaft/corona-impfstoff-erstmals-wirksamkeitsdaten-praesentiert-ld.1586125;NZZ;Alan Niederer;;;
10.03.2020;Der französische Starökonom Thomas Piketty will den Reichen an den Kragen ;"Die Ökonomie gebiert in aller Regel keine Superstars. Was die als «trostlose Wissenschaft» (Dis­mal Science) verschriene Disziplin an Erkenntnissen offeriert, sorgt selten für lebhafte Debatten über den akademischen Zirkel hinaus. Bei Thomas Piketty liegen die Dinge anders. Seit der französische Ökonom im Jahr 2013 sein Werk «Das Kapital im 21. Jahrhundert» veröffentlicht hat, ist sein Name auch Personen geläufig, die wenig mit wirtschaftswissenschaftlicher Forschung am Hut haben. Gern und oft zitiert wird Piketty vor allem von sozialistischen Politikern, die mit dem Ver­weis auf den Franzosen allerlei Ausbauten des Sozial- und Steuerstaates legitimieren. Wider die Ungleichheit

Das grosse Thema des 48-Jährigen ist nämlich die Ungleichheit. Als Haupttreiber dieser Ungleichheit sieht Piketty den Umstand, dass die Rendite des Kapitals im Kapitalismus meist höher liege als das Wachstum der Wirtschaft. Diese Entwicklung (mit der Formel r>g auf den Punkt gebracht) gefährde nicht nur den sozialen Frieden, sondern auch die Demokratie. Als Rezept ge­gen die unheilvolle Kraft empfiehlt der Franzose drastische Mass­nahmen. So soll eine Vermögenssteuer von bis zu 90% dafür sorgen, dass der Anteil der Reichen am Kuchen nicht zu gross wird. Damit käme man dem marxistischen Traum einer Überwindung des Privateigentums schon ziemlich nahe.

Am Mittwoch hat Piketty in Zürich die deutsche Übersetzung seines neuen Buchs präsentiert. «Kapital und Ideologie» nennt sich das voluminöse Werk. Es wartet erneut mit einer scharfen Kritik am real existierenden Wirtschaftssystem auf. Und erneut untermauert der Erfolgsautor seine Thesen mit einer Fülle an Daten aus aller Welt, wobei er einen weiten wirtschaftshistorischen Bo­gen spannt. Dem geplanten Vortrag an der Universität Zürich kann das Publikum aufgrund des Coronavirus zwar nur virtuell per Live-Stream beiwohnen. Einigen Medienvertretern stand Piketty auf Einladung des Schweizerischen Instituts für Auslandforschung (SIAF) dennoch Rede und Antwort.

Was dabei auffiel: Dieser Mann hat eine unbändige Energie. In atemberaubendem Tempo präsentiert er seine Kernaussagen («Ungleichheit ist kein Naturgesetz»), macht Exkurse zum Wahlkampf in den USA («Bernie Sanders und Elizabeth Warren sind keine Extremisten»), fordert massive Staatseingriffe («un­regulierter Kapitalismus führt zu Vermögenskonzentration»), kritisiert Thatcher und Reagan («ein grenzen­loser Glaube an die Selbstregulierung der Märkte») und distanziert sich von seinem Seelenverwandten Marx («ich glaube nicht an den Klassenkampf als Motor der Geschichte»). Nicht immer kann man den raschen Gedankensprüngen folgen, doch jeder Exkurs ist garniert mit diversen Daten und Statistiken.
Der Big-Data-Ökonom

So rasant der Big-Data-Ökonom sein Weltbild skizziert, so schnell verschwindet er wieder, um sich hinter verschlossener Tür mit einer Delegation der Sozialdemokratischen Partei des Kantons Zürich zu treffen. Für Sozialdemokraten ist Piketty so etwas wie ein akademischer Kronzeuge, wenn es darum geht, die Steuerschraube an­zu­ziehen und den Staat auszubauen. Doch der Ökonom und Aktivist hat in seinem Buch nicht nur lobende Worte für die Sozialdemokratie. Letztere sei längst nicht mehr die Partei der sozial benachteiligten Schichten, sondern eine Vertretung von Hochgebildeten und Privilegierten. Gerne hätte man zugehört, wie der Gast diese Botschaft den Parteivertretern präsentierte.

Doch Piketty sieht sich nicht als Pessimist. Er hat auch Positives zu erzählen. Dazu gehört, dass sich die Ungleichheit im 20. Jahrhundert stark zurückgebildet hat. Die Welt ist heute deutlich egalitärer als vor hundert Jahren. Seit den 1980er und 1990er Jahren sei nun aber in fast allen Teilen der Welt wieder eine Umkehr zu beobachten. An der «neuen ultra-inegalitären Erzählung» sei zwar das Desaster des Kommunismus nicht unschuldig. Die historische Analyse zeige aber, dass es stets der Kampf für Gleichheit und Bildung gewesen sei, der die Wirtschaftsentwicklung und den menschlichen Fortschritt möglich gemacht habe, nicht aber die «Heiligsprechung von Eigentum, Stabilität und Ungleichheit».

Piketty versteht es, seine Anliegen in provokantem Stil zu formulieren. Damit allein ist seine Popularität aber kaum zu erklären. Weit wichtiger ist, dass er mit dem Thema der Ungleichheit zur richtigen Zeit auf das richtige Thema gesetzt hat. Niemand kann heute noch ernsthaft bestreiten, dass die Globalisierung neben vielen Gewinnern – vor allem in aufstreben­den Ländern wie China und Indien – auch viele Verlierer mit sich gebracht hat. Zu Letzteren gehört in vielen westlichen Industriestaaten der Mittelstand, der einkommensmässig kaum mehr vom Fleck gekommen ist. Der heruntergewirtschaftete Rostgürtel in den USA steht exemplarisch für diese Entwicklung.
Manna vom Himmel

Aber auch die Finanzkrise war dem Support für das real existierende Wirtschaftssystem nicht eben förderlich. Wenn Bankmanager in der Krise die Verluste dem Staat überlassen und danach die Gewinne mit­tels unanständig hoher Saläre wieder in die eigene Tasche stecken, fallen Forderungen nach höheren Steuern für Vermögende auf fruchtbaren Boden. Zumal Piketty auch gleich eine Verwendung vorschlägt: So sollen die Erlöse aus der Vermögenssteuer genutzt werden, um jedem jungen Bürger eine grosszügige Starthilfe zu finanzieren. Überschlagsmässig erhielte jeder 25-Jährige rund 120 000 € ausbezahlt, wie Manna vom Himmel.

Dass solche Ideen manchen Sozialromantiker zum Träumen bringen, versteht sich von selbst. Durchdacht ist das Ganze aber nicht. Was Piketty nämlich nur am Rande be­achtet, sind die Anreizwirkungen, die mit einer solchen Umverteilung einhergehen. Wenn die Frucht der eigenen Arbeit und das Vermögen vom Staat fast vollständig eingezogen werden, wird man­cher Unternehmer seine Anstrengungen und Investitionen stark zurückfahren. Das Resultat wäre, dass es am Schluss gar nichts mehr zu verteilen gäbe, schon gar keine 120 000 € pro Bürger. Doch über technische Einzelheiten der Umsetzung redet Piketty nicht gern. Sein Eifer gilt dem kühnen Wurf – und die Leser lieben ihn dafür.";https://www.nzz.ch/wirtschaft/der-franzoesische-staroekonom-thomas-piketty-will-den-reichen-an-den-kragen-ld.1545829;NZZ;Thomas Fuster;;;
21.11.2019;«Facebook ist phantastisch für die Demokratie», sagt Stanford-Professor Michal Kosinski;"Michal Kosinski könnte Donald Trump zum Sieg verholfen haben. Nur indirekt, aber mit dem Vorwurf muss er seit Trumps Wahlsieg im Jahr 2016 leben. Denn Kosinski ist ein Psychologe, der mithilfe des Analysierens von Daten in das Innere unserer Herzen und Köpfe vordringt.

Die ganze Geschichte beginnt Mitte der 2010er Jahre, als Kosinski am renommierten Zentrum für Psychometrie an der Universität Cambridge arbeitet. Er findet heraus, dass Algorithmen mit nur wenigen auf Facebook gegebenen Likes ein umfassendes Persönlichkeitsprofil des Nutzers erstellen können.

Diese Methode wendet später die Wahlkampffirma Cambridge Analytica für Trump in abgewandelter Form an. Sie nutzt für die Kampagne widerrechtlich über Facebook gewonnene persönliche Daten. Das kommt im Rahmen der Cambridge-Analytica-Affäre im Frühjahr letzten Jahres an die Öffentlichkeit. Die Empörung über Facebooks legeren Umgang mit Nutzerdaten ist gross. Kosinski wird unfreiwillig in den Strudel der Ereignisse mit hineingerissen. Heute lehrt und forscht der gebürtige Pole an der Universität Stanford.

Herr Kosinski, Sie fordern, die Privatsphäre aufzugeben. Was bedeutet Privatsphäre eigentlich für uns Menschen?

Für den grössten Teil der Menschheitsgeschichte gab es so etwas wie Privatsphäre nicht. Wir lebten in kleinen Gruppen in der Savanne, und es gab keine Rückzugsmöglichkeiten. Alles, was in der Gemeinschaft geschah, spielte sich vor den Augen aller ab. So ähnlich war – und ist noch heute – das Leben auf dem Land. Erst das Leben in der Stadt hat uns Privatsphäre ermöglicht. Das hat seine Vorteile im Vergleich zum Leben in der Gruppe in der Savanne. Wenn wir zum Beispiel Ansichten vertreten, die von einer Gruppe nicht geteilt werden, wären wir in der Savanne möglicherweise ausgegrenzt worden. Das Stadtleben hingegen bietet den Schutz der Anonymität und damit enorme Freiheiten.

Warum sollten wir diese Freiheiten aufgeben?

Zum einen müssen wir aufhören, diese Thematik schwarz-weiss zu betrachten. So ist das Individuum in der Dorfgemeinschaft zwar weniger frei, aber die Gemeinschaft wird zusammengehalten durch eine Art sozialen Klebstoff: Inakzeptables Fehlverhalten kann relativ schnell erkannt und geahndet werden. Im Stadtleben ist dieser soziale Klebstoff weniger vorhanden, und Menschen neigen mehr zu Fehlverhalten wie Diebstahl. Dafür haben sie aber auch mehr Freiheiten. Es ist immer ein Trade-off. Zum andern aber müssen wir uns bewusst werden, dass es keinen Weg mehr zurück gibt. Das realisiert man erst, wenn man versteht, wie moderne Technologie funktioniert und welche Vorteile Big Data und Algorithmen uns bieten.

Warum gibt es keinen Weg mehr zurück?

Die aufkommende Landwirtschaft beispielsweise hatte für die Gesellschaft nebst Vorteilen auch sehr hohe Kosten. So konnten sich die Menschen nun ausreichend ernähren, die Bevölkerung wuchs. Auf der anderen Seite führte die Sesshaftigkeit dazu, dass es nun verstärkt Kriege gab: Die Bauern konnten den Feinden ja nicht entfliehen, sonst hätten sie ihr Land verlassen müssen. Auch waren sie schlechtem Wetter ausgesetzt, dem sie ja nicht ausweichen konnten. Und so ist und war es mit vielen neuen Technologien: Zunächst bringen sie uns Nachteile. Aber es gibt keinen Weg zurück. Die sesshafte Landbevölkerung konnte nicht mehr zu Nomaden werden, dazu war sie viel zu gross geworden.

Heisst das, dass wir erst durch eine lange Periode der Misere gehen müssen, bevor sich neue Technologien wie Big Data und KI auszahlen?

Ja, wir werden die Schattenseiten der Technologien zu spüren bekommen. Aber es gibt keinen Weg zurück, auch nicht für Europa. Was, meinen Sie, wird in China, Russland oder in den USA passieren? Dieser Fehler ist schon einmal in Europa begangen worden, und das ist der Grund, warum es Firmen wie Google oder Facebook eben nicht in Europa, sondern in den USA gibt. Und da die USA in Regulierungsfragen Europa folgen wollen, werden die Firmen bald nach China ziehen beziehungsweise dort gegründet werden. Wenn wir nicht aufpassen, werden die Amerikaner und die Europäer bald chinesische Tech-Dienstleistungen nutzen – weil die dank den generierten Datenmengen einfach besser sein werden. Amerika und Europa drohen zu den Verlierern der Digitalisierung zu werden, weil sie etwas versuchen zu schützen, was man nicht mehr schützen kann: die Privatsphäre.

Was bekämen wir dafür, wenn wir unsere Privatsphäre aufgäben?

Die neuen Dienstleistungen und Produkte, die auf dem Sammeln von Daten basieren, bereichern unser Leben enorm. Natürlich ist es unheimlich, wenn unsere genetischen Daten gesammelt werden, aber auf der anderen Seiten können so Krankheiten leichter erkannt und behandelt werden. Anstatt das Unausweichliche zu bekämpfen, sollten wir es gestalten.

Wie sollen wir eine Gesellschaft gestalten, in der uns unsere Daten nicht mehr gehören?

Das Problem ist eben nicht, dass unsere persönlichen Daten nicht mehr in Datenbanken versteckt sind, sondern was mit ihnen gemacht werden kann. Versicherungskonzerne könnten DNA-Daten nutzen, um Kunden mit genetischen Defekten vom Versicherungsschutz auszuschliessen. Wenn nun aber die Branche so reguliert wäre, dass die Versicherung verpflichtet wäre, alle Kunden zu versichern, dann wäre es im Interesse der Versicherung, die Interessen des Kunden zu vertreten und dem Kunden zu helfen, mit dem Gesundheitsrisiko bestmöglich umzugehen. Wie viele Menschenleben könnte man retten, wie viele Kinder schützen, wenn man Zugang hätte zu den Suchanfragen auf Google: Wenn dort jemand nach Methoden sucht, um sich umzubringen, oder ein Kind fragt, ob es normal sei, dass es von einem Erwachsenen an den Genitalien angefasst worden sei, dann könnte die Suizid- und Missbrauchsprävention einschreiten. Aber das gäbe natürlich einen riesigen Aufschrei der Datenschützer.

Wir würden uns schnell in Richtung Überwachungsstaat bewegen.

Es herrscht in der Diskussion eine grosse Scheinheiligkeit. Wir dürfen solche Daten nicht nutzen, um Leben zu retten. Aber Facebook und Google dürfen die Daten nutzen, um uns allen möglichen Mist zu verkaufen.

Lassen Sie uns über die Präsidentenwahlen in den USA im kommenden Jahr sprechen. Befürchten Sie, dass die Wähler über die sozialen Netzwerke von den Kandidaten, anderen Politikern oder gar dunklen Kräften aus dem Ausland manipuliert werden?

Nein, die sozialen Netzwerke sind phantastisch für die Demokratie. Diese Angst, dass etwa Facebook zum Werkzeug der Massenmanipulation wird, ist ein weiteres Beispiel dafür, dass sich heute viele über Probleme aufregen, die gar keine sind, und die Leute die wirklichen Probleme ausser acht lassen. Nehmen wir Fake-News. Natürlich gibt es Falschnachrichten. Es gibt davon sehr viel mehr als früher, weil jeder im Internet diese zum Besten geben und sie etwa über die sozialen Netzwerke hundert- oder millionenfach verbreiten kann. Die Frage ist doch aber, ob ein Bürger im Durchschnitt heute mehr diesen Falschnachrichten glaubt, als er es früher getan hätte. Die Menschen sind heute so gut informiert wie noch nie zuvor. Natürlich gibt es immer auch einmal einen Rückschlag, wie es die Anti-Impf-Kampagne gezeigt hat; aber der Trend ist eindeutig.

Dann gehen Sie davon aus, dass es ebenfalls ein Mythos ist, dass Menschen über das Internet und Plattformen wie Facebook oder Twitter radikalisiert werden?

Ja, weltweite Umfragen über Werte und Ansichten zeigen, dass die Menschen generell in diesem Bereich in eine Richtung gehen: Immer mehr Menschen setzen sich für Menschenrechte ein, für Demokratie oder für die Rechte von Homosexuellen. Und das auch, dank den im Internet vorhandenen Informationen. Das ist ein enormer Fortschritt.

Wenn man nach Amerika schaut oder auf den Populismus, dann deutet das doch auf eine gewisse Spaltung hin.

Dann schauen Sie einmal fünfzig Jahre zurück. Da hatten wir in Europa noch den Eisernen Vorhang. Das war eine echte Spaltung. Oder blicken wir nach Amerika. Es ist noch nicht lange her, dass es in dem Land zwei Fraktionen gab, von denen die eine der Minderheit der Afroamerikaner die vollen Bürgerrechte vorenthalten wollte. Diese fundamentale Spaltung ist überwunden. Heute diskutiert Amerika darüber, ob die Gesundheitsversorgung 80 oder 100 Prozent der Bürger abdecken soll. Das sind doch Nuancen. Natürlich wird heute sehr laut gestritten. Aber das kommt daher, dass der Zugang zu Kommunikationskanälen durch die sozialen Netzwerke und das Internet im Allgemeinen demokratisiert ist.

In Ihrer Forschung haben Sie gezeigt, dass es nur wenig Datenpunkte braucht, um wesentliche Merkmale einer Persönlichkeit herauszufinden. Wie gross schätzen Sie die Gefahr ein, dass diese Erkenntnisse im nächsten amerikanischen Wahlkampf missbraucht werden und die Kandidaten oder ausländische Akteure mit auf das Individuum abgestimmten Falschinformationen eine Art Gehirnwäsche vornehmen?

Das ist doch lächerlich. Es ist eine gute Sache, wenn sich die Politiker an jeden einzelnen Wähler mit einer individuellen Botschaft wenden können. Stellen Sie sich vor, ein Politiker hätte die Möglichkeit, im Wahlkampf an fast jede Haustüre zu klopfen und mit dem Bürger ein Gespräch zu beginnen? Das ist im realen Leben nicht möglich, aber auf Facebook und anderen sozialen Netzwerken sehr wohl. Die sozialen Netzwerke sind gut für die Demokratie.

Was sind denn die tatsächlichen Risiken, die wir in der Diskussion Ihrer Meinung nach übersehen?

Wir ignorieren die Gefahr, dass Algorithmen unsere Demokratien vollständig umkrempeln werden. Stellen wir uns einmal zwei Politiker vor. Der eine bedient sich der Algorithmen, um seine Wahlbotschaft zu übermitteln, der andere nicht. Ersterer wird mit höherer Wahrscheinlichkeit die Wahlen gewinnen. Der Wahlsieger wird sich wieder der Algorithmen bedienen, um seine politischen Anliegen durchzubringen. Und damit wird er erfolgreicher sein als ein Amtsinhaber, der Algorithmen nicht nutzt. Die Algorithmen werden dann aufgrund der Daten immer besser. Und das führt eines Tages dazu, dass der Politiker nicht mehr der Akteur, sondern nur noch die Schnittstelle ist für einen Algorithmus, der eigenständig handelt und entscheidet. Gleiches könnte für Konzernchefs, Generäle oder Ärzte gelten: Sie alle geben ihre Entscheidungsbefugnis freiwillig ab. Das ist ein enormes Risiko.

Wohin führt das?

Erstens laufen wir Gefahr, als Gesellschaft von Algorithmen gesteuert und nicht von Menschen regiert oder geführt zu werden. Zweitens würden wir dabei Algorithmen folgen, die wir nicht mehr verständen. Unsere Gehirne sind für die Komplexität der Algorithmen nicht gemacht. Die Zeit wird kommen, in der die Algorithmen Entscheidungen treffen, die für uns als Homo sapiens sehr schlecht sind, und wir Menschen werden einfach die entsprechenden Befehle ausführen.

Das hört sich an wie Science-Fiction

Das ist zum Teil schon Realität. Wenn die Zentralbank Zinsentscheidungen fällt, dann basiert das auf Algorithmen, die Terabytes an Daten verarbeitet haben. Die Händler an der Wall Street stützen ihre Käufe und Verkäufe bereits zum überwiegenden Teil auf Algorithmen.

Sie selbst wenden als Psychiater auch Algorithmen an, um aus Daten auf die Persönlichkeit von Menschen zu schliessen. Vertrauen Sie diesen?

Es ist sehr einfach, mit nur wenigen Daten zu berechnen, ob jemand an Depressionen leidet. Aber wie die Algorithmen zu ihrem Schluss kommen, das durchschaue ich oft erst nach wochen- oder gar monatelangen Untersuchungen – obwohl es vergleichsweise einfache Algorithmen sind. Und nun stellen Sie sich einen Ingenieur bei Google vor. Der will einfach nur immer bessere Algorithmen machen. Den interessiert es wahrscheinlich nicht, wie der Algorithmus zu einer Entscheidung gelangt. Das ist sehr gefährlich.

Wie können wir das Risiko mindern, eines Tages von Algorithmen gesteuert zu werden?

Zunächst müssen wir dieses Risiko überhaupt erst einmal erkennen, und aufhören, uns wegen Fake-News oder Radikalisierung die Köpfe rot zu reden. Und dann müssen wir einsehen, dass es keinen Weg zurück gibt. Es gibt immer mehr Daten, die Algorithmen werden immer besser, und wir werden sie immer stärker nutzen. Der Fortschritt lässt sich nicht aufhalten. Schliesslich müssen wir fordern, dass künstliche Intelligenz transparent und nachvollziehbar ist. ";https://www.nzz.ch/feuilleton/michal-kosinski-facebook-ist-phantastisch-fuer-die-demokratie-ld.1520699;NZZ;Christiane Hanna Henkel;;;
28.11.2020;Neues aus der Wissenschaft: Warum Delphine nicht an der Taucherkrankheit leiden und wie schnell die Schweizer Alpen wachsen;"Reduktion des Ozons hat Vögel gerettet

Manchmal nützen Vorschriften, die zum Wohl von Menschen gedacht sind, auch der Tierwelt. Die verbesserte Luftqualität, die in den USA dank einem Bundesprogramm zur Reduktion der Ozonbelastung zustande gekommen ist, hat wohl den Verlust von 1,5 Milliarden Vögeln in den letzten 40 Jahren abgewendet. Das berichten Forscher, die Veränderungen der Vogelhäufigkeit, der Luftqualität und des gesetzlichen Status für Tausende US-Bezirke verfolgt haben («PNAS»).

Während Ozon in der oberen Atmosphäre vor UV-Strahlen schützt, schadet es in Bodennähe der Gesundheit. Das unter anderem von Autos produzierte Gas gefährdet vor allem kleine Zugvögel wie Spatzen oder Finken, indem es ihre Atemwege angreift und Insekten zum Verschwinden bringt. So umgehen Delphine die Taucherkrankheit

Delphine sind intelligente Tiere, elegante Schwimmer - und gewiefte Taucher. Letzteres haben jetzt spanische Forscher in einer Studie mit Grossen Tümmlern (Tursiops truncatus) eindrücklich demonstriert, jener Delphinart, die in allen Ozeanen verbreitet ist und häufig auch in Delphinarien gehalten wird.

In Trainings haben die Zoologen den Tieren beigebracht, auf Kommando die Luft unterschiedlich lange anzuhalten. Die Delphine scheinen menschlichen Apnoe-Tauchern kaum nachzustehen. Sollten sie möglichst lange Zeit ohne Luft auskommen, verringerten sie unmittelbar davor aktiv die Rate ihres Herzschlags. Ausserdem hielten sie diese länger und ausgeprägter tief, als wenn sie die Luft nur kurze Zeit anzuhalten hatten («Frontiers in Physiology»).

So konnten die Tiere den Verbrauch von Sauerstoff minimieren. Diese Fähigkeit dient den Delphinen auch zur Vorbeugung der Dekompressionskrankheit, die unter Tauchern besonders gefürchtet ist. Dazu kommt es, wenn der unter hohem Wasserdruck im Blut gelöste Stickstoff beim zu schnellen Auftauchen Bläschen bildet und das Gewebe schädigt. Der grösste Katalog aller Pflanzen

Nichts ist mühsamer für einen Botaniker, als wenn er sich mit der allzu oft unklaren Nomenklatur von Pflanzen herumschlagen muss. Künftig dürfte seine Arbeit einfacher werden - dank dem neuen Leipziger Katalog der Gefässpflanzen (LCVP). In diesem ausserordentlichen Kompendium haben Fachleute des Botanischen Gartens Leipzig die nach eigenen Angaben «grösste und kompletteste Liste von wissenschaftlichen Namen aller Pflanzenarten der Welt» zusammengetragen («Scientific Data»).

Zu den Gefässpflanzen gehören die Bärlapppflanzen, die Farne und die Samenpflanzen. Der Leipziger Katalog umfasst volle 1 315 562 wissenschaftliche Namen von 351 180 Gefässpflanzen und 6160 natürlichen Hybriden. Die deutschen Experten mussten Tausende von Studien durchackern, um unterschiedliche Schreibweisen oder Synonyme für die gleiche Art zu klären.
Das Gehirn lässt sich nicht täuschen

Das menschliche Gehirn ist viel flexibler, als man denkt. Das zeigt sich einmal mehr in der Leistung des Gedächtnisses. An sich müsste man meinen, dass Dinge, die der Mensch schon Hunderte Male erlebt hat, jeweils die genau gleichen Erinnerungen wachruft. Doch das Gegenteil ist der Fall, wie Neuropsychologen in Experimenten festgestellt haben. Das menschliche Gehirn lässt sich nicht täuschen: Je öfter Menschen in einem Video mit einer bestimmten Handlung konfrontiert worden sind, desto eher und sicherer konnten sie kleinste Abweichungen erkennen («PNAS»).
Die Schweizer Alpen wachsen stets weiter

Die Österreicher haben mehr Pistengaudi in ihren Skigebieten, die Schweizer dafür die höheren Berge. Und das dürfte noch ziemlich lange so bleiben. Geologen der Universität Bern haben in einer zehn Jahre langen Forschungsarbeit festgestellt, dass die Alpen in der Schweiz stets weiterwachsen. Das Wachstum kommt dadurch zustande, dass in den Zentralalpen die Hebung der Berge - verursacht durch verschiedene Kräfte im Erdinnern - schneller erfolgt als die Abtragung von Gesteinsmaterial durch Erosionsprozesse («Earth Science Reviews»).

Pro tausend Jahre heben sich die Zentralalpen um ungefähr 800 Millimeter. In geologischen Dimensionen betrachtet, ist das ein wirklich beachtliches Tempo. Das dürfte allerdings bei weitem nicht ausreichen, um die Auswirkungen der Erderwärmung mit einem Wachstum nach oben, in kühlere Gefilde, zu kompensieren. In den Westalpen befinden sich laut den Berner Forschern Abtragung und Hebung einigermassen im Gleichgewicht, in den Ostalpen erfolgt die Abtragung sogar schneller als die Hebung.
Erhöhtes Depressionsrisiko bei extremen Frühchen

Mädchen, die viel zu früh geboren werden, haben eine erhöhtes Risiko, im Laufe der Kindheit und Adoleszenz an Depressionen zu erkranken. Dies zeigt eine finnische Studie, die 38 000 Personen mit Depression und 149 000 gesunde Kontrollpersonen miteinander verglichen hat. Mädchen, die vor der 28. Schwangerschaftswoche zur Welt kamen, wiesen ein dreimal so hohes Risiko für eine Depression auf («Journal of the American Academy for Child & Adolescent Psychiatry»). Bei Knaben und bei Kindern, die nach der 28. Woche geboren wurden, zeigte sich kein solcher Zusammenhang.";https://nzzas.nzz.ch/wissen/warum-delphine-nicht-an-der-taucherkrankheit-leiden-ld.1589225;NZZ;Patrick Imhasly, Theres Lüthi, Martin Amrein;;;
03.07.2020;Ein Datenschatz in der Medizin liegt brach;"Barbara Biedermann ist Hausärztin in Adetswil bei Zürich und hat eine Vision. Sie will die Medizin besser machen. Nicht mit einer neuen Therapie, sondern indem sie die klassische Patientenuntersuchung mit moderner IT kombiniert. «Damit sollten sich die Qualität der Diagnosen verbessern und viele unnötige Untersuchungen verhindern lassen», sagt Biedermann. Der Ansatz habe also auch das Potenzial, die Medizin günstiger zu machen.

Was nach der Quadratur des Kreises klingt, ist ein Computerprogramm namens Cobedias. Das Akronym steht für «Comprehensive Bedside Diagnosis», zu Deutsch: umfassende Diagnose am Patientenbett. Die Software ist das Steckenpferd von Biedermann, die neben ihrer Praxis noch eine Titularprofessur an der Universität Basel innehat.
Alles begann mit einer Nationalfonds-Studie

Begonnen hat alles vor rund zwanzig Jahren im Bruderholzspital im Kanton Baselland. Im Rahmen eines vom Nationalfonds finanzierten Forschungsprojekts konnte Biedermann zeigen, dass der Algorithmus der Software bei einer Modellkrankheit (Arteriosklerose) funktioniert. Das heisst, mit den «am Krankenbett» gewonnenen Informationen liess sich beim Patienten zuverlässig die Krankheit bestimmen. Beflügelt von diesem Erfolg, entwickelte die Ärztin ihre Idee mit einer IT-Firma in ein marktreifes Produkt für alle Krankheiten weiter. So ist die Cobedias-Software entstanden. Sie ist seit 2014 verfügbar und wird ständig weiterentwickelt.
Barbara Biedermann: Hausärztin in Adetswil bei Zürich.
Barbara Biedermann: Hausärztin in Adetswil bei Zürich.
pd

Das System hilft Ärzten, ihre Patienten korrekt und vor allem vollständig zu untersuchen. Zudem werden die gewonnenen Informationen – oder Daten – in digitaler Form dokumentiert. So kann eine Datenbank mit klinischen Patienteninformationen aufgebaut werden. «Etwas, das es in dieser Form bis heute in der Schweiz nicht gibt», sagt Biedermann. Das ist erstaunlich, wird doch in der Medizin ständig von Big Data gesprochen. «Dabei geht es um Labordaten, Gentests oder Daten aus bildgebenden Untersuchungen», erklärt die Ärztin. «Nicht um klinische Patientendaten.»

Die klinischen Patientendaten stammen aus dem Gespräch, das der Arzt mit seinem Patienten führt (Anamnese), und der anschliessenden Ganzkörperuntersuchung (Status). In der Anamnese wird nicht nur das gegenwärtige Problem des Patienten besprochen. Es kommen auch frühere Krankheiten und Operationen, familiär gehäufte Störungen sowie Allergien, Suchtverhalten und regelmässig eingenommene Medikamente zur Sprache. Bei der Untersuchung des Körpers muss der Arzt all seine Sinne einsetzen, indem er zum Beispiel die durch Krankheiten veränderte Hautfarbe beurteilt (Inspektion), innere Organe abtastet (Palpation), Bauch und Brustraum abklopft (Perkussion), Herz und Lunge abhört (Auskultation) und verschiedene Funktionsprüfungen vornimmt.

Auf diese Weise kommen im Cobedias-System pro Patient rund 7000 Informationspunkte zusammen, wobei nur knapp 300 aktiv dokumentiert werden müssen (die anderen sind vorgegebene Normalbefunde). Die so entstandenen Datensätze lassen sich in anonymisierter und aggregierter Form auf die unterschiedlichsten medizinischen Fragestellungen hin auswerten. «Davon können Patienten, Ärzte und die Gesellschaft gleichermassen profitieren», sagt Biedermann.
Informationsbasis für ärztliche Entscheide

Dabei ist die systematische Patientenuntersuchung keine Erfindung der Adetswiler Hausärztin. Schon seit den Anfängen der modernen Medizin liefern die Anamnese und der Status den Ärzten die Basis, um beim Patienten über weiterführende diagnostische Abklärungen und seine Behandlung zu entscheiden. Dass diese Basis auch in Zeiten der hochtechnisierten Spitzenmedizin immer noch unabdingbar ist, zeigt sich darin, dass sie im Studium immer noch gelehrt wird. Zudem belegen Studien, dass erfahrene Hausärzte alleine mit Anamnese und Status bei 80 bis 90 Prozent ihrer Patienten die zugrunde liegende Krankheit diagnostizieren oder zumindest mit hoher Wahrscheinlichkeit vermuten können.

«Das hat damit zu tun, dass der menschliche Körper seit Jahrhunderten auf ähnliche Weise krank wird», erklärt Biedermann. Die für eine Krankheit typischen Veränderungen zu erkennen und richtig zu deuten, sei somit für die ärztliche Arbeit zentral. Biedermann spricht in diesem Zusammenhang vom klinischen Phänotyp einer Krankheit – dies im Gegensatz zur genetischen Grundlage vieler Störungen (Genotyp).

«Den klinischen Phänotyp möglichst rasch zu kennen, wäre auch bei einer neuen Krankheit wie Covid-19 sehr hilfreich», betont Biedermann. Dafür brauche es aber verlässliche Daten aus den Spitälern und den Arztpraxen. Gerade die Hausärzte seien oft die Ersten, die einen Patienten sähen. Weil die meisten Mediziner aber keine systematischen, sondern nur problemzentrierte Untersuchungen durchführen – im Fall von Fieber und Husten fokussiert man sich beispielsweise auf die Atemwege –, hat es laut Biedermann in der gegenwärtigen Pandemie relativ lange gedauert, bis man realisierte, dass viele Covid-19-Patienten an einer Riechstörung leiden.
Das Gesundheitssystem tickt anders

Der Ansatz der Adetswiler Hausärztin klingt so logisch und nachvollziehbar, dass man sich fragt, warum sich das Cobedias-System oder etwas Ähnliches nicht schon längst in der Medizin durchgesetzt hat. Tatsächlich wird die Software, die als Jahreslizenz für drei Ärzte 1620 Franken kostet, erst in zwei Arztpraxen eingesetzt. Für den fehlenden Erfolg sieht Biedermann zwei Hauptgründe. «Mit rund einer Stunde Zeitaufwand dauert die systematische Untersuchung und Dokumentation vielen Ärzten zu lange», sagt sie. Das habe auch damit zu tun, dass das Tarifsystem Tarmed den zeitlichen Aufwand für technische Untersuchungen wie Ultraschall oder Labortests deutlich besser honoriere (vgl. Kasten ganz unten). Ein weiteres Hindernis sei die schwierige Integration der Software in die bestehenden Praxis- und Spitalinformationssysteme. So habe eine Offerte für das Spital Wetzikon ergeben, dass die Anpassungen am Spitalinformationssystem dreimal so teuer wären wie das Cobedias-System. Das sei dem Spitalrat zu viel gewesen.

Müsste Biedermanns Idee einer Datenbank für klinische Patientendaten nicht bei der Schweizerischen Gesellschaft für allgemeine innere Medizin (SGAIM) und den universitären Instituten für Hausarztmedizin auf grosses Interesse stossen? Warum leistet niemand Schützenhilfe?

Der Internist und SGAIM-Co-Präsident Drahomir Aujesky findet Biedermanns Ansatz im Grundsatz richtig. «Ich bin überzeugt, wir könnten eine bessere Medizin machen, wenn wir mehr Zeit in das Gespräch und die klinische Untersuchung investieren würden», sagt der Chefarzt der Universitätsklinik für allgemeine innere Medizin am Inselspital Bern. Doch das Gesundheitssystem laufe dem zuwider. Neben der ungenügenden Vergütung für Anamnese und Status sieht der Arzt auch den enormen Zeitdruck und vor allem den zu geringen Stellenwert des ganzheitlichen klinischen Ansatzes in der Medizin als wichtige Hemmfaktoren.

In seiner eigenen Klinik versuche er mit entsprechenden fachlichen Weiterbildungen Gegensteuer zu geben, sagt Aujesky. Neben solchen lokalen Initiativen gebe es in der Schweiz aber keine übergeordneten Bemühungen zur Wiederbelebung von Anamnese und Status. «Das ist auch deshalb schade, weil die heutige fragmentierte Medizin viele Redundanzen und Fehlinformationen produziert», sagt der Medizin-Professor. Ein Big-Data-Ansatz ohne klinische Patientendaten sei deshalb zum Scheitern verurteilt. Andererseits dürfe man die klinische Untersuchung aber auch nicht romantisieren, warnt Aujesky. Viele klassische Tests seien nicht besonders aussagekräftig. Ihren Nutzen gelte es in Studien kritisch zu überprüfen. Dafür brauche es aber auch verlässliche klinische Patientendaten.

Aus eigener Erfahrung weiss Aujesky, dass die meisten Patienten eine ausführliche klinische Untersuchung schätzen. Wenn etwas in der Betreuung schieflaufe, höre er oft den Satz: «Doktor X hat mich nicht einmal richtig untersucht.» Das sei schade, sagt Aujesky. Denn die persönliche Zuwendung sei Teil des medizinischen Erfolgs. Damit werde das Vertrauensverhältnis zwischen Patient und Arzt gestärkt.
«Moderne Medizin hängt Patienten ab»

Dieser Ansicht ist auch Thomas Rosemann vom Institut für Hausarztmedizin der Universität Zürich. Die moderne Medizin hänge viele Patienten ab, sagt er. «Ein Grund ist, dass das Gespräch und die klassische Untersuchung in der Medizin an Bedeutung verlieren.» Dieser Trend habe aber auch rationale Gründe, betont der Hausarzt-Professor. Statt den Bauch lange mit dem Stethoskop abzuhören, könne eine kurze Ultraschalluntersuchung viel präzisere Informationen liefern. Auch die Auskultation des Herzens stosse rasch an Grenzen.

Von einer Software wie Cobedias würde Rosemann erwarten, dass sie den Arzt bei der Entscheidungsfindung unterstütze. Ein solches intelligentes System würde zum Beispiel beim Symptom Atemnot eine Warnlampe einschalten und dem Mediziner «sagen», mit welchen weiteren Fragen an den Patienten und mit welchen Untersuchungen er am besten einen Herzinfarkt, eine Lungenembolie oder anderes nachweisen oder ausschliessen könne.

Solche Hilfsmittel, die anhand von Punktwerten (Scores) die Wahrscheinlichkeit einer vermuteten Diagnose angeben, gibt es zwar schon. Meist liegen sie aber als einfache Fragebögen vor. «In die elektronische Krankengeschichte integriert, könnten sie dem Praktiker einen echten Mehrwert bieten», sagt Rosemann. Doch so weit sei man noch lange nicht. Das habe auch mit der Abneigung vieler Ärzte gegen verordnete Strukturen und Standardisierungsbemühungen zu tun. Eine gewisse Vereinheitlichung sei jedoch eine Voraussetzung, wenn man wie Barbara Bidermann klinische Daten sammeln wolle.
Kein Goldstandard für gute Medizin

«Für eine systematische und vollständige klinische Untersuchung fehlt uns schlicht die Zeit», sagt Felix Huber, Hausarzt und Mitgründer des Praxisnetzwerks Medix. Der erfahrene Mediziner bezweifelt zudem, dass Biedermanns Ansatz gegen medizinische Überversorgung hilft. Das Gegenteil könnte eintreffen, warnt er. Denn beim rigiden Abarbeiten von vorgeschriebenen Untersuchungen und Tests könnten auch viele Befunde ohne klinische Relevanz erhoben werden.

Gleichzeitig räumt der Medix-Pionier aber ein, dass die Arbeit in der Praxis immer eine Gratwanderung sei. «Wenn wir wie bei der problemzentrierten Untersuchung Abkürzungen nehmen, besteht die Gefahr, dass wir etwas verpassen.» Bevor man aber alle Hausärzte verpflichte, systematisch klinische Daten zu sammeln, müsse man den Ansatz in einer Studie mit freiwillig teilnehmenden Ärzten sauber evaluieren. «Ohne das fehlt der Beweis, dass sich damit eine bessere und kostengünstigere Medizin realisieren lässt.»

Die Frage nach dem Nutzen sei umso schwieriger zu beantworten, als es keinen anerkannten Goldstandard für gute Medizin gebe, erklärt Huber. «Macht der alte, erfahrene Arzt die beste Medizin?» Schliesslich wisse dieser oft schon anhand von ein paar Fragen, in welche Richtung es beim Patienten gehe. «Oder ist die junge Ärztin besser?» Ihr medizinisches Wissen sei meist riesig. Doch wegen der fehlenden Erfahrung veranlasse sie oft viele Zusatzuntersuchungen.

Der Hausarzt-Professor Rosemann spricht in diesem Zusammenhang von den zwei Seiten der Medizin. «Die eine Seite ist das wissenschaftliche Fundament», sagt er. Dafür wären mehr klinische Patientendaten sinnvoll. Auf der anderen Seite seien Ärzte aber auch Individualisten mit je eigenem Arbeits- und Kommunikationsstil. Und das sei gut so, sagt Rosemann. Denn diese Unterschiede seien wichtig, weil auch die Patienten verschiedene Vorlieben und Ansprüche an die Ärzte hätten.";https://www.nzz.ch/wissenschaft/patientendaten-ein-datenschatz-liegt-in-der-medizin-brach-ld.1563785;NZZ;Alan Niederer;;;
23.07.2019;Yuval Noah Harari: «Vor einer vergleichbaren Herausforderung hat die Menschheit noch nie gestanden»;"Herr Harari, wie man hört, soll aus Ihrem Weltbestseller ein Hollywoodfilm werden. Wenn Sie selber Regie führten, welches Genre würden Sie für Ihre «Kurze Geschichte der Menschheit» wählen – Drama, Komödie, Katastrophenfilm?

Wir arbeiten tatsächlich an diesem Projekt, und es ist ziemlich schwierig. Die ganze Menschheitsgeschichte in einem Film darzustellen, ohne bei einer Doku zu enden: Das ist eine knifflige Aufgabe. Zurzeit studieren wir an verschiedenen Formen herum. Den Katastrophenfilm und die Komödie kann ich aber definitiv ausschliessen! Im Moment sieht es am ehesten nach einem Drama aus, das aber mit Science-Fiction-Elementen angereichert wird, wobei wir nicht ins Phantastische driften werden. Schliesslich erzählen wir eine wirkliche Geschichte.

In dieser Geschichte ist der Mensch der Hauptdarsteller. Was ist das hier und heute für Sie: «der Mensch»?

Ich glaube, dass man im Jahr 2019 vor allem eines über uns wissen muss: Der Mensch ist heute ein «hackable animal» – ein Tier, das gehackt werden kann. Bisher gab es grossen Aufruhr und viele Diskussionen um gehackte Computer, Bankkonten, Online-Profile, Smartphones oder staatliche Systeme. Doch noch haben wir nicht richtig begriffen, dass einige Firmen und Staaten parallel dazu auch die Technologien entwickeln und erlangen, um uns Menschen zu hacken.

Was soll das konkret bedeuten?

Einen Menschen zu hacken, heisst: ihn besser zu verstehen und zu durchschauen, als er selber das vermag. Früher oder später werden verschiedene Instanzen, seien es nun Unternehmen oder Staaten, die Gefühle, Wünsche, Ängste und Gedanken der Menschen mithilfe von Algorithmen ermitteln können. Die Folgen liegen auf der Hand: Wer die inneren Regungen der Menschen kennt, kann ihre Handlungen antizipieren. Und ihre Begehren natürlich auch manipulieren. Letztlich werden diese Instanzen also immer mehr Entscheidungen an unserer Stelle treffen, weil sie unsere inneren Abläufe absolut perfekt erfassen.

Der Mensch kann wohl nur dann derart umfassend gehackt werden, wenn er selber eine Art Computer ist. Glauben Sie, dass wir, mitsamt unseren Gefühlen und Gedanken, nichts als eine Ansammlung von Algorithmen sind?

Was ich selber glaube, ist nicht von Bedeutung. Ich stütze mich auf die Wissenschaft und gebe das zurzeit dominante Paradigma wieder. Und das lautet in den Life-Sciences ganz klar so: Nicht nur der Mensch, sondern alle Organismen sind im Prinzip Algorithmen, die Informationen verarbeiten. Da kommt also Information rein, das Gehirn – unser eingebauter Algorithmus – verarbeitet sie weiter, und daraus resultieren dann Bewegungen und Entscheidungen, aber eben auch Emotionen, Empfindungen und Persönlichkeitsmerkmale.

Wir befänden uns demnach in einem Wettbewerb der Datenverarbeitungssysteme, und der Mensch mit seinem schwachen Algorithmus stünde mittelfristig auf verlorenem Posten?

Nun, über lange Zeit hinweg verfügte der Mensch über das stärkste System. Zunehmend aber wird es tatsächlich übertroffen von anderen Algorithmen.

Diese mechanische Sicht des Menschen gibt es seit langem, schon im 18. Jahrhundert versetzte das Konzept des «homme machine» die Zeitgenossen in Aufregung. Was ist am jetzigen Maschinen-Paradigma neu?

Die Idee der hochentwickelten Maschine hat als solche tatsächlich nichts Originelles. Wenn Menschen früher mit Maschinen verglichen wurden, dann waren es Uhren oder Dampfmaschinen; sie bildeten die Modelle, nach denen unterschiedliche Bewegungen funktionieren sollten. Mit dem Paradigma des Computers wird nun aber jeder erdenkliche Lebensbereich erfasst. Alles, bis hin zum sexuellen Begehren, ist nichts als verarbeitete Information: Das Auge oder die Nase nimmt etwas wahr, das Hirn erkennt die Muster der eingespeisten Daten und gibt daraufhin seine Befehle aus. Ob wir uns zu jemandem hingezogen fühlen oder nicht, ist also eine reine Frage der Mustererkennung.

In dieser stark auf neuronale Prozesse fokussierten Perspektive bleibt vieles ausgeklammert: Was ist Bewusstsein und Geist? Wie beeinflusst der lebende Körper den menschlichen «Computer»? Meines Wissens sind diese Fragen bis heute nicht beantwortet. Könnte die vermeintlich so wissenschaftliche Algorithmentheorie nicht eine jener Fiktionen sein, mit denen sich der Mensch so gerne die Welt erklärt?

Doch, das ist durchaus möglich! Ich bin selber skeptisch gegenüber Teilen dieser Thesen, und zwar eben weil bis anhin gute Theorien zum Bewusstsein und zur Erklärung von subjektiver Erfahrung fehlen.

In Ihren Büchern ist von dieser Skepsis aber nichts zu spüren.

Insbesondere mein letztes Buch war dezidiert politisch gedacht. Es gibt offene philosophische Debatten, ja, aber ganz egal, wie wir uns zu ihnen stellen, eins müssen wir sehen: Die Technologien, die in uns eingreifen können, sind da. Als wir uns noch nach Ähnlichkeiten mit Uhrwerken befragten, gab es keine Möglichkeiten, die menschliche Maschine umzupolen. Das ist jetzt zum allerersten Mal der Fall, und um dieses Problem müssen wir uns dringender kümmern als um die gewiss interessanten philosophischen Fragen. Wir haben keine Zeit zu verschwenden. Daher täten wir meiner Meinung nach gut daran, die Philosophie eine Weile lang beiseitezulassen und den Fokus auf das zu richten, was hier und heute geschieht und täglich extremer wird. Ob wir nun ganz genau wissen, wie und wieso das funktioniert, scheint mir in dieser Situation wirklich zweitrangig.

Was, zum Beispiel, geschieht denn heute an manipulativen Eingriffen ins Innerste des Menschen?

Ganz konkret kann ich es Ihnen anhand meiner eigenen Biografie erläutern. Als ich fünfzehn war, habe ich selber nicht realisiert, dass ich schwul bin. Zwar fühlte ich mich von Jungen angezogen, aber es gab eine Blockade in meinem Kopf, ich hatte kein Bewusstsein für meine «andere» sexuelle Orientierung. Heute können Firmen mit geeigneten Technologien die Präferenzen der Menschen ohne weiteres ermitteln – man braucht dafür nur ihre Augenbewegungen aufzuzeichnen. Noch ehe ich selber es begriffen hatte, hätte also irgendein Konzern bemerken können, dass ich am Strand nur Jungs beobachte.

Das Resultat wäre wohl eine weitere Personalisierung der Werbung: Die fragliche Firma hätte vermutlich versucht, Ihnen ihre Produkte mit schönen Männerbildern schmackhaft zu machen.

Ja, aber das ist nur die eine Seite. Natürlich ist es schlimm genug, dass ich auf diese Weise manipuliert und zu vermutlich sinnlosen Käufen animiert werden kann. Jetzt stellen Sie sich aber einmal vor, was passiert, wenn Staaten diese Technologien einsetzen und Ihre sexuelle Orientierung registrieren. Bekanntlich gibt es nicht wenige Länder, die Homosexualität mit dem Tod bestrafen . . . Ich denke, das macht es deutlich: Es ist höchste Zeit, dass wir diese Probleme ernst nehmen und überlegen, wie wir mit ihnen umgehen, wie wir die Zukunft gestalten wollen.
«Die Geisteswissenschafter verfehlen ihre wichtigste Rolle, wenn sie nur interne Debatten in ihren Fachgebieten führen», sagt Yuval Noah Harari. Als Redner ist er auf der ganzen Welt unterwegs, hier 2018 in Amsterdam. Einverstanden. Aber wie soll das gehen? Wer alles menschliche Denken und Handeln auf biochemische Prozesse reduziert, bestreitet in aller Regel auch die Existenz des freien menschlichen Willens. Auch Sie tun das in Ihren Schriften dezidiert. Wie kann der Mensch die Zukunft in die «richtige» Richtung lenken, wenn er nicht frei ist, selbstbestimmte Entscheidungen zu treffen?

Wenn ich sage, dass wir keinen freien Willen haben, heisst das nicht, dass wir keine Handlungsmacht besitzen und keine Entscheidungen treffen können. Wir müssen das klar vom freien Willen trennen, und eigentlich würde ich sogar sagen: Je weniger wir an der naiven Vorstellung des freien Willens festhalten, desto mehr Handlungsfähigkeit können wir erlangen.

Das müssen Sie mir erklären.

Es ist doch so: Am einfachsten zu manipulieren sind ausgerechnet jene Leute, die alles, was ihnen dauernd durch den Kopf geht, als Ausdruck ihres eigenen Willens oder als Produkt ihres autonomen Geistes sehen. Solche Personen haben keinerlei über sie hinausreichende Neugier und werden sich nie fragen: Moment einmal, wieso genau habe ich nun diese Cornflakes eingekauft oder jene Partei gewählt? Jede Wahl ist diesem naiven Paradigma gemäss einfach ein Reflex des mysteriösen freien Willens. Dagegen bringt es einen sehr viel weiter, wenn man kritisch danach fragt, wie Vorstellungen, Ideen und Wünsche eigentlich entstehen und geformt werden. Denn erst wenn ich begreife, wie stark mein Denken von allen möglichen biologischen, kulturellen und sozialen Faktoren geprägt wird, kann ich mir so etwas wie Freiheit überhaupt erkämpfen.

Das würde bedeuten: Man muss sich der äusseren Einflüsse bewusst werden, um sich von ihnen frei zu machen – und letztlich wohlüberlegt auf die Welt einzuwirken?

Ja, genau. Das Konzept des freien Willens erweckt den Eindruck, dass Freiheit etwas ist, das man einfach so hat. Aber nein: In unserem normalen Zustand sind wir Menschen dauernd in der Zange etlicher Zwänge, unser Denken speist sich aus anderen Quellen als aus unserer «Seele». Was wir zum Beispiel über die Migration denken, hängt davon ab, in welchem Milieu wir leben und welche Medien wir konsumieren. Um frei zu handeln, ist es unerlässlich, all diesen Mechanismen nachzuforschen. Und wer sich fraglos mit jedem seiner Gedanken identifiziert, tut genau das Gegenteil.

Was formt in Ihren Augen die Geschichte des Menschen, was treibt sie voran?

Es ist eine Kombination von vielen Faktoren. Von materialistischen Sichtweisen, die alles auf die Geografie oder die Ökonomie zurückführen, halte ich nichts. Ich glaube dagegen, dass Erzählungen, Fiktionen, Mythologien einen enormen Einfluss auf den Verlauf der Geschichte haben. Denken Sie doch einmal: Sind Nationen nicht die grössten Kräfte in der gegenwärtigen Welt?

Doch, das kann man gewiss so sehen.

Eben, und Nationen sind nichts anderes als Geschichten! Wir reden von Russen, Franzosen oder Deutschen, als ob das natürliche Spezies wären, doch das ist völlig falsch. Anders als etwa Schimpansen und Gorillas, die sich biologisch unterscheiden, sind die unterschiedlichen Nationen reine Erfindungen, sie haben absolut keine objektiven Realitäten.

Aber sehr reale, nicht selten blutige Auswirkungen – die Konflikte zwischen Franzosen und Deutschen haben Hunderttausende Tote gefordert.

Natürlich, das belegt ja gerade die Macht der Fiktionen. Diese Nationen, die wir heute als völlig normal erachten, haben vor 5000 Jahren in keiner Weise existiert – und 5000 Jahre sind in der Menschheitsgeschichte eine wahrlich kurze Zeit. Doch der Mensch lebt eben davon und prägt seine Geschichte dadurch, dass er solche Storys erfindet – und dann im Kollektiv an sie glaubt. Mit dem Geld oder den Rechtssystemen ist es ja nicht anders. Daneben halte ich aber auch Zufälle für enorm wichtig. Dass zum Beispiel das Christentum oder der Islam eine Weltreligion wurde, war mitnichten determiniert. Wäre Mohammed in einer seiner arabischen Schlachten umgekommen, wäre alles ganz anders verlaufen.

Wenn Storys so zentral sind, dann ist es entscheidend, was für Geschichten wir uns erzählen . . .

Natürlich, eben darum investieren wir jetzt ja zum Beispiel in den Film, und übrigens auch in ein Kinderbuch.

. . . die Frage ist nur: Welche Erzählung braucht die Welt heute?

Die Menschheit muss zurzeit den Blick schärfen für drei existenzielle Herausforderungen: den Atomkrieg, die ökologische Krise und die technologische Disruption.

Seit ein paar Monaten ist Punkt zwei, die Ökokrise, das dominante Thema. Sie selber fokussieren seit je auf die technologische Herausforderung; den Klimawandel handelten Sie in Ihren bisherigen Büchern nur ganz am Rand bzw. in Unterkapiteln ab. Haben Sie sich verschätzt?

Es gibt einen guten Grund, warum ich mich auf die dritte Herausforderung konzentriere: Sie ist die komplizierteste. Die Gefahren, die von ihr ausgehen, sind nicht grösser, aber vermutlich schwieriger zu erkennen. Natürlich gibt es beim Klimawandel noch die paar Leute, die meinen, das sei alles Fake. Doch keiner steht hin und sagt: Ja, es gibt den Klimawandel, und das ist eine wunderbare Sache, lasst uns die Erderwärmung fördern! Im technologischen Bereich ist die Situation sehr viel diffuser. Es ist ganz und gar nicht klar, ob wir hier etwas stoppen sollen und was wir von den neuen Technologien eigentlich wollen.

Viele von ihnen integrieren wir dankbar und freiwillig in unseren Alltag.

Absolut, und die Forschung, gerade im Hirnbereich, hat ja auch ein grossartiges Potenzial. Just gestern hat man mir auf einem Panel ein wunderbares Projekt vorgeführt, das Gelähmten ermöglichen soll, die Beine wieder zu bewegen. Vereinfacht gesagt, liest da ein Computer die Nachricht im Hirn «Beweg das Bein» und schickt die Information an den richtigen Ort im Körper. Das ist einerseits phantastisch, bedeutet aber andererseits eben auch, dass ein Computer lesen kann, was in einem Hirn vor sich geht. Ein Bewegungsbefehl ist unproblematisch, klar, aber was ist mit unseren Emotionen oder Ideen? Und nicht nur diese Verbindungen zwischen Computern und Gehirnen haben wir heute, mit Bio- und Gentechnik wird es auch möglich, die menschlichen Körper zu verändern. Wie wollen wir mit alledem umgehen? Vor einer vergleichbaren Herausforderung hat die Menschheit noch nie gestanden.

Das ist eine überraschende Aussage für einen Historiker. Mir scheint, dass auch frühere Gesellschaften immense Herausforderungen kannten. Überschätzen Sie nicht die Bedeutung unserer Gegenwart?

Nein. Alle früheren Revolutionen haben bloss die äussere Welt verändert. Egal, wie stark die Sesshaftigkeit oder das Christentum unsere Kulturen umgekrempelt hat, egal, wie die Reformation die Wissenschaften und die Industrialisierung das Arbeiten verändert haben: Wir sind immer die gleichen Tiere geblieben, die wir schon vor 30 000 Jahren waren.

Der Mensch hat sich doch stets verändert, immer hat er sich «optimiert», sich Krücken gebastelt, Prothesen eingesetzt oder die Augen scharfsichtig gelasert.

Das sind noch immer äussere Adaptionen, nie zuvor gab es die Möglichkeit, die Strukturen von Körper und Geist auf wirklich profunde Weise zu verändern. Doch wenn künstliche Intelligenz und Biotechnologien unsere Gehirne modifizieren, werden aus diesen «Upgrades» komplett andere Menschen resultieren. In 100 oder 200 Jahren wird die Erde von Wesen dominiert sein, die etwa so viel mit uns heutigen Menschen gemein haben wie wir selber mit Neandertalern oder Schimpansen.

Wäre es schlimm, wenn der Mensch verschwände?

Das kommt darauf an, wodurch er genau ersetzt wird. Wie die neuen Wesen beschaffen sein werden, ist ja vollkommen ungewiss. Man sollte sich das Ganze auch nicht als Hollywood-Katastrophe vorstellen, die den jetzigen Menschen vernichtet, sondern als graduellen Prozess. Ganz allmählich werden sich gewisse Menschen technologisch «optimieren», und dieses Szenario birgt vor allem die Gefahr einer Spaltung: Wenn jene Klassen, die es sich leisten können, langsam zu Supermenschen werden, bleibt der alte Homo sapiens abgehängt zurück. Diese extreme Ungleichheit könnte zu einer ganz neuen Art von Regime führen und das alte faschistische Ideal des «neuen Menschen» plastisch durchsetzen.

Ich erkenne in diesen Horrorszenarien noch eine ganz andere Gefahr: Sie lenken uns von den sehr realen Problemen ab, die sich hier und jetzt in einigen liberalen Ordnungen zeigen. Mich ängstigt weniger der Aufstieg von Supermenschen als jener von autoritären Führern, zum Beispiel in Osteuropa.

Aber diese Dinge hängen doch zusammen! Gerade die illiberalen Regierungen stützen sich ja immer mehr auf die fraglichen Technologien, auf künstliche Intelligenz und Big Data. In Ungarn zum Beispiel geschieht das selbstverständlich, und in China entsteht mit einem technologiebasierten gesellschaftlichen Ranking-System geradezu der Prototyp eines neuen totalitären Regimes.

Wenn wir einmal von den hypothetischen «neuen Menschen» absehen: Wie sind die Totalitarismen des 20. Jahrhunderts an Schrecken zu überbieten?

Nun, «total» wird in unserem Jahrhundert wirklich «total» bedeuten. Nicht nur wird Überwachung rund um die Uhr möglich sein, sie wird sich eben auch aufs Fühlen und Denken erstrecken. Die Propagandaslogans eines Regimes nachzubeten, wird künftig nicht mehr reichen. Gut möglich zum Beispiel, dass in Nordkorea in 10 oder 20 Jahren jedermann ein Armband mit Sensoren tragen muss. Fühlt ein Mensch dann innerlich Ärger aufsteigen, während er bei einer Parade dem Machthaber applaudiert – dann wird er umgehend grosse Probleme bekommen. Eine derart extreme Form von Totalitarismus hat sich nicht einmal George Orwell in «1984» ausmalen können. Aber jetzt ist sie in Reichweite.

Sie reden regelmässig mit wichtigen Politikern, treffen Merkel, Macron und andere zum Gespräch. Welche Ratschläge geben Sie ihnen?

Ich habe keine Ratschläge oder Empfehlungen abzugeben. Es geht eher darum, die Diskussion zu öffnen, ein Bewusstsein für die Probleme zu schaffen. Die Politiker stecken oft in ihrem Kurzfristdenken fest, selten schauen sie weiter als bis zu den nächsten Wahlen. Ich versuche, ihren Blick auf Dinge zu lenken, die in der Zukunft liegen – die wir aber heute mit Entscheidungen beeinflussen können und müssen.

Ist es eine Aufgabe oder gar Pflicht des Intellektuellen, Politik und Gesellschaft aufzurütteln?

Ja, ich glaube, das ist die Hauptaufgabe von Historikern, Philosophen oder Soziologen. Ein Arzt ist dazu da, Leute zu heilen. Ein Computerexperte hat die Aufgabe, Algorithmen zu entwickeln. Idealerweise sind ihm auch die Gefahren seines Tuns bewusst, und es wäre gut, wenn er die Öffentlichkeit darüber auch in Kenntnis setzte. Primär ist er aber nun einmal mit dem Entwickeln seiner Technologien befasst. Die Geisteswissenschafter aber verfehlen ihre wichtigste Rolle, wenn sie nur interne Debatten in ihren Fachgebieten führen. Sie sollen raus, die Leute informieren und Diskussionen führen.

Sie waren ursprünglich Mediävist. Jetzt reden und schreiben Sie über alles Erdenkliche, von der Technologie über das Glück bis zum Terrorismus, und die ganze Welt hängt an Ihren Lippen. Wie fühlt sich das an?

Sehr gut fühlt sich das an, vor allem die Interaktion mit der Öffentlichkeit. Ich bin froh, dass ich nicht mehr in meiner akademischen Blase sitze und nur mit mir selber rede. Doch meine Mittelalterkenntnisse sind eine sehr gute Basis für das, was ich jetzt tue. Ich habe gelernt, in der langen Dauer zu denken, und weiss, dass die Welt, wie wir sie seit fünfzig Jahren kennen, keinen «natürlichen» Zustand darstellt. Weder wird es so weitergehen wie bisher, noch war irgendetwas immer schon so. Menschen können sich ganz unterschiedlich organisieren, und es ist ein ziemlicher Zufall, dass wir hier und heute auf diese scheinbar so normale Weise auf der Welt leben.";https://www.nzz.ch/feuilleton/yuval-noah-harari-der-mensch-kann-gehackt-werden-ld.1496741;NZZ;Claudia Mäder;;;
23.03.2020;Ethisch sterben lassen – ein moralisches Dilemma;"«Sterben lernen im Anthropozän» lautet der Titel des 2015 auf Englisch erschienenen Bestsellers von Roy Scranton. Das Anthropozän, also das Zeitalter, in dem der Mensch das Schicksal eines Planeten prägt, wird häufig mit der Frage und der Forderung nach Nachhaltigkeit in Verbindung gebracht, etwa in der Uno-Agenda 2030. Denn wir Menschen als sowohl dominante als auch rapide sich ausbreitende Spezies gefährden die Lebensgrundlage vieler Lebensformen, einschliesslich unserer eigenen.

Wir stecken in einem Dilemma fest zwischen besserem Wissen und gleichzeitiger Fortführung liebgewonnener Gewohnheiten eines auf Ausbeutung angelegten Lebens. Was läge da näher, als sich mit der Frage zu beschäftigen, wie dem Problem effektiv zu Leibe gerückt werden kann?
Auf halber Strecke zu den Dystopien

Mit dieser Frage befasst sich nicht zuletzt eine Reihe von Romanen. Im Rahmen der literarischen Freiheit werden dort dystopische Welten imaginiert, in welchen die Erde durch Depopulation «gerettet» wird, da die menschliche Intelligenz das Dilemma anscheinend selber nicht auflösen kann: Tom Hillenbrands «Hologrammatica», Frank Schätzings «Tyrannei des Schmetterlings» oder Willemijn Dickes «iGod» sind in der Fiktion angesiedelte Beispiele, in denen künstliche Intelligenzen zum Schutz der Menschheit Menschen vernichten. Die Freiheit der Schriftsteller führt zu tollkühnen Science-Fiction-Abenteuern, die das Umsetzungsdilemma der nachhaltigen Entwicklung grausam lösen – um es so infrage zu stellen.

Doch wie weit sind diese Fiktionen noch von der Realität entfernt? Das Wort «Depopulation» zieht schon seit geraumer Zeit in Think-Tanks und Workshops seine Kreise. Wir möchten aufzeigen, dass die Voraussetzungen, auf denen die Schriftsteller ihre Abenteuerromane aufbauen, bereits im Heute angelegt sind. Dreh- und Angelpunkt bilden dabei die aus der praktischen Philosophie stammenden Dilemmas, also jene Situationen, für die es keine erfreuliche oder richtige Lösung gibt, sondern alle Handlungsoptionen ein Problem darstellen. Das bekannteste ist das bei Digitalisierungsthemen immer wieder aus der Mottenkiste geholte moralische Dilemma des sogenannten «Trolley-Problems». Dieser der ethischen Schule des Utilitarismus zuzurechnende Denkansatz fragt danach, wer sterben muss, wenn nicht alle überleben können. Tut man nichts, so sterben mehrere Menschen, die von einem Tram (Trolley) überrollt werden. Stellt man hingegen eine Weiche, sterben weniger Menschen. Aber man tötet. Die heutige Rechtsprechung verbietet, ein Leben gegen ein anderes aufzuwiegen.

Doch in der jüngeren Forschung der Digitalisierungsethik für autonome Fahrzeuge wurde das Thema wieder und wieder diskutiert. Die dahinterstehende Überlegung lautet: Welches Leben ist im Falle eines unvermeidbaren Unfalls mehr wert? Aufgegriffen wurde die Frage in dem höchst kontroversen «Moral Machine Experiment», das am Beispiel des autonomen Fahrens kulturelle Unterschiede bei hypothetischen Tötungsentscheidungen erforscht. Man stelle sich vor, im «echten Leben» entschiede ein digital ermittelter «Personenwert» über Leben und Tod. Man muss da nicht nur an den chinesischen «Social Credit Score» denken. Der auch hier schon zur Anwendung kommende «Customer Lifetime Value» funktioniert im Prinzip ähnlich, indem er das statistisch noch zu verdienende und auszugebende Geld eines Menschen errechnet. Man muss sich fragen: Sind wir mit den Scoring-Ansätzen des Überwachungskapitalismus nicht schon auf halber Strecke zu den Dystopien? Und welche Konsequenzen hätte das für die demokratisch-rechtsstaatliche Grundordnung?
Am Anfang stehen Daten

Mit Big Data verbreitet sich die kommerzielle Verwertungslogik schon in einst besonders geschützten Bereichen. So etwa in der Medizin, wenn Patienten danach «priorisiert» werden, wann und bei wem sich welche Operation oder welches teure «Ersatzteil» noch lohnt. Grundlage für diese in letzter Konsequenz auch über Leben und Tod entscheidenden Computerprogramme sind die Erhebung und die digitale Speicherung von Patientendaten.

Dabei drohen bereits zahlreiche datenschutzrechtliche Prinzipien zu fallen, die bisher dem Schutz der Patienten dienen sollten. Nicht nur wird bei den Patientendossiers das «opt-in»-Prinzip der informationellen Selbstbestimmung infrage gestellt. Es scheint, die elektronischen Patientendossiers sollen sogar verpflichtend und auch für die Forschung von nicht gemeinwohlorientierten Institutionen und Unternehmen verfügbar gemacht werden. Damit wird das bis anhin im Gesundheitssystem so wichtige Prinzip der Menschlichkeit zugunsten ökonomischer Gesichtspunkte in den Hintergrund gedrängt. Wer «schlechte Gene» hat, wäre nicht nur öfter oder schwerer krank – er würde vom Gesundheitssystem auch zunehmend mit seinen Problemen allein gelassen, entgegen dem Solidar- und dem Subsidiaritätsprinzip, welche einst die Grundlage der Krankenversicherungen waren.

Die entscheidende Frage lautet also: Wie sollen oder dürfen autonome Systeme entscheiden, wenn es um Leben und Tod geht? Regina Surber von ICT4Peace warnte daher in dieser Zeitung zu Recht davor, dass autonome Intelligenz auch in Friedenszeiten lebens- und sterbensrelevante Informationen bereithalten und anwenden könnte.

Mit Blick auf das Anthropozän betonen wir deshalb, dass eine gesetzliche Regulierung von Tötungs- oder Sterbeentscheidungen durch autonome Systeme Technologien legitimeren könnte, die Menschen in Krisensituationen, wie sie in einer nichtnachhaltigen Welt entstehen könnten, autonom töten. Wir sehen da die Gefahr einer eklatanten moralischen Entgleisung. Aus unserer Sicht sind solche Technologien unter keinen Umständen zulässig – nicht einmal zur Rettung der Welt. Sie verstossen gegen fundamentalste Rechtsprinzipien. Sie gingen selbst über das Kriegsrecht hinaus, gemäss dem die gezielte Tötung von Zivilbevölkerung nicht legitim ist.
Roboter und Mensch: die Prinzipien

Kritik ist das eine. Doch wie sollte man autonome Intelligenz dann gestalten? Science-Fiction-Fans kennen die vom Physiker Isaac Asimov im Kriegsjahr 1942 bereits aufgestellten Grundprinzipien für den Umgang von Robotern mit Menschen:

1. Ein Roboter darf die Menschheit nicht schädigen oder durch Passivität zulassen, dass die Menschheit zu Schaden kommt. 2. Ein Roboter darf keinen Menschen verletzen oder durch Untätigkeit zu Schaden kommen lassen, ausser er verstiesse damit gegen das vorige Gesetz. 3. Ein Roboter muss den Befehlen der Menschen gehorchen – es sei denn, solche Befehle stehen im Widerspruch zu den vorhergehenden Gesetzen. 4. Ein Roboter muss seine eigene Existenz schützen, solange sein Handeln nicht den ersten drei Gesetzen widerspricht. – Mit Blick auf das ethische Dilemma des «Trolley-Problems» sollten diesen Regeln noch zwei hinzugefügt werden: 5. Menschen und Roboter müssen alles dafür tun, dass das Auftreten von ethischen Dilemmas minimiert wird. 6. Falls es nicht möglich ist, alle Wünsche gleichzeitig zu erfüllen, falls also trotz aller Bemühungen, Regeln 1 bis 5 zu erfüllen, ethische Dilemmas bzw. Ressourcen- oder Zielkonflikte unvermeidbar sind, dann soll der Fairness halber das Prinzip der Chancengleichheit angewandt werden.
Die erleichternde Gerechtigkeit des Zufalls

Grade die dabei ins Spiel kommende Zufallskomponente, die wir im Religiösen als die «Unergründlichkeit der Wege des Herrn» oder im Versicherungsrecht als «höhere Gewalt» kennen, erschafft eine erleichternde Gerechtigkeit des Zufalls, die sich der utilitaristischen Verwertungslogik des «Trolley-Problems» entzieht. Die Grundbotschaft dabei bleibt: Es kann jeden treffen, im Guten wie im Schlechten. Ganz im Unterschied zu einem monetären oder Scoring-basierten hierarchischen System, wo es einigen egal sein kann, wie es den anderen geht, weil sie die Missstände nicht spüren. Und genau das liefert den lebenswichtigen Anreiz, alle Möglichkeiten auszuschöpfen, das Sterberisiko insgesamt zu minimieren. Noch gibt es viele Optionen zur Verbesserung unseres Systems. Wir sollten sie nutzen.

Es gibt vermutlich keine problematischeren Anwendungen als algorithmenbasierte Entscheidungen über Leben und Tod, egal wie fortschrittlich die dabei eingesetzte Technik ist. Die heute auf dem Wege befindlichen technischen Lösungen berühren den Kerngehalt unserer offenen, demokratischen und rechtsstaatlichen Gesellschaft fundamental, etwa die Menschenwürde, den Schutz der Privatsphäre, den Gleichheitsgrundsatz, die Gewaltenteilung, das Tötungsverbot, um nur einige zu nennen. In der digitalen Gesellschaft der Zukunft droht vieles davon zu Makulatur zu werden. Im Zusammenhang mit der gegenwärtigen Diskussion um den «Klimanotstand», der als Türöffner der Aushebelung der demokratisch legitimierten Rechtsstaatlichkeit fungieren kann, weisen wir daher ausdrücklich darauf hin, dass laut Bundesverfassung auch in Krisensituationen zu gelten hat: «Der Kerngehalt der Grundrechte ist unantastbar.»";https://www.nzz.ch/meinung/ethisch-sterben-die-gefahr-der-moralischen-entgleisung-ld.1542682;NZZ;Dirk Helbing, Peter Seele;;;
24.09.2019;So wird uns künstliche Intelligenz zum Glück verführen (oder zwingen);"In welcher Dystopie leben wir denn nun? Die meisten Gebildeten haben George Orwells «1984» und Aldous Huxleys «Schöne neue Welt» gelesen. Der Einfluss dieser Bücher ist so gross, dass wir dazu neigen, alle verunsichernden neuen Erscheinungen entweder als «orwellsche» oder als «huxleysche» Phänomene zu sehen.

Wer den Argwohn hegt, wir würden unsere Freiheit an einen brutal repressiven Staat verlieren, der uns mit dem Stiefel das Gesicht zertritt, denkt an Orwell. Wer glaubt, wir würden sie an eine hedonistische, durch Designerbabys aus dem Reagenzglas erweiterte Konsumkultur verlieren, zitiert Huxley.
Das grosse Glück

Doch ein für beide geeignetes, überragendes Science-Fiction-Werk ist das weit frühere Meisterwerk des russischen Satirikers Jewgeni Samjatin mit dem Titel «Wir». Das von 1920 bis 1921 in den frühen, turbulenten Jahren der bolschewistischen Herrschaft geschriebene Buch ist erstaunlich weitsichtig. In dem «Vereinigten Staat» sind die einzelnen Menschen blosse «Nummern», gekleidet in genormte Uniformen, die statt Namen nur Zahlen tragen. Alle Wohnungen sind vollständig aus Glas gebaut, und Vorhänge können nur für staatlich lizenzierten Sex zugezogen werden. Die Geheimpolizei, das Beschützeramt, ist allgegenwärtig. Anders als im Sowjet-Britannien Orwells, wo es Möglichkeiten gibt, den Überwachungsbildschirmen zu entgehen, erfolgt die Überwachung im Vereinigten Staat andauernd und unausweichlich. Anders als in Huxleys auf Eugenik beruhendem Utopia ist Vergnügen eine Pflicht und freudlos.

Die Hauptfigur von «Wir», D-503, ist Mathematiker und Ingenieur, der an der Konstruktion eines Raumschiffs namens Integral arbeitet, aber von dem Verdacht gequält wird, dass nicht alles menschliche Leben auf mathematische Formeln zu reduzieren sei. D-503s Leben beginnt sich aufzulösen, als er von I-330, einer Femme fatale, verführt wird – sie weiht ihn in die verbotenen Vergnügungen des Alkohols, des Tabaks und der ungeplanten Sexualität ein.

Der allmächtige Wohltäter, konfrontiert mit einer von I-330 angeführten Rebellion, die damit droht, die Grüne Mauer zwischen dem Vereinigten Staat und einer bis dahin verborgenen natürlichen Welt niederzureissen, befiehlt eine massenhafte Gehirnoperation bei allen Nummern. Er behauptet, die einzige Möglichkeit, das allumfassende Glück zu bewahren, sei die Abschaffung der Phantasie.

«Worum haben die Menschen von Kindesbeinen an gebetet, wovon haben sie geträumt, womit haben sie sich gequält?», fragt der Wohltäter D-503. «Dass irgendeiner ihnen ein für alle Mal sage, was das Glück ist, und sie mit einer Kette an dieses Glück schmiede.»
Die grosse Hypnose

Orwell räumte freimütig ein, was er Samjatin verdankte, während Huxley wenig plausibel verneinte, ihn gelesen zu haben. Doch Samjatin hat es zumindest verdient, mit beiden zusammen als gleichrangiger Meister der dystopischen Science-Fiction anerkannt zu werden – nicht zuletzt, weil er das albtraumhafte Panoptikum vorwegnahm, das Stalin in den Ruinen des Russischen Imperiums errichten sollte. (Als Orwell schrieb, war das Wesen der totalitären Bestie schon allzu offensichtlich.) Samjatin, der wegen abweichender Ansichten zweimal ins Gefängnis geworfen worden war, erhielt 1931 die Erlaubnis, ins Exil zu gehen. Er hatte Glück.

Ich selbst habe einen beträchtlichen Teil meiner Karriere damit zugebracht, mir mithilfe der Anwendung von Geschichte mögliche Zukünfte vorzustellen. Doch in diesem Jahr habe ich mit einem alternativen Ansatz experimentiert – der Anwendung von Science-Fiction. Diese Literaturgattung mochte ich als Junge, gab sie aber mehr oder weniger auf, als ich an die Universität ging – in der irrigen Überzeugung, sie sei nicht ausreichend seriös. In Wahrheit gibt es nur wenige literarische Formen, die erhellender sind.

Von H. G. Wells bis Margaret Atwood haben Hunderte grosser Geister in ihre Kristallkugeln geblickt und sich die möglichen Folgen grosser Katastrophen und neuer Technologie vorgestellt. Das Studium der Vergangenheit hilft uns zu erkennen, auf welche Weisen sich die Welt wiederholen könnte, doch wir brauchen Science-Fiction, um zu sehen, was an der Zukunft neu sein wird.

Samjatin, Huxley und Orwell stimmten im Wesentlichen darin überein, dass die Macht des Staates unaufhaltsam zunehmen werde. In einem Brief an Orwell meinte Huxley, nachdem er 1949 «1984» gelesen hatte, die einzige Frage sei, wie brutal die Zwangsmassnahmen des Staates in der künftigen Welt seien.

«Die Philosophie der herrschenden Minderheit in ‹1984› ist ein Sadismus, der bis an sein logisches Ende geführt wurde», schrieb Huxley (der, ganz nebenbei gesagt, viele Jahre zuvor in Eton Orwell das Französische beigebracht hatte). Und weiter: «Ob die Politik des Stiefels im Gesicht tatsächlich unbegrenzt weitergehen kann, scheint zweifelhaft. Ich selbst glaube, die herrschende Oligarchie wird weniger beschwerliche und verschwenderische Arten der Herrschaft und der Befriedigung ihrer Lust an der Macht finden . . . Ich glaube, die Herrschenden der Welt werden innerhalb der nächsten Generation entdecken, dass die Konditionierung der Kinder und die Narko-Hypnose effizientere Herrschaftsinstrumente sind als Schlagstöcke und Gefängnisse und dass die Lust an der Macht ebenso umfassend befriedigt werden kann, wenn man den Menschen suggeriert, sie würden ihre Knechtschaft lieben, wie wenn man sie mit der Peitsche und mit Fusstritten zum Gehorsam zwingt.»
Die grosse Farce?

Wenn ich nun über die Welt im Jahr 2019 nachdenke, finde ich die Weisheit dieser Worte verblüffend. Im China von Xi Jinping sehen wir den Totalitarismus 2.0. Der Stiefeltritt ins Gesicht bleibt natürlich eine Möglichkeit, wird aber immer weniger benötigt, weil das System des Sozialkredits wächst und dabei all die digitalen Daten sammelt und analysiert, die von den chinesischen Bürgern erzeugt werden.

«Das Politik- und Rechtssystem der Zukunft ist vom Internet und von Big Data nicht zu trennen», erklärte Jack Ma von Alibaba einer Kommission der Kommunistischen Partei, die 2017 die Durchsetzung der Gesetze überprüfte. «Künftig», sagte er, «werden die Bösen nicht einmal mehr in der Lage sein, den öffentlichen Raum zu betreten.»

Beispiel: In China sind inzwischen manche Klassenzimmer mit auf künstlicher Intelligenz beruhenden Kameras und Scannern für Gehirnwellen ausgestattet, mit denen der Konzentrationsgrad der Schüler überwacht wird.

Für diejenigen, die die Freiheit des Menschen lieben, bleibt als einziger Trost, dass demokratische Staaten zu solchen Dingen anscheinend weniger fähig sind – obwohl ich den Verdacht habe, dass dies eher auf Inkompetenz als auf die Gewaltenteilung, die Herrschaft des Gesetzes oder den freiheitlichen Geist zurückzuführen ist. Selbstverständlich müssen wir uns wegen der bei Google und Facebook im Bau befindlichen Panoptiken im privatwirtschaftlichen Sektor Sorgen machen. (Wer daran zweifelt, dass die Giganten des Silicon Valley totalitäre Neigungen haben, sollte einfach einmal einen Blick in die geleakte Google-Präsentation «Der gute Zensor» werfen.)

Doch die Technologie, die Menschen dazu dient, Geld zu machen, scheint letztlich weniger gefährlich zu sein als eine Technologie, die dazu dient, Bürger «glücklich» zu machen.

Zur Illustration: In den letzten Wochen hat die Ausgelassenheit des Planeten stark zugenommen – dank den Bemühungen von WeWork, einer heftig überbewerteten Tech-Firma, die gemeinsam genutzten Büroraum vermietet. Angeblich 47 Milliarden Dollar wert, hat WeWork vor ein paar Wochen seinen geplanten Börsengang verschoben. In der letzten Woche bezeichnete Larry Ellison, ein Gründer des Tech-Riesen Oracle, das Unternehmen als fast wertlos.

Adam Neumann, der langhaarige israelische Mitgründer von WeWork, erklärte einmal, seine «Mission» sei es, «das Bewusstsein der Welt emporzuheben». Ein anderer Spruch von ihm lautet: «Die Energie des Wir ist grösser als jeder Einzelne, aber in jedem Einzelnen von uns präsent.»

Ach ja, die Energie des Wir. Während ich mir gerade noch vorstellen kann, dass Samjatins Wohltäter diese Worte spricht – oder vielleicht auch Huxleys Mustapha Mond –, ist Neumann eher eine Figur aus einem Roman von Douglas Adams. Mag sein, dass Dystopia unser aller Schicksal sein wird, aber solange man uns keiner Gehirnoperation unterzogen hat, besteht eine gute Chance, dass die Zukunft eher wie in «Per Anhalter durch die Galaxis» aussieht – und nicht wie die Hölle auf Erden.";https://www.nzz.ch/feuilleton/niall-ferguson-wir-leben-in-der-dystopie-ld.1510605;NZZ;Niall Ferguson;;;
22.03.2019;Tech-Camp: Das Startup Code Excursion steht für die neue, weibliche IT-Generation;"Am Anfang stand eine Kündigung. Die Schweizerin Corina Schedler und die Schwedin Louise Serenhov arbeiteten 2017 neben dem Studium beim schwedischen Startup Dooify in Stockholm. Der Entwickler sprang ab, und das Team sah sich ohne grössere IT-Kenntnisse vor einem Problem. Kurzerhand entschieden die beiden Frauen, sich auf eigene Faust ein Grundwissen im Programmieren anzueignen und damit die Seite des Startups zu unterhalten. Vom Eigeninteresse zur Geschäftsidee

Doch wie gelingt der Einstieg in die Kunst des Programmierens? Schedler und Serenhov mussten ein Modell finden, das kostengünstig und zeitlich flexibel ist. Sie entschieden sich für einen Online-Kurs, den man selbständig unter einer zeitlichen Limite absolvieren konnte, am Ende lockt ein Fullstack-Developer- Zertifikat. Zu diesem Zeitpunkt setzt die Idee zu Code Excursion ein.

Die beiden jungen Frauen wussten, dass es ein solcher Online-Kurs für Einsteiger in sich hat. So entschieden sie sich, eine Facebook-Gruppe zu gründen und darin nach allfälligen motivierten Mitstreiterinnen zu suchen. Zu Beginn von Code Excursion kam so wöchentlich eine bunte Gruppe Frauen zusammen, die allesamt demselben Kurs folgten. Schon bald verfügten Schedler und Serenhov über Kenntnisse, die ihnen erlaubten, neue Kurse für Einsteiger anzubieten und diese gleich selber zu unterrichten.  Aus Schweden in die Schweiz

Nach dem Engagement in Stockholm zog es Schedler zurück in die Schweiz, die Erfolgsgeschichte von Code Excursion Stockholm im Gepäck. 2018 gründete Sie mit Céline Nauer den Schweizer Ableger. Die Grundidee – ein inspirierender und motivierender Ort der Zusammenkunft für Programmier-Interessierte, blieb länderübergreifend bestehen. Ein Gespräch mit den Gründerinnen

Seither hat sich das Angebot von Code Excursion in einem wachsenden Tempo weiterentwickelt. Zu den Programmierkursen gesellen sich Workshops zu diversen Front- und Back-End-Projekten, Vorträgen mit Protagonistinnen aus der Industrie und im Frühjahr 2019 auch das erste Digital & Detox Camp in Portugal. Wir haben uns mit Corina Schedler und Céline Nauer über Frauen in der IT-Branche, die Wichtigkeit technologischer Kenntnisse in der Zukunft und Portugal als Standort für das Camp unterhalten. NZZ Bellevue: Wieviel Zeit verstrich zwischen der Idee und dem ersten Workshop?

Corina Schedler: Gerade mal 30 Tage! Im August 2017 noch sassen Louise und ich auf ihrem Balkon bei der Fika (schwedische Kaffeepause). Sie hatte gerade ihren ersten Online-Programmierkurs gemacht, bei mir lief es etwas schleppend mit dem selbständigen Lernen neben der Arbeit. Uns fehlte ein Ort, um sich auszutauschen und motiviert zu bleiben. Im September fand dann der erste Kurs in Stockholm statt. Wie koordiniert ihr die Projekte in Stockholm und in der Schweiz?

Corina Schedler: Wir sind sehr unabhängig unterwegs, da sich die Projekte in den beiden Ländern unterschiedlich entwickeln. Über Slack stehen wir regelmässig in Kontakt und tauschen uns über den aktuellen Stand aus. In der Schweiz halten wir wöchentliche meist virtuelle Treffen ab und koordinieren die nächsten Schritte.  Seit dem Start der ersten Kurse in Zürich und St.?Gallen bauen wir zudem auf Verstärkung in den Bereichen Organisation, Recht und Finanzen.

Warum «Women only» als Konzept?

Corina Schedler: Zu Beginn ging es uns nicht darum, ausschliesslich Frauen zu fördern. Wir wollten lediglich in einer positiven, motivierten Gruppe, frei von Hierarchien und Konkurrenzdenken, lernen zu coden. Aufgrund der zahlreichen Anmeldungen beim ersten Aufruf auf Facebook merkten wir, dass viele andere Frauen genau das gleiche Bedürfnis hatten, und so entschieden wir uns, mit «Women only» als Konzept weiterzufahren.

Céline Nauer: Zudem haben wir uns auch Gedanken über die tiefe Frauenquote in der IT-Branche gemacht. Eine diverse Community in dieser Branche wäre ein Vorteil für alle Beteiligten. Zu dieser Diversität möchten wir mit Code Excursion und einer gezielten Förderung beitragen.

Welche Rolle wird eurer Meinung nach Programmieren in naher Zukunft in unserer Gesellschaft einnehmen?

Céline Nauer: Eine massgebende! Die Digitalisierung ist bereits im vollen Gange, der Prozentsatz technologienaher Berufe steigt stetig an. Ein Grundverständnis fürs Programmieren und IT wird in Zukunft unerlässlich sein. Junge Leute in Ausbildung werden bereits heute durch angepasste Lehrpläne und Initiativen auf das Thema Programmieren sensibilisiert und darin ausgebildet. Wichtig ist, dass Personen, die bereits ins Berufsleben eingetreten sind, diese Wissenslücke schliessen, um auch in Zukunft wettbewerbsfähig zu bleiben. Warum sollten sich mehr Frauen in der Materie des Programmierens auskennen?

Corina Schedler: Wie in allen Bereichen braucht es auch in diesem Bereich mehr Gleichstellung. Laut Statistik arbeiten momentan nur rund 15 Prozent Frauen in der ICT-Branche in der Schweiz. Studiengänge an Hochschulen mit Richtungen Informatik haben es weiterhin schwer, Frauen zu erreichen – so liegt der Frauenanteil in MINT-Fächern bei ca. 30 Prozent, in der Informatik sogar nur bei ca. 12 Prozent.

Céline Nauer: Es ist besonders wichtig, dass wir jetzt handeln und besonders jungen Frauen die nötigen Fähigkeiten zum Programmieren mitgeben. Nur so können wir in Zukunft eine Gleichstellung in der IT-Branche erreichen. Wie kam es zur Idee des «Digital and Detox»-Camps in Portugal?

Corina Schedler: In Stockholm trafen wir uns für die ersten Workshops von Code Excursion in einem Coworking-Space in einer Wohnung in Gamla Stan (Altstadt von Stockholm). Wir zogen alle unsere Schuhe aus, fühlten uns wie zu Hause – dieses Gefühl ist mir geblieben. Codieren heisst nicht isoliert in einem fensterlosen Raum vor sich hin tippen – dieses Image hat mich vom ersten Moment an unheimlich gestört, und ich wollte es ändern. So kam mir die Idee, das Programmieren in die Natur zu holen, in Gesellschaft von inspirierten Menschen, kombiniert mit einem sportlichen Ausgleich. Gemeinsam mit Céline entschieden wir, das Camp-Angebot «Digital and Detox» zu lancieren. Nach Portugal führte uns der Kontakt einer guten Freundin, die im Surferort Peniche wohnt und uns diese wunderschöne Location vermitteln konnte.  Céline Nauer: Programmieren zu lernen, erfordert Ausdauer und einen starken Willen. Die Umgebung und Stimmung trägt erheblich zum Erfolg bei. Unsere Idee ist es, mit einem Ausgleich durch Yoga und guter Ernährung das optimale Lernklima zu schaffen, damit alle mit Spass programmieren können.

Was ist das Programm im Camp, und welches sind die Voraussetzungen, um daran teilzunehmen?

Das Camp wie auch die Kurse sind auf Anfängerinnen ausgerichtet. Ein eigener Laptop sowie gute Englischkenntnisse reichen, um zu starten. Nebst den Grundlagen zum Web-Development werden die Teilnehmerinnen auch in das technologische Ökosystem eingeführt. Das erlernte Wissen wenden wir sogleich bei realen Projekten an.

Wie sieht die Zukunft von Code Excursion aus?

Corina Schedler: Anfang April 2019 reisen wir das erste Mal nach Portugal, worauf wir uns sehr freuen. Danach startet die detaillierte Planung der 3-Monatigen Intensivkurse im Herbst in St.?Gallen und Zürich. Zudem werden wir im zweiten Halbjahr 2019 einige Firmen-Workshops zum Thema Web-Development und Data Science aufbauen und betreuen.";https://bellevue.nzz.ch/reisen-entdecken/it-start-up-code-excursion-bietet-code-camps-fuer-frauen-an-ld.1468105;NZZ;Linda Horber;;;
09.09.2020;Sie jagen Cyberkriminelle und retten Leben – diese Zürcher Startups belegen die ersten Plätze des «Swiss Startup Awards»;"Die besten sechs Schweizer Startups kommen in diesem Jahr alle aus Zürich. Sie haben die Fachjury des Swiss Startup Award am meisten überzeugt. Erfahren Sie, wieso.
Inhaltsverzeichnis

    1. Platz : Cutiss – Haut für Brandopfer

2. Platz: Wingtra – Drohnen für den Bergbau
3. Platz: Piavita – digitale Hilfe für kranke Pferde
4. Platz: Exeon – Hackern auf der Spur
5. Platz: Versantis – Hoffnung für Leberkranke
6. Platz: 9T Labs – Carbon-Teile aus dem 3-D-Drucker
Das Startup Cutiss ist aus einem Forschungsprojekt des Universitätskinderspitals Zürich entstanden. Die Gründerinnen Daniela Marino und Fabienne Hartmann-Fritsch haben eine Methode entwickelt, mit der aus Hautzellen ein grösseres Stück Haut gezüchtet und dem Patienten transplantiert werden kann. Dem Patienten wird dazu ein Stück Haut entnommen, das gerade einmal so gross ist wie eine Briefmarke – aus diesem kann dann im Labor eine 70 Mal grössere Fläche Haut hergestellt werden. Der Vorteil der Methode: Die Haut wird als eigene erkannt, wächst mit, und es entstehen kaum Narben.

Derzeit wird das Verfahren in Studien getestet. 2022 könnte die Ersatzhaut auf den Markt kommen. Die Nachfrage dürfte gross sein: 50 Millionen Menschen leiden weltweit an den Folgen von tiefen Hautverletzungen.
2. Platz: Wingtra – Drohnen für den Bergbau Das Startup Wingtra wurde von Maximilian Boosfeld, Basil Weibel, Sebastian Verling und Elias Kleinmann gegründet und ist ein ETH-Spin-off. Wingtra entwickelt Drohnen, die vertikal starten und landen und mit denen grosse Landflächen vermessen werden können. Die Drohnen sind 3,5 Kilogramm schwer, fliegen bis zu 55 Kilometer pro Stunde und können Distanzen bis 60 Kilometer überwinden. Sie werden etwa im Bergbau oder in der Landwirtschaft eingesetzt. In der Landwirtschaft können anhand der Daten Krankheitserreger frühzeitig erkannt und Ernteausfälle reduziert werden.  Eigentlich hatten die Gründer zuerst eine Transportdrohne entwickeln wollen, dann entschieden sie sich für die Vermessungsdrohne – ein Entscheid, der sich auszahlt: Mittlerweile arbeiten mehr als 80 Personen für Wingtra. Eine Drohne kostet 20 000 Franken.
3. Platz: Piavita – digitale Hilfe für kranke Pferde Dorina Thiess und Sascha Bührle wollen die Pferdemedizin revolutionieren. Sie haben mit ihrem Team einen Sensor entwickelt, der Körperkerntemperatur, Puls, Körperbewegung und Atemrhythmus misst. Der Sensor wird den Pferden mit einem Gurt um den Bauch geschnallt. Die Tierärzte müssen nicht vor Ort sein, sie können aus der Ferne überwachen, wie es den Pferden geht.  Thiess und Bührle haben das Startup 2016 gegründet. Thiess ist Betriebswirtschafterin, Bührle Ingenieur. Angefangen hat alles mit einem Sensor, den Bührle für sein Downhill-Mountainbike entwickelt hat und mit dem er die Federung optimieren wollte. Bührle entwickelte dazu eine Technologie, die es erlaubt, durch mehrere Schichten zu messen. Gemeinsam mit Thiess überlegte er dann, wem ein solcher Sensor sonst noch dienen könnte. Das Fazit: Tierärzten. Der Sensor erspart den Tierärzten Piketteinsätze, lange Anreisewege und Nachtwachen.
4. Platz: Exeon – Hackern auf der Spur

Das Startup Exeon hat eine Software entwickelt, die Hackerangriffe erkennt. Normalerweise können Hacker 200 Tage in einem System wüten, bis sie entdeckt werden. Der angerichtete Schaden ist dann bereits enorm: Im Durchschnitt beträgt er 3,5 Millionen Dollar. Mit der Software namens «ExeonTrace» sollen Hacker schneller erkannt werden.

Das Konzept hat der Gründer David Gugelmann im Rahmen seiner Doktorarbeit an der ETH entwickelt. Es basiert auf Big Data und Machine-Learning-Algorithmen. Eigentlich hatte Gugelmann anfänglich nicht vor, sich selbständig zu machen, die Doktorarbeit hatte für ihn Priorität. Dann wurde er von einem grossen Schweizer Unternehmen angefragt, wie es mit dem Projekt weitergehe. Gugelmann gründete Exeon, das Schweizer Unternehmen wurde sein erster Kunde.
5. Platz: Versantis – Hoffnung für Leberkranke

In der Schweiz warten mehr als 400 Personen auf eine Leberspende, in den USA sind es mehr als 11 000. Viele von ihnen werden sterben, bevor sie ein Spenderorgan erhalten. Ihnen will das Startup Versantis helfen. Versantis wurde 2015 als Spin-off der ETH Zürich gegründet. Das Startup hat ein Arzneimittel entwickelt. Das Besondere: Das Medikament gibt keine Wirkstoffe ab, sondern nimmt Gifte und schädliche Verbindungen auf. Es kann also die Funktionen einer kranken Leber teilweise ersetzen.

Das Medikament wird den Patienten mittels Infusion in die Bauchhöhle gespritzt. Dort nimmt es überschüssiges Ammoniak und Gifte auf. Nach zwei Stunden kann es dann wieder abgelassen werden. Zurzeit wird das Medikament am Unispital in Frankfurt an Patienten getestet. Verschiedene Pharmaunternehmen haben bereits ihr Interesse am Medikament angemeldet.
6. Platz: 9T Labs – Carbon-Teile aus dem 3-D-Drucker

Carbon ist fester als Stahl, aber bis zu fünfmal leichter. Deshalb ist es in der Industrie gefragt. Es hat jedoch einen Nachteil: Es ist teuer. Carbon herzustellen, ist kostenintensiv. Das Zürcher Startup 9T Labs will das nun ändern. Es hat einen Drucker entwickelt, der Bauteile aus Carbon herstellen kann. Das Bauteil kann am Computer konstruiert und dann ausgedruckt werden.

9T Labs wurde 2018 von den ETH-Absolventen Martin Eichenhofer, Giovanni Cavolina und Chester Houwink gegründet. Im selben Jahr wurden sie von der Europäischen Weltraumorganisation in ein Förderprogramm aufgenommen. Das Potenzial des Carbon-3-D-Druckers ist gross: Der Gesamtmarkt für Carbon-Bauteile wird auf 80 Milliarden Dollar geschätzt.";https://www.nzz.ch/zuerich/swiss-startup-award-cutiss-wingtra-und-co-begeistern-experten-ld.1575560;NZZ;Claudia Rey;;;
25.03.2020;Wie das Internet in der Corona-Krise neu erfunden wird;"Auf Bildschirmen können wir beobachten, wie in den grossen Städten die Menschen, die wegen des Virus zu Hause bleiben müssen, trotzdem zusammenfinden, wie sie, vor der Wohnung auf kleinen Vorbauten stehend, durch Strassenschluchten voneinander getrennt, gemeinsam klatschen, singen, beten. Sie haben sich über Social-Media-Kanäle verabredet für ein kurzes Rendez-vous in der Realität. Im Internet gibt es Filme von Menschen, die auf ihren Balkonen Yoga praktizieren, Fussball trainieren, Marathons absolvieren. Im richtigen Leben, auf dem eigenen Balkon, beobachten wir die Nachbarin, wie sie am frühen Morgen eine gelbe Maschine hervorholt und zusammenbaut. Dann ist der Hochdruckreiniger einsatzbereit, die Apokalypse ist da. Die Frau beginnt, überaus geräuschvoll den Balkon zu putzen.

Wir schliessen die Tür, ziehen uns ins Home-Office zurück. Es braucht keine Posaunen und Trompeten, um uns zu versichern, dass wir auf der Welt nicht allein sind. Wir können uns die neuesten Nachrichten besorgen, wir können einkaufen, Rechnungen bezahlen, Blumen verschicken. Wir können an Besprechungen und Konferenzen teilnehmen, Vorlesungen und Seminare besuchen. Wir sind mit Lichtwellenleitern und mehradrigen Kupfersträngen miteinander verbunden. Wie ginge es uns jetzt, wenn es das Internet nicht gäbe? Man wagt es nicht, sich das auszudenken.
Das Zirpen der Modems

Wagen wir es; erinnern wir uns an das Zirpen der Modems, das Rascheln der Swisscom-Rechnungen, das Kratzgeräusch des Data-Scraping. Die ersten Internetanschlüsse wurden in der Schweiz zwar bereits 1987 eingerichtet, aber noch zu Beginn der 1990er Jahre war das Internet ein Kommunikationssystem, das auch hierzulande über Mietleitungen hauptsächlich Universitätsangehörige miteinander verband. Zu Beginn des neuen Jahrtausends hatte gerade einmal ein Drittel aller Schweizer Zugang zum Internet. «Zugang» meinte die Möglichkeit, sich bei Bedarf ins Internet «einzuwählen» – ein umständlicher und geräuschvoller Vorgang, der mehrmals täglich wiederholt werden musste, denn jede Sekunde im Internet kostete Geld. Erst 2004 wurden Breitbandanschlüsse in der Schweiz heimisch, erst jetzt wurden Pauschaltarife üblich, erst jetzt war es auch Privaten möglich, zu vertretbaren Kosten dauerhaft am Internet teilzuhaben.

Um Daten nicht nur anzuschauen, sondern auf dem eigenen Computer weiterzuverarbeiten, war man damals oft auf das Data-Scraping angewiesen, auf das aufwendige Umformatieren von Daten, die, aus dem Zusammenhang gerissen, ihre Bedeutung verloren hatten. Erst nach und nach konnten sich allgemein akzeptierte Datenaustauschformate und Programmierschnittstellen etablieren. So wurde es möglich, dass komplizierte, mehrstufige Transaktionen automatisch ablaufen. Beeindruckend ist in diesem Zusammenhang etwa ein Informationsangebot der amerikanischen Johns Hopkins University, die Daten über die Ausbreitung des Coronavirus aus vielen über die ganze Welt verstreuten Quellen zusammenträgt, visuell aufbereitet und interaktiv erfahrbar macht. Dieselben Daten stehen auch auf anderen Websites in anderer Darstellung und mit anderen Interaktionsmöglichkeiten zur Verfügung. Sie enthalten auch Zahlen aus der Schweiz. Es ist fast nicht zu glauben: Beim Einsammeln dieser Daten kommen beim Schweizer Bundesamt für Gesundheit auch Faxgeräte zum Einsatz. Schweizer Ärzte müssen laut Medienberichten in ein Formular, das sie aus dem Internet heruntergeladen und ausgedruckt haben, von Hand Werte von einem Computerbildschirm abschreiben und per Fax nach Bern melden, wo jemand die Daten dann wieder ab Papier in den Computer eintippt.

Diese Rückständigkeit ist zum Lachen. Und während wir losprusten, wird uns bewusst, wie weit wir in der Digitalisierung schon vorangekommen sind. Und in den vergangenen paar Tagen haben wir uns erneut bewegt. Wir alle kennen das Internet seit vielen Jahren, es ist aus unserem Alltag schon lange nicht mehr wegzudenken. Aber erst jetzt sind wir alle zusammen so weit gegangen, uns in einer Notsituation vollständig auf diese Kommunikationsinfrastruktur zu verlassen. Leistungsfähige, gratis nutzbare, mit gängigen PC- und Smartphone-Betriebssystemen kompatible Video-Conferencing-Software gibt es seit sieben, acht Jahren. Aber erst jetzt wird diese Software von vielen als unabdingbar erkannt. Deshalb sind jetzt beispielsweise Sprechgarnituren, die die Teilnahme an Video- oder Telefonkonferenzen erleichtern, bei den grossen Online-Händlern ausverkauft.

Es waren viele kleine Schritte: die erste E-Mail-Adresse, das erste Analogmodem, der erste Breitbandanschluss, das erste Selfie, der erste Video-Chat. Zu Beginn vergangener Woche, nachdem der Bundesrat uns in eine «ausserordentliche Lage» versetzt hatte, sind viele von uns bei der Digitalisierung wiederum ein kleines Schrittchen weitergegangen. Sie haben sich im Home-Office eingerichtet und angefangen, ihr Berufsleben und auch private Beziehungen computertechnisch neu zu organisieren. Es war nur ein kleines Schrittchen, aber es wurde eine Schwelle überschritten. Wir sind in eine neue Ära eingetreten. Willkommen im Internetzeitalter.
Atome durch Bits ersetzen

Wie es wäre, wenn man im Internetzeitalter leben würde, glaubten wir schon lange zu wissen. Die Geschichten, die dieses Leben beschreiben, sind älter als das Internet. Unter dem Eindruck des Ersten Weltkriegs hat ein naturwissenschaftlich gebildeter Theologe (Pierre Teilhard de Chardin) die Entstehung einer Gegenwelt beschrieben, die er sich als «sozialen Denkapparat», als ein «Gewebe» von menschlichen Gehirnen vorstellte. Seine Ideen wurden von einem kanadischen Medienwissenschafter (Marshall McLuhan) popularisiert, der in den 1960er Jahren glaubte beobachten zu können, wie der Mensch sein Nervensystem in elektronische Netze überträgt und wie diese Netze neue Räume entstehen lassen. Von diesen Visionen liessen sich amerikanische Science-Fiction-Autoren (William Gibson) inspirieren, aber auch die Sachbuchautoren nahmen Notiz.

Als besonders wirkungsmächtig erwies sich in den 1990er Jahren «Being Digital», das Buch des MIT-Professors Nicholas Negroponte. Es beschreibt eine globale Revolution, die nicht mehr rückgängig zu machen sei und die darin bestehe, dass Atome durch Bits ersetzt würden. Dass dieses kommerziell sehr erfolgreiche Buch nicht richtig verstanden worden ist, zeigt sich daran, dass jetzt, wo wir im Internetzeitalter angekommen sind, viele Menschen nichts Wichtigeres zu tun haben, als WC-Papier zu horten.

Einst imaginierte man sich das Internet als friedliche Gegenwelt, als himmlische Stadt oder – bei McLuhan – als «globales Dorf», in dem wir alle die «Gemütlichkeit des Stammeslebens» erleben könnten. Der Cyberspace, so erhoffte man sich, würde die Menschen einander näherbringen. Doch heute schätzen wir an diesen Formen der Telekommunikation etwas anderes: dass sie es uns erlauben, zusammenzuarbeiten, ohne einander nahe zu kommen; sie sind uns ein Medium für das Social Distancing.

Der Rückblick auf die Internet-Visionen vergangener Zeit macht deutlich, dass Innovationen zu Beginn oft überschätzt, in ihren langfristigen Wirkungen aber oft unterschätzt werden. Die hochfliegenden Visionen der Internet-Schwarmgeister haben sich in nichts aufgelöst. Menschen sind halt Menschen, ein sauberer Hintern ist ihnen wichtiger als die digitale Apotheose.
Der Zauber des Anfangs

Nach der Internet-Euphorie der 1990er Jahre, nach dem Digitalisierungsfrust der 2010er Jahre ist nun Pragmatismus angesagt. Wir sind im Internet angekommen, aber die Reise ist nicht zu Ende. Wir erleben den Zauber eines Neubeginns. Wenn man sieht, wie erfinderisch jetzt in der Not viele Menschen die Kommunikationsmöglichkeiten des Internets nutzen, um nicht nur sich selbst, sondern auch anderen zu helfen, könnte man wieder ins Schwärmen geraten.

Der Übertritt ins neue Zeitalter gelang nur stolpernd: Einige Internet-Service Provider wurden durch die grosse Nachfrage überrascht. Doch die Anfangsschwierigkeiten konnten rasch überwunden werden. Seitdem in den 1970er Jahren der Aufbau des Internets begonnen hat, ist es das erste Mal, dass diese Infrastruktur so umfassend einer Belastungsprobe unterzogen wurde. Noch vor wenigen Jahren wäre das Internet in einer solchen Situation überfordert gewesen. Wenn jetzt sehr viele Menschen erfahren, dass man sich auf diese Technik verlassen kann, dass sie das Leben bereichert, dass sie existenziell wichtig ist, dann wird das der Digitalisierung einen Schub verleihen. Wir können nicht mehr zurück.";https://www.nzz.ch/meinung/wie-wir-ins-internet-zeitalter-hineingestolpert-sind-ld.1547942;NZZ;Stefan Betschon;;;
06.03.2020;Wo die repräsentative Demokratie versagt, übernehmen bald Algorithmen die Staatsgewalt;"Die Bildung einer Regierung wird in unseren parlamentarischen Demokratien zunehmend zum Schauspiel, zur Tragödie, zuweilen zur Farce. Natürlich zählen Kompromisse und Koalitionen seit je zum Instrumentarium politischer Machterhaltung. Doch was wir nördlich oder südlich der Alpen beobachten können, hat eine neue Qualität. Da wird das politische System als das vorgeführt, als was es inzwischen viele sehen: einen Schatten seiner eigenen Bestimmung, die darin bestünde, den Volkswillen zu repräsentieren und Frieden auf Zeit zu schaffen.

Die Vorkommnisse in Thüringen, Italien oder Israel sind ein Beispiel für die Verkommnisse der politischen Ordnung, die den Staat ausmacht. Sie sind auch ein konkretes Signal, dass man sich auf gravierende Veränderungen des politischen Systems einstellen muss. Doch wo die analoge politische Gestaltung des Staatswesens versagt, da wird die Technologie mit ihrem effizienten und agnostischen Zugriff auf alle verwendbaren Daten Schritt für Schritt übernehmen.
Die Gamification der Politik

Das ist ein Treppenwitz der Staatsgeschichte. Wird häufig vor dem Einfluss der Technologie, vor allem bei Wahlen, als einem gefährlichen Spiel gewarnt, könnte sich die Logik bald genau umgekehrt entfalten: Die von Menschen gemachte Politik erweist sich als neue Form von Gamification. Macht wird zum Spiel, dessen Regeln den politischen Willen überlagern, der von den Bürgerinnen und Bürgern ausgeht.

Das ist, zugegeben, nicht ganz neu. Der Politikwissenschafter Ulrich Sarcinelli hat die Anfänge dieser Degeneration politischer Repräsentation bereits Mitte der achtziger Jahre als «symbolische Politik» beschrieben. Der US-Ökonom Paul Krugman sah uns 2017 im «Zeitalter der Fake-Politik» angekommen. Der libertäre Internetunternehmer Peter Thiel kommt gar zu dem Ergebnis, dass «Freiheit und Demokratie nicht mehr länger vereinbar» sind.

Ruhiger lässt sich konstatieren: Das parteipolitische System ist immer ineffizienter geworden. Es spiegelt an vielen Stellen nicht mehr den Willen der Bürger und korrumpiert den Grundgedanken, der das Wesen des demokratischen Staates beschreibt: Das Volk übt die Staatsgewalt aus. Bisher musste man das irgendwie hinnehmen. Doch Technologie könnte hier mal ganz anders wirken als erwartet: als Disruption der Degeneration.

In nahezu allen Lebensbereichen hat Technologie in den vergangenen Jahren dazu beigetragen, Entscheidungen datenbasiert besser zu machen, um schneller auf veränderte Rahmenbedingungen reagieren zu können. Nur in der praktischen Umsetzung von Demokratie durch Wahlen ist sie noch immer nicht angekommen. Naiv ist, wer glaubt, der technologische Fortschritt werde sich nicht auch die Entscheidungsprozesse der Staatslenkung und -verwaltung zu eigen machen. Vielleicht ist das, gemessen am heutigen Stand der Erkenntnis, nicht mehr Dystopie, sondern Utopie der demokratischen Rettung?
Asimov sah es kommen

Der amerikanische Professor und Science-Fiction-Autor Isaac Asimov hat in der Kurzgeschichte «Franchise» (1955) die frühe Version einer «elektronischen Demokratie» entworfen. In dieser entscheidet der zufällig ausgewählte Amerikaner Normal Muller über die politischen Geschicke des gesamten Landes.

Ihm werden Fragen gestellt, und die Antworten darauf werden mithilfe des Computers Multivac ausgewertet und auf die Wahlpräferenzen der gesamten Bevölkerung hochgerechnet. Muller ist stolz, dass durch ihn die amerikanische Bevölkerung in die Lage versetzt wird, «frei und ungehindert ihr Wahlrecht auszuüben». Algorithmische Prognostik ersetzt individuelle Stimmabgabe.

So würde das heute sicher nicht aussehen. Denn die technischen Möglichkeiten der Datenauswertung reichen inzwischen viel weiter, als Isaac Asimov sich das Mitte der fünfziger Jahre vorzustellen vermochte. Längst lassen sich über Analysen von Twitter-Daten, Google Trends und anderen grossen digitalen Datensätzen ziemlich genaue Prognosen darüber erstellen, wie Menschen einkaufen, investieren und sich sonst so verhalten. Auch Wahlausgänge lassen sich vorhersagen.

So hat Univac 111, der erste kommerzielle Grosscomputer in den USA, schon 1952 auf Basis einer Stichprobe von einem Prozent der Wahlbürger korrekt den Erdrutschsieg Eisenhowers vorhergesagt, während die meisten Umfragen Stevenson vorne sahen. Bei der US-Präsidentschaftswahl 2016 sahen fast alle Meinungsforschungsinstitute Hillary Clinton vorne, die südafrikanische Firma Brandseye sagte einen Wahlsieg Trumps voraus.

Die Datenfirma analysiert per Algorithmus weltweit Tweets auf Stimmungslagen hin und prognostizierte den Sieg Trumps wie auch zuvor schon die Brexit-Entscheidung der Briten. Obwohl längst nicht alle betroffenen Bürger auf Twitter unterwegs sind, erlauben die zugrunde liegenden Datensätze erstaunlich präzise Prognosen.

Eine algorithmische Wahl, gestützt auf die Rechen- und Prognosekapazitäten künstlich intelligenter Systeme, könnte hinreichend genau beschreiben, was die Bürger wollen. Die Berechnungen liessen sich permanent auf Basis wachsender Datenmengen und immer zeit- und passgenau durchführen. Damit trügen sie auch den Veränderungen der Meinungsbildung Rechnung, die jederzeit bei einer Entscheidung auch kurzfristig möglich sind. Nicht ein ausgewählter Prototyp käme zu Wort, sondern jede Präferenz hätte theoretisch die gleiche Chance, in einem automatisierten Entscheidungsprozess berücksichtigt zu werden.

So ungewöhnlich dieser Gedanke erst einmal sein mag: Ist es wirklich vorstellbar, dass alle Lebensbereiche, das Einkaufen, die Partnersuche, die Jobsuche, zunehmend durch Algorithmen gesteuert werden, während die Entscheidungsfindung in Staat und Politik in der vordigitalen Unzulänglichkeit des Staatswesens, im menschlichen Makel steckenbleibt?
Volkswille statt Parteiinteressen

In einer Umfrage des Center for the Governance of Change unter 2500 Erwachsenen in Grossbritannien, Spanien, Deutschland, Frankreich, Italien, Irland und den Niederlanden sagte im Frühjahr 2019 ein Viertel der Befragten, politische Entscheidungen sollten lieber durch eine künstliche Intelligenz als durch Politiker getroffen werden. Das spiegelt zum einen den Vertrauensverlust, der Institutionen und ihren Repräsentanten seit einiger Zeit entgegenschlägt. Es spiegelt aber auch die Vorstellung, dass technologisch gestützte Entscheidungen vielleicht genauer, treffender oder gar gerechter sein könnten.

Das liesse sich beispielsweise auch am Haushaltsrecht des Parlaments erproben. Die Zuteilung der Mittel hängt nicht selten auch davon ab, wie durchsetzungsfähig ein Minister oder eine Ministerin ist. Ebenfalls davon, welche parteipolitischen Interessen mehr oder weniger Gewicht haben. Die Interessen der Bevölkerung können sich davon durchaus unterscheiden. Würde man einen Haushaltsentwurf auf der Grundlage datenbasierter Bedarfsanalysen durch ein KI-System erstellen lassen, die Bedürfnisse des Volks rückten anstelle parteipolitischer Interessen wieder in den Vordergrund.

Der Aufschrei aller Nostalgiker und Technophobiker schallt schon aus der Zukunft heran: Wie kann man es wagen, dem Volk und dem Parlament sein jeweils höchstes Recht zu nehmen? Das ist einerseits ein legitimer Einspruch. Aber er greift anderseits längst ins Leere.

Denn die Gamification des Staates hat beide Rechte weitgehend ausgehöhlt. Mithilfe von algorithmischen Vorausberechnungen könnte man die Logik des demokratischen Entscheidungsrechts umdrehen: Eine KI erarbeitet die Entscheidungsvorlagen auf Basis von Big Data. Das Volk und das Parlament stimmen dann darüber ab. So liesse sich ein demokratischer Vorbehalt in einem System garantieren, das von der perfektionierten Prognostik profitiert.

Eine wesentliche Voraussetzung dafür ist bis jetzt nicht gegeben: Die Daten müssten um die Verzerrungen bereinigt werden, die eine künstliche Intelligenz oft genauso wenig gerecht machen wie eine menschliche. Das wird nicht gelingen, wenn auf Basis von Daten der Vergangenheit in die Zukunft extrapoliert wird.

Wenn man dem Staatswesen aber eine Stunde null der Datensammlung gönnte, um ab dann ein neues, inklusives Datenrepositorium aufzubauen, wären die Vorzeichen andere. Die Qualitätssicherung dieser Daten müsste Verfassungsrang erhalten, die Aufsicht über sie höchsten Ansprüchen genügen.

Digitalisierung macht das Leben an vielen Stellen direkter. Dem werden sich auch Staat und Politik nicht dauerhaft entziehen können. Eine algorithmische Repräsentation politischer Präferenzen würde politisches Entscheiden auf die Ebene einer digitalen Direktdemokratie heben. Mit einem Abstimmungsvorbehalt für die errechneten Massnahmen bliebe der Mensch immer im Loop – allerdings am Ende der Entscheidungskette.

Was geschieht, wenn er am Anfang steht, haben wir jetzt lange genug als Sinnentstellung des politischen Systems beobachten können.";https://www.nzz.ch/feuilleton/die-zukunft-des-staates-uebernehmen-bald-algorithmen-ld.1543920;NZZ;Miriam Meckel;;;
01.02.2019;In Chinas Südwesten gedeiht der gläserne Mensch;"Der Regenbogen ist in dieser zerklüfteten Karstlandschaft sozusagen zu Hause. Nebel und Wolken hängen oft zwischen den pittoresken Bergkegeln, die sich schroff auftürmen und abfallen wie in der chinesischen Tuschmalerei. Die Flüsse schlängeln sich zwischen den Hügelketten hindurch, die sich aneinanderreihen bis an den Horizont. Da und dort sind sattgrüne Reisterrassen in die steilen Hänge gemeisselt, schmiegen sich Dörfer in die Flanken und Täler.  Ein Sprichwort besagt, dass man in der Provinz Guizhou alle vier Jahreszeiten an einem Tag erleben könne. Ein anderes, dass es hier keine drei Fuss flachen Landes, keine drei Tage ohne Regen und keinen Menschen mit drei Yuan gebe. Ein drittes, dass die Menschen hier ohne Reiswein nicht wüssten, wie glücklich sie sind.

Billy Zhang, der uns die Tür zu dieser Welt öffnet, drückt es prosaischer aus: «Wenn der Himmel in meiner Kindheit einmal strahlend blau war, setzte meine Mutter mich draussen nackt auf einen Stein und suchte mich nach Läusen ab.» Er erzählt von bitterer Armut, einer Mahlzeit pro Tag und manchmal solchem Hunger, dass seine Familie Ratten ass. Der Vater zog in der Not nachts los, um Fische – angelockt von einem glimmenden Harzstück – mit blosser Hand zu fangen. Nur eine seiner fünf Schwestern habe die Schule besucht. Der drahtige Mittvierziger mit Cowboygang ist ein Miao. Das Bergvolk, das auch in den Bergen von Vietnam, Laos und Thailand heimisch ist und dort Hmong geheissen wird, bildet die grösste Minderheit in der Provinz Guizhou, die ungefähr viermal so gross ist wie die Schweiz, flächen- wie bevölkerungsmässig. Die Miao und ein Dutzend andere ethnische Minderheiten – von den Yao über die Dong bis hin zu den Buyi – machen gut einen Drittel der Bevölkerung aus (nur in Tibet und Xinjiang, wo die Uiguren heimisch sind, ist der Anteil noch grösser). Ein Zehntel der 36 Millionen Einwohner kann weder lesen noch schreiben. Die Provinz im Südwesten Chinas war vor vier Jahren denn auch noch die ärmste des Landes, mit einem Pro-Kopf-Einkommen von jährlich 4297 Yuan, umgerechnet 615 Franken.
Der Schlüssel zur Revolution

Die Tradition wird von den Bergvölkern seit je mündlich weitergegeben, wie Billy Zhang erläutert. Die Natur gilt ihnen als beseelt. Guizhou ist ein Landstrich der Mythen und Legenden, wo Geister hinter Felsvorsprüngen oder unter Bananenstauden lauern. Der grösste Mythos von allen aber ist Mao Zedong und die grösste Legende jene vom Langen Marsch.  Das kam so: Die Rote Armee war im Bürgerkrieg von den Streitkräften Tschiang Kai-scheks in Südchina eingekesselt, als sie im Oktober 1934 die feindlichen Linien durchbrach und sich durch einen Gewaltmarsch in schwer zugängliches Gebiet zu retten suchte. Rund 90 000 Soldaten brachen auf, nur 7000 von ihnen erreichten nach 12 500 Kilometern ein Jahr später das Ziel in Yan’an, in Nordwestchina, wo sie ihre kommunistische Basis errichteten.

So zumindest ist es in die chinesische Geschichte eingegangen. Die Oberkommandierenden liessen sich über weite Strecken auf Bettgestellen tragen. Den wichtigsten Zwischenhalt – etwa auf halber Strecke – legten die roten Truppen in Guizhou ein. An der Konferenz in Zunyi, einer grauen Stadt im Nordwesten der Provinz, geisselte Mao die bisherige Kriegsführung und stieg zum neuen Führer auf. Zunyi markierte die Abkehr von der Doktrin der Kommunistischen Internationale, die auf eine Revolution der Arbeiter setzte, hin zu den Bauern und zur Guerillataktik. Es war der Schlüssel zum Erfolg, zur Revolution, die zur Gründung der Volksrepublik China am 1. Oktober 1949 führte. Mao und Genossen stiessen darauf mit Maotai an, dem hochprozentigen Schnaps aus dem gleichnamigen Ort in Guizhou. Die revolutionäre Geschichte, sie spielte in Peking vor fünf Jahren wahrscheinlich mit beim Entscheid, Guizhou zum nationalen Daten- und Online-Zentrum zu entwickeln. Staats- und Parteichef Xi Jinping will China zur «Cybermacht» formen. Er hat die Nation dazu aufgerufen, den «Expresszug des Internets zu besteigen». Es gibt kein Halten mehr. Die von Peking und Schanghai mehr als anderthalbtausend Kilometer entfernte Provinz wurde 2016 zur nationalen Big Data Experimental Zone erklärt. Wo einst Kohle abgebaut wurde, bohren IT-Konzerne wie Apple, Tencent und Huawei jetzt Tunnel für Datenzentren in die grünen Hügel um die Provinzhauptstadt Guiyang. Das lausige Wetter ist ein Bonus: Es hilft, den Stromverbrauch für die Klimatechnik in Grenzen zu halten.
Der gläserne Chinese

Rund 9000 Unternehmen beschäftigen sich heute in der Provinz mit Internet, Online-Business und Big Data. Nicht alle sind aus eigenem Antrieb in Guizhou. Apple zum Beispiel, das grösste börsenkotierte Unternehmen der Welt, wurde von der chinesischen Regierung vor die Wahl gestellt, entweder alle iCloud-Daten seiner chinesischen Kunden in der Volksrepublik zu speichern oder aber auf das Geschäft in dem 1,4 Milliarden Einwohner zählenden Land zu verzichten. Der amerikanische Konzern hat gekuscht. Die Daten und die kryptografischen Schlüssel dazu werden jetzt von einem Joint Venture von Apple und dem chinesischen Staatsbetrieb Guizhou-Cloud Big Data Industry verwaltet. Das ist heikel, denn einen Datenschutz, der den Namen verdient, gibt es in China nicht. Die Behörden haben leicht Zugriff auf die Datensammlungen der Unternehmen. Und in Guizhou teilen sie ihre Datenmassen, Big Data, auch bereitwillig mit privaten Unternehmen. Erfasst und analysiert werden persönliche Eckdaten, Bewegungsprofile, finanzielle Transaktionen, Konsum- und Kontaktmuster – und mit Algorithmen werden diese Daten schliesslich in Beziehung zueinander gesetzt.

    Einen Datenschutz, der den Namen verdient, gibt es in China nicht. Die Behörden haben leicht Zugriff auf die Daten der Unternehmen.

So hat zum Beispiel das Startup Xinge Technology unter Einsatz von Drohnen eine 3-D-Karte von Wohnkomplexen entwickelt, die Informationen über die Bewohner der einzelnen Zimmer liefert, die Farbe Rot signalisiert einen verurteilten Straftäter. Und eine Reportage des Deutschlandfunks zeigt, dass die Firma den Behörden auch gerne Empfehlungen gibt, wo sie Überwachungskameras anbringen sollten.

Ganze Viertel sind in Guiyang über Nacht aus dem Boden geschossen. Spiegelverglaste Forschungszentren und gesichtslose Hochhäuser vermitteln den Eindruck einer Retortenstadt. 70 000 Server, so schreibt die «China Daily» begeistert, stünden mittlerweile in der nationalen Sonderzone. Die zweieinhalb Millionen Einwohner der Stadt könnten 180 Dienstleistungen über ein einziges Konto beziehen. Es sei gelungen, die Daten von 54 verschiedenen Ämtern und Instituten zusammenzuführen. Die zeitliche Verzögerung im Internet sei von 30 auf 3 Millisekunden gesenkt worden. Und das Wirtschaftswachstum der Provinz schnellte hoch auf über 10 Prozent, mehr als irgendwo sonst in China.
Irrwitzige Jagd nach Rekorden

Von Guangzhou und Chengdu, den Hauptstädten der Provinzen Guangdong und Sichuan, aus, ist Guiyang heute im Hochgeschwindigkeitszug bequem in vier Stunden zu erreichen. Auch einige der teuersten Autobahnen des Landes – mit Tunnels und Viadukten ohne Zahl – sind durch die Provinz gezogen worden. Sechs der zehn höchsten Brücken weltweit erheben sich heute in Guizhou. Und auch das Autobahnkreuz bei Guiyang sucht weltweit seinesgleichen: Das mehrstöckige Gewimmel von Strassen erinnert an eine extravagante Carrera-Bahn. Und als wäre das nicht schwindelerregend genug, hat Gouverneur Sun Zhigang versprochen, 17 Flughäfen, 4000 Kilometer an Highspeed-Zugstrecken und 10 000 Kilometer Autobahn bis ins Jahr 2020 zu bauen. The sky is the limit – die Entscheidungsträger scheinen es wörtlich zu nehmen.

Ob entsprechender Bedarf besteht, ist eine andere Frage. Nur tröpfchenweise rollen Fahrzeuge über die gebührenpflichtigen Autobahnen. Sie sind auch für die Benutzer sündteuer. Meist sind es bullige SUV made in China mit Marken wie Lynk & Co, Landwind oder Trumpchi. Ein Hauch von Wildem Westen liegt über der Provinz, es ist schwer zu verkennen.

Er geht einher mit Gigantomanie. Die Marktflecken scheinen einander mit Monumenten übertrumpfen zu wollen: Da wird ein riesiger Heldenfries in eine Felswand gehauen, dort einer legendären Schönheit eine 88 Meter hohe Statue errichtet – ein Denkmal scheusslicher als das andere. Und dabei ist auch der Schuldenberg der Provinz so angewachsen wie nirgends sonst in China, wo die Verschuldung insgesamt laut der Bank für Internationalen Zahlungsausgleich alarmierende 250 Prozent des Bruttoinlandprodukts überschritten hat. Sobald man von der Autobahn ab und in die Hügel fährt, wird die Fortbewegung allerdings mühsam. Die Strassen winden sich schmal durchs Gelände, werden in der Monsunzeit oft von Bergstürzen verschüttet und sind auch Monate danach nicht immer geräumt. Die Natur ist und bleibt eine Herausforderung in dieser atemberaubenden subtropischen Landschaft. Manche Dörfer sind nur in stundenlangem Fussmarsch zu erreichen. Die traditionellen Holzhäuser werden hier im Winterhalbjahr gemeinschaftlich gebaut – und die Helfer werden im Gegenzug grosszügig verköstigt. Ein Fest.

Und einige Dörfer entziehen sich dem Befehl von oben. Die Leute von Zhongdong zum Beispiel wohnen in einer Höhle, obwohl die Behörden alles versucht haben, sie herauszulocken. Sogar Betonhäuser wurden den Leuten weiter unten am Hang gebaut, wie Billy uns zeigt. Die Geister aber sprachen sich gegen den Umzug aus – über ihr Sprachrohr, einen Schamanen.

    Es gibt in Guizhou zwar keinen anderen Gott als die Kommunistische Partei, aber die Bergvölker glauben an Geister.

Es gibt in Guizhou zwar keinen anderen Gott als die Kommunistische Partei, aber die Bergvölker glauben an Geister. Und sie suchen den Rat von Medien, die zwischen Dies- und Jenseits vermitteln. Bänder an den Bäumen, Blutspuren von geopferten Tieren auf den Pfaden – die Anzeichen sind in den Dörfern allgegenwärtig. Und in grösseren Ortschaften finden sich auch Schamanen mit Vorzimmern, wo die Hilfesuchenden warten wie andernorts beim Arzt.
Ein Mann, vier Namen

Billy Zhang ist der Erste seines Dorfes, der Englisch gelernt und die Universität besucht hat. «Das habe ich Mao zu verdanken», sagt der Mann mit dem kurzgeschorenen Haar. «Erst die Kulturrevolution hat uns Entwicklung und den Anschluss an die Welt gebracht.» Er meint jene gewalttätige Zeit 1966 bis 1976, als Mao seine parteiinternen Kritiker durch Massenkampagnen auszuschalten suchte – und den Klassenkampf bis in die Familien hineintrug. Drangsaliert wurden vor allem die Intellektuellen, «die stinkende neunte Klasse», wie Mao sie nannte. Mindestens 400 000 Menschen fielen den Foltermethoden in jenen Jahren zum Opfer. Schätzungsweise 17 Millionen Jugendliche wurden aus den Städten aufs Land geschickt. Billy erinnert sich an einen aus Schanghai in sein Dorf verpflanzten jungen Mann, der sich auf den steilen Feldern so linkisch anstellte, dass er keine Hilfe, sondern eine Belastung war. Schliesslich hätten die Dorfbewohner den Mann aus der Grossstadt gebeten, doch lieber die Kinder zu unterrichten. So habe er lesen und schreiben und die ersten englischen Wörter gelernt, sagt Billy. Auf den Einwand, dass er nur Unterricht erhalten habe, weil die Dorfbewohner die Weisung der Parteifunktionäre missachteten – unter eigenem Risiko –, schaut er überrascht und schweigt.

Der Ungereimtheiten sind viele. Es fängt, um Konfuzius zu bemühen, bei der korrekten Bezeichnung, dem Namen, an. Billy – so hat ihn vor ein paar Jahren einfach ein Tourist genannt, der sich seinen chinesischen Namen Zhang Zhongbiao, den er in der Schule erhalten hatte, nicht merken konnte. Und als dann andere ausländische Touristen aufgrund eines Tripadvisor-Eintrags nach Billy Zhang suchten, realisierte er zunächst gar nicht, dass er gemeint war. Und das ist nur ein Teil der Geschichte.

Nach vier Töchtern habe seine Mutter eine Schamanin um Hilfe gebeten, berichtet unser Reisegefährte. Diese erklärte, um einen Sohn zu bekommen, müsse sie etwas Lebendiges zu sich nehmen. Es fanden sich zwei Spinnen, die in Reiswein getunkt wurden. «Wenn du dieses Glas mit zwei Spinnen trinkst, bekommst du zwei Söhne», versprach die Schamanin. Die Mutter schaute sich die haarigen Spinnen an und entschied, ein Sohn genüge. Als ein Jahr später tatsächlich ein Stammhalter geboren wurde, durfte die Schamanin ihm zum Dank den Namen geben. Sie nannte ihn Ye, Stein in der Sprache der Miao.
Hunde gelten als Delikatesse

Allein, der kleine Junge weinte wochenlang, vielleicht litt er an Koliken, so dass die Mutter erneut die Schamanin aufsuchte. Diese steigerte sich in eine Trance hinein und liess den Geist des Grossvaters aus ihrem Mund sprechen: Der Name sei falsch, der langersehnte Junge sei doch der Himmel, Vai. So, sagt unser Begleiter, werde er von der Familie und von Freunden genannt.

Es ist, mit anderen Worten, kompliziert. Wie nur sind die Parallelwelten in Guizhou miteinander in Einklang zu bringen, die im Kopf einfach nicht zusammenzubringen sind? Es bleibt ein Geheimnis.

Allein die 9,5 Millionen Miao in Südwestchina zerfallen in drei Dialekt- und insgesamt 86 Untergruppen, die sogar Vai nicht alle voll versteht. Die Traditionen sind lokal, die wenigsten Bergbewohner kennen die Welt jenseits ihres Distrikts. Die Ausflüge beschränken sich üblicherweise auf stundenlange Fussmärsche zu den Wochenmärkten – mit der Ware an Tragstangen. Da sind zum Beispiel die rund 5000 Langhorn-Miao, die an Festtagen riesige Perücken tragen: Wollbahnen, zusammengesponnen aus den Haaren von Ahnen und Tieren, werden dafür um ein Tierhorn oder eine Holzsichel geschlungen. Oder die Gulong-Miao, deren glänzende Festtagstrachten mit Schweineblut und Eiweiss gefärbt und über mehrere Frauengenerationen bestickt wurden mit grossblättrigen Blumen, knopfäugigen Wesen und geometrischen Mustern. Völkerkundemuseen und Sammler rund um die Welt reissen sich um die einmaligen Kleidungsstücke, weshalb an Festtagen zunehmend maschinell gefertigte Trachten zu sehen sind.

    Die Küche der Bergvölker ist rau wie ihr Leben. Im Sommerhalbjahr arbeiten sie mehr oder weniger durch, in der kühleren Jahreszeit hauen sie auf den Putz.

Nicht weniger spektakulär ist der Schmuck. Die Eltern von Mädchen, so verlangt es die Tradition, sollten ab der Geburt für Silberschmuck sparen. Insbesondere das Putzwerk auf dem Kopf sieht je nach Untergruppe unterschiedlich aus und reicht von Haarnadeln, an deren Ende Vögelchen sitzen, bis hin zu Hauben, voll behängt mit feinen Blättern. Und das Ganze wird womöglich noch gekrönt mit einem Aufsatz in Form zweier Hörner oder eines Baums. Bis zu zehn Kilogramm wiegt der Kopf- und Halsschmuck, unter dem ledige Frauen an Volksfesten zur Brautschau stehen. Wasserbüffelkämpfe laden zu Wetten ein. Und die Hundekadaver liegen reihenweise zum Zerhacken und zum Verzehr in scharfsaurer Suppe aus.

Die Küche der Bergvölker ist rau wie ihr Leben in dieser Landschaft. Im Sommerhalbjahr arbeiten die Leute mehr oder weniger durch, da haben sie weder Energie noch Zeit zum Feiern. In der kühleren Jahreszeit hauen sie dafür auf den Putz. Wir besuchen ein kleines Erntedankfest, das im jährlichen Turnus zwischen drei Dörfern stattfindet und zu dem die Frauen ihre Trachten in überdimensionierten Taschen mitbringen und nur bei trockenem Wetter überhaupt anziehen für einen kurzen Ringelreihen. Und wir besuchen grosse Volksfeste mit Zehntausenden von Besuchern, die dem beliebtesten Musikinstrument der Miao, der aus fünf oder sechs Bambuspfeifen bestehenden Lusheng, gewidmet sind. Die trötende Musik, am ehesten vergleichbar mit dem Klang der Dudelsäcke, wird hopsend zelebriert, so dass die Röcke fliegen und der Schweiss fliesst.

Und zum Übernachten fahren wir dann in eine Stadt, die sich wie aus dem Nichts zwischen den Hügeln erhebt – und wo wir bereits auf der Einfallstrasse von Überwachungskameras erfasst werden. Anshun zum Beispiel, einst Umschlagplatz des von den Bergvölkern angebauten Opiums, heute ein Zentrum der chinesischen Luftfahrtindustrie. Der neue chinesische Kampfjet FTC-2000 G mit einer Reichweite von 2400 Kilometern, der drei Tonnen Bomben, Raketen oder Missile zu transportieren vermag, wird hier gebaut. Bei aller Wildheit – wir werden wie Objekte in einem gigantischen Logistikunternehmen bewegt und verfolgt. Der Pass dient der eindeutigen Identifizierung beim Kauf eines Zugbilletts, beim Buchen eines Hotelzimmers und selbst beim Lösen eines Eintrittstickets zu Freiluftmuseen. Ohne geht gar nichts. Und damit alle Informationen schön eindeutig bleiben, ist es in China nicht möglich, zwei Hotelzimmer für dasselbe Datum zu buchen – etwa, weil man sich noch nicht sicher ist, wo man genau sein wird. Die Registrierung ausländischer Gäste ist dem Vernehmen nach so aufwendig, dass viele Hotels lieber auf die Fremden verzichten.

    Die «Great Firewall» blockiert Google, Yahoo und Twitter, Whatsapp, Facebook und Youtube. Aber dafür gibt es WeChat

Die «Great Firewall» blockiert Google, Yahoo und Twitter, Whatsapp, Facebook und Youtube. Aber dafür gibt es WeChat, das die Funktionen eines Messaging-Dienstes mit den Möglichkeiten von Facebook, Uber, Booking.com und Lieferando vereint und das bargeldlose Bezahlen fast überall ermöglicht. Das ist praktisch für den Benutzer wie für die Staatssicherheit. Kaum jemand zahlt im Supermarkt um die Ecke mehr bar – auch wenn die Tagelöhner, die davor mit ihren Werkzeugen, Latten und Rohren stehen, vielleicht ein anderes Bild vermitteln. Bereits wird getestet, ob sich WeChat auch als elektronischer Sozialversicherungs- und Personalausweis eignet.

«Die Jungen schwärmen in die Stadt wie die Fische nachts zum Licht», sagt Vai auf der nächsten Fahrt in die Berge unvermittelt. «Meine Welt ist am Verschwinden.» Am Rückspiegel des Autos baumelt derweil ein goldenes Herz, das Mao Zedong zeigt wie einen jungen Gott, schlanker und attraktiver, als er je war.";https://www.nzz.ch/international/in-chinas-suedwesten-gedeiht-der-glaeserne-mensch-ld.1452697;NZZ;Manuela Kessler;;;
29.03.2019;Microsoft automatisiert die Speicherung von Informationen in DNA-Molekülen;"Die DNA ist die Trägerin des Erbguts. Der komplette Bauplan des Lebens ist in der wechselnden Abfolge von vier Basenpaaren gespeichert, die zusammen mit anderen chemischen Bausteinen das Rückgrat dieses Biomoleküls bilden. Eines der hervorstechenden Merkmale dieses biologischen Informationsspeichers ist seine Dauerhaftigkeit. Zwar kommt es im Erbgut immer wieder zu spontanen Mutationen. Doch in fossilen Knochen findet man auch nach Jahrhunderten noch halbwegs intakte DNA-Fragmente.
Speichertechnik der Natur

Das weckt das Interesse von Forschern, die sich mit der Speicherung von binären Informationen beschäftigen. Wichtige Daten, die langfristig erhalten werden sollen, werden heute vor allem auf Magnetbändern gespeichert. Diese halten jedoch im besten Fall ein paar Jahrzehnte. Deshalb denken Forscher schon seit geraumer Zeit darüber nach, die DNA als dauerhaftes Speichermedium zu nutzen. Einen wichtigen Schritt in diese Richtung hat nun ein Forscherteam der University of Washington in Seattle und des amerikanischen Softwarekonzerns Microsoft gemacht. Die Gruppe hat ein System entwickelt, das Daten automatisch in künstlich hergestellten DNA-Strängen ablegt und sie später wieder abrufen kann. Zu Demonstrationszwecken haben die Entwickler das Wort «Hello» gespeichert und wieder decodiert.

Neu ist nicht das Speichern der Daten an sich. Dem Forscherverbund ist es bereits vor rund zweieinhalb Jahren gelungen, 200 Megabyte Daten in DNA zu schreiben. Mittlerweile ist ein Gigabyte möglich. Der jüngste Erfolg liegt vielmehr in der Automatisierung des Verfahrens und wird von den Wissenschaftern in einer Pressemitteilung als «entscheidender Schritt» gefeiert, um die Technik aus dem Labor in kommerzielle Rechenzentren bringen zu können. «In einem Datencenter können Leute ja nicht mit Pipetten herumlaufen – das wäre zu anfällig für menschliche Fehler, zu kostspielig und zu kompliziert», erklärt Chris Takahashi, Senior Research Scientist der University of Washington.
Automatisiert ablegen und auslesen

Um digitale Dateien in DNA zu speichern, muss Software zunächst die Nullen und Einsen der binären Codes in lange Folgen der vier Buchstaben A, C, G und T übersetzen. Diese Buchstaben stehen für die vier Basen Adenin (A), Cytosin (C), Guanin (G) und Thymin (T), aus denen die DNA besteht.

Zur chemischen Fertigung der künstlichen DNA werden die benötigten Flüssigkeiten und Elemente einer Synthesemaschine zugeführt. Diese erzeugt DNA-Stränge mit der gewünschten Abfolge der Basen. Beim Abrufen der Daten werden die Basensequenzen der DNA-Stränge wiederum ausgelesen und zurück in binären Code übersetzt. Entscheidende Teile dieser Synthetisierung und Sequenzierung konnten bereits vorher automatisch ausgeführt werden. Dem Forscherteam ist es nun aber gelungen, auch solche Zwischenschritte zu automatisieren, die bisher noch manuell erledigt werden mussten. Dadurch verringert sich die Fehleranfälligkeit des Systems.

Für diese Art der Informationsspeicherung spricht nicht nur die Langlebigkeit der DNA. Ein zweiter Vorteil ist, dass das Molekül nur wenig Platz beansprucht. Deshalb können auf kleinstem Raum Unmengen von digitalen Daten untergebracht werden. Datenmengen, die mit bisherigen Speichermethoden ein ganzes Datencenter belegen, könnten laut Microsoft in einem Volumen von ein paar Würfeln gespeichert werden.
Speicherzentren auf DNA-Basis

Entsprechend sehen die Forscher in der DNA-Speichertechnik eine vielversprechende Lösung, um der explodierenden Datenmenge Herr zu werden, die die Welt jeden Tag erzeugt – angefangen bei Geschäftsdokumenten und persönlichen Videos über medizinische Aufnahmen bis hin zu Bildern aus dem Weltall.

Das automatisierte DNA-Data-Storage-System von der University of Washington und Microsoft ist derweil noch sehr langsam. Die Informationen auszulesen, dauerte 6 Minuten – die 5 Bytes des Wortes «Hello» als künstliche DNA zu speichern, erforderte 21 Stunden. Am Ende der Entwicklung sollen jedoch DNA-Speicherzentren stehen, die so einfach funktionieren wie Cloud-Speicherdienste heute, so die Vision der Forscher.";https://www.nzz.ch/digital/dna-microsoft-automatisiert-die-datenspeicherung-in-biomolekuelen-ld.1470792;NZZ;Jochen Siegle;;;
22.07.2015;«Wissenschaft ist eine Reputationsökonomie»;"Herr Fecher, öffentlich zugängliche Archive für Forschungsdaten schiessen derzeit wie Pilze aus dem Boden. Trotzdem werden sie noch kaum genutzt. Haben die Forscher kein Interesse daran?

Punkto Infrastruktur passiert im Moment sehr viel, das stimmt. Was fehlt, sind Anreize für die Forscher, die Möglichkeiten, die sich ihnen bieten, zu nutzen. Bisher geschah das meist auf freiwilliger Basis, selten wird ein öffentlicher Zugang zu den Daten explizit gefordert. Ausserdem sind Daten für den Forscher ein wertvoller Rohstoff. Gibt er den her, muss er damit rechnen, dass andere in seinen Daten interessante Ergebnisse finden. Deshalb ist im Data-Sharing die Infrastruktur derzeit weiter als die Forscher.

Wie sorgt man bei einem solchen Wildwuchs von digitalen Archiven dafür, dass die Forscher finden, was sie suchen?

Die Lösung könnten Meta-Suchmaschinen sein, mit denen man nicht die Daten selbst, sondern deren Dokumentation durchsucht. Die Daten müssen einfach auffindbar und ihre Qualität sichergestellt sein. Derzeit wird einfach viel ausprobiert, es entstehen zahllose Archive für einzelne Disziplinen oder Institute. Was davon in zehn Jahren noch Bestand haben wird, ist aber noch völlig unklar.

Sie glauben demnach, dass sich langfristig nur wenige, gute Datenarchive durchsetzen?

Ja, denn wenn eine Plattform für Data-Sharing erfolgreich sein will, muss sie dafür sorgen, dass alles gut dokumentiert ist. Jeder Datensatz muss von beschreibenden Dateien begleitet werden und bestimmten Qualitätsanforderungen genügen. Die Entstehung der Daten muss lückenlos nachvollziehbar sein. Nur so ist ein Vergleich mit anderweitig erhobenen Daten möglich. Derzeit werden aber oft Datensätze öffentlich gemacht, die gerade das nicht leisten.

Ist es denn überhaupt sinnvoll, für alle Fachbereiche eine Offenlegung der Forschungsdaten zu verlangen?

Ich bin gegen eine rigorose Pflicht zur Veröffentlichung. Vielmehr müsste sich das Anreizsystem ändern – dahingehend, dass die Forscher das freiwillig tun. Wissenschaft funktioniert über Reputation – aber zurzeit zählen da praktisch nur verschriftlichte Studienergebnisse. Zwischenprodukte wie Daten oder Code werden hingegen noch nicht genügend gewürdigt.

Gibt es Forschungsbereiche, in denen sich der Aufwand schlicht nicht lohnt, weil eine Nachnutzung durch andere Forscher kaum zu erwarten ist?

Es geht ja nicht nur darum, neue Forschung mit alten Daten zu ermöglichen, sondern auch um die Überprüfbarkeit von Forschungsergebnissen. Sofern es keine datenschutzrechtlichen Bedenken gibt, halte ich die Veröffentlichung von Daten deshalb für generell wünschenswert – allerdings ist der Bedarf nicht in allen Fachbereichen gleich hoch. Etwa könnte die evidenzbasierte Medizin im Bereich der Krebsforschung, wo man oft viel zu wenige Patienten für eine gute Studie hat, davon immens profitieren.

Viele Forscher sträuben sich aber gerade deshalb gegen das Offenlegen ihrer Daten, weil das viel Arbeit bedeutet, die kaum oder gar nicht honoriert wird.

Dem könnte man mit guten Infrastrukturen zur Datenarchivierung begegnen. Diese müsste so aufgebaut sein, dass der Aufwand nicht mehr nur beim Forscher liegt. Ausserdem sollten Datenveröffentlichungen bei der Vergabe von Geldern und Stellen ins Gewicht fallen.

Helfen dabei neue Fachjournale wie «Scientific Data», die sich auf reine Datenpublikation spezialisiert haben?

Ja, da sie die gängigen Reputationsmechanismen der Wissenschaft bedienen. Genauso wichtig sind Datenzitationen und Datenveröffentlichungsrichtlinien. Zum Beispiel verlangt PLoS seit einiger Zeit, dass alle den PLoS-Veröffentlichungen zugrundeliegenden Daten öffentlich sein müssen. Seitdem sind Qualität und Replizierbarkeit gestiegen, weil sich kaum ein Forscher traut, öffentlich zugängliche Daten zu manipulieren.";https://www.nzz.ch/wissenschaft/bildung/wissenschaft-ist-eine-reputationsoekonomie-1.18583381;NZZ;Helga Rietz;;;
22.07.2019;In Schanghai gibt es eine chinesische Nasdaq-Version;"Chinas neue Technologiebörse ist am Montag fulminant gestartet. Die am Sci-Tech Innovation Board (Star) gehandelten Aktien von 25 Börsenneulingen legten um durchschnittlich 140% gegenüber dem Ausgabepreis zu. Gewinner des Tages war Anji Microelectronics Technology aus Schanghai. Die Valoren des auf Materialien für die Produktion von Halbleitern spezialisierten Unternehmens gingen mit einem Plus von mehr als 400% – gegenüber dem Ausgabepreis – aus dem ersten Handelstag.

In der ersten Woche dürfte es an der Technologiebörse noch einige Kursturbulenzen geben: Bis Freitag wird der Handel selbst bei extremen Preisschwankungen der Valoren nur vorübergehend ausgesetzt. Von kommender Woche an gilt die Vorgabe, dass die Aktien vom Handel ausgeschlossen werden, sobald sie gegenüber dem Eröffnungskurs um 20% zulegen oder verlieren. An den anderen chinesischen Börsen in Schanghai und Shenzhen liegt der Grenzwert bei Verlusten bzw. Gewinnen von 10% pro Tag.
Neue Finanzierungsquelle

Chinas Partei- und Staatschef hatte im November vergangenen Jahres angekündigt, dass es bald eine neue Technologiebörse in Schanghai geben werde. Wenige Monate später ist es so weit. Star wird in dem asiatischen Land denn auch bereits als chinesische Nasdaq-Version bezeichnet. «Das neue Börsensegment ist ein wichtiger Schritt im Zuge der Reformen des chinesischen Kapitalmarktes. Es bietet für die innovativen Technologiefirmen die Chance, neue Finanzierungsquellen zu erschliessen und damit schneller zu wachsen», sagt der Präsident von UBS Securities in Hongkong, Eugene Qian.

Im Fokus stehen Firmen, die in den Segmenten Big Data, Biotechnologie, Cloud-Computing, Internet, künstliche Intelligenz, neue Materialien und Energien sowie Software über Kerntechnologien verfügen und als Marktführer gelten. Sie sollten grosses Wachstumspotenzial vorweisen und nicht zu hoch bewertet sein. Die neue Technologiebörse wendet sich damit nicht an Firmen wie den finanziellen Arm des E-Commerce-Konzerns Alibaba, Ant Financial, der für das mobile Bezahlsystem Alipay verantwortlich ist; die Marktbewertung von Ant Financial soll bereits die Marke von 150 Mrd. $ überschritten haben. Über den Börsengang gibt es schon seit längerem Spekulationen. Star wird als Versuch Pekings angesehen, mehr chinesischen Firmen einen Börsengang in der Heimat schmackhaft zu machen. Alibaba, Tencent, Xiaomi und andere chinesische Technologiekonzerne haben in der Vergangenheit die Standorte Hongkong und New York präferiert. Das bisherige Prozedere für die Zulassung zum Aktienhandel in Schanghai und Shenzhen gilt als bürokratisch und langwierig. Es gibt auch Vorwürfe, das intransparente Verfahren schliesse Korruption mit ein.

Die China Securities Regulatory Commission (CSRC) wertet Star als Testlauf, um daraus Rückschlüsse für die anderen Börsen in Schanghai und Shenzhen zu ziehen. So ist in China nun erstmals Firmen der Börsengang erlaubt, die noch keine Gewinne ausgewiesen haben. Bis jetzt ist dieser Schritt nur gestattet, wenn die Unternehmen mindestens drei Jahre lang schwarze Zahlen in den Bilanzen stehen haben. Zudem gibt es für die neue Technologiebörse auch keine Obergrenze für das Kurs-Gewinn-Verhältnis, das in China seit 2014 bei einem Wert von 23 liegt; am Ende des ersten Handelstages lag diese Kennziffer für die 25 Star-Unternehmen bei 120.

Anfang März hatte der chinesische Regulierer Richtlinien veröffentlicht, wie Broker den Marktwert von Firmen berechnen, die noch keine Gewinne erzielen. So sollen sie die letzte Finanzierungsrunde des Unternehmens als Richtwert verwenden und sich an ähnlichen, an ausländischen Börsen kotierten Firmen orientieren.

Laut dem Wirtschaftsmagazin «Caixin» sind die Broker dennoch verunsichert. Sie müssen bei der Präsentation des Marktwertes das gewählte Verfahren und die gewählten Daten offenlegen. In einem nächsten Schritt wird im Rahmen eines Bookbuilding-Verfahrens geprüft, ob sich die Berechnungen der Broker mit der Nachfrage von Investoren decken. Der Chef der CSRC, Yi Huiman, hatte bereits im Februar dieses Jahres Bedenken geäussert, dass einige chinesische Broker nicht genügend Expertise haben, um solche Kalkulationen durchzuführen.
124 neue Milliardäre an einem Tag

Nach dem Börsengang von 25 Firmen am Montag sollen in naher Zukunft mindestens 120 weitere Technologieunternehmen hinzustossen, die ihre Unterlagen bereits eingereicht haben. Es wird dann auch den 50 Firmen umfassenden SSE-Star-50-Index geben, der in Abhängigkeit von der Marktkapitalisierung der Kandidaten laufend überarbeitet werden soll.

Wie überhitzt der Markt am ersten Handelstag war, zeigt der Einsatz des «circuit breaker». Mit diesem automatischen Stoppmechanismus soll die Volatilität aus dem Markt genommen werden. Die bis Ende dieser Woche geltende Regelung sieht vor, dass der Handel für zehn Minuten ausgesetzt wird, wenn der Valor gegenüber dem Eröffnungskurs mehr als 30% gewonnen oder verloren hat; eine weitere Pause von zehn Minuten greift automatisch, wenn die Aktie im Vergleich mit dem Eröffnungskurs um 60% zugelegt hat oder gesunken ist. Am Montag wurde der Aktienhandel für insgesamt 21 Firmen vorübergehend ausgesetzt.

Nach dem ersten Handelstag hat die Marktkapitalisierung von 16 der 25 Börsenneulinge die Schwelle von 10 Mrd. Yuan, was umgerechnet rund 1,43 Mrd. Fr. entspricht, übertroffen. Zudem dürfen sich 124 Chinesen nun als Milliardäre bezeichnen. Es handelt sich jedoch nur um eine Bestandsaufnahme. Die Investoren an der Technologiebörse werden sich in den kommenden Wochen erst noch finden müssen.";https://www.nzz.ch/finanzen/in-schanghai-gibt-es-eine-chinesische-nasdaq-version-ld.1497447;NZZ;Matthias Müller;;;
05.11.2019;Das Silicon Valley spaltet die Demokraten;"Es waren Schlagzeilen, die Mark Zuckerberg eigentlich zu vermeiden versucht hatte: Der Facebook-Chef habe dem Wahlkampfteam des demokratischen Präsidentschaftsanwärters Pete Buttigieg Mitarbeiter vermittelt, titelte Bloomberg vor wenigen Tagen. Auf persönliche Empfehlung von Zuckerberg und seiner Frau Priscilla Chan hin arbeiteten nun zwei Bekannte für den aufstrebenden Demokraten. In einer Facebook-Telefonkonferenz noch am gleichen Tag versuchte Zuckerberg, die Meldungen herunterzuspielen: Weder er noch Facebook würden sich hinter einen bestimmten Kandidaten stellen.

Im derzeitigen gespaltenen politischen Umfeld bemüht sich Facebook tunlichst, jeglichen Anschein einer Bevorzugung eines Kandidaten zu vermeiden. Vehement streitet Zuckerberg seit Monaten ab, sein Netzwerk zensiere die freie Rede von Konservativen; ein Vorwurf, der auch gerne von Präsident Trump kommt.
Unterstützung von der «Left Coast»

Doch tatsächlich zählt das Silicon Valley seit Jahren zu den wichtigsten Geldgebern demokratischer Politiker, konkret die meist wohlhabenden Angestellten der Technologiefirmen spenden gerne «links»: Wie eine Untersuchung der Analysefirma GovPredict für die Jahre 2004 bis 2018 zeigt, gingen 90 Prozent der Spenden in diesem Zeitraum an demokratische Kandidaten. Das erklärt sich dadurch, dass das Silicon Valley im progressiven Kalifornien beheimatet ist, und die dortige politische Einstellung spiegelt sich auch in den Mitarbeitern der Tech-Firmen. Nicht umsonst trägt die Westküste den Spitznamen «Left Coast».

Zwar spenden auch die Technologiefirmen selbst über «political action committees» (PAC) an Kandidaten, allerdings bemühen sie sich dabei um politische Parität: Wie eine Aufstellung der Nichtregierungsorganisation Open Secrets zeigt, gaben die grossen Technologiekonzerne im Wahlkampf 2020 bisher fast gleiche Summen an die beiden Parteien. Ein ähnliches Bild zeigt sich für die vergangenen landesweiten Wahlen.
Die Konzerne selbst bemühen sich um politische Neutralität «Zerschlagt ‹Big Tech›»

Doch nun ist das Silicon Valley selbst zu einem Politikum geworden: Einige linke demokratische Präsidentschaftskandidaten profilieren sich im Wahlkampf mit scharfer Kritik an den Technologieriesen, allen voran die Senatorin aus Massachusetts Elizabeth Warren. «Die heutigen grossen Technologiefirmen haben zu viel Macht – zu viel Macht über unsere Wirtschaft, unsere Gesellschaft und unsere Demokratie», twitterte Warren im Frühjahr. Ihr Wahlprogramm sieht vor, zahlreiche frühere Firmenübernahmen rückgängig zu machen.

Am Caltrain-Bahnhof in San Francisco, von wo aus die Züge in die Heimatstädte von Facebook, Google und anderen Tech-Riesen losfahren, hängte sie im Sommer riesige Plakate auf mit der Aufforderung «Zerschlagt ‹Big Tech›» und posierte unter diesen.

Speziell gegen Facebook führt Warren einen Kleinkrieg, und sie gelobt, den Konzern im Fall ihres Wahlsiegs zu zerschlagen. Eine Warren-Präsidentschaft wäre für Facebook «richtig doof», hatte Zuckerberg bei einer firmeninternen Veranstaltung gesagt, von der später Audioaufnahmen an die Öffentlichkeit gelangten, «wir würden uns massiv gegen eine Zerschlagung wehren». Der Kampf zwischen Warren und Facebook ist inzwischen in der nächsten Runde angekommen, weil das soziale Netzwerk sich dagegen wehrt, Anzeigen von Anwärtern auf politische Ämter zu zensieren, auch wenn sie offensichtliche Unwahrheiten enthalten.

Vor wenigen Tagen gab Warren nun bekannt, dass sie im Wahlkampf keine Spenden von mehr als 200 Dollar von Technologiefirmen annehmen werde. Bereits getätigte Spenden werde ihr Wahlkampfteam zurückgeben. Damit legt sie «Big Tech» in die gleiche Schublade der in den Augen der Linken moralisch verwerflichen Sektoren wie «Big Pharma», «Big Banks» und «Big Oil».

Interessanterweise spenden Angestellte von Tech-Firmen trotz Warrens feindlichen Aussagen weiterhin fleissig: Die 70-Jährige habe im zweiten Quartal 2019 mehr als 120 000 Dollar in Zuwendungen von Mitarbeitern von Amazon, Apple, Facebook und Google erhalten, meldete «Politico». Offenbar überzeugen ihre linken Positionen viele Kalifornier.
Buttigieg als idealer Kandidat

Doch nicht alle Kandidaten haben eine derart feindliche Haltung gegenüber den Technologiekonzernen angenommen – zumal sie schlichtweg auf die Gelder von dort angewiesen sind, um in dem monatelangen Wahlkampf zu überleben. Der langjährige Google-Chef Eric Schmidt war jüngst Gastgeber für eine Fundraising-Veranstaltung für Joe Biden, und der Linkedin-Gründer Reid Hoffmann rührte die Werbetrommel für Cory Booker.

Speziell Pete Buttigieg versteht die Geldhähne des Silicon Valley anzuzapfen – mehr noch als selbst die kalifornische Senatorin Kamala Harris. Der 37-jährige Buttigieg ist für viele Millennials in Kalifornien eine Identifikationsfigur, zumal er homosexuell ist und Kalifornien Fahnenträger der LGBT-Bewegung ist. Als Bürgermeister von South Bend, Indiana, hat er versucht, Tech-Firmen anzulocken und “Silicon Prairie” zu schaffen. Buttigieg hält auch regelmässig Fundraising-Veranstaltungen im Technologie-Mekka ab: Wie Bloomberg schreibt, organisierte unter anderem der CEO von Netflix, Reed Hastings, einen Fundraising-Event für ihn, ebenso einer der Mitgründer der Smart-Home-Firma Nest sowie eine ranghohe Managerin von Uber.

Aus Sicht des Silicon Valley sei Buttigieg als gemässigter, wirtschaftsfreundlicher Kandidat der ideale Präsidentschaftsanwärter, sagte der Politologe Raphael Sonenshein von der California State University in Los Angeles gegenüber Bloomberg: Anders als Warren wolle Buttigieg die Tech-Firmen eben nicht zerschlagen, sondern besser regulieren. «Seine Botschaft passt zum Zeitgeist der Innovation und zu einem nach vorne gewandten Blick.» Sein Wahlkampf-Manager sagte im Frühjahr gegenüber der Associated Press: «Wir wollen einen Wahlkampf, der sich ein wenig 'disruptive' anfühlt, unternehmerisch eben. Derzeit fühlt es sich an wie ein Startup.» Die Strategie zahlt sich für Buttigieg buchstäblich aus: Er hat schon fünf Mal so viele Spendengelder aus dem Silicon Valley erhalten wie der derzeitige Frontrunner Joe Biden.

Dabei kann Buttigieg auf enge Verbindungen speziell zu Zuckerberg setzen: Die beiden besuchten zeitgleich die Harvard University, und auch wenn sie sich in der Zeit nicht persönlich kannten, war Buttigieg doch befreundet mit zwei von Zuckerbergs Mitbewohnern. Später war Buttigieg der 287. Nutzer des sozialen Netzwerks, und als Zuckerberg 2017 quer durch Amerika reiste, fuhr ihn sein «Freund Pete Buttigieg, einer der jüngsten Bürgermeister Amerikas», persönlich durch Indiana.
Kalifornien wählt früher

Eine Neuerung im Wahljahr 2020 könnte den Geldgebern aus dem Silicon Valley eine noch grössere Hebelwirkung verschaffen: Im demokratischen Vorwahlkampf 2020 stimmt Kalifornien nicht mehr als Schlusslicht im Juni ab, wenn der Spitzenreiter der Demokraten quasi schon feststeht, sondern erstmals deutlich früher, am «Super Tuesday». Die Stimmen der Bürger im bevölkerungsreichsten Gliedstaat dürften so gewichtiger in der Frage sein, wer sich als Herausforderer von Donald Trump durchsetzt.";https://www.nzz.ch/international/das-silicon-valley-spaltet-die-demokraten-ld.1519923;NZZ;;;;
25.03.2019;Nestlé investiert in seine Forschung in China;"Der Nahrungsmittelkonzern Nestlé baut seine Forschungs- und Entwicklungsaktivitäten in China aus. Nur ein Steinwurf entfernt vom im Nordosten Pekings gelegenen Künstlerviertel 798 ist am Montag ein Forschungszentrum eingeweiht worden, durch das Nestlé schneller auf die sich dynamisch entwickelnden Wünsche der chinesischen Kunden reagieren will. Welche Bedeutung die Investition für Nestlé hat – China ist inzwischen der zweitwichtigste Markt weltweit –, zeigt der Umstand, dass mit Konzernchef Mark Schneider und dem für technologische Entwicklungen Verantwortlichen, Stefan Palzer, zwei der führenden Manager aus der Zentrale nach China reisten. Nestlé wertet die Investitionen jedoch nicht als Eingeständnis, dass man in dem asiatischen Land der Konkurrenz punkto Forschung und Entwicklung hinterherhinke. «Wir waren schon immer in China präsent», sagte Palzer. Allerdings hat das Geschäft des Nahrungsmittelkonzerns im Reich der Mitte jahrelang unter betrieblichen Schwierigkeiten gelitten und erst im vergangenen Jahr wieder zu florieren begonnen.
Die fünf wichtigsten Einzelmärkte von Nestlé Parallel dazu betreibt Nestlé bereits seit Oktober vergangenen Jahres in der unweit von Hongkong gelegenen Metropole Shenzhen ein Technologiezentrum, das bei der Entwicklung von Kaffeemaschinen neue Akzente setzen soll. Shenzhen hat den Ruf, das «Silicon Valley der Hardware» zu sein. In der Millionenstadt hat sich eine einzigartige Infrastruktur entwickelt, in der alles zu finden ist, was für den Bau von Technologieprodukten wie Drohnen, Smartphones oder auch Kaffeemaschinen benötigt wird. Das Zentrum, das eine Ergänzung des Nestlé-Forschungsstandorts in Orbe ist, befindet sich im Net Valley Technology Park in Shenzhen, wo sich die führenden Hersteller von Kaffeemaschinen niedergelassen haben.
«Wir lernen viel von China»

Ein Beleg, welche Bedeutung innovative und auf die – chinesischen – Kundenbedürfnisse abgestimmte Hardware hat, ist in den neuen Räumlichkeiten des Forschungsstandorts Peking zu sehen. In der Cafeteria gibt es eine Maschine, bei der die Nestlé-Angestellten ihren Kaffee mit dem Smartphone zahlen. Bargeld gehört der Vergangenheit an. In Shenzhen tüftelt Nestlé darüber hinaus an neuen Kaffeemaschinen, um die Anforderungen der weltweiten Kundschaft zu befriedigen. So bietet eine Maschine den Konsumenten die Möglichkeit, über eine App den Kaffee ihrer Wahl brauen zu lassen: Soll der Kaffee stärker oder schwächer, soll er gesüsst oder ungesüsst sein? Solche Wünsche lassen sich über das Smartphone regeln. Mit der Eröffnung der Forschungseinrichtungen in Peking und Shenzhen beginne nicht der schleichende Abschied vom Standort Schweiz, sagte Palzer. «Unser Heimatland Schweiz ist auf Feldern wie Life-Sciences oder bei der Verfahrenstechnik noch immer eine der weltweit führenden Nationen. Und so findet auch unsere Grundlagenforschung mehrheitlich in der Schweiz statt.» Allerdings ist China in Bereichen wie Digitalisierung und künstlicher Intelligenz dem Rest der Welt inzwischen voraus. Der Entscheid Nestlés, die Forschungsaktivitäten in China auszubauen, zeigt: Das einst belächelte asiatische Land, das als verlängerte Werkbank zur Erfüllung westlicher Konsumentenwünsche wahrgenommen wurde, ist nicht nur ein lukrativer Markt, sondern setzt in bestimmten Sektoren neue Massstäbe. «Wir lernen viel von China», sagte Palzer bei der Eröffnung der Forschungseinrichtung.
Amerika ist der wichtigste Markt von Nestlé Die chinesische Hauptstadt als Standort ist aus diversen Gründen für den Nahrungsmittelkonzern eine konsequente Wahl. Unweit des Forschungszentrums liegt die China-Zentrale, weshalb sich die Aktivitäten der rund 40 Forscher mit jenen der für das Tagesgeschäft Zuständigen leichter koordinieren lassen. Zudem befindet sich am neuen Forschungsstandort auch das Institut für Lebensmittelsicherheit in China – ein Ableger des Institute of Food Safety and Analytical Sciences in Lausanne. Diverse Skandale haben in der jüngeren Vergangenheit das Vertrauen der Chinesen in die Fähigkeiten der Nahrungsmittelproduzenten erschüttert, weshalb die Machthaber in Peking einen stärkeren Fokus auf dieses sensible Thema legen. Die Nähe zu den zuständigen chinesischen Behörden ist hilfreich, um ein Sensorium für anstehende Entwicklungen und Gesetzesänderungen zu entwickeln.

Schliesslich gibt es in der chinesischen Hauptstadt Top-Universitäten und zahlreiche Startups, mit denen Nestlé kooperiert und von denen der Konzern lernen kann. So arbeitet der Nahrungsmittelriese bereits mit einem auf Big Data spezialisierten Unternehmen zusammen. Dieses wertet Diskussionen in den sozialen Netzwerken Chinas aus und liefert auf Basis dieser Daten Rückschlüsse darauf, welche Trends zu erwarten sein werden.
Mahnende Worte des Schweizer Botschafters

Eine riesige Herausforderung bleibt die Entwicklung neuer Produkte, mit denen Nahrungsmittelkonzerne die chinesischen Konsumenten für sich gewinnen wollen. Im Fokus hat Nestlé neben den Millennials und den Bewohnern der noch weniger wohlhabenden Tier-3- und Tier-4-Städte, wo eine neue Mittelschicht entsteht, auch die in den ländlichen Regionen lebenden Chinesen. Die jüngere Generation der Millennials zeichnet neben einem stetig wachsenden Einkommen zwei Eigenschaften aus: Die chinesische Küche hat zwar ihre Geschmäcker von Kindesbeinen an geprägt. Allerdings sind sie im Gegensatz zu ihren Eltern weltoffener aufgewachsen. Sie besuchen in den chinesischen Millionenstädten westliche Restaurants, und auch Reisen ins Ausland haben ihr Geschmacksempfinden beeinflusst. «Dennoch gibt es zwischen chinesischen und schweizerischen Konsumenten noch immer Unterschiede», sagte Palzer. So sind etwa bei chinesischen Snacks die Grenzen zwischen süss und salzig oft fliessend, was dem westlichen Gaumen fremd erscheint.

Der neue Schweizer Botschafter in China, Bernardino Regazzoni, betonte bei seiner kurzen Ansprache am neuen Standort die Innovationskraft seines Heimatlandes, die in Peking auf chinesische Dynamik treffe. Einen kleinen Seitenhieb auf die anwesende politische Prominenz aus Peking und Shenzhen konnte sich Regazzoni nicht verkneifen. Schweizer Investoren benötigten in China ein gutes wirtschaftliches Umfeld. «Ihr Know-how und ihr geistiges Eigentum müssen geschützt werden.» Das vom Nationalen Volkskongress verabschiedete Gesetz zur Regelung ausländischer Investoren sei denn auch ein Schritt in die richtige Richtung, fügte Regazzoni an.";https://www.nzz.ch/wirtschaft/nestle-intensiviert-forschung-in-china-ld.1469767;NZZ;Matthias Müller;;;
02.02.2019;Die Welt wird immer besser. Aber je besser die Welt, desto krasser die Nörgelei!;"Es ist kaum zu glauben, dass eine Verteidigung von Vernunft, Wissenschaft und Humanismus zu Kontroversen führen sollte, zumal wir in einer Zeit leben, in der diese Ideale jede Hilfe brauchen können. Aber ein Kollege sagte mir: «Du hast die Köpfe der Leute explodieren lassen.»

Mein Buch «Aufklärung jetzt» ist deshalb von Kritikern auf der Linken wie auf der Rechten attackiert worden. Sie gaben der Aufklärung die Schuld für Rassismus und Imperialismus, existenzielle Bedrohungen und epidemische Einsamkeit, Depressionen und Suizid. Sie mäkelten, die Daten, die den Fortschritt belegen, beruhten nur auf Rosinenpicken. Und sie höhnten, mit kaum verhüllter Schadenfreude, die Aufklärung sei eine Idee mit Verfallsdatum, sie habe im Zeitalter des autoritären Populismus, der sozialen Netzwerke und der künstlichen Intelligenz keine Zukunft.

Deshalb nehme ich Stellung zu den Kontroversen, die im Jahr seit der Veröffentlichung von «Aufklärung jetzt» ausgebrochen sind. Ich denke also über den Stand des Projekts der Aufklärung und auch über ihre Feinde nach. Erster Einwand

Herr Pinker, Sie verstehen die Aufklärung nicht. Es gab viele «Aufklärungen», nicht nur eine. Die Denker der Aufklärung waren nicht alle humanistische Wissenschafter, unter ihnen fanden sich auch Missionare oder Rassisten. Und was ist mit Rousseau, was mit Marx?

Die vielen Attacken gegen «Aufklärung jetzt», die uns belehren, was die Aufklärung «wirklich» war, verfehlen ihr Ziel. Mein Buch tritt, wie es sein Untertitel sagt, «für Vernunft, Wissenschaft, Humanismus und Fortschritt» ein; es setzt sich nicht einfach mit dem ganzen Haufen von Denkern des 18. Jahrhunderts auseinander.

Natürlich bleibt der Begriff unscharf, selbstverständlich gab es Einflüsse aus früheren Epochen und Auseinandersetzungen zwischen Autoren, so mit Rousseau, den Anthony Kenny als «gigantischen Kuckuck im Nest der Aufklärung» sieht. Die Frage, ob ein Denker zur Aufklärung zu zählen ist, lässt sich also nicht korrekt beantworten. Und es lässt sich nicht bestreiten, dass sich unter diesen Männern und Frauen des 18. Jahrhunderts auch Rassisten, Sexisten, Antisemiten und Sklavenhalter finden. Ich feiere denn auch nicht die Denker der Aufklärung, sondern ihre Ideen. Ich wählte für den Titel den Terminus «Aufklärung» einfach, weil er griffiger ist als «säkularer Humanismus», «liberaler Kosmopolitismus» oder auch «offene Gesellschaft».
Zweiter Einwand

Die Aufklärung verdient keine Feier. Sie brachte der Welt Rassismus, Imperialismus, Genozide und Sklaverei.

An dieser Behauptung ist nur richtig, dass einige dieser Übel auch nach dem 18. Jahrhundert herrschten. Davon abgesehen, geht das Argument genau andersrum: Jedes dieser Übel ist so alt wie die Menschheit, und erst im 18. Jahrhundert sahen sie Denker als moralische Probleme, die es zu lösen galt.

Der Rassismus lässt sich mit der menschlichen Neigung zur Xenophobie erklären: Wir lehnen Gruppen von Fremden ab, die sich in ihrer Erscheinung oder ihrer Lebensart von uns unterscheiden. Das zeigt sich zu allen Zeiten, auch bei den grössten Denkern. So lästerten Aristoteles über die Barbaren und Cicero über die Briten, die alten Griechen über die Afrikaner, die Spanier im Mittelalter über die Juden und die Europäer im 16. Jahrhundert über die Indianer.

Der Imperialismus geht ebenso auf die Antike zurück. Fast immer in der Geschichte galt für politische Führer: «Ich kam. Ich sah. Ich siegte.» (Die Liste auf Wikipedia umfasst 154 Reiche zwischen 2300 vor Christus und 1700 nach Christus, mit der Anmerkung: «Die Liste ist nicht komplett; du kannst mithelfen, sie zu ergänzen.») Erst in der Aufklärung stellten Denker das Recht der Europäer infrage, den Rest der Welt zu kolonisieren.

Diese neuen Antiimperialisten, wie Bentham, Diderot oder Kant, gingen von zwei Ideen aus: einerseits vom Prinzip, dass alle Menschen moralischen und politischen Respekt verdienten, einfach weil sie Menschen waren; anderseits von einer frühen evolutionären Anthropologie, wonach alle Völker ihre eigenen Kulturen schufen, um sich ihrer Umwelt anzupassen und in ihrer Gemeinschaft zusammenzuleben. Gestützt darauf verurteilten diese Denker alle Verletzungen der Würde und der Freiheit der Menschen, wie Sklaverei oder Imperialismus. Und sie verzichteten darauf, bei den Sitten und Gebräuchen der verschiedenen Völker zu werten.

Die Sklaverei gab es auch nicht erst im 18. Jahrhundert, wie alle wissen, die den Film «Spartacus» kennen: Zu allen Zeiten galten Sklaven als die begehrteste Beute von Eroberern. Der Aufklärung die Schuld für die Sklaverei zu geben, ist besonders lächerlich angesichts der Tatsache, dass die Sklavenbefreiung erst im 18. Jahrhundert einsetzte. Die Historikerin Katie Kelaidis stellt denn auch fest: «Jahrtausende lang klagten grosse Morallehrer über die Unmenschlichkeit der Sklaverei, aber keiner – nicht Jesus, nicht Buddha, nicht Mohammed, nicht Sokrates – dachte vor den Aufklärern an die Befreiung aller Sklaven.»

Es stimmt, dass in der zweiten Hälfte des 19. Jahrhunderts wissenschaftliche Theorien zu rassistischen Hierarchien und ethnischen Nationalismen aufkamen, die zu den Weltkriegen und den Völkermorden des 20. Jahrhunderts führten. Wer diese Greuel der Aufklärung anlastet, macht aber denselben Denkfehler wie jene, die jedes Ereignis, das nach dem 18. Jahrhundert eintrat, darauf zurückführen. Ja, diese Kritiker unterschlagen die wichtigste intellektuelle Bewegung des 19. Jahrhunderts: die Gegenaufklärung.
Dritter Einwand

Wie können Sie sagen, wir sollten uns keine Sorgen machen, denn alles werde gut? Was ist mit dem Plastik in den Ozeanen? Mit der Opioidkrise? Mit den Amokläufen an Schulen? Oder mit den sozialen Netzwerken?

Beim Schreiben von «Aufklärung jetzt» machte ich dieselbe Erfahrung wie schon bei «Gewalt: Eine neue Geschichte der Menschheit»: «Fortschritt» ist ein exotisches, kontraintuitives Konzept. Viele Leute meinen, bei der Frage, ob es den Fortschritt gibt, komme es darauf an, ob jemand Optimist oder Pessimist ist, also das Glas als halb voll oder als halb leer sieht.

Es geht bei dieser Frage aber nicht um Optimismus, sondern um das, was Hans Rosling «Factfulness» nannte: darum, unser Verständnis der Welt an der empirischen Realität zu kalibrieren. Wenn das Wohlbefinden sich nach allen Kriterien, wie Gesundheit, Einkommen, Wissen und Sicherheit, über die Zeit verbessert hat, können wir von Fortschritt sprechen. Tatsächlich stellen wir fest, dass es so ist. Wie Hans Rosling und andere gezeigt haben, verleugnen die meisten Leute den Fortschritt denn auch nicht aus Pessimismus, sondern aus Ignoranz.

Dass es den Fortschritt gibt, heisst allerdings nicht, dass es überall und jederzeit allen besser geht. Das wäre nicht Fortschritt, sondern ein Wunder. Der Fortschritt ist aber kein Wunder, er ist das Ergebnis des Lösens von Problemen. Probleme sind unvermeidlich, und Lösungen können zu neuen Problemen führen. Deshalb sehen wir, dass sich viele Aspekte des Lebens verbessern, einige aber verschlechtern. Die Frage, ob es den Fortschritt gibt, lässt sich nicht mit dem Aufstellen von Listen entscheiden, was in der Welt alles schiefläuft, wie es Journalisten gerne machen, damit sich ihr Publikum gruselt. Entscheidend ist die Antwort, die Barack Obama auf die Frage gab, wann wir leben wollten, wenn wir nicht wüssten, wer wir wären: «Jetzt.»

Der Fortschritt bedeutet nicht, dass die Welt perfekt ist, nur besser. Und er verleitet uns auch nicht dazu, das reale Leiden von Menschen und die sehr realen Gefahren für die Menschheit zu verkennen. Wir dürfen nicht aufhören, uns zu sorgen, weil ja alles gut wird. Wie es der Welt künftig geht, hängt davon ab, was wir jetzt tun. Und dabei kommt es darauf an, was wir als Fortschritt sehen.
Vierter Einwand

Alle Daten, die zeigen sollen, dass die Welt besser wird, sind doch pures Rosinenpicken.

Offenbar können ausgerechnet die Fortschrittlichsten nicht glauben, dass es den Fortschritt gibt. «Kennt jemand griffige Widerlegungen der neoliberalen und konservativen Daten zum sozialen Fortschritt der letzten dreissig Jahre?», fragte so der linke Aktivist David Graeber auf Twitter. «Diese Daten können nur von rechten Think-Tanks kommen. Wo finde ich die Daten der anderen Seite? Ich sehe keine.»

Fürwahr, denn das Bild der Welt, das «Aufklärung jetzt» zeigt, sammelt alle Rosinen. Ich fing mit den drei Variablen an, die alle, die über den sozialen Fortschritt nachdenken, als Grundlage für das Messen des Wohlbefindens anerkennen: Langlebigkeit, Vermögen und Bildung (also gesund, reich und weise sein). Dazu beschaffte ich Daten zu einem der psychologischen Gründe des Fortschritts, liberale Werte, und zu einem seiner psychologischen Ergebnisse, Glück. Auf jedem dieser Graphen ist der Fortschritt mit blossem Auge zu sehen.

Stellen wir diesem Bild der Welt die Alternative gegenüber, die uns die Medien zeigen. Journalismus ist per definitionem Rosinenpicken. Er berichtet von raren Ereignissen, wie Kriegen, Epidemien oder Katastrophen, nicht vom Alltag, also von Frieden, Gesundheit und Sicherheit. Diese Neigung zum Negativen verstärkt sich noch, weil die Journalisten um Klicks kämpfen und als Moralprediger ihr Publikum aus seiner Selbstzufriedenheit reissen wollen. Deshalb spottete das Satiremagazin «The Onion»: «CNN hält die Morgen-Meetings ab, um zu entscheiden, weshalb die Zuschauer den Tag lang in Panik ausbrechen sollen.»

Dagegen fordert uns die Umweltfrage tatsächlich heraus. Ich konnte keine Langzeit-Datenreihen zur Umweltqualität zeigen, weil es sie nicht gibt. Aber ohnehin behauptet niemand, der Zustand der Umwelt habe sich in den letzten 250 Jahren verbessert – im Gegenteil: Vieles am Fortschritt der Menschheit ging auf Kosten der Umwelt.

Immerhin gibt es Daten für das letzte Jahrzehnt im Environmental-Performance-Index. Ich stellte da fest, dass sie in 178 von 180 Ländern besser geworden sind, und zwar am deutlichsten in den am höchsten entwickelten. Vielleicht erholt sich die Umwelt also; dabei hilft, dass die Wachstumsrate der Bevölkerung seit ihrem Höhepunkt (schon 1962!) stark zurückgeht.

Natürlich gibt es alarmierende Trends, so die Zunahme des CO2-Ausstosses, den Verlust von Biodiversität oder den Rückgang der Trinkwasserreserven. Aber ich wollte nicht einen umfassenden Umweltbericht vorlegen, sondern nur dem Fatalismus entgegentreten, den die orthodoxen Journalisten und Aktivisten pflegen.
Fünfter Einwand

Wie erklären Sie Donald Trump? Den Brexit? Den autoritären Populismus? Zeugen sie nicht vom Ende der Aufklärung und von der Umkehr des Fortschritts?

Zwar bieten die Ideale der Aufklärung – Vernunft, Wissenschaft, kosmopolitischer Humanismus, demokratische Institutionen – die besten Aussichten für die Menschheit; sie leuchten den Menschen aber nicht zwingend ein. Die Leute fallen leicht zurück in Magie, Autoritätsglauben, Stammesdenken und Nostalgie nach einer goldenen Zeit. Und das Denken der Aufklärer hat sich auch nie wirklich durchgesetzt.

In einzelnen Phasen, die seit 1945 länger wurden, nahm zwar ihr Einfluss zu; sie kämpften aber immer gegen Romantiker, Nationalisten, Militaristen und andere Ideologen der Gegenaufklärung. Wie ich in «Aufklärung jetzt» feststelle, ist «die intellektuellen Wurzeln des Trumpismus» kein Oxymoron: Viele in Trumps Brain-Trust und in der Alt-Right-Bewegung berufen sich stolz auf Theoretiker der Gegenaufklärung. Deren Thesen können die Menschen in Phasen des ökonomischen, demografischen und kulturellen Wandels ansprechen, besonders jene Gruppen, die sich verachtet und abgehängt fühlen.

Bevor wir aber eine Zukunft sehen, die für immer wie ein Stiefel auf ein Gesicht tritt, sollten wir die Perspektiven des autoritären Populismus zurechtrücken. Trotz einigen Erfolgen scheint er zu stagnieren. Eine Mehrheit der Amerikaner hält an ihrer Ablehnung von Trump fest, und in Europa kamen die populistischen Parteien in den Wahlen von 2018 durchschnittlich nur auf 13 Prozent.

Und die Segmente, die der Populismus am stärksten anspricht, sind im Niedergang: ländliche Regionen, ältere und weniger gebildete Wähler, weisse Mehrheiten. Zudem führen uns die Querelen mit Trump und dem Brexit stets vor Augen, dass der Populismus in der Theorie besser funktioniert als in der Praxis.
Sechster Einwand

Was sagen Sie denn zur Epidemie von Einsamkeit, Depressionen und Suizid in den höchstentwickelten liberalen Gesellschaften?

Ich sage nichts zu dieser Epidemie, weil es sie nicht gibt. Zwar leiden Teile der Bevölkerung wirklich unter tragischen Schicksalen, vor allem weisse Amerikaner im mittleren Alter mit schlechter Ausbildung in ländlichen Gebieten. Aber der Glaube, die Menschen würden immer unglücklicher, ist ein Irrglaube, der sich hartnäckig hält.

Max Roser und Mohamed Nagdy zeigen in ihrer Studie «Optimism & Pessimism»: «In jedem Land meinen die Leute, die anderen seien weniger glücklicher, als sie es von sich selber sagen.» So glauben die Amerikaner, weniger als die Hälfte ihrer Landsleute seien glücklich; in Wirklichkeit sind es 90 Prozent.

Darüber hinaus zeigt «Our World in Data», dass seit 1994 die Prävalenz von psychischen Erkrankungen und Drogenmissbrauch etwa gleich geblieben ist und die Suizidrate um 38 Prozent abgenommen hat. Das lässt sich gemäss Studien aus China und Indien auch damit erklären, dass die Frauen mehr Freiheiten geniessen und sich dem Druck in ihren Dörfern entziehen.

Wir müssen also die hundert Jahre alte These von Emile Durkheim überdenken, dass die ländlichen Gemeinschaften mit ihrer Intimität und ihren Normen die Menschen vor Anomie und Suizid schützen. Aber darum scheren sich die Kritiker nicht, die sich nach dem traditionellen Landleben sehnen, das sie selber nie erdulden mussten.

Das Leben zwingt uns immer wieder, eine Wahl zu treffen. Die Freiheit in der modernen Gesellschaft, die es so noch nie gab, bietet uns auch die Möglichkeit, Intimität für Autonomie einzutauschen und Versuchungen zu erliegen, die langfristig für uns nicht gut sind. Wir brauchen unsere Freiheit nicht immer weise – seit je. Der Jurist Richard Posner wirft den Anklägern der Moderne denn auch vor, ihr Denkfehler sei «das Vergleichen einer idealisierten Vergangenheit, deren Laster sie verschweigen, mit einer dämonisierten Gegenwart, deren Tugenden sie übersehen».
Siebter Einwand

Die Aufklärung fällt ihren eigenen Schöpfungen zum Opfer: sozialen Netzwerken und künstlicher Intelligenz.

Das war 2018 die unwiderstehliche Storyline zum 200-Jahre-Jubiläum von Mary Shelleys «Frankenstein». Wie das Wiederbeleben eines Leichnams mittels Elektrizität ist das Verdrängen der Menschen durch künstliche Intelligenz aber eine Science-Fiction-Fantasie. In meinem Buch lege ich dar, weshalb uns die künstliche Intelligenz weder unterwerfen noch, als unbeabsichtigter Kollateralschaden, auslöschen wird.

Was die Supercomputer mit ihrem Deep Learning heute schaffen, erscheint uns zwar wie Magie. «Tief» ist an ihrem Lernen aber nur, dass sie aus zahlreichen Schichten bestehen; ihr Verständnis ist so dünn wie eine Zwiebelschale. Nach viel Training können sie Inputs verblüffend gut in Output umsetzen (so erkennt Facebook, ob jemand das Foto einer Person oder einer Katze hochlädt), doch sie verstehen nicht die Bedeutung dessen, was sie berechnen.

So kann das Programm für ein Videogame selbst mit kleinsten Veränderungen seiner Spielregeln nicht umgehen. Das ist der Grund, weshalb Experten meinen, die künstliche Intelligenz komme trotz ihren derzeitigen Erfolgen nicht weiter; für weitere Fortschritte brauche es ganz andere Algorithmen.

Als furchterregende Magie gelten derzeit auch die sozialen Netzwerke. Sie tragen angeblich die Schuld an jedem Problem des Planeten, vom Niedergang der Demokratie bis zu den Schwierigkeiten der Generation Z oder iGen, die nach 1995 geboren ist. Bevor wir die westliche Zivilisation abschreiben, sollten wir aber die historische Perspektive bewahren.

Neue Medien eröffnen meist einen Wilden Westen von Apokryphen, Plagiaten und Verschwörungstheorien, bevor Massnahmen greifen, die der Wahrheitsfindung dienen. Für solche Massnahmen setzt sich immer jemand ein, weil die Wahrheit ist, was nicht verschwindet, auch wenn wir nicht daran glauben. Es gibt deshalb so früh in der Geschichte der sozialen Netzwerke wenig Grund zu befürchten, dass sie in eine heimtückische Gehirnkontrolle ausarten, also die Demokratie und die anderen Institutionen der Aufklärung zerstören.
Achter Einwand

Die Leute sind irrational; sie scheren sich nicht um Fakten und Argumente. Was wollten Sie also mit «Aufklärung jetzt» erreichen?

Schon Thomas Paine schrieb: «Sich mit jemandem auseinanderzusetzen, der auf den Gebrauch des Verstands verzichtet, ist dasselbe, wie einem Toten Medizin zu verabreichen.» Ich habe «Aufklärung jetzt» nicht für Leute geschrieben, die sich nicht um Fakten und Rationalität scheren, sondern für Sie.

Tatsächlich gibt es noch Leute, die Fakten schätzen und Meinungen ändern, zumindest solange dies nicht ihren Glaubenssätzen widerspricht – vor allem, wenn sie die Informationen in Graphen präsentiert bekommen. (Davon gibt es 75 in «Aufklärung jetzt».) Ich kann mich denn auch, bei allen Mühen für meine Verteidigung, über das Echo nicht beklagen. Dankbar bin ich vor allem für drei Reaktionen:

Erstens luden mich mehrere Staatschefs oder ihre Berater zu Gesprächen ein. Sie suchten keinen politischen Rat, denn dafür bin ich nicht kompetent, sondern die Gelegenheit, über die Ziele einer liberalen, demokratischen Regierung nachzudenken. Es reicht nicht, kein Populist, kein Sozialist oder einfach ein Verwalter des Status quo zu sein. In einer Demokratie brauchen effektive Führungspersonen Überzeugungen, was die Werte, ja die Würde ihrer Mission angeht.

Die Ideale der Aufklärung sind ein guter Anfang; solche Sätze lassen sich kaum übertreffen: «Wir halten diese Wahrheiten für ausgemacht, dass alle Menschen gleich erschaffen worden, dass sie von ihrem Schöpfer mit gewissen unveräusserlichen Rechten begabt worden, worunter sind Leben, Freyheit und das Bestreben nach Glückseligkeit. Und dass zur Versicherung dieser Rechte Regierungen unter den Menschen eingeführt worden sind, welche ihre gerechte Gewalt von der Einwilligung der Regierten herleiten.»

Zweitens sahen Journalisten das Problem mit der zerstörerischen Negativität in ihrer professionellen Kultur. Sie verscheucht das Publikum: In einer internationalen Umfrage gaben kürzlich ein Drittel der Befragten an, sie nähmen die News nicht mehr zur Kenntnis. Denn die Nachrichten, die unsere Medien verbreiten, sagen wenig über den wahren Zustand der Welt. Sie schaden dem Glauben, dass es den Fortschritt gibt. Und sie schaffen perverse Anreize für Terroristen, Amokläufer und twitternde Politiker.

Drittens, und das ermutigt mich am meisten, schrieben mir Leser, «Aufklärung jetzt» habe ihr Leben verändert. Seit ich zum «Psychologen» gesalbt wurde, muss ich mich immer gegen die Erwartung verwahren, dass es mein Geschäft sei, für die geistige Gesundheit der Menschen zu sorgen. Zum ersten Mal in meinem Leben habe ich diesen Anspruch vielleicht erfüllt.

So schrieb mir eine Lehrerin: «Ich kann jetzt jeden Tag mit meinen Schülern sprechen und ihnen den Kontext zu den furchterregenden Schlagzeilen bieten, über die sie diskutieren möchten. Und ich kann vor allem jede Nacht besser schlafen, weil ich weiss, dass es mit der Welt in die richtige Richtung geht. Danke, dass Sie uns die Mittel gegen diese Kultur des Schreckens geben. Dank Ihnen bin ich eine viel glücklichere Person und Lehrerin.»";https://www.nzz.ch/feuilleton/die-aufklaerung-funktioniert-eine-verteidigung-von-steven-pinker-ld.1456042;NZZ;Steven Pinker;;;
05.01.2017;Warum wir uns dem Wissen verweigern;"Ignoranz gilt im Allgemeinen als unerwünschter Geisteszustand, und die Idee einer willentlichen Ignoranz dürfte erst einmal mit Stirnrunzeln quittiert werden. Aber Menschen entscheiden sich oft für das Nichtwissen; damit demonstrieren sie eine Art negativer Neugier, die im Widerspruch zu den Konzepten der Ambiguitätsaversion und eines grundsätzlichen Verlangens nach Gewissheit steht. Dieses Verhalten kontrastiert auch mit der Annahme, dass ein Zuwachs an Wissen und Information in jedem Fall erwünscht ist – eine Vorstellung, die von Aristoteles («Alle Menschen streben von Natur aus nach Wissen») über das Bild des Informations-Junkies bis zum Tun und Treiben staatlicher Überwachungsprogramme immer neue Ausprägungen gefunden hat. Kassandras Qual

Willentliche Ignoranz kann definiert werden als der bewusste Entscheid, die Antwort auf eine Frage nicht wissen zu wollen, die einen persönlich betrifft – noch wenn die Antwort umsonst, also mit keinerlei Suchkosten verbunden ist. Das Konzept unterscheidet sich von den Studienbereichen der Agnotologie oder der Soziologie des Unwissens: Diese untersuchen die systematische Schaffung von Ignoranz durch Ablenkung, Verschleierung oder das Unterschlagen von Wissen, wie es etwa die Tabakindustrie in ihrem Bemühen praktiziert hat, den Konsumenten die Zusammenhänge zwischen Rauchen und Krebs nicht bewusst werden zu lassen. Die willentliche Ignoranz hingegen ist nicht fremdem Einfluss geschuldet, sondern selbst gewählt. Aber warum sollten sich Menschen dem Wissen verweigern? Die wenigen Studien und die noch rareren Erklärungsversuche umreissen mindestens vier Motive.

    Die willentliche Ignoranz hingegen ist nicht fremdem Einfluss geschuldet, sondern selbst gewählt.

Das erste ist die Neigung, potenziell schlechten Nachrichten auszuweichen, besonders wenn keine Möglichkeit besteht, das Angekündigte abzuwenden oder sich davor zu schützen. In der griechischen Mythologie empfing Kassandra von Apollo die Gabe, in die Zukunft zu sehen; dazu aber auch den Fluch, dass niemand ihren Prophezeiungen glauben würde. Kassandra sah den Fall Trojas, den Tod ihres Vaters, ihre eigene Ermordung voraus; das Wissen um die kommenden Greuel wurde ihr zur Quelle endloser Qual. Der technologische Fortschritt verschiebt laufend die Grenze zwischen dem, was wir nicht wissen, und dem, was wir wissen können, und rückt Kassandras Gabe in immer greifbarere Nähe. Als James Watson, der zusammen mit Francis Crick die Struktur der DNS entdeckt hatte, eine Analyse seines eigenen Genoms vornehmen liess, stellte er die Bedingung, dass sein ApoE4-Genotyp – der Indikator für das Risiko einer Alzheimer-Erkrankung – ihm verschwiegen und vor der Publikation auch aus seiner Genomsequenz getilgt wird.

    Der technologische Fortschritt bringt es mit sich, dass wir immer häufiger werden entscheiden müssen, wie viel wir unserer Zukunft auch wissen wollen.

Forscher behaupten, Biomarker entdeckt zu haben, welche auf die voraussichtliche Lebensdauer und Ursache des Todes des Trägers schliessen lassen; andere wollen voraussagen können, ob eine Heirat in einer Scheidung enden wird. Aber wollen Sie wirklich wissen, wann Sie sterben werden? Oder ob Sie gelegentlich den Scheidungsanwalt aufsuchen sollten? Die wenigen verfügbaren Studien besagen, dass 85 bis 90 Prozent der Durchschnittsbevölkerung nichts Genaueres über ihren Tod und die Stabilität ihrer Ehe wissen wollen. Zwar sind wir nicht wie Kassandra dazu verdammt, die Zukunft zu sehen; aber der technologische Fortschritt bringt es mit sich, dass wir immer häufiger werden entscheiden müssen, wie viel wir von dem, was auf uns zukommt, auch wissen wollen.
Ignoranz kann profitabel sein

Das zweite Motiv ist das Verlangen nach Überraschung und Spannung. Je nach Herkunftsland wollen 30 bis 40 Prozent der Eltern nicht über das Geschlecht ihres ungeborenen Kindes informiert werden, sogar wenn eine Ultraschalluntersuchung oder eine Amniozentese durchgeführt wurde. Diese Eltern wollen nicht auf das Überraschungsmoment verzichten und messen ihm offenbar mehr Wert bei als der Möglichkeit, aufgrund gesicherten Wissens besser vorauszuplanen.

Ein drittes Motiv besteht in der Möglichkeit, strategisch Profit aus der Ignoranz zu schlagen; der Ökonom Thomas Schelling hat dies schon in den 1950er Jahren formuliert. Ein alltägliches Beispiel sind die Leute, die mit der Nase auf dem Smartphone-Display durch die Strassen wandern und es den anderen überlassen, Kollisionen zu vermeiden. In eine ähnliche Richtung geht die These, dass seit der Finanzkrise von 2008 auch Banker und politische Entscheidungsträger diese strategische Blindheit praktizieren, um die Risiken, die sie eingehen, auszublenden und effiziente Reformen zu verhindern.

    Die Frage, wie wir entscheiden, was wir wissen wollen und was nicht, sollte mit mehr Neugier und mit grösserer wissenschaftlicher Aufmerksamkeit behandelt werden.

Letztlich wird willentliche Ignoranz aber auch im Interesse von Fairness und Unparteilichkeit eingesetzt. So wie Justitia sich oft mit einer Augenbinde präsentiert, wird an vielen amerikanischen Gerichtshöfen die kriminelle Vorgeschichte eines Angeklagten nicht ins laufende Verfahren einbezogen: Die Geschworenen sollen nicht wissen, ob er schon andere Delikte begangen hat, und entsprechend unparteiisch urteilen. Was John Rawls den «Schleier des Nichtwissens» nennt, ist eine Form von Ignoranz im Dienst der Gerechtigkeit. Mehr als ein Kuriosum

Trotz diesen Einsichten wird das Phänomen der willentlichen Ignoranz mehrheitlich als Kuriosum behandelt. Wissenschaft und Science-Fiction zelebrieren den Wert der exakten Voraussage und des umfassenden Wissens dank Big-Data-Analyse, Präzisionsmedizin und Überwachungsprogrammen – weitgehend ohne diese Methoden zu hinterfragen. Aber wie das Beispiel Kassandras zeigt: Nicht jeder Mensch erträgt den Blick in die Zukunft. Die Frage, wie wir entscheiden, was wir wissen wollen und was nicht, sollte mit mehr Neugier und mit grösserer wissenschaftlicher Aufmerksamkeit behandelt werden.";https://www.nzz.ch/feuilleton/willentliche-ignoranz-wissen-gerd-gigerenzer-ld.138068;NZZ;Georg Gigerenzer;;;
04.12.2017;Öko und Techno – im Städtebau kein Widerspruch;"In Feuilletons und Leitartikeln wird heftig über die Neuerfindung der Städte des 21. Jahrhunderts debattiert. Die diesbezüglichen Prozesse und Perspektiven sind zu einem der prominentesten gesellschaftlichen Themen geworden – und das aus gutem Grund.

Seit 2008 leben mehr Menschen in Städten als auf dem Land. Bis zum Ende dieses Jahrhunderts werden die Städte für beinahe 90 Prozent des Bevölkerungswachstums und für 60 Prozent des Energieverbrauchs verantwortlich sein. Einerseits funktionieren diese geschäftigen Knotenpunkte des menschlichen Lebens als Zentren der Innovation auf unserem Planeten; andererseits verursachen sie den Löwenanteil der Umweltverschmutzung.

Nach manchen Schätzungen gehen auf die heutigen Städte um die 75 Prozent des weltweiten CO2-Ausstosses zurück, zudem sind sie für zahllose andere Schadstoffemissionen verantwortlich. Die Städte verschlingen weite Flächen von Wald, Ackerland sowie anderer Landschaft, sie verpesten Flüsse, Meere und Böden. Kurz gesagt: Falls wir Städte nicht bald richtig bauen, ist eine gesunde Zukunft für die Menschheit kaum vorstellbar, ganz zu schweigen von einer gesunden Biosphäre.
Smart oder grün?

Wenn ich es richtig sehe, lassen sich die Standpunkte zur Neuerfindung der Städte grossenteils zwei Lagern zuordnen. Die eine Seite ruft nach «smarten», «digitalen» oder «hochtechnisierten» Lösungen für den Städtebau. Der Schwerpunkt liegt auf den Informations- und Kommunikationstechnologien mit dem Potenzial, die Funktionsweise urbaner Räume zu verbessern. Angeheizt von dem ständig wachsenden Datenmaterial über Städte (zum Klima, zu den Verkehrsströmen, zum Grad der Umweltverschmutzung, zum Ausmass des Energieverbrauchs usw.), haben sich mehrere Schlüsselbereiche herausgeschält, die als mögliche Anwendungsfelder für Hightech-Eingriffe herangezogen werden; dazu zählen etwa die Bewegungsmuster von Menschen, die Verteilung von Energie, Nahrung und Wasser, der Umgang mit Müll. Die Befürworter stellen sich sprechende Städte vor, die für ihre Bewohner Live-Updates über die Umweltverschmutzung, das Parkieren, den Verkehr sowie die Wasser-, Strom- und Energieversorgung bereitstellen. Dank Erfindungen wie Ultra-Low-Power-Sensoren und webbasierten drahtlosen Netzwerken werden Smart Cities bald schon Wirklichkeit sein. Das andere Lager klärt uns über den Bedarf an «grünen», «biophilen» oder sogar «wilden» Städten auf, in denen die Natur erhalten, wiederhergestellt und wertgeschätzt wird. Selbstverständlich waren Städte immer schon Orte, an denen das Wilde gerade nicht wohnt und die dazu entworfen wurden, die Menschen durch Mauern von der Natur zu trennen.

Eine wachsende Zahl aktueller Forschungsergebnisse belegt jedoch die für die Gesundheit positive Wirkung des Kontakts mit der städtischen Natur. Zu den Vorteilen zählen die Abnahme von Stress, ein gestärktes Immunsystem und erhöhte Konzentrationsfähigkeit. Noch wichtiger sind vielleicht die physischen, psychischen und emotionalen Pluspunkte, die offenbar so wichtig für eine gesunde Kindheit sind. Die Befürworter grüner Städte argumentieren zudem, dass viele der dringendsten Fragen unserer Zeit, darunter der Klimawandel, das Artensterben und der Habitatsverlust, gar nicht angegangen werden können, solange der Mensch die Natur um ihn herum nicht versteht und schützt.
Kein Widerspruch

Da haben Sie es also: Big Data gegen Mutter Natur – zwei Ansichten über die Zukunft der Städte, die allem Anschein nach an entgegengesetzten Enden des Spektrums angesiedelt sind. Die eine Seite schätzt technische Innovation; die andere die Weisheit der Natur und die Bindung zu ebendieser. Schaut man jedoch genauer hin, fällt auf, dass diese beiden Sichtweisen sich keinesfalls ausschliessen. In Wirklichkeit ergänzen sie sich sogar.

Eine Stadt kann durchaus gleichzeitig hochtechnisiert und reich an Natur sein. Heutzutage behauptet kaum noch ein Anhänger grüner Städte, wir müssten «zurück zur Natur». Stattdessen setzen sich die Vertreter dieser Denkschule für eine Zukunft ein, die gleichermassen von Technologie wie von Biologie bestimmt ist. Man prägt neue Begriffe wie «technobiophile Städte» oder «Nature Smart Cities», um dieses Mischkonzept zu beschreiben: urbane Räume, in denen die biologische ebenso wie die digitale Welt willkommen sind.

Ja, in Nature Smart Cities wird es eine Menge begrünter Dächer, begrünter Mauern und vernetzter begrünter Räume geben. Die Aussaat heimischer Pflanzen zieht heimische Insekten an, die wiederum heimische Vögel sowie andere Tierarten anlocken und Hinter-, Innen- und Schulhöfe in Miniatur-Ökosysteme verwandeln. Diese Kleinode urbaner Natur verbessern nicht nur die Gesundheit der Menschen, sondern sind für Unmengen bedrohter Arten die letzte Hoffnung. Darüber hinaus vermögen reich begrünte Städte intelligente Technologien vollauf nutzbar zu machen, die den Bewohnern der Stadt beim Umstieg auf erneuerbare Energien wie Wind- und Wasserenergie oder bei der Nutzung von Erdwärme helfen. Das grüne Verkehrswesen reduziert den CO2-Ausstoss und verbessert den Umweltschutz. Grüne Gebäude können wie Bäume funktionieren, weil sie mit Solarenergie betrieben werden und Abfall wiederverwerten. Städte arbeiten dann wie Wälder.
Mitverantwortung

Interessanterweise betonen beide Sichtweisen bezüglich unserer gemeinsamen urbanen Zukunft die Bedeutung einer informierten und engagierten Bürgerschaft. Es könnte gut sein, dass die Digitaltechnologien und Big Data den Menschen tatsächlich die Kontrolle zurückgeben, beispielsweise in Form erhöhter Partizipation an der Kommunalpolitik («E-Governance»). Darüber hinaus können ganz normale Bürger als Laienwissenschafter oder Laiennaturkundler wichtige Aufgaben bei der Wiederherstellung der urbanen Tier- und Pflanzenwelt übernehmen, indem sie Arten beobachten und Anpassungen vornehmen, um Qualität und Quantität der Natur um sie herum zu verbessern. Wir haben hier also eine Möglichkeit, den Menschen zu helfen, auf der Basis fundierter wissenschaftlicher Daten zu handeln (und nebenbei noch das wissenschaftliche Verständnis zu fördern).

Das Nachdenken über die Zukunft unserer Städte ist sehr viel mehr als nur heisse Luft. Zumindest im urbanen Raum können Mutter Natur und Big Data hervorragende Bettgefährten abgeben. Tatsächlich hängt vielleicht nicht nur unser Überleben, sondern ein Grossteil der Biodiversität unseres Planeten davon ab, dass diese Ehe vollzogen wird. Sollte sie gelingen, werden wir die Geburt einer neuen Stadt erleben – einer Stadt, in der die Menschheit ebenso blüht und gedeiht wie die Natur.";https://www.nzz.ch/feuilleton/oeko-und-techno-im-staedtebau-kein-widerspruch-ld.1325279;NZZ;Scott Sampson;;;
20.09.2013;Ignoranz, Bequemlichkeit und der gläserne Mensch;"Möglich, dass die künftige Geschichtsschreibung einen Streitfall wiedergeben wird, der sich im Jahr 2023 ereignete. Damals, wird es vielleicht heissen, reichte das Internetministerium, das kurze Zeit nach dem NSA-Überwachungsskandal 2013 gegründet worden war, eine Unterlassungsklage gegen die Vereinigung der Datenschutzaktivisten ein. Deren sogenannter «weisser Block» hatte schon lange – unter Berufung auf den französischen Philosophen Gilles Deleuze und andere Kritiker der kybernetischen Kontrollgesellschaft – gefordert, Kapseln der Nichtkommunikation zu erzeugen, zum Beispiel durch Deaktivierung des GPS am Smartphone. Die Deaktivierung war 2023 zwar nicht mehr möglich, aber der Besitz eines Smartphones noch keine Vorschrift. Das wollte das Internetministerium nun ändern.
Science-Fiction?

Denn das Verkehrsministerium beabsichtigte, «Presence Technology», die mit der Präzision von fünf Zentimetern die Position ihres Trägers anzeigt, in der Verkehrsregelung einzusetzen, was auch deswegen wichtig war, weil Fahrzeuge inzwischen geräuschlos fuhren. Kollisionen jeder Art konnten somit blind und taub vermieden werden durch automatisch ausgelöste Warnsignale oder Bremsbefehle an zwei Träger eines Sensors, deren Positionskoordinaten das Abstandslimit unterschritten. Die Datenschutzaktivisten hatten keine Argumente gegen dieses als sicher erachtete Verfahren, forderten aber eine Anonymisierung, da die Vermeidung von Kollisionen zwischen einem Auto und einem Fahrrad nicht die Identifizierung der Fahrer voraussetze.

Dieser Perspektive schloss sich das Internetministerium nicht an, mit der Begründung, modernstes Data-Mining errechne aus der Kenntnis der physischen und psychischen Kondition der Fahrer, ihrer Alltagsroutinen, der Fahrzeugmodelle und vieler anderer verfügbarer Daten die Wahrscheinlichkeit einer Kollision und könne entsprechend effektiv Präventivmassnahmen auslösen. Da Verkehrssicherheit keine Privatangelegenheit sei, dürfe sich kein Bürger der Identifizierung entziehen. Der Aufruf, Lücken in der kybernetischen Kommunikation zu schaffen, wurde somit als verkehrsgefährdend, von manchen auch als terroristisch eingestuft und gerichtlich untersagt.

Science-Fiction? «Presence Technology» und «Smart Things» sind inzwischen so sehr Zeitungsthema wie die Nutzung von GPS weitverbreitete Realität; Algorithmen suchen längst «Big Data» nach verdeckten Korrelationen ab; «Self-Tracker» füttern immer mehr freiwillig den Datenpool mit Beobachtungen zu ihrem eigenen Alltagsverhalten; und Rationalisierung gehört nicht erst seit der Digitalisierung der Gesellschaft zu den Mitteln moderner Staatsführung. Ist es etwa Science-Fiction, dass Menschen bald ihrer Brille befehlen werden, Informationen über den Menschen am Nachbartisch zu finden? In einem Kommentar zu Google Glass wurden den Trägern solcher Überwachungsgeräte höhere Versicherungsbeiträge prophezeit; wegen all der Blessuren und gebrochener Nasenbeine, die sie sich vonseiten Belästigter zuziehen könnten. Doch eher dürfte diese Prophezeiung sich als Fiktion erweisen. Google Glass wird so erfolgreich sein wie das iPad, und die Nutzer werden alle möglichen Discounts erhalten, wenn sie die Versicherungsfirmen mit durch die Brille schauen lassen.

Der NSA-Skandal hat den gläsernen Menschen wieder zum Thema gemacht. Aufrufe zur Rettung der bürgerlichen Grundrechte und des Internets, wie es einst war, versetzen viele Gemüter in Kampfstimmung. Einige Kommentare erwähnten bei der Gelegenheit auch, dass der gläserne Mensch nur teilweise ein Produkt des Geheimdienstes ist. In der Tat: Stärker noch als durch Überwachung wird der Mensch transparent durch Ignoranz, Geiz und Bequemlichkeit.

Ignorant ist die Auskunft: «Ich habe ja nichts zu verbergen.» Weiss man denn, welche Muster die Algorithmen im eigenen Verhalten entdecken und inwiefern einem dies von Nachteil sein kann? Wie kann man sein Grundrecht auf informationelle Selbstbestimmung ausüben, wenn der moderne Analytiker einem gar nicht sagt, was er gefunden hat, und auch nicht fragt, ob er davon öffentlich Gebrauch machen darf? Wie unsolidarisch diese Auskunft zudem ist, zeigte schon eine Nachricht im August 2013 über die erste Trauung zweier Männer in einer evangelischen Kirche. Was vor wenigen Jahrzehnten verboten oder verpönt gewesen war, wird nun gesellschaftlich anerkannt und kirchlich gesegnet, womit viele, die gestern noch etwas zu verbergen hatten, aus heutiger Sicht schon damals keine schlechten Menschen waren. Wer das Lied der Transparenz singt, ist moralisch kaltherzig gegen die Ansprüche von Minderheiten und politisch naiv, weil er unterstellt, die geltenden Gesetze und Moralvorstellungen der Gesellschaft seien unantastbar. Sollte es nicht vielmehr Pflicht aller Bürger sein, das Recht des Verbergens zu schützen, indem es selbst praktiziert wird; vielleicht als in der Verfassung verankerte «Supergrundpflicht»?
Zauberlehrling 2.0

Unsolidarisch ist auch, persönliche Daten für finanzielle Vorteile herzugeben. Weniger deutlich, wenn man individuelle Konsumgewohnheiten für Kumulus-Bons preisgibt oder für den kostenlosen E-Mail-Dienst Google das Mitlesen erlaubt. Aber wer Versicherungsrabatte einstreicht, weil er seinen Autofahrstil oder seine körperliche Bewegung überwachen lässt, sorgt dafür, dass jene, die den errechneten Durchschnitt verpassen oder den Nachweis ihrer Durchschnittlichkeit nicht erbringen wollen, mehr zahlen müssen. Die persönliche Freizügigkeit mit Daten hat Konsequenzen auch für andere.

Der grösste Antrieb für den gläsernen Menschen aber ist die Bequemlichkeit, also die Delegierung möglichst vieler Aufgaben an «intelligente» Alltagsdinge, die untereinander kommunizieren: der Swimmingpool, der sich aufheizt, wenn im Kalender ein Barbecue eingetragen ist; der Kühlschrank, der Bestellungen aufgibt, wenn die Milch alle ist oder ihr Verfallsdatum erreicht hat; das Auto, das Staumeldungen und Baustellenverzeichnisse konsultiert und automatisch die Fahrroute anpasst. Vom Medienwissenschafter Marshall McLuhan stammt die Bezeichnung der Medien als «extensions of man» bzw. «Organverlängerung». Mit dem «Internet der Dinge» übernimmt der Computer nicht mehr nur das Rechnen für uns, sondern auch das Beobachten und Auswerten unserer Umwelt. Dass die Schattenseite der Organverlängerung für McLuhan die «Amputation» ist, wird niemanden überraschen; wir wissen, dass unsere Beine vom Autofahren nicht kräftiger werden. Das Internet der Dinge verdoppelt den Schatten: Es macht unser Denken nicht elastischer und arbeitet umso besser, je mehr es über uns weiss.

Die Kommunikation der Dinge, so wird es in den Geschichts«büchern» der Zukunft heissen, war der Triumph künstlicher Intelligenz und menschlicher Bequemlichkeit über die verbliebenen Datenschutzbemühungen des frühen 21. Jahrhunderts. Das Leben der Menschen wurde einfacher und zugleich kontrollierbarer, denn fast jede menschliche Aktion ging nun mit der Datenakkumulation einher – ein Paradies für Mediziner, Stadtplaner und Verkehrsregler. Der entstandene Big-Data-Pool geht nicht auf den Geheimdienst zurück, sondern ist die Erbschaft der Aufklärung, für die jeder unvermessene Hügel eine Beleidigung der Vernunft war. Die Digitalisierung der Gesellschaft ist die Erweiterung des Zollstocks ins Soziale.

Der gläserne Mensch ist somit Produkt einer kulturellen und technologischen Disposition, wobei McLuhan das Abhängigkeitsverhältnis zwischen beiden fatalistisch bestimmt: «Wir formen unser Werkzeug, und danach formt unser Werkzeug uns.» Eine alte Geschichte also: Die Maschinen übernehmen das Kommando, wie der Supercomputer HAL in Stanley Kubricks Film «2001: A Space Odyssey». Eine Art Zauberlehrling 2.0, nur weiss nun, anders als in Goethes Ballade, auch der Meister nicht, wie er die gerufenen Geister wieder loswird. – Science-Fiction?
«Yes we scan»

Der Technikphilosoph Hans Jonas beschrieb das Problem 1979 in seinem Buch «Das Prinzip Verantwortung» als Ohnmacht gegenüber dem Gelingen: Das Verhängnis des Menschen ist der Triumph des Homo faber, der schliesslich zum willenlosen Vollstrecker seines eigenen Könnens wird. «Yes we scan» stand auf einem Plakat gegen die Überwachungspraxis der NSA, was zusammen mit dem berühmten Satz, der damit gegen Obama aufgerufen wurde, das fatalistische Verhältnis der Moderne zur Technologie kennzeichnet: Wir können immer besser «Big Data» auswerten, und also tun wir es auch.

Sind wir diesem technischen Gelingen schon unterlegen – oder können wir uns der Macht, die die Algorithmen über uns zu haben beginnen, noch entziehen? Die Antwort hängt auch davon ab, welches Problembewusstsein aus der Diskussion um den NSA-Skandal hervorgeht und wo man den Feind vermutet: im Geheimdienst und im Staat, in den Netzgiganten und Programmierern – oder auch in uns selbst, die wir die Delegierung der Unfallvermeidung an Algorithmen so entlastend empfinden, dass wir die dafür nötige Datenpreisgabe nicht nur in Kauf nehmen, sondern von den verbliebenen Fortschrittsfeinden auch erzwingen.";https://www.nzz.ch/feuilleton/ignoranz-bequemlichkeit-und-der-glaeserne-mensch-1.18152511;NZZ;Roberto Simanowski;;;
31.07.2013;Himmlische Verhältnisse in der Atacamawüste;"Trostloser dürfte es auch auf dem Mars nicht aussehen. Ausser Geröll und Staub gibt es hier nichts, an dem das Auge Halt finden könnte. Für Pflanzen ist es viel zu trocken. Die einzigen Farbtupfer im eintönigen Braun sind die Abfallreste, die sich links und rechts der schnurgeraden Strasse angesammelt haben und vom Wind grossflächig verteilt werden. Ab und zu taucht am Wegesrand ein mit Graffiti verzierter Felsbrocken auf. Sonst aber gibt es wenig, was auf menschliche Zivilisation schliessen liesse.
Auf hoher Warte

Unsere Reisegruppe von Journalisten befindet sich im Norden Chiles auf dem Weg von Antofagasta zum Cerro Paranal, einem (einst) 2660 Meter hohen Berg in der Atacama-Wüste. Nach einer zweistündigen Fahrt nähern wir uns endlich dem Ziel unserer Reise. Vor uns erhebt sich der Cerro Paranal, der mit seiner abgesprengten Kuppe einen etwas derangierten Eindruck macht. Umso majestätischer wirken die vier Grossteleskope, die auf dem Plateau thronen. Wüsste man es nicht besser, man könnte sich auf einem Aussenposten der Menschheit wähnen.

Die vier Teleskope – jedes mit einem Spiegeldurchmesser von 8,2 Metern – gehören der Europäischen Südsternwarte (ESO). Zusammen mit vier kleineren Hilfsteleskopen bilden sie das Very Large Telescope (VLT), das seit Ende der 1990er Jahre das europäische Flaggschiff der bodengebundenen Astronomie ist. Die Liste der Entdeckungen, die dem VLT zu verdanken sind, ist lang. Zu den Highlights gehört das erste Bild eines extrasolaren Planeten. Mit dem VLT wurde auch der bisher älteste Stern in der Milchstrasse gefunden und der definitive Beweis erbracht, dass sich im Zentrum unserer Galaxie ein massereiches Schwarzes Loch verbirgt.

Lässt man den Blick vom Cerro Paranal in die Ferne schweifen, fragt man sich, warum die ESO ausgerechnet hier – fernab von Strom, Wasser und anderen Annehmlichkeiten des Lebens – ein Teleskop dieses Kalibers baute. Ein fast das ganze Jahr über wolkenloser Himmel und eine geringe Lichtverschmutzung, das seien die wichtigsten Anforderungen für den Standort gewesen, erklärt uns Andreas Kaufer, der Direktor des Paranal-Observatoriums. Man habe damals Standorte in Chile, Australien und Südafrika evaluiert. Aber nirgendwo seien die Beobachtungsbedingungen so gut wie hier.

Quelle: Eso.org http://www.eso.org/public/outreach/products/virtualtours/index.html

Die europäischen Astronomen sind nicht die Einzigen, die es in die Atacama-Wüste zieht. Auch andere Länder verfolgen hier astronomische Projekte. Der Grund dafür ist die aussergewöhnliche Lage. Gegen Osten bilden die Anden ein fast unüberwindliches Hindernis für Regenwolken, die aus dem Amazonasbecken heranziehen. Im Westen sorgt der kalte Humboldtstrom dafür, dass die vom Pazifik kommenden Wolken abregnen, bevor sie die Küste erreichen. Beides zusammen macht die Gegend um Antofagasta zum wahrscheinlich trockensten Ort der Erde. Im Jahresdurchschnitt werden hier nur wenige Millimeter Niederschlag gemessen. Die Luftfeuchtigkeit liegt unter 10 Prozent. Das sind Bedingungen, wie Astronomen sie mögen – und noch mehr mögen sie, dass sie in vielen Fällen nicht einmal mehr vor Ort sein müssen, um von diesen exquisiten Beobachtungsbedingungen profitieren zu können.
Astronomie als Dienstleistung

Die Zeiten, in denen Astronomen nächtelang vor ihrem Teleskop sassen und Fotoplatte um Fotoplatte belichteten, sind nämlich längst vorbei. Zwar gibt es an den heutigen Grossteleskopen immer noch den sogenannten Visitor Mode, bei dem die Astronomen selbst anreisen, um die wissenschaftlichen Instrumente zu justieren und den richtigen Zeitpunkt für eine Beobachtung festzulegen. Der Visitor Mode wird jedoch mehr und mehr vom sogenannten Service Mode zurückgedrängt. Die Astronomen definieren hier zwar vorab das Objekt und die Bedingungen, unter denen es beobachtet werden soll. Die langen Nachtschichten delegieren sie jedoch an andere.

Das ist am Very Large Telescope nicht anders. Hier werden 60 bis 70 Prozent der Beobachtungen im Service Mode durchgeführt. Dafür steht eine 180-köpfige Mannschaft aus Astronomen, Ingenieuren und Technikern bereit, die den Betrieb der vier Teleskope im Schichtbetrieb sicherstellt. Einer der festangestellten Astronomen ist Valentin Ivanov, den wir nachmittags bei einem Rundgang auf dem Gelände des Large Telescope antreffen. Stolz erzählt Ivanov, dass er nun bereits seit 12 Jahren als Service-Mitarbeiter für die ESO tätig ist. So lange hielten es nicht viele aus. Typischerweise wechselten die festangestellten Astronomen nach vier bis fünf Jahren auf einen Posten, der es ihnen erlaube, der eigenen Forschung nachzugehen. Denn dafür bleibe hier oben kaum Zeit, bedauert Ivanov.

Inzwischen ist es in der Atacama-Wüste Abend geworden. Mit dem Sonnenuntergang ist endlich der Moment gekommen, die Kuppeln der vier Grossteleskope zu öffnen. Wie von Geisterhand gleiten die Schiebetüren zur Seite und geben den Blick auf den Himmel frei, an dem sich zaghaft die ersten Sterne abzeichnen. Im Kontrollraum des Very Large Telescope treffen wir Ivanov wieder. Zusammen mit Arbeitskollegen sitzt er vor einer Armada von Computern und steuert den Ablauf der Messungen. Im Moment besteht seine Aufgabe darin, mit einem der Instrumente die Rotverschiebung einer Galaxie zu messen, um daraus ihre Entfernung zu bestimmen. Warum es ausgerechnet diese Galaxie ist, weiss Ivanov nicht. Mit etwas Glück wird er es vielleicht Monate oder Jahre später erfahren, wenn die entsprechenden Forschungsergebnisse publiziert werden. Das seien schöne Momente, wenn man das Resultat seiner Arbeit an einer Konferenz oder in einer Publikation zu sehen bekomme.

Ein paar Tische weiter sitzt Fernando Selman, ein Kollege von Ivanov. Mit den Beobachtungsbedingungen am heutigen Abend ist er gar nicht zufrieden. Die Luft sei zu unruhig. Das beeinträchtige die Qualität der Beobachtungen, bemängelt er. Zwar ist jedes der vier Teleskope mit einer adaptiven Optik ausgestattet, die es erlaubt, Bildfehler auf raffinierte Weise zu korrigieren. Sind die Störungen aber zu gross, hilft auch das nicht weiter. Abschreiben müsse man die Nacht deshalb aber nicht, sagt Selman. Nicht jede Beobachtung setze ideale Bedingungen voraus. In der langen Liste der anstehenden Aufgaben gebe es immer welche, die sich auch bei weniger guten Verhältnissen erledigen liessen.

Den Rest der Nacht werden Ivanov und Selman damit verbringen, auf ihren Monitoren Zahlenkolonnen und Kurven zu verfolgen, die nur der Eingeweihte versteht. Fast bedauert man die beiden, dass sie nur in gefilterter Form mitbekommen, welch grossartiges Schauspiel sich über ihren Köpfen abspielt. Die Milchstrasse ist zum Greifen nah. Auch die Kleine und die Grosse Magellansche Wolke, zwei Nachbargalaxien der Milchstrasse, sind deutlich zu erkennen. Inmitten der Milchstrasse springen vier helle Sterne ins Auge. Unsere Begleiterin Laura Ventura erklärt uns, dass es sich dabei um das Kreuz des Südens handelt, das bereits den Seefahrern des 16. Jahrhunderts als Orientierungshilfe diente. Laura macht uns auch auf den «Kohlensack» aufmerksam, einen tiefschwarzen Fleck in unmittelbarer Nachbarschaft des Sternbildes. Fast scheint es, als sei der Milchstrasse hier die Lust vergangen, Sterne zu produzieren. Die Erklärung ist jedoch profaner. Bei dem Fleck handelt es sich um eine sogenannte Dunkelwolke. Solche Wolken bestehen aus kalten Gasen und Staub und haben die Eigenschaft, das Licht dahinter liegender Sterne zu verschlucken. Deshalb erscheinen sie dunkel.

Obwohl Dunkelwolken unspektakulär aussehen, stecken sie voller Überraschungen. Davon kann Nicolas Peretto ein Lied singen. Erst vor wenigen Wochen lieferte der Forscher von der Cardiff University handfeste Belege dafür, dass in einer Dunkelwolke in der Milchstrasse, die 11 000 Lichtjahre entfernt ist, gerade ein Monsterstern am Entstehen ist. Schon jetzt hat sich im Zentrum der Wolke Gas mit der Masse von 500 Sonnen zusammengeballt – und immer noch strömt weiteres nach. Peretto und seine Mitarbeiter gehen davon aus, dass sich in diesem Kokon aus Gas in den nächsten 100 000 Jahren ein Stern bilden wird, der 50- bis 100-mal so massereich wie die Sonne ist.
Unsichtbares wird sichtbar

Mit dem Very Large Telescope hätte Peretto allerdings nichts davon mitbekommen. Denn auch mit den besten optischen Teleskopen kann man nicht in die lichtundurchlässigen Dunkelwolken hineinsehen. Für seine Beobachtung nutzte Peretto deshalb das Atacama Large Millimeter/submillimeter Array (Alma), ein 1,5 Milliarden Dollar teures Radioteleskop, das die ESO gemeinsam mit Partnern aus Amerika und Asien in der Atacama-Wüste gebaut hat. Mit seinen 66 untereinander verbundenen Radioantennen gilt Alma als das bis anhin grösste (und teuerste) Projekt der bodengebundenen Astronomie. Alma operiert im Spektralbereich zwischen Infrarot- und Radiostrahlung. Deshalb kann es Dinge sehen, für die das VLT blind ist – etwa die Sternentstehung in Dunkelwolken.

Peretto war einer der ersten Astronomen, die von den Vorzügen dieses aussergewöhnlichen Teleskops profitieren konnten. Offiziell wurde Alma zwar erst diesen Frühling eingeweiht. Die ersten Untersuchungen hatten aber schon im September 2011 begonnen. Der Andrang auf das neue Radioteleskop sei riesig gewesen, erinnert sich Peretto. Obwohl damals erst ein Viertel der Antennen zur Verfügung gestanden habe, seien für den ersten Beobachtungszyklus über 1000 Anträge eingegangen. Davon seien lediglich 100 berücksichtigt worden. Peretto gehörte zu den Auserwählten. Bereits am ersten Tag stand die Beobachtung der Dunkelwolke auf dem Programm.

Zu diesem Zeitpunkt habe er das Alma-Teleskop noch nicht aus eigener Anschauung gekannt, sagt Peretto. Zu Gesicht bekommen habe er es erst Monate später, als er eine Konferenz in Chile besucht habe. Ein grosses Handicap sei das aber nicht gewesen. Um das Alma-Teleskop nutzen zu können, müsse man es nicht in allen seinen Details kennen. Bei der Formulierung des Antrags und der Vorbereitung der Beobachtung sei er von regionalen Alma-Zentren unterstützt worden. Den Rest der Arbeit hätten die speziell geschulten Operateure und Astronomen vor Ort erledigt.

Vor Ort heisst im Falle von Alma allerdings nicht wirklich vor Ort. Zwischen dem Operation-Center und dem auf 5000 Meter stehenden Teleskop liegen 2100 Höhenmeter und eine knapp einstündige Autofahrt. Die Höhe ist ein Tribut an die Millimeter- und Submillimeter-Strahlung, bei der Alma operiert. Da diese Strahlung relativ stark vom Wasserdampf in der Atmosphäre absorbiert wird, wurde der Standort so gewählt, dass ein möglichst grosser Teil der Atmosphäre unter einem liegt.

Quelle: Eso.org http://www.eso.org/public/outreach/products/virtualtours/index.html

Was gut für das Teleskop ist, ist allerdings nicht unbedingt gut für den Menschen. Bevor unsere Reisegruppe Alma besuchen kann, müssen wir einen medizinischen Test über uns ergehen lassen. Zudem müssen wir eine Erklärung unterschreiben, dass wir uns der Risiken bewusst sind, die ein Aufenthalt in dieser Höhe haben kann. Von schwerer Übelkeit ist da die Rede, die sogar zum Tod führen könne. Ganz so schlimm wird es schon nicht kommen, denkt man sich und beherzigt den Tipp der uns begleitenden Krankenschwester, auf dem Weg zum Teleskop viel zu trinken.

Auf einer breiten, aber nicht asphaltierten Piste kämpft sich unser Reisebus den Berg hinauf. Bald passieren wir eine halb verfallene Behausung, die einst von Einheimischen aus getrockneten Kakteen gebaut wurde. Inzwischen beginnt sich die Höhe bemerkbar zu machen. Der Sauerstoffgehalt der Luft ist hier oben nur noch halb so hoch wie auf Meeresniveau. Der eine oder andere in unserer Reisegruppe ist deshalb froh über die Sauerstoffflasche, die wir vor Antritt der Fahrt in die Hand gedrückt bekommen haben. Andere vertrauen eher den lokalen Bräuchen und kauen Kokablätter. Angeblich soll das die Sauerstoffaufnahme ins Blut fördern.
Arbeiten in dünner Luft

Inzwischen sind wir auf der von Vulkanen umsäumten Chajnantor-Hochebene angekommen. Karg ist es auch hier. Die Landschaft ist aber viel abwechslungsreicher als rund um den Cerro Paranal. Die über die Hochebene verstreuten Antennen verleihen der Szenerie fast etwas Surreales. Wir sind froh, dass wir warme Kleidung haben, denn hier oben weht ein kalter Wind. Langsam schleichen wir zwischen den Antennen hin und her. Hier oben arbeiten zu müssen, ist bestimmt kein Vergnügen, denkt man sich.

Tatsächlich ist die Belegschaft vor Ort auf ein absolutes Minimum beschränkt. In Schichten erledigen die Techniker jene Arbeiten, die sich nicht aus der Ferne steuern lassen. Dazu gehören etwa die Überwachung und die Wartung des Supercomputers, der die Signale der 66 Antennen sammelt und sie miteinander korreliert. Obwohl die Techniker an die Höhe gewöhnt sind, werden sie über einen dünnen Schlauch in der Nase ständig mit Sauerstoff versorgt. Das sei eine Vorsichtsmassnahme, erklärt man uns. Der Sauerstoffmangel könne nämlich zu Konzentrationsstörungen führen, die man hier oben lieber vermeiden wolle. Denn jeder Fehlgriff könne teure Folgen haben.

Die Hauptarbeit wird deshalb in der Operation Support Facility auf 2900 Metern geleistet. Von hier aus wird das Alma-Teleskop nicht nur gesteuert, hier wurden auch die Antennen zusammengebaut, bevor sie aufs Hochplateau verfrachtet wurden. Bei einem Durchmesser von 12 Metern und einem Gewicht von 100 Tonnen ist das eine echte Herausforderung. Der Transport ist Sache von Lore und Otto. So heissen die beiden Ungetüme auf 28 Rädern, die eigens für diese Aufgabe gebaut wurden. 20 Meter lang und 10 Meter breit sind diese fahrenden Kolosse, ihre Höchstgeschwindigkeit liegt bei 20 Kilometern in der Stunde. In beladenem Zustand sind sie halb so schnell. Erst jetzt verstehen wir, warum die zum Teleskop führende Piste so breit ist.

Wir verstehen jetzt auch eine Bemerkung, die Lars Lindberg Christensen, der Leiter der Kommunikationsabteilung der ESO, während der Fahrt aufs Chajnantor-Plateau fallengelassen hatte. Mit dem Alma-Teleskop sei man nicht nur in technischer, sondern auch in logistischer Hinsicht an die Grenze des Machbaren gegangen. Dass sich dieser Aufwand gelohnt hat, steht ausser Frage. Obwohl das Alma-Teleskop noch nicht seine volle Leistungsfähigkeit erreicht hat, spielt es bereits jetzt in der Topliga der Teleskope mit. Seit der Inbetriebnahme von Alma ist kaum eine Woche vergangen, in der es nicht für Schlagzeilen gesorgt hat.

Für die ESO ist das allerdings kein Grund, die Hände in den Schoss zu legen. Das nächste Megaprojekt ist bereits in Vorbereitung. Und wieder wird es in der Atacama-Wüste stehen, in Sichtweite des Very Large Telescope. Als Standort wurde der Cerro Amazones ausgewählt. Noch steht der 3064 Meter hohe Berg unversehrt in der Landschaft. Doch schon bald wird auch seine Kuppe weggesprengt – um Platz für ein Teleskop zu machen, das nicht nur «very large», sondern «extremely large» sein wird.";https://www.nzz.ch/wissen/wissenschaft/himmlische-verhaeltnisse-in-der-atacamawueste-1.18125366;NZZ;Christian Speicher;;;
15.05.2020;Der Bund muss raus aus den Datensilos;"Wenn eine Organisation ein neues Kompetenzzentrum gründet, stellt sich dem kritischen Beobachter stets die Frage: Waren diese Kompetenzen bisher nicht vorhanden? So auch bei der Ankündigung des Bundesrats diese Woche. Ab Januar 2021 soll im Bundesamt für Statistik (BfS) ein neues Kompetenzzentrum für Datenwissenschaft entstehen. Hat dieses Amt keine Datenkompetenz?, fragt man sich also. Ausgerechnet das Amt der Statistiker?

Selbstverständlich hat das BfS darin viel Erfahrung. Von den derzeit 814 Mitarbeiterinnen und Mitarbeitern muss den meisten in Sachen Datenverarbeitung und darauf aufbauender Statistik nichts vorgemacht werden. Dieses Wissen ist in rauen Mengen vorhanden.

Wer verstehen will, worum es dem Bundesrat wirklich geht, der muss zwischen den Zeilen des kurzen Pressecommuniqués lesen. Da steht zum Beispiel: «Sie [die Datenwissenschaft] umfasst Algorithmen, mit denen bestimmte komplexe Aufgaben automatisiert werden können. Dadurch kann der Bund seine Aufgaben effizienter erfüllen.» Weiter: Das Zentrum werde zwar im BfS angesiedelt sein. Es handle sich dabei aber um ein interdisziplinäres Konstrukt. Und schliesslich: «Das Kompetenzzentrum wird dazu beitragen, unter Berücksichtigung des Datenschutzes transparente Informationen zu produzieren.»

Übersetzt heisst das: Der Bundesrat erkennt in der Verwaltung einen ineffizienten Umgang mit Daten. Er sieht auch, dass die Bundesämter in Datenfragen nicht eng genug kooperieren. Und er anerkennt, dass veröffentlichtes Zahlenmaterial und die daraus abgeleitete Information oft nicht transparent sind, weil die genauen Methoden oder Rohdaten fehlen.

Tatsächlich ist es so, dass viele Ämter – vom Bundesamt für Gesundheit bis zur Eidgenössischen Finanzverwaltung – Mühe damit haben, Daten untereinander oder mit der Öffentlichkeit zu teilen. Zu gross ist die Angst davor, die Interpretationshoheit zu verlieren. Im Einzelfall ist es in der Vergangenheit gar vorgekommen, dass ein Amt Daten im PDF-Format weitergegeben hat, damit die empfangende Stelle die Daten erst mit Zeitverzögerung nutzen konnte. Zur statistischen Auswertung oder Verknüpfung mit den eigenen Daten mussten sie in solchen Fällen zuerst von Hand aus den digitalen Dokumenten herausgezogen werden. Oder es musste Software entwickelt werden, damit die Daten mit Computerunterstützung extrahiert werden konnten. Beides kostet Zeit und Steuergelder.

Selbstverständlich gibt es Ausnahmen. Das Bundesamt für Landestopografie (Swisstopo) hat erst im April nach jahrelangen Gesprächen angekündigt, bis Ende 2020 sämtliche digitalen Geobasisdaten der Schweiz frei zur Verfügung zu stellen. Für alle Unternehmen, die mit Geodaten arbeiten, wie etwa Architekturbüros oder Ersteller von Online-Karten-Applikationen, bedeutet dies eine finanzielle Entlastung und im besten Fall einen Entwicklungsschub.

Doch die Initiative von Swisstopo bleibt eben eine Ausnahme. Die meisten Bundesämter horten ihre Daten weiterhin am liebsten in Silos. Die wichtigste Aufgabe des neuen Datenzentrums des BfS ist es deshalb, die Silomentalität aufzubrechen. Nur dann können die vom Bundesrat erwähnten Algorithmen gebaut werden, die den Verwaltungsapparat effizienter machen könnten. Vor allem aber müssen die Kantone einbezogen werden. Denn in den meisten Fällen sind sie es, die die Daten erfassen, verarbeiten und an den Bund weiterleiten. Hier braucht es klare Regeln, wie Daten erfasst werden, und es braucht universelle Schnittstellen, damit die Kantone Daten – am besten auf Knopfdruck – effizient mit Dritten teilen können. Ebenso wichtig wie Daten-Know-how sind im vom Bundesrat angekündigten neuen Kompetenzzentrum deshalb Sozialkompetenz und das richtige Gespür für die Mechanismen der föderalen Schweiz.";https://www.nzz.ch/meinung/der-bund-muss-raus-aus-den-daten-silos-ld.1556706;NZZ;Barnaby Skinner;;;
28.05.2020;«Das Bundesamt für Gesundheit muss die Daten gar nicht schicken. Die bleiben schön bei ihm»;"Herr Ulrich und Herr Kuonen, wie haben Sie sich während der Corona-Krise informiert?

Ulrich: Über alle möglichen Kanäle. Soziale Netzwerke waren für mich wichtig. Das Berufsnetzwerk Linkedin etwa. Dort war die Diskussion eine andere als bei Facebook, viel sachlicher. Und natürlich habe ich mich auch über die klassischen Medien informiert: Fernsehen, Radio, Zeitung – logischerweise habe ich auch die NZZ gelesen. Ich bin ein News-Junkie.

Kuonen: Ich las vor allem meine Hauszeitung, den «Walliser Boten». Und ich bin auf dem sozialen Netzwerk Twitter aktiv. Dort habe ich viel Zeit verbracht.

Wurden Sie gut informiert?

Ulrich: Ich habe in den Medien eine Lernkurve beobachtet. Zuerst ging es darum, möglichst viele Daten zu Corona-Infizierungen zu haben. Hauptsache, man hatte irgendetwas. Auch das BfS spürte diese grosse Nachfrage. Wir zählten im Monat März 1,5 Millionen Seitenaufrufe auf unserer Website. Normalerweise sind es etwa 1 Million, also zwei Drittel davon. Dann setzte die zweite Phase ein. Plötzlich begannen sich einige Medien zu fragen: Wie repräsentativ sind diese Daten? Wie vergleichbar? Wer hat die Daten wie erhoben?

Kuonen: Ich habe davon nicht genug gesehen und hätte mehr Aufklärungsarbeit von den Medien erwartet. Ich sah viele sinnlose Datenvisualisierungen. Ich finde, die Zeitungen und das Fernsehen hätten mehr Zeit damit verbringen müssen, den Leuten zu erklären, was aus den dargestellten Kurven eben nicht zu lesen war.

Ulrich: Im Grossen und Ganzen war die Pandemie doch ein Paradebeispiel dafür, wie der gesellschaftliche Meinungsbildungsprozess funktioniert und welche Rolle Daten dabei spielen. Die letzten Monate haben mir gezeigt, dass das Zusammenspiel der Medien, der Politik und der Verwaltung in unserer Demokratie gut funktioniert. Und die genannte Lernkurve sehe ich überall – nicht nur bei Medienkonsumenten und den Journalisten selber. Auch in der Verwaltung hat man viel gelernt.

Hat das BAG als Hüter der Daten zu Corona-Infizierten einen guten Job gemacht?  Ulrich: Es trägt die Verantwortung für die erhobenen Daten. Und im Gesundheitswesen sind Daten eben besonders heikel. Näher beim Menschen kann man ja kaum sein. Der Datenschutz spielt hier eine grosse Rolle. Das Amt muss deshalb sehr genau prüfen, welche Daten es freigibt. Und welche es zurückhält. Ich glaube, was die Publikation von Daten angeht, in welchem Format, zu welchem Zeitpunkt, da hat wie gesagt auch das BAG in den letzten Wochen viel gelernt. Und eben nicht nur das BAG, die ganze Verwaltung.

Mitten in der Corona-Krise haben Sie kommuniziert: Ab 2021 wird das Bundesamt für Statistik ein Kompetenzzentrum für Datenwissenschaften schaffen. Warum?

Ulrich: Wir arbeiten schon länger daran. Das Ganze wurde schon im Jahr 2017 vom Gesamtbundesrat angestossen. Vielleicht wird jetzt in der Krise und dem Daten-Durcheinander, das teilweise geherrscht hat, die Dringlichkeit davon der Öffentlichkeit irgendwie klarer. Der Bundesrat hat verstanden: Wenn jedes Amt seine eigenen Daten erhebt und analysiert, dann entstehen ganz viele Doppelspurigkeiten. Und das neue Kompetenzzentrum kann zur Lösung dieses Problems beitragen.

Tun Sie da dem Bundesrat nicht zu viel Ehre an? Er reagierte damals auch auf politischen Druck. Aus Sicht des Steuerzahlers ergibt es tatsächlich keinen Sinn, wenn Daten vom Bund zwei- oder gar dreimal erhoben werden, nur weil die Ämter keine Daten miteinander teilen wollen.  Ulrich: Ja, am Anfang stand eine Motion der FDP-Fraktion: «Keine Doppelspurigkeiten in der Verwaltung». Für uns war dieser Vorstoss eine Steilvorlage. Er forderte den Bundesrat im Jahr 2016 auf, sicherzustellen, dass Unternehmen nicht die gleichen Daten in unterschiedlichen Verfahren an den Bund übermitteln müssen. Und die Idee des Kompetenzzentrums ist es nun, solche Lösungen zu unterstützen, indem innerhalb der Verwaltung Daten-Brücken gebaut werden.

Kuonen: Im Grunde geht es darum, einen Kulturwandel herbeizuführen. Die Bundesverwaltung soll offener mit ihren Datenschätzen umgehen und das Potenzial von innovativen datenwissenschaftlichen Methoden ausschöpfen.

Der vielzitierte Kulturwandel also. Das hört sich immer so wunderbar an. Aber warum sollen Bundesämter, die seit Jahrzehnten Datenhoheiten geniessen, diese plötzlich aufgeben?

Kuonen: Als ich 2016 als Statistiker das Mandat beim BfS im Bereich Datenwissenschaft begann, traf ich auf die Haltung: Ah, da kommt jetzt der von der Privatwirtschaft, um mit seinen neuen Ideen aus dem Bund ein Startup zu machen, um agiler zu werden. All diese nichtssagenden Schlagwörter eben. Aber ich habe ein inneres Feuer für das Statistik-System Schweiz. Und dieses Feuer versuche ich allen in der Verwaltung zu vermitteln.

Aber was heisst das? Reden wir wieder über den Fall des Bundesamts für Gesundheit. Seit Wochen weigert sich das BAG, Daten zu Corona-Infizierungen mit der Öffentlichkeit oder mit der Forschung zu teilen. Dabei könnte man anhand dieser Daten verstehen, wo aktuell die grösste Gefahr besteht, sich mit dem Virus anzustecken.

Ulrich: Der Bundesrat hat letztes Jahr beschlossen, dass nicht schützenswerte Daten ab sofort per Default mit der Öffentlichkeit geteilt werden müssen. Wir sind die operative Geschäftsstelle dieser Strategie. Und wir müssen dafür sorgen, dass sie umgesetzt wird. Wenn das BAG nun aufgrund rechtlicher Abwägungen zu dem Schluss kommt, dass die Corona-Infizierungen etwa auf Gemeindeebene als besonders schützenswert eingestuft werden, weil damit in kleinen Gemeinden Personen identifiziert werden können, dann gilt das.

Aber grundsätzlich kann dieses Argument doch für alle Daten gelten.

Ulrich: Unser neues Kompetenzzentrum für Datenwissenschaften gibt uns nun Möglichkeiten, einen neuen Weg einzuschlagen. Wir werden die Daten nutzen und datenwissenschaftlich auswerten können, ohne den Datenschutz zu verletzen und ohne dass das BAG die Rohdaten abgeben muss. Im Grunde schaffen wir hier eine geschützte Umgebung, um datenwissenschaftliche Methoden testen und dann die Testergebnisse und die Methoden publizieren zu können. Die Rohdaten bleiben bei den Fachverantwortlichen – und trotzdem kann das Potenzial der Daten ausgeschöpft werden. Zudem können die Fachverantwortlichen die Resultate ja auch selber nutzen und weiterbearbeiten.

Das ist interessant. Könnten beispielsweise auch Medien und damit die Öffentlichkeit Teil einer solchen geschützten Datenplattform werden?

Ulrich: Warum nicht? Vielleicht nicht mit Zugriff auf alle Rohdaten, aber auf einen Teil der Daten und auf die datenwissenschaftlichen Methoden, damit für die Öffentlichkeit nachvollziehbar bleibt, wie eine gewisse datenbasierte Aussage zustande kommt. Um beim BAG-Beispiel zu bleiben: Es könnte der Eindruck entstehen, dass in der Gemeinde X das Risiko einer Covid-19-Ansteckung grösser ist als in der Gemeinde Y. Dann könnte dieser Verdacht in einem nächsten, unabhängig geführten Schritt überprüft werden, bevor Schlussfolgerungen gezogen und allfällige Massnahmen ergriffen werden.

Mit anderen Worten: Das neue Kompetenzzentrum für Datenwissenschaften des BfS funktioniert als Schnittstelle nicht nur für den Datentausch innerhalb der Verwaltung, sondern auch zur Öffentlichkeit oder zur Privatwirtschaft?

Kuonen: Ich möchte hier die Daten-Diskussion abbrechen. Es geht um den Einsatz innovativer datenwissenschaftlicher Methoden. Daten sind das Mittel zum Zweck. Um nochmals Ihr Beispiel zu nennen: Das BAG muss die Daten gar nicht schicken, die bleiben schön bei ihm. Es geht darum, das Amt mithilfe datenwissenschaftlicher Ansätze darin zu befähigen, die entwickelten Algorithmen auf seine Daten in seinem Haus anzuwenden.

Auch Sie brauchen also keinen direkten Zugriff auf die Rohdaten?

Kuonen: Genau, wir selber brauchen keinen Dateneinblick, vorausgesetzt, die Qualität stimmt. Das Einzige, was zurückkommt, ist das Resultat der Rechnung des Algorithmus. Das ist die Zukunft der Datenwissenschaft. Es geht nicht darum, mit den Daten herumzuspielen – hier etwas Cooles zu bauen, da etwas Cooles. Im Fokus ist immer die ganz konkrete gewünschte Leistung vom Zentrum. Das könnte zurzeit also sein: die Häufung von Corona-Infizierungen räumlich darstellen, ohne den Datenschutz zu verletzen.

Wie viele Leute werden künftig in diesem neuen Kompetenzzentrum arbeiten?

Ulrich: Wir beginnen mit zehn Vollzeitstellen.

Finden Sie diese Leute mit dem nötigen Know-how in der Schweiz? Oder müssen Sie international aktiv werden?

Kuonen: Datenwissenschaft ist Team-Sport. In der Privatwirtschaft sehe ich eine grosse Nachfrage nach «Data-Scientists». Das ist «Guguus». Das Kernteam muss aus Leuten bestehen, die etwas von maschinellem Lernen verstehen. Also davon, wie man mit Daten einen Algorithmus trainiert. Dann braucht es Leute, die Datenbanken pflegen können und wissen, was Daten-Management bedeutet; dann braucht es ein paar Leute mit sehr guten Statistik-Kenntnissen oder Leute, die wissen, wie man Daten verknüpft. Das sind ganz unterschiedliche Skills, die da gefragt sind. Vor allem müssen die Leute kreativ, kommunikationsfähig und anpassungsfähig sein und die Probleme erkennen und lösen wollen.

Haben Sie eine Vision für das Kompetenzzentrum?

Kuonen: Ich bin nur der Berater des Bundes. Langfristig stelle ich mir vor, dass die gesamte öffentliche Hand eine interne Suchmaschine hat, in der die Datensätze aller Ämter beschrieben sind; genauso die Methoden, mit denen die Daten analysiert wurden und werden; und eine nachvollzierbare Dokumentation der einzelnen Schritte im datenwissenschaftlichen Prozess. Davon träume ich.

Das tut man auch als Journalist oder als interessierter Bürger.

Ulrich: Es geht uns grundsätzlich darum, ein System von Wissen aufzubauen. Covid-19 zeigt dessen Dringlichkeit. In der Schweiz ist der Datenschutz sehr wichtig. Unsere Idee ermöglicht es uns als Land, alle verfügbaren Daten zur Bekämpfung des Coronavirus zu nutzen, ohne dabei den Datenschutz auszuhebeln. So etwas haben wir heute nicht.

Haben Sie denn konkrete Anwendungen, die Ihnen bereits vorschweben?

Ulrich: Das Zentrum startet offiziell erst im Januar 2021. Aber unter der Internetadresse www.experimental.bfs.admin.ch finden Sie erste kleinere Vorprojekte: etwa ein Mobilitäts-Monitoring von 2500 Personen. Es zeigt, wie sich während der Pandemie das Bewegungsverhalten der Schweizerinnen und Schweizer verändert hat. Oder eine Schätzung, wie sich die Erwerbsbeteiligung von Personen auf Gemeindeebene Jahr um Jahr verändert.

Viel Arbeit des Kompetenzzentrums dient wohl internen Zwecken. Wenn künftig in der Verwaltung Algorithmen zum Einsatz kommen: Werden Sie da immer transparent machen, wie diese Algorithmen funktionieren?

Kuonen: Wir bauen hier ein Dienstleistungszentrum. Die genutzten datenwissenschaftlichen Methoden werden grundsätzlich – unter Wahrung des Datenschutzes – immer dokumentiert und öffentlich gemacht.

Unter Wahrung des Datenschutzes – das heisst also: Nicht alle Algorithmen, die das Zentrum künftig entwickelt, werden transparent sein?

Kuonen: Die ersten Projekte, die wir seit 2018 durchgeführt haben, hatten dieses Mantra: eine nachvollziehbare Dokumentation, und der gesamte Code musste reproduzierbar sein. Künftig werden wir die erforderliche Transparenz jedes einzelnen Schrittes im datenwissenschaftlichen Prozess in jedem einzelnen Fall prüfen müssen. Neben der Transparenz werden aber auch den Grundwerten Datenschutz, Datensicherheit, Daten-Governance und Reproduzierbarkeit grösste Bedeutung beigemessen.";https://www.nzz.ch/schweiz/das-amt-muss-uns-nur-erlauben-unsere-algorithmen-mit-seinen-daten-zu-fuettern-ld.1558564;NZZ;Barnaby Skinner;;;
18.09.2020;Nachhaltigkeit mit mehr und besseren Daten bewertbar machen;"Die Kapitalzuflüsse in nachhaltige Anlagen und das Interesse an besagter Anlageform haben sich nach Covid-19 weiter beschleunigt. Die Pandemie brachte Nachhaltigkeitsprobleme im Gesundheitswesen und in vielen anderen Bereichen zum Vorschein. Die Regierungen konzentrieren sich in ihren Wiederaufbauplänen stärker auf Nachhaltigkeit, und die Präferenzen der Konsumenten verlagern sich in die gleiche Richtung. Mit Folgen: In der ersten Hälfte 2020 kamen nachhaltige Anlagefonds trotz der signifikanten Volatilität an den Finanzmärkten in den Genuss robuster Zuflüsse an Kundengeldern.

Sprichwörtlich lautet das Problem: «Was man nicht messen kann, kann man nicht steuern.» Anleger, die Nachhaltigkeit in ihre Finanzinstrumente einbeziehen möchten, müssen die Nachhaltigkeitsprofile von Unternehmen und Staaten vergleichen und ihre relative Performance prüfen. Hierzu sind umfassende, einheitliche Informationen über ihre Aktivitäten notwendig. Dank einer Kombination aus staatlichen Vorschriften, Anlegernachfrage und Unternehmensinitiativen hat sich die Transparenz in der Nachhaltigkeit in den letzten zehn Jahren verbessert. Doch trotz den ermutigenden Fortschritten, die gemeinschaftlichen Initiativen wie dem Impact Management Project zu verdanken sind, muss die Branche die unterschiedlichen globalen Offenlegungsrahmen vereinheitlichen, um die Aussagekraft von Nachhaltigkeits-Ratings zu verbessern.
Ein breites Spektrum

Nachhaltigkeits-Ratings erfassen ein breites Spektrum von Faktoren – von der Diversität am Arbeitsplatz bis zu Umweltauswirkungen. Zusätzlich wird ihre Ermittlung durch die Komplexität der einzelnen Unternehmen erschwert, die möglicherweise mit globalen Lieferketten in mehreren Jurisdiktionen arbeiten. Viele Anleger sehen es als hilfreich an, wenn sie mit einem Gesamt-Nachhaltigkeits-Rating arbeiten können, das die verschiedenen Faktoren nach ihrer Wesentlichkeit oder Relevanz gewichtet.

Bei solchen Ratings sind diese Faktoren jedoch zwangsläufig unterschiedlich gewichtet, je nachdem, welcher Aspekt für die Analyse am wichtigsten erscheint. Für viele Anleger ist es zunehmend wichtiger geworden, die Ratings in ihre Komponenten zu zerlegen, also zum Beispiel den Umgang eines Unternehmens mit Wasser oder seine Haltung in Bezug auf Produkte und Dienstleistungen zu betrachten. Manche schlüsseln sie auch noch detaillierter auf. Damit können Anleger diese Komponenten in ihren Beurteilungen flexibler gewichten.

Es ist davon auszugehen, dass Anleger detailliertere Nachhaltigkeitsdaten als Basis für ihre Entscheidungen fordern. Ein Zugang zu den Rohdaten ermöglicht es, Portfolios auf individuelle Präferenzen zuzuschneiden, und verbessert die Transparenz in Bezug auf wichtige Aspekte. Die Tatsache, dass viele Datenanbieter ihre Gesamt-Nachhaltigkeits-Ratings mittlerweile kostenlos zur Verfügung stellen, verdeutlicht die zunehmende relative Bedeutung der zugrunde liegenden Rohdaten.

Die Technologie sorgt ebenfalls für einen Wandel. Obwohl sich die Transparenz und Vergleichbarkeit durch Regulierungsbestimmungen und die Annäherung der Standards verbessert haben, sind die von den Unternehmen gemeldeten Nachhaltigkeitsdaten immer noch rückblickend. Alternative Daten können Einblicke bieten, die zeitnaher und für Anlageentscheidungen relevanter sind.

Beispiele dafür sind Daten von Satelliten und Sensoren, die Emissionen messen, die Unternehmen nicht selbst ermitteln können, oder datenwissenschaftliche Instrumente, die frühzeitig auf Kontroversen hindeuten oder die Stimmung erfassen. Solche Ansätze können Lücken schliessen und ausgewiesene Daten ergänzen. Das nächste Ziel sind Daten zu den tatsächlichen sozialen und ökologischen Auswirkungen von Investitionen. Solche Daten würden es den Anlegern erleichtern, ihre Engagements nach Risiko- und Ertragseffekten zu optimieren.
Ein Flickenteppich

Ohne detaillierte, einheitliche Offenlegungen von Unternehmen und Staaten, die am Kapital der Anleger interessiert sind, stösst die Technologie jedoch an ihre Grenzen. Anleger, die auf bessere Kennzahlen drängen, sei es für Berichte an Anspruchsgruppen oder weil sie ihr Portfolio ergebnisorientiert steuern möchten, werden weiterhin eine treibende Kraft für eine verbesserte, detailliertere Berichterstattung sein.

Angesichts des wachsenden Flickenteppichs nationaler und grenzüberschreitender Regulierungsbestimmungen zu diesem Thema muss die Branche auch selbst – über Rechnungslegungsgremien und andere Organisationen – nach globalen Standards streben, statt sich ausschliesslich darauf zu verlassen, dass die Regierungen dieses Problem lösen werden.

Wenn ihnen umfassendere Informationen zur Verfügung stehen, können Anleger besser zwischen wesentlichen und unwesentlichen Daten unterscheiden und Investments im Quervergleich beurteilen. Obwohl sich diese Ratings gelegentlich unterscheiden werden, wird sich die Informationslage verbessern, und es wird mehr Transparenz darüber herrschen, warum sie sich unterscheiden und in welcher Beziehung dies steht zu der Philosophie dahinter.

Intelligent ausgewertet, ist das wachsende Universum der Nachhaltigkeitsdaten eine gute Nachricht für Anleger, die sich der Ansicht anschliessen, dass man nicht steuern kann, was man nicht messen kann. Es ist jedoch für alle Anleger eine gute Nachricht – nicht nur für Anleger mit Fokus auf nachhaltiges Investieren. Denn je mehr wesentliche Informationen verfügbar sind, umso besser ist der Einblick in Themen, die wirklich eine Rolle spielen.";https://www.nzz.ch/finanzen/nachhaltigkeit-mit-besseren-daten-bewertbar-machen-ld.1574047;NZZ;Markus Haefele, Anton Simonet;;;
12.04.2019;Der westliche Vorsprung in der Militärtechnologie schwindet;"Seit Jahrzehnten setzt der Westen in der Militärtechnik auf überlegene Qualität statt Masse. Dies hat das globale Kräftegleichgewicht mitbestimmt. So konnten die USA und ihre Verbündeten ab Mitte der siebziger Jahre eine «Ausgleichsstrategie» gegenüber der Sowjetunion verfolgen. Der zahlenmässigen Überlegenheit der Ostblockstaaten sollte mit weit reichenden Sensoren, Präzisionswaffen und überlegenen Führungssystemen entgegengewirkt werden. Der Plan ging auf. Die Administration Reagan verfolgte in den achtziger Jahren sogar das Ziel, die Sowjetunion unter Rückgriff auf fortgeschrittene Technologien regelrecht «kaputtzurüsten».

Dass hochentwickelte Waffensysteme tatsächlich eine rapide Schwächung des Gegners bei geringen eigenen Verlusten ermöglichen, stellten die USA und ihre Verbündeten bald nach Ende der Blockkonfrontation praktisch unter Beweis: Im Golfkrieg 1991 hatten die nach sowjetischem Muster ausgerüsteten Streitkräfte des Iraks der von den USA angeführten Koalition zur Befreiung Kuwaits nichts entgegenzusetzen. Seitdem galt die massive technologische Überlegenheit der mit Marschflugkörpern, Präzisionsbomben und Tarnkappenflugzeugen ausgerüsteten amerikanischen Streitkräfte und ihrer Verbündeten als unbestritten. Die westlichen Hauptwaffensysteme waren denjenigen der Herausforderer so klar überlegen, dass sie zugleich in kleineren Stückzahlen beschafft werden konnten. An diesem Nimbus der westlichen Wehrtechnik haben auch die Kriege im Irak und in Afghanistan wenig geändert – sie demonstrierten nur die Unfähigkeit westlicher Streitkräfte, ihre technologische Überlegenheit im für sie neuen Kontext der Aufstandsbekämpfung in strategische Erfolge umzusetzen. Im nächsten Jahrzehnt muss sich der Westen hingegen mit dem möglichen Verlust der Technologieführerschaft auseinandersetzen. Vor allem China hat die Voraussetzungen geschaffen, um in wichtigen Teilbereichen mit dem Westen gleichzuziehen. Büssen die westlichen Staaten und vor allem die USA ihre gewohnte militärtechnologische Führerschaft ein, so wird dies weitreichende Folgen für sie und die Weltordnung insgesamt haben.
Bröckelndes Fundament

Dabei geht es nicht länger nur um die Frage, ob Konkurrenten in der Lage sind, Technologien stur zu kopieren. Vielmehr sind es grundlegend veränderte Innovationsmuster, aus denen sich die Möglichkeit ergibt, den Westen ernsthaft herauszufordern. Die grossen Fortschritte in der Wehrtechnik, von denen westliche Streitkräfte bis heute zehren, beruhten im Kern auf staatlichen Investitionen in spezialisierte und nach aussen abgeschirmte Forschung und Entwicklung (F&E). Die daraus resultierenden Technologien wurden dann teilweise – etwa im Fall des Satellitennavigationssystem GPS oder des Internets – für kommerzielle Zwecke zur Verfügung gestellt. Es handelte sich also um klassische Spin-offs militärischer Anwendungen. Bei anderen Technologien – etwa zur Reduktion von Radarsignaturen («Stealth») – wurde die Geheimhaltung nie aufgegeben. Sie zu kopieren, ist auch heute noch mit grossem Aufwand verbunden.

Die kommende Generation militärisch nutzbarer Technologien speist sich demgegenüber immer öfter aus zivilen F&E-Prozessen, die von privaten Technologiefirmen vorangetrieben werden. Das nötige Grundlagenwissen wird grenzübergreifend aufgebaut, an den Universitäten weitergegeben und in wissenschaftlichen Journals breit gestreut. Das notwendige Anwendungswissen kann oft mit legitimen Mitteln – durch Kauf, Lizenzierung, Übernahmen oder Joint Ventures – erworben werden. Dies gilt für Bereiche wie die Bio-, Nano-, oder Quantentechnologie ebenso wie für Robotik, künstliche Intelligenz und Datenwissenschaft. Wo die Marktmechanismen nicht weiterführen, wird mit Industriespionage nachgeholfen. Statt ausgebildeten Agenten kommen dabei vermehrt technische Mittel und eigene Staatsbürger ohne nachrichtendienstliche Spezialkenntnisse zum Einsatz.

In einem System globaler Wissenstransfers ist Datendiebstahl jedoch bei weitem nicht mehr die einzige Möglichkeit, um zu den Technologieführern aufzuschliessen. Immer öfter folgt auf die ursprüngliche Aneignung neuer Technologien eine steile Lernkurve, an deren Ende die eigenständige Weiterentwicklung auf hohem Niveau steht. Dieses Muster zeigt sich besonders eindrucksvoll im Fall der Volksrepublik China. Sie hat im F&E-Bereich massiv in Humankapital und Infrastruktur investiert. Andere Akteure – Indien, Malaysia, Vietnam oder Brasilien etwa – haben gute Chancen, dieses Modell mittelfristig mit allerdings wohl geringerem Erfolg nachzubilden. Russland beschreitet demgegenüber bis jetzt einen eigenständigen, vom Staat gezeigten Weg, der sich in Teilen weiterhin aus der Technologiebasis der untergegangenen Sowjetunion speist. Aber auch hier entsteht mittlerweile ein zeitgemässes, zum Teil privatwirtschaftlich getriebenes Innovationsumfeld.
Chinesische Vorteile

Obwohl die erleichterte Aneignung zunächst ziviler Technologien eine kritische Voraussetzung für die zukünftige militärische Anwendung darstellt, gehen damit weitere Herausforderungen einher. Besonders die Systemintegration – also die erfolgreiche Bündelung zahlreicher Technologien zu einem funktionstüchtigen Gesamtsystem – stellt im militärischen Bereich eine Hürde dar. Dabei sind bei zivilen Ausgangskomponenten meist starke Anpassungen erforderlich, um deren Sicherheit, Zuverlässigkeit und Widerstandsfähigkeit unter den erschwerten Bedingungen des militärischen Einsatzes zu gewährleisten. Je mehr zivile Komponenten zum Einsatz kommen, desto mehr teure Arbeit ist hier vonnöten.

Vordergründig scheint dies einen Vorteil für die erfahrenen Systemintegratoren im Westen darzustellen. Es bleibt jedoch abzuwarten, ob das chinesische Innovationsmodell sich in der Nutzbarmachung vorrangig ziviler Technologien nicht als überlegen erweist. Zwei Umstände könnten darauf hindeuten. Zum einen ist der Staatskapitalismus chinesischer Prägung durch eine enge Verbindung militärischer und kommerzieller Strukturen geprägt, die unter dem Stichwort «zivil-militärischer Fusion» in naher Zukunft weiter vertieft werden soll. Somit können Wirtschaftsakteure unmittelbar zur Umsetzung militärischer Prioritäten verpflichtet werden.

Zum anderen kann der Kostenfaktor, der den Rüstungsanstrengungen westlicher Demokratien klare Grenzen setzt, bei der Umsetzung strategisch wichtiger Projekte fast nach Belieben ausgehebelt werden. Dabei können die staatlichen Stellen gegenüber den formal privatwirtschaftlichen Akteuren der Volksrepublik starken Einfluss geltend machen.

Wie sich eine Kombination aus wachsender Wirtschaftskraft und Innovationsfähigkeit mittelfristig auf das militärische Gleichgewicht auswirkt, lässt sich bereits heute anhand des Taiwan-Szenarios verdeutlichen. 1996 hatte sich die Führung in Peking von der amerikanischen Marine demonstrieren lassen müssen, wie wenig sie gegen eine mögliche Intervention der USA zugunsten der «abtrünnigen» Inselrepublik ausrichten könnte. Den Drohgebärden der amerikanischen Flugzeugträger-Kampfgruppen rund um die Taiwanstrasse hatten die chinesischen Streitkräfte nichts Entsprechendes entgegenzusetzen. Seitdem hat Festlandchina sein Verteidigungsbudget um rund 750 Prozent erhöht und seine Streitkräfte auf einen Konflikt mit den USA eingestellt. Dabei setzte die Volksbefreiungsarmee ihrerseits auf eine «Ausgleichsstrategie», in der neben einem Luftverteidigungssystem zum Teil russischer Provenienz vor allem präzisionsgelenkte, konventionell bestückte Kurz- und Mittelstreckenraketen eine zentrale Rolle spielen.
Steigende Interventionskosten

Obwohl sie auf einer anfangs noch sehr bescheidenen industriellen Kapazität beruhten, führten diese Massnahmen zu einer dramatischen Verschlechterung der Ausgangslage für die amerikanischen Streitkräfte. Eine Studie der Rand Corporation kam 2017 zum Schluss, dass sich der notwendige Kräfteansatz für eine erfolgreiche Luftoffensive zur Unterstützung Taiwans seit 1996 mindestens verzehnfacht hat. Die alles entscheidende Eroberung der Lufthoheit ist damit kaum mehr in einem operativ relevanten Zeitraum und bei erträglichen eigenen Verlusten zu bewerkstelligen. Vor diesem Hintergrund verwundert es auch nicht, dass das Selbstbewusstsein, mit dem Washington traditionell militärische Unterstützung zugesichert hatte, geschwunden ist.

Mit der Fähigkeit, zeitgerecht und mit politisch vertretbaren Kosten militärisch zu intervenierten, steht und fällt nicht nur das Bündnis mit Taiwan. Auch Alliierte, die nicht an vorderster Front von den sich verschiebenden Kräfteverhältnissen betroffen sind, müssen damit rechnen, dass die Glaubwürdigkeit der amerikanischen Sicherheitsgarantien weiter erodiert. Dies gilt umso mehr, als mögliche Herausforderer bei den Militärtechnologien der nächsten Generation – etwa bei künstlicher Intelligenz, Hyperschallwaffen oder Quantenradaren – keine jahrzehntelangen Rückstände aufholen müssen.

Die Abnahme der westlichen militärtechnologischen Überlegenheit lässt sich vermutlich nicht mehr verhindern. Allerdings werden insbesondere die USA den Verlust ihres Vorsprungs keinesfalls passiv hinnehmen. Die systematische Abschöpfung technologischer Kompetenz und der Diebstahl geistigen Eigentums werden nicht länger als Kavaliersdelikte behandelt. Dementsprechend wird dieser Problembereich nun auf hoher Ebene verstärkt thematisiert.

Die Einführung von Screening-Mechanismen für ausländische Direktinvestitionen hat sich dabei zuletzt als möglicher Ansatz etabliert. Dies kann jedoch bestenfalls eine Teillösung sein. Auch engere Partnerschaften zwischen staatlichen und privaten Akteuren im Bereich der Cybersicherheit können den Abfluss kritischer Kompetenzen verringern. Einem System effektiven Wirtschaftsschutzes, das auch für den langfristigen Erhalt der eigenen Konkurrenzfähigkeit zu forcieren wäre, steht dagegen gerade in Europa die Naivität vieler Politik- und Wirtschaftsakteure im Umgang mit den Schwellenländern entgegen. Die Erkenntnis, dass Fragen der nationalen Sicherheit aus diesen Beziehungen nicht länger ausgeklammert werden können, dürfte sich jedoch in den kommenden Jahren nach und nach durchsetzen.

Auf der militärischen Ebene sind Initiativen zur Erhaltung technologischer Vorsprünge in kritischen Bereichen bis jetzt unzureichend finanziert. Dies gilt selbst für die USA. Zugleich bleibt das zivile Innovationsbiotop des Silicon Valley für das Pentagon, allen anderslautenden Hoffnungen zum Trotz, schwer zugänglich. Das zu ändern, wird gerade jene Aufstockungen der staatlichen F&E-Budgets erfordern, die man durch Kooperation mit den zivilen Unternehmen gerne vermieden hätte.
Herausforderungen für Europa

Für Europa ergibt sich mit wachsender Verantwortung für die eigene Sicherheit auch die Chance, in der Verteidigungs- und Industriestrategie neue Wege zu beschreiten. Zugleich kann sich die Militärplanung auf die historische Tatsache besinnen, dass die umfassende Technologieführerschaft weder eine ausreichende, noch eine zwingend notwendige Voraussetzung für eine eigenständige Verteidigungsfähigkeit darstellt. Fest steht jedoch, dass ohne die militärtechnologische Überlegenheit der USA, die bis jetzt vergleichsweise «kostengünstige» Interventionen ermöglicht hat, auch der Wert von Beistandszusagen im Rahmen von Bündnissen schwinden wird.";https://www.nzz.ch/international/militaer-technologie-westlicher-vorsprung-schwindet-ld.1474351;NZZ;Michael Haas;;;
19.12.2017;Ein Numerus clausus ist Planwirtschaft par excellence;"In Deutschland balgen sich im Fach Medizin 43 000 Bewerber um 9200 Ausbildungsplätze. Also wird mittels Numerus clausus (NC) ausgesiebt. Derzeit geht ein Fünftel der Plätze an diejenigen mit den besten Abiturnoten, ein Fünftel an Personen auf einer Warteliste. Den Rest vergeben die Universitäten selbst, wobei auch da der Abiturschnitt zentral ist. Das Bundesverfassungsgericht hat den NC zwar bestätigt, verlangt aber Korrekturen. So müssen die Universitäten neben den Noten mindestens noch ein Kriterium einbeziehen, und die Eignungsgespräche müssen standardisiert sein. Die Wartezeit auf der Liste von derzeit 15 Semestern gelte es zu begrenzen. Das sind letztlich aber kosmetische Korrekturen. Erstaunlich ist, dass der NC generell eine hohe Akzeptanz geniesst. Dabei setzt der Staat in immer mehr Fächern auf Zugangsbeschränkungen, um die Misere knapper Plätze zu verwalten. Ökonomen befassen sich generell damit, wie sich Knappheitsprobleme entschärfen lassen. Ausgangspunkt ist dabei, dass das Studium eine Investition ist, die weitgehend demjenigen zugutekommt, der es absolviert.

Heute werden alle Plätze massiv subventioniert. Die logische Folge ist, dass die Nachfrage das Angebot weit übersteigt. Doch es gäbe eine in Deutschland tabuisierte Alternative: Studiengebühren verbunden mit einem staatlichen Kreditsystem. Der Einzelne (und seine Eltern) würde mehr als heute überlegen, ob die künftigen (auch nichtmonetären) Erträge die Kosten übersteigen. Fakultäten mit hoher Nachfrage könnten dank den Gebühreneinnahmen zudem ihr Angebot ausdehnen. In einem solchen System würde demjenigen mit erfolgversprechender Perspektive die Möglichkeit zum Studium nicht verwehrt. Ein NC ist dagegen ein massiver Eingriff in die Lebensplanung. Damit masst sich der Staat an, besser zu wissen, ob sich eine Investition für eine bestimmte Person lohnt, als diese selbst. Doch diese Wahl sollte jeder für sich treffen.";https://www.nzz.ch/wirtschaft/ein-numerus-clausus-ist-planwirtschaft-par-excellence-ld.1341275;NZZ;Christoph Eisenring;;;
12.10.2017;Eine Welle aus Beton wird zum Dach der Zukunft;"Wellenförmig geschwungen und fast wie eine Skulptur anmutend, steht ein ausladendes Betondach in der Halle des Robotic Fabrication Laboratory der ETH Hönggerberg. Die Stützen rund um das filigran wirkende Gebilde sind bereits entfernt. In wenigen Tagen wird die selbsttragende Konstruktion zertrümmert, denn es handelt sich um einen Prototypen zur Erprobung einer neuen Bautechnik. Im Frühling 2018 dann soll ein identisches selbsttragendes Leichtbau-Dach auf der südwestlichen Ecke des Nests in Dübendorf errichtet werden. Das Nest ist das Forschungsgebäude der Empa und der Eawag für die Energieoptimierung und Erprobung neuer Bautechnologie.
Positive Energiebilanz

Einige der an dem Projekt Beteiligten erkennen in der von den Bauten des Schweizer Bauingenieurs Heinz Isler (1926–2009) inspirierten, mehrfach geschwungenen Schale einen Elefanten, ein Architekturkritiker umschrieb sie als «Picknickdecke in einem Tornado». Das Dach ist ein Element der ins Nest integrierten sogenannten HiLo-Wohneinheit. HiLo kann für «high efficiency – low energy», «high performance – low impact» oder auch für «hightech – low weight» stehen.
Im «Nest»-Gebäude werden neue Technologien direkt mit den Bewohnern getestet.  Mit der Wohneinheit wollen Forscher und Architekten um Philippe Block vom Institut für Technologie in der Architektur sowie Arno Schlüter, Professor für Architektur und nachhaltige Gebäudetechnologien, fortschrittliche Leichtbauweisen von Böden und Dachstrukturen mit modernster Gebäudetechnik und Energiemanagement kombinieren sowie neue Design-Formen erproben. Die HiLo-Unit auf dem Nest soll als Plus-Energie-Penthouse mit einer adaptiven Solarfassade sowie mit im Dach integrierten Dünnschicht-Solarzellen mehr Energie erzeugen, als die Bewohner verbrauchen.
Stahlseilnetz und Gewebe

Unter der Betonhülle mit den Solarzellen liegen die Isolationsschicht sowie die Heizschlangen für die Wasserheizung, gegen innen schliesst eine Lage Sichtbeton die Schale ab. Dank dieser Sandwich-Konstruktion erfüllt das Dach die heutigen Energiestandards; es ist gut isoliert, und es gibt keine Kältebrücken. Wirklich revolutionär aber ist die Konstruktionsmethode der Schale an der Schnittstelle von Architektur und Bautechnik, die Block und seine Gruppe in mehrjähriger Arbeit entwickelten: Anstelle von gekrümmten Holz- oder Kunststoffverschalungen, die mit einem enormen Materialverbrauch verbunden sind, setzen die ETH-Forscher auf eine textile Schalung. Diese liegt auf einem Stahlseilnetz auf, das zwischen peripheren, auf Stützen installierten Randbalken gespannt ist. Damit können bereits während des Betonierens Arbeiten im Gebäudeinneren vorgenommen werden. Auch beim Bau von Brücken über Strassen oder Eisenbahnlinien bringt diese Vorgehensweise klare Vorteile.  «Man muss klug mit der Geometrie arbeiten, dann gewinnt man Stabilität ohne grosse Materialmengen auf dem Dach», erklärt Philippe Block bei einer Besichtigung des selbsttragenden Wellendachs im Robotic Fabrication Laboratory auf dem Campus Hönggerberg. Er beruft sich auf die Tragwerke früherer Epochen, als man mit Bögen arbeitete, um stabile und gleichzeitig leichte Konstruktionen zu schaffen. Nach denselben historischen Bauprinzipien gingen Block und seine Gruppe auch bei den selbsttragenden, nur 2 Zentimeter dicken und 5 mal 5 Meter grossen Zwischenbodenelementen vor, die ohne Armierungseisen auskommen und gleichfalls in der HiLo-Unit ihren Praxistest erfahren sollen.
Handarbeit bleibt zentral

Das neue Leichtbausystem ist höchst flexibel und fein justierbar, und man kann damit verblüffende Formen gestalten. Die Wissenschafter des von Philippe Block geleiteten Nationalen Forschungsschwerpunkts Digitale Fabrikation entwickelten Algorithmen, mit denen sich das Seilnetz so berechnen lässt, dass es bei der Belastung durch den feuchten, noch nicht verfestigten Beton exakt die gewünschte Schalungsform bildet.  Es gebe theoretisch 90 Trillionen Möglichkeiten zum Justieren der einzelnen Aufhängepunkte des Netzes, rechnet Block vor. Weil sich die Justierung mit eigens entwickelten Computerprogrammen in Kombination mit regelmässigen Messungen exakt bestimmen liess, dauerte es weniger als eine Woche, bis die Fachleute des Industriepartners Marti AG das Stahlseilnetz gespannt hatten. Eine herkömmliche, wesentlich materialintensivere Schalung aus Holz samt Gerüst nähme ein Vielfaches dieser Zeit in Anspruch. Das Seilnetz wiegt nur 500 Kilogramm, und das Kunststoffgewebe bringt weitere 300 Kilogramm auf die Waage.  Die Form ist zwar digital berechnet, aber den Beton verarbeiten nach wie vor Handwerker. Keine Maschine könne so präzise arbeiten wie ein guter Fachmann, betont Philippe Bock. Die Firma Bürgin Creations entwickelte eine Spritzmethode mit vermindertem Druck, damit das Schalungsnetz keinen Schaden nimmt. Und der Zementhersteller Holcim Schweiz lieferte die passende, selbstverdichtende Betonmischung. Damit wurde es möglich, die Schale exakt auf Mass zu formen. An deren fünf Auflagepunkten ist der Beton bis zu 12 Zentimeter dick, während die tragende Schale nur 5 Zentimeter misst, und an den Rändern verringert sich die Dicke sogar bis auf bescheidene 3 Zentimeter.
Effizient und reproduzierbar

Ein halbes Jahr haben die Forschergruppen um Block und Schlüter gemeinsam mit Praktikern von Partnerunternehmen aus der Bauwirtschaft an dem Prototypen der Betonschale gearbeitet. «Wir wollten nicht primär schnell sein, sondern das System gemeinsam entwickeln und erproben und auch auf seine Ästhetik hin testen. Jetzt wissen alle Partner, wie es funktioniert», erklärt Block. Das Stahlseilnetz ist inzwischen in vier gut transportierbare Teile zerlegt, die im kommenden Frühling in Dübendorf innerst kürzester Zeit wieder zusammengefügt werden können.

Anders als eine herkömmliche Schalung mit Holz ist dieses System grundsätzlich beliebig oft verwendbar - und dies auch für nicht identische Dachformen. Damit verringert sich der Aufwand für die Betonierung von einander ähnlichen Bauwerken massgeblich. Für den Aufbau des Daches auf dem Dübendorfer Forschungsgebäude rechnet Block mit lediglich acht bis zehn Wochen.";https://www.nzz.ch/zuerich/eine-welle-aus-beton-wird-zum-dach-der-zukunft-ld.1320316;NZZ;Alois Feusi;;;
10.10.2020;Deep Learning braucht Deep Thinking;"Die Künstliche-Intelligenz-Forschung (KI-Forschung) setzt enorme Hoffnungen in automatisiertes Lernen. Zunehmend bedienen sich Wissenschafter der Methoden des Deep Learning, um aus den Datenfluten Sinnvolles und Erkenntnisbringendes zu fischen. In nahezu allen wichtigen heutigen Disziplinen ist der Big-Data-Ansatz unumgänglich. Ob wir mit Teleskopen den Nachthimmel abtasten, mit Detektoren neue Teilchen im Large Hadron Collider suchen oder eine Krankheit wie Krebs studieren – immer müssen wir uns auf riesige Datenmengen abstützen. Betrachten wir kurz zwei aktuelle Beispiele, eines aus der Biologie, das zweite aus der Physik.

Dank Röntgenstrahl-Kristallografie machten die Biologen eine der wegweisenden Entdeckungen des 20. Jahrhunderts: Die geometrische Struktur von Molekülen – Proteinen, RNA, DNA – bestimmt wesentlich die Lebensaktivitäten. Proteine haben eine oft komplexe, geradezu exotische Gestalt, und diese bedingt die Art der Interaktion. Proteine passen sich einander an, wie dreidimensionale Puzzlestücke. Die Anpassung entpuppt sich als ein dorniges Problem mit enorm vielen Parametern, das heisst: als ein Big-Data-Problem.
Fingerabdruck eines Proteins

Der Biologe und Informatiker Bruno Carreira und sein Team von der ETH Lausanne haben ein neuronales Netz entwickelt – «Geometric Deep Learning» – , das mögliche Wechselwirkungen von Proteinen viel schneller erkennen und klassifizieren kann als herkömmliche Methoden. Es versetzt die Forscherinnen und Forscher in die Lage, die vertrackte innere Struktur eines Proteins zu umgehen und nur dessen Oberfläche zu scannen – den Fingerabdruck des Proteins (von daher der Name molecular surface interaction fingerprinting, Masif). Das KI-System erkennt Merkmale der Oberfläche, und es kann sie zu einem Protein-Fingerabdruck kombinieren.

Dadurch eröffnet sich ein vielversprechender Horizont. So verursacht zum Beispiel Krebs Mutationen in der Proteinoberfläche, was dazu führt, dass das Protein nicht mehr normal funktioniert. Masif könnte solche Störstellen auffinden und günstigstenfalls zu ihrer Reparatur beitragen. Geradezu brennend aktuell wird dieses Problem im Kontext der Corona-Pandemie. So weist ja auch Sars-CoV-2 mit seinen Proteinzäpfchen eine recht bizarre Oberfläche auf. Den Protein-Fingerabdruck des Virus zu kennen, könnte also durchaus helfen, es an gezielter Stelle zu attackieren, indem das neuronale Netz bis anhin unbekannte Einfallstore entdeckt. So liessen sich möglicherweise sogar antivirale Proteine von null auf synthetisieren.

Das klingt alles wunderbar, hat aber einen störrischen Haken. Die Zelldynamik versteht man auf diese Weise nicht. Das heisst, um die kausalen Mechanismen der Proteinbindungen wirklich zu erklären, benötigt man Theorie – «tiefe» Theorie. Die fundamentalste Theorie, über die wir verfügen, ist die Quantentheorie. Mit ihr ist jedoch noch kaum etwas gewonnen. Um das Verhalten eines komplexen biophysikalischen Systems wie eines Proteins zu erklären, muss man die Schrödinger-Gleichung mit sehr viel Variablen lösen – primär ein rechnerisches Problem.

Bereits heute leisten potente Rechner unentbehrliche Dienste bei dieser Aufgabe. Kürzlich kreierte ein Team um den Quantenphysiker Renato Renner an der ETH Zürich einen künstlich intelligenten Agenten namens Scinet, der «von selbst» anhand von Daten zu gewissen theoretischen «Einsichten» gelangt: zum Beispiel zum Sprung von der geozentrischen zur heliozentrischen Beschreibung der Planetenbahnen. Das ist ein durchaus bemerkenswertes Resultat. Der Anspruch, den die SciNet-Forscherinnen und -Forscher daraus ableiten, macht allerdings stutzig: «Unsere Arbeit liefert einen ersten Schritt in der Beantwortung der Frage, ob die traditionellen Methoden der Physiker, Naturmodelle zu bilden, sich von selbst aus den experimentellen Daten ergeben, ohne physikalisches und mathematisches Vorwissen; und ob alternative elegante Formalismen existieren, welche einige fundamentale konzeptuelle Probleme der modernen Physik lösen könnten, wie etwa das Messproblem der Quantentheorie.»
Zuerst kommt das Denken

Spätestens jetzt stellt sich noch eine andere Frage: Sind denn konzeptuelle Probleme nicht Probleme des Konzipierens, also des Denkens, und setzt ein solches Denken nicht ein Vorwissen voraus? Alle fundamentalen Begriffe der Physik sind dem Denken entsprungen, nicht dem Datensammeln: Raum, Zeit, Bewegung, Materie, Kausalität, Energie, Fernwirkung, um nur einige zu nennen. Man könnte sogar sagen: Physik ist Metaphysik mit experimentellen Mitteln. Aber was heisst eigentlich Theorie und Verstehen?

Darauf gibt es eine ganze wissenschaftsphilosophische Palette von Antworten. Ich erdreiste mich hier, sie möglichst einfach zu charakterisieren: Theorie ist Denken im Konjunktiv, sie beginnt stets mit der Wendung «Stellen wir uns vor, dass . . .» oder «Was wäre, wenn . . .». Empirie dagegen ist Denken im Indikativ, sie beginnt mit der Wendung «Schauen wir, was ist». Der Prähominide, der vor 50 000 Jahren nicht einfach fragte: «Wo ist das Mammut?», sondern: «Wo könnte sich das Mammut unter diesen Wetterbedingungen aufhalten?», begann zu theoretisieren. Daten lassen sich überall da sammeln, wo die Fragen «Wer?», «Wo?», «Was?», «Wann?», «Wie?» beantwortbar sind. Nur nicht bei der Frage «Warum?». Modelle ergeben sich nicht «von selbst aus den experimentellen Daten». Die Dynamik der Himmelskörper lesen wir nicht aus noch so riesigen Datenmengen heraus; das Higgs-Teilchen wurde nicht aus Korrelationen im Large Hadron Collider entdeckt; und Krebs verstehen wir nicht aus Bayesschen Netzwerken. Wir brauchen Theorien, sonst droht die Arbeit des Verstehens in der Datenschwemme zu ersticken.

Sagen wir es geradeheraus: Deep Learning ist wesentlich Statistik, nicht Wissen. Die statistischen Methoden sind heute äusserst elaboriert, man sollte von ihnen jedoch nicht erwarten, dass sie es auf die Stufe der Intelligenz schaffen, wie wir sie vom Menschen her kennen. Diese Intelligenz setzt planvolle Intervention und Imagination voraus. Ob wir sie Maschinen vermitteln können, ist eine offene Frage. Ein führender KI-Forscher, Stuart Russell, greift nachgerade zu einer Tautologie: «Bis jetzt begreifen wir sehr schlecht, warum Deep Learning so funktioniert, wie es funktioniert. Möglicherweise lautet die beste Erklärung: weil tiefe Netzwerke tief sind; weil sie zahlreiche Schichten haben und jede dieser Schichten lernen kann, nach einfachen Regeln einen Input in einen Output zu verwandeln; so dass sich diese simplen Transformationen zu jener komplexen Transformation addieren, die vonnöten ist, um von einer Fotografie zu einer Kategorie zu gelangen.» Die Erklärung wirft einen nicht gerade um. Sie erinnert an die von Molière verspottete «einschläfernde Qualität», die erklären soll, warum Schlafmittel wirken.
Ein Symptom von Wahnsinn

Um zu verstehen, muss man über bestehende Daten hinaussehen – «hinaussehen» im Sinne von imaginieren: Deep Thinking. Was damit gemeint ist, möchte ich kurz anhand eines Kontrasts exemplifizieren. Deep Learning funktioniert oft nach dem Prinzip: immer mehr vom Gleichen. In der Tat könnte man ironisch anmerken, dass das «tief» im «tiefen Lernen» nichts weiter bedeutet als mehr von gleichen Verarbeitungsschichten in neuronalen Netzen. Eine «flache» Tiefe also. Und hier gilt die altbekannte Warnung: Immer das Gleiche zu tun und Anderes zu erwarten, gilt als ein Symptom von Wahnsinn.

Ich unterstelle der Deep-Learning-Forschung nicht (direkt) Wahnsinn, aber vielleicht lohnt es sich gelegentlich, an das Prinzip des Deep Thinking zu erinnern: weniger vom Gleichen, und mehr Anderes. Alternativen zu begangenen Wegen finden. Solche Alternativen zu finden, setzt ein Denken im Konjunktiv voraus. In diesem Zusammenhang kommt mir das berühmte Gedicht von Robert Frost in den Sinn: «The Road Not Taken». Vor allem die Zeilen (in der Übersetzung von Paul Celan): «Im Wald, da war ein Weg, der Weg lief auseinander, und ich – ich schlug den einen ein, den weniger begangenen, und dieses war der ganze Unterschied.» Fortschritt in der Forschung beruht darauf, dass man beiden Wegen folgen kann, dem aktuell begangenen und dem potenziell unbegangenen. Letzteren markiert Deep Thinking.";https://www.nzz.ch/wissenschaft/kuenstliche-intelligenz-deep-learning-braucht-deep-thinking-ld.1562845;NZZ;Eduard Kaeser;;;
27.08.2020;Jetzt sollen KI-Maschinen Verbrecher im Voraus an ihrer Visage erkennen können – doch Statistik ersetzt nicht die Urteilskraft;"Es gibt Zombie-Ideen, die gehören längst beerdigt. Zum Beispiel die Idee, aus der äusseren Erscheinung eines Menschen lasse sich zwingend auf sein Inneres schliessen. Die neuere Geschichte der Idee beginnt im 18. Jahrhundert, bei Johann Caspar Lavaters Physiognomik. Sie setzt sich fort in den biometrischen Statistiken von Francis Galton und der Schädelvermessung – der Phrenologie – von Franz Joseph Gall, und sie erreicht eine erste wissenschaftliche Klimax im Werk des italienischen Forensikers Cesare Lombroso, dessen Buch über den verbrecherischen Menschen den Typus des «geborenen» Delinquenten zwischen Primitivem und Geistesgestörtem situiert. Die Idee zieht ihre hässliche rassenbiologische und eugenische Spur durchs 20. Jahrhundert, und sie erreicht das frühe 21. Jahrhundert in Gestalt eines 2016 publizierten Artikels zweier chinesischer Deep-Learning-Forscher – Xiaolin Wu und Xi Zhang –, betitelt mit «Automated Inference on Criminality Using Face Images» (automatisierte Rückschlüsse auf Kriminalität mittels Gesichtsbildern).

Deep-Machine-Learning ist der Hotspot der heutigen Forschung zur künstlichen Intelligenz (KI). Man misst nicht mehr die Schädelgrösse, sondern greift zurück auf Datensätze, zum Beispiel auf Bilder von verurteilten Kriminellen. Wu und Zhang verwendeten 1800 Fotos von chinesischen Männern im Alter zwischen 18 und 55 Jahren; darunter 700 Bilder von verurteilten Delinquenten und 1100 von unbescholtenen Bürgern. Sie brachten den entwickeltsten KI-Systemen – sogenannten neuronalen Faltungsnetzen – bei, dank bestimmten Gesichtsmerkmalen Kriminelle von Nichtkriminellen zu unterscheiden, mit einer Erfolgsrate von angeblich fast neunzig Prozent. Die Autoren tönen denn auch ziemlich unbescheiden: «Mittels ausgedehnter Versuche und Vergleichsprüfungen haben wir gezeigt, dass datenorientierte Klassifikationssysteme via überwachtes Maschinenlernen fähig sind, verlässliche Schlüsse über Kriminalität zu ziehen.»
Höchst fragwürdige Variable

Wu und Zhang gerieten schnell in die Kritik. Tatsächlich weist ihr Verfahren zahlreiche technische Schwächen auf. So stellt sich beim Maschinenlernen generell die Frage, ob das KI-System über den Bereich der Trainingsdaten hinaus verlässliche Ergebnisse liefert. Aber es gibt ein weitaus ernsteres, nichttechnisches Problem. Relevante wissenschaftliche Fragen lassen sich heute nicht in einem sozialen Vakuum stellen. Das gilt besonders für KI-Systeme. Sie dienen dem maschinellen Mustern von Menschen. Man speist diese Systeme ja mit einem Lernstoff aus dem menschlichen Umfeld. Kriminalität ist ein solches Umfeld. Ein Gerichtsurteil erweist sich – bei aller anzustrebenden Fairness – nie als neutral, sondern hängt ab vom Eindruck, den sich Strafverfolger, Richter und Geschworene vom Angeklagten machen. Die «kriminelle Erscheinung» ist also a priori eine höchst fragwürdige Variable; und als erst recht dubios muss der Schluss von der kriminellen Erscheinung auf den kriminellen Charakter gelten.

    Das Wettrüsten in künstlicher Intelligenz ist fatal, weil man hier an einer Technologie des Misstrauens herumbastelt.

Und genau an dieser höchst neuralgischen Stelle sollen uns KI-Systeme helfen? – Wenn man den Algorithmus auf vorsortierte Daten loslässt, ist die Hypothese durchaus plausibel, dass er eigentlich nicht inhärente kriminelle Tendenzen «erkennt», sondern die Urteilsgepflogenheiten einer bestimmten Gerichtspraxis. Statt unvoreingenommen zu urteilen, übernimmt er einfach die eingedrillten Voreingenommenheiten, verstärkt sie gegebenenfalls noch. Indem er unsere Physiognomie in Zahlenwerte transformiert, überzieht er uns mit einem künstlichen Datengesicht und «sieht» uns darunter gar nicht mehr. – Was untergründig mit dem sogenannten «Gyges-Effekt» korrespondiert, dass sich viele Netznutzer «gesichtslos» wähnen.
Nihilistische Anthropologie

Was ist eigentlich der Antrieb zur Erforschung von solchen KI-Systemen? Wu und Zhang antworten: «Anders als ein menschlicher Untersucher oder Richter trägt ein automatisierter Klassifizierer absolut [sic] keinen subjektiven Ballast [. . .] [Er] macht die Kompetenz des menschlichen Richters und Urteilers völlig entbehrlich.» Dieses Argument, menschliches Urteilsvermögen sei subjektiv «kontaminiert», vernimmt man immer wieder aus Kreisen der KI. Kaum verwunderlich, dient es doch als Legitimation des ganzen Projekts, menschliche Fähigkeiten durch künstliche zu ersetzen, zu verbessern, ja gar zu überwinden.

Aber dieses Projekt drückt eine höchst fragwürdige, letztlich nihilistische Anthropologie aus: Der Mensch ist ein einziges Defizit. Selten wird zurückgefragt: Ist denn der subjektive «Ballast» etwas, das eine lernende Technologie zwingend beheben soll? Könnte man den Spiess nicht ebenso gut umdrehen? Denn gerade weil das KI-System uns ja nicht als Menschen «wahrnimmt», ist es weniger flexibel und adaptierfähig; klassifiziert es stur oder schlimmstenfalls auf eine Weise, die dem Menschen undurchsichtig bleibt. Der Hinweis auf die statistische Verlässlichkeit eines künstlichen Klassifizierers zielt am Problem vorbei. Statistik ersetzt nicht die Urteilskraft.

Der explizite Anspruch der Autoren, die künstliche Urteilskraft der KI-Systeme gegen die menschliche auszuspielen, kaschiert die implizite «Zweckdienlichkeit» solcher Systeme im gesellschaftlichen Kontext. Wer KI-Systeme studiert, kann dies nicht mehr mit «reinem» Forschungsinteresse legitimieren. Die diskriminatorische Wirkung ist Technologie-inhärent. Wären also Gesellschaften zum Beispiel nicht rassistisch, würde ein automatisierter Klassifizierer sie wahrscheinlich rassismusaffiner machen; und sind sie rassistisch, verstärkt der Klassifizierer den Rassismus. Bedenkt man überdies, dass autoritäre Regime – allen voran das chinesische – geradewegs nach effizienten Erkennungsinstrumenten gieren, dann entlarvt sich die akademische «Unschuld» der KI-Forschung als Naivität, professionelle Blindheit oder Zynismus – oft als Mixtur aus allem.
Dual-Use-Technologie

Genauso wie sich die Kernspaltung nicht als «rein» physikalisches Problem entpuppte, müssen wir heute die lernenden Algorithmen als «hybrides» wissenschaftlich-sozial-ethisch-politisches Problem erkennen, vor allem, wenn wir uns ihr Potenzial zur Dual-Use-Technologie bewusst machen. Das Tückische an den neuen Technologien ist, dass wir sie produzieren können, ohne sie richtig zu verstehen. Luke Stark, Medienwissenschafter bei Microsoft, sprach unlängst in einem Artikel von der Gesichtserkennung als dem «Plutonium der KI». Die Metapher suggeriert, dass Produktion und Gebrauch von Apps in der Gesichtserkennung genauso strikten gesetzlichen Auflagen genügen müssten wie Produktion und Gebrauch von nuklearem Material. Ob die Regulierungen eingehalten werden, ist eine andere Frage. Jedenfalls hat die KI-Forschung das Stadium der verlorenen Unschuld erreicht, wie vorher schon die Physik mit der Kernspaltung und die Biologie mit der Genmanipulation.

Wettrüsten in künstlicher Intelligenz ist fatal, weil man hier an einer Technologie des Misstrauens herumbastelt. Was eigentlich paradox ist, denn nicht wenige KI-Forscher sind so blind verschossen in die Maschine, dass sie ihr mehr vertrauen als dem Menschen. Und ob sie es wollen oder nicht, spielen sie dadurch einer Politik des Misstrauens in die Hände. Vertrauen ist ein zutiefst menschlicher, subjektiver, ergo nichtquantifizierbarer Parameter. Eine Technologie, welche diese Subjektivität abschaffen will, erzeugt Vertrauensverlust. Sie ist sozial hochgiftig. Deshalb beruht die Zukunft der Zivilgesellschaft darauf, dass wir die KI-Systeme in den gesetzlichen Griff bekommen, bevor sie uns als Gewohnheiten infiltrieren. Sonst macht die automatisierte Gesichtserkennung aus uns allen im Endeffekt Vorverbrecher. Sie präpariert eine schiefe Bahn. Wir befinden uns schon darauf.";https://www.nzz.ch/meinung/koennen-ki-maschinen-verbrecher-im-voraus-erkennen-ld.1560567;NZZ;Eduard Kaeser;;;
12.11.2019;Patienten brauchen Ärzte mit menschlicher Intelligenz;"Die Informationen über künstliche Intelligenz (KI) in der Medizin treffen fast schon im Stundentakt in der Redaktion ein. Ein Schweizer Spital schwärmt von seiner KI in der Augenklinik. Dabei liefert ein «zuverlässiges und schnelles Machine-Learning-Tool» dem Arzt noch während der Konsultation die Ergebnisse einer umfassenden Augenuntersuchung. Eine andere Klinik setzt seit neustem «Watson for Genomics» ein. Das KI-System von IBM soll die Onkologen befähigen, dem Patienten «eine personalisierte und evidenzbasierte Krebsbehandlung» zu bieten. Dazu muss die intelligente Maschine den molekularbiologischen Fingerabdruck vom Tumor des Patienten kennen. Danach sucht sie im Internet nach behandlungsrelevanten Informationen und fasst diese für den Arzt zusammen.

Andere KI erkennen Lungenkrebs oder Hirnblutungen auf CT-Bildern oder Hautkrebs auf Fotos – und das offenbar so treffsicher wie Radiologen. Eine KI sagt dem Arzt, bei welchem Patienten mit Herzschwäche ein erhöhtes Risiko für einen plötzlichen Herztod bestehe. Eine weitere klassifiziert Blasenkrebs anhand von Gewebeschnitten. Mit Smartphone und KI ausgerüstet, werden selbst Laien zu kleinen Dr. Watsons: Von der diabetischen Netzhautveränderung über Herzrhythmusstörungen bis zu Alzheimer lässt sich fast alles diagnostizieren. Und selbst vor dem mutmasslichen Patientenwillen macht KI nicht halt: Hätte der komatöse Patient eine Wiederbelebung gewünscht? Einer Organspende zugestimmt? Die Maschine weiss die Antwort.
Neue Technologien werden überschätzt

Bei all den faszinierenden oder – je nach Standpunkt – auch beängstigenden neuen Möglichkeiten fragt man sich natürlich: Machen sie die Medizin besser? Die ehrliche Antwort ist: Wir wissen es noch nicht. Denn dafür ist es zu früh. Was wir aber wissen: Neue Technologien werden fast immer überschätzt. Die Medizingeschichte ist voll von Beispielen. Eines ist das Brustkrebs-Screening mittels Mammografie. Während Jahren haben die Befürworter den Frauen im Brustton der Überzeugung erklärt, dass es nur Vorteile bringe, wenn der Krebs möglichst frühzeitig erkannt werde. Die Schattenseiten der Mammografie wurden dabei unter den Teppich gekehrt. Dass etwa regelmässiges Röntgen selber Brustkrebs auslöst. Oder dass die Mammografie, wie alle diagnostischen Verfahren, nicht perfekt ist und relativ viele falsch-positive Ergebnisse produziert. Das führt bei den Frauen zu unnötigen Ängsten und ebenso unnötigen Behandlungen.

Dass neue Möglichkeiten immer auch zu neuen, unerwarteten Problemen führen, hat schon die Entdeckung von Penizillin klargemacht. Obwohl damit bakterielle «Killer-Krankheiten» wie die Tuberkulose oder der Wundbrand heilbar wurden, sind die Infektionskrankheiten nicht verschwunden – im Gegenteil: Erst der laxe Umgang mit den «Wundermitteln» hat zum weltweiten Problem mit resistenten Keimen geführt. Und auch die Krebszellen haben angesichts unserer Therapien ihre Fähigkeit verfeinert, den Angreifern Paroli zu bieten. Das zeigt, wie wandelbar und lernfähig biologische Systeme sind. Es zeugt daher schon von seltener Ignoranz und Arroganz, wenn Tech-Visionäre wie Mark Zuckerberg uns erklären, dass wir bald alle Krankheiten heilen, verhindern oder managen könnten.

Skepsis ist daher bei KI die einzig vernünftige Haltung. Denn die marktschreierischen Versprechen der Hersteller klingen viel zu süss in unseren Ohren. Demnach wartet die perfekte, auf den einzelnen Menschen zugeschnittene Medizin gleich um die Ecke. Statt naiv die Werbebotschaften zu schlucken und die neuen Technologien rasch in den klinischen Alltag zu integrieren, sollten wir vermehrt nach Beweisen für den Nutzen jeder einzelnen KI-Anwendung verlangen. Denn auch beim Brustkrebs-Screening hat erst die rigorose wissenschaftliche Aufarbeitung den Wert der Vorsorgemassnahme ins richtige Licht gerückt. Und dieser ist deutlich geringer als anfänglich behauptet.

Nur wenn sich ein KI-System im Praxistest bewährt, soll es Eingang in die medizinische Routine finden. Das schreibt auch das Krankenversicherungsgesetz mit seinen WZW-Kriterien vor: Eine Massnahme muss wirksam, zweckmässig und wirtschaftlich sein, um von der Krankenkasse vergütet zu werden. Dabei sollte stets der Patient im Zentrum stehen: Was nützt ihm das neue Tool? Ist es besser als der bisherige Therapiestandard? Dass bei solchen Abwägungen oft auch andere Interessen im Spiel sind, hat die Einführung des Operationsroboters Da Vinci beispielhaft gezeigt. Das Gerät steht heute in jeder grösseren Klinik. Vielerorts wurde es vor der klinischen Evaluation, die je nach Einsatzgebiet unterschiedliche Ergebnisse zutage förderte, angeschafft. Das Vorpreschen hat auch mit der verschärften Konkurrenz um Patienten und Ärzte zu tun, der viele Kliniken ausgesetzt sind. In einem solchen Klima werden Roboter und KI gebraucht, damit ein Spital attraktiv bleiben kann.
Ärzte könnten eine aktivere Rolle spielen

Solche Fehlentwicklungen könnte nur ein von der Öffentlichkeit getragenes rigoroses Health-Technology-Assessment verhindern. Doch an den damit verbundenen Einschränkungen des Angebots ist niemand so richtig interessiert. Selbst die Bürger, die mit ihren Krankenkassenprämien und Steuern das System am Laufen halten, wollen als Patienten von der neusten Technologie profitieren. Hier könnten die Ärzte als zentrale Figuren im Gesundheitssystem eine aktivere Rolle spielen und vor der Implementierung neuer Technologie auf aussagekräftige Nutzen-Risiko-Analysen dringen. Ihre Berufsethik sollte sie zudem vor heiklen Interessenkonflikten bewahren.

Es spricht vieles dafür, dass der Wert der medizinischen KI heute masslos überschätzt wird. Das beginnt schon damit, dass ein guter Arzt die meisten Krankheiten im Gespräch mit dem Patienten und anhand von ein paar einfachen Zusatzuntersuchungen abschliessend beurteilen und behandeln kann. Für eine gute medizinische Versorgung braucht es also vor allem solides ärztliches Handwerk und menschliche Intelligenz. Kommt dazu, dass die meisten kommerziellen Zusatzangebote und Gesundheits-Apps die Abläufe im Gesundheitswesen verkomplizieren und damit das System verteuern. Oder braucht es unbedingt einen Schrittzähler, um sich selber zu mehr Bewegung zu motivieren? Das Erkennen einer Herzrhythmusstörung mit dem Smartphone ist vielleicht in technischer Hinsicht interessant. Medizinisch dürfte der Nutzen bescheiden sein.

Angesichts des schier unbegrenzten Angebots an KI-Hilfestellungen fragt man sich auch, welches Medizin- und Patientenverständnis hinter den neuen Technologien steckt. Muss man beim Patienten nur die im Körper zu jeder Sekunde anfallenden biochemischen und elektrophysiologischen Daten auslesen, um relevante Störungen festzustellen? Diese Sichtweise verkennt, dass Medizin neben einer naturwissenschaftlich-technischen auch eine menschliche Seit hat. Medizin ist daher keine Naturwissenschaft, sondern in den Worten des Schweizer Psychosomatik-Pioniers Rolf Adler «die Wissenschaft von der Natur des Menschen».

Da diese Natur aus mindestens einer biologischen, einer psychologischen und einer sozialen Komponente besteht, muss der Arzt bei seiner Arbeit mehr als die messbaren Körpersignale berücksichtigen. Er kann das, weil er ein Mensch ist und die verschiedenen «Naturen» von sich selber kennt. So kann er im Gespräch zwischen den Zeilen lesen und nichtverbale Informationen wie Mimik, Gestik und Sprachklang interpretieren. Erst mit solchen «Daten» lässt sich die individuelle Wirklichkeit des Patienten erfassen und seine Situation beurteilen.
Ohne menschliche Zuwendung geht es nicht

Künstliche Intelligenz kann dagegen sehr gut grosse Datenmengen analysieren und dabei Muster erkennen. Selbstlernende Maschinen dürften daher in der Klinik immer mehr Routine-Jobs wie das Auslesen von CT-Bildern übernehmen und die Ärzte bei der Triage von Patienten unterstützen. Dass die meisten Menschen im Krankheitsfall mehr als einen analytischen Medizintechnokraten suchen, zeigt auch der grosse Zulauf bei der Komplementärmedizin. Sie hat zwar keine über den Placeboeffekt hinausreichenden Therapieerfolge vorzuweisen. Dafür nimmt sie sich für ihre Patienten genügend Zeit. Das macht deutlich, dass eine Medizin ohne menschliche Zuwendung zum Scheitern verurteilt ist.

Eine menschliche Medizin zu fordern, heisst aber nicht, die Augen vor dem technischen Fortschritt zu verschliessen. Denn das würde ebenfalls in die Sackgasse führen. Um diesem Schicksal zu entgehen, haben die Vorreiter der Heilkunst im ausgehenden Mittelalter die obskure Viersäftelehre zugunsten der Empirie aufgegeben. Erst diese Fokussierung auf das Objektivierbare hat den Siegeszug der Medizin ermöglicht. Und die Fortschritte gehen weiter. Hat das Stethoskop einst die Beurteilung von Herzerkrankungen revolutioniert, würden wir heute in der gleichen Situation die Echokardiografie bevorzugen. Und morgen? Vielleicht wird es eine KI-Lösung sein. Dagegen ist nichts einzuwenden, solange der Nutzen der Technologie erwiesen ist und wir als Patienten mit dem Arzt zusammen über die Behandlung entscheiden. In einem solchen Szenario wäre KI auch keine gehypte und potenziell schädliche Spielerei, sondern ein seriöses Hilfsmittel für eine schrittweise Verbesserung der Medizin – zum Nutzen des Patienten.";https://www.nzz.ch/meinung/ki-in-der-medizin-es-braucht-vor-allem-menschliche-intelligenz-ld.1521205;NZZ;Alan Niederer;;;
08.10.2019;Computer und künstliche Intelligenz machen Arbeit, viel Arbeit. Sie ist schlecht bezahlt – und das KI-Prekariat bleibt unsichtbar;"Einer der Ersten, der durch künstliche Intelligenz (KI) beruflich deklassiert wurde, war Garri Kasparow. Der Schachweltmeister musste sich 1997 in einem medial gross aufgemachten Schaukampf gegen einen Computer namens Deep Blue geschlagen geben.

Die Niederlage gegen diesen Computer von IBM beschäftigt Kasparow noch heute. «Ich bin ein schlechter Verlierer», schreibt er in dem Buch «Deep Thinking» (2017). Er fühlt sich betrogen. Er glaubte, bei einem wissenschaftlichen Experiment mitzuwirken, doch für IBM war das Ganze ein PR-Gag. Dass IBM nach dem Sieg eine Revanche ablehnte, den Computer rasch verschrottete und sich weigerte, Kasparow oder anderen Interessierten Einblick in die innere Funktionsweise dieser Maschine zu gewähren, weckte Zweifel an der Überlegenheit der künstlichen Intelligenz.
Das KI-Prekariat

Nicht Deep Blue war intelligent, sondern die Menschen, die sich diese Maschine ausgedacht hatten. «Der wahre Wettkampf fand nicht zwischen Kasparow und der Maschine statt, sondern zwischen Kasparow und einem Team aus Ingenieuren und Programmierern», schrieb 1999 der amerikanische Philosoph John Searle. Deep Blue gebe nur vor, eine denkende Maschine zu sein. «In Wirklichkeit aber versteckten sich in ihm menschliche Experten – und zwar in grosser Zahl». Deep Blue wäre somit nichts weiter als ein elektrifizierter Schachtürke.

Der Schachtürke war eine Holzpuppe, die im späten 18. Jahrhundert den Zuschauern vorgaukelte, denken zu können. Sie konnte mit dem Kopf wackeln und die Augen rollen. Die linke Hand bewegte Schachfiguren bei Schauwettkämpfen gegen Menschen. Meistens gewann die Puppe. Dieser Automat beeindruckte mächtige Politiker – unter ihnen Napoleon –, faszinierte bedeutende Wissenschafter – Charles Babbage – und inspirierte Künstler wie Edgar A. Poe. Nachdem ihr letzter Besitzer finanziell Schiffbruch erlitten hatte, wurde die Puppe in Kisten verpackt und eingelagert, bis 1840 ein Arzt sie für 400 Dollar auslöste, um sie zu untersuchen. Das Geheimnis der menschenähnlichen Maschine: ein doppelter Boden, ein Geheimfach, das es einem kleinwüchsigen Menschen ermöglichte, unerkannt Roboter zu spielen.

Der Schachtürke, dieses seltsame Miteinander eines Maschinenmenschen und einer Menschmaschine, ist ein gutes Modell, um sich von künstlicher Intelligenz eine Vorstellung zu machen. Damit soll nicht gesagt werden, die Leistungen der KI-Systeme beruhten auf Betrug. Hingegen schon, dass gerade auch die neuesten und besten dieser Maschinen auf fortwährende menschliche Unterstützung angewiesen sind. Viele der jüngsten Fortschritte der KI unter dem Paradigma des Machine-Learning verdanken sich nicht nur hardware- oder softwaretechnischen Innovationen, sondern auch neuen Möglichkeiten, menschliche Urteilskraft rasch und billig für den Dienst an den Maschinen aufzubieten. Die KI braucht Wissenschafter und Experten, sie braucht aber auch Heerscharen von Hilfsarbeitern, die Lernmaterialien zusammenstellen und die Lernfortschritte der Systeme überwachen.

Diese Hilfsarbeiter erledigen Aufgaben, die wenig Intelligenz erfordern, aber doch nicht automatisiert werden können. Sie verfassen Bildlegenden, übersetzen kurze Texte, evaluieren Übersetzungen, verschriftlichen gesprochene Sprache, tippen handschriftlich ausgefüllte Formulare ab oder diagnostizieren Krankheitssymptome.

Sie arbeiten allein, ohne Arbeitsvertrag und ohne Sozialversicherung. Sie sind nicht Taglöhner, sondern «Minutenlöhner», denn die Aufgaben, die ihnen von internetbasierten Vermittlungsplattformen zugewiesen werden, lassen sich oft sehr schnell erledigen. Ihr Verdienst setzt sich zusammen aus Rappen-Beträgen. Sie sind die Gestrauchelten der Gig-Economy, die Habenichtse der Sharing Economy, die Randständigen des Crowdsourcing. Sie bilden das KI-Prekariat. Man nennt sie Mikro-Jobber, Clickworker oder Crowdworker. Es gibt sie weltweit, in Industriestaaten und auch in Entwicklungsländern. Aber sie bleiben unerkannt. Deshalb wird ihre Arbeit auch als «ghost work» beschrieben, als Geisterarbeit.

Diese Arbeit ist nicht sichtbar, denn Aussenstehenden kommt es so vor, als wäre sie von künstlich intelligenten Maschinen ohne menschliches Zutun erledigt worden. In dieser Illusion werden sie von grossen Software-Firmen bestärkt. Facebook beschäftigt Tausende von Clickworkern, die Hassbotschaften, Fake-News und pornografische Darstellungen aussortieren müssen. Doch die Topmanager von Facebook tun so, als liessen sich die Ausscheidungen menschlicher Dummheit mathematisch-sauber durch KI herausfiltrieren. Google präsentierte im Frühling 2018 eine KI-Software, die selbständig fernmündlich Terminabsprachen treffen kann. Kürzlich wurde bekannt, dass diese Telefongespräche, bei denen der Computer sich anhört wie ein Mensch, tatsächlich oft von Menschen durchgeführt werden. Apple, Google und Amazon verkaufen Gadgets, die gesprochene Sprache verstehen und mit Menschen Gespräche führen können. Damit das funktioniert, beschäftigen sie Tausende von Clickworkern, die aus der Ferne die Dialoge zwischen Mensch und Maschine mitverfolgen.
Weniger als der Mindestlohn

Der britische Mathematiker Charles Babbage ist zweimal gegen den Schachtürken angetreten und hat zweimal verloren. Er habe den starken Verdacht gehabt, gegen einen Menschen gespielt zu haben, wird berichtet. Babbage hat sich in der ersten Hälfte des 19. Jahrhunderts als einer der Ersten an die Konstruktion eines universell programmierbaren Computers herangewagt. Er war ein Universalgelehrter mit vielen Interessen, unter anderem beschäftigte er sich auch mit wirtschaftlichen Fragen, insbesondere mit der Optimierung von industriellen Arbeitsprozessen. Er hat zur Ökonomie der arbeitsteiligen Industrieproduktion ein zu seiner Zeit wichtiges Buch publiziert. Der erste Mensch, der sich mit der Mechanisierung von Denkarbeit beschäftigte, studierte auch die Möglichkeiten der Rationalisierung von industriellen Arbeitsprozessen. Bevor die Computer die Fabrikarbeit veränderten, hat die Fabrikarbeit die Computer geprägt. Was einst auf dem Arbeitstisch von Babbage nebeneinander lag, ist jüngst wieder zusammengekommen. Frühindustrielle Formen der Arbeitsorganisation werden in der digitalen Ökonomie neu entdeckt.

2001 haben Mitarbeiter von Amazon in den USA ein «Hybrid machine/human computing arrangement» zum Patent angemeldet. Dieses «Arrangement» ist seit 2005 unter dem Namen Mechanical Turk oder MTurk bekannt geworden. Der Name verweist auf den Schachtürken und bezeichnet eine Internetplattform, die Mikro-Jobs an Mikro-Jobber verteilt. 2011 behauptete Amazon, der Mechanical Turk könne auf mehr als 500 000 «turk workers» in 190 Ländern zurückgreifen. Laut einer 2018 publizierten Studie von Forschern der New York University gibt es rund 100 000 bis 200 000 aktive «turkers», die – so die Resultate einer anderen Studie – pro Stunde durchschnittlich zwei Dollar verdienen.

Neben Amazons MTurk gibt es heute zahlreiche weitere solche Plattformen, beispielsweise Clickworker, Cloudfactory, Crowdflower, Crowdsource oder Mobileworks. Einige dieser Plattformen haben sich auf die Bedürfnisse von bestimmten Branchen spezialisiert. Viel Arbeit gibt es von der Autoindustrie, die im Hinblick auf das autonome Fahren auf die Arbeit von sehr vielen Clickworkern angewiesen ist, die auf Videoaufnahmen einzelne Elemente von Strassenszenen benennen.
Ein Roboter als Chef

Seit Jahrzehnten werden die Auswirkungen der Informatik auf den Arbeitsmarkt diskutiert. Dass die Digitalisierung – oder, wie man früher sagte: die Automatisierung – Arbeitsplätze vernichten würde, war dabei unbestritten. Man stellte sich vor, dass es eine Verlagerung geben würde: Gewisse Jobs würden vernichtet, andere neu geschaffen; gewisse Berufe würden verschwinden, andere entstünden neu. Die Frage war, wie viel zusätzliche Arbeit die Computer bringen würden. Jetzt sieht man: Es ist sehr viel Arbeit, aber es ist miserabel bezahlte Hilfsarbeit. Die Frage war, welche Berufe verschwinden würden, welche sich behaupten könnten. Jetzt sieht man: Arbeitnehmer werden nicht durch Roboter ersetzt, sondern durch Heerscharen von Hilfsarbeitern, die unter der Aufsicht von Robotern kleinste Teile jener Arbeit erledigen, die einst einem berufstätigen Menschen ermöglichte, Berufsstolz zu empfinden. In der neuen Arbeitswelt ist der Roboter nicht Kollege, sondern Chef.

Crowdworker, so fordert etwa die Schweizer Gewerkschaft Syndicom, müssten «fair entschädigt und sozialversichert» werden; es brauche «Zertifizierungssysteme für Crowdwork-Plattformen». Doch in der neuen Arbeitswelt ist der Arbeitskampf schwierig. «Ghost work» ist schwer fassbar, das KI-Prekariat bleibt unsichtbar, die Lieferketten, die Clickworker und die für die Arbeitsvorbereitung zuständigen Roboter aneinanderbinden, umspannen die ganze Welt.
Licht ins Dunkel

Doch es stellen sich nicht nur schwierige sozialpolitische Fragen. Es gibt auch technische Unwägbarkeiten. Wie soll die KI das Leben der Menschen verbessern, wenn sie auf Beiträge angewiesen ist von Menschen, die unter menschenunwürdigen Bedingungen arbeiten? Wer wird sich als Spitalpatient der Diagnose einer Software ausliefern, die auf dem Urteil von Tausenden miserabel bezahlter, namenloser Menschen beruht, die in Fronarbeit medizinische Symptome in maschinenlesbarer Form etikettiert haben? Wer wird sich wohl fühlen in einem autonom fahrenden Auto, das nur deshalb eine gestrichelte von einer durchgehenden Mittellinie zu unterscheiden weiss, weil Tausende von Clickworkern unter Zeitdruck Hunderttausende von Bildelementen vermutlich richtig beschrieben haben?

Es gibt viele Hinweise darauf, dass clevere Clickworker Methoden gesucht und gefunden haben, mit denen sie die Roboter, die ihre Arbeit koordinieren, täuschen können. Sie haben die Mikro-Jobs, die ihnen zugewiesen wurden, aufgeteilt und an weitere, noch schlechter bezahlte Mikro-Jobber weitergegeben; sie nutzen im Internet verfügbare KI-Software – etwa für die automatische Sprachübersetzung –, um beispielsweise den Output von Übersetzungssoftware ohne eigene Denkarbeit zu evaluieren; sie haben Scripts oder Bots entwickelt, um Clickwork ohne menschliches Zutun zu erledigen.

Auch ohne Geisterarbeiter sind die Leistungen von KI-Systemen schwer zu bewerten. Durch Prozesse des Machine-Learning entstanden, geprägt durch ein komplexes Zusammenspiel von Algorithmen und Metaalgorithmen, von Trainingsdaten und statistischen Auswertungen ist die Funktionsweise dieser Systeme für Menschen kaum noch nachvollziehbar. Wenn auch noch Geisterarbeiter in diesen Maschinen herumspuken, ist diese KI nicht mehr beherrschbar. Es gilt, in der KI die geheimen Schubladen zu öffnen, die doppelten Böden auszuleuchten.";https://www.nzz.ch/meinung/kuenstliche-intelligenz-geister-in-der-maschine-ld.1513894;NZZ;Stefan Betschon;;;
19.12.2019;Mozilla Deep Speech: Schreiben, wie einem der Schnabel gewachsen ist;"Die gemeinnützige Mozilla Foundation, die vor allem als Hersteller des beliebten Firefox-Webbrowsers bekannt ist, hat eine neue Version ihres Spracherkennungssystems Deep Speech vorgestellt.

Das Programm basiert auf Open-Source-Code und soll eine nichtkommerzielle Alternative zu den proprietären Systemen der grossen Tech-Giganten bieten, die den Markt mit Sprachassistenten wie Siri (Apple), Google Assistant oder Alexa (Amazon) dominieren.
Open-Source-Technologie für Sprachassistenten

Die Stiftung aus Mountain View im Silicon Valley treibt das Projekt aus der Überzeugung voran, dass eine so entscheidende Technologie wie Spracherkennung nicht nur grossen Unternehmen zur Verfügung stehen sollte, so dass auch Startups, kleine Firmen oder Forschungseinrichtungen damit Neues kreieren können.

Deep Speech kann als sogenannte Sprache-zu-Text-Engine Gesprochenes in Echtzeit in Text konvertieren und ermöglicht beispielsweise die Transkription von Vorträgen, Telefonaten, Fernsehsendungen oder anderen Audio-Live-Streams, während diese stattfinden.

Mozilla stellt nicht nur das Programm öffentlich zur Verfügung, sondern auch ein auf Englisch trainiertes Modell. Eingesetzt wird die englische Spracherkennung bereits für verschiedene digitale Open-Source-Assistenten wie etwa Mycroft oder Leon. Auch etwa ein Telefonvermittlungssystem, das Telefonnachrichten für Unternehmen transkribiert, nutzt die Mozilla-Spracherkennung.
Deep Speech wird schneller und kleiner

Mit der jüngsten Version (v0.6) seines Voice-Recognition-Programms hat Mozilla es geschafft, das System deutlich effizienter und schlanker zu machen. Für mehr Leistungsfähigkeit sorgt ein neuer Streaming-Decoder, der für konsistent niedrige Latenzzeiten und geringe Speicherausnutzung sorgt. Dadurch kann die Spracherkennung um 73 Prozent schneller reagieren und die Umwandlung von Sprache zu Text innerhalb von nur 260 Millisekunden liefern.

Zudem bietet Mozilla mit dem jüngsten Release nun auch eine platzsparende Variante des Deep-Speech-Modells. Das kleinere KI-Programm nutzt eine Version des zugrunde liegenden Machine-Learning-Systems, die speziell auf mobile Geräte und integrierten Einsatz ausgerichtet ist (Tensorflow Lite). Dadurch benötigt Deep Speech nur noch 3,7 statt 98 MByte Speicherplatz. Die mobile Lite-Spracherkennung wurde bereits auf Englisch trainiert und ist für Windows, Mac-OS, Linux und auch Android verfügbar.

Damit kommt Mozilla dem Ziel näher, dass Deep Speech künftig auch auf Smartphones oder In-Car-Systemen ausgeführt werden kann und mehr oder weniger «allgegenwärtig» wird. Es bleibt jedoch abzuwarten, wie sehr die Erkennungsrate und eventuell auch die Robustheit durch die Verkleinerung des Deep-Speech-Modells leiden, wie Torsten Zesch vom Language Technology Lab der Universität Duisburg-Essen gegenüber der NZZ erklärt.

Der Computerlinguist hat zusammen mit seinem Kollegen Aashish Agarwal das erste deutschsprachige Deep-Speech-Modell trainiert und im Oktober öffentlich zugänglich gemacht. «Grundsätzlich kann Deep Speech als Open-Source-System ähnliche Ergebnisse wie die Spracherkennung kommerzieller Anbieter liefern», sagt Zesch.
Sprachsteuerung offline

Die Praxis der grossen Tech-Unternehmen, einen Teil der aufgezeichneten Sprachbefehle zur Verbesserung ihrer Systeme auch von menschlichen Mitarbeitern auswerten zu lassen, hat in diesem Jahr bereits für grossen Unmut gesorgt. Deep Speech ist im Gegensatz zu den meisten anderen Spracherkennungssystemen von Anfang an darauf ausgerichtet, auf Geräten offline zu funktionieren. So ist bei der Nutzung keine Internetverbindung notwendig, die Sprachschnipsel werden nicht an Server von Unternehmen gesendet, um in Text umgewandelt zu werden.

Indes ist laut Zesch nach wie vor das grösste Problem, dass frei verfügbares Trainingsmaterial für die automatische Erkennung gerade in nichtenglischen Sprachen nur begrenzt vorhanden ist. Hätte man etwa mehr frei verfügbare deutschsprachige Daten mit allen Dialekten und guten Sprechern, wäre die Fehlerrate bei der deutschen Spracherkennung noch deutlich niedriger, so der Professor für Informatik und angewandte Kognitionswissenschaft.

Hier setzt die Mozilla-Stiftung mit einem flankierenden Projekt namens «Common Voice» an, das entsprechende Sprachdaten bereitstellt und das Ziel verfolgt, durch Crowdfunding riesige Stimmdatensätze in verschiedenen Sprachen aufzubauen, um damit das künstlich intelligente Deep-Speech-System zu füttern.
Jeder kann bei Common Voice beitragen

Dank reger Unterstützung der Mozilla-Community wird ähnlich wie beim freien Online-Lexikon Wikipedia die Datenbank ständig weiter ausgebaut. Entsprechend kann auch jeder Internetnutzer, der zur Verbesserung der quelloffenen Spracherkennung beitragen möchte, einfach über die Common-Voice-Website oder per App mit seiner Stimme Sätze beisteuern oder bereits vorhandene Samples auf Richtigkeit überprüfen.

Anfang dieses Jahres hat Mozilla die seit dem Common-Voice-Start im Juli 2017 gesammelten Sprachdaten in einer Datenbank vollständig öffentlich gemacht: Nach Angaben der Stiftung sind dabei rund 1400 Stunden Sprachaufzeichnungen von mehr als 42 000 Mitwirkenden in zunächst 18 verschiedenen Sprachen eingeflossen – neben Englisch etwa Französisch, Deutsch und Mandarin. Immer mehr Sprachen folgen seither, insbesondere auch solche mit vergleichsweise kleinem Verbreitungsgebiet wie etwa Norwegisch, Friesisch oder Baskisch, die für kommerzielle Anbieter von Sprachdiensten eher uninteressant sind.

An den Umfang der Datensätze, auf die kommerzielle Anbieter zugreifen können, kommt man jedoch noch bei weitem nicht heran. Die beiden Forscher der Universität Duisburg-Essen haben das deutsche Deep-Speech-Modell mit Datensätzen von Common Voice und zwei weiteren Open-Source-Datenbanken lernen lassen.
Kooperation mit Entwicklungsministerium

Mozilla will die grösstmögliche Vielfalt an Stimmen sammeln und damit erreichen, dass in Zukunft jeder dank Spracherkennung mit Computern sprechen kann und auch verstanden wird. Auch afrikanische Sprachen hat die Stiftung im Visier. Vergangenen Monat haben Mozilla und das deutsche Bundesministerium für wirtschaftliche Zusammenarbeit und Entwicklung (BMZ) angekündigt, Sprachproben in afrikanischen Sprachen zu sammeln, um die Entwicklung lokaler sprachbasierter Technologien und Produkte voranzutreiben.

Die Initiative folgt auf ein Mozilla-Pilotprojekt, das Anfang des Jahres in Kooperation mit einem Startup aus Rwanda und der Deutschen Gesellschaft für Internationale Zusammenarbeit (GIZ) zur Errichtung einer Datenbank mit Sprach-Samples auf Kinyarwanda gestartet wurde, einer in Afrika von mehr als 12 Millionen Menschen gesprochenen Sprache. Weitere afrikanische und auch asiatische Sprachen sollen folgen.";https://www.nzz.ch/international/frei-zugaengliche-spracherkennung-fuer-jeden-ld.1529745;NZZ;Jochen Siegle;;;
18.03.2020;Mit Smartphone-Daten und künstlicher Intelligenz: Wie das Silicon Valley gegen das Coronavirus kämpft;"Es klang wie die langersehnte frohe Botschaft: Die amerikanische Regierung stehe kurz davor, die «Testkapazitäten für das Coronavirus massiv auszubauen», teilte der amerikanische Präsident bei einer Pressekonferenz vergangenen Freitag mit – dank einer Partnerschaft mit Google. Google mache «rasante Fortschritte» mit einer Website, die helfe, festzustellen, ob ein Patient sich testen lassen sollte und wo er «bequem» ein Testzentrum finde. In den nächsten Tagen werde das Portal online gehen, versprach Donald Trump.

Angesichts des eklatanten Mangels an Tests in den USA waren das grandiose Nachrichten; bisher wurden erst wenige zehntausend der 330 Millionen Amerikaner auf das Coronavirus getestet. Rund 6000 Erkrankungsfälle waren am Dienstag bestätigt, die Dunkelziffer dürfte markant höher liegen. Das Versprechen des Präsidenten klang plausibel – wieso sollte der Internetgigant nicht auch eine Lösung für dieses Problem parat haben?

Die Realität war jedoch ganz anders. Tatsächlich arbeite die Firma Verily, eine Tochter des Google-Mutterkonzerns Alphabet, an einer ersten Testversion eines Online-Portals, stellte der CEO Sundar Pichai am Wochenende klar. Die Plattform steht jedoch nur Anwohnern der Bay Area in Kalifornien zur Verfügung. Trotzdem war der Ansturm so gross, dass sie am Montag nach nicht einmal 24 Stunden vorübergehend offline gehen musste. Darüber hinaus tüftelt Google an einem landesweitem Portal mit Informationen zu der Pandemie, das allerdings erst in einigen Tagen online gehen dürfte. Auf zusätzliche Tests wartet Amerika nach wie vor vergeblich.
Task-Force mit der Regierung

Trump hat jedoch insofern recht, als dass man im Silicon Valley derzeit verzweifelt nach Lösungen sucht, um die Coronavirus-Krise einzudämmen. «Führende Technologiefirmen und grosse Online-Plattformen werden eine entscheidende Rolle spielen in dem Versuch, alle Kräfte aufzubieten», sagt der Technologiebeauftragte der Regierung, Michael Kratsios. Seit zwei Wochen arbeiten Vertreter der Regierung und der Tech-Firmen deswegen in einer «Technology and Research Task-Force» zusammen. Jüngst diskutierte diese am Sonntag, wie man mehr Infizierungen per Telemedizin diagnostizieren könnte, um Spitäler zu entlasten, und wie die Tech-Konzerne die Gesundheitsbehörde CDC (Centers for Disease Control and Prevention) unterstützen könnten.

Wie die «Washington Post» meldet, interessiert sich die Regierung auch für anonymisierte Bewegungsdaten von Bürgern, die Facebook, Google und andere Technologiefirmen standardmässig über die Standortdaten des Smartphones erfassen. Facebook hat diese Daten in der Vergangenheit schon in statistischer Form Wissenschaftern zur Verfügung gestellt. Wie Mitarbeiter von Facebook gegenüber der Zeitung sagten, stelle Facebook Dritten aber nur Daten zur Verfügung, die den Aufenthaltsort einer Person bis auf 500 Meter genau zeigten und keine Informationen zu Einzelpersonen enthielten.

Diese Smartphone-Daten könnten der Regierung helfen zu verstehen, wo das nächste Epizentrum eines Coronavirus-Ausbruchs liegen könnte. Das Vorhaben sei aber erst in einem frühen Stadium, sagte die Projektleiterin gegenüber der «Washington Post»: Es sei schwierig, in Echtzeit eingehende Standortdaten mit analogen Informationen von Städten und Spitälern abzugleichen, um zu sehen, wie sich das Virus ausbreite. Die Daten zeigten aber auf, ob sich die Bevölkerung tatsächlich an das «social distancing» halte.

Darüber hinaus hat das Weisse Haus am Montag Experten für künstliche Intelligenz um Hilfe dabei gebeten, aus einer neu eingerichteten Datenbank mit 29 000 wissenschaftlichen Veröffentlichungen mögliche Antworten zum Coronavirus herauszufiltern. Es sei für Menschen schwierig, händisch Zehntausende von Artikeln zu durchforsten und die Ergebnisse zusammenzutragen, sagte Anthony Goldbloom, der Mitgründer der Machine-Learning-Plattform Kaggle, die an dem Projekt mitwirkt. Algorithmen sollten dabei helfen.
Geld für neuartige Startups

Losgelöst von den Kooperationen mit Washington suchen Amerikas Technologiefirmen derzeit auch eigenständig nach Antworten, die im Kampf gegen die Pandemie helfen könnten – etwa auf die Fragen wie schnell sich das Virus verbreiten dürfte oder wie viele Spitalbetten zu jedem Zeitpunkt verfügbar sein dürften. Ein Hochleistungsrechner von IBM unterstützt Wissenschafter zudem dabei, mögliche Komponenten eines Medikaments gegen das Coronavirus zu erforschen.

Der bekannte Silicon-Valley-Investor Sam Altman rief am Wochenende dazu auf, ihm Firmen und Projekte vorzuschlagen, die an Tests, Medikamenten und Impfstoffen gegen das Virus arbeiteten oder die möglichst schnell Beatmungsgeräte und Schutzmasken herstellen könnten – er versprach, Investorengelder für sie zu finden. Innert kürzester Zeit meldeten Nutzer Dutzende von Vorschlägen. Andere Geldgeber im Silicon Valley haben ähnliche Aufrufe gestartet. Vergangene Woche hatten sich Epidemiologen und andere Mediziner in einem offenen Brief an Technologiefirmen gewandt mit der Aufforderung, aktiver im Kampf gegen die Pandemie zu werden.

Auch Mark Zuckerberg, einer der wohlhabendsten und führenden Köpfe des Silicon Valley, unterstützt mit seiner privaten Stiftung Chan Zuckerberg Initiative den Kampf gegen das Coronavirus, etwa im Bezug auf die Sequenzierung des Virus. «Wir arbeiten daran, die Testmöglichkeiten in der Bay Area zu vervierfachen», versprach Zuckerberg zudem in einem Blog-Post. Die Westküste generell und speziell das Silicon Valley als Heimat der Tech-Riesen zählen zu den Epizentren der Ausbreitung in Amerika; seit Dienstagmorgen herrscht eine Ausgangssperre in Nordkalifornien, ebenso wie bereits in New York.
Eindämmung von Fake-News

Bisher allerdings waren einige Technologiefirmen weniger Teil der Lösung, sondern vielmehr des Problems: In den vergangenen Wochen haben sich in den sozialen Netzwerken Falschinformationen über das Coronavirus wie Lauffeuer verbreitet, sowohl mit Blick auf seinen Ursprung und auf vermeintliche Heilmittel als auch im Bezug auf angebliche Grenzschliessungen zwischen den Gliedstaaten.

Das amerikanische Aussenministerium zählte mit Stand Februar mehr als zwei Millionen Tweets mit Verschwörungstheorien zum Virus, die zum Teil im Ausland verfasst worden waren. Der Generaldirektor der Weltgesundheitsorganisation (WHO), Tedros Adhanom Ghebreyesus, gab bereits Mitte Februar, als das Virus noch nicht als Pandemie klassifiziert war, an der Münchner Sicherheitskonferenz zu bedenken: «Wir bekämpfen nicht nur eine Epidemie, wir bekämpfen eine ‹Infodemie›. Falschnachrichten verbreiten sich schneller und einfacher als das Virus und sind genauso gefährlich.»

Facebook und Google, die Marktführer für Online-Werbung, boten der WHO daraufhin Gratisanzeigen im Wert von mehreren Millionen Dollar auf ihren Plattformen an. Am Montag wandten sich zudem die grossen sozialen Netzwerke mit einem Versprechen an die Öffentlichkeit: Sie würden alles in ihrer Macht Stehende unternehmen, um Inhalte aus zuverlässigen Quellen auf ihren Plattformen hervorzuheben und wichtige Updates so schnell wie möglich zu verbreiten.

Facebook spendete zudem jeweils eine Million Dollar an lokale Publikationen in den USA, um deren Berichterstattung zum Coronavirus zu unterstützen, sowie an Fact-Checker, die Meldungen zur Pandemie überprüfen. Darüber hinaus stellen mehrere Technologieunternehmen einige Dienste nun gratis zur Verfügung: Schulen und Firmen können die Werkzeuge von unter anderem Google, Microsoft und Cisco für Videokonferenzen und virtuelles Lernen derzeit kostenlos nutzen.";https://www.nzz.ch/international/usa-tech-konzerne-helfen-im-kampf-gegen-coronavirus-ld.1547096;NZZ;Marie-Astrid Langer;;;
29.02.2020;EEG-Messungen zeigen, welche Patienten gut auf Antidepressiva reagieren;"Längst nicht alle Personen, die unter einer schweren Depression leiden, profitieren von einer Behandlung mit Antidepressiva. Welchen Betroffenen diese zugutekommt und welchen nicht, lässt sich bis jetzt schwer vorhersehen. Aus diesem Grund haben Ärzte meist keine andere Wahl, als nacheinander verschiedene Medikamente auszuprobieren.

Bevor sie sehen, ob die Behandlung wirkt, vergehen jedoch für gewöhnlich mehrere Wochen – für die Patienten eine quälend lange Zeit. Forscher suchen daher nach Mitteln und Wegen, die es bereits vor Ausstellung des ersten Rezepts ermöglichen, die Erfolgsaussichten einer Therapie mit einem der gängigen Antidepressiva abzuschätzen. Ein aussichtsreiches Verfahren, um an die benötigte Information zu gelangen, könnte die Messung der elektrischen Hirnaktivität sein. Im wachen Ruhezustand abgeleitet, liefert das Enzephalogramm (EEG) offenbar wertvolle Hinweise darauf, ob die betreffende Person auf Antidepressiva anspricht oder nicht. Was schon früher beobachtet wurde, legen nun auch die Ergebnisse einer Untersuchung von Forschern der Stanford University nahe.1
Wirkt es oder nicht?

Gestützt auf die Methoden des maschinellen Lernens, haben die Projektleiter in den EEG-Daten von 309 stark depressiven Patienten, die im Rahmen einer Studie teilweise mit dem Antidepressivum Sertralin und teilweise mit einem Placebo behandelt worden waren, nach diagnostisch wegweisenden Signalen gefahndet. Ihr besonderes Interesse galt dabei der Frage, ob sich das EEG von Personen, bei denen das Antidepressivum versagt hatte, vor Beginn der Therapie von jenem der erfolgreich behandelten Patienten unterscheidet.

Wie Amit Etkin, Wei Wu und die anderen Studienautoren berichten, identifizierte die lernfähige Maschine nach einigem Training charakteristische EEG-Unterschiede zwischen den beiden Patientengruppen. Anhand dieser Signale liess sich der Erfolg einer Sertralin-Behandlung gut voraussagen, und das weitaus besser als mit den üblicherweise erfassten Parametern, etwa der Schwere der Erkrankung, dem Alter und dem Geschlecht sowie der persönlichen Geschichte, beispielsweise traumatischen Kindheitserlebnissen.

Um sicherzustellen, dass der entdeckte Biomarker Allgemeingültigkeit besitzt und kein Kuriosum der untersuchten Studienpopulation darstellt, testeten die Wissenschafter seinen Vorhersagewert auch bei den Teilnehmern von drei weiteren Depressionsstudien. Auch hier gelang es damit, den Nutzen einer Sertralin-Therapie mit recht grosser Genauigkeit vorauszusagen.
Von Anwendung weit entfernt

Wie Ulrich Hegerl von der Psychiatrischen Universitätsklinik der Goethe-Universität Frankfurt auf Anfrage klarstellt, hätten Forscher auch schon früher versucht, aus dem EEG Rückschlüsse auf den Erfolg einer Behandlung mit Antidepressiva zu ziehen. Aufgrund von methodischen Problemen sei daraus allerdings nichts geworden. «Was den neuen Ansatz auszeichnet, sind die Grösse der untersuchten Population, die Validierung an anderen Datensätzen und der Machine-Learning-Ansatz. Von einer klinischen Anwendung ist er allerdings noch weit entfernt», sagt der Psychiater, der selber viele Jahre in diesem Bereich geforscht hat.

Wie er ferner einräumt, sind EEG-Messungen extrem anspruchsvoll. Dafür benötige man gut ausgebildetes Personal – das heute rar gesät sei. Denn mit dem Aufkommen der neuen bildgebenden Verfahren, etwa dem Kernspin, hätten viele Experten umgesattelt. Dabei enthalte das EEG eine Fülle von wertvollen Informationen. Diesen Schatz könne man allerdings nur heben, wenn man berücksichtige, dass das EEG-Signal in hohem Masse vom Wachzustand des Patienten abhänge. So variiere es etwa in Abhängigkeit davon, wie gut er geschlafen habe, ob er aufgeregt sei, wie viel Kaffee er getrunken oder ob er Schlafmittel eingenommen habe. «Werden solche Einflussfaktoren nicht bedacht, was häufig nicht geschieht, kommt sehr viel Rauschen in die Daten», betont Hegerl.";https://www.nzz.ch/wissenschaft/eeg-messungen-zeigen-bei-wem-antidepressiva-wirken-ld.1543029;NZZ;Nicola von Lutterrotti;;;
21.04.2016;Die Erfindung des Informationszeitalters;"Kurz vor dem Ende, wenige Wochen vor seinem Tod, machte sich Claude Shannon, krank und dement, eine letzte Notiz: «Das Jahr 2001 könnte der Anfang sein vom Ende», die menschliche Rasse – «dumm, Entropie-erhöhend und militant» – könnte ersetzt werden durch eine Spezies, die die Logik besser beherrscht, die Energie zu sparen versteht und freundlich ist: Computer. Am 24. Februar 2001, im Alter von 85 Jahren, verstarb Shannon.
Alles ist Information

Shannon wurde vor 100 Jahren, am 30. April 1916 geboren in Petoskey, einem kleinen Städtchen am östlichen Ufer des Michigansees. An der University of Michigan absolvierte er gleichzeitig das Grundstudium für Mathematik und für Elektroingenieure. Danach erhielt er am Massachusetts Institute of Technology (MIT) bei dem berühmten Vannevar Bush eine Anstellung als Assistent, der sich um den Betrieb eines analogen Computers zu kümmern hatte. Bush hat den jungen, schüchternen Mann sehr geschätzt, er beschrieb ihn als «liebenswürdige» Person und als Mathematik-Genie.

Unter der Aufsicht von Bush schrieb Shannon eine Master-Arbeit, die Boolesche Algebra und Schaltungsentwurf zusammenbrachte. In seiner Doktorarbeit stellte Shannon seine mathematischen Fähigkeiten in den Dienst der Genetik. Seine erste Arbeitsstelle fand er 1940 bei einer Telefongesellschaft, in den berühmten Bell Labs in New York. Während des Krieges beschäftigte er sich mit Problemen der Flugzeugabwehr und mit Kryptologie.

Shannon verdankt seinen Ruhm hauptsächlich einem rund 50-seitigen Aufsatz, der im Herbst 1948 publiziert worden ist: «A Mathematical Theory of Communication». Dieser Text sei, so schrieb 1990 die Wissenschaftszeitschrift «Scientific American», «die Magna Charta des Informationszeitalters».
Schutz vor Störungen

Das Verständnis der Informationstheorie wird nicht nur durch komplizierte mathematische Formeln erschwert, sondern auch dadurch, dass man zu wissen glaubt, was Information ist. Doch für Shannon ist Information nicht etwas, das eine Bedeutung vermitteln kann; bei der Bestimmung des Informationsgehalts spielt der Inhalt einer Nachricht keine Rolle. Die semantischen Aspekte der Kommunikation, so Shannon explizit, seien für den Ingenieur irrelevant.

Shannon beschreibt den Informationsgehalt mit dem Begriff Entropie in Abhängigkeit von den Auswahlmöglichkeiten, die der Sender hat. Je grösser die Auswahlmöglichkeiten des Senders, umso grösser ist die Unsicherheit aufseiten des Empfängers, umso grösser der Informationsgehalt. Die Formeln von Shannon bringen Entropie als Mass für den Informationsgehalt in Beziehung mit der Übertragungskapazität des Kanals. Sie erlauben es, zu berechnen, welche Informationsmenge über einen bestimmten Kanal maximal übertragen werden kann, und zeigen, dass es möglich ist, durch das Hinzufügen von Redundanz, die Nachrichtenübertragung vor Störungen zu schützen.
Trittbrettfahrer

Aus Shannons «A Mathematical Theory of Communication» wurde bald – in einem von Warren Weaver herausgegebenen Buch – «THE Mathematical Theory of Communication». Schliesslich war nur noch von «Informationstheorie» die Rede. Bereits Anfang der 1950er Jahre bildeten sich Institutionen – das London Symposium on Information Theory oder die amerikanische Professional Group on Informationen Theory –, die den Informationsaustausch rund um die Informationstheorie organisierten. Das amerikanische Institute of Radio Engineers begann 1953 mit der Herausgabe der «Transactions on Information Theory».

Diese Zeitschrift wurde bald nicht nur von Mathematikern und Elektroingenieuren geschätzt, sondern auch von Biologen, Psychologen, Ärzten, Sprachwissenschaftern. Informationstheorie wurde in den 1950er Jahren begeistert aufgenommen als eine neue Über-Disziplin, die Technik, Linguistik und Biowissenschaften vereint. Bald würde man, so die Hoffnung, mit den Formeln von Shannon nicht nur Telefonnetze beschreiben können, sondern auch die Kommunikation zwischen Singvögeln, Eskimos oder Nervenzellen im menschlichen Gehirn. 1953 erklärte das amerikanische Wirtschaftsmagazin «Fortune» seinen Lesern, es sei keine Übertreibung zu sagen, dass der Fortschritt in Friedenszeiten und die Sicherheit im Krieg eher auf die Informationstheorie angewiesen seien als auf Beweise, dass die von Albert Einstein postulierte Äquivalenz von Masse und Energie funktioniert. Die Informationstheorie war in der Öffentlichkeit angekommen, Shannon trat nun auch vor die TV-Kameras.

Bereits 1956 glaubten sich in den «Transactions» die Puristen, zu denen sich auch Shannon zählte, wehren zu müssen gegen die vielen Trittbrettfahrer, die den Begriff «Informationstheorie», der damals offenbar leicht Fördergelder lockermachte, für alles Mögliche verwendeten. «Auch wenn diese Welle der Popularität», so schrieb Shannon, «für uns alle, die wir auf diesem Gebiet tätig sind, sicherlich erfreulich ist und begeisternd, so ist sie doch auch gefährlich. Es könnte leicht passieren, dass der Erfolg über Nacht zerfällt, wenn es sich herausstellt, dass der Gebrauch von interessant klingenden Begriffen wie Information, Entropie, Redundanz nicht alle unsere Probleme löst.»
Sputnik-Schock

Ende der 1950er Jahre liess der Sputnik-Schock das Interesse an der Informationstheorie wieder auflodern. Die Funksignale diese sowjetischen Erdsatelliten wurden in den USA als Startschuss für einen Wettlauf ins All aufgefasst. In aller Eile wurde eine Defense Advanced Research Projects Agency (Darpa) aufgebaut, um die Forschungsbemühungen von Privatfirmen und universitären Laboren zu bündeln und zu fördern.

Wie lassen sich Informationen so codieren, so dass sie per Funk wohlbehalten über mehrere Hunderttausend Kilometer transportiert werden können? Diese Frage, für deren Lösung Shannon die Werkzeuge bereitgestellt hatte, beschäftigte während den 1960er und 1970er Jahre viele Wissenschafter. Die Lösungen, die sie entwickelten, brachten nicht nur die Raumfahrt voran, sondern erlaubten Ende der 1960er auch den Bau erster Modem für die Datenfernübertragung über Telefonleitungen. Computernetze weiteten sich aus, es wurden schliesslich, mit Geldern der Darpa, Verfahren entwickelt, um diese Netze zu vernetzen – das Internet war geboren.
Flucht vor dem Ruhm

Shannon scheint den Rummel um seine Person nicht gemocht zu haben. 1956 hatte er sich von den Bell Labs verabschiedet, um am MIT eine Professorenstelle anzutreten. Doch die Ruhe, die er für seine wissenschaftliche Arbeit brauchte, konnte er auch an der Universität nicht finden. Bald begann er seine Verpflichtungen in der Lehre zu vernachlässigen, blieb zu Hause. Die Doktorandenseminare fanden in seinem Wohnzimmer statt.

Er begann seltsame Maschinen zu bauen, einen Computer für die Verarbeitung von römischen Zahlen, ein Einrad mit einer exzentrischen Achse, auf dem der Fahrer auf und ab hüpft. Er baute Jonglier-Maschinen. Er beschäftigt sich auch theoretisch mit dem Jonglieren, und schlug – scherzhaft – die Gründung einer Jonglierwissenschaft («Jugglology») vor. Er sammelte Musikinstrumente und begann Nonsense-Gedichte zu schreiben. In einem Brief an den «Scientific American» schrieb er 1981: «Ich bin zum Schluss gekommen, erstens, dass ich als Poet besser bin denn als Wissenschafter, zweitens, dass der <Scientific American> Platz für die Publikation von Gedichten schaffen sollte.»

Seine letzte Maschine, «The ultimate Machine», präsentiert sich als kleines Kästchen mit einem einzigen Schalter. Wird die Maschine eingeschaltet, öffnet sich der Deckel: Eine Hand schaltet die Maschine wieder aus und verschwindet unter dem sich schliessenden Deckel.

Shannon hat die rasche Ausbreitung der Personalcomputer in den 1980er Jahren und die Popularität des Internets in den 1990er Jahren nicht mehr bewusst erlebt. Er litt an Alzheimer und war seit Mitte der 1980er Jahre ein Pflegefall. Kurz vor seinem Tod machte er sich noch die Mühe, in einer Skizze den Trauerzug für seine Beerdigung zu planen. Es wäre ein Karneval-Umzug geworden: Jazz-Musiker und Jongleure hätten den Zug angeführt, auf einem Wagen hätte ein Mensch gegen einen Computer Schach gespielt, eine grosse Folk-Band hätte den Schluss gebildet.";https://www.nzz.ch/digital/technikgeschichte-die-erfindung-des-informationszeitalters-ld.15409;NZZ;Stefan Betschon;;;
18.11.2019;Geisteswissenschaften sind unentbehrlich. Sie lehren uns das, was wir mehr und mehr brauchen: kritisches Denken;"Es ist ein gigantischer Fehler der heutigen Universitätslandschaft in vielen Ländern, dass die Geisteswissenschaften höchstens marginal zur Kenntnis genommen werden. Und es lässt aufhorchen, wenn Geisteswissenschafter selber anfangen, in die Kritik an ihrer Disziplin einzustimmen, wie Hans Ulrich Gumbrecht dies in dieser Zeitung getan hat (NZZ 29. 10. 19). Das Modell der Zukunft besteht darin, zur Einheit der Wissenschaften im Gefüge der Universität zurückzukehren. Der primäre Gegenstand aller Untersuchungen ist und bleibt der Mensch, der seine eigenen Lebensbedingungen erforscht und optimiert und dabei an nichtmenschlichen Umweltbedingungen interessiert ist, die in naturwissenschaftlichen Grundlagendisziplinen erkundet werden.

Daher lautet die Hauptfrage allen kritischen Denkens, wie wir seit Kant wissen: «Was ist der Mensch?» Ein Teil der Antwort auf diese Frage besagt, dass der Mensch ein geistiges Lebewesen ist. Solange wir uns nicht der Frage widmen, wie der Geist in die Natur passt, was sich nur in massiven Verbünden von geistes-, natur- und technowissenschaftlicher Expertise beantworten lässt, werden viele der dringenden Fragen unserer Zeit immer grössere Probleme generieren.

Die Geisteswissenschaften erforschen die historisch sich verändernde Dimension der Selbstinterpretationen des Menschen. Dazu wurden über Jahrtausende Praktiken der Interpretation symbolischer Codes und kultureller Phänomene entwickelt. Die Geisteswissenschaften erforschen mit aller wünschenswerten wissenschaftlichen Strenge, wie sich der Mensch in symbolischen kulturellen Kontexten selbst inszeniert.
Wie wir Strategien durchschauen

Die Selbstbildfähigkeit des Menschen heisst traditionell «Geist», und dieser ist in seiner äusserst komplexen Ausdifferenzierung Gegenstandsfeld eines eigenen Wissenschaftstypus. Wer Fremdsprachen und fremde Kulturen nicht methodologisch angeleitet erforschen und dies auch nicht staatlich fördern möchte, begeht einen kulturellen Fehler.

Doch nicht nur das: Auch strategisch grenzt es in einer globalen Wettbewerbssituation zwischen europäischen und US-amerikanischen und chinesischen Sozialmodellen an Idiotie, dass es kaum Lehrstühle für Sinologie gibt, durch deren Tätigkeit wir überhaupt in den Stand gesetzt werden, die ökonomischen und politischen Strategien Chinas zu durchschauen. Die chinesische Regierung fördert die geisteswissenschaftliche Erforschung fremder Kulturen – zum Zweck strategischer Ausrichtung etwa ihrer KI-Förderung. Dabei wirken neben technischen Experten auch Kunsthistoriker, Philosophen und Literaturwissenschafter mit.

Der brasilianische Staatspräsident, Jair Bolsonaro, hat unlängst mit einem Rundumschlag gegen die Geisteswissenschaften, allen voran gegen die Soziologie und Philosophie, auf sich aufmerksam gemacht. Ähnliche populistische Stimmungsmacher findet man in Ungarn, China, Russland, der Türkei und leider an vielen weiteren Orten.
Angriffe auf das Wissen

Freilich hat Bolsonaro durch den Entzug von Forschungsmitteln und Stipendien nicht nur die Geisteswissenschaften, sondern den gesamten Wissenschaftsbetrieb angegriffen. Denn solche Attacken führen letztlich zur Unterminierung von Objektivität: An die Stelle des Versuchs, evidenzbasierte politische Entscheidungen in einer durch die Digitalisierung immer komplexer werdenden Weltlage hervorzubringen, tritt der Angriff auf die Wissenschaften und die Orientierung an der Wahrheit.

Es ist also kein Zufall, dass die Geisteswissenschaften zur Zielscheibe des realhistorischen Populismus geworden sind. Denn sie liefern die methodologischen Grundlagen zur Entzifferung und Kritik der gegenwärtigen Verhältnisse. Ohne Politikwissenschaft könnte man die Regierungsformate des Populismus nicht einmal angemessen beschreiben. Ohne philosophisch begründete Ethik lässt sich der heute grassierende Kulturrelativismus nicht kurieren, der uns weismacht, in Russland oder China gälten andere Werte als in Europa, ohne eine übergeordnete moralische Autorität, die begründete, partiell kulturunabhängige Urteile fällen kann.

Wer heute die Geisteswissenschaften angreift und etwa zugunsten von Mint-Fächern und rein technischen Hochschulen mit weiterer Unterfinanzierung und Stellenabbau bedroht, öffnet dem Illiberalismus Tür und Tor. Universitätsreformen sind oft nichts anderes als der Vorhof dafür.
Der Mensch erforscht sich selbst

Alle grossen Schlagwörter unserer Zeit – von der Digitalisierung über die Risiken von KI, Fake-News/Deep Fakes bis hin zur Unterminierung der Demokratie durch soziale Netzwerke, Redefreiheit – verweisen auf Forschungsfelder, zu denen die historisch ausgerichteten Geisteswissenschaften dank ihren Analysemethoden einen privilegierten Erkenntniszugang haben.

Deswegen erstaunt es nicht, dass der Unternehmensberater Christian Madsbjerg in seinem Buch «Sensemaking. The Power of the Humanities in Age of the Algorithm» oder der führende KI-Forscher Stuart Russell aus Berkeley in seinem Buch «Human Compatible» dazu auffordern, die Geisteswissenschaften (alias «humanities») zu konsultieren, denn sonst tappen wir in die Falle tappen, dass ihre Reflexionskompetenz an die falsche Adresse delegiert wird.

Der menschliche Geist lässt sich in seinen historisch, sozial und kulturell diversen Codes nicht mit den Methoden des Machine Learning oder der kognitiven Neurowissenschaft entziffern, die vielmehr mit schockierender Regelmässigkeit philosophische, historische und soziologische Fehlschlüsse begehen und aufgrund ihrer strukturellen Macht auch noch verbreiten. Der Präsident der Northeastern University in Boston, Joseph E. Aoun, hat zu Recht dazu aufgefordert, ein Forschungsprogramm namens «Humanics» aufzulegen, das die Selbsterforschung des Menschen in allen wissenschaftlichen Registern vorantreibt.

Dabei ist die Selbstverzwergung zu vermeiden, die darin liegt, die Geisteswissenschaften auf subjektiv-ästhetische Formate des Kulturkommentars herunterzustufen. Geisteswissenschafter sind mehr als Zulieferer von Feuilletons, Theaterheften und Museumskatalogen, sie erkennen unter reflexiven Theoriebedingungen, wie sich der Mensch selbst beschreibt und welche Formen des Menschseins möglich und wirklich sind.
Naturwissenschaft braucht Reflexion

Was nützen Naturwissenschaften und Technik, wenn sie – wie in China – zur Unterwerfung der Bevölkerung eingesetzt werden? Was nützen diese Disziplinen dem Menschen, wenn sie, wie bisher, weitgehend ohne ethische Reflexion voranschreiten? Und was nützt Machine Learning, wenn es Cambridge Analytica und soziale Netzwerke hervorbringt, die gerade systematisch zur Selbstzerstörung des demokratischen Rechtsstaats führen?

Hier fängt kritisches Denken an, das in der gegenwärtigen Diskussion aufgrund eines unbegründeten Abgesangs auf die Geisteswissenschaften zu kurz kommt. Die ETH Zürich, gemäss allen Rankings eine der führenden Hochschulen der Welt, hat ein Programm namens Critical Thinking aufgelegt, das laut Website darauf abzielt, «die Fähigkeit zu erwerben und die Möglichkeit zu erhalten, begründete Entscheidungen zu treffen. Das fängt beim Lesen an und ho?rt beim Kochen auf – oder umgekehrt.»

Ich erspare mir an dieser Stelle eine kritische Analyse dieses Werbetextes und weise vielmehr darauf hin, dass die Kompetenz des kritischen Denkens, die hier gefordert wird, exakt die Domäne der Geisteswissenschaften ist. Diese muss man ins Gespräch mit den anderen Formen des Wissens bringen, um nicht den Fehler zu begehen, gegen den man mit dem «march for science» auf die Strasse zieht.

Wer die Geisteswissenschaften attackiert, unterminiert die Fähigkeit des methodologischen, reflexiven kritischen Denkens. Wer für die Naturwissenschaften und die Technik marschiert, ohne die Geisteswissenschaften gleichzeitig und mit denselben Argumenten zu stärken, zerstört die Grundlagen des Projekts der Aufklärung, dem wir den Begriff des kritischen Denkens verdanken.";https://www.nzz.ch/feuilleton/kritisches-denken-wieso-wir-die-geisteswissenschaften-brauchen-ld.1521256;NZZ;Markus Gabriel;;;
21.12.2018;Warten auf die Intelligenzexplosion;"Vielleicht sind ja die Maschinen bereits sehr viel gescheiter als die Menschen, vielleicht konnten sie bereits – von den Menschen unbemerkt – übermenschliche Fähigkeiten, Superintelligenz, entwickeln. Sie verraten sich dadurch, dass sie sich dumm stellen. Sehr dumm. Microsoft Outlook zum Beispiel. So dumm kann man gar nicht sein, so dumm ist nur, wer so tut als ob, wer sich verstellt.
Der teuerste Wecker

Was haben wir denn erwartet? Wie haben wir uns denn die Machtübernahme der Maschinen vorgestellt? Als Mediengrossereignis? So wie damals, im Mai 1997, als ein von IBM-Ingenieuren konstruierter Computer namens Deep Blue vor laufenden TV-Kameras den Schachweltmeister Garri Kasparow besiegte? Ist ein Computer, der pro Sekunde 200 Millionen Schachpositionen berechnen kann, intelligent? Wie intelligent ist er? Könnte er auch, wie Kasparow es getan hat, ein Buch schreiben über Intelligenz? Deep Blue sei etwa so intelligent wie ein Digitalwecker, schrieb Kasparow 2017 in «Deep Thinking»: «Nicht dass ich mich besser fühle, weil ich gegen einen zehn Millionen Dollar teuren Wecker verloren habe.»

Warum sollten die Maschinen uns ins Vertrauen ziehen, warum sollten sie uns mitteilen, dass sie das Stadium der Superintelligenz erreicht haben? Wären sie uns feindlich gesinnt, würden sie durch dieses Bekenntnis einen Vorteil verspielen, würden sie uns lieben, müssten sie befürchten, uns zu ängstigen.
Gedankenexperimente

Wie können wir herausfinden, was die Maschinen im Schilde führen? Reden wir mit ihnen! Es sei die Sprachbegabung, die den Menschen zum Menschen mache, so behaupteten bereits die ersten Philosophen, und im Gespräch, so glaubten Jahrtausende später die Computerwissenschafter, könnten Menschen die Denkfähigkeiten der Computer am ehesten beurteilen.

Als einer der Ersten hat nach dem Zweiten Weltkrieg Alan Turing die Frage aufgeworfen, ob man Maschinen bauen könne, die denken könnten. Es gab damals weltweit nur rund ein Dutzend Computer, deshalb dürfte Turing mit seinen Ideen viele seiner Zeitgenossen überfordert haben. Später dann, in den 1950er Jahren, begann sich «Elektronengehirn» als Synonym für Computer einzubürgern, und schon bald – früher, als Turing vorausgesagt hatte – war für die Menschen künstliche Intelligenz das Normalste der Welt, obwohl sich die Realisierung einer solchen Maschine als sehr viel schwieriger erwies, als Turing vermutet hatte. «Können Maschinen denken?» Mit dieser Frage begann Turing 1950 einen Aufsatz für die britische Fachzeitung «Mind». Turing löst diese Frage so, wie Alexander den Gordischen Knoten gelöst, so, wie Kolumbus ein Ei auf die Spitze gestellt hat: mit Gewalt. Definieren zu wollen, was Denken sei, sei «gefährlich», schrieb Turing. Also verzichtet er darauf und nennt eine «denkende Maschine» eine, die sich im Imitationsspiel bewährt, die sich so präsentiert, als könnte sie denken. Dieses Spiel wird heute Turing-Test genannt.
Die Stunde der Wahrheit

Turing sagte 1950 voraus, dass es gegen Ende des 20. Jahrhunderts erstmals Maschinen geben werde, die gemäss den Regeln dieses Spiels Intelligenz beweisen könnten. Turing verlegt die künstliche Intelligenz (KI) um 50 Jahre in die Zukunft. Diese Zurückhaltung ist unter Computerwissenschaftern selten. Meistens wird die Ankunft von Denkmaschinen um 15 bis 25 Jahre in die Zukunft verlegt. Diese Zahlen nennt der britische Mathematiker Stuart Armstrong, der 2012 eine Sammlung von 257 Prognosen ausgewertet hat. Dieser Zeitrahmen wird wohl durch das Bemühen bestimmt, einerseits den Durchbruch der KI möglichst nahe bei der Gegenwart anzusiedeln, um die Menschen aufzurütteln, und andererseits die Stunde der Wahrheit möglichst weit in die Zukunft zu verlegen, um die eigene Glaubwürdigkeit nicht zu gefährden.

Also: Es wird noch rund zwei Jahrzehnte dauern, bis uns die Maschinen intellektuell ebenbürtig sind. Die Inselbegabung, über die etwa ein Schachcomputer verfügt, wird sich nach und nach zu einer Artificial General Intelligence (AGI) ausweiten, so stellt man sich das vor. Dank dieser Fähigkeit könnte sich eine einzelne Maschine dann in verschiedenen Anwendungsbereichen bewähren, sie könnte den Menschen auf Augenhöhe begegnen. Die Begegnung dauerte aber nur kurz, bald würde sich die Maschine zur Superintelligenz aufschwingen und eine Intelligenzexplosion auslösen. Die Menschen blieben allein zurück.

Das Konzept der «Intelligenzexplosion» hat der britische Mathematiker Irving John Good sich ausgedacht. Anstatt «Superintelligence» verwendete er den Begriff «Ultraintelligence». Die Erfindung einer superintelligenten oder eben einer «ultraintelligenten» Maschine wäre die «letzte Erfindung des Menschen», so schrieb Good 1964. Denn in der Folge würden sich die Maschinen selber um das Erfinden neuer Maschinen kümmern. Mit hoher Wahrscheinlichkeit, so Good, werde diese letzte Erfindung noch im 20. Jahrhundert gelingen.
Das Gehirn nachbauen

Die Qualität der Voraussagen sei zumeist schlecht, schreibt Armstrong zusammenfassend in seinem Bericht über KI. Viele Prognostiker verliessen sich auf ihr Gefühl, nur selten werde versucht, die der Prognose zugrunde liegenden Annahmen explizit zu machen und ein Modell zu entwickeln, das die für die Entwicklung der KI relevanten Faktoren und ihr Zusammenspiel beschreibe.

Ein oft verwendetes Modell basiere auf dem Mooreschen Gesetz, auf der von Gordon Moore publizierten Vermutung, dass die Zahl der Komponenten, die auf einem Computerchip platziert werden könnten, sich alle zwei Jahre verdoppeln lasse. Dieses «Gesetz» lässt sich so interpretieren, dass die Rechenleistung exponentiell wächst. Man müsste also nur die Rechenleistung messen, um Aussagen über die Leistungsfähigkeit eines KI-Systems machen zu können. Doch dieser Zusammenhang zwischen Rechenleistung und Intelligenz ist nicht sehr eng; Deep Blue würde, wäre er nicht verschrottet worden, gegen neuere Schachcomputer, die weniger Rechenleistung besitzen, verlieren.

Wenn man annimmt – was zweifelhaft ist –, dass das Mooresche Gesetz seine Gültigkeit noch lange behalten wird und die Rechenleistung sich stetig steigern lässt, kann man berechnen, wann die Computer in der Lage sein werden, ein menschliches Gehirn nachzuahmen. Sei einmal ein künstliches Gehirn da, so glaubt unter anderen der schwedisch-britische Philosoph Nick Bostrom, sei auch künstliche Intelligenz nicht mehr weit. Bostrom – mit dem Buch «Superintelligence» (2014) berühmt geworden – schätzt, dass für eine elektrophysiologisch exakte Nachbildung des Gehirns eine Rechenleistung von 1022 Gleitkommaoperationen pro Sekunde (Flops) benötigt werde. Die Leistung der schnellsten Supercomputer der Welt liegt heute bei 1015 Flops. Die Rechenleistung müsste also um den Faktor 10 000 000 gesteigert werden. Ungefähr in 25 Jahren, so schätzte Bostrom 2008, werde die computertechnische Nachbildung des menschlichen Gehirns möglich sein.

Es gibt ernstzunehmende Wissenschafter – der amerikanische Physiker Freeman Dyson –, die glauben, dass es KI in absehbarer Zukunft nicht geben wird, und es gibt unter denjenigen, die KI für möglich halten, auch solche – der britische Physiker David Deutsch –, die glauben, dass der Erfolg von einer einzelnen Innovation, von einer nicht vorhersehbaren und nicht planbaren «Inspiration» abhängt. Die meisten Wissenschafter aber, die sich berufen fühlen, sich zur Zukunft der KI zu äussern, glauben, dass ein kontinuierlicher Fortschritt zur AGI führt. Wie viele Schritte sind es noch zum Ziel? Wie weit sind wir in den vergangenen 70 Jahren schon gekommen?
Grosse Fortschritte

In einzelnen Anwendungsbereichen der KI gibt es normierte Verfahren, die es erlauben, den Fortschritt zu bewerten. Bei den Schachcomputern verweist die Elo-Zahl auf die Spielstärke; bei der Spracherkennung ermöglicht die vom amerikanischen National Institute of Standards and Technology (Nist) entwickelte Speaker Recognition Evaluation einen Vergleich verschiedener Systeme. Standardisierte Tests gibt es auch für die Gesichtserkennung oder für die Handschrifterkennung. Die Qualität von maschinell hergestellten Übersetzungen wird gemäss einem Bleu (Bilingual Evaluation Understudy) genannten Verfahren bewertet; bei der Bildanalyse liefert die Imagenet-Datenbank einen Rahmen, um die Fähigkeiten von Softwareprogrammen zu testen.

All diese Messmethoden zeigen, dass in jüngster Vergangenheit in den genannten Gebieten – zumeist dank Techniken des Machine-Learning – grosse Fortschritte erzielt worden sind. In einigen Fällen werden den Computern bereits übermenschliche Fähigkeiten nachgesagt. Allerdings ist der Vergleich mit Menschen schwierig: Wenn diese etwa beim Verschriftlichen von Telefongesprächen oder bei Wiedererkennen von Gesichtern in der Imagenet-Datenbank mehr Fehler machen als die Maschinen, ist dies oft auch der Müdigkeit geschuldet. All diese Messmethoden sind in Teilbereichen wichtig, sie erlauben es, die Inselbegabungen der Computer zu bewerten. Aber sie ermöglichen es nicht, einen Bezug herzustellen zu einer universellen Intelligenz.

Was ist denn überhaupt Intelligenz? KI-Forscher haben von Turing gelernt, dass es gefährlich ist, sich auf diese Frage einzulassen; sie beschäftigen sich kaum je damit. Eine Ausnahme ist der deutsche Computerwissenschafter Marco Hutter, der an der Australian National University in Canberra lehrt und forscht. Er hat eine grosse Sammlung von Definitionen angelegt. Seine eigene Definition, die seiner Ansicht nach die Essenz einer Vielzahl von anderen Definitionen in sich vereinigen soll, lautet: «Intelligenz misst die Fähigkeit eines Agenten, Ziele in einer Vielzahl von Umgebungen zu erreichen.» Nun verdanken sich allerdings bedeutende Fortschritte in der Geschichte der Wissenschaft der Tatsache, dass Ziele verfehlt wurden (Kolumbus) oder dass Ziele erst als solche erkannt wurden, nachdem sie zufällig erreicht wurden (Serendipität). Könnte es auch eine Maschine geben, die ins Grübeln verfällt, die sich dem dumpfen Brüten überlässt, die das sinnlose Sinnieren liebt?
Täuschung als Prinzip

Nach dem Krieg bis zu seinem Selbstmord 1954 hat sich Turing in mehreren Vorträgen und Aufsätzen mit den Möglichkeiten künstlicher Intelligenz befasst. Seine Ideen sind gehaltvoll und weisen weit über seine Zeit hinaus. Beispielsweise hat er auch bereits über Programmiermethoden nachgedacht, die man heute als Machine Learning bezeichnen würde. Nach seinem verfrühten Tod ging er vergessen. Was blieb, war der Turing-Test.

Viele Computerwissenschafter haben die Simplizität dieses Spielchens kritisiert. Doch es hat die Wissenschaft von der künstlichen Intelligenz stärker geprägt, als viele wahrhaben wollen. Mehr noch: Turings Gedankenspiele haben wie ein Schwelbrand die philosophischen Grundlagen der Neuzeit zerstört. In diesem Sinne äusserte sich kürzlich der Schweizer Technikphilosoph Walther Zimmerli anlässlich einer Vorlesung am Collegium Helveticum in Zürich. Kernaufgabe der neuzeitlichen Philosophie sei seit Descartes der methodische Zweifel gewesen, der Kampf gegen die Täuschung. Doch Turing habe die «Täuschung zum Prinzip erhoben».
Der schöne Schein

Intelligent ist, was intelligent zu sein scheint. In diesem schönen Schein gefallen sich viele KI-Forscher und auch die Marketingverantwortlichen von Firmen, die KI-Produkte verkaufen möchten. Alle bejubeln die Erfolge der KI, doch es gibt keinen Massstab, um den Fortschritt zu vermessen, es gibt keine Verfahren, um Wissenslücken zuverlässig aufzuspüren. Alle tun so als ob. Doch dann hat ein geistig zurückgebliebener, 13-jähriger Hip-Hop-Fan aus der Ukraine dem Spiel ein Ende bereitet. Er heisst Eugene Goostman und spricht nur gebrochen Englisch. Er ist nicht einfach ein dummer Junge, er ist eine Maschine, die sich dumm stellt. Und die Menschen sind darauf hereingefallen.

Nichts weniger als einen «Meilenstein in der Geschichte der Computerwissenschaft» vermeldete die britische University of Reading am 8. Juni 2014. Erstmals habe ein Computer den Turing-Test bestanden. Das habe «grosse Implikationen für die heutige Gesellschaft», das sei ein «Weckruf». Doch wer sich die Protokolle der Gespräche ansieht, die Eugene und die Juroren geführt haben, wird Intelligenz nirgends finden. Die Software gibt ausweichende, dümmliche Antworten. Auf die Frage, wie viele Beine ein Kamel habe, antwortet Eugene: «Etwas zwischen zwei und vier. Vielleicht drei? :-))).» Die Frage, wie viele Beine eine Ameise habe, ergibt die Antwort: «Etwas zwischen zwei und vier. Vielleicht drei? :-))).» Eugene konnte das Imitation-Game gewinnen, weil die Juroren nicht damit rechneten, dass eine Maschine so gar nichts unternehmen würde, um Intelligenz vorzutäuschen.
Unerfüllte Hoffnungen

Die von unbekannten russischen und ukrainischen Programmierern geschaffene Eugene-Software ist sang- und klanglos in der Versenkung verschwunden. Ihr Sieg war aber insofern historisch, als er gezeigt hat, dass der Turing-Test nichts taugt. Seither wurden die Anstrengungen verstärkt, um Alternativen zu finden. Es wurde etwa ein Lovelace-Test vorgeschlagen: Hier soll sich der Computer als kreativer Künstler hervortun.

Bei einem anderen Test wird vom Computer verlangt, dass er bei TV-Serien voraussagen kann, wo die Menschen lachen werden. Dann gibt es die Winograd Schema Challenge: Sie wurde vom altgedienten kanadischen KI-Forscher Hector Levesque vorgeschlagen, der Name verweist auf den amerikanischen Computerwissenschafter Terry Winograd. Bei diesem Test geht es darum, Fragen zu beantworten, die das Sprachverständnis auf die Probe stellen. Beispiel: «Der Hammer passt nicht in den Koffer – er ist zu gross: Wer ist zu gross, der Hammer oder der Koffer?» Die amerikanische Association for the Advancement of Artificial Intelligence hat 2016 erstmals eine Winograd Schema Challenge durchgeführt. Das Preisgeld von 25 000 Dollar musste nicht ausbezahlt werden, die beste Software konnte nur knapp zwei Drittel der Fragen richtig beantworten.

Die hier beschriebenen Turing-Test-Alternativen sind alle auf kognitive Fähigkeiten ausgerichtet, die als typisch menschlich gelten. Wollte ein KI-Forscher eine Software entwickeln, die sich in diesen Tests bewährt, müsste er sich wiederum auf ein Imitationsspiel einlassen. Doch man wird die Begutachtung eines neuen Düsenjets nicht einem Vogelkundler überlassen wollen, deshalb sollte man sich auch beim Testen der Maschinenintelligenz von psychologischen und neurologischen Vorkenntnissen lösen. Die Faszination der KI besteht ja doch auch darin, dass sie Hoffnung macht auf völlig neuartige Formen von Gescheitheit.";https://www.nzz.ch/digital/warten-auf-die-intelligenzexplosion-ld.1446855;NZZ;Stefan Betschon;;;
23.04.2017;Coronavirus und Home-Office: Die Gewinner von heute könnten die Verlierer von morgen sein;"Die Corona-Krise betrifft uns alle – einige mehr, andere weniger. Seit dem 16. März 2020 arbeiten in der Schweiz zahlreiche Menschen im Home-Office, während andere den Alltag im besten Fall mit Lesen verbringen, sich nach besseren Zeiten sehnen oder aber um ihre existenzielle Grundlage bangen müssen. Einige gehören schliesslich zu den vom Bund ausdrücklich ausgenommenen Bereichen des Lockdowns und gehen trotz Ansteckungsgefahr ihrer Beschäftigung weiter am alten Arbeitsplatz nach (zum Beispiel im Spitalwesen oder in der Bauwirtschaft).
Weniger betroffene Dienstleister

Sieht man von diesen Ausnahmen ab und vernachlässigt man auch die aufgrund des Lockdowns reduzierte Nachfrage nach allem Möglichen, kann man über die Affinität von Berufen zum Home-Office die Verteilungswirkungen der Corona-Krise nach Berufsgruppen abschätzen. Aufgrund der Zusammensetzung der entsprechenden Erwerbstätigen ist es zudem möglich, die Auswirkungen auf Branchen, Regionen und einzelne Bevölkerungsgruppen zu eruieren. Genau das haben wir für die Schweiz auf der Basis einer relativ komplexen Methodik (mithilfe von Machine-Learning) errechnet.  Berufe mit notwendigem physischem Kundenkontakt eignen sich wenig für das Home-Office; auf einer Skala von 0 (völlig ungeeignet) bis 1 (sehr gut geeignet) figuriert zum Beispiel der Zahnarzt mit einem Wert von 0,03. Anders ist dies für die Hochschuldozentin, die – wie die jetzige Erfahrung zeigt – auch virtuell mit den Studierenden in Kontakt treten kann und in unserer Analyse entsprechend mit einem Wert von 1 erscheint. Die Anwendung dieser Erkenntnisse auf die gewichtete Zusammensetzung von Branchen mit Erwerbstätigen zeigt, dass die Gastronomie einer der tiefsten Indizes hat und somit, aus dieser Optik, von der Krise stark belastet wird. Mit am wenigsten negativ betroffen sind die Finanzdienstleister und Versicherer und der IT-Sektor.

Zwischen den Regionen gibt es aufgrund der branchenmässigen Zusammensetzung ein starkes Stadt-Land-Gefälle – das Oberengadin figuriert zum Beispiel mit einem tiefen Wert von 0,23. Grosse Unterschiede bestehen aber auch zwischen Wohnorten, mit einem tiefen Index von 0,11 für Arosa, immerhin 0,34 für Davos gegenüber Zürich mit 0,57. Generell gilt, dass die Erwerbstätigen mit eher guten Ausweichmöglichkeiten in Richtung Home-Office relativ gut ausgebildet sind und zu den obersten Einkommensgruppen gehören. Die Analyse impliziert also, dass die individuelle und die kollektive wirtschaftliche Betroffenheit durch die Corona-Krise sehr unterschiedlich und zum Teil auch überraschend ausfallen dürften. Für eine abschliessende Einschätzung der relativen Verlierer und Gewinner der gegenwärtigen Krise müssten allerdings weitere Faktoren (etwa die direkten Eingriffe und Kompensationen durch den Bund) mitberücksichtigt werden. Zudem hilft es den Home-Office-affinen Berufen in der Luftfahrtbranche wenig, wenn die Flugzeuge nicht fliegen. Ein zumindest kleiner Trost für all die heute von der Krise stark gebeutelten Beschäftigten dürfte aber folgende Überlegung sein.
Coiffeure sind schwerer zu ersetzen

Die sich unter Corona als krisenresistent entpuppenden Arbeitsplätze dürften in der langen Frist nämlich durch genau die Tätigkeiten und Berufe geprägt sein, die durch Globalisierung und Digitalisierung vermehrt unter Druck kommen. Es sind diejenigen Tätigkeiten, die dereinst auch eher in ein Home-Office jenseits der Landesgrenze verschoben und Opfer internationalen Outsourcings werden könnten. Die heute wirtschaftlich leidenden und deshalb deprimierten (gut ausgebildeten) Verkäuferinnen in Kleidergeschäften, die Coiffeure und Arbeitskräfte in der Gastronomie und im Tourismus hingegen müssen langfristig weniger Angst vor Kollegen und Kolleginnen im fernen Ausland haben.";https://www.nzz.ch/wirtschaft/coronavirus-und-home-office-nach-schweizer-regionen-und-branchen-ld.1552902;NZZ;Christian Rutzer, Rolf Weder;;;
03.10.2019;Wie Google das Cloud-Computing und dabei auch noch sich selber neu erfinden will;"Die Geschichte von Google begann, als die beiden Firmengründer – Larry Page und Sergey Brin – ihr Projekt aus ihrer Studentenbude an der Stanford University in eine Garage in Menlo Park zügelten, erste Investoren fanden und erfahrene Informatiker wie Urs Hölzle als Mitarbeiter gewinnen konnten. Das war im Sommer 1998.

Urs Hölzle, wie haben es die beiden jungen Studenten damals geschafft, Sie zu überzeugen? Sie waren in dieser Garage der Erwachsene, Sie hatten als Computerunternehmer und auch als Computerwissenschafter bereits viel erreicht.

Ich war damals schon seit ein paar Jahren Assistenzprofessor an der University of California in Santa Barbara. Doch ich wollte weg von der Uni, weil mein Lebenszentrum im Silicon Valley war. Hier lebte meine Frau, die an der Stanford University ihren Abschluss machte, und hier war der Sitz einer von mir gegründeten Software-Firma. So kam es, dass ich häufig im Flugzeug sass und zwischen Süd- und Nordkalifornien hin- und herpendelte. Das fand ich mit der Zeit mühsam. Ich wollte mir eine Auszeit nehmen, kam dann durch Zufall zu Google. Dieses Projekt hat mich sofort begeistert. Das war ein technisch interessantes Projekt, und vor allem waren die paar Leute, die damals diese Firma ausmachten, interessant. Anfangs dachte ich, ich bleibe ein Jahr, bis meine Frau ihr Studium abgeschlossen hat. Dann würden wir nach Europa zurückgehen. So fing das an. Haben Sie je bereut, dass Sie Ihre akademische Karriere aufgegeben haben?

Nein, das habe ich nie bereut. Mein Job bei Google fühlt sich ein bisschen an, als wäre ich an einer Uni. Es gibt hier eine intellektuell stimulierende Umgebung. Wir haben viele hochqualifizierte Mitarbeiter und probieren laufend neue Sachen aus. Aber wir haben mehr Möglichkeiten als an einer Uni. Wir haben mehr Leute, mehr Ressourcen, und vor allem haben wir es mit Problemen zu tun, die für die Praxis relevant sind. Ich kann hier mehr erreichen, als ich es könnte, wenn ich Professor wäre.

Kann im Bereich der Informatik die universitäre Forschung überhaupt noch mit den Privaten mithalten? Ist es ein Problem, wenn die computerwissenschaftliche Forschung privatisiert wird?

Ich glaube nicht, dass die Gefahr besteht, dass die Forschung privatisiert wird. Solche Befürchtungen haben wir schon diskutiert, als ich Anfang der 1990er Jahre in Stanford meine Dissertation vorbereitete. Diese Universität leistete sich damals noch eine eigene Chipfabrikation. Doch die Halbleiterfertigung wurde rasch teurer, bald konnten die Universitäten nicht mehr mithalten. Mussten sie deshalb auf Forschungsaktivitäten im Bereich Prozessorarchitektur verzichten? Nein. Man konnte zwar keine Halbleiterprodukte mehr machen, aber man konnte Ideen entwickeln und sie mithilfe von Simulationen überprüfen. Viele Innovationen, die heute in der Halbleitertechnik wichtig sind, kommen aus der akademischen Welt. Auch bei der Software-Entwicklung können die Universitäten heute mit privaten Firmen mithalten. Dank dem Cloud-Computing müssen sie kein grosses Rechenzentrum selber aufbauen. So haben die jüngsten technischen Entwicklungen die computerwissenschaftliche Forschung an den Universitäten erleichtert. Dazu kommt: Viele Firmen machen es so wie Google und publizieren die Resultate ihrer Forschungsabteilungen.

Ihre Berufskarriere umfasst viele Stationen, Sie waren Professor, Firmengründer. Sie haben sich mit Software-Entwicklung und programmiertechnischen Problemen beschäftigt. Sie haben sich aber auch bei Google um die Hardware gekümmert, um die Konfiguration von Server-Computern, um den Aufbau von energieeffizienten Rechenzentren. Hat Sie das ETH-Studium gut für diese beruflichen Herausforderungen vorbereitet?

Ich bin vor gut 30 Jahren fertig geworden an der ETH Zürich, ich habe damals wenig gelernt, was heute noch direkt für die Praxis relevant wäre. Aber die Grundlage, die diese Ausbildung schuf, war gut, sie ermöglichte es mir, Jahr für Jahr Neues zu lernen.

Würden Sie wieder Informatik studieren?

Ja, ganz klar. Aber nicht nur, weil mich die Informatik für sich genommen interessiert. Informatik ist eine Hilfswissenschaft, es braucht sie in der Chemie, in der Biologie, in der Pharmabranche, bei den Banken. Wer Informatik studiert, kann das mit anderen Interessen verbinden. Das macht den Beruf interessant, weil man sich nicht auf ein einzelnes Gebiet beschränken muss.

Was sind die wichtigsten Basisinnovationen, die in jüngster Vergangenheit die Computerwelt verändert haben?

Open-Source-Software hat alles verändert. Das ist ein wichtiger Trend. Alle setzen heute darauf. Als ich studierte, gab es am MIT Software-Aktivisten, die sich für freie Software engagierten. Man hielt sie für Spinner. Doch inzwischen ist freie Software, die im Quelltext in veränderbarer Form verfügbar ist, sehr wichtig. Und ich glaube, dass die Bedeutung von Open-Source-Software durch das Cloud-Computing weiter verstärkt wird. Denn jetzt kann man diese Software als Service nutzen, ohne selber am Quelltext herumzubasteln. Das ist auch der Ansatz, den wir bei Google mit Anthos verfolgen. Anthos ist Open-Source-Software. Der Quelltext ist offen. Aber man muss sich darum nicht kümmern, man kann die Fähigkeiten dieser Software als Dienstleistung von uns beziehen. Open-Source-Software ist wichtig und wird noch wichtiger werden. Bei den grundlegenden Software-Programmen, die jede Firma braucht, wird es bald nur noch Open Source geben. Heute ist der Anteil der freien Software auf einem Computer vielleicht 40 bis 70 Prozent, bald werden es 80 oder 90 Prozent sein.

Die zweite wichtige Basisinnovation ist das Cloud-Computing. Dadurch lassen sich viele Sachen vereinfachen. Und der dritte wichtige Trend ist Machine-Learning. Hier sind seit sieben, acht Jahren gewaltige Fortschritte passiert. Es ging alles sehr schnell. Dieser Ansatz wurde bereits in den 1980er und 1990er Jahren ausprobiert. Es hat damals nicht funktioniert, weil man nicht genügend Trainingsdaten hatte. Jetzt sind Sachen realisiert worden, die man noch vor wenigen Jahren für unmöglich hielt. Auch Experten wurden überrascht, beispielsweise im Bereich der Bilderkennung.

Google hat vor einem Jahr eine Forschungsarbeit vorgestellt, die Hoffnung darauf machte, dass sich Rechenzentren mithilfe von künstlicher Intelligenz effizienter betreiben lassen. Rechenzentren verbrauchen sehr viel Energie, und ein grosser Teil davon wird für die Kühlung aufgewendet. Sie haben sich persönlich immer wieder für effizientere Rechenzentren engagiert. Kann die künstliche Intelligenz hier helfen?

Ja, die Hoffnungen haben sich erfüllt, Machine-Learning hat sich in den Rechenzentren durchgesetzt. Wir sind jetzt daran, diese Erkenntnisse auch für Klimaanlagen ausserhalb der Rechenzentren fruchtbar zu machen. Die sind typischerweise falsch eingestellt. Doch mit Machine-Learning kann man das optimieren. Wir konnten eine Verbesserung von 40 Prozent erreichen. Es werden bei Bürogebäuden in Sachen Energieeffizienz bald grosse Verbesserungen möglich sein.

Als Anbieter von Cloud-Computing-Dienstleistungen ist Google ein Nachzügler. Jetzt haben Sie eine Aufholjagd gestartet. Sie haben die Prognose gewagt, dass Google dereinst mit Cloud-Dienstleistungen mehr Geld verdienen werde als mit Online-Werbung. Wie wollen Sie starken Mitbewerbern wie Amazon oder Microsoft Marktanteile wegschnappen?

Cloud-Computing ist bei Google das Projekt Nummer eins. Unsere Anthos-Software gibt uns die Chance, die Mitbewerber zu überholen. Das passiert jetzt vielleicht nicht in diesem Jahr, aber vielleicht im nächsten. Noch immer sind 80 bis 90 Prozent der IT-Ressourcen noch nicht in die Cloud verlagert worden. Die Entwicklung hin zum Cloud-Computing ist noch sehr jung. Aber es ist heute allen klar: Es führt kein Weg an der Cloud vorbei.

Heute fragen sich viele IT-Verantwortliche: Soll ich mich für die Microsoft-Cloud entscheiden, oder brauche ich die Amazon-Cloud? Sie denken, das sei die wichtigste Entscheidung. Doch diese Denkweise ist veraltet. In den 1990er Jahren war es so, dass die Wahl eines Hardware-Lieferanten auch die Software-Ausstattung bestimmte. Da gab es die HP-Version eines Unix-Betriebssystems und die dazu passende Datenbank-Software, oder es gab die Sun-Version eines Unix-Systems mit einer anderen Datenbank. Heute entscheidet man sich für Linux und MySQL, und auf welcher Hardware diese Open-Source-Software läuft, spielt keine Rolle. Die Hardware kann man jederzeit auswechseln. In der Cloud ist diese Öffnung noch nicht passiert. Bald soll es so sein, dass der Entscheid für Anthos keine Abhängigkeiten von einem Lieferanten schafft. Es spielt keine Rolle, was unterhalb von Anthos läuft, Google-Cloud oder Amazon-Cloud oder was auch immer. Wir möchten einen offenen Standard schaffen, der für die nächsten 20 Jahre definiert, was die Cloud ist. Die Standardisierung ist wichtig, damit im Enterprise-Computing wieder Innovationen möglich werden.

Vielen europäischen Informatikverantwortlichen macht die amerikanische Cloud Act Bauchweh. Wie können die Kunden von Google sicher sein, dass die Daten den regulatorisch vorgegebenen Rechtsraum nicht verlassen?

Es gibt seit kurzem mit der Google-Cloud-Platform-Region in Zürich für Schweizer Kunden eine schweizerische Lösung. Das ist eines von weltweit 19 regionalen Angeboten. Die Präsenz vor Ort hat nicht nur technische Vorteile, sondern klärt auch die juristische Situation.

Sie haben Google vor 15 Jahren nach Zürich gebracht. Inzwischen ist Zürich der grösste Forschungsstandort von Google ausserhalb des Silicon Valley. Knapp 4000 Leute arbeiten hier für Google. Sind Sie mit der Entwicklung, die hier passiert ist, zufrieden?

Dass Google hier so gross geworden ist, war nicht geplant. Der hiesige Standort hat sich entwickelt, weil eine organische Entwicklung möglich war, weil es hier die richtigen Rahmenbedingungen gab. Google Zürich wurde grösser als die anderen Standorte, weil es hier besser funktioniert hat als anderswo.

Sind Sie oft in Zürich?

Ich bin eigentlich nur noch selten in der Schweiz.

Gibt es Sachen aus der Schweiz, die Sie vermissen, wenn Sie im Ausland sind?

Ja, ganz sicher, Schoggi ist in den USA ein Problem und auch das Brot. Und manchmal, bei gewissen politischen Themen, fühle ich mich ein bisschen fremd in den USA. Doch Kalifornien ist nicht die USA, und das Silicon Valley ist nicht Kalifornien. Es ist oft sonnig und warm, ich kann mit dem Velo zur Arbeit fahren, das ist superschön. Ich komme gerne nach Hause, ich meine in die Schweiz, um Freunde und Verwandte zu besuchen, aber ich fliege auch gerne wieder zurück in mein Zuhause im Silicon Valley.";https://www.nzz.ch/digital/unser-mann-im-silicon-valley-ld.1512839;NZZ;Stefan Betschon;;;
09.10.2019;Oracle entdeckt die Cloud 2.0;"Viele Geschichten über Informatik-Innovationen beginnen am europäischen Forschungszentrum für Teilchenphysik (Cern) in Genf. Die Experimente, die hier durchgeführt werden, produzieren sehr grosse Datenmengen, deshalb sind die Wissenschafter in Sachen Informatik stets auf das Neueste und Beste angewiesen.
Elastische Hardware

Und auch wenn es darum geht, einem interessierten Publikum die Forschungsresultate vor Ort zu zeigen, braucht es leistungsfähige Computer. Alle paar Jahre, wenn die Teilchenbeschleuniger für die Wartung ausgeschaltet werden müssen, können sie auch von Aussenstehenden besichtigt werden. Die sogenannten «Open Days» des Cern sind sehr populär, der Besucherandrang ist gross. Deshalb ist es keine triviale Angelegenheit, eine Web-basierte Software zu entwickeln, die es Interessierten erlaubt, sich für dieses Ereignis anzumelden.

Die zuständigen Programmierer haben heuer für dieses Problem zusammen mit Oracle eine Cloud-basierte Software entwickelt. Sie mussten sich deshalb nicht um die Hardware kümmern, und konnten doch sicher sein, dass die Lösung auch bei Grossandrang stets zuverlässig funktionieren würde. Insgesamt haben sich innert weniger Tage knapp 90 0000 Besucher für die «Open Days» angemeldet. Bei Belastungsspitzen musste die Hardware-Kapazität rasch erweitert werden, die Prozessor-Leistung wurde dann fast um das Fünffache erhöht.

Diese «Elastizität» ist einer der Gründe, die für eine Cloud-Lösung sprechen. An der Oracle Cloud Infrastructure schätzten die Cern-Programmierer zudem auch noch das Autonomous Transaction Processing: Die Software kümmert sich selber um die Datenbankverwaltung. Dank Machine Learning kann das System Routineaufgaben wie Geschwindigkeitsopitimierung oder Datensicherung selbständig ausführen. Selbst das Einspielen von Software-Updates erzwingt keinen Betriebsunterbruch.

Die kalifornische Software-Firma Oracle ist – am Umsatz gemessen – die drittgrösste Software-Firma der Welt. Es ist auch eine der ältesten Firmen in dieser Branche. Das erste und nach wie vor zentrale Software-Produkt ist eine Datenbank. Diese Software wurde laufend erweitert, sie ist inzwischen so etwas wie ein Software-Museum: Viele Informatik-Trends der Vergangenheit haben hier Spuren hinterlassen, diese Software unterstützt ihre Anwender nicht nur beim Umgang mit Datenbank-Tabellen auf der Basis der SQL-Abfragesprache, sondern sie sie hat auch etwas zu bieten, wenn es um In-Memory-Computing geht, um Machine Learning, Microservices oder um die Blockchain.
Regionale Cloud-Angebote

Oracle ist mit dem Verkauf von Software-Produkten gross geworden, jetzt dreht sich alles um Cloud-Computing-Dienstleistungen. Dabei bemüht sich die amerikanische Firma, geografisch in nächster Nähe der Kunden präsent zu sein. Der Aufbau von regionalen Cloud-Angeboten wird rasch vorangetrieben: Bis Ende 2020 sollen 20 neue Zentren eingerichtet werden. Dann wird Oracle weltweit in 36 Regionen mit einem eigenständigen Cloud-Angebot vor Ort sein. In der Schweiz gibt es seit September eine eigene «Oracle Cloud Region».

«Ich fühle mich nicht als Herausforderer, sondern als Pionier», sagt Hanspeter Kipfer im persönlichen Gespräch. Er leitet die Schweizer Niederlassung von Oracle. Ihm zufolge steckt die «Cloudifizierung» noch in den Anfängen; vier von fünf Firmen hätten noch nicht damit angefangen, die Software in die Cloud zu verlagern. Trotz starker Konkurrenz sei Wachstum angesagt.

In jüngster Vergangenheit sah sich Oracle gezwungen, sich selber im Zeichen des Cloud-Computing neu zu erfinden. Dieser Wandel zeigt sich äusserlich in einem veränderten Corporate Image. Das knallige, aggressive Rot, das bisher das Erscheinungsbild dieser Firma prägte, wurde durch zarte Pastelltöne abgelöst. Die matten Farbflächen werden von simplen Strichzeichnungen strukturiert, die an asiatische oder afrikanische Kunst erinnern. Das neue Image soll, so heisst es in einem internen Papier, eine Firma zeigen, die «persönlicher», «menschlicher» und «umgänglicher» geworden sei.
Autonome Software

Das Firmenlogo von Oracle ist wärmer, weicher geworden, der Chef aber hat nichts von seiner zupackenden, angriffigen Art verloren. Larry Ellison, seit 42 Jahren an der Spitze von Oracle, hat bei seinen öffentlichen Auftritten anlässlich der Hausmesse Oracle World im September immer wieder den direkten Vergleich mit der Konkurrenz gesucht. Software-Hersteller wie Salesforce oder SAP seien Oracle unterlegen, weil sie ihre Applikationen nicht als Cloud-Dienstleistung verkaufen könnten; Cloud-Anbieter wie Amazon seien im Nachteil, weil sie den Kunden zwar Rechenleistung aber keine Applikationen offerieren könnten. Detailliert schilderte Ellison, welche Kunden man den Konkurrenten bereits abspenstig machen konnte.

Oracle hat sich selbst als Dienstleistungsfirma neu erfunden und wagt sich jetzt daran, das Cloudcomputing zu erneuern. Anlässlich der Oracle World präsentierte Ellison die zweite Generation des Cloud Computing. Das wichtigste Merkmal der neuen Cloud sei die Autonomie, die Fähigkeit, sich selber zu verwalten. Diese Fähigkeit basiert hauptsächlich auf einer neuen Linux-Variante.

Das Adjektiv «Autonomous» bezeichnet eine Weiterentwicklung von Oracle Linux. Dieses Betriebssystem kann sich selber konfigurieren. Deshalb, so behauptete Ellison, gebe es nun keine Konfigurationsfehler mehr, die Sicherheit sei besser gewährleistet. Software-Updates könnten im laufenden Betrieb eingespielt werden. Wann immer auf der Ebene des Betriebssystems eine Sicherheitslücke entdeckt wird, kann Oracle Software-Updates bereitstellen, die dann unverzüglich und ohne Betriebsunterbruch automatisch installiert werden.";https://www.nzz.ch/digital/oracle-entdeckt-die-cloud-20-ld.1514368;NZZ;Stefan Betschon;;;
20.06.2018;Faule Fussballspieler sind besonders wertvoll – das sagt zumindest eine Maschine;"Es sind zwei Sekunden. Zwei Sekunden, die aus einem gut bezahlten Talent einen teuren Star machen. Zwei Sekunden, in denen Irdische gerade einmal ihren Kaffee verschütten können. Doch dem Kolumbianer James Rodríguez reichen sie aus, um gegen Uruguay Kunst auf den Rasen zu bringen.

Ein kühner Pinselstrich, der am Fuss beginnt und an der Unterlatte des gegnerischen Tors endet. Später wird Rodríguez' «Werk» zum schönsten Tor der WM 2014 gewählt. Es ist wohl auch einer der Gründe, wieso er innert vier Wochen seinen Marktwert auf 60 Millionen Euro steigert – und damit zum wertvollsten Mittelfeldspieler der WM wird.

Doch wenngleich das Tor zum Träumen einlädt, wahre Kenner der Fussballkunst sollten während der WM auf andere Dinge schauen. Die wertvollsten (sprich: teuersten) Spieler des vergangenen Turniers zeichneten sich vereinfacht gesagt vor allem durch drei Dinge aus: ein Auge für Mitspieler, Leidensfähigkeit und Faulheit. Oder um das letzte Attribut neutraler zu formulieren, Effizienz. Zu diesem Schluss kommt allerdings kein Trainer, kein Spielerberater, kein TV-Experte – sondern eine Maschine.

Die NZZ hat für die vergangene WM in Brasilien Daten von über 600 von der Fifa gelisteten Spielern gesammelt. Wie viele Minuten hat Lionel Messi gespielt? Wie viele Bälle hat Mats Hummels wieder gewonnen? Und was war eigentlich Neymar nach der WM wert? All diese Fragen lassen sich mit diesem Datensatz beantworten. Wir wollten aber wissen: Kann eine Maschine aus den Statistiken eines Spielers seinen Marktwert ableiten? Dafür fütterten wir einen Algorithmus namens «Ada Boost» mit den Daten von jenen 271 Spielern, die im Schnitt mindestens eine Halbzeit spielten und einen Marktwert von mehr als null nach der WM hatten. Anschliessend schauten wir, ob die Maschine «lernen» konnte, welche Faktoren mit einem hohen Marktwert einhergehen. Und siehe da: Ada konnte.

Tatsächlich pickte die Maschine für die letzte WM jeweils mehr als die Hälfte der 20 teuersten Spieler für die Mannschaftsteile Sturm, Mittelfeld und Abwehr richtig heraus. Anscheinend kann also eine Maschine die Klasse eines Lionel Messi, eines Andrés Iniesta oder eines Mats Hummels erkennen. Doch woran macht Ada den Wert eines Spielers fest?
So funktioniert Ada Boost Die jeweils drei Kriterien pro Mannschaftsteil, die laut Ada am meisten Einfluss auf den Marktwert eines Spielers hatten, sind im Folgenden dargestellt. Sie sind teilweise überraschend, teilweise haftet ihnen etwas Orakelhaftes an. Denn bei aller maschinellen Intelligenz kann Ada nur Trends erkennen. Sie kann sagen, welche Statistik stark mit dem Marktwert ansteigt oder fällt. Ob das jedoch wirklich der Grund für den hohen Marktwert nach der WM ist und was der Trend bedeutet, bedarf immer noch der menschlichen und damit durchaus streitbaren Interpretation.  Stürmer: Vorbereiter statt Knipser

(83 Spieler in die Analyse eingeschlossen, 11 der Top 20 richtig vorhergesagt)

Die 20 teuersten Stürmer standen besonders lange auf dem Rasen. Das mag banal klingen, zeigt aber, wie unabdingbar sie für ihre Mannschaft sind und dass ihre Trainer sie möglichst lange auf dem Feld lassen wollen. Der absolute Spitzenreiter in Sachen Spielzeit war an der WM 2014 Lionel Messi. Nur in der Vorrunde gegen Nigeria hatte der Argentinier vorzeitig Feierabend. Sonst musste er gleich drei Mal sogar ein Spiel mit Verlängerung durchstehen, also 120 Minuten, was seinen Schnitt von 99 Minuten pro Spiel erklärt. Was genau macht aber die Unabdingbarkeit von Spielern der Messi-Klasse aus? Menschliche Experten werden sagen, dass es die Tatsache sei, dass sie jederzeit Entscheidendes in einem Spiel tun könnten, ein Tor oder einen Assist. Laut unserer Maschine Ada ist es aber vor allem der letztere Aspekt, der den Ausschlag gibt. Messi und Co. haben ein viel besseres Auge für ihre Mitspieler.

Die Top 20 spielten im Mittel 0,7 erfolgreiche Pässe in den Strafraum. Der Rest ihrer Zunft kann von solcher Massarbeit nur träumen. Nicht einmal die Hälfte der anderen Stürmer schaffte es überhaupt, einen entscheidenden Pass in den Sechzehner zu spielen – weshalb auch der Mittelwert für sie bei null liegt.
Die 20 wertvollsten Stürmer der WM 2014 Was aber verwundert, ist eine andere Statistik. Knipser im Strafraum sind die Edelstürmer laut Adas Analyse nicht wirklich. Die Top 20 feuerten einen besonders grossen Anteil ihrer Schüsse von ausserhalb ab – und das, obwohl der überwiegende Teil der Tore im Strafraum erzielt wird. Steigt also der Marktwert, je öfter und ineffizienter man von draussen draufhaut? Wohl eher nicht. Plausibler ist der umgekehrte Zusammenhang. Gute Spieler sind aus allen Lagen gefährlich. Gleichzeitig haben sie auch mehr Freiheiten und mehr Verantwortung. Es wird von ihnen erwartet, selbst in schwieriger Position auf das Tor zu schiessen. Man lässt am Ende halt lieber Cristiano Ronaldo schiessen, statt seine weniger talentierten Mitspieler herumstochern und den Ball verlieren zu lassen. Teure Mittelfeldspieler müssen leiden. Denn weil sie so gut sind, können sie offenbar oft nur noch mit einem Foul gestoppt werden. Das beste Beispiel dafür war bei der vergangenen WM der Chilene Arturo Vidal. Vidal, Spitzname «Krieger», ist selbst kein Kind von Traurigkeit. Aber während des letzten Turniers wurde er 3,5 Mal pro Spiel gefoult. Mittelfeldspieler, die dagegen nicht zu den Top 20 gehörten, kamen in der Regel glimpflicher davon – wenngleich es auch Ausnahmen gibt. Wilson Palacios ist so ein Beispiel. Der Honduraner wird nur Insidern ein Name sein. Doch er wurde in gerade einmal 133 Minuten Einsatzzeit 8 Mal gefoult, was ihm zum schmerzhaften Spitzenreiter der letzten WM macht.
Die 20 wertvollsten Mittelfeldspieler der WM 2014 Eine weitere Statistik, die Ada als markant für teure Mittelfeldspieler ausmachte, war, dass die Top 20 mehr gute Flanken schlugen als der Rest. Ausserdem wichtig für den Wert eines guten Mittelfeldstrategen: die gelaufenen Meter, wenn der Gegner den Ball hat. Oder genauer gesagt, die nicht gelaufenen Meter.

Was zunächst nach Leistungsverweigerung klingt, hat wahrscheinlich andere Gründe. Gute Spieler und Mannschaften halten länger den Ball und lassen durch effiziente Pässe vor allem den Gegner rennen. Das Paradebeispiel dafür ist der Spanier Andrés Iniesta. Fast jede Minute, die Iniesta auf dem Platz stand, hat er einen Pass gespielt (0,89 Pässe pro Minute), und von diesen Pässen kamen 82 Prozent an. Entsprechend musste er nur 3,2 Kilometer pro Spiel gegen den Ball laufen. Abwehr: Spielerisch die Maloche umgehen? Das grösste Rätsel, das Ada bereithält, ist ihre Bewertung von Abwehrspielern. Die drei Hauptkriterien, auf die Ada schaut, sind nachvollziehbar: wiedergewonnene Bälle, gelaufene Meter ohne Ballbesitz und Punkte, die die Nationalmannschaft des Spielers bei der vorherigen WM errungen hat. Überraschend ist aber, in welche Richtung die Trends für die Top 20 zeigen. Die Crème de la Crème gewinnt nämlich im Mittel mehr Bälle, läuft aber auch weniger Meter gegen den Ball. Wie kann das sein?  Die Daten deuten darauf, dass Abwehrspieler auf zwei Arten glänzen können. Entweder klassisch, indem sie den Ball erobern. Das war der Fall bei Mats Hummels. Oder Abwehrspieler können auf moderne Art das Spiel bestimmen. Indem sie spüren, wie sich eine Aktion entwickelt, sich entsprechend positionieren und Angriffe unterbinden, bevor sie brenzlig werden. Indem sie beim Spielaufbau helfen, den Gegner so wenig in Ballbesitz kommen lassen und so entsprechend weniger Meter gegen den Ball arbeiten müssen – so wie die Brasilianer David Luiz und Thiago Silva. Es bleibt aber noch eine Erklärung offen dafür, wieso die Punkteausbeute bei der vergangenen WM ein guter Indikator für den Marktwert war. Das könnte damit zusammenhängen, dass nach der letzten WM vor allem Abwehrspieler aus Nationen hoch im Kurs waren, die bereits 2010 glänzten. Zum anderen könnte der Trend aber auch darauf deuten, dass Abwehrspieler mit der Erfahrung von Halbfinals und Finals mehr wert sind. Schliesslich kann ein Fehler in der Nachspielzeit über Cup oder frühe Heimreise entscheiden. In solchen Situationen auf die Erfahrung früherer K.-o.-Spiele zurückgreifen zu können, könnte helfen.

Die traurige Ausnahme von der Regel: die Niederlande. Schaut man auf die Punkteausbeute des Teams, also drei Punkte für Siege und einen Punkt für ein Unentschieden, so ist die Bilanz für Oranje respektabel. Leider reichte es aber beide Male nicht für den Titel – auch weil die eigenen Verteidiger auf der grossen Bühne patzten. 2006 kassierte John Heitinga im Final eine rote Karte in der 109. Minute. Wenig später kassierten seine verbliebenen Kollegen das entscheidende Gegentor. 2010 war es im Elfmeterschiessen der niederländische Verteidiger Ron Vlaar, der mit einem schlecht platzierten Schuss die Niederlage seiner Mannschaft einleitete. Dass niederländische Verteidiger also nach der WM 2014 so wenig wert waren, könnte auch mit ihrem Ruf als Verlierer zu tun haben – etwas, das nicht in den Daten steckt, die Ada analysierte Und was ist mit den Goalies?

Natürlich können auch Torwarte durch eine gute WM ihren Marktwert stark steigern. So geschehen bei Keylor Navas. Der Nationaltorwart von Costa Rica spielte eine überragende WM und wechselte wenig später zu Real Madrid. Das Problem ist: Es gibt zu wenig Daten zu Torhütern. Entsprechend haben wir uns dagegen entschieden, ein Modell für den Wert für Torhüter zu errechnen. Die Unterscheidung zwischen Titan und Fliegenfänger überlassen wir lieber den Experten auf der Couch und in den Fernsehstudios.
";https://www.nzz.ch/sport/fussball-wm-2018/faule-fussballspieler-sind-besonders-wertvoll-das-sagt-zumindest-eine-maschine-ld.1388000;NZZ;Haluka Maier-Borst, Parijat Ghoshal, Balz Rittmeyer;;;
05.06.2020;Diese Pandemie haben wir wohl erstmals in Echtzeit erlebt. Und dagegen half oftmals nur eins: Abtauchen ins digitale Elysium;"Heute will ich Ihnen einmal von einer schönen Seite der Krise berichten.

Meine Frau und ich sitzen beim Abendessen. Vor uns liegt, klein, schwarz und elegant, ein Smartphone. Eine nahe Telekom-Antenne übermittelt die Daten eines Musikstreamings. Das Gerät vor mir entschlüsselt diese und gibt sie als Töne per Bluetooth auf einen Boom weiter, der als grauer Zylinder auf unserem Tisch sitzt, sozusagen als digitaler Bruder der Rotweinflasche. Aus diesem erklingt die Musik. Wir können die Lieder beliebig per leichtem Tastendruck auf dem Smartphone wechseln. Damit gehen wir durch die halbe Pop- und Rockgeschichte unseres Lebens. Die Ingredienzen für unser Essen haben wir über das Internet bei einem Grossverteiler bestellt, der die Nahrungsmittel auf einer vielschichtigen Website präsentiert und nach Bestellung zuverlässig nach Hause liefert. Während des Essens rufen unsere Enkel über Facetime an und wollen eine Gutenachtgeschichte hören.

Für viele von uns ist das in der Zwischenzeit banal und alltäglich. Was allerdings nicht banal ist: Von den Tausenden Krisen der Menschheit hat keine einzige, auch keine der kürzlichen, auf diesem digitalen Niveau stattgefunden. Dies ist die erste voll digitalisierte Krise der Geschichte. Wir erleben sie deshalb auch ganz anders als ihre Vorgängerinnen: unmittelbarer, wuchtiger und aufgeklärter.
Der unendliche Raum des Wissens

Die Grundsteine für dieses digitale Erlebnis wurden über viele Zwischenstationen seit dem Zweiten Weltkrieg gelegt: Erfindung des Computers, Durchbruch des Realtime- und Personal Computing, Internet (1969), World Wide Web (1989) und Windows (seit 1985), das erstmals das zeitverschobene Arbeiten in einer virtuellen Welt erlaubte, in der die Sonne nie untergeht.

Was aber die Technologie wirklich zu dem machte, was wir in der voll digitalisierten Krise erleben, sind zwei relativ junge Elemente: das erste ist die beinahe vollständige elektronische Erfassung aller Daten der Menschheit. Nach einer Studie von Kenneth Cukier und Viktor Mayer-Schönberger waren im Jahr 2000 nur gerade ein Viertel aller aufbewahrten Informationen in digitaler Form abgelegt, während der Rest auf analogen Trägern wie Papier oder Film festgehalten war. Lediglich dreizehn Jahre später wurde nur noch ein verschwindender Anteil von weniger als 2 Prozent aller Daten in einer anderen Form als digital gespeichert. Dies war ein gewaltiger Schritt und ermöglichte den Aufstieg von Big Data und damit den Durchbruch der künstlichen Intelligenz einschliesslich des Machine-Learning.

Im Jahre 2007 brachte dann Apple das erste iPhone auf den Markt, mit dem jedermann weltweit, ohne grosses Vorwissen, mit minimalem Aufwand und zu geringen Kosten, Zugriff auf viele dieser Daten erhielt. Man kann sich das so vorstellen: Während die «Datafizierung» einen breiten Datenstrom kreierte, hat uns Steve Jobs eine Angel gegeben, mit der wir alle in diesem Strom fischen dürfen. Damit hat eine unglaubliche Demokratisierung des Zugangs zu Wissen stattgefunden.

In der «alten» Welt, die von der Erfindung des Buchdrucks bis zu dieser Desintermediation des Wissens dauerte, wurde das Wissen von ständisch organisierten Experten gehortet und nur dosiert, gegen entsprechende Honorierung, weitergegeben an Banken, Börsen- und Immobilienmakler, Ärzte, Juristen, Professoren und Journalisten. Wissen und Verständnis waren Blackboxes, und die Herren über diese waren hohe Priester, Magier oder zumindest gerissene Geschäftemacher und talentierte Schwadroneure.
Die neue Währung heisst Vertrauen

Heute brauchen wir die Experten mehr denn je, aber anders und entmystifiziert. Während sie in der alten Welt Informationen und Daten lieferten, ist es heute ihre Aufgabe, Erklärung und Verständnis zu vermitteln. Die reine Information können wir uns meistens selbst verschaffen, online. Wir müssen den Medien nicht einfach vertrauen, dass die Daten stimmen, sie müssen sich unser Vertrauen verdienen (und haben Sie schon einmal bemerkt, wie viel disziplinierter und ausgewogener die Medien in der heutigen Corona-Situation sind im Vergleich zur Finanzkrise?). Und wenn wir zum Arzt oder zur Juristin gehen, hören wir ihnen zwar genau zu, diskutieren aber zugleich, was wir darüber schon im weltweiten Netz gelesen haben.

Die Digitalisierung ist der zentrale Aspekt, wie wir diese Krise verstehen müssen, im Entstehen, in deren Überwindung und schliesslich in den Nachwirkungen und philosophischen Grundfragen dazu. Man kann argumentieren, dass diese Krise ohne die Mechanismen der digitalen Welt nicht in dieser Form und Wucht entstanden wäre. Wir wissen aus vielen empirischen Studien, dass heute skandalträchtige und andere schlimme Nachrichten innert weniger Stunden in der ganzen Welt bekanntwerden. Demgegenüber wurde, um ein beliebiges Beispiel zu nehmen, der Felssturz von Plurs, wo im Jahre 1618 der Berg ein ganzes Städtchen verschluckte, erst 24 Stunden später im benachbarten Chiavenna und noch viel später im Mittelland ruchbar.

Im Falle von Corona haben wir über fast alles in kürzester Zeit Bescheid gewusst: die Entstehung der Seuche, ihre Ursachen, die rasche Verbreitung, die hohe Mortalität unter älteren Leuten, die Chancen von Medikamenten und Impfstoffen, die erfolgreichsten Verhaltensweisen, um eine Ansteckung zu vermeiden. Damit ist eine nahezu fieberhafte Datenlage entstanden, in der wir uns alle stündlich Sorgen und Angst machen.

Man kann sich fragen, was geschehen wäre, wenn sich diese Neuigkeiten nur im Schneckentempo wie beim Bergsturz von Plurs verbreitet hätten. Wäre die Seuche dann auch so gefährlich geworden oder gar gefährlicher, weil sie uns schlicht überrannt hätte? Oder auch harmloser, weil sie uns nach Jahr und Tag einfach wie eine besonders schlimme Grippewelle erschienen wäre?

Wir werden es wohl nie wissen, da wir dies selbst mit einer raffinierten Simulation nur unzureichend erklären könnten und da eben viele andere Faktoren, oftmals interdependent mit der Digitalisierung, beim Ausbruch der Seuche eine Rolle spielten: Globalisierung, Zusammenbruch der Distanz zwischen unseren Lebensräumen mit denjenigen der freien Natur, unsere ungehemmte Reiselust und die anschwellende Event-Sucht, die nicht zuletzt dadurch angetrieben wird, dass sich so viele Leute in der digitalen Welt einsam und rastlos fühlen.
Die grosse Frage ist: Demokratie oder Diktatur?

Am ausgeprägtesten ist die Funktion der Digitalisierung bei der Bewältigung der Krise: Rund um den Globus wurden Millionen von grösseren und kleineren Unternehmen in virtuelle Operationen übergeführt, wir kaufen alle Waren über digitale Kanäle. Die Forscher treiben ihre Programme dank künstlicher Intelligenz, Big Data und grenzenloser Kommunikation und Austausch über Zoom in einem Tempo voran, das die Wissenschaft noch nie gesehen hat.

Am interessantesten werden indes die Nachwirkungen sein. Dass die Krise der Digitalisierung einen Schub verleihen wird, ist in der Zwischenzeit ein Gemeinplatz. Wichtiger scheint mir Folgendes: Weil die Krise so aufklärerisch wirkt und wir mitten darin dank der Digitalisierung so viel mehr wissen, sehen wir jetzt auch mit geschärftem Auge, dass die Kernfrage der nächsten Zeit sein wird, ob wir eigentlich auf eine digitale Diktatur oder eine digitale Demokratie zusteuern. 1984 oder offene Gesellschaft?

Wir sehen alle, wie China und wie – anders, als wir meinen – der Westen damit umgeht. Die chinesische Regierung nutzt die digitalen Waffen gekonnt, um die Bevölkerung in der Krise und generell zu überwachen, zu domestizieren und zu bestrafen. So weit sind wir nicht, indessen sind es die Grau- und Zwischentöne in der westlichen Welt, die interessieren und Anlass zur Besorgnis geben.

In Israel ist die digitale Erfassung der Infektionsketten so weit fortgeschritten, dass Soldaten gezielt einzelne infizierte Quarantänebrecher aus ihrem Haushalt herausfischen können. Die EU hat die Digitalisierung als einen politischen Hauptpunkt auf ihre Fahne geschrieben. Ihr nahestehende Kreise arbeiten an einer Tracing-Applikation, die die Infektionsdaten zentral speichern würde. Demgegenüber soll die schweizerische Lösung, soviel ich verstehe, gerade das verhindern und mit einer Lösung arbeiten, bei der die entscheidenden Daten nur von Gerät zu Gerät ausgetauscht werden, womit eine zentrale Kenntnisnahme verhindert wird.

Dies sind konkrete, Corona-getriebene Problemstellungen. Aber es gibt natürlich viele andere wie den Umgang mit Finanz-, Reise-, Konsum- oder Lesedaten. Das haben wir schon vor der Corona-Krise gewusst, und die üblichen Antworten waren Ausbau des Datenschutzes und Zerschlagung oder zumindest Beschränkung der grossen Technologieplattformen.

Aber letzten Endes geht es offensichtlich um viel mehr, nämlich die Machtfrage: Soll der Staat eine wesentliche digitale Macht sein, oder sollen wir die Verfügung über die Daten dem Individuum beziehungsweise dem freien Markt überlassen? Soll beispielsweise die EU – wo im politischen Überbau staatsfreundliche Politiker sozial- und christlichdemokratischer Couleur das Sagen haben, während liberale Kräfte weitestgehend abwesend sind – digitale Macht entfalten und Daten auf einem zentralen Server speichern? Oder sollen wir nicht vielmehr liberale Ansätze wie die schweizerische Tracing-Applikation stützen und vor allem die Unterschiede und Nuancen zwischen digitaler Demokratie und digitaler Diktatur sichtbar machen? Das liberale Manifest kreist um Fragen wie Staatsquote, Regulierung und Freiheit der Marktteilnehmer. Es muss dringend um die digitale Machtfrage erweitert werden.
Im Zweifelsfall hilft nur abtauchen

Und nun zum letzten Aspekt, wie ich die Krise digital erlebe: Vor vielen Jahren war ich in Innsbruck, um die Hofkirche zu besuchen. Dort stehen 28 kolossale Figuren in schwarzer Bronze, mit denen Kaiser Maximilian I. seine wichtigsten Zeitgenossen verewigte: von Maria von Burgund bis zu Johanna der Wahnsinnigen. Am Eingang zur Kirche steht an einer Wand: «Gute Nacht, Welt! Ich geh ins Tirol.»

Ich hatte damals und lange danach nicht begriffen, ob das nur ein Scherz ist oder einen tieferen Sinn hat. Dank Corona weiss ich es: Es gibt Situationen, in denen man abtauchen muss.

Mein Tirol ist die digitale Welt. Ich habe alle Informationen und alle Unterhaltungsmittel zur Hand, ich kann diese mit der Möblierung meiner alten Welt, der Bibliothek, der Musiksammlung und den Sport-Utensilien verbinden und mir eine schöne neue Welt des Eskapismus bauen. Ich weiss, ich kann mir diesen Kokon nur leisten, weil ich alt und privilegiert bin. Aber bitte gönnt ihn mir, und deshalb: Gute Nacht Welt, ich bin im digitalen Elysium. (Und ihr jüngeren Mitbürger, seid stets wachsam und auf der Hut, so dass ihr dies in euren alten Tagen ebenfalls wünschen könnt.)";https://www.nzz.ch/feuilleton/coronavirus-diese-pandemie-haben-wir-erstmals-in-echtzeit-erlebt-ld.1558596;NZZ;Peter Kurer;;;
14.03.2019;Die «Schummelalgorithmen» hinter der künstlichen Intelligenz;"Deutsche Parlamentarier liessen sich am Montag in Berlin über «Schummelalgorithmen» informieren. Klaus-Robert Müller, Computerwissenschafter an der Technischen Universität Berlin, plädierte vor der Enquête-Kommission Künstliche Intelligenz des Deutschen Bundestags für ein verstärktes Engagement bei der Erforschung der Erklärbarkeit von künstlicher Intelligenz. Er ist mit dieser Forderung nicht allein, rund um die Welt beschäftigt «explainable AI» (XAI) Dutzende von hochkarätigen Wissenschaftern. Unter anderen hat die Forschungsagentur der amerikanischen Streitkräfte, die Defense Advanced Research Projects Agency (Darpa), XAI zu einem Schwerpunktthema ihrer Forschungsförderung erklärt.
Geniale Lösungen

Die künstlich intelligenten Maschinen sind wie Tiere, die rechnen können. Was geht in ihrem Kopf vor? Es gibt wissenschaftliche Berichte über eine Schimpansin, die nicht nur rechnen, sondern Zahlen auf Papier auch lesen konnte. Von einem Papagei wird berichtet, der die Resultate seiner Berechnungen aussprechen konnte. Und dann gab es da auch Hans, das Pferd, das die Resultate von Rechenaufgaben durch Klopfen mit den Hufen kommunizierte. Der Kluge Hans sorgte einst mit seinen Rechenkünsten für Schlagzeilen, bis man herausfand, dass er mit Zahlen gar nichts anfangen konnte. Wie oft es klopfen musste, entnahm das Pferd der Mimik und der Körperhaltung des Menschen, der die Rechenaufgaben stellte. Hans ist im Ersten Weltkrieg ums Leben gekommen, seine Betreuer und ihre eigenartigen tierpädagogischen Konzepte sind vergessen. Was aber blieb, ist der Kluger-Hans-Effekt: Er beschreibt bei Experimenten oder Befragungen schwer feststellbare, störende Einwirkungen.

Es gibt den Kluger-Hans-Effekt auch in der Computerwissenschaft. Müller verwendet ihn im Titel eines wissenschaftlichen Aufsatzes, den er zusammen mit Kollegen der TU Berlin, des Fraunhofer-Heinrich-Hertz-Instituts, der Korea University und der Singapore University of Technology and Design diese Woche bei «Nature Communications» publiziert hat. Die Forscher haben die Fähigkeiten von KI-Systemen überprüft, sie wollten herausfinden, ob hinter den guten Leistungen intelligente Entscheidungen stehen oder nur statistisch erfolgreiche Zufälligkeiten.
Flippern ohne Flipperhebel

Machine-Learning-Systeme werden oft als «Blackboxes» charakterisiert. In jüngster Vergangenheit wurden nun aber im Rahmen der XAI-Forschung Verfahren entwickelt, um bei diesen Kisten den Deckel zu heben und das Innere auszukundschaften. Müller und seine Kollegen haben ein bereits bekanntes Verfahren (Layer-wise Relevance Propagation, LRP) erweitert, um Systeme zu beobachten, bei denen visuelle Informationen wichtig sind. Die Ergebnisse sind vernichtend. Bei seinem Vortrag vor der Enquête-Kommission sagte Müller, dass die Hälfte der untersuchten Systeme «Schummelalgorithmen» verwende.

Da ist zum Beispiel diese Software, die mit hoher Zuverlässigkeit auf Fotos Pferde erkennen kann. Eine erstaunliche Leistung! Während der Trainingsphase wurden dem System Bilder mit Pferden gezeigt, bald kannte es sich mit Pferden aus und war in der Lage, Pferde auch auf Bildern, die es noch nie zu Gesicht bekommen hatte, zu erkennen. Müller und seine Kollegen konnten mit ihren Methoden nun aber zeigen, dass die Software mit Pferden nichts am Hut hat. Es war einfach so, dass die Pferdefotos alle aus demselben Archiv stammen und an derselben Stelle mit demselben Copyright-Vermerk gekennzeichnet sind. Die Software erkannte nicht Pferde, sondern orientierte sich an den Copyright-Hinweisen.

Oder da ist diese Software, die sich anhand eines Computerspiels von Atari das Flippern beigebracht hat. Die Software analysiert die Vorgänge auf dem Bildschirm und versucht dann das Game so zu steuern, dass es eine möglichst hohe Punktzahl erreicht. Sehr schnell war die Software am virtuellen Flipperkasten überraschend gut. Doch Müller et al. fanden heraus, dass die Software gar nicht flippert. Die Software hatte durch Versuch und Irrtum herausgefunden, dass es möglich ist, die Spielfläche zu kippen, so dass der Ball zwischen zwei Schlagtürmen stecken bleibt und den Punktezähler in die Höhe treibt. Im wirklichen Leben würde diese Spielstrategie einen «Tilt» und den Abbruch des Spiels provozieren. Das Atari Pinball Game war aber offenbar nicht auf einen Spieler vorbereitet, der flippert, ohne die Flipperhebel zu bedienen.
Gefährliche Unzuverlässigkeit

Die künstlich intelligente Software ist wie ein Idiot savant manchmal zu Spitzenleistungen fähig, manchmal abgrundtief dumm. Doch wenn KI bald auch den Arzt, den Banker oder den Chauffeur ersetzen soll, möchte man schon gern wissen, woran man ist. Sie sollte auch in der Lage sein, einen Kreditentscheid, eine Krebsdiagnose oder eine Notbremsung zu begründen. «Schummelalgorithmen» kommen zwar vielleicht zu einem richtigen Resultat, aber sie tun es auf krummen Wegen. Deswegen sind sie nicht vertrauenswürdig. Wo solche Verfahren beispielsweise bei medizinischen Diagnosen eingesetzt werden, bilden sie eine Gefahr: «Ich möchte so nicht behandelt werden», sagte Müller vor den Parlamentariern.";https://www.nzz.ch/digital/wenn-der-computer-mit-den-hufen-klopft-ld.1467140;NZZ;Stefan Betschon;;;
30.10.2019;Ameisen arbeiten nach Algorithmen – und welcher Code prägt die Menschen? ;"Supercomputer spielen besser Schach, Go und Poker als der Mensch und sind ihm mittlerweile auch in der Krebsdiagnose überlegen. Längst wird künstliche Intelligenz in Systemen eingesetzt, in denen sie selbständig Entscheidungen trifft, etwa im Hochfrequenzhandel. Das stellt grundlegende Annahmen zum Verhalten von Akteuren auf den Prüfstand: Wie verhalten sich intelligente Maschinen in bestimmten Situationen? Handeln sie wirklich rational? Weichen sie vom programmierten Skript ab und lernen, situativ auf bestimmte äussere Einflüsse zu reagieren?

Unter dem Stichwort «machine behavior» hat sich in der KI-Forschung eine eigene Subdisziplin entwickelt. Iyad Rahwan, Direktor am Max-Planck-Institut für menschliche Entwicklung in Berlin, untersucht das Verhalten von intelligenten Maschinen – ja, das Verhalten. Sein Ansatz: Weil algorithmische Systeme so komplex sind, muss man beobachten, was sie in der Wildnis tun – als handelte es sich um Tiere. Die Idee klingt verwegen, doch ist es erkenntnistheoretisch erstaunlich logisch, das künstliche Habitat wie ein Ökosystem in der Natur zu betrachten. Man beobachtet Muster und leitet daraus Regelmässigkeiten und Gesetze ab. Wie ein Verhaltensforscher. Nur beobachtet man nicht Wölfe oder Elefanten, sondern Computer.

Laut Rahwan hat auch eine Waschmaschine ein Verhalten – und nicht, wie Systemtheoretiker argumentieren würden, eine blosse Funktion, nämlich zu waschen. Analog sind Denkmaschinen nicht nur auf bestimmte nützliche Funktionen programmiert, sondern auch auf regelbasierte Verhaltensmuster, die sich in Abhängigkeit der Umwelt (Reize, Signale) ändern.

«Vielleicht gibt es Lehren aus der Natur, die es uns erlauben, nicht nur Lösungen zu entwickeln, sondern Probleme frühzeitig zu erkennen», sagte Rahwan in einem Interview mit dem «Quanta Magazine». «Was ist das Äquivalent zum Zusammenbruch einer Kolonie? Gibt es ein Äquivalent zur Artenbildung?» Der Informatiker argumentiert, dass man eines Tages Maschinen wie Pflanzen oder Tiere in bestimmte Gattungen und Spezies einordnen könne.
«Anternet» statt Internet

Die Natur als Konzept für Maschinenintelligenzen zu nehmen, ist kein neuer Gedanke. Der Mathematiker Nils Barricelli versuchte bereits in den 1950er Jahren, evolutionäre Prozesse am Computer zu simulieren. Es gibt schon länger eine Reihe von bioinspirierten Algorithmen (etwa in der Schwarmrobotik), die aus der Beobachtung der Tierwelt, etwa von Vogel- und Fischschwärmen, entwickelt wurden. Und doch fördert die Symbiose verschiedener Disziplinen zuweilen erstaunliche Erkenntnisse zutage.

Die Stanford-Biologin Deborah Gordon hat bei Ameisenkolonien einen Algorithmus identifiziert, der erklärt, wie die Insekten Netzwerke weben und reparieren. Ameisen bilden zwar Staaten, ihre Kolonien sind aber hierarchielose Organisationen, das heisst, Ameisen erteilen untereinander keine Befehle. Da die Tiere fast nichts sehen, folgen sie bei der Futtersuche einem chemischen Botenstoff zur Informationsübertragung, den der Vorgänger gewissermassen als solidarische Duftmarke hinterlässt. Auf diese Weise entsteht eine Art analoge Netzwerkgesellschaft. Wenn der Pfad unterbrochen wird, steuern die Ameisen automatisch zum vorherigen Knotenpunkt und stellen das Netzwerk wieder her. Sie folgen also einem ihnen eingeschriebenen Navigationsprogramm.

Das Verblüffende ist nicht nur, dass Ameisenkolonien schon seit Jahrmillionen Algorithmen operationalisieren und eines der ersten Computersysteme der Evolution waren. Gordon und ihr Kollege, der Computerwissenschafter Balaji Prabhakar, fanden heraus, dass dieser Algorithmus Ähnlichkeiten mit dem Internetprotokoll TCP (Transmission Control Protocol) aufweist. So wie es beim Internetprotokoll ein Time-out gibt, wenn ein Link nicht funktioniert, werden in einer Ameisenkolonie keine weiteren Arbeiterinnen mehr ausgeschickt, wenn der erste Futtertrupp nach 20 Minuten nicht zurückkehrt.

Die Forscher sprechen vom «Anternet», dem Ameisen-Netz. Ameisenkolonien sind informationsverarbeitende Systeme, wenngleich die Datenmenge im Vergleich zu Grossrechnern recht beschaulich ist. Doch es gibt Parallelen: Weder der Router in einem Datennetzwerk noch eine Ameise, die jeweils kleinsten Einheiten der Ordnungen, wissen, was der jeweils andere tut. Und trotzdem funktionieren diese kollektiven Systeme. Sogar erstaunlich gut: In Ameisenkolonien gibt es zum Beispiel keinen Stau. Und auch keine Aufstände oder Kriege. Alles läuft ohne Störungen in vorgegebenen Bahnen.
Ein funktionierendes Europa

Die Erkenntnis könnte nicht nur etwas zum Verständnis von Maschinenverhalten, sondern auch zum Funktionieren moderner Gesellschaften beitragen. Und zwar nicht nur in normativer Hinsicht, bezogen auf die Frage, ob eine dezentrale, hierarchielose und faktisch herrschaftsfreie Staatsorganisation die bessere Gesellschaft wäre. Sondern auch in Bezug auf die Frage, wie die Gesellschaft sich selbst sieht. Sind Gesellschaften nicht auch ein Netzwerk? Und sind dann Phänomene wie Stau oder Streiks, kybernetisch gedacht, Störungen im Betriebsablauf?

Die Digitalisierung ist ja, nach dem Soziologen Armin Nassehi, auch ein Programm, das bestimmte Muster in der Gesellschaft sichtbar macht, die dort schon in analoger Zeit eingeschrieben waren. So werden wir in unserer Formelhaftigkeit entlarvt. Vielleicht gibt es, und das kann man frei von jedem sozialdeterministischen Anflug sagen, einen Algorithmus in der Gesellschaft, den wir gar nicht sehen, aber nach dem wir alle «funktionieren». Einen Code, den wir trotz aller Selbstbeschreibung und normativen Postulaten noch gar nicht entschlüsselt haben.

Was ist es für eine Software, die uns jeden Morgen aufstehen und mehr oder weniger zivilisiert zur Arbeit fahren lässt? Und wenn es die gibt – können wir sie wieder deinstallieren? Welches Programm hält die Gesellschaft zusammen? Ist mit Ameisen ein besserer Staat zu machen, weil sich jeder an die Regeln hält? Sind Ameisen dem Menschen nicht meilenweit voraus, weil sie von der italienischen Riviera bis in den Nordwesten Spaniens einen knapp 6000 Kilometer langen Superstaat gebaut haben, also ein Europa, das funktioniert? Und wenn die Insekten die besseren politischen Baumeister sind – gilt das nicht erst recht für KI?
Demokratie ist lernbar

Man kann diese Überlegungen zu folgendem Gedankenexperiment verdichten: Angenommen, man lässt eine künstliche Intelligenz mit einem basalen Set an Regeln und Verhaltensweisen auf die Menschheit los, in die «Wildnis» der Zivilisation. Würde das System bestimmte Normen und Werte wie Freiheit und Gleichheit auf dem Wege des Machine-Learning erlernen? Oder würde es in seiner operanten Blindheit nur die schlechten Verhaltensweisen des Menschen adaptieren, die Gesellschaft in ihrer Illiberalität gewissermassen entlarven (was ja auch eine Funktion sein könnte)? Wenn ein maschinell lernender Algorithmus nach dem millionsten Foto den Unterschied zwischen einer Katze und einem Hund erkennt, könnte er dann auch zwischen Freiheit und Unfreiheit differenzieren (und vielleicht sogar für eine Handlungsalternative optieren)? Kurz: Kann eine KI demokratisches Handeln lernen?

Normen sind etwas Regel- oder Musterhaftes, und Computer sind gut darin, regelmässig wiederkehrende Muster zu erkennen. Ein Gesetz folgt einem deterministischen Bauprinzip, weshalb sich juristische Codes auch in Maschinencodes formalisieren lassen. Hat Politik in ihrer Erwartbarkeit nicht auch etwas Mechanistisches, das repetitive Abspulen von Programmen sogar etwas Simulatorisches?

Wer KI-Systemen die Demokratiefähigkeit abspricht, verkennt den Umstand, dass auch der Mensch demokratische und gesellschaftliche Spielregeln erst in Sozialisationsagenturen wie der Familie oder der Schule lernen muss. Niemand wird als Demokrat oder Autokrat geboren. Und genauso ist jede Technik zunächst einmal wertneutral. Womöglich könnte man einen maschinell lernenden Algorithmus viel schneller zum Demokraten erziehen als den Menschen, worin ja auch eine Utopie liegen könnte.

Die Frage ist aber nicht, ob sich Maschinen irgendwann selbst demokratische Werte beibringen, sondern ob noch ein Gemeinwesen zu machen ist mit Bürgern, die immer mehr Entscheidungen an autonome Systeme delegieren. Der für die Souveränität einer Gesellschaft entscheidende Ansatz kann daher nicht darin bestehen, die wild gewordenen Algorithmen zu domestizieren, sondern die Hyperreguliertheit des Alltags einzuhegen, die mit der Installation solcher Systeme einhergeht. Ein Computer ist eine Normbefolgungsmaschine – er verstösst nicht gegen Programmierbefehle. Der Verstoss gegen Normen und Regeln, die strukturelle Offenheit, ist aber genau das Wesensmerkmal einer demokratischen Gesellschaft.";https://www.nzz.ch/feuilleton/gibt-es-einen-algorithmus-nach-dem-gesellschaften-funktionieren-ld.1518313;NZZ;Adrian Lobe;;;
06.07.2017;Den Tiefpunkt überwinden – ohne erneut zu fallen;"Im Alter von 47 Jahren begann Marion Suppiger eine neue Stelle im Pflegebereich. Sie war unsicher und fühlte sich unzulänglich. Schon seit mehreren Jahren hatte sie immer wieder depressive Verstimmungen gehabt. Die Unsicherheit im Beruf brachte das Fass zum Überlaufen. Ein paar Monate nach dem Neuanfang kam der Zusammenbruch: Erschöpfungsdepression (Burnout) lautete die Diagnose. Ein fünfwöchiger Aufenthalt in einer Rehaklinik, Psychotherapie und Antidepressiva halfen ihr, die Krise zu überwinden. Drei Monate später begann sie wieder zu arbeiten, anfangs reduziert, dann immer mehr. Es ging bergauf.

Bald fühlte sie sich wieder stabil, und nach zwei Jahren versuchte sie erstmals, die Medikamente abzusetzen. Doch das gelang ihr nicht. «Ich habe mehrmals probiert, die Dosis langsam zu reduzieren. Aber es ging nicht gut. Ich bin ständig in Tränen ausgebrochen, war zickig und niedergeschlagen», sagt sie.
Es bleibt nur Ausprobieren

Absetzsymptome, wie Suppiger sie erlebte, sind häufig. Viele Personen fallen nach dem Absetzen wieder in eine Depression. Von einem Rückfall spricht man, wenn eine Person innerhalb von sechs Monaten nach einer erfolgreich abgeschlossenen Behandlung erneut depressiv wird. Treten die Symptome später wieder auf, wird es als eine neue Episode bezeichnet. Nach mehreren depressiven Episoden steigt das Risiko für einen Rückfall. Weil Antidepressiva davor schützen können, wird den Betroffenen empfohlen, die Medikamente langfristig einzunehmen, manchmal sogar ein Leben lang.

Wer wirklich von einer langfristigen Einnahme profitiert, ist allerdings unklar. Wahrscheinlich seien es etwa ein Drittel der erfolgreich behandelten Personen, sagt Quentin Huys von der Translational Neuromodeling Unit der Universität und der ETH Zürich. Ein anderes Drittel habe auch nach dem Absetzen keinen Rückfall und ein weiteres Drittel werde trotz Medikamenten rückfällig, schätzt der Psychiater.

Das bedeutet, dass etwa zwei Drittel dieser Personen nicht von einer langfristigen Medikation profitieren. Man nehme an, dass mehrere Episoden und mehr verbleibende Symptome bei Abschluss der Behandlung das Risiko erhöhen würden, sagt Huys. Doch reichen diese Kriterien nicht für eine zuverlässige Prognose aus.

Um herauszufinden, ob man die Medikamente noch braucht, bleibt also nur das Ausprobieren. Ein Rückfall ist für die Betroffenen und ihre Bezugspersonen aber eine schmerzliche Erfahrung. Deshalb suchen Forscher unter der Leitung von Huys in Zusammenarbeit mit Wissenschaftern von der Berliner Charité nach messbaren Markern, anhand deren man vorhersagen kann, ob ein sicheres Absetzen möglich ist.

Sie konzentrieren sich in der sogenannten Absetzstudie auf eine breite Palette möglicher Kriterien: psychische Eigenschaften wie etwa bestimmte Denkmuster, die Hirnaktivität der Probanden und genetische Merkmale, aber auch Auffälligkeiten im Immunsystem. Denn bei Depressionen scheinen Botenstoffe des Immunsystems eine Rolle zu spielen. So sind einzelne Entzündungsstoffe oft erhöht.
Ein schlimmer Verdacht

Eine weitere Frage, die die Forscher umtreibt, ist, ob allein das Absetzen der Antidepressiva einen Rückfall auslösen kann. Einige Studien, in denen die Patienten mit älteren und stärkeren Antidepressiva behandelt worden seien, wiesen darauf hin, sagt Huys. Eine mögliche Erklärung dafür wäre, dass beim Absetzen der Medikamente ein Ungleichgewicht im Hirnstoffwechsel entsteht. Denn Antidepressiva verändern die Verfügbarkeit der Hirnbotenstoffe (Neurotransmitter), welche die Emotionen und das Denken beeinflussen, beispielsweise Serotonin, Dopamin oder Noradrenalin.

Während einer Depression helfen die Medikamente, ein krankhaftes Ungleichgewicht zu normalisieren. Das Absetzen bringt das Gleichgewicht aber erneut durcheinander. Ärzte empfehlen daher, die Dosis der Medikamente langsam über mehrere Wochen zu reduzieren, damit sich der Stoffwechsel regulieren kann. Womöglich gelingt manchen Patienten diese Regulation nicht, und sie erleiden deshalb einen Rückfall.

Dieser Verdacht beruhe allerdings auf älteren, methodisch nicht ganz einwandfreien Studien, relativiert Huys. Und vielleicht trete das Problem auch nur in Zusammenhang mit den alten, stärkeren Antidepressiva auf. Falls der Effekt aber auch bei neueren Medikamenten zum Tragen käme, wäre dies verheerend. Denn heute nehmen immer mehr Menschen Antidepressiva ein, auch wenn sie nicht unbedingt davon profitieren – die Wirksamkeit der Medikamente ist nur bei mittelschweren bis starken Depressionen erwiesen. Dennoch verschreiben laut Umfragen viele Hausärzte auch bei leichten Depressionen häufig Antidepressiva.

Sollte diese Praxis die Patienten anfälliger für einen Rückfall machen, wäre dies fatal, sagt Huys. Bis anhin seien die Hinweise dafür wenig stichhaltig, dennoch wollen die Forscher den Verdacht überprüfen.
Mittels «machine learning» Rückfälle vorhersagen

Sie untersuchen 80 Freiwillige, die wie Frau Suppiger ihre Medikamente absetzen wollen, und begleiten sie über sechs bis neun Monate. Vor und nach der schrittweisen Reduktion der Medikamente messen sie die Hirnaktivität der Probanden mittels funktioneller Magnetresonanztomografie (fMRI) und EEG. Sie testen ausserdem das Verhalten und typische Denkmuster der Probanden, etwa wie stark sie zum Grübeln neigen oder sich auf Negatives konzentrieren. Mit mathematischen Modellen wollen sie die neurobiologischen Daten mit dem Verhalten in Verbindung bringen. Darauf basierend versuchen sie dann mit Methoden des «machine learning» Rückfälle vorherzusagen.

Bei den im Blut gemessenen Parametern verfolgen die Forscher zwei Ansätze. Eine ihrer Hypothesen lautet, dass verbleibende depressive Symptome nach einer erfolgreichen Behandlung das Risiko für einen Rückfall erhöhen. Womöglich findet man biologische «Restsymptome» im Immunsystem oder in der sogenannten Epigenetik, das sind Veränderungen auf der Ebene der Gene, die deren Aktivität beeinflussen. Laut der Hypothese sollten Personen, die vor dem Absetzen der Medikamente mehr biologische Restsymptome aufweisen, anfälliger für einen Rückfall sein. Im zweiten Ansatz wollen die Forscher überprüfen, ob das Absetzen der Medikamente zu Veränderungen bei den im Blut gemessenen Parametern führt, die womöglich einen Rückfall auslösen.
Intensive Weiterbehandlung ist nötig

Für die Betroffenen wäre es extrem hilfreich, wenn man ihnen eine individualisierte Prognose über den Verlauf ihrer Krankheit geben könnte, sagt Birgit Watzke vom Psychologischen Institut der Universität Zürich, die nicht an der Studie beteiligt ist. Deshalb sei es wichtig, auf allen Ebenen nach solchen Markern zu suchen, in der Hirnaktivität, im Blut und in der Psyche. Es sei gut, dass die Rückfallprophylaxe bei Depressionen derzeit vermehrt in den Fokus der Forschung rücke. Denn es sei nun einmal Fakt, dass die Krankheit bei 30 bis 50 Prozent der Betroffenen wiederkehre oder lang anhalte.

Für die Vorbeugung von Rückfällen gebe es im psychotherapeutischen Bereich vielversprechende Ansätze, sagt Watzke. Diese erforderten aber eine relativ intensive Weiterbehandlung, was nicht bei allen Patienten gut ankomme. Sie organisiert deshalb eine Studie, die untersucht, ob eine niedrigschwellige Rückfallprophylaxe in Form von verhaltenstherapeutischen Telefongesprächen nach einer abgeschlossenen Akuttherapie vor einem Rückfall schützen kann. «Wir sind gespannt, ob wir mit der bald anlaufenden ‹NaTel-Studie› Menschen mit Depression ein wirksames und praktikables Angebot machen können», sagt die Psychologin.

Die ehemalige Burnout-Patientin Suppiger führt auch heute noch alle paar Wochen ein Gespräch mit ihrer Psychologin. Im Rahmen der Absetzstudie konnte sie die Medikation erfolgreich beenden und erlebte keinen Rückfall. Heute gehe es ihr so gut wie noch nie, sagt sie. Durch die Psychotherapie könne sie mit schwierigen Situationen mittlerweile viel besser umgehen.";https://www.nzz.ch/wissenschaft/depressionen-den-tiefpunkt-ueberwinden-ohne-erneut-zu-fallen-ld.1304678;NZZ;Lena Stallmach;;;
21.07.2018;Herzschlag im Auge des Algorithmus;"Kameras sind heute allgegenwärtig: Laptops haben eine eingebaute Webcam; Smartphones tragen auf Vorder- und Rückseite kleine Linsensysteme, mit denen sich Videos und Fotos aufnehmen lassen. Die Videoüberwachung an Bahnhöfen und belebten Plätzen wird weiter ausgebaut. Und im Takt der Digitalisierung wird es bald noch viel mehr Videoaufnahmen geben. Doch schon Videos einfacher Qualität können weit mehr verraten, als man leichthin vermuten mag. So lassen sich mit Webcams aus den Gesichtern gefilmter Personen deren Herzrhythmus und Atemfrequenz bestimmen – berührungslos und ganz ohne zusätzliche Strahlungsquellen oder Optiken. Das haben Forscher bereits 2011 gezeigt.1

Im Alltag liessen sich damit die Vitalfunktionen von Babys oder pflegebedürftigen Angehörigen überwachen, mit einem zusätzlichen Infrarotsensor sogar im dunklen Schlafzimmer. Zeigt sich ein auffälliges Signal, könnte das «Smart Home» seinen Bewohner per Nachricht aufs Smartphone warnen – und gleich eine Liste mit den Kontaktdaten der online am besten bewerteten Kardiologen der Gegend mitliefern.
Winzige Bewegungen analysiert

Bisher mussten die Videos allerdings aufwendig verarbeitet werden, damit zuverlässig auf die Vitalfunktionen geschlossen werden konnte. Bewegungen der beobachteten Probanden störten die Messungen genauso wie wechselnde Lichtverhältnisse. Diese Probleme haben Daniel McDuff von Microsoft Research und Weixuan Vincent Chen vom Massachusetts Institute of Technology nun gelöst. Ihnen ist es gelungen, eine künstliche Intelligenz namens «DeepPhys» so zu trainieren, dass sie direkt aus dem Videosignal die gesuchten Messwerte ableitet. 2 In Tests war sie dem bisherigen Stand der Technik überlegen und lieferte einen mittleren Fehler von nur wenigen Herzschlägen und Atemzügen pro Minute.

Mit dem Rhythmus des Herzens ändert sich das Farbspektrum des Lichts, das von der Hautoberfläche reflektiert wird. Verantwortlich dafür ist der Farbstoff der roten Blutkörperchen, Hämoglobin. Dieser absorbiert vor allem grünes Licht, in geringerem Mass aber auch andere Farben. Ändert sich über den Zyklus eines Herzschlags die Menge an Blut, die unter der Hautoberfläche strömt, so variiert damit auch die farbliche Zusammensetzung des von ihr reflektierten Lichts. Aus dem zeitlichen Verlauf dieses Farbspektrums lässt sich der Herzrhythmus bestimmen. Ausserdem erschüttert jeder Herzschlag den Körper minimal. Dieser Rhythmus lässt sich ebenfalls aus Videobildern rekonstruieren. Beim Atmen hebt und senkt sich zudem der Brustkorb, wodurch sich auch Kopf und Schultern bewegen. Anhand dieser Parameter lässt sich aus dem Videosignal die Atemfrequenz ableiten.

Frühere Ansätze erreichten dies, indem sie den Farbwert jedes einzelnen Pixels des Kamerabildes betrachteten und dessen zeitliche Veränderungen beobachteten. Die Auswertealgorithmen basierten also auf menschlichem Wissen, das die Entwickler in programmiererischer Handarbeit vorgeben mussten.

Ganz anders gingen nun Chen und McDuff vor. Sie präsentierten einem künstlichen neuronalen Netz verschiedene Sätze vorher aufgezeichneter Trainingsdaten. Diese enthielten das Videosignal des Gesichts eines Probanden zusammen mit der zugehörigen Herz- und der Atemfrequenz. Die Vitalwerte waren zu diesem Zweck parallel zur Videoaufnahme mit etablierten Messgeräten wie etwa einer Fingersonde aufgezeichnet worden. «DeepPhys» lernte, aus den in den Videoaufnahmen steckenden Signalen (etwa die sich ändernde Lichtreflexion der Haut) genau die Werte für Herz- und Atemfrequenz abzuleiten, die die Fingermessung erfasst hatte. Dieses Trainingsprinzip heisst «Deep Learning».

Dann wendeten die Forscher ihr System auf bisher unbekanntes Videomaterial an. Sie prüften, wie sich «DeepPhys» bei Videos mit grösseren Kopfbewegungen und wechselndem Licht schlug. Je herausfordernder das Videomaterial, desto überlegener war «DeepPhys» den bis anhin führenden Methoden. Auch aus Infrarotaufnahmen konnten die Forscher die Vitalwerte bestimmen. Die Leistung des Algorithmus variiere zwar mit Hautton, Gesichtsbehaarung und Make-up, sei aber «in der Regel gut», schreiben Chen und McDuff.
Dystopische Szenarien sind denkbar

Florian Gallwitz von der Technischen Hochschule Nürnberg findet die Arbeit hochinteressant. Man sehe darin den Trend, dass «ingenieursmässig konstruierte, komplexe Verarbeitungspipelines» durch Ende-zu-Ende-Verarbeitung mit künstlichen neuronalen Netzen ersetzt werden: gemessenes Signal rein, fertiges Ergebnis raus. Die Anwendung zur Herz- und Atemanalyse aus Videos sei reizvoll, aber auch ein wenig unheimlich, so Gallwitz. Die Maschine erlerne eine Fähigkeit, die der Mensch noch nicht einmal im Ansatz selbst beherrsche.

In der Tat sind in der Zukunft dystopische Szenarien denkbar. So könnten Krankenkassen ähnliche Systeme nutzen, um zu testen, ob es Hinweise auf eine Vorerkrankung bei einem potenziellen Kunden gibt. Dazu könnte auch Videomaterial dienen, das sich im Internet finden lässt, auf Youtube oder Facebook zum Beispiel.

Daniel McDuff erklärt, man erforsche deshalb auch, wie man diesen Befürchtungen richtig begegne. Technologien könnten immer auch für schlechte Zwecke benutzt und niemals vollständig kontrolliert werden. Es sei wichtig, sich Gedanken zu machen, wie eine Regulierung aussehen könnte. Vorerst handele es sich bei «DeepPhys» jedoch nur um ein Forschungsprojekt. Auf dem Weg zu einem marktfähigen Produkt seien noch einige Aufgaben zu lösen.";https://www.nzz.ch/wissenschaft/herzschlag-im-auge-des-algorithmus-ld.1401397;NZZ;Philipp Hummel;;;
19.09.2019;Wenn ein Algorithmus die ganze Literaturgeschichte revolutioniert: Im Maschinozän schreiben Maschinen bessere Texte als Menschen;"Jeden Morgen, so überliefern es zahlreiche Geschichten, stand sie auf im Morgengrauen. Sie brühte sich eine Tasse Kaffee, um damit dem Licht beim Aufstehen zuzuschauen. Dann begann das Schreiben. Ein Tageswerk und ein Lebenswerk.

Für die kürzlich verstorbene Autorin Toni Morrison war Schreiben Leben. Mit ihren Worten hat sie einen Teil der Geschichte der USA, der Rassendiskriminierung, der sozialen Ungerechtigkeiten und menschlichen Abgründe geschrieben.

Das Leben einer jeden Literatin und eines jeden Literaten endet. Übrig bleibt der Teil des Narrativs über die Menschengeschichte, der entstanden ist. Neues kommt aus dieser Feder nicht mehr hinzu. «We die. That may be the meaning of life», sagte Toni Morrison in ihrer Rede anlässlich der Verleihung des Literaturnobelpreises 1993. «But we do language. That may be the measure of our lives.»
Neue kreative Kraft

Morrison hat den Umgang mit Sprache als exklusiven Zugang des Menschen zu seinem eigenen historischen Narrativ begriffen. Mit Erzählungen schreibt die Menschheit sich überhaupt erst in ihre Existenz hinein. Damit hatte sie lange recht. Mit intelligenten Maschinen könnte das anders werden. Ein Algorithmus, der die Literaturgeschichte revolutioniert? Bis anhin scheint das ein Szenario aus der Science-Fiction. Aber nicht mehr lange. In der Kurzgeschichte «The Great Automatic Grammatizator» beschreibt Roald Dahl schon 1948, wie Maschinen Schritt für Schritt auch kreative Tätigkeiten übernehmen. Fritz Leiber macht die menschlichen Schriftsteller in seinem Roman «The Silver Eggheads» zu Aufsichtsräten einer automatisierten Schreibfabrik. Sie werden nur noch gebraucht, um ihre Roboterkollegen zu überwachen. Und im neuesten Werk Ian McEwans, «Maschinen wie ich», produziert der humanoide Roboter Adam am laufenden Band Haikus, 2000 Stück insgesamt, für die allzu menschliche Miranda, in die er sich unsterblich (sic!) verliebt hat.

Ganz real macht die künstliche Intelligenz (KI) rasante Fortschritte im Umgang mit Sprache. Mit Machine Learning erobert KI die Sprache und macht der menschlichen Kreativität Konkurrenz.

Software wird nicht müde, leidet nicht unter Schreibblockaden und ist nie kreativ ausgebrannt. Wenn die Maschinen einmal begonnen haben, Geschichte zu schreiben, werden sie nicht wieder aufhören. Auch sie gehen dann mit Sprache um, aber sie sterben nicht, sondern setzen an zu einer unendlichen Erzählung des Mensch-Maschinozäns.
Die journalistische Ursünde

Alles begann im Journalismus. Seit Jahren sind hier Programme im Einsatz, die Standardtexte produzieren können, vornehmlich in der Sport- und Finanzberichterstattung, in der Zahlen, Fakten, Ergebnisse im Vordergrund stehen. Bloomberg News verwendete eine Software namens Cyborg. Die spuckt jedes Quartal mehr als tausend Berichte zur Marktentwicklung von Unternehmen aus. Etwa ein Drittel der gesamten Nachrichtenberichterstattung soll inzwischen von Software stammen.

Das Wirtschaftsmagazin «Forbes» hat schon frühzeitig mit einer Software von Narrative Science gearbeitet, einem der ersten Anbieter automatisierter Textherstellung, und testet jetzt ein Eigenprodukt namens Bertie, das Textentwürfe und -vorlagen produziert. Die US-Nachrichtenagentur Associated Press nutzt das Programm Wordsmith von Automated Insights, um mehr als 3000 Finanzberichte pro Jahr schreiben zu lassen.

Algorithmisch produzierte Texte sind sehr genau im Umgang mit Zahlen und Texten, machen nahezu keine Rechtschreibfehler. Sie können vorher Statistiken und Tabellen vergleichen, als Hand- und Kopfarbeit meist wenig inspirierender Alltag der schreibenden Zunft.

Im besten Falle befreit die KI also ihr menschliches Pendant von den mühsamen, zeitraubenden Standardaufgaben, mit denen kein Preis zu gewinnen und kein Gefühl von Sinnhaftigkeit im eigenen Job zu erringen ist. Journalistinnen und Journalisten können die frei werdende Zeit nutzen, um zu recherchieren, wieder mehr an die Orte des Geschehens zu gehen und sich dem Reality-Check auszusetzen, der ja in letzter Zeit gelegentlich an der Grenze zwischen Fakt und Fiktion etwas vernachlässigt wurde.

Algorithmen im Journalismus, das mag noch angehen, aber Software, die Literatur produziert? Stuart Frankel, Gründer und CEO von Narrative Science, sieht es so: «Wenn ein Algorithmus in der Lage ist, aus Daten eine Geschichte zu schreiben, wird das irgendwann zwangsläufig passieren.»

Die ersten Beispiele gibt es bereits. Philipp M. Parker, Insead-Professor, hat eine Software für automatisierte algorithmische Bücher entwickelt. Aus im Internet vorhandenen Daten kreiert sie elektronische Bücher, die über Amazon zu beziehen sind. Mehr als 100 000 Bücher von ihm sind dort erhältlich, darunter viele Arten von Speziallexika oder Trendanalysen wie «Die 2020–2025-Vorhersage für zubereitete frische Austern». Wer Austern durch «Feuerwehrfahrzeuge», «Bewegungsmelder» oder «Kabelverschraubungen» ersetzt, versteht das Prinzip.
Maschinelle Fake-News

Sicher, solche Werke sind noch keine Literatur. Diese klare Einordnung fällt einem bei der Leistung der neuen KI von OpenAI deutlich schwerer. GPT-2 lässt sich mit beliebigen Anfängen für journalistische oder literarische Texte füttern und macht daraus dann eine ganz eigene, neue Erzählung.

Was GPT-2 zum ersten Satz von Jane Austens «Stolz und Vorurteil» einfällt, ist atemberaubend. Eine völlig andere Geschichte entsteht da, aber eine, die überzeugen kann. Auch bei der potenziellen Leistungsfähigkeit literarischer Algorithmen schneidet GPT-2 im «Winograd-Test» deutlich besser ab als alle bisherigen Programme. Der Test prüft die Fähigkeit im Umgang mit mehrdeutigen Aussagen.

«Das Buch passt nicht ins Regal, weil es zu gross ist», bei diesem Satz weiss ein Mensch sofort, was zu gross ist. Software weiss das nicht. Sie versteht ja nichts, sondern berechnet Texte auf Grundlage von Daten in mathematischer Genauigkeit. GPT-2 versteht solche Sätze mit einer Trefferquote von mehr als 70 Prozent.

Die Entwickler der Software namens GPT-2 haben sich zunächst gar nicht getraut, ihre Entwicklung im Internet zugänglich zu machen, weil sie erzählerisch so kraftvoll daherkommt. Die Angst vor Fake-News hat die Freude über die Leistungsfähigkeit zunächst geschlagen.

Doch sosehr man die Phantasie in der Literatur schätzt, so wenig hat sie in der Berichterstattung über die faktische Welt zu suchen. Das Allen-Institute für künstliche Intelligenz und die Universität Washington haben deshalb eine KI namens Grover entwickelt, die Fake-News erkennen und enttarnen kann.

Mit einer Treffsicherheit von 73 Prozent erkennt sie auch, ob ein Text von einem Menschen oder einer Maschine geschrieben wurde. Und weil man am besten entlarvt, was man selbst schaffen kann, produziert Grover auch Fake News. In den ersten Tests zeigte sich: Menschen bewerten die KI-Propaganda vertrauenswürdiger als menschliche Werke.

Um diese Verwirrung noch zu steigern: In knapp zehn Prozent der Fälle konnte Grover auch die selbstproduzierten Fake-News nicht mehr als solche erkennen. Hier wird es dann plötzlich sehr menschlich im Leben einer literarischen Software.
Ungeahnte Möglichkeiten

Diese neuen Systeme arbeiten mit unüberwachtem Lernen, bei dem die Software in Datenmengen Muster erkennt, ohne dass sie zuvor Zielvorgaben bekommen hätte oder die Daten elektronisch gekennzeichnet worden wären. Derzeit scheint diese Variante maschinellen Lernens vielversprechend zu sein für den Zugang der KI zu Formen der Kreativität.

Denn auch Menschen lernen am effektivsten unüberwacht, also indem sie Schlussfolgerungen daraus ziehen, wie sie die Welt beobachten, ohne zuvor gesagt zu bekommen, wie sie beobachten sollen. GPT-2 und Grover zeigen, welche kreative Kraft in dieser Entwicklung steckt. Menschen werden rechnen müssen mit Maschinen. Sie werden bald einen Teil der Geschichten erzählen, die bisher ganz in der Hand der schreibenden Menschheit lagen.

Irgendwann in Zukunft werden Technologien alle Formen von Sprache schaffen. Das formt dann nicht nur das menschliche Denken, sondern auch die Anfänge eines eigenen Maschinendenkens. Maschinen schreiben dann über Menschen und irgendwann auch über sich selbst.

Wenn Maschinen uns erzählen, wer sie und wer wir sind, dann entstehen womöglich ganz neue Perspektiven auf unsere eigene Kultur und Geschichte. Die zufällige oder beabsichtigte Kontaktimprovisation beim gemeinsamen Schreiben von Mensch und Maschine könnte bisher unvorstellbare Werke hervorbringen.

Aus einer Marketingsicht liegt darin sogar eine ökonomische Chance. Gerade beim literarischen Schreiben gilt dann: «Human ist das neue Schwarz.» Menschliche Autoren können sich bald vielleicht für ein Biosiegel der hand- und hirngemachten Literatur qualifizieren, das die Preise ihrer Werke sicher merklich in die Höhe treiben wird. Verknappung war ja schon immer eines der schlagkräftigsten Verkaufsargumente. Der einzelne Mensch ist auch als Literat immer endlich. Der Algorithmus schreibt in alle Ewigkeit, auch wenn ihn dann kein Mensch mehr liest.

Befragt danach, was mit der Menschheit geschehen wird, wenn KI-Systeme 2050 Musik komponieren, Kochrezepte kreieren und Gedichte und Romane schreiben können, antwortet übrigens GPT-2: «Es wird unmöglich werden, mit anderen Menschen zu kommunizieren, und schliesslich wird alles im Chaos und im Verlust der Menschlichkeit enden. Was Menschen dann damit machen werden, bleibt jedem selbst überlassen.»

Das Gute ist: Es braucht nur einen Klick – und GPT-2 produziert eine neue Geschichte, die dieses Mal ein gutes Ende nimmt.";https://www.nzz.ch/feuilleton/maschine-und-mensch-ein-algorithmus-revolutioniert-die-literatur-ld.1506743;NZZ;Miriam Meckel;;;
30.12.2017;AR-Brille geht auch in schick;"Es muss nicht immer klobig sein: Die amerikanische Firma Magic Leap stellt endlich ihre seit Jahren angekündigte Augmented-Reality-Brille vor. Bei der «Magic Leap One» handelt es sich um eine schlanke AR-Brille, die von einem Mini-Computer für den Hosenbund angetrieben wird.

Seit 2010 entwickelt das ebenso geheimnisvolle wie gehypte Unternehmen aus Florida an einem «Mixed Reality»-Headset, das die virtuelle Realität mit der natürlichen Wahrnehmung eines Nutzers vermischen kann – daher auch der Begriff «Mixed Reality». Anfang des kommenden Jahres soll nun endlich eine «Creator's Edition» der futuristischen Brille an Entwickler ausgeliefert werden.

Das komplette Paket besteht aus Brille, einem Mini-PC, der am Gürtel des Nutzers zu befestigen ist, und einem kabellosen Controller. Wann eine Version für Verbraucher auf den Markt kommt, ist offen und auch zum Preis der AR-Brille ist bis dato nichts bekannt.
AR soll nie schöner gewesen sein

Die MR-Brille von Magic Leap ist mit Lichtfeld-Technik und Machine Learning ausgestattet. Sie setzt im Gegensatz zu vergleichbaren «MR»-Geräten wie der «Hololens» von Microsoft oder «Meta2» von Metavision bei Augmented-Reality-Anwendungen auf eine ganz besondere Lichtfeld-Technologie, mit deren Hilfe virtuelle Umgebungen besonders lebensecht wirken sollen.

Dank dieser Technik soll sich mit der «Magic Leap One» die Interaktion mit virtuellen Gegenständen wesentlich realistischer anfühlen als bisher: Der Hersteller verspricht, dass Nutzer in der virtuellen Welt das Gefühl haben, sich Objekten tatsächlich nähern und diese anfassen zu können. Zudem soll das Sichtfeld deutlich breiter sein als bei «MR»-Brillen von Konkurrenzanbietern wie eben Microsoft oder Metavision.

In einem imposanten Werbevideo für diese Technologie zeigte Magic Leap schon vor einiger Zeit, wie ein kleiner virtueller Elefant aus den mit einer herkömmlichen Kamera gefilmten Händen eines Kindes in die Höhe fliegt.
Interessante Hardware

Auch die verbaute Hardware klingt spannend: Die «Magic Leap One» hat sechs Kameras, vier Mikrofone und ein spezieller «SoC»-Prozessor (System-on-a-Chip) für Machine Learning an Bord.

Firmenangabe zufolge soll die neuartige Brille insbesondere im Gaming-Bereich zum Einsatz kommen. Aber auch Einsatzzwecke beim Online-Shopping in 3D oder der Interaktion mit projizierten Objekten (zum Beispiel Screens mit TV-Programmen) sind denkbar.
1,9 Milliarden Venture Capital

Man darf gespannt sein, ob «Magic Leap One» den hohen Erwartungen gerecht werden kann. Nicht zuletzt ist der Erwartungsdruck auf das geheimniskrämerische Startup enorm: Tech-Riesen (darunter Google, Qualcomm oder Tencent) sowie einige bekannte Risikokapitalgeber aus dem Silicon Valley haben mehr als 1,9 Milliarden Dollar in die Firma mit Sitz im sonnigen Plantation in Südflorida investiert.

Grund genug für den Magic Leap-CEO Rony Abovitz, ehrgeizige Pläne für das Startup zu formulieren: Der umtriebige Mr. Abovitz hat einen Zehnjahresplan für das Startup und ein breites Produkteportfolio angekündigt, bei dessen Entwicklung auch die Nutzerschaft mitbestimmen soll, in welche Richtung sich die Geräte weiterentwickeln. Das Unternehmen soll auch über 150 Patente besitzen.";https://www.nzz.ch/digital/ar-brille-geht-auch-in-schick-ld.1342891;NZZ;Jochen Siegle;;;
21.10.2020;Googles gute Dienste dürfen besseren Lösungen für die Nutzer nicht im Weg stehen;"Nun ist es also passiert. Google, oder besser: die Muttergesellschaft Alphabet muss sich bald mit der Klage der amerikanischen Regierung herumschlagen. Diese wirft dem Konzern vor, sich «wie eine böse» Monopolistin zu verhalten. Nach vergleichbaren Vorspielen in Europa und jüngst in einem Ausschuss des Repräsentantenhauses überrascht weniger die Klage vor dem Kartellgericht an sich als der frühe Zeitpunkt.

Der Termin wird offensichtlich vom politischen Kalender diktiert. Präsident Donald Trump und sein Justizminister William Barr wollen unmittelbar vor der Wahl Anfang November scheinbar ihrem Ärger über die Tech-Firmen aus dem Silicon Valley noch einmal Luft verschaffen. Sie fühlen sich offenkundig von den Konzernen schlecht behandelt. Der hektische Aktivismus soll wohl Handlungsfähigkeit vortäuschen. Tatsächlich werden die Protagonisten vielleicht gar nicht mehr im Amt sein, wenn sich die Gerichte in den kommenden Monaten ernsthaft mit der 64-seitigen Klageschrift befassen werden.
Die Klageschrift hat es in sich

Das Dokument hat es aber in sich. Denn die Klageschrift widerspricht praktisch in allen Punkten «dem Image vom guten, beinahe schon gemeinnützigen Unternehmen», das Google von Anfang an zu verbreiten versucht hat. Manche der sogenannten Stakeholder von Google scheinen so weitsichtig gewesen zu sein, die Konflikte zwischen den gewinnorientierten Interessen der Firma und dem angestrebten Image vorauszusehen. So sei das eigene Personal darauf getrimmt worden, mit Formulierungen äusserst vorsichtig umzugehen, wenn diese die starke Marktstellung und das aggressive Vertreten der eigennützigen Strategie zu stark herausstellen könnten.

Bei oberflächlicher Betrachtung ist diese nicht unmittelbar zu erkennen. Googles Suchmaschine ist gratis. Sie liefert meist so gute Ergebnisse, dass sie mit grossem Abstand von den meisten Konsumenten der westlichen Welt verwendet wird. Dazu gibt es einen sehr guten, kostenlosen Mail-Dienst, eine gut bestückte Videoplattform und weitere Gratisangebote. Das Paket scheint zunächst nicht mit der Definition eines Monopols übereinzustimmen, wonach eine Firma einen schlechten Service bietet und überhöhte Preise verlangt, sobald sie eine marktbeherrschende Stellung erreicht hat.
Was kosten Googles Dienste wirklich?

Allerdings zeigt diese Betrachtungsweise nicht das ganze Bild. Schliesslich wird naiverweise ausgeblendet, welchen Preis die Konsumenten tatsächlich für die Verwendung von Googles Diensten bezahlen. Das tun sie, indem sie mit einem Klick die Nutzungsrechte akzeptieren – und die geben dem kalifornischen Konzern das Recht, wie ein Krake persönliche Daten zu sammeln und mit diesen hausieren zu gehen. Sie sind im Werbegeschäft Gold wert, vor allem wenn sie zur punktgenauen Zuordnung von Werbebotschaften zu kaufinteressierten Nutzern führen. Letztlich ist den Google-Nutzern also nicht klar, was ihre persönlichen Daten wert sind, was genau mit diesen passiert und was sie für die Angebote des Internetriesen wirklich zahlen.

Ein weiterer Gesichtspunkt ist Googles Auftreten im Suchmaschinen- und Werbegeschäft. Zunächst scheint es eine logische Erklärung für Googles hohe Marktanteile zu geben: Die Ergebnisse der Suchmaschine seien so gut, dass sie immer mehr Nutzer anzögen. Mehr Nutzer bedeuteten mehr Daten. Mehr Daten machten die selbstlernenden Algorithmen hinter dem Suchmechanismus immer intelligenter und die Suchergebnisse immer präziser – ein positiver Kreislauf. Er spiegelt sich in der materiellen Betrachtungsweise: Mehr Nutzer und mehr Daten bedeuteten höhere Werbeeinnahmen, und diese ermöglichten höhere Investitionen in die unglaublich umfangreiche und komplexe Infrastruktur des Unternehmens. Auch das scheint ein positiver Feedbackloop zu sein. Tatsächlich sind Googles Ertragsmargen schon seit Jahren so hoch wie bei erfolgreichen Softwareunternehmen.
Fürchtet Google die latente Konkurrenz?

Allerdings fragt sich, wieso Google trotz diesen mutmasslichen Vorteilen so viel Geld für Kontrakte mit Smartphone-Herstellern, Browser-Anbietern und Telekommunikationsunternehmen in die Hand nimmt, um die eigene Suchmaschine möglichst immer an prominentester Stelle präsentieren zu können.

Kann es sein, dass trotz Googles mutmasslichem technologischem Vorsprung und seinen theoretischen Grössenvorteilen noch bessere, noch günstigere und möglicherweise auch noch deutlich transparentere Angebote auf den Markt drängen könnten? In diesem Fall wären die Konsumenten bessergestellt als mit dem Status quo. Folglich muss sichergestellt werden, dass potenzielle Konkurrenten die Möglichkeit haben, mit gleich langen Spiessen zu kämpfen. In diesem Sinne bleibt zu hoffen, dass der Kartellprozess gegen Google zu mehr Klarheit und zu einem eindeutigeren Ergebnis führt als im Fall Microsoft vor zwanzig Jahren. Damals sind die Kläger als Raubtiere in die juristische Auseinandersetzung gestartet und letztlich bestenfalls als Bettvorleger gelandet.";https://www.nzz.ch/meinung/googles-gute-dienste-duerfen-besseren-loesungen-fuer-die-nutzer-nicht-im-weg-stehen-ld.1582821;NZZ;Christof Leisinger;;;
22.10.2014;Wenn Computer sprechen lernen;"Auf den ersten Blick gleicht Andrew Ng einem verunsicherten Akademiker, der sich im Büro verlaufen hat. Seit diesem Sommer ist der Stanford-Professor Leiter der Forschungsabteilung von Baidu, dem Suchmaschinen-Anbieter aus China. Baidu wird in der Branche auch Google Chinas genannt, weil das Unternehmen den Suchmaschinenmarkt im Reich der Mitte dominiert (Google ist dort aus politischen Gründen nicht präsent). Mit einer Marktkapitalisierung von rund 70 Mrd. $ darf sich der Konzern durchaus mit den Grossen des Silicon Valley messen und vergleichen.
Grosse Ambitionen

Ngs Büro befindet sich denn auch in Sunnyvale, inmitten des berühmten Silicon Valley, gleich neben einem grossen Raumfahrt-Forschungszentrum. Das Ganze fühlt sich nicht nur sehr futuristisch an, sondern Ng hat entsprechend auch viel über die Zukunft zu sagen. Der Wissenschafter sieht vor allem grosses Potenzial bei der Fähigkeit von Computern, Sprache zu erkennen und Bilder wahrzunehmen und sie zu interpretieren.

Seit der Nominierung von Ng als Forschungsleiter bei Baidu ist die Marktkapitalisierung des Unternehmens um rund 35% gestiegen. Ob der Akademiker wirklich so viel wert ist, wird sich zeigen. Er selbst winkt ganz bescheiden ab. Er wolle nicht noch mehr Öl ins Feuer giessen. Die Euphorie um den Bereich «Machine Learning», wie künstliche Intelligenz dieser Tage im Silicon Valley genannt wird, sei ihm schon viel zu gross. Doch die Ambitionen von Ng sind beachtlich.
Ökosystem von Spezialisten

Ein gutes Beispiel ist die Fähigkeit von Computern, Sprache zu erkennen. Geforscht worden ist in dieser Sache schon viel, doch wirklich nutzbare Produkte sind bis jetzt rar. Ng und andere Akademiker haben allerdings in den letzten Jahren grosse Fortschritte gemacht. Dank neuen Algorithmen und grosser Rechenkapazität ist es ihnen gelungen, den Computern das Verstehen von Sprache beizubringen. Insofern sieht Ng die Möglichkeit, dass bereits in fünf Jahren ein Grossteil der Interaktion mit Handys per Sprache und nicht mehr per Finger geschehen wird.

Auf die Frage, ob er sich als Begründer der modernen künstlichen Intelligenz sehe, reagiert Ng heftig mit einem «Nein». Viele vor ihm hätten die Grundlagen gelegt. Er sei bloss ein Glied in einer langen Kette von Wissenschaftern. Sein Vermächtnis sei es, vielen Studenten die Materie beigebracht zu haben. Andrew Ng ist nämlich auch Mitgründer von Coursera, einer Online-Universität, die vor allem im Bereich Informatik sehr beliebt ist. Der Kurs «Machine Learning» wird dort von Ng angeboten und hat laut Angaben des Unternehmens bereits Hunderttausende von Absolventen. So viel Gehirnschmalz sei noch nie in einen Bereich geflossen, sagt Ng, sichtlich stolz darauf, dass viele seiner Studenten interessante Anwendungen von künstlicher Intelligenz entwickelt haben. Mit seinem Kurs auf Coursera hat Ng ein Ökosystem von Spezialisten geschaffen, die nun in der Lage sind, die Innovation weiter voranzutreiben.

Die Sprache als neues User-Interface ist schon von vielen Computerfachleuten prognostiziert worden. Bill Gates von Microsoft und Steve Jobs von Apple haben bereits vor Jahrzehnten davon gesprochen. Doch die Algorithmen waren damals nicht bereit. Heute ist es der Branche laut Ng möglich, diesen Sprung zu machen. Die Auswirkungen dessen sind nicht zu unterschätzen. Das letzte Mal wurde das User-Interface radikal geändert, als Apple und später Microsoft die Fenster und den Mausklick in ihre Betriebssysteme einbauten. Das ist schon lange her. Seither lernen Generationen von Benutzern, wie man auf dem Computer tippt und mit der Maus umgeht. Selbst die Einführung von Fingerabdruck-Kontrollen beim iPhone hat die Grundlage des User-Interface nicht stark verändert. Nun steht die Branche allerdings laut Ng möglicherweise vor einer neuen fundamentalen Veränderung.
Suche nach den Gewinnern

An der Wall Street spekulieren die Auguren schon länger, wie sich die Errungenschaften von «Machine Learning» auf die Branche auswirken könnten und wer die Gewinner und Verlierer solcher Änderungen sein könnten. Die Antwort darauf ist nicht einfach. Doch wer hier richtig liegt, kann an der Börse bestimmt gutes Geld verdienen.";https://www.nzz.ch/finanzen/wenn-computer-sprechen-lernen-1.18408485;NZZ;Krim Delko;;;
07.08.2019;Ian McEwan: «Wir befinden uns im freien Fall – dank unserer eigenen Cleverness»;"Ian McEwan, gewöhnlich begegne ich Ihnen in der mentalen Privatsphäre des Lesers. Gestern jedoch war ich Teil des Publikums beim öffentlichen Gespräch über Ihren neuen Roman. Was geht Ihnen durch den Kopf, wenn Sie als Schriftsteller auf einem Podium sitzen?

Inzwischen wird es vom Schriftsteller erwartet, professioneller Interpret der eigenen Arbeit zu sein. Als ich mit dem Schreiben begann, war dies kaum der Fall. Natürlich sind Autoren schon immer aufgetreten, Dickens etwa hat Bühnenlesungen auf geradezu theatralische Weise abgehalten. Aber Tolstoi und Kafka haben sich nicht endlos erklärt. Es handelt sich dabei um einen merkwürdigen Vorgang, der dazu führt, dass das gesamte eigene Werk im Präsens weiterlebt, weil man jederzeit zu allem befragt werden könnte. Meine erste Story erschien vor mittlerweile fünfzig Jahren, und sich mental mit etwas auseinanderzusetzen, das man vor einem halben Jahrhundert geschrieben hat, hat etwas sehr Eigenartiges.

Auf Podien sind Sie nicht nur Interpret Ihres eigenen Werks, sondern – wie viele Autoren seit den Anschlägen vom September 2001 – auch gefragter Kommentator des Zeitgeschehens. Inwiefern beeinflussen die Erwartungen, die Ihre Leserschaft an Sie als «public intellectual» hat, Ihr Schreiben?

Ich würde sogar weiter zurückgehen und sagen, es begann mit der Rushdie-Affäre. Merkwürdig, dass Sie mich darauf ansprechen, weil ich gerade eine Anfrage vom «New Statesman» erhielt, der einen Essay, den ich 1989 für das Magazin über die Rushdie-Affäre geschrieben hatte, in Buchform wiederveröffentlichen möchte. Der Essay hiess «Do You Dare Like This Book».

Er erschien, kurz nachdem über Rushdie die Fatwa verhängt worden war.

Ja. Ich hatte den Essay vollkommen vergessen und kann mich auch nicht erinnern, was ich darin geschrieben hatte. Aber dieses Ereignis, eher als der 11. September, hat Schriftsteller meiner Generation dazu gedrängt, Position zu beziehen. Doch Sie fragen mich nach öffentlichen und privaten Räumen. Nach den Auswirkungen, die Ihr Auftreten als öffentlicher Intellektueller auf Ihre Arbeit als Romancier hat.

Mein eigener Eindruck ist, dass ich an keinerlei öffentliche Erwartungen denke, sobald ich die Tür meines Arbeitszimmers hinter mir schliesse. Ich bin dann in einem luxuriösen, privilegierten Raum geistiger Freiheit – im Bewusstsein, dass es sich dabei um eine Minderheitenposition handelt. Wir leben nicht in einem Zeitalter der Meinungsfreiheit, diese ist auf gewisse Länder begrenzt. Wenn ich öffentlich rede, kommt es mir fast so vor, als würde ich über einen anderen Autor sprechen. Ich bin dann nicht der Schriftsteller, der allein in seinem Arbeitszimmer sass und über einen Zeitraum von zwei oder drei Jahren ein Buch hervorgebracht hat.

Mit der künstlichen Intelligenz haben Sie in Ihrem neuen Roman, «Maschinen wie ich», dennoch abermals ein Thema aufgegriffen, das Teil eines aktuellen öffentlichen Diskurses ist.

Das ist richtig.

Ihr Interesse an dieser Materie reicht bis ins Jahr 1975 zurück, als Sie an einem Magazinartikel über Maschinenintelligenz und den 1954 verstorbenen Alan Turing arbeiteten, der nun als Nebenfigur im Roman auftritt. «Können Maschinen denken?» Was hat Sie damals an dieser von Turing aufgeworfenen Frage interessiert?

Ich war zu jener Zeit vielleicht eine Art frustrierter Naturwissenschafter, ich hatte ein schwelendes Interesse an Mathematik und war fasziniert von diesem Mann, der ein Universalgelehrter gewesen war. Turing war nicht nur ein guter Mathematiker, auch Biologie fesselte ihn. Er hat viel zur Morphologie gearbeitet, zur Frage, weshalb Pflanzen eine spezifische Form haben; für die Biologie ist das nach wie vor ein grosses Thema. In gewisser Weise fühlte ich mich von der Vorstellung angezogen, dass es in modernen Zeiten noch immer möglich war, ein Universalgelehrter zu sein. Jetzt ist es meines Erachtens unmöglich.

Welche Vorstellungen hatten Sie Mitte der siebziger Jahre von der Zukunft – etwa vom Jahr 1982, in dem «Maschinen wie ich» spielt?

In den Siebzigern hing der künstlichen Intelligenz noch etwas Phantastisches an, bis in die achtziger Jahre hat sie nichts hervorgebracht. All ihre Versprechungen hatten sich in Luft aufgelöst, die Computerrevolution hatte noch nicht stattgefunden. Die Zeit der Lochkarten war zwar vorüber, aber es gab noch diese riesigen IBM-Bandlaufwerke, und nur grosse Unternehmen konnten sie sich leisten. Computer waren noch nicht Teil unseres Lebens, so dass der Gedanke, dass KI irgendeine Auswirkung auf menschliches Leben und die Gesellschaft haben könnte, einfach nicht aufkam. Ausserdem durchlebte England in den Siebzigern eine Reihe politischer Krisen, 1979 wurde Mrs. Thatcher Premierministerin, und wir schlitterten den Berg hinab. Wir hatten genug damit zu tun, die Gegenwart zu bewältigen.

    «Bei der Behauptung, dass Bewusstsein nicht existiere, handelt es sich um eine der dümmsten Ideen, auf die ein denkender Mensch je gekommen ist.»

Dennoch: KI interessiert Sie seit langem. Weshalb haben Sie mit dem Schreiben eines Romans darüber bis heute gewartet?

Weil wir inzwischen in ein silbernes Zeitalter der künstlichen Intelligenz eingetreten sind. Endlich ist es so weit, dass die KI ihr Versprechen einzulösen beginnt. Vor etwa zehn Jahren machte die Software beträchtliche Fortschritte, Leute schrieben Programme für Machine-Learning, eine entscheidende Entwicklung. Algorithmen wurden komplexer und drangen in unser aller Leben ein. Als ich Gelegenheit hatte, mit dem KI-Forscher Demis Hassabis über Intelligenz und Machine-Learning zu sprechen, begriff ich, dass eine einzigartige industrielle Revolution bereits ihren Anfang genommen hatte. Dass wir uns an der Schwelle einer revolutionären Technologie befanden, die möglicherweise das Schicksal der Menschheit verändern wird. Ein Roboter namens Adam wird in «Maschinen wie ich» Teil einer Dreiecksbeziehung. Können Sie sagen, wo in Adam sich «der Geist in der Maschine» befindet?

In den Netzwerken, genau wie im menschlichen Gehirn. Es gibt keinen einzelnen, fixen Ort. Der kartesische Dualismus hält weder in technischer noch in philosophischer Hinsicht stand, aber Gilbert Ryles Buch «Der Begriff des Geistes», auf das Sie mit dem Zitat Ihrer Frage anspielen, ist dennoch ein dummes Buch, über das ich an der University of Sussex noch Essays schreiben musste. Es handelt sich dabei um ein ultrabehavioristisches Buch, das uns zu überzeugen versuchte, dass es so etwas wie «Geist» nicht gibt. Dass es sich dabei lediglich um eine bestimmte Art des Verhaltens handelt.

Ab und zu kommt dieser Gedanke aber nach wie vor wieder auf.

Ja, und mein Freund, der britische Philosoph Galen Strawson, widmet sein Leben der Feststellung, dass es sich bei der Behauptung, dass Bewusstsein nicht existiere, um eine der dümmsten Ideen handelt, auf die ein denkender Mensch jemals gekommen ist. Natürlich liefert sich Galen diesbezüglich Kämpfe mit Daniel Dennett. Ich war neunzehn Jahre alt, als ich über Gilbert Ryle geschrieben habe, Sie erinnern mich also gerade daran, dass mich die Dinge, über die wir hier sprechen, bereits mein Leben lang interessieren.

Ein Dualismus anderer Art liegt C. P. Snows Ende der fünfziger Jahre aufgestellten These von den «zwei Kulturen» zugrunde, an die ich beim Lesen Ihrer Bücher mitunter denke. Halten Sie die Natur- und Geisteswissenschaften für zwei nach wie vor nicht zu vereinbarende Welten?

Die These entbehrte nicht einer gewissen Realität, und das britische Bildungswesen stützt sie nach wie vor. Sobald sie sechzehn sind, beschäftigen sich die meisten Kinder entweder nicht mehr mit den Natur- oder aber nicht mehr mit den Geisteswissenschaften. Zugleich wird diese Dualität aber auch durchbrochen, etwa durch John Brockmans Website Edge.org, wo ein grossartiger Dialog zwischen Wissenschaftern und Künstlern geführt wird. Bei öffentlichen Auftritten fragen mich Leute trotzdem: «Weshalb interessieren Sie sich für Naturwissenschaften?» Die Frage erscheint mir ebenso dumm wie die Frage: «Weshalb interessieren Sie sich für Literatur?» Oder: «Weshalb interessieren Sie sich überhaupt für irgendetwas?» n der Vorstellung des Erzählers von «Maschinen wie ich» verschmilzt das Gesicht Alan Turings mit dem des Malers Lucian Freud, was mir wie eine symbolhafte Verbindung erscheint. Verstehen Sie Ihr Werk als eine Brücke zwischen den «zwei Kulturen»?

Man kann die Verschmelzung beider Gesichter durchaus so verstehen, aber ich sehe es nicht als meine Mission, zwischen den beiden Kulturen zu vermitteln. Im Grunde ist es ganz einfach: Wenn man sich alles vor Augen hält, was ich jemals geschrieben habe, sieht man den gesamten Inhalt meines Geistes. Alles, was mich interessiert, wird irgendwann zum Gegenstand eines Romans.

Ihr 2005 erschienener Roman «Saturday» wurde als Roman bezeichnet, der «bewusst vom Bewusstsein» handle.

Handeln nicht alle Romane vom Bewusstsein? Es gibt da kein Entkommen.

    «Wir sind über den Punkt hinaus, an dem es noch möglich war, über das Menschliche nachzudenken, ohne Technologie und die Naturwissenschaften einzubeziehen.»

Wie hat sich Ihr Wissen um das, was Bewusstsein ausmacht, seit der Arbeit an «Saturday» verändert?

Ich würde zum Beispiel nie wie Daniel Dennett ein Buch schreiben, das «Consciousness Explained» heisst. In der Welt der Neurowissenschaften würden viele einen weiten Bogen um diesen Titel machen, weil es sich um ein Projekt handelt, das kaum begonnen hat. Ich habe kürzlich angefangen, mich mit dem Feld der «information biology» zu befassen. Wie ist es möglich, dass eine Zelle so viele Informationen enthalten kann? Wir wissen nicht, was belebte von unbelebten Dingen unterscheidet, obwohl manche behaupten, dabei handele es sich um die Fähigkeit, Informationen zu verarbeiten und zu nutzen. Aber wie? Die Komplexität einer Zelle scheint endlos.

In gewisser Weise befindet sich die Menschheit also noch immer am Anfang von allem?

Wir stehen allenfalls bis zu den Knöcheln im endlosen Ozean der Möglichkeiten. Abgesehen von den grossen Problemen wie Klimawandel und der Möglichkeit eines Nuklearkriegs liegt die grösste Herausforderung meiner Meinung nach jedoch in der Frage: Was geschieht, wenn wir Maschinen herstellen, die intelligenter als wir selbst und die dem Menschen zudem in moralischer Hinsicht überlegen sind?

Eine Frage, der Sie in «Maschinen wie ich» nachgehen.

Und was geschieht, wenn diese Maschinen an der nächsten Generation ihrer Konstruktion mitarbeiten und die ganze Sache über unseren Verstand hinauszuwachsen beginnt? Wir werden uns Gewissheit darüber verschaffen müssen, wo wir aufhören und sie beginnen. Aber diese Fragen bleiben vollkommen offen, weil die Vielzahl der Möglichkeiten in mathematischer Hinsicht kolossal ist und unsere Technologien und die Auswirkungen, die sie auf unser Leben haben, eine Komplexität erreicht haben, die Vorhersagen völlig unmöglich macht.

Welche Relevanz messen Sie der Literatur der Zukunft bei? Ihr Roboter Adam prophezeit, dass Literatur als Beschreibung der «Varianten menschlichen Versagens» zukünftig überflüssig sein wird, «da wir einander dann zu gut verstehen».

Ja, weil wir in einer Cloud alle miteinander vernetzt sein werden. Es wird keine Täuschungen mehr geben oder all die anderen Dinge, von denen Romane handeln. Aber bei dieser Vermutung handelt es sich um Adams jugendlichen Überschwang. Ich habe nichts dafür übrig. Die Auswirkungen unserer Technologien auf unsere Privatsphäre, unser soziales und politisches Leben nehmen so rapide zu, dass sich für den Roman aussergewöhnliche Möglichkeiten eröffnen. Eines Tages werden wir auf das beginnende einundzwanzigste Jahrhundert zurückblicken wie auf das viktorianische Zeitalter, als man davon überzeugt war, dass man nicht mehr atmen könne, sobald ein Eisenbahnzug schneller als fünfundzwanzig Meilen pro Stunde fuhr.

Sind die Weisheiten der Literatur den Erkenntnissen der Naturwissenschaften ebenbürtig?

Es handelt sich dabei um parallele Erkundungen. Literatur ist in moralischer Hinsicht neutral, sie macht Menschen nicht besser oder schlechter, aber sie erweitert bis zu einem gewissen Punkt die Wahrnehmung und das Verständnis. Sollten wir das Interesse an Literatur verlieren oder sollte sie plötzlich irrelevant werden, befänden wir uns an einem sehr viel dunkleren Ort. Wir wären nicht mehr in der Lage, über uns selbst zu reflektieren – über das, was wir sind oder tun. Wir sind jedoch über den Punkt hinaus, an dem es noch möglich war, über das Menschliche nachzudenken, ohne Technologie und die Naturwissenschaften einzubeziehen.

In der futuristischen Realität des Jahres 1982, in der «Maschinen wie ich» spielt, erkennt man Züge unserer eigenen Zeit – nicht zuletzt im Bemühen einer politischen Gruppierung um den Austritt Grossbritanniens aus der EU.

Es handelt sich um keinen historischen Roman. Das Buch in einer erfundenen, veränderten Vergangenheit spielen zu lassen, heisst, sich der reinen Spekulation und gedanklichen Phantasmen hinzugeben, und Sie haben recht: Der Roman handelt ebenso sehr von heute wie von damals. Es gibt einen Anflug von Brexit, der Roman ist voll der Ängste, die uns heute heimsuchen.

    «Boris Johnson katapultiert sich mit seinem Brexit-Versprechen auf eine Mauer zu, die er selbst errichtet hat.»

«Was wäre, wenn . . .?» Die zentrale Frage, von der Ihr Roman ausgeht, beschäftigt nicht nur Literaturinteressierte, sondern auch die Leser einer allenthalben im Umbruch befindlichen Realität.

Stimmt. Die Menge der «Was wäre, wenn . . .?», die sich vor uns auftürmt, ist unendlich. Was wäre, wenn eine Gänseschar über der Beringstrasse das amerikanische Raketenabwehrsystem alarmieren würde und eine Maschine sehr schnell eine Entscheidung treffen müsste? Was wäre, wenn der Totalzusammenbruch des Finanzsystems, der 2008 und 2009 nicht sehr fern schien, uns noch bevorstünde und nicht nur wenige, sondern alle Banken pleitegingen?

Was wäre, wenn Boris Johnson sein Versprechen wahr machen und Grossbritannien Ende Oktober aus der EU führen würde – auch ohne Vertrag?

Johnson katapultiert sich mit diesem Versprechen auf eine Mauer zu, die er selbst errichtet hat. Er wird entweder mit uns an Halloween aus der EU austreten oder sein Wort brechen, was für Politiker keine Unmöglichkeit ist. Grossbritannien befindet sich in einer konstitutionellen Krise, und wir haben keine Ahnung, worauf sie hinauslaufen wird. Es ist durchaus möglich, dass wir in der EU verbleiben werden, aber für alle Zeiten Narben davontragen. Zutiefst gespalten, so wie es die Vereinigten Staaten seit vielen Jahren sind. Beinahe «zwei Kulturen», aber nicht im Sinne C. P. Snows, sondern zwei politische Kulturen, deren Annahmen bezüglich der Gegenwart und der Zukunft vollkommen verschieden sind.

Brauchen Politiker der Ära Trump die Qualitäten eines Rockstars, um erfolgreich zu sein?

Man darf nicht vergessen, dass Johnson nicht durch eine allgemeine Parlamentswahl ins Amt kam. Für ihn gestimmt hat lediglich eine Mehrheit der sehr kleinen Mitgliedschaft der Tories – zu rund drei Vierteln Leute, die fünfundsechzig Jahre oder älter sind. Sie sind grösstenteils weiss, sie gehören mehrheitlich der Mittelschicht an. Für die Bevölkerung des Landes sind sie in keiner Weise repräsentativ, aber sie sind von Boris Johnson geblendet. Zum Teil hat das damit zu tun, dass in England – und ich spreche bewusst von England und nicht von Grossbritannien –, die alte, nicht sonderlich hilfreiche Tradition existiert, jemandem, der als «upper-class» gilt, mit Faszination und Bewunderung zu begegnen. Eton, Oxford, vornehm.

Ein Nachkomme der «Brideshead»-Generation.

Ich selbst spreche als Republikaner, und wie wundervoll die Queen auch sein mag, ich glaube, dass die Monarchie ihren Beitrag zu dieser merkwürdigen Tendenz der Engländer leistet, Eton-Schüler an die Macht zu bringen. Sie sind von ihrem eigenen Anrecht darauf vollkommen überzeugt, obwohl sie oft nicht sonderlich gebildet sind. Sie sind schlagfertig und haben eine gewisse Art von Glanz. Boris Johnson ist einer von diesen Typen, die einen Essay erst in der Nacht vor dem fälligen Abgabetermin angehen. Er bewerkstelligt das sehr schnell, und das Ergebnis ist sehr clever, aber nicht fundiert.

Vielleicht gelingt es Johnson, die Union-Jack-Fähnchen zu verkaufen, mit deren Vertrieb der Erzähler von «Maschinen wie ich» eine Pleite erlebt hat.

Ich sehe Boris Johnson eigentlich gar nicht als Scharlatan. Das Problem liegt weniger in Johnson selbst als in den Wählern. Wie heisst es in einem berühmten Gedicht von Brecht: «Wäre es da nicht doch einfacher, die Regierung löste das Volk auf und wählte ein anderes?» Irgendwie hat es unser System diesen Etonians ermöglicht, in die politische Sphäre vorzudringen, obwohl sie weder sonderlich talentiert oder klug sind noch eine Begabung für Politik besitzen.

Was brauchte es, um dieses System zu verändern?

Unser Social Engineering hat zu einer Welt geführt, in der die sechs oder sieben Prozent, die eine Privatschule besucht haben, einen riesigen, völlig überproportionalen Teil unserer kulturellen und politischen Eliten stellen. Natürlich gibt es Dinge, die man ändern könnte, und als jemand vom linken Flügel habe ich mein Leben lang darüber geredet.

    «Inzwischen leben wir in einer beinahe vorreligiösen, heidnischen Zeit mit allen möglichen dunklen Machtphantasien, die die populistischen Bewegungen antreiben.»

Und wie lautet Ihre Botschaft heute?

Wir müssen uns im Klaren darüber sein, wie weit wir die Kontrolle über unser Leben bewahren wollen. Über die Art und Weise, wie wir das Leben für uns selbst beschreiben und welche Rolle die Literatur dabei spielt. Wir müssen uns darüber im Klaren sein, dass unsere Existenz von der gewaltigen Revolution, die wir durchleben, nicht mehr getrennt werden kann. Die künstliche Intelligenz ist nur ein Aspekt des gesamten digitalen Augenblicks. Wir hätten uns niemals vorstellen können, welchen Einfluss das Internet auf unsere Politik haben würde. Dass es einen amerikanischen Präsidenten ins Amt heben würde. Wir befinden uns also in einem freien Fall, den wir zum Teil unserer eigenen Cleverness verdanken. Unserer technischen Cleverness, die so viele mögliche Zukünfte erzeugt, dass wir darüber keine Vermutungen mehr anstellen können.

Was geht Ihnen momentan durch den Kopf, wenn Sie in die Privatsphäre Ihres Arbeitszimmers eintreten?

Ich habe gerade Stefan Zweigs «Die Welt von gestern» gelesen und mache mir Gedanken über die merkwürdige Welle des Irrationalismus, die derzeit über Europa hinwegrollt und eine ihrer interessantesten Manifestationen in der Weigerung von Leuten findet, sich gegen Masern impfen zu lassen. In den USA herrscht im Staat New York bereits eine Epidemie. Es betrifft nicht nur rückständige Länder, Frankreich ist führend in der Anti-Impf-Kampagne. Darüber hinaus erinnere ich mich an die Siebziger, als wir glaubten, dass sich die Religion aus der öffentlichen Debatte verabschiedet habe. Mit dem Islamismus ist sie zurückgekehrt, und inzwischen leben wir in einer beinahe vorreligiösen, heidnischen Zeit mit allen möglichen dunklen Machtphantasien, die die populistischen Bewegungen antreiben. Einer der sensibelsten Indikatoren für den Zustand Europas war schon immer der Antisemitismus.

Der mittlerweile wieder zunimmt.

Nicht nur in Form unbedachter Ausrutscher auf der Strasse, vielmehr dringt der Antisemitismus auch in die Politik ein. Beispielsweise in Ungarn. Und all das so bald nach dem Zweiten Weltkrieg und der «Endlösung». Das alles geht mir momentan durch den Kopf, und natürlich ist der Brexit Teil davon. Trump ist Teil davon, Brasiliens Bolsonaro ist Teil davon. Aber was mir noch fehlt, ist die Sprache, sind die Situationen, der Impuls, der diesen Dingen eine fiktive Stimme gibt. Das, was die Literatur hinzufügen würde, was ihr Beitrag wäre.

Wenn Alan Turing recht hat und Adam uns allen einen Spiegel vorhält: Was sehen Sie in diesem Spiegel?

Ich sehe ein verwirrtes Kind.

An Ihrem Schreibtisch sitzt ein Kind?

Nein, Sie hatten gefragt, was ich im Spiegel meiner Figur sehe. An meinem Schreibtisch sitzt ein alter Mann.";https://www.nzz.ch/feuilleton/ian-mcewan-wir-verlieren-unsere-zukunft-an-die-technologie-ld.1499323;NZZ;Thomas David;;;
18.01.2018;Das sind die Jobs der Zukunft;"Der rasch voranschreitende technologische Wandel führt zu tiefgreifenden Veränderungen auf dem Arbeitsmarkt. In der Debatte um die Zukunft der Arbeit stehen vor allem die Arbeitsplätze im Vordergrund, die im Zuge der Digitalisierung und Automatisierung gefährdet sind. Industrieroboter verdrängen Fabrikarbeiter, in der Logistik wächst der Einsatz von Drohnen, Finanzberater konkurrieren zunehmend mit Algorithmen.

Nachdem in den vergangenen Jahren vor allem Routinetätigkeiten ersetzt worden sind, stehen nun Stellen im Dienstleistungssektor und in der Administration des Industriebereichs im Fokus. Dass Aufgaben automatisierbar sind, heisst jedoch noch nicht, dass die Arbeitsplätze in Zukunft wegrationalisiert werden. Wenn etwa einfache steuerliche Probleme zunehmend von Maschinen gelöst werden, haben Steuerberater mehr Zeit, sich auf komplizierte Sachverhalte zu konzentrieren und den Kundenkontakt zu pflegen.
Neue Stellen für Hochqualifizierte

Laut einer Studie des Beratungsunternehmens Deloitte sind von der Automatisierung zum Beispiel besonders Sekretariatskräfte, Telefonistinnen und Buchhalter betroffen, während demgegenüber Berufe wie Anwalt, medizinische Assistentin oder Kinderbetreuer als zukunftssicher gelten. Die Beschäftigung in der Schweiz dürfte künftig besonders bei Bürokräften ohne höhere Ausbildung sowie Malern oder Maurern schrumpfen. Dagegen prognostiziert Deloitte für hochqualifizierte Berufe wie Arzt oder Ingenieur einen Zuwachs bei den Arbeitsplätzen. Zudem entstehen zusätzliche Stellen in der Informations- und Kommunikationstechnologie.

Ein Arbeitsplatz ist einerseits tendenziell weniger von technischen Umwälzungen bedroht, je anspruchsvoller und je unberechenbarer die Tätigkeiten sind. Zukunft haben aber auch soziale und kreative Aufgaben. Ein Computer kann zwar etwa innert Kürze grosse Mengen an Wissen durchforsten, es mangelt ihm aber an Mitgefühl.

Entsprechend werden künftig wohl insbesondere auch Kompetenzen wie Sozialkompetenz, Kreativität und Intuition an Bedeutung gewinnen. Fähigkeiten wie Flexibilität, kritisches Denken, Unternehmergeist und Selbständigkeit dürften in der Arbeitswelt von morgen ebenfalls wichtiger werden. Damit die Beschäftigten die künftig geforderten Qualifikationen erlangen, spielt die Aus- und Weiterbildung eine Schlüsselrolle.

Viel schwieriger, als die durch den Wandel bedrohten Tätigkeiten zu identifizieren, ist eine Prognose, welche ganz neuartigen Berufe künftig entstehen könnten. So liegt es zwar etwa auf der Hand, dass die maschinelle Fertigung von Menschen überwacht werden muss oder dass der Bedarf an Bioingenieuren, Big-Data-Spezialisten oder Tele-Chirurgen wachsen wird. Doch welche weiteren Zukunftsberufe könnten entstehen? Das Unternehmen Cognizant, das Firmen im Bereich Business- und Technologiedienstleistungen berät, stellt in einem neuen Bericht 21 Berufe vor, die in den kommenden zehn Jahren für neue Beschäftigung sorgen könnten. Die Tätigkeiten lassen sich unter den Schlagworten Coachen, Umsorgen und Vernetzen zusammenfassen. Genannt werden etwa Jobs wie Quantum-Machine-Learning-Analyst, Genetic-Diversity-Officer oder Artificial-Intelligence-Assisted Healthcare-Technician, die hohe technische Kenntnisse erfordern. Daneben werden aber auch Berufe wie Virtual-Store-Sherpa, Highway-Controller oder Personal Memory-Curator aufgezählt, für deren Ausübung es geringe technische Fähigkeiten braucht.

Nachfolgend ein paar weitere Beispiele:
Technische Zukunftsberufe

    Datendetektiv Recherchiert in den betriebsinternen Daten und arbeitet anhand der untersuchten Informationen Vorschläge aus.
    Cyber-Stadtplaner Stellt sicher, dass Biodaten, Einwohnerdaten oder Investitionsgüterdaten in den Städten ungehindert fliessen. Sorgt dafür, dass die Technik- und Übertragungsanlagen störungsfrei funktionieren.
    Augmented-Reality-Journey-Builder Entwickelt die Reise von Kunden, die ihr reales Erleben mit Augmented Reality erweitern. Schreibt, designt, kalibriert, baut und personalisiert die digitale Geschichte.
    Edge-Computing-Master Definiert die Roadmap für das Internet der Dinge und kalkuliert unter anderem die technischen Anforderungen. Mit Edge-Computing wird die dezentrale Datenverarbeitung am Rand des Netzwerks bezeichnet.

Weniger technische Zukunftsberufe

    Persönlicher Gedächtniskurator Unterstützt ältere Kunden mit Gedächtnisverlust darin, virtuelle Umgebungen zu erschaffen, in denen sie sich aufhalten können.
    Digitaler Schneider Besucht Kunden zu Hause, um online bestellte Kleidung auf Mass anzupassen und fertigzustellen.
    Ethikbeauftragter Stellt sicher, dass die Verteilung des Unternehmenseinkommens den ethischen Standards der Firma entspricht.
    Fitness-Commitment-Counselor Bietet als Fitness-Beauftragter oder Diät-Coach Beratungen aus der Ferne an und überwacht digital die körperliche Aktivität der Kunden.
    Spaziergänger/Gesprächspartner Unterhält sich mit älteren Menschen und begleitet sie auf Spaziergängen. Er vernetzt sich mit den Senioren über eine Internetplattform.";https://www.nzz.ch/wirtschaft/welches-sind-die-jobs-der-zukunft-ld.1348670;NZZ;Natalie Gratwohl;;;
10.03.2016;Google-Software gewinnt beim Go;"Ein Computerprogramm namens Alphago hat am Mittwoch in der südkoreanischen Hauptstadt Seoul einen der besten Go-Spieler der Welt – Lee Sedol – geschlagen. In einer Pressekonferenz nach dem Spiel gab sich Sedol kleinlaut und bescheiden: «Ich war nicht darauf vorbereitet, dass Alphago ein derart perfektes Spiel spielen würde.» Der Journalist des «Guardian» will Tränen in seinen Augen gesehen haben.
Medienwirksame Inszenierung

Alphago wurde in London in den Geschäftsräumen der Jungfirma Deepmind entwickelt. Vor zwei Jahren hat sich Google die Firma einverleibt. Mehrere hochrangige Google- oder Alphabet-Manager – darunter Eric Schmidt – waren nach Seoul gereist, um dem Spiel beizuwohnen. Die kalifornische Internetfirma misst dem öffentlichen Debüt von Alphago offenbar grosse Bedeutung zu, der Match wurde medienwirksam inszeniert.

Beim Go-Spiel sitzen sich zwei Spieler gegenüber und placieren abwechselnd weisse oder schwarze Steine auf einem Brett, das 19 mal 19 Felder umfasst. Ziel ist es, mit den eigenen Steinen möglichst grosse Gebiete zu umranden und gleichzeitig die Bewegungsfreiheit des Gegners zu begrenzen, seine Steine zu umzingeln. Um sich in dem Spiel zu bewähren, sei Intuition und Kreativität gefragt, heisst es. Für Computer ist Go eine grosse Herausforderung, weil die Zahl der möglichen Züge sehr gross ist und weil es schwierig ist, zu erkennen, ob ein Zug vorteilhaft war.

Viele der über Alphago geschriebenen Artikel erwecken den Eindruck, als ob hier aus dem Nichts heraus ein Durchbruch erzielt worden wäre. Doch das Brettspiel Go ist in der Computerwissenschaft ein altes Forschungsthema. Go-Software gibt es seit den 1970er Jahren, die besten Programme erreichten das Niveau eines fortgeschrittenen Amateurs. Alphago, Ende Januar in der Wissenschaftszeitung «Nature» vorgestellt, kann sich mit Profis messen.
Klettern im Suchbaum

Einer der bei der Entwicklung von Alphago massgebenden Deepmind-Mitarbeiter – David Silver – hat an der University of Alberta seine Doktorarbeit über Go-Software bei einem Professor geschrieben, der seinerseits schon – an der ETH Zürich – mit einer Arbeit über Go promoviert worden war. Das Thema verbindet Generationen von Computerwissenschaftern, wichtige Durchbrüche liegen Jahre zurück.

Einen wichtigen Fortschritt brachte die sogenannte «Montecarlo Revolution», angestossen durch Arbeiten etwa von Rémi Coulom (2006). Bei der Monte Carlo Tree Search (MCTS) wird die Spielstrategie durch den Zufall bestimmt. Der Verlauf einer Go-Partie lässt sich als Baum, als Anordnung von Ästen beschreiben, als hierarchische Struktur, die – ausgehend von einem Wurzelknoten, dem ersten Zug – wächst und sich immer weiter verzweigt. Um die Auswirkungen eines bestimmten Spielzugs zu evaluieren, müssten alle nachfolgenden Verzweigungen untersucht werden. Es gibt bei Go, so schreiben Silver und seine Koautoren, durchschnittlich bei jedem Zug 250 Auswahlmöglichkeiten. Beim Schach sind es 35. Die durchschnittliche Spieltiefe, die Dauer des Spiels, wird bei Go mit 150 angegeben, beim Schach sind es 80. Der Go-Suchbaum umfasst somit 250¹?? Verzweigungen. Sie alle zu untersuchen ist unter realistischen Bedingungen nicht durchführbar.
Grosser Lernaufwand

Bei der Monte-Carlo-Methode wird das Herumklettern im Suchbaum durch den Zufall geleitet. Auf Grund einer sehr grossen Zahl von zufällig durchgespielten Partien lässt sich für einen bestimmten Zug in einer bestimmten Situation eine Erfolgswahrscheinlichkeit ermitteln.

Die Entwickler von Alphago haben MCTS kombiniert mit neuronalen Netzen jenes Typs, der sich jüngst bei der Bilderkennung bewährt hat. Diese sogenannten Convolutional Neural Networks (CNN) helfen, die Breite und Tiefe des Suchbaums weiter zu reduzieren. Sie übernehmen die Auswahl von Spielzügen und die Bewertung von Spielverläufen. Diese Software-Komponenten können sich verändern, sie sind «lernfähig», indem sie entweder die Spiele von menschlichen Go-Spielern analysieren oder selber Partien generieren. Aufgrund eines mehrwöchigen Lernvorgangs mit mehreren Hundert Millionen Lernschritten konnte Alphago umfassendes statistisches Material über Go-Spielsituationen anlegen und im Oktober vergangenen Jahres den europäischen Go-Champion Fan Hui schlagen. Bei diesem Spiel kam ein leistungsfähiger Computer mit 48 Prozessoren und 8 Grafikkarten zum Einsatz.
Mondlandung

Der Sieg gegen Fan Hui, der erst im Januar publik gemacht wurde, war der erste, den eine Software über einen professionellen Go-Spieler erringen konnte. Am Mittwoch ist das nun in Seoul zum zweiten Mal geglückt. «Wir sind auf dem Mond gelandet», kommentiert Demis Hassabis per Twitter den Erfolg der Alphago-Software. Hassabis ist Mitbegründer und Chef von von Deepmind. Auch nach der zweiten Niederlage am Donnerstag ist für den Menschen noch nicht alles verloren: Das Ringen zwischen Mensch und Maschine geht weiter, das letzte von fünf Spielen wird am kommenden Dienstag ausgetragen.
(Bild: PD)
Von Ameisen und Übermenschen ";https://www.nzz.ch/digital/machine-learning-google-software-gewinnt-beim-go-ld.6762;NZZ;Stefan Betschon;;;
22.07.2016;Die Erfindung des Rauschens;"Mit zwei lauten Explosionen ging im Sommer 1945 in Japan der Weltkrieg zu Ende. Als der Waffenlärm verebbte, war etwas Neues zu hören: Rauschen.

Vor dem Krieg, am 16. Februar 1939, war das Rauschen noch nicht da. An diesem Tag schrieb ein 23-jähriger Amerikaner namens Claude Shannon einen Brief an Vannevar Bush, Professor für Elektrotechnik am Massachusetts Institute of Technology (MIT). Shannon teilte seinem Doktorvater mit, dass er sich dereinst mit den theoretischen Grundlagen von Kommunikation beschäftigen wolle. Er fügte in den mit Maschine geschriebenen Brief ein von Hand gezeichnetes Diagramm ein, das – wie er schrieb – «praktisch alle Kommunikationssysteme» charakterisiert: Es gibt in dieser Zeichnung zwei Kästchen, Sender und Empfänger, dazwischen Pfeile, die zwischen der Mitteilung in ihrer ursprünglichen, in der gesendeten und in der empfangenen Variante vermitteln.

Nach dem Krieg, im Herbst 1948, ist das Rauschen da: Shannon publiziert seinen berühmten Aufsatz zur Informationstheorie: «A Mathematical Theory of Communication». Der Text beginnt mit dem bekannten Diagramm: ein Kästchen für den Sender, ein Kästchen für den Empfänger. Zusätzlich gibt es nun aber in der Mitte ein weiteres Kästchen: Rauschen. – Man darf von der «Erfindung des Rauschens» reden, denn Shannon und Norbert Wiener, von dem noch die Rede sein wird, haben mit diesem Begriff nicht eine physikalische Realität beschrieben, sondern ein neues Paradigma entwickelt. Als eine Art Kontrastmittel machte das Rauschen abstrakte Konzepte wie Information oder Kommunikation erstmals fassbar.
Unterwasserkabel

Shannons «Informationstheorie», so heisst es, sei die Gründungsurkunde, die «Magna Charta», des Informationszeitalters. Shannon wurde auch schon mit Kopernikus verglichen, die Informationstheorie habe eine «kopernikanische Wende» bewirkt. Doch Shannon, der «Vater des Digitalzeitalters», hatte seine Vorgänger. Ende der 1850er Jahre wurde im Atlantik ein Seekabel versenkt. Die telegrafische Verbindung zwischen den USA und Europa konnte aber nicht lange aufrechterhalten werden. Die Signalqualität verschlechterte sich rasch, die Morsesignale verloren sich im Meeresrauschen. Die Techniker glaubten die Störungen durch eine Erhöhung der Spannung beseitigen zu können und zerstörten gerade so das Kabel.

Die Probleme, die hier sichtbar wurden, haben in der Folge einige der besten Wissenschafter beschäftigt, zuerst den britischen Physiker Lord Kelvin, später in den USA etwa Harry Nyquist oder Ralph Hartley: «Transmission of information» (1928). Es ist in dieser Arbeit fast schon alles da, was die Informationstheorie aus dem Jahr 1948 ausmacht, nur etwas fehlt: das Rauschen.

Hartley befasst sich zwar am Rand auch mit Verzerrungen oder mit Störungen. Er glaubt aber nicht, dass es möglich sei, den Einfluss solcher «Interferenzen» auf die Übertragung von Informationen allgemeingültig zu beschreiben. Shannon hat das geleistet. Dank seiner Theorie lässt sich das Rauschen bändigen.

Die Katastrophe des Zweiten Weltkriegs hat die Mathematik erschüttert, hat Mathematiker dazu gebracht, das Unsichere, schwer Fassbare – das Rauschen – berechnen zu wollen.

Die Schnelligkeit der deutschen Flugzeuge überforderte die herkömmliche Flugabwehr, U-Boote verunsicherten den Atlantik. Um sie zu zerstören, brauchte es die Möglichkeit, den Funkverkehr abzuhören, die Verschlüsselung zu durchbrechen. Die Auswertung von Radarsignalen, die Entschlüsselung oder Verschlüsselung von Nachrichten – diese Aufgaben forderten die Mathematiker heraus. Es ging darum, dem Rauschen einen Sinn abzugewinnen.

In den USA organisierte das National Defense Research Committee (NDRC) diese wissenschaftliche Arbeit. Das NDRC wurde von Vannevar Bush geleitet, der während des Kriegs mit Hunderten von Millionen Dollar die Arbeit von Tausenden von Wissenschaftern und Ingenieuren unterstützte. Aufseiten der Alliierten sorgte der Krieg dafür, dass einige der besten Mathematiker des 20. Jahrhunderts – John von Neumann, Claude Shannon, Alan Turing und Norbert Wiener – zusammenkamen und eng zusammenarbeiteten.

Von Neumann spielte im Krieg eine wichtige Rolle bei der Entwicklung der Bombe – der Atombombe –, während Shannon und Wiener sich mit der Entwicklung eines «Debomber» beschäftigten: So nannte Wieners Mitarbeiter Julian Bigelow ein Flugabwehrsystem.
Mathematische Feuerwolken

Wiener hatte 1940 zunächst vorgeschlagen, feindliche Flugzeuge mithilfe einer Feuerwolke zu vernichten. Es sollten brennbare Gase in flüssiger Form in der Luft entzündet werden. Der Vorschlag wurde abgelehnt, Wiener begann, seine Feuerwolke zu mathematisieren. Es ging darum, den Flug eines angreifenden Flugzeugs für 20 bis 30 Sekunden vorauszusagen. Eine Serie von Radar-Messungen musste geglättet werden, um eine Trajektorie herauszuarbeiten. Shannon und Wiener forschten unabhängig voneinander. Wiener entwickelte seine Ideen in einem Seminarraum des MIT in Boston, Shannon arbeitete als Angestellter der Bell Labs in New York. Shannon war Teil einer grösseren Gruppe von Ingenieuren und Mathematikern, Wiener hatte nur einen einzigen Mitarbeiter: Bigelow. Während die Gruppe der Bell Labs mit geometrischen Methoden versuchte, die Bahn eines Flugzeugs vorauszusagen, setzte Wiener auf Statistik.

Wiener und Bigelow bildeten ein seltsames Gespann: hier der dickliche, kurzsichtige knapp 50-jährige Mathematiker, dort, 20 Jahre jünger, ein sportlicher Elektroingenieur, der sich handwerklich geschickt bei der Reparatur von Maschinen aller Art bewährte. Wiener, zigarrenrauchend, Benzedrin-Tabletten kauend, aufgeregt hin und her gehend, füllte die Wandtafel mit mathematischen Formeln. An Bigelow lag es, das Nachdenken des Mathematikers in die richtige Richtung zu lenken und die technische Umsetzbarkeit seiner Formeln sicherzustellen.

Als nach dem Krieg in den USA die wissenschaftlichen Arbeiten zu den Problemen der Flugabwehr publiziert wurden, behauptete der Leiter dieses militärischen Forschungsprogramms, der Elektroingenieur Harold Hazel, dass diese Forschung den Kriegsverlauf beeinflusst habe. Zwei Dinge hätten die Entwicklung von Flugabwehrkanonen vorangebracht: der Einsatz von neuartigen Rechenmaschinen und die Anwendung von kommunikationstheoretischen Prinzipien. «Oberflächlich betrachtet gibt es keinen Zusammenhang zwischen Feuerleitung und elektrischer Kommunikation. Bei genauerer Betrachtung zeigt es sich aber, dass es in beiden Fällen darum geht, nützliche Information zu trennen von ungewollter und unvermeidbarer Information in Form von Rauschen.»

Shannon und Wiener arbeiteten unabhängig voneinander, aber sie kannten sich und wussten, dass sie sich mit ähnlichen Problemen befassten. In seiner Informationstheorie würdigt Shannon Wiener in einer Fussnote als Quelle der Inspiration. Wiener sei es gewesen, der als Erster Kommunikationstheorie als statistisches Problem formuliert habe.

Nach der Flugabwehr war es ab 1943 das «Project X», das Shannon beschäftigte, die Entwicklung eines abhörsicheren Telefons. Dabei arbeitete er auch mit Turing zusammen. Was braucht es, um eine Nachricht zu verschlüsseln, um ein Signal zu verstecken? Rauschen.
Die Nachricht als Projektil

Wenn man die von Shannon während der frühen 1940er Jahre geschriebenen Aufsätze zu kommunikationstheoretischen Problemen liest – etwa «Communication in the Presence of Noise» –, glaubt man manchmal neben dem Schützen einer Flugabwehrkanone zu sitzen und in den Himmel zu starren: Die Nachrichtenübertragung, so erklärt Shannon einmal, sei eine Projektion, die Nachricht also – so könnte man verdeutlichend sagen – ein «Projektil», das von dem einen Raum in einen anderen Raum geschleudert wird. Das Rauschen vernebelt dort den Einschlagsort. Aus den Kästchen im ursprünglichen Diagramm sind Räume geworden, aus den Pfeilen Geschossbahnen. Umgekehrt, wenn man Wieners oder auch Shannons Aufsätze zur Flugabwehr liest, fühlt man sich einem Übermittlungssoldaten nahe, der dem Rauschen in seinen Kopfhörern Nachrichten abzugewinnen versucht.

Diese Analogien sind nicht zufällig. In dem berühmten Buch «Cybernetics» (1948) versucht Wiener rund um die beiden Begriffe «control» und «communication» eine neue Metawissenschaft, die Kybernetik, zu begründen. Er habe während des Kriegs herausgefunden, so schreibt er einleitend, dass Steuerungs- und Regelungstechnik (control) von der Nachrichtentechnik (communication) nicht getrennt werden könne. Kybernetik und Informationstheorie boten nach dem Krieg Wissenschaftern aus unterschiedlichsten Disziplinen – Ingenieuren, Linguisten, Biologen – eine intellektuelle Heimat.

«Nützliche Information zu trennen von wertloser Information in Form von Rauschen» – so haben Wissenschafter im Zweiten Weltkrieg den Auftrag von Übermittlungssoldaten und von Flugabwehrschützen definiert, und so haben einige von ihnen später die eigene Mission verstanden. Dann vermehrten sich die Informationen, die Forscher bekamen es mit Big Data zu tun.
Das Ende der Wissenschaft?

Big Data wird vielerorts als neues wissenschaftliches Paradigma wahrgenommen, so als ob da, wo viel Rauschen ist, automatisch auch nützliche Information einfach zu haben sei. Der amerikanische Journalist Chris Anderson hat in einem Beitrag für die Zeitschrift «Wired» 2008 bereits das «Ende der Wissenschaft» verkündet. Weil es so viele Daten gebe, liessen sich neue wissenschaftliche Erkenntnisse automatisch generieren.

In dem Buch «The Signal and the Noise» (2012) widerspricht der Star-Statistiker Nate Silver diesem naiven Glauben an die Kraft von Big Data. «Zahlen sprechen nicht für sich selber», schreibt er. Mithilfe von Computern ist es möglich, in sehr grossen Datenbeständen rasch Korrelationen auszumachen. Aus diesen Korrelationen lassen sich Hypothesen gewinnen, aber keine Theorien, Fragen, aber keine Antworten.";https://www.nzz.ch/feuilleton/betriebsgeraeusche-der-moderne/informationstheorie-die-erfindung-des-rauschens-ld.107127;NZZ;Stefan Betschon;;;
28.11.2016;«Weil Technik Spass macht – und weil Frauen die Fähigkeiten dazu haben»;"Frau Ehmann, Digitalisierung durchdringt unser ganzes Leben. Und zwar nicht zum Selbstzweck, sondern als Mittel zum Zweck. Welcher Zweck steht hier besonders im Vordergrund?

Es geht vor allem darum, effizienter zu werden: Auf dem Smartphone sind alle Informationen der Welt jederzeit abrufbar; wir können mit anderen Personen in Kontakt treten und Arbeitsprozesse einfacher gestalten und neu verknüpfen.

Aber die rasante Entwicklung macht auch Sorgen. In welchen Bereichen laufen Menschen Gefahr, von digitalisierten Prozessen und künstlicher Intelligenz ersetzt zu werden?

Repetitive Tätigkeiten stehen hier zwar unter Druck, aber das bedeutet nicht, dass Menschen mit solchen Jobs keine Arbeit mehr haben werden, sondern, dass sie einen anderen Job haben werden. Arbeitsmarkt-Ökonomen erwähnen in diesem Zusammenhang gerne, dass wir vor dreissig Jahren unseren Grosseltern noch nicht sagen konnten: «Ich werde SEO-Specialist» (Spezialistin für Search-Engine-Optimization / Suchmaschinenoptimierung, die Red.). Genauso wenig wissen wir heute, was die Jobs der Zukunft sein werden, weil es diese Jobs noch nicht gibt.

Was halten Sie von der digitalen Vermessung des Menschen, Stichwort «self tracking», «personal doctor». Ist das ein Segen oder eine Gefahr?

Technologie in der Medizin hat enormes Potenzial: Herzrhythmusstörungen oder Hautkrebs können frühzeitig und genauer diagnostiziert werden als mit herkömmlichen Methoden. Jeder, der dem Tod auf diese Weise entrinnt, wird die technologischen Errungenschaften wahrscheinlich als Segen empfinden. Für alle anderen kann es hilfreich sein, den eigenen Körper besser zu verstehen und persönliche Gewohnheiten bei Bedarf anzupassen.

In der Arbeitswelt ist Digitalisierung nicht mehr wegzudenken. Das sieht man auch hier, im Impact Hub in Zürich, wo viele Leute allein an ihren Laptops sitzen und arbeiten. Nur: Ist das wirklich besser als 1:1, im Team in einem Büro?

Die Tatsache, dass wir unsere Arbeit «allein» verrichten, ist nicht neu – aber die Werkzeuge haben sich geändert: Textilarbeit wurde in der Antike von Hand, im Mittelalter am Webstuhl und wird heute am Computer verrichtet. Teamarbeit ist ein Phänomen, das erst in den vergangenen Jahrzehnten starken Auftrieb erfahren hat und von der Digitalisierung begünstigt wurde – globale Zusammenarbeit ist heute sehr viel einfacher und schneller möglich: Ob man am Morgen mit Sydney, am Nachmittag mit New York oder am Abend mit Los Angeles per Video-Conferencing spricht – die Welt rückt näher zusammen.

Werden Lehrer im komplett digitalisierten Unterricht der Zukunft überflüssig?

Nein. Ich bin davon überzeugt, dass Lehrerinnen und Lehrer in absehbarer Zeit nicht verschwinden werden. Ihr Job dürfte sich weiter wandeln – laut dem renommierten deutschen Hirnforscher Gerald Hüther dürfte ihre Rolle als Potenzialentfalter an Bedeutung gewinnen. Zusätzlich zur Wissensvermittlung inspirieren, fördern und führen sie Schülerinnen und Schüler beim Lernen.

Was hat es mit der Vorstellung auf sich, der Lehrer werde vermehrt zum Coach im digitalen Raum, der vor allem in Kommentarsprechblasen am Bildschirm mit seinen Schülern kommuniziert? Die dank digitaler Vernetzung eines Tages vielleicht gar nicht mehr angewiesen sind auf ein Klassenzimmer?

Auch in der Schule der Zukunft werden neben fachlichen vor allem soziale Kompetenzen im Zentrum stehen, und die werden am besten vermittelt, wenn man mit anderen Schülern im Klassenverbund sitzt. Gerade im Zuge der Digitalisierung dürften soziale, emotionale und kreative Fähigkeiten wichtiger werden – weil diese in absehbarer Zukunft nur schwer von Maschinen übernommen werden können und daher vorerst dem Menschen vorbehalten bleiben dürften.

Sie haben Management Science and Engineering an der Stanford University studiert. Was machen die Amerikaner anders als wir?

Einiges! In den USA fragt man sich: «Können wir das? – und wenn ja, wie erreichen wir globale Skalierung, wie können wir damit Erfolg haben auf dem Weltmarkt?» Im deutschsprachigen Raum lautet die Frage häufig: «Dürfen wir das? – und wenn ja, in welcher Nische?» Beide Wege können zum Erfolg führen. Doch einfach loszulegen und in grossen Dimensionen zu denken, würde uns manchmal auch guttun.

Wie war das in Brasilien, wo Sie in einem Startup gearbeitet haben?

Das war sehr spannend. Dort kann man noch viel aufbauen. Als Studentin in Stanford im Silicon Valley hatte ich Tech-Luft geschnuppert, doch in São Paulo sass ich zum ersten Mal am Hebel. Meine Aufgabe als Head of Business Intelligence war es, herauszufinden, wie die Firma organisch wachsen kann, also ohne Geld zu investieren. Welche Felder ich dabei auch definierte – ich konnte mich immer gleich mit den Ingenieuren, den Produkt- oder den Community-Managern zusammensetzen, und sie konnten die nächsten Schritte gleich implementieren.

Kurze Entscheidungswege – eine dynamische Firmenkultur: Ist das der Grund, weshalb vor allem junge Menschen für Startups arbeiten?

Solche Strukturen sind sicherlich interessant. Schon kleinere Tech-Unternehmen können sehr viel bewirken. Wenn Sie einen Code oder eine Software einmal geschrieben haben, können Sie diese Produkte zehn, hundert oder tausend Nutzern zur Verfügung stellen. Man kann in diesem Geschäft sehr schnell eine grosse Reichweite erzielen. Das ist attraktiv für junge Berufsleute.

Machen wir einen weiteren Schritt zurück in Ihrer Biografie. Im Bachelor haben Sie Maschinenbau an der ETH Zürich studiert, als eine von wenigen Frauen. Hat Sie diese Erfahrung bestärkt in Ihrem weiteren Werdegang?

In der Schule hatte ich die Schwerpunktfächer Mathematik und Physik gewählt. An der ETH konnte ich mich in meinen Stärken entfalten. Ich war zwar eine von wenigen Frauen an der Hochschule, doch nach den ersten Wochen habe ich mich an die Situation gewöhnt. Ich hatte eine coole Zeit an der ETH: Die Schönheit von Logik und von Naturwissenschaften näher kennenzulernen – das ist unglaublich toll. Ich würde mir wünschen, dass mehr Frauen diesen Weg gehen.

Warum?

Weil es Spass macht – und weil Frauen die Fähigkeiten dazu haben. Wie es denn gewesen sei im Technikstudium – «als eine von wenigen Frauen»? Das höre ich häufig, und ich finde diese Fragen irritierend, weil sie suggerieren, dass man als Frau «nicht dazugehört». Man könnte sich denken: «Wow, ich entspreche offenbar nicht den Erwartungen. Hätte ich vielleicht etwas anderes machen sollen. . .?» Ich möchte vermeiden, dass Frauen aus solchen Gründen Mint-Fächer abwählen in der Schule oder sich im Studium gegen diesen Bereich entscheiden. Ich habe diese vermeintlich besondere Situation unter vielen Männern nie so wahrgenommen. Ich hatte sehr gute Freundschaften im Studium, zu Frauen wie zu Männern. Wir haben zusammen gelernt, zusammen vor Klausuren gezittert und danach zusammen gefeiert. Ich habe das viel mehr als ein Miteinander erlebt denn als ein Gegeneinander.

Trotzdem hält sich das Bild, dass Frauen eher davor zurückschreckten, sich durchsetzen zu müssen im Beruf. War das bei Ihnen anders?

Ich hatte vielleicht andere Voraussetzungen. Als wir in Bolivien lebten, waren wir in den Ferien meistens im Jeep unterwegs. Mein Vater hat uns Kindern gezeigt, wie der Motor funktioniert; ich hatte früh eine Nähe zur Technik. Unsere Eltern haben immer an uns geglaubt. Ein solches Umfeld hilft natürlich. Ich wünschte mir, dass mehr Menschen eine solche Unterstützung erfahren auf ihrem Weg – damit sie ihr Potenzial entfalten können, egal welches Geschlecht, welche Herkunft oder Hautfarbe sie haben.
Ehmann hat Maschinenbau an der ETH Zürich studiert. Sie sagt: «Die Schönheit von Logik und von Naturwissenschaften näher kennenzulernen – das ist unglaublich toll.»  Sie haben Vorstellungen erwähnt, die sich in der Gesellschaft reproduzieren. Ist es problematisch, wenn kleine Mädchen mit Puppen spielen?

Nein. Sie sollten mit Puppen spielen, wenn sie mit Puppen spielen möchten. Genauso sollten sie mit Lego etwas bauen, wenn sie mit Lego etwas bauen möchten. Frauen und Männer haben das gleiche Potenzial, und die Gesellschaft täte gut daran, ihr volles und nicht nur das halbe Potenzial auszuschöpfen.

Sprechen wir von We Shape Tech, einem Netzwerk, in welchem Sie sich für mehr Frauen in technischen Berufen einsetzen. Hören Sie oft den Vorwurf, dass solche Initiativen Frauen vor allem deswegen fördern, weil sie Frauen sind, und weniger, weil sie gut sind?

Das hört man immer wieder. Janina Kugel, die Personalchefin von Siemens, hat dies einmal wunderbar auf den Punkt gebracht: Wenn eine Frau in eine Rolle gehievt würde, nur weil sie eine Frau ist, wäre sie morgen wieder weg. Leistung zählt. Gerade heute, da wir immer mehr Wettbewerb zwischen den Besten weltweit haben. Von der Forschung wissen wir, dass die Zahlen weiblicher beziehungsweise männlicher Bewerber damit korrelieren, ob die in der Stellenbeschreibung verwendete Sprache weiblich oder eher männlich konnotiert ist. Frauen verhandeln auch ihr Salär genauso gut wie Männer – sofern sie wissen, dass sie verhandeln sollten. Anonyme Bewerbungsprozesse können ebenfalls helfen, mehr Frauen anzusprechen. Das Potenzial von Kandidatinnen ist da, die Leistung auch. Jetzt kommt es nicht zuletzt auf die Kommunikation von Firmen an.

Wie gehen Sie vor, um junge Frauen zu unterstützen in der Branche?

Zum Beispiel indem wir talentierten Frauen eine Bühne bieten. «Role model»-Veranstaltungen sind sehr wichtig: Man glaubt das, was man sieht. Wenn eine Schülerin von vierzehn Jahren nur eine Stunde im Jahr mit einer Chemikerin, einer Physikerin oder einer Mathematikerin verbringen kann, steigt die Wahrscheinlichkeit, dass dieses Mädchen später ein solches Fach studiert, um 30 Prozent. Das hat eine Harvard-Studie ergeben. Nur eine Stunde – das ist sehr wenig.

Geht es bei Ihren Treffen primär um Networking, oder verstehen Sie sich auch als Ort, um Fachwissen weiterzugeben?

Wir bauen auf vier Säulen auf. Vorbilder sichtbar machen, Wissen vermitteln, ein Netzwerk etablieren und Jobmöglichkeiten schaffen. Wir hatten neulich einen Workshop zu Machine-Learning mit Turicode, einem Startup, oder einen Coding-Event mit Tamedia, an dem jede ihren Laptop bei sich hatte und einen Algorithmus zur Bilderkennung trainieren oder ihre erste Website entwickeln konnte.

Die Themen Ihres Netzwerks gehen weit über We Shape Tech hinaus. Sind wir gerüstet für die Aufgaben der digitalen Zukunft – als Gesellschaft insgesamt?

Wir sind dabei, die Weichen richtig zu stellen. Immer mehr vernetzte Maschinen generieren immer mehr Daten, aber es gibt nicht genug Spezialisten, die diese Daten analysieren können. Neugierde und Faszination für die Möglichkeiten der digitalen Welt sind daher absolut zentral. Codes schreiben, etwas ausprobieren am Computer, die Erfahrung machen, dass Technik Freude bereitet – all das ist wichtig, auch für den technologischen Fortschritt, den Motor unseres Wirtschaftswachstums, der langfristig immer wieder neue Arbeitsplätze schaffen wird.";https://www.nzz.ch/schweiz/frauen-und-maenner-haben-das-gleiche-potenzial-ld.1524865;NZZ;Walter Hagenbüchle, Robin Schwarzenbach;;;
15.06.2018;Amerika und China teilen die Welt auf;"Es scheint, als leide die Weltpolitik an Drehschwindel. Der amerikanische Präsident brüskiert am Treffen von sieben führenden Wirtschaftsnationen seine Verbündeten, kurz darauf hofiert er den nordkoreanischen Diktator in Singapur. Zieht man alles ab, was bei der kanadischen Gipfel-Pleite auch eine Rolle gespielt haben dürfte – vor allem aufgeblasene und schnell beleidigte Politiker-Egos –, dann bleibt unter dem Strich eine Erkenntnis übrig: Der «Westen» und die G-7 verlieren an Bedeutung, während sich eine G-2-Welt etabliert mit Peking und Washington als Hauptakteuren.
Auf Augenhöhe mit Trump

Wie im Zeitraffer fassen die Tage zwischen den Begegnungen in Kanada und Singapur die tektonische Plattenverschiebung zusammen. Die USA fokussieren sich auf China, den geostrategischen Rivalen, den Konkurrenten um die Vormachtstellung im Zeitalter von künstlicher Intelligenz und Machine Learning, aber eben auch den engen Handelspartner. Nicht umsonst bemerkte Trump nach den Misstönen beim G-7-Gipfel spitz, die G-20 (mit China als Mitglied) sei ohnehin viel wichtiger.

Amerika und China bestimmen als G-2 zunehmend die grossen Fragen, ob Klimapolitik oder den Umgang mit Nordkorea. So sass in Singapur ein unsichtbarer Gast mit am Tisch – der chinesische Präsident Xi Jinping. Peking hält das Schicksal der Kim-Dynastie in seinen Händen; es ist Schutzmacht und in jüngster Zeit grollender Nachbar. China benötigt zwar auf der koreanischen Halbinsel einen Pufferstaat zum amerikanisch dominierten Süden, aber es fürchtet auch den Halbstarken mit Atomraketen, der Asien in einen Krieg zu stürzen vermag.

Weil es Kim Jong Un mit der Zurschaustellung seiner Atommacht übertrieb, setzte Peking die Uno-Sanktionen gegen Nordkorea durch. Trump verdankt Xi nicht wenig. Ohne chinesische Nachhilfe hätte Kim wohl kaum eine vollständige atomare Abrüstung seines Landes versprochen, auch wenn es sich nur um eine unverbindliche Absichtserklärung handelt und sie ihm überdies die ersehnte Anerkennung auf Augenhöhe einbringt. Der Deal lohnt sich auch für China. Trump kündigte ein Ende der gemeinsamen Militärmanöver mit Südkorea an und sprach vage von einem Abzug der US-Truppen. Beides sind seit je chinesische Anliegen.

    «Der «Westen», dieses Phänomen, das nach 1989 als Begriff Karriere machte, verschwindet nicht, aber er schrumpft.»

Washington und Peking mögen sich mit Zöllen piesacken, zugleich können sie effizient zusammenarbeiten. Jeder Vergleich mit dem einstigen Ost-West-Konflikt zwischen den USA und der Sowjetunion zielt daher ins Leere. Die Dynamik wird nicht von einem Rüstungswettlauf unter Androhung der gegenseitigen Vernichtung bestimmt, sondern von wirtschaftlicher und politischer Konkurrenz, in der niemand mit einem endgültigen Sieg rechnen kann. Wer glaubt noch, China lasse sich «totrüsten» oder ökonomisch in die Knie zwingen wie Moskau?

Zwar richtet sich der Globus nicht mehr ausschliesslich entlang der Kraftlinien zweier Supermächte aus wie im Kalten Krieg. Indien, Russland oder die düpierten Europäer, Kanadier und Japaner haben Gewicht und einen beträchtlichen Handlungsspielraum. Aber es ist eben keine multipolare Welt, wie sie Moskau seit dem Untergang der Sowjetunion imaginiert. Die Akteure sind nicht gleichberechtigt. Der «Westen», dieses Phänomen, das nach 1989 als Begriff Karriere machte, verschwindet nicht, aber er schrumpft. Nicht mehr der Atlantik, sondern der Pazifik ist das Meer der Meere und die Drehscheibe der G-2-Welt. Auf diese Entwicklung reagieren die beiden zentralen Protagonisten sehr unterschiedlich. Die traditionelle Landmacht China baut ihren Einfluss mit der Seidenstrassen-Initiative auf dem von Amsterdam bis Wladiwostok reichenden eurasischen Grosskontinent aus. Die USA, seit einem Jahrhundert die Herrin über die Meere, wollen ihren maritimen Vorsprung sichern.

Natürlich errichtet und finanziert Peking auch Häfen, ob in Pakistan, Sri Lanka oder Djibouti. Gleichwohl legt China nicht zuletzt aus innenpolitischen Gründen einen besonderen Akzent auf Zentralasien, das es mit Infrastrukturprojekten – mit Strassen, Eisenbahnen und Pipelines – durchdringt und wo das Reich der Mitte für viele Staaten bereits ein wichtigerer Handelspartner als Russland ist. China will mit der Initiative, die ebenfalls unter dem Namen «Road and Belt» firmiert, auch seine westlichen Provinzen Tibet und Xinjiang entwickeln. Beide sind wirtschaftlich rückständig und politisch unbotmässig, da lässt es Peking nicht an wohlstandsfördernder Bevormundung fehlen.

Der Aufstieg Chinas bedeutet das Ende der unangefochtenen Vormachtstellung Amerikas, weshalb die USA versuchen, mit Indien ein Gegengewicht zu schaffen. Zwar hinkt Indien hinter dem chinesischen Rivalen her, aber es gewinnt unaufhaltsam an Stärke, auch militärisch. US-Vertreter reden nie mehr nur vom Pazifik, sondern immer vom indopazifischen Raum. Die Botschaft ist unmissverständlich: Indien ist Teil der Machtbalance in Asien. Aber wie China hat Amerika beide Schauplätze im Blick – neben Indischem und Pazifischem Ozean auch das eurasische Herzland. Auf dessen äusserstem westlichem Zipfel, Europa geheissen, sind die USA derzeit stark vertreten, mit Truppen genauso wie mit Konzernfilialen.

Eurasien rückt in den Mittelpunkt, weil die natürlichen und die von Menschen geschaffenen Hindernisse an Bedeutung verlieren, die Europa, Russland, den Nahen Osten, Zentral-, Süd- und Ostasien lange Zeit trennten. Wenn die Eisenbahnprojekte fertiggestellt sind, gelangen Frachtcontainer bequem per Zug von den chinesischen Industriezentren ins Ruhrgebiet, schneller und billiger als auf dem Seeweg. Wo einst Grenzen waren, ist heute eine unermessliche Landmasse.

Wie China haben die USA das strategische Potenzial Eurasiens erkannt und werden daher allen Provokationen Trumps zum Trotz ihre Präsenz in Europa nicht aufgeben. Räumten sie diese Bastion, überliessen sie Russland und China ein Spielfeld, auf dem diese beiden Länder ohnehin besser positioniert sind. Nur die Europäer träumen noch ein bisschen. Wo heute der «Westen» allen Beobachtern wie selbstverständlich von der Zunge geht, wird dies in 15 Jahren vielleicht «Eurasien» sein. Beide Begriffe sind letztlich nur Konzepte, die zwar auf Interessen – und im Fall der transatlantischen Gemeinschaft auch auf Werten – fussen, die sich aber mit den Erfordernissen der Zeit verändern. Unveränderlich ist nur die Geografie.

Dass der amerikanische Präsident alle geopolitischen Folgerungen bedachte, als er Justin Trudeau, den Gastgeber des G-7-Gipfels, in einem Tweet der Lüge zieh und ihn unehrenhaft und schwach nannte, darf bezweifelt werden. Aber die strategischen Veränderungen bilden das Fundament des von Trump auf reichlich indezente Weise geäusserten amerikanischen Selbstbewusstseins. Die USA haben bessere Karten als alle anderen westlichen Nationen, um in dem Powerplay mit China mitzuhalten.
Partner und Rivalen

Ob es in 15 Jahren noch die G-7 geben wird, ob Nordkorea bis dahin nuklear abgerüstet hat und wann der letzte Vorhang für Trumps turbulente Vaudeville-Show fällt, lässt sich heute allenfalls erahnen. Bis dahin werden noch viele Kulissen auf der grossen Theaterbühne verschoben. Aber mit Sicherheit sind die USA dann in Europa wie in Ostasien noch immer eine Macht, mit der man rechnen muss. Und die sino-amerikanische Dominanz prägt die internationalen Beziehungen.

Trumps Asienpolitik steckt voller Widersprüche und lässt dennoch eine Linie erkennen. Der von ihm angeordnete Rückzug aus dem pazifischen Freihandelsabkommen war ein Eigengoal und nützt nur China. Das Treffen mit Kim hingegen ist für den Präsidenten ein Erfolg; er kann sich im Jahr der Zwischenwahlen als Macher inszenieren, und er unterstreicht den amerikanischen Gestaltungsanspruch in Ostasien. Peking wird allerdings dafür sorgen, dass die Bäume Washingtons in der Region nicht in den Himmel wachsen. Im Übrigen vertraut es auf die Überlegenheit seiner auf lange Zeiträume angelegten Strategie über das kurzfristige und manchmal erratische Handeln westlicher Politiker. In der G-2-Welt ist es eben von der Kooperation zur Konfrontation nie weit.";https://www.nzz.ch/meinung/amerika-und-china-teilen-die-welt-auf-ld.1395054;NZZ;Eric Gujer;;;
27.09.2019;Der Mensch ist das einzige Tier, das sich selber zum Problem wird und dauernd an sich scheitert;"Vielleicht ist dem Menschen nicht zu helfen. Nicht nur, wenn es um Klimawandel, Krieg und Flüchtlingskatastrophen geht. Ganz grundsätzlich. Auch wenn man den Eindruck haben sollte, es stehe für einmal alles zum Besten: Sogar dann fehlt dem Menschen etwas, und das spürt er selber auch. Weil er das Wesen ist, dem immer etwas fehlt. Das Wesen, dem nicht zu helfen ist. Obwohl er fast alles kann: Am Ende scheitert der Mensch wieder, und zwar an sich.

Niemand wird leugnen, dass er sich auf dieser Erde eine einmalige Position geschaffen hat. Seine Überlegenheit ist so imposant wie erdrückend. Und auch wenn der Begriff «Krone der Schöpfung» heute bestenfalls noch als ironische Wendung durchgeht: Dass er sich als Herr der Welt versteht, macht der Mensch überall deutlich, wo er sich zeigt. Er diktiert sein Gesetz allem, was auf der Erde lebt, auch wenn er ganz genau weiss, dass er das eigentlich gar nicht kann, weil sich die Erde nie vollends beherrschen lässt.

Er behandelt andere Lebewesen, als ob ihm die alleinige Verfügungsgewalt über sie verliehen wäre. Er hat die Gabe des Mitgefühls und erhebt als einziges Lebewesen den Anspruch auf eine unantastbare Würde, aber kann sich, auch gegenüber seinesgleichen, unfassbar verächtlich verhalten. Er weiss immer mehr von der Welt und versteht sie doch immer weniger.
Die Begabung zur Vernunft

Zumindest eines aber müsste der Mensch gelernt haben: dass sich seine Disposition den Zufällen einer Natur verdankt, deren Gesetze er nur annähernd durchschaut. Und dass er nie wirklich weiss, was er auslöst, wenn er sich anschickt, diese Natur nach seinem Willen zu formen. Trotzdem greift er dort, wo er kann, bedenkenlos in sie ein. Damit gefährdet er auch sein eigenes Überleben, aber das scheint ihn nicht zu kümmern.

Ein widersprüchliches Wesen also. «Was ist der Mensch?», hat Kant vor mehr als zweihundert Jahren gefragt, und die Frage ist nicht ungehört verhallt. Im Gegenteil, es wimmelt von Antworten, und im Einzelnen mögen sie durchaus aufschlussreich sein. Nur, jede Antwort schafft neue Fragen. Aufs Ganze gesehen, fallen die Versuche, zu sagen, was dieser Mensch denn nun eigentlich sei, verwirrend vielfältig aus.

Vielleicht hängt es auch damit zusammen, dass es in den letzten Jahrzehnten still wurde um die philosophische Anthropologie. Gewiss, das hatte auch historische Gründe. Die Greuel des Zweiten Weltkriegs waren Beweis dafür, dass der Mensch zu allem fähig ist und man vielleicht am besten beraten wäre, sich für ihn gar keine Hoffnungen zu machen. Humanität, Humanismus? Selbst der Holocaust hatte gezeigt, dass es Menschen gab, die sich diesen Werten verpflichtet fühlten und bereit waren, bis zum Äussersten zu gehen, um sie nicht zu verraten. Aber es war auch klargeworden, dass sie denen unterliegen, die alles mit Füssen treten, was als menschlich gelten müsste.
Emotionale Störmanöver

Trotz allem, was die philosophische Bestimmung des Menschen betrifft, schien das Grundsätzliche seit längerem geklärt und war selbstverständlich geworden: Seine politische Natur, die Sprache und die Begabung zur Vernunft boten ein Raster, in dem man alles Menschliche mehr oder weniger ausreichend verorten konnte.

Dass die Frage nach dem Wesen des Menschen nun wieder gestellt wird, und zwar mit erhöhter Dringlichkeit, ist kein Zufall. Machine Learning und künstliche Intelligenz zwingen uns dazu, genauer zu bestimmen, was es mit der vielgepriesenen Vernunft auf sich habe – zu fragen, was das spezifisch Menschliche an ihr sein soll. Und ob das Kennzeichen der menschlichen Intelligenz vielleicht gerade darin liegt, dass sie nicht nur vernunftgeleitet ist, sondern darauf beruht, dass rationale Prozesse immer wieder durchkreuzt werden von emotionalen Störmanövern.

Das ist nicht die einzige Herausforderung, vor der die Philosophie vom Menschen steht: Transhumanisten sehen die Zukunft des Homo sapiens in der Verquickung von Mensch und Maschine und der Überwindung des Todes. Antispeziesisten fordern gleiche Rechte für alle Lebewesen, was der Daseinsform Mensch, wie wir sie pflegen, die Grundlage entziehen würde. Und schliesslich führen uns Umweltprobleme, Klimawandel und Genforschung vor Augen, wie tiefgreifend wir die Welt verwandelt haben – und dass wir weit davon entfernt sind, mit den Konsequenzen unseres Tuns zurande zu kommen.
Ein Tier, das seine Gründe hat

Das alles schwingt mit, wenn Volker Gerhardt den Versuch unternimmt, zu bestimmen, was den Menschen ausmacht. Mit seinem Buch «Humanität. Über den Geist der Menschheit» zieht der deutsche Philosoph die Summe aus einem jahrzehntelangen Kreisen um Probleme des Verhältnisses von Vernunft und Interesse, Glaube, Wissen, Wille und Macht – hinter denen die grossen Fragen lauern: was wir Menschen sind und wie wir die Welt verstehen. Wie wir uns selber überhaupt verstehen können, wie wir uns verstehen wollen – und welchen Platz wir uns einräumen in einer Welt, die nicht nur die unsere ist.

Gerhardt erschliesst das Terrain über grosse Transversalen. Er fasst den Menschen über seine grundlegenden Tätigkeiten. Natürlich über die Technik, mit deren Hilfe er die Welt bewohnbar macht – als Wesen, das Teil der Natur ist, von seiner physischen Ausstattung her aber schlechter als jedes andere für ein Leben in ihr gerüstet ist. Aber ebenso über die soziale Organisation, über das Suchen und Fragen, das Unterscheiden und Abgrenzen – wozu auch das Nein-Sagen gehört. Über das Spielen, das Schaffen von Neuem und den Bezug auf den Mitmenschen, ohne den kein Mensch Mensch sein kann. Aber auch über das Töten, Vernichten, Zerstören.

So ergibt sich ein ebenso perspektivenreiches wie tiefenscharfes Bild. Es ist Volker Gerhardts grosse Leistung, dass er die einzelnen Aspekte zu ihrem Recht kommen lässt und zugleich zeigt, dass sie sich nur im komplexen Zusammenspiel zu dem formen, was den Menschen prägt. Gegen alle modische Rationalitätsskepsis hält er daran fest, den Menschen auf ein Handeln zu verpflichten, das sich in Vernunft begründet. Sein Begriff vom Menschen als «Tier, das seine Gründe hat», ist Verpflichtung und Anspruch zugleich. Ein Anspruch, den die Menschheit immer wieder übertrifft, auch wenn sie im Ganzen daran scheitert. Der Mensch weiss sich zu helfen. Aber er dürfte das einzige Lebewesen sein, das sich selber zum Problem macht. Und mit sich selber nie fertig wird.";https://www.nzz.ch/feuilleton/was-ist-der-mensch-das-tier-das-sich-selber-zum-problem-wird-ld.1511577;NZZ;27.09.2019;;;
06.08.2019;China hat den Code des Silicon Valley geknackt;"Es gibt viele Dinge, die man den Wagniskapitalgebern ankreiden kann. Doch was sich Peter Thiel neulich in einer Kolumne in der «New York Times» geleistet hat, übertrifft alles. Er greift in seinem Text unter anderem Google an, weil das Unternehmen ein Forschungszentrum in China aufbaut.

In der derzeitigen Handelskrieg-Rhetorik sorgt allein das Wort China für starke Emotionen. Thiel nutzt das gezielt. Er zeichnet ein Bild des Kalten Krieges 2.0, in dem sich China und die USA um die Vorherrschaft in der künstlichen Intelligenz streiten. Da jegliche Forschung in China von der Armee kontrolliert werde, helfe Google damit indirekt der Volksrepublik, meint Thiel.

Weiter behauptet er, dass die Fortschritte der künstlichen Intelligenz vor allem in militärischen Anwendungen zum Tragen kommen würden. Es ist ungewöhnlich, dass ein Mann, der einst zum Kern des Silicon Valley gehörte und hier auch seine Milliarden verdient hat, solch engstirnige Aussagen macht.

Bemerkenswert ist allerdings auch, dass Thiel dabei nicht einmal erwähnt, dass er selber mit seinem Portfoliounternehmen Palantir Technologies sehr dick mit dem Militärapparat des Pentagons vernetzt ist. Er nutzt also die Kolumne, um einen Werbetext für eines seiner Unternehmen zu verbreiten. Dieses Verhalten ist selbst im von Eitelkeit und intellektueller Arroganz durchtränkten Silicon Valley bemerkenswert.

Doch die wirkliche Gefahr hier ist nicht Thiel. Er ist schliesslich ein reicher Mann, der sich solche Meinungen leisten kann. Problematisch ist hier die falsche Wahrnehmung der Verhältnisse im Technologiesektor. Es geht dabei freilich nicht um einen Kalten Krieg 2.0, den man mit der Rhetorik der sechziger Jahre bekämpfen könnte. Natürlich stehen China und die USA in einem Konkurrenzkampf, wenn es um die künstliche Intelligenz geht.

Doch die Welt der Innovation hat sich seit dem nuklearen Wettrüsten stark verändert. Es sind nicht mehr staatlich kontrollierte Forschungsstellen, die den Fortschritt antreiben, sondern Tausende von kleinen Startups bringen mit unternehmerischer Energie das Gebiet weiter. Dass dabei sehr viel in China stattfindet, kommt zum einen daher, dass zahlreiche Softwarespezialisten von den Universitäten Chinas und aus Übersee in das Gebiet der künstlichen Intelligenz eintreten.

Zum anderen hat China ähnlich wie das Silicon Valley einen fruchtbaren Boden geschaffen für Startups, die in einem Ökosystem von Venture-Capital, akademischer Forschung und unternehmerischem Nachwuchs gedeihen. Diese Entwicklung kann weder eine Kolumne von Thiel noch ein Dekret von Präsident Trump stoppen.

Chinas Venture-Zentren in Schanghai, Peking und Shenzhen sind die ersten ernsthaften Konkurrenten des Silicon Valley. Das geben die führenden Vertreter des Valley selber zu. Und sie wissen auch, wie man so einen Wettbewerb am besten führt. Man schottet sich nicht ab, sondern bleibt offen und versucht sich gegenseitig voranzutreiben.

China ist sehr gut in der Anwendung von massiven Datenmengen zur Verbesserung von Machine-Learning-Algorithmen. Kalifornien ist immer noch die Vormacht, wenn es um die Entwicklung und das Design von neuen Softwarekonzepten geht. Ein sorgfältiger Austausch mit Fokus auf die eigenen Stärken ist wohl langfristig viel produktiver als das Wiederkäuen der Rhetorik des Kalten Krieges.";https://www.nzz.ch/finanzen/china-hat-den-code-des-silicon-valley-geknackt-ld.1500105;NZZ;Krim Delko;;;
07.09.2018;KI mit Empathie;"Es gibt in den Köpfen der Menschen verschiedene Vorstellungen von der Realität, und diese Vorstellungen müssen mit der Realität nicht übereinstimmen: Um das zu verstehen, muss ein Kind einen grossen Entwicklungsschritt tun. Irgendwann zwischen drei und fünf Jahren entwickelt es eine sogenannte Theory of Mind und ist so in der Lage, Annahmen zu treffen über das, was in den Köpfen anderer Menschen vor sich geht.

Das kognitionswissenschaftliche Konzept der Theory of Mind ist jahrzehntealt. Zoologen, die Menschenaffen studierten, oder Psychologen, die sich mit autistischen Kindern beschäftigten, haben es benutzt und weiterentwickelt. Jetzt beginnen auch Computerwissenschafter, die im Bereich der künstlichen Intelligenz tätig sind, sich dafür zu interessieren. Sie haben Empathie, die Fähigkeit, sich in andere Menschen einzufühlen, als wichtigen Aspekt von Intelligenz erkannt.
Suchen nach Schokolade

Das ist der Grund, warum Alexa, Cortana, Siri und der Google Assistant immer wieder enttäuschen: Diese Programme verstehen gesprochene Befehle, aber nicht den Kontext, in dem der menschliche Gesprächspartner seine Wünsche formuliert. In der Psychologie gibt es einen einfachen Test, um festzustellen, ob ein Kind über eine Theory of Mind verfügt. Dem Kind wird eine Geschichte erzählt: Hans hat einen Schokoladenriegel in die Schachtel A getan. Nachdem er den Raum verlassen hat, packt ein anderes Kind den Riegel in Schachtel B. Hans kommt zurück: Wo wird er nach der Schokolade suchen, in Schachtel A oder Schachtel B? Kleine Kinder werden auf B zeigen, grössere, die sich in Hans hineinversetzen können, tippen auf A.

Forscher von Google und der Google-Tochter Deepmind haben den Versuch unternommen, einer Software Empathie beizubringen. Das Forscherteam um Neil Rabinowitz hat unterschiedliche Bewohner für eine Gridworld genannte Simulation eines Ökosystems programmiert. Gridworld repräsentiert ein schachbrettartiges Spielfeld, in dem simple Regeln über Gedeih und Verderb von künstlichen Organismen bestimmen. Ein neuronales Netzwerk übernahm die Aufgabe, sich in die verschiedenen Gridworld-Software-Agenten hineinzuversetzen. Der Versuch, einer Maschine zu einer Theory of Mind zu verhelfen, sei eine «grand challenge» und stelle eine der grossen Forschungsfragen im Bereich der künstlichen Intelligenz dar, schreiben die Forscher. «Wir versuchen nicht, das alles zu lösen.» Wohl aber wolle man erste einfache Schritte auf dem Weg zu einer Lösung wagen.

Wie kann man die empathischen Fähigkeiten einer Software messen? Mit dieser Frage beschäftigen sich die Computerwissenschafter um Erin Grant von der University of California in Berkeley. Bei ihrer jüngsten Arbeit wurden sie von der Psychologin Alison Gopnik unterstützt, die seit Ende der 1980er Jahre in Berkeley forscht und lehrt.
Überforderte Maschinen

Das Team hat ein Set von Fragen und Antworten entworfen, mit dem es möglich sein soll, zu erkennen, ob ein neuronales Netzwerk über Ansätze einer Theory of Mind verfügt. In der jüngsten Arbeit des Teams werden auch Fragen berücksichtigt, bei denen es darum geht, was ein Mensch glaubt, dass ein anderer über einen Dritten denkt. Bei Tests zeigte sich, dass gängige Modelle von neuronalen Netzwerken mit diesen Fragen, die jeder Mensch problemlos beantworten kann, überfordert sind.";https://www.nzz.ch/digital/ki-mit-empathie-ld.1418190;NZZ;Stefan Betschon;;;
16.09.2020;Vor den Galapagosinseln, Iran oder Nordkorea: Die chinesischen Schiffe fischen, wo es ihnen gefällt. Das steckt dahinter;"Das Meer ist voller roter Punkte. Jeder einzelne Punkt steht für ein chinesisches Schiff. Sichtbar wird das auf digitalen Karten, die internationale Schiffsbewegungen über Satellitensignale der einzelnen Schiffe abbilden.

Eine Person, die sich für solche Karten interessiert, ist die Aktivistin Veronica Llanes aus Ecuador, die sich für den Schutz der Galapagosinseln einsetzt. Täglich brütet die 28-Jährige auf Websites wie Global Fishing Watch und Vessel Finder über einer dort erkennbaren Ansammlung chinesischer Hochseeschiffe vor den Galapagosinseln, wie sie am Telefon erzählt. Dass es chinesische Schiffe sind, weiss sie, weil ihr lokale Fischer Fotos der imposanten Schiffe geschickt haben. Auch Bilder von PET-Flaschen mit chinesischen Schriftzeichen, die es in Ecuador und auf den Galapagosinseln an den Strand gespült hat, erreichten sie. Llanes taucht gerne, sie sagt: Nirgends sonst auf der Welt gebe es eine solche Artenvielfalt unter Wasser, so viele geschützte Meerestiere, die den chinesischen Netzen in die Fänge gehen könnten. «Ich muss handeln», habe sie sich gesagt.

Von Mitte Juli bis Anfang September fischten Hunderte von chinesischen Schiffen in der Nähe der Galapagosinseln. Umweltschützer fürchteten, das Ökosystem könnte Schaden nehmen. China sagte: Wir fischen legal. Hochseefischen in internationalen Gewässern darf jeder. «Fernfischerei» nennen das die Fachleute – das Fischen ausserhalb der eigenen Wirtschaftszone. Die Fernfischerei ist kaum reguliert und schwierig zu überwachen (siehe Box unten). Dabei sind 90 Prozent der Fischbestände laut der Welternährungsorganisation der Uno bereits ausgeschöpft. Gleichzeitig ist Chinas Fernfischereiflotte in den letzten Jahren stetig gewachsen. Heute ist China die führende Fischereination der Welt. Im Südchinesischen Meer, vor Westafrika, Iran und laut Studien heimlich vor Nordkorea – weltweit fischen die Chinesen. Das führt zu Konflikten. Doch China ist nicht das einzige Problem.
Die chinesische Fernfischereiflotte ist auf heimischer, fremder und internationaler See unterwegs – wie hier in Gewässern vor der südchinesischen Provinz Guangdong.  Über 300 Schiffe vor den Galapagosinseln

Riesenschildkröten, Leguane, Albatrosse, Finken – auf den Galapagosinseln tummeln sich viele seltene Tiere. Sie haben Charles Darwin zu seiner Evolutionstheorie inspiriert. Unter Wasser sieht es ebenso bunt aus. Unterschiedliche Meeresströmungen sorgen für ideale Temperaturen und nährstoffreiches Wasser. Etwa 2900 verschiedene Arten von Meerestieren kommen deswegen aus dem ganzen Pazifik zu den Küstengewässern um die Galapagosinseln, unter ihnen Haie, Seelöwen, Delphine.

Die reiche Meeresfauna lockt die Fischer an. Ecuador hat deswegen 1998 ein Naturschutzgebiet in einem Radius von 40 Seemeilen (zirka 74 Kilometer) um die Galapagosinseln gezogen. Die industrielle Fischerei ist in diesem Gebiet verboten. Die lokalen Fischer fischen dort trotzdem heimlich.

Um das Naturschutzgebiet der Galapagosinseln zieht sich 200 Seemeilen weit die ausschliessliche Wirtschaftszone (AWZ) Ecuadors. Die AWZ basiert auf dem Seerechtsübereinkommen der Uno. Ausländische Fischer brauchen eine Genehmigung des jeweiligen Landes, um in dessen AWZ zu fischen. Da die Galapagosinseln mehr als 900 Kilometer vom Festland entfernt sind, bleibt zwischen der AWZ der Inseln und derjenigen des ecuadorianischen Festlandes ein Streifen internationales Gewässer. Dort hatten sich im Juli und August mehr als 300 chinesische Schiffe stationiert. Die Fläche der Flotte war zeitweise grösser als jene der Galapagosinseln. Die Mehrheit der chinesischen Schiffe vor den Galapagosinseln fängt Tintenfische. Eine Beobachtungsmission der ecuadorianischen Marine Anfang August schätzte, dass jedes der Schiffe ungefähr 1000 Tonnen Fracht tragen könne. Die Flotte kann also etwa 300 000 Tonnen Fisch aus dem Wasser holen. Die Marine fand ausserdem heraus, dass 149 der chinesischen Schiffe zeitweise ihr Identifikationssystem ausgeschaltet hatten, damit sie nicht geortet werden konnten.

Der Meeresschützer John Hourston sagt am Telefon: «Diese vielen Schiffe, die über Wochen um die Galapagosinseln herum fischen, sind eine grosse Belastung für das Ökosystem.» Der ehemalige Fischer arbeitet freiwillig bei der Blue Planet Society, einer britischen Organisation, die gegen Überfischung und Umweltverschmutzung der Weltmeere kämpft. «Meine grösste Sorge ist, dass vor den Galapagosinseln verdeckte illegale Aktivitäten stattfinden», sagt Hourston.

Im Jahr 2017 haben ecuadorianische Behörden im Laderaum eines chinesischen Schiffs 6600 tote Haie gefunden, die meisten von ihnen bedrohte Hammerhaie. Im Mai dieses Jahres fanden Zollbeamte in Hongkong zwei Container aus Ecuador voller Haifischflossen von über 38 000 geschützten Haien. Wer sie gefischt hat, geht aus den Berichten nicht hervor. Die Aktivistin Veronica Llanes machen solche Berichte misstrauisch gegenüber der chinesischen Flotte vor den Galapagosinseln. «Wir wissen nicht, was sie fangen», sagt sie. Deswegen hat Llanes eine Petition gestartet mit dem Ziel, Druck auf die ecuadorianische Regierung aufzubauen, um die Fernfischerei nahe den Galapagosinseln zu stoppen. Eine halbe Million Menschen auf der ganzen Welt sind dem Aufruf bisher gefolgt.

Peking hat auf das Misstrauen Ecuadors und des übrigen Auslands reagiert. Die chinesische Botschaft in Ecuador erklärte, die Vorwürfe der illegalen Fischerei entbehrten jeder Grundlage. Die Schiffe vor den Galapagosinseln operierten legal und bedrohten niemanden. Ausserdem seien die Beziehungen zwischen China und Ecuador hervorragend.

In der Tat ist das hochverschuldete Ecuador abhängig von China. Das Land lebt vom Erdöl, und China kauft es. Die Verträge zwischen den beiden Staaten laufen über Jahre, und Ecuador erhält dafür Darlehen von China in Milliardenhöhe. Ecuador exportiert auch über die Hälfte seiner Crevettenproduktion nach China.
Chinas Geheimverträge gehen zulasten von Kleinfischern

Chinesische Fischereischiffe finden sich nicht nur vor der südamerikanischen Küste, sondern auf der ganzen Welt. Sie sind vor allem im Pazifik sowie in den Gewässern vor Ost- und Westafrika unterwegs. Neben China fischen aber auch andere Länder ausserhalb ihrer eigenen Wirtschaftszone. Forscher des Stimson Center fanden heraus, dass China von 2015 bis 2017 für knapp 60 Prozent des globalen Fischfangs verantwortlich war.

Chinas Fernfischereiflotte stelle fast 17 000 Schiffe, schätzt das Overseas Development Institute für die Jahre 2017 bis 2018. Die Fernfischereiflotte der EU-Staaten habe im Jahr 2019 223 Schiffe gehabt, schreibt die Coalition for Fair Fisheries Arrangements. Auch in Bezug auf die ganze Fischereiflotte hat China nach den Schätzungen von Global Fishing Watch bei weitem die meisten Schiffe. Dabei fischen Chinas Fernfischer nicht nur in internationalen Gewässern. Einige Länder erzielen hohe Einkünfte damit, dass sie China Fischereirechte in ihrer Wirtschaftszone abtreten. Auf diese Weise gefährden diese Regierungen die Lebensgrundlage heimischer Fischer. Ein Beispiel dafür ist Iran. Dort beschweren sich die Fischer seit einigen Jahren über Chinesen, die in iranischen Gewässern fischen. «Ich habe immer 60 bis 70 Kilogramm Fisch pro Tag gefangen», sagt ein Fischer gegenüber einem lokalen Fernsehsender. «Heute ist nichts mehr übrig.» Die Schuld gibt er den Chinesen. Berichte von lokalen Fischern, die sich über chinesische Schiffe in ihren Gewässern beschweren, gibt es weltweit. Laut dem Meeresbiologen Daniel Pauly, Professor an der University of British Columbia, schliesst China mit vielen Ländern Fischereiverträge ab. Das geschehe jedoch meist sehr diskret. «Wenn chinesische Schiffe auftauchen, denken die lokalen Fischer natürlich zunächst, diese seien illegal dort», erklärt Pauly. Der Forscher gilt als führender Fischereiexperte. Er hat jahrelang zum Einfluss der Menschen auf den Ozean geforscht und dabei bewiesen: Wenn die kommerziellen Fischereien so weitermachen, rotten sie nicht nur die Fischbestände aus, sondern schaffen sich auch gleich selbst damit ab.

Eine besonders schädliche Methode ist zum Beispiel das Fischen mit Grundschleppnetzen. Trawling wird das im Jargon genannt. Dabei wird ein riesiges, trichterförmiges Netz über den Meeresboden gezogen. Umweltexperten beklagen, dass hierbei grosse Meerestiere wie Delphine gefangen würden und der Meeresboden zerstört werde. Trawling ist in iranischen Gewässern nicht erlaubt. Iranische Fischer sagen, China halte sich nicht an das Verbot.

Dass China des Trawling bezichtigt wird, kommt nicht von ungefähr. Von allen 152 Küstenstaaten belegt China auf dem Index der illegalen, nicht gemeldeten und nicht geregelten Fischerei (IUU) den ersten Platz. Der IUU-Index wird von der norwegischen Regierung finanziert, dahinter stecken eine internationale Beratungsfirma und eine Nichtregierungsorganisation mit Sitz in Genf. Auch China führt einen IUU-Index. Darauf seien chinesische Fischereien seit 2013 nicht mehr erschienen, berichtet die regierungstreue Zeitung «Global Times».
China will Fisch essen – und Präsenz markieren

Die Fernfischerei schädigt die Ozeane und bedroht die Lebensgrundlage lokaler Fischer. Gleichzeitig sind viele der industriellen Fangmethoden wie das Trawling sehr teuer. Bringt das Fischen ausserhalb der eigenen Gewässer den Ländern also satte Gewinne? Eine Studie von Forschern von Global Fishing Watch, National Geographic Society und verschiedenen Universitäten aus dem Jahr 2018 zeigt: Über die Hälfte der Fernfischerei rentiert nur dank staatlichen Subventionen. China, Japan oder Spanien stecken jährlich Hunderte von Millionen Dollar in die Fernfischerei.

Ein Grund für die staatlichen Subventionen ist die weltweit wachsende Nachfrage nach Fisch. Allein durch Fischzuchten und den Fang aus heimischen Gewässern kann diese nicht gedeckt werden. China hat fast 1,4 Milliarden Menschen zu ernähren. Seit den späten 1970er Jahren essen die Chinesen mit zunehmendem Wohlstand immer mehr Fisch. Chinas Landwirtschaftsministerium ging 2015 von 14,3 Kilogramm Fisch und Meeresfrüchten jährlich pro Kopf für die städtische Bevölkerung aus. Der weltweite Durchschnitt liegt knapp unter 20 Kilogramm. Der Meeresschützer Hourston sagt, hinter den Subventionen für die Fernfischerei Chinas stünden auch geopolitische Interessen. «Ein Gebiet mit Fischereiflotten zu kontrollieren, ist Machtprojektion mit nichtmilitärischen Mitteln.» Das gilt ganz besonders für das Südchinesische Meer, wo es immer wieder zu Zusammenstössen der chinesischen Flotte mit Fischerbooten aus den Philippinen und aus Vietnam kommt. China beansprucht dort einen Grossteil der rohstoffreichen Gebiete, obwohl ein internationales Schiedsgericht 2016 diese Ansprüche zurückwies.
«Es wäre das Beste, die Fischereisubventionen zu verbieten»

Die hohe Nachfrage und geopolitische Interessen machen es schwer, die Probleme der lokalen Fischer zu lösen und das Ökosystem der Meere zu schützen. Dabei gibt es Ideen, wie die Fernfischerei reduziert werden könnte.

Im Fall der Galapagosinseln beispielsweise fordern Umweltaktivisten, dass Ecuador die AWZ ausweitet. Wenn das gesamte Gebiet zwischen dem Festland und den Inseln unter der Kontrolle Quitos wäre, könnten die wandernden Meerestiere besser geschützt werden, argumentieren sie. Doch die AWZ können nicht eigenmächtig von einem Land vergrössert werden, und dass alle Länder sich darauf einigen, ist unrealistisch. Zudem könnten lokale und – falls Ecuador entsprechende Verträge abschliesst – auch fremde Fischer weiterhin dort fischen.

Vielen Entwicklungsländern wie Ecuador fällt es schwer, ihre Gewässer gründlich zu überwachen. «Ecuador braucht Hilfe», sagt der Meeresschützer Hourston. «Es fehlt dem Land schlicht an Ressourcen, um das Weltkulturerbe der Galapagosinseln zu schützen.» Der Fischereiexperte Pauly fordert: «Es wäre das Beste, die Fischereisubventionen zu verbieten.» Eine entsprechende Vorgabe sollte bereits einmal in die «Ziele für nachhaltige Entwicklung» der Uno aufgenommen werden. Ein konkreter Vorschlag liegt vor, unterzeichnet von 91 Uno-Mitgliedsstaaten. Doch die Staatenwelt hat sich bis heute nicht einigen können.

Für die Fische in den ecuadorianischen Gewässern gibt es allerdings etwas Hoffnung: Ecuador hat sich bei China beschwert, wie ein Brief der Regierung Ecuadors als Reaktion auf die Petition der Aktivistin Llanes zeigt. Sie feiert den Erfolg. China hat für die Region um die Galapagosinseln von September bis November ein dreimonatiges Fischerei-Moratorium verhängt. Ecuador möchte ein permanentes Moratorium durchsetzen, berichtete die spanische Nachrichtenagentur EFE. Ein Blick auf die aktuellen Karten zeigt: Der rote Fleck verschiebt sich langsam von den Galapagosinseln weg. Die chinesischen Schiffe ziehen ab – zumindest vorläufig.";https://www.nzz.ch/international/galapagos-inseln-iran-nordkorea-china-fischt-wo-es-will-ld.1573141;NZZ;Vanessa Möller, Katrin Büchenbacher, Adina Renner;;;
08.11.2018;Ein diskreter Aktivplayer;"Es gibt nicht viele Unternehmen, die den Erfolg des amerikanischen Kapitalismus besser verkörpern als Goldman Sachs. Insbesondere die Sparte Investment-Banking gilt als führend, im letzten Jahr hat beispielsweise keine andere Bank mehr Fusionen und Übernahmen arrangiert als die US-Grossbank. Im amerikanischen Finanzsystem hat Goldman Sachs eine herausragende Stellung, und viele ehemalige Mitarbeiter haben auch nach ihrer Bankkarriere beachtenswerte Aufgaben übernommen. Malcolm Turnbull, bis im August australischer Premierminister, Romano Prodi, ehemaliger italienischer Ministerpräsident, und Henry Paulson, früherer US-Finanzminister, sind einige prominente Beispiele.

Der Erfolg von Goldman Sachs bringt allerdings nicht nur Vorteile mit sich. Um die Bank ranken sich zahlreiche, zum Teil abstruse Gerüchte. So können sich Verschwörungstheoretiker im Internet darüber informieren, wie die Grossbank zusammen mit ehemaligen Angestellten die Weltherrschaft an sich reissen will. Der zuständige Mann für Europa sei der Ex-Mitarbeiter Mario Draghi, wobei es auch Stimmen gibt, die sagen, dass Goldman Sachs gar nicht daran interessiert sei, die Weltherrschaft an sich zu reissen, weil und solange die US-Bank die globale Wirtschaft kontrolliere.
Lang währende Präsenz in der Schweiz

Vor diesem Hintergrund und weil das Investment-Banking so prominent ist, gerät manchmal etwas in den Hintergrund, dass die Grossbank auch über ein bedeutendes Investment-Management verfügt. Das Asset-Management als Teil davon hatte im dritten Quartal 2018 über 1000 Mrd. $ Assets under Supervision, wobei Goldman Sachs die Grösse durch weitere Zukäufe steigern will, um die Skalenvorteile zu erhöhen. In der Schweiz beschäftigt Goldman Sachs Asset Management (GSAM) noch ein eher kleines Team mit rund einem Dutzend Mitarbeiter, die Tendenz ist allerdings steigend.

Zudem verfügt Goldman Sachs über ein lokales Investment-Banking-Team, wobei die Grossbank bereits seit 1974 in der Schweiz vertreten ist; Zürich war eine der ersten Auslandsvertretungen der Amerikaner und ist Sitz einer eigenen Tochterfirma. Über die Goldman Sachs Bank AG wickelt der Asset-Management-Arm der US-Grossbank seine Geschäfte in der Schweiz ab. Im Moment sondiert die US-Bank zudem den Einstieg in den Schweizer Hypothekenmarkt. Dabei soll der Fokus auf Kunden liegen, die Mühe haben, einen Kredit für ein Eigenheim zu erhalten.

Das Gebäude, in dem sich der Zürcher Sitz von Goldman Sachs befindet, zeugt von der diskreten Ausrichtung und von einem zurückhaltenden Auftritt. Wer die Claridenstrasse Nummer 25 betritt, steht nicht im glamourösen Eingangsbereich einer Grossbank, sondern in einem Raum, der an den weitläufigen Treppen- und Fahrstuhlaufgang einer Industriehalle erinnert. Der Aufzug, mit dem man in die zweite Etage fährt, wirkt in die Jahre gekommen und vibriert vernehmlich. Ist man oben angekommen, ändert sich das Bild von nüchterner Industriegeschäftigkeit nur geringfügig.

Vom Aufzug sind es nur ein paar Schritte zur unscheinbaren Glastür, die den Besucher in die Räume von Goldman Sachs führt. Das leise Surren vor dem Öffnen verrät allerdings, dass die Tür einen Schliessmechanismus besitzt und wohl trotz ihrer scheinbaren Unscheinbarkeit hohen Sicherheitsstandards genügt. Im Inneren bestätigen sich dann die räumlichen Erwartungen; als Gast wird man in ein Sitzungszimmer geführt, das geschäftiges Goldman-Sachs-Ambiente ausstrahlt.
Wichtige Aktivstrategien

Der Repräsentant von Goldman Sachs Asset Management in der Schweiz ist Pascal Mischler. Er ist ein Veteran im Finanzgeschäft mit perfekt sitzendem Anzug, dezenter Krawatte und einem gepflegten Englisch, in das sich manchmal ein kaum wahrnehmbarer französischer Akzent einschleicht.

Ein wichtiges Thema für Goldman Sachs Asset Management, erklärt Pascal Mischler, seien Aktienstrategien, die auf Big Data und Machine-Learning setzten. 90% aller heute existierenden Daten wurden in den letzten zwei Jahren produziert, deswegen sehen viele Finanzunternehmen bei Datenstrategien grosse Chancen. Investment-Unternehmen verwenden gerne das Beispiel mit den Satellitenbildern, welche die Parkplatzbelegung vor einem Supermarkt zeigen. Daraus errechnen komplexe Algorithmen wahrscheinliche Umsatzzahlen, was dann zu überlegenen Anlageentscheiden führen soll.

Auf dem Feld datenbasierter Strategien tummeln sich allerdings bereits zahlreiche Akteure. Die Frage, worin sich Goldman Sachs Asset Management von den Konkurrenten unterscheide, beantwortet Pascal Mischler mit dem Hinweis auf zwei Punkte: GSAM setze schon sehr lange, bereits seit 1989, auf datengestützte Strategien und sei heute ein etablierter Branchenführer. «Ausserdem erlauben es Grösse und Marktposition von Goldman Sachs Asset Management, fähige Mitarbeiter in einem hart umkämpften Markt anzuwerben», erklärt Mischler. «Wir haben mit mehr als 150 Leuten eines der grössten Kompetenzteams für datenbasierte Anlagestrategien.» Die Mehrrenditen der Big-Data-Strategien von GSAM weisen laut Mischler historisch eine sehr geringe Korrelation zu anderen aktiven Aktienanlagen auf. Das könne die Gesamtportfolio-Volatilität minimieren und zur Diversifizierung beitragen.

«Ein besonderes Augenmerk legen die Kunden im Moment auf alternative Anlagen», fährt Pascal Mischler fort. «Wir sind spät im Konjunkturzyklus, und unsere Klienten überlegen, wie sie sich für eine mögliche Wende positionieren sollen.» Alternatives seien in so einer Situation eine gute Wahl, weil sie in einem möglichen Down-Swing stabiler seien. Ausserdem erlauben gewisse Formen von Alternativanlagen eine geringe Korrelation zu traditionellen Aktien- und festverzinslichen Anlagen, was eine wichtige Diversifikationsmöglichkeit in einer Baisse bieten kann.
Schwierige Zeiten für Aktivplayer

Dass Mischler gerne über Big-Data-Strategien und alternative Investments spricht, ist nicht überraschend. GSAM ist ein Player, der hauptsächlich auf aktive Anlagen setzt. Im Gegensatz dazu bieten andere Grosskonzerne der Branche wie Blackrock oder State Street sowohl aktive als auch passive Strategien an. Für GSAM sind daher erfolgreiche Aktivstrategien von überragender Wichtigkeit.

Allerdings kann sich auch GSAM einem Grosstrend nicht entziehen: der säkularen Umschichtung von Aktiv- zu Passivanlagen. Im letzten Jahr haben Investoren erhebliche Anlagemittel aus den Aktivfonds von GSAM abgezogen. Zwar will das Unternehmen auch in Zukunft keine rein passiven Produkte wie Index-ETF anbieten, weil das nicht zur Kernkompetenz zählt. In den USA bietet GSAM aber bereits seit drei Jahren verschiedene Aktien- und Bond-ETF, insbesondere einige Smart-Beta-ETF, an. 2019 solle auch in Europa eine ETF-Produktreihe eingeführt und vertrieben werden, bestätigt Pascal Mischler.

Für die Zukunft zeigt sich der Schweiz-Chef von Goldman Sachs Asset Management zuversichtlich. Die Wirtschaft werde immer mehr von der Generation der Millennials bestimmt, die sich in ihrem Konsum- und Ausgabeverhalten, aber auch in ihren Lebensansichten deutlich von den vorangehenden Generationen unterscheide. «Die Belegschaft von Goldman Sachs Asset Management besteht bereits aus sehr vielen Millennials, sie machen rund 60% der Angestellten aus», offenbart Pascal Mischler. GSAM bildet also in der Belegschaft sowohl die ältere Generation als auch die Jüngeren ab. «Deswegen», so argumentiert Mischler, «verstehen wir die Bedeutung des wichtigen Segments der Millennials und können ihre Bedürfnisse besonders gut analysieren.»

Auch wenn GSAM langfristig gut aufgestellt ist und effiziente Investmentstrategien verfolgt, bleibt der kurz- und mittelfristige Erfolg ungewiss. Der enorme Margendruck und die anhaltende Umschichtung von Aktiv zu Passiv machen die Marktposition eines Aktivplayers schwierig. Es wird deshalb interessant sein zu beobachten, wie sich die Einführung der ETF-Produktreihe in Europa auswirkt.";https://www.nzz.ch/finanzen/fonds/goldman-sachs-ein-diskreter-aktivplayer-ld.1433100;NZZ;Patrick Herger;;;
13.06.2019;Das Silicon Valley und seine Sprösslinge: Vom Ende der Euphorie und vom Beginn des Unbehagens ;"Es ist ein seltenes Naturschauspiel. Einmal im Jahr öffnen sich in den Baumwipfeln des brasilianischen Regenwaldes für ein paar Stunden grosse, weisse Blüten – alle zur gleichen Zeit. Dabei stehen die blühenden Bäume sehr vereinzelt und meist mehrere Kilometer voneinander entfernt. «Es ist, als hätten sich die Bäume untereinander abgesprochen», erklärt Joseph Mascaro, der beim in San Francisco ansässigen Satellitenbetreiber Planet wissenschaftliche Programme betreut. Das junge Unternehmen lässt derzeit rund 130 Satelliten, die meist nicht grösser sind als eine Schuhschachtel, im unteren Orbit um die Erde kreisen, und hatte so das Phänomen der gleichzeitig blühenden Einzelgänger-Bäume überhaupt erst entdeckt.
Die Satelliten sehen alles

Den Satelliten bleibt auch sonst kaum etwas verborgen. In der höchsten Auflösung werden pro Bildpunkt 72 Quadratzentimeter Erdoberfläche aufgenommen. Und in dieser hohen Auflösung wird jedes Stück Erde mindestens einmal am Tag fotografiert. Im Fotoarchiv von Planet lagern von jedem Fleckchen Erde mindestens 800 Bilder. Dank Bilderserien kann Planet zudem die Bewegung von Fahrzeugen oder Menschengruppen über Tage, Wochen, Monate und gar Jahre nachzeichnen. Das Unternehmen kann etwa das Anschwellen von Flüchtlingscamps der Rohingya beobachten, die Veränderung der Erdölbestände in den diversen Lagern weltweit oder die Zahl der Autos, die in amerikanischen Häfen verladen werden. Die kalifornische Firma war unter den Ersten, die über den jüngsten Raketenabschuss des nordkoreanischen Machthabers Kim Jong Un Bescheid wussten. Ein Satellit hatte per Zufall noch die Rauchspuren im Himmel aufgenommen.

Die Daten verkauft Planet an öffentliche Institutionen, die damit etwa Flüchtlingsströme oder die Auswirkungen von Klimaveränderungen analysieren; und an private Unternehmen; oder wertet sie gemeinsam mit Konzernen wie Google aus. Denn das wahre Potenzial der Daten ergibt sich erst dann, wenn sie Computern gefüttert werden und diesen helfen, selbst zu lernen und neues Wissen zu kreieren. «Im Zeitalter der Satelliten und künstlicher Intelligenz bleibt nichts mehr verborgen», muss Mascaro zugeben. «Künftig müssen wir uns entscheiden, was wir wissen wollen, was wir nicht wissen wollen und wer was wissen darf und wer nicht.» Damit drückt Mascaro das derzeitige Gefühl bei vielen Beobachtern im Silicon Valley gegenüber dem heraufziehenden Zeitalter der künstlichen Intelligenz aus: Unbehagen.
Der Satellitenbetreiber Planet lässt derzeit rund 130 kleine Satelliten im unteren Orbit um die Erde kreisen. Eine kleine Auswahl aus dem Fotoarchiv von Planet. – Khi Solar One, Südafrika, Mai 2016. Über 4000 Spiegel lenken das Sonnenlicht auf einen zentralen, 205 Meter hohen Turm. Die in diesem Heizkessel erzeugte Energie kann eine Temperatur von 530° Celsius erreichen und für zwei Stunden gespeichert werden. Die 2013 in der südafrikanischen Region Nordkap fertiggestellte Anlage war eines der ersten solarthermischen Kraftwerke in Südafrika. (Bild: Planet Labs)

Dabei hat noch bis vor kurzem im Tal der Technologie ekstatische Euphorie geherrscht. Weltweit galt das Silicon Valley als Vorbild für Innovation und Fortschritt. Firmen wie Google, Amazon, Facebook, Uber und Airbnb stellten Branchen auf den Kopf, kreierten für die Konsumenten günstige und nützliche Dienstleistungen, die aus dem modernen Leben nicht mehr wegzudenken sind. Die amerikanischen Plattform-Firmen stiegen innerhalb von wenigen Jahren zu den grössten, rentabelsten und wertvollsten der Welt auf. Zudem hatten sie sich alle in ihren Leitbildern der Verbesserung der Welt verschrieben.

Doch in vielen Fällen ist auch das Gegenteil passiert. Facebook etwa ist auch zu einem Einfallstor fremder Geheimdienste in den USA geworden. Zudem ist es wie Alphabet und Amazon längst tief in die Privatsphäre seiner Nutzer vorgestossen. Und das sei erst der Anfang, befürchten viele. Denn der technologische Fortschritt dürfte diese Riesen dank ihren umfangreichen Datenschätzen und massiven finanziellen Ressourcen zu den wichtigsten Akteuren im Zeitalter der künstlichen Intelligenz machen. Im sonst so optimistischen, nach vorne strebenden und technologiefreundlichen Silicon Valley blickt man auf die Probleme, die Firmen wie Google und Facebook trotz besten Absichten in den vergangenen Jahren geschaffen haben, und fragt sich bange: Was ist, wenn sich das im heraufziehenden Zeitalter der künstlichen Intelligenz wiederholt?
Idylle mit Universitätsanschluss

An diesem frühsommerlichen Vormittag weht ein lauer Wind durch die mit Einfamilienhäusern gesäumte Strasse unweit der Universität Stanford. In den Vorgärten blühen Rosen und Fingerhut, Bougainvilleen ranken an einigen Fassaden der kleinen, schmucken Häuser aus dem Anfang des letzten Jahrhunderts hoch. Bananenpflanzen zeugen von dem ganzjährig milden Klima. Vor dem Haus mit der Nummer 367 in der Addison Avenue steigt eine Gruppe von Managern aus einem Reisebus aus. Sie sind Kunden des Technologiekonzerns HP, dem das Haus samt angrenzender Garage gehört. Sie werden in der Sonne an Stehtischen mit langen weissen Tischdecken speisen und Wein trinken und später sagen können, dass sie die «Wiege des Silicon Valley» gesehen haben.

Das jedenfalls sagt ein Schild vor dem Haus. In dieses waren Ende der 1930er Jahre die beiden Stanford-Studenten William Hewlett und David Packard eingezogen. Sie waren dem Rat des damaligen Dekans der Ingenieurwissenschaften Frederick Terman gefolgt, doch ein Unternehmen zu gründen. Mit dem Ziel, junge Talente davon abzuhalten, einen der gutbezahlten Jobs in der Industrie an der Ostküste anzunehmen, hatte Terman ihnen und anderen Studenten erstmals ein Startkapital – heute würde man es Risikokapital nennen – angeboten. In der Garage des Wohnhauses an der Addison Avenue machten sich Hewlett und Packard sogleich an die Arbeit. Die dort entwickelten Produkte sollten den Grundstein legen für einen der ersten und lange Zeit führenden Technologiekonzerne der USA: Hewlett-Packard (HP).

Und gleichzeitig war das «Modell Silicon Valley» geboren. Die um die Ecke liegende Universität Stanford sollte in den nächsten Jahrzehnten wissenschaftliche Erkenntnisse und betriebswirtschaftlich nutzbare Konzepte entwickeln und zwischen Forschung, Lehre und Wirtschaft hin und her wechselnde Professoren sowie hochmotivierte, oft unternehmerisch denkende Studenten hervorbringen. Und für die sprudelnden Geschäftsideen sollten die sich hinter der Universität an der Sandy Hill Road ansiedelnden Wagniskapitalfirmen das Risikokapital bereitstellen.
Das «Silicon Valley 2.0»

Doch so wenig wie das blumenbepflanzte, pittoreske Wohnviertel mit der HP-Garage heute typisch ist für das von Verkehrsstaus und Obdachlosen geprägte Bild der Städte im Silicon Valley, so wenig taugt das Bild der Bastler-Garage heute als Modell für den Erfolg von Jungunternehmen. Das weiss kaum jemand besser als der Unternehmer und Wagniskapitalgeber Reid Hoffman. Denn der Gründer des inzwischen an Microsoft verkauften Karrierenetzwerks Linkedin und Partner bei der Risikokapitalfirma Greylock war in der letzten Dekade massgeblich am Aufbau des Modells «Silicon Valley 2.0» beteiligt.  Das, was heute im Zeitalter der globalisierten Märkte, der Digitalisierung und der künstlichen Intelligenz das Silicon Valley ausmacht, sind laut Hoffman die dort entwickelten Methoden, um «turbogetriebenes» Wachstum zu bewerkstelligen. «Heute im Silicon Valley Unternehmer zu sein, ist so, als ob man beim Absprung von der Klippe sein eigenes Flugzeug zusammenbauen müsste», so fasst er die Bedeutung des Faktors Zeit zusammen. Denn in vielen digitalen Märkten könne es nur einen Gewinner geben. In dem Zusammenhang verweist Hoffman auf den Film «Glengarry Glen Ross», in dem vier New Yorker Immobilienmakler gegeneinander um ihren Job kämpfen. Dass es nur einen Gewinner geben wird, erklärt der von der Firma beauftragte Trainer mit folgenden Worten: «Wie ihr alle wisst, der erste Preis ist ein Cadillac Eldorado. Will jemand den zweiten Preis wissen? Der zweite Preis ist ein Steakmesser-Set. Und der dritte Preis ist, dass du gefeuert wirst. Kapiert ihr’s jetzt?»

Wie Startup-Unternehmen heute die Konkurrenz abhängen und zum weltweiten Marktführer aufsteigen können, hat Hoffman in seinem jüngst erschienenen Buch «Blitzscaling» zusammengefasst. In der Stärke des «Silicon Valley 2.0», Firmen mit entsprechenden Wachstumsmodellen und milliardenschweren Kapitalinfusionen von null auf hundert zu beschleunigen, liegt gleichzeitig aber auch eine grosse Gefahr: Auf Verluste wird keine Rücksicht genommen. «Wenn Sie heute ein Technologieunternehmen gründen, dann kann das Wachstum umwerfend sein. Es ist, als würden Sie Wasser aus einem Feuerwehrschlauch trinken», erklärt Hoffman. So folgte Facebook etwa dem Motto «Move fast and break things». Die Nebeneffekte sind bekannt.

Hoffman ist das Dilemma zwischen den Nebenwirkungen, die das Turbo-Modell mit sich bringt, und den Gewinnen, die eben nur dem schnellsten und aggressivsten Unternehmen winken, durchaus bewusst. Heute würde er Facebooks Motto abändern in «Bewege dich schnell, und zerstöre nur einige Dinge». Auch sieht Hoffman die Verantwortung, die die enorme Grösse mit sich bringt: «Die Technologiekonzerne sind mittlerweile kritische Teile im Räderwerk der Gesellschaft und bestimmen über ihre Dienste mit, wie sich die Gesellschaft entwickelt.»
Die Vorreiter der künstlichen Intelligenz

Im heraufziehenden Zeitalter der künstlichen Intelligenz werden die Technologiekonzerne eine wohl noch grössere Rolle spielen. Denn sie verfügen über das, was sie erst richtig mächtig macht: Daten. Entweder sammeln sich diese auf ihren Plattformen in Form von Nutzerdaten, oder sie erwerben sie von Firmen wie dem Satellitenbetreiber Planet. Die Daten sind sozusagen der Lehrstoff für die Computer, die immer mehr Aufgaben eigenständig übernehmen. Zusammen mit milliardenschweren Forschungsbudgets dürften laut vielen Experten Konzerne wie Alphabet, Facebook, Amazon, Apple und Microsoft sowie deren chinesische Konkurrenten wie Tencent und Alibaba das Zeitalter der künstlichen Intelligenz anführen. Universitäten und andere staatliche Forschungseinrichtungen hätten das Nachsehen.  Es ist das im «Silicon Valley 2.0» herrschende Mantra des Strebens nach Grösse, verbunden mit dem Willen, hohe Risiken einzugehen, das auch die Stanford-Professorin für Computerwissenschaften Fei-Fei Li umtreibt. Wie viele im Silicon Valley hadert auch die Expertin für künstliche Intelligenz mit den ungewollten Nebenwirkungen, die der rasante Aufstieg etwa der vom Datensammeln lebenden Plattform-Firmen wie Google und Facebook mit sich gebracht hat. Vor allem aber hadert sie damit, diese Gefahren nicht erkannt zu haben: «Wir dachten ursprünglich, dass die Social Media die Menschen zusammenbringen würden.» Die Wissenschafterin fordert deshalb: «Wir müssen aus den jetzigen Problemen mit Facebook und Co. für das Zeitalter der künstlichen Intelligenz schon jetzt unsere Lehren ziehen.»

Sie hat daher an der Universität Stanford erst jüngst mit dem Stanford Institute for Human-Centered Artificial Intelligence (HAI) ein Institut aus der Taufe gehoben, das künstliche Intelligenz zu einem interdisziplinären Thema machen, ausser den Chancen auch ihre Gefahren aufzeigen und schliesslich Vorschläge an die Politik abgeben soll. Das HAI folgt dabei einem Leitbild, das den Menschen in den Mittelpunkt stellt und der Technologie die Aufgabe zuteilt, den Menschen zu bereichern, statt ihn zu ersetzen. Li selbst gehört zu den anerkanntesten Forschern im Bereich künstliche Intelligenz. Die gebürtige Chinesin, die mit ihren Eltern als Teenager in die USA gekommen war, hat überdies in den letzten anderthalb Jahren wichtige Einblicke in Googles Initiativen im Bereich künstliche Intelligenz erhalten. Während eines Sabbaticals von der Uni hatte sie bis Ende vergangenen Jahres die Rolle der Chef-Wissenschafterin für künstliche Intelligenz und Machine-Learning des wichtigen Geschäftsbereiches Cloud Computing inne.
Die Googlesche Geheimniskrämerei

Doch Fragen zu ihrer Zeit bei Google darf man ihr nicht stellen; schon vor dem Gespräch machen ihre Medienberater dies unmissverständlich klar. Damit deutet Li, wenn auch wohl unfreiwillig, auf einen weiteren Grund hin, warum man im bisher so technologiegläubigen Silicon Valley den Bereich künstliche Intelligenz auch mit Unbehagen betrachtet, nämlich die Intransparenz. Keiner der Konzerne muss Rechenschaft über seine Forschungstätigkeiten abgeben. Und die Steuerung der Forschung liegt letzten Endes in der Hand der wichtigen Aktionäre, und das sind bei Alphabet die Gründer Larry Page und Sergey Brin, bei Facebook Mark Zuckerberg und bei Amazon dessen Gründer und Chef Jeff Bezos. Allein mit ihrem Privatvermögen könnten sie die Forschung in dem Feld beeinflussen; Bezos macht das im Bereich der Raumfahrt ja bereits vor.

Zudem kennt die Technologie keine Grenzen. «Die hinter der künstlichen Intelligenz stehenden Technologien verbreiten sich schnell um die ganze Welt. Das lässt sich nicht eindämmen», erklärt Li und verweist auf den Bereich von Atomwaffen, deren Verbreitung erst nach langen Verhandlungen in der Welt und auch dann nur lückenhaft eingeschränkt werden konnte. Der Ansatz ihres neuen, gut ausgestatteten Instituts mag vielversprechend sein, vor allem weil er interdisziplinär ist und den Tunnelblick vieler Forscher öffnet. Er mag die Diskussionen zum Thema künstliche Intelligenz an der Universität, im Silicon Valley und darüber hinaus anregen und vor allem die Regulatoren und Politiker in Washington für die Probleme sensibilisieren und sie dabei unterstützen, kompetente Antworten zu finden. Und Li mag als hochgeschätzte Wissenschafterin, als jemand, der Einblicke in die Forschung im Bereich künstliche Intelligenz bei Google hat und der im Silicon Valley bestens vernetzt ist, eine einflussreiche Stimme sein. Doch wie weit kann der Einfluss auf die Tech-Konzerne dabei überhaupt reichen?
Es bleibt nur die Hoffnung

Der Risikokapitalgeber Hoffman macht sich keine Illusionen. Die Tech-Konzerne stünden im Bereich künstliche Intelligenz in einem mörderischen Wettbewerb und würden sich sicherlich von niemandem zurückhalten lassen. Er fügt an: «Wir können nur hoffen, dass sie uns zuhören.» ";https://www.nzz.ch/wirtschaft/silicon-valley-und-seine-sproesslinge-vom-ende-der-euphorie-ld.1485668;NZZ;Christiane Hanna Henkel;;;
14.06.2016;Kreative Computer – wenn die Software zum Erfinder wird;"Alex Reben hatte zweieinhalb Millionen Ideen in nur drei Tagen. Dass die Einfälle gewissermassen alle unbrauchbar sind und keinen Innovationswert haben, ist erst einmal unproblematisch. Denn es geht hier vor allem ums Prinzip. Der Robotiker und Computerwissenschafter hat im Rahmen des Kunstprojekts «All Prior Art» einen Algorithmus entwickelt, der millionenfach Erfindungen generiert mit dem einzigen Ziel, die Patentanmeldung derselben zu verhindern. Die Pointe dabei: Wenn sie öffentlich publiziert sind, gelten sie nicht mehr als neu – und also auch nicht mehr als patentierbar.

Das System extrahiert Versatzstücke aus bereits angemeldeten und beantragten Patenten und fügt diese zu neuartigen Anleitungen zusammen. Die Patente hören auf sperrige Kürzel wie «1461187712-3f68906c-5b50-4e26-a28c-e9e5ef4d0643» und sind im Grunde Nonsens-Betriebsanleitungen, die technische Geräte oder Techniken miteinander kombinieren, die nicht miteinander kompatibel sind – zum Beispiel ein Roboter-Telefonbuch und einen nasalen Pfropf, der mit magnetischem Schmuck dekoriert ist.

Das Ganze klingt, als sei es der Phantasie eines LSD-Junkies entsprungen. Elektronikkonzerne dürften daran auch in der Tat nicht interessiert sein. Doch das ist sekundär. Reben will mit «All Prior Art» die Schaffung neuer Produkte verhindern. Das Projekt zielt auf sogenannte «Patent-Trolle» ab, die mit der Aussicht auf lukrative Gewinne mehr oder weniger wahllos Patente anmelden. Nun gehören Spekulationen zum Wesen des Wirtschaftens, richtig beikommen konnte man den Patent-Zockern bisher nie. Der neue Clou ist jedoch, dass die Kosten dieser computerisiert erzeugten Ideen gegen null tendieren.
Kreative Programme

Das Kunstprojekt wirft eine Reihe von Fragen auf. Was kann überhaupt noch als Erfindung gelten in einer Zeit, in der schamlos kopiert und plagiiert wird? Wird das Urheberrecht heutigen Kulturtechniken wie Mash-up, Remix und Sampling noch gerecht?

Erfindungen sind im Prinzip von Entdeckungen abzugrenzen. Bei einer Entdeckung handelt es sich um das Aufdecken von etwas Bekanntem. Sie ist, wie der Jurist Andreas Böhm schreibt, «reine Erkenntnis und bereichert das ‹Wissen›, während die Erfindung das ‹Können› bereichert». Eine Entdeckung ist etwa die Beschreibung der Tendenz einer Magnetnadel nach Norden, der erstmalige Einbau in einen Kompass hingegen stellt eine Erfindung dar. Entdeckungen werden vom Gesetz nicht geschützt, weil dies eine Entwicklungshemmung nach sich zöge. So weit die rechtspolitische Theorie.

    Kann auch künstliche Intelligenz Anspruch auf Autorschaft oder Urheberschaft eines Werks beanspruchen?

Sollte sich unter der Flut von Erfindungen jedoch eine brauchbare Idee finden, ein Nugget, das für die Industrie dienstbar gemacht werden kann – wem gebührt dann das geistige Eigentum? Dem Algorithmus? Dem Programmierer? Oder anteilig den Patentinhabern, aus denen der Computer die Essenz extrahierte? Oder impliziert die Terminologie («geistig») nicht sogar, dass hier ein Geist im Sinne menschlicher Intelligenz am Werk sein muss? Anders gewendet: Kann auch künstliche Intelligenz Anspruch auf Autorschaft oder Urheberschaft eines Werks beanspruchen? Schon heute generieren Computerprogramme etwa bei der Nachrichtenagentur Associated Press millionenfach Texte: Sportberichte, Finanzmarktberichte, Wetterberichte, Fernsehtipps. Die Sprachroboter durchforsten Datenbanken und bauen die Textbausteine zu einem neuen Text zusammen. Ein solches Elaborat ist nicht pulitzerpreisverdächtig, aber trotz seiner Trivialität so neuartig, dass man eine Autorschaft in Erwägung ziehen könnte (unter der Voraussetzung, dass es über die reine Wiedergabe von Fakten hinausgeht, die nicht unter Copyright fallen).

Java, C und Co. heissen zwar Programmier-«Sprachen», aber es sind letztlich technische Systeme zur Formulierung von Anweisungen. Menschliche Sprache hingegen ist eine Kulturtechnik, die u. a. zur Formulierung von Meinungen, Ansichten und normativen Aussagen eingesetzt wird. Und diese Inhalte finden wir schützenswert. Die Meinungsfreiheit ist ja eine Freiheit der Ansichten und nicht der Zeichenketten.

Die entscheidende gedankliche Arbeit, das Programmieren, liegt auf nichttechnischem Gebiet. Doch was ist, wenn durch das Sprachwerk neue Gedanken formuliert werden? Wer ist dann der Urheber? Vorstellbar sind drei Rechtskonstruktionen.

Erstens: Die Autorschaft des computergenerierten Werks kann dem Programmierer zugeschrieben werden, der die Software konzipiert hat. Zweitens: Sie steht dem Daten-Provider zu, der das Rohmaterial zur Verfügung stellt. Das kann vom Börsenguru, der mit seinen Statistikaufzeichnungen die Datenbanken von Roboter-Journalisten für Finanzberichte speist, bis hin zum Wissenschafter, der seine Versuche dokumentiert, gehen. Oder drittens: Die Autorschaft gebührt dem Algorithmus selbst unter Anerkennung «seiner» geistigen Leistung. Allein, können Algorithmen, die von Menschen programmiert wurden, Urheber sein?
Der neue Rembrandt

Vor kurzem kreierten niederländische Forscher einen Algorithmus, der mithilfe von 3-D-Druck und Big Data nicht nur Gemälde von Rembrandt reproduzierte und diese perfekt nachahmte, sondern es auch vermochte, selbständig ein für den Maler typisches Motiv zu erschaffen. Das Ergebnis – «The Next Rembrandt», sprich: ein Porträt eines Mannes mit Musketier-Bart, Hut und Halskrause – würde sich perfekt in eine Rembrandt-Galerie einfügen. Ist das Kunst? Künstliche Intelligenz? Oder einfach nur das geniale Werk eines Algorithmus? Hat sich der Computer schamlos am Stil eines der grössten Maler der Geschichte bedient?

    Hat sich der Computer schamlos am Stil eines der grössten Maler der Geschichte bedient?

Bei der Frage, ob das geistige Eigentum dem Programmierer oder dem Datenlieferanten zugerechnet werden kann, verhält es sich wie bei einer Fotografie. Auch hier kämen der Hersteller der Kamera und der Fotograf als Urheber in Betracht. Der Supreme Court entschied in einem berühmten Fall einer Foto von Oscar Wilde aus dem Jahr 1884, dass das Copyright dem Fotografen zusteht. «Neuen» Technologien wie der Fotografie sei keine Kreativität inhärent, weil erst aus der Imagination des Fotografen ein Akt der Kunst werde, urteilten die obersten Richter. Analog zu dieser – zugegeben sehr weit zurückliegenden – Entscheidung könnten Gerichte eher auf das Urheberrecht des Datenlieferanten abstellen, weil der Softwareentwickler nur das «Gehäuse» ähnlich wie den Fotoapparat entwickelt und erst die Daten den Output zu dem machen, was es eigentlich ist – ein neuartiges Werk.
Romantik und Archaik

Die in Stanford lehrende Rechtsprofessorin Annemarie Bridy plädiert in ihrem Aufsatz «Coding Creativity: Copyright and the Artificially Intelligent Author» dafür, dass man die «Kreativität der Coder mit der Kreativität der Codes» abwägend vergleichen und dem Algorithmus eine Autorschaft konzedieren müsse – nämlich dann, wenn die Kreativität der Codes überwiege (was bei machine learning irgendwann der Fall sein wird). Das führt in der Praxis zu ganz neuen Copyright-Problemen, wenn sich Algorithmen an Patenten wie aus einem Steinbruch bedienen und Versatzstücke zu neuen Patenten zusammenschmieden.

Innovation ist ja immer ein Verarbeiten oder Recycling bereits bestehender Ideen. Man muss nicht jedes Mal das Rad neu erfinden. Doch was bedeutet es für den Wert eines Patents, wenn plötzlich ein Algorithmus millionenfach Ideen auf den Markt wirft und der – in einem freien Land eigentlich offene – Weg der Erfindung dann gewissermassen gesperrt ist? Was bedeutet es für den Wissenschaftsprozess, wenn Heureka-Momente gleichsam automatisiert werden? Hängen wir womöglich einer veralteten romantischen Vorstellung eines Tüftlers nach, der in seiner Werkstatt eine Erleuchtung hat und eine bahnbrechende Erfindung macht?

Der Schweizer Entrepreneur Julian Nolan sagte dem Wissensmagazin «New Scientist»: «Die Art und Weise, wie Erfindungen gemacht werden, ist höchst archaisch und ineffizient.» Nolan ist CEO von Irova, einem Unternehmen mit Sitz in Lausanne, das sich auf die Generierung computergestützter Erfindungen spezialisiert hat. Das Startup hat einen Algorithmus entwickelt, der herausfindet, ob zwei Informationsstücke eine potenziell innovative oder kreative Verbindung haben. Eine Software schürft im Netz mit Big-Data-Methoden nach verwertbaren Informationen und bastelt diese zu einer kohärenten Lösung zusammen. Das KI-System, das auf der Cebit in Hannover präsentiert wurde, soll in der Lage sein, Patente hundertmal schneller als in konventionellen Verfahren zu entwickeln und diese effektiver zu lizenzieren. Ist künstliche Intelligenz womöglich doch der bessere Erfinder als das menschliche Gehirn? Viele Erfindungen verdanken wir dem Zufall. Alexander Fleming vergass 1928, das Fenster zum Labor zu schliessen, bevor er in die Sommerferien aufbrach, so dass die Pilzsporen hereinflogen und den Erregern auf den Petrischalen den Garaus machten. Thomas Alva Edison probierte für die Erfindung der Glühbirne um die 9500 kleine Kohlefäden aus, bis er denjenigen fand, der die Birne dauerhaft zum Leuchten brachte. Und der schwäbische Chemiker Christian Friedrich Schönbein entdeckte Mitte des 19. Jahrhunderts per Zufall Zellulosenitrat, aus dem später der Filmstreifen Zelluloid entwickelt wurde. Forscher sprechen auch von der «Serendipität» – durch Zufall etwas finden, wonach man gar nicht gesucht hat. Die Frage ist: Lässt sich diese Serendipität auch automatisch generieren? Kann man vielleicht sogar etwas wie eine Heureka-Maschine schaffen, die am Fliessband Patente generiert?

In den 1990er Jahren versuchten Wissenschafter der Stanford University unter Leitung von John Kaza mithilfe der genetischen Programmierung (GP), eines Suchverfahrens, das Prozesse der Evolution simuliert, Techniken wie die ersten Filter, Verstärker oder Linsen, die in den 1920er und 1930er Jahren in den Bell Labs entwickelt wurden, «neu» zu erfinden. Das Experiment glückte. Der Algorithmus reproduzierte nicht nur die optischen Systeme, sondern verbesserte sie obendrein noch. Kaza gilt als Pionier der genetischen Algorithmen, die nach dem Vorbild der Natur mit Mutation, Rekombination und Selektion operieren. Es ist ein wenig so, als könne die Computertechnik von der Natur lernen.
Wem gehört die Maschine?

«Innovation ist etwas, was nicht automatisch durch Ausführung eines Algorithmus kreiert werden kann», sagt der Computerwissenschafter Stephann Makri von der City University London, der sich auf die Mensch-Maschine-Interaktion spezialisiert hat. Zukünftige digitale Tools könnten jedoch als «Katalysatoren für die Entwicklung von Ideen» dienen. Sie könnten mit Ideen spielen und beim Innovationsprozess eine wichtige Rolle spielen, so Makri.

Die Soziologin Anabel Quan-Haase, die an der University of Western Ontario lehrt und die Rolle der Serendipität im Wissenschaftsprozess erforscht, sieht dies differenzierter. Sie sagt im Gespräch: «Wenn wir uns die Geschichte der Erfindungen anschauen, stellen wir fest, dass es Erfinder oft schwer hatten, ihre Zeitgenossen vom Wert der Erfindung zu überzeugen. Es brauchte Zeit, bis die Leute den Nutzen erkannten.» Für Quan-Haase ist Serendipität ein Phänomen, das aus zwei Komponenten besteht. Die erste, der Zufall, der auf Wahrscheinlichkeit beruht, könne theoretisch automatisiert werden. Doch könne das andere Element, das Erkennen, nicht so leicht automatisiert werden. Es bedürfe des Vorwissens und vor allem der Erfahrung, um die neuen Erkenntnisse einzuordnen.

Das Kunstprojekt «All Prior Art» hält die Soziologin für interessant – es ist ja auch eine Art Erfindung. «Ich denke, die Maschine ist cool, sie versucht uns von der Idee wegzubewegen, dass wir besitzen, was wir in Form von Patenten entdecken, und lässt uns über Erkenntnis als Teil der öffentlichen Sphäre nachdenken.» Vielleicht kommt ja wirklich irgendwann eine bahnbrechende Erfindung aus einer Maschine heraus. Und dann wäre die entscheidende Frage, wem die Maschine gehört – einer einzelnen Person, einigen wenigen oder allen, also der Öffentlichkeit?";https://www.nzz.ch/feuilleton/kuenstliche-intelligenz-kreative-computer-wenn-die-software-zum-erfinder-wird-ld.88633;NZZ;Adrian Lobe;;;
03.05.2018;«Dass uns wegen der Automatisierung die Arbeit ausgeht, ist wenig plausibel»;"Der Weg in die Zukunft ist mit alten Ideen gepflastert. Heute gilt das besonders für die Prognose von radikal neuen Zeiten, in denen eine künstliche Superintelligenz uns Menschen in arbeitslose Nichtsnutze verwandeln werde, umgeben von Robotern, die uns observierten, mit uns kommunizierten und sich zum Trost für uns prostituierten. Die Ideen, aus denen die Prognose zusammengesetzt ist, sind fünfzig bis zweihundertfünfzig Jahre alt, und um ihre empirische Grundlage steht es heute nicht viel besser als früher.
Wiederkehrende Ängste

Seit einem Vierteljahrtausend ersetzen Maschinen menschliche Arbeitskraft. Zu Beginn der Industrialisierung wurden vorrangig physische Fertigkeiten automatisiert, aber schon damals kamen Informationstechnologien zum Einsatz, die man als digitale Programmiersysteme bezeichnen könnte. Webmaschinen empfingen die Daten für die Gestaltung der Stoffmuster von Lochkarten. Einer der wichtigsten Pioniere des automatischen Webstuhls, Jacques Vauquanson, hatte zuvor mit einer anderen digitalen Technologie die «Robotik» des 18. Jahrhunderts revolutioniert: Er baute nach 1730 menschenähnliche Automaten, darunter einen berühmten Flötenspieler, der mithilfe einer Stiftwalze musizieren konnte.

Bald traten mechanische Roboter, die virtuos musizieren, zeichnen oder schreiben konnten, an Fürstenhöfen und Jahrmärkten auf. Entsprechend naheliegend war es, die baldige Ankunft eines Zeitalters zu erwarten, in dem Menschen und Maschinen nicht mehr unterscheidbar sein und der technologische Fortschritt in Massenarmut münden würde. Anders als heute war nach 1800 die Erfahrung eines rapiden technologischen Wandels neu und der Zusammenhang zwischen der Industrialisierung und der Verelendung breiter Schichten offensichtlich. Und anders als heute brachte die Phantasie einer Gesellschaft mit Androiden grosse Literatur hervor – man denke nur an E. T. A. Hoffmanns virtuose Erzählungen «Die Automate» (1814) oder «Der Sandmann» (1816). Im späten 19. Jahrhundert wurde das digitale Programmiersystem der Lochkarte bereits für Rechenleistungen eingesetzt. Spätestens ab diesem Zeitpunkt nahmen Maschinen den Menschen auch intellektuelle Arbeit ab, und gewisse Aufgaben erledigten sie rasch effizienter. 1890 setzte das Census Bureau der Vereinigten Staaten eine von Herman Hollerith entwickelte Lochkartenprogrammierung für die Volkszählung ein und beschleunigte damit die Auswertung der gesammelten Daten um mehrere Jahre. Wir blicken also auf 130 Jahre der maschinellen Datenverarbeitung zurück, und trotz enormen Fortschritten fehlen bis heute konkrete Anhaltspunkte, dass den Menschen deswegen die Arbeit ausgehen könnte.
Robocalypse Redux

Dennoch kommt und geht die Angst vor automatisierungsbedingter Massenarbeitslosigkeit in Zyklen. Es ist ein bisschen wie mit der Ankündigung der biblischen Apokalypse in vormodernen Zeiten: Zwischen dem 1. und dem 18. Jahrhundert hat jede Christengeneration von neuem das Weltende in unmittelbarer Zukunft erwartet, wobei sich Untergangspropheten von der Tatsache, dass alle Apokalyptiker vor ihnen falsch gelegen hatten, nicht in Verlegenheit bringen liessen: Gott hatte den gläubigen Sündern in seiner unendlichen Gnade länger Aufschub gewährt, spätestens jetzt aber war seine Geduld aufgebraucht und das Ende nahe. Die Rolle von Gott wird heute von der Technologie gespielt. Sie allein bestimmt, so die Botschaft von Automatisierungsapokalyptikern, wann das Verhängnis eintritt, und sollte sie frühere Prophetien nicht erfüllt haben, so wird sie es demnächst tun.

Was aber unterscheidet die heutige Automatisierungsapokalyptik von früheren Ausgaben? Aufschlussreich ist der Vergleich mit der Nachkriegszeit, als die meisten Szenarien und Rezepte entworfen wurden, die derzeit wieder ins Spiel gebracht werden – von der künstlichen Superintelligenz bis zum bedingungslosen Grundeinkommen. Damals wurden gerade die ersten Computer in Betrieb genommen. Spekulationen schossen ins Kraut, nun beschleunige sich der technologische Wandel derart, dass die Maschinen den Menschen davongaloppierten. Expertenkommissionen empfahlen Massnahmen zur Abfederung der drohenden Massenarbeitslosigkeit, die Sekretärinnen, Buchhalter, Logistiker, Übersetzer und weitere Berufe ereilen könnte. Es ist eine Ironie der Geschichte, dass jene Epoche, die aus der Rückschau zum Zeitalter der Vollbeschäftigung und des Arbeitskräftemangels verklärt wurde, von gewaltigen Automatisierungsängsten geprägt war.

    Wir blicken auf 130 Jahre der maschinellen Datenverarbeitung zurück.

Gleichzeitig machte man sich in der Nachkriegszeit Hoffnungen auf einen vollautomatisierten Haushalt, angeregt durch Geräte wie Waschmaschinen, Geschirrspüler, Kühlschränke und Elektroherde. Der Wirtschaftssoziologe Warner Bloomberg phantasierte 1955 von einem «denkenden Backofen», dem man sagen könne: «Brate mir ein dreipfündiges Huhn!», worauf die Maschine das gewünschte Tier aus dem Gefrierfach holen, stopfen, marinieren, garen, schneiden und servieren würde. Dass uns Bloombergs Backofen noch heute futuristisch vorkommt, hat nicht etwa damit zu tun, dass er technologisch nicht machbar wäre, sondern damit, dass er nach wie vor nicht wünschbar ist. Neue Technologien haben nicht die deterministische Zwangsläufigkeit, die ihnen Automatisierungspropheten gerne unterstellen. Maschinen können noch so raffiniert sein – wenn sie keine Nachfrage erzeugen, werden sie zu smartem Müll.

Die Nachkriegsjahrzehnte waren von einem hohen Produktivitätswachstum geprägt, an dem der Industriesektor grossen Anteil hatte. Es erzeugte starke «Spillover-Effekte», die wiederum zu einem unerwarteten Wachstum des Dienstleistungssektors beitrugen. Angesichts der wirtschaftlichen Dynamik und der fehlenden Erfahrung mit Computertechnologie besass das Szenario einer automatisierungsbedingten Massenarbeitslosigkeit damals eine gewisse Plausibilität. Heute stehen die Dinge anders: Wir blicken auf eine Dekade mit äusserst schleppendem Produktivitätswachstum zurück, und in aktuellen Wirtschaftsdaten finden sich kaum Anhaltspunkte, mit denen man die Beschwörung eines immer schneller und fundamentaler werdenden Wandels des Wirtschaftslebens auf ein empirisches Fundament stellen könnte. Die Rede von der Industrie 4.0 wiederum geht an der Tatsache vorbei, dass heute, im Gegensatz zu den Umbrüchen, die mit der Industrie 1.0, 2.0 und 3.0 gemeint sind, gar keine neue Basistechnologie vorliegt. Der digitale Wandel beruht weiterhin auf der Mikroelektronik, für die bereits der Begriff der Industrie 3.0 reserviert ist. Die Einbildung, wir erlebten gerade die grösste technologische Revolution der Geschichte, zehrt von einer hohen Dosis Geschichtsvergessenheit.
Problematische Oxford-Studie

Welche Faktoren haben also in den vergangenen Jahren dazu beigetragen, dass die Angst vor der Automatisierung wieder Hochkonjunktur hat? Der Verweis auf die Digitalisierung reicht als Erklärung nicht aus, weil uns die digitale Transformation des Wirtschaftslebens bereits seit Jahrzehnten begleitet. Drei Faktoren sind ins Feld zu führen. Erstens eine Dienstleistungsbranche, deren heutige Grösse ebenfalls einem Spillover-Effekt aus produktiveren Wirtschaftssektoren zu verdanken ist: die Beratungsindustrie.

Weltweit tätige Beratungsfirmen gehören heute zu den energischsten Antreibern der Automatisierungsdebatte. Sie benutzen dabei ein Schlagwort, das ihnen schon vor der letzten Finanzkrise dazu gedient hat, einen vermeintlich fundamentalen Wandel anzuzeigen: «This time is different!» Wenn Consultants Manager mit gewagten Prognosen zum Glauben verleiten, es werde in nächster Zukunft alles anders, steigt das Beratungsbedürfnis ihrer Kunden. Automatisierungsängste sind heute, anders als noch in den 1950er Jahren, ein Milliardengeschäft.

Zweitens leistet die Medienindustrie einen bedeutenden Beitrag zur gegenwärtigen Automatisierungsdebatte. Journalisten gehören zu den Berufsgruppen, die am stärksten von der Digitalisierung betroffen sind. Technologisch bedingte Arbeitslosigkeit ist für viele Medienschaffende ein reales Risiko, und deshalb besteht in der Branche eine höhere Affinität für digitale Weltuntergangsszenarien. Allerdings: Wenn Journalisten zu Propheten einer intelligenten Roboterwelt ohne arbeitende Menschen werden, verdecken sie gerade die Gefahren, denen sie selbst ausgesetzt sind. Der Beruf des Journalisten wird nicht durch künstliche Intelligenz bedroht, sondern durch die Netzwerkeffekte des digitalen Plattformkapitalismus, die es Google und Facebook ermöglichen, ein Oligopol für Online-Werbung zu errichten, ohne selber mediale Inhalte zu produzieren. Indem Medien die Automatisierungsapokalypse befeuern, lenken sie von den realen Problemen ab, die digitale Technologien für den wirtschaftlichen Wettbewerb und die demokratische Politik schaffen.

    Automatisierungsängste sind heute, anders als noch in den 1950er Jahren, ein Milliardengeschäft.

Ein dritter Faktor der aktuellen Automatisierungsapokalyptik ist die Wissenschaft. Forschende werden heute mehr denn je an der Aufmerksamkeit gemessen, die sie unter Fachkollegen und Fachfremden erregen. Wer mit einer steilen, aber wackligen These einen Hype erzeugt, erhält mehr «citations» und «impacts» als jemand, der mit einer sorgfältigen Berechnung einen neuen Beweis erbringt. Symptomatisch dafür ist die Resonanz, die Carl Benedikt Frey und Michael A. Osborne mit The Future of Employment: How Susceptible are Jobs to Computerisation? erzielt haben. Das Working-Paper, das 2013 auf einen Server der Universität Oxford hochgeladen wurde, machte die beiden Autoren über Nacht berühmt. Entscheidend war eine exakte Prognose im Abstract: «Nach unseren Schätzungen sind rund 47% der gesamten US-Beschäftigungen gefährdet.»

Wie kamen die beiden Autoren zu dieser präzisen Kalkulation eines künftigen Risikos, das «möglicherweise in ein bis zwei Jahrzehnten» eintrete? Frey und Osborne nahmen eine Liste des amerikanischen Bureau of Labor Statistics mit zirka 700 Berufsbezeichnungen. Was sie mit der Liste anstellten, war in ein paar Sätzen in der Mitte der Studie beschrieben, dort, wo kaum ein Journalist hinsehen würde: Sie gingen die Berufsliste zu zweit durch und strichen jene Profile an, die sie für automatisierbar hielten. Anschliessend luden sie ein paar Machine-Learning-Spezialisten zu einem Workshop ein, schauten sich mit ihnen die Aufgaben der Berufe an und schätzten ihr Automatisierungspotenzial ein. Danach folgte der entscheidende Teil, die Quantifizierung von Spekulationen mithilfe von ein paar Formeln. Heraus kam die Zahl von «rund 47%», die im Abstract auftauchte, als handle es sich um das Ergebnis einer mathematisch exakten Kalkulation.

Publizistisch ist die Rechnung von Frey und Osborne aufgegangen. Medien auf der ganzen Welt präsentierten die Zahl der sogenannten Oxford-Study als quasi wissenschaftlichen Fakt und intensivierten mit ihr die Automatisierungshysterie. Dabei steigerte sich die Berechnung eines künftigen Risikos zur Behauptung eines determinierten Geschehens. Ein kleines, aber nicht unwesentliches Detail fand dagegen kaum Beachtung: Als das Working-Paper von Frey und Osborne vier Jahre später als offizielle Publikation in einer Fachzeitschrift erschien, war die ominöse Zahl von 47% aus dem Abstract verschwunden.
Unheilige Koalitionen

Gibt es neben den speziellen Faktoren eines jeden Automatisierungshypes auch allgemeine Faktoren, die immer wieder die Angst einer Arbeitsmarktapokalypse schüren? Auffallend ist das wiederkehrende Zusammenwirken von unheiligen Koalitionen, in einer ähnlichen Spielart, wie wir sie aus der Politik kennen. Sie bestehen aus Technologie-Enthusiasten auf der einen und Kapitalismuskritikern auf der anderen Seite. Während Erstere Maschinen für eine unkontrollierbare Macht des Fortschritts halten, die den Menschen bald in allen Tätigkeiten übertreffen und alles Bestehende zur Errichtung einer besseren Welt zerstören werde, sehen Letztere den Moment gekommen, in dem der Kapitalismus seine eigenen Kinder fressen und den Menschen aufs Abstellgleis der Weltgeschichte stellen werde. Die Automatisierungsphantasien von Fortschrittsaposteln und Untergangspropheten treffen sich dabei in einem entscheidenden Punkt: der Annahme eines darwinistischen Kampfes zwischen Mensch und Maschine, dessen Ausgang längst festzustehen scheint.

Es scheint Zeiten zu geben, die für unheilige Koalitionen dieser Art ein besonders günstiges Umfeld bieten. In den 1950er, 1970er und 2010er Jahren genügten jeweils die Erfahrung einer überstandenen Konjunkturkrise und die Erwartung eines bevorstehenden Technologieschubs, um der Prophezeiung vom automatisierungsbedingten Ende der Arbeit neue Nahrung zu geben. Da die konjunkturelle Berg-und-Tal-Fahrt ebenso zu den Konstanten des Kapitalismus gehört wie die technologische Innovation, dürfte auch den Apokalyptikern der Automatisierung die Arbeit so schnell nicht ausgehen. Und erinnern wir uns: Bei ihren christlichen Vorgängern hat es mehr als eineinhalb Jahrtausende gebraucht, bis nicht mehr jeder Komet und jedes Erdbeben als Ankündigung des nahen Weltendes gedeutet worden ist.";https://www.nzz.ch/wirtschaft/dass-uns-wegen-der-automatisierung-die-arbeit-ausgeht-ist-wenig-plausibel-ld.1382478;NZZ;Caspar Hirschi;;;
17.01.2017;«Zürich war die beste Idee, die wir je hatten»;"Gerade einmal sechs Jahre nach der Gründung von Google wurde 2004 ein Büro für zwei Mitarbeiter am Limmatquai eröffnet – das erste Forschungszentrum ausserhalb der USA. Bereits 2008 wechselte der kalifornische Konzern in die Gebäude auf dem Hürlimann-Areal. Und 2016 kündigte Google den zweiten Standort im Gebäude der ehemaligen Sihlpost an. In den neuen Büros an der Europaallee sollen bis ins Jahr 2021 in Zürich 5000 «Zoogler», wie die Zürcher Google-Delegation genannt wird, arbeiten können.
Mitarbeiter aus über 75 Nationen

Dass Google eine Niederlassung in Zürich betreibt, hat die Limmatstadt dem Basler Urs Hölzle zu verdanken, der als einer der Ersten überhaupt von den Google-Gründern Larry Page und Sergej Brin eingestellt wurde. Hölzle selbst kann sein Engagement nicht ganz abstreiten, betont aber, dass Google nur wegen der hervorragenden Infrastruktur und Mitarbeitern weiterhin in Zürich investiere. «Zürich war die beste Idee, die wir je hatten», sagte Hölzle am Dienstag den Medien.
Google in Zürich Bereits vor der Expansion in das Gebäude der ehemaligen Sihlpost betrieb Google in Zürich den grössten Entwicklungsstandort ausserhalb der Vereinigten Staaten – mit über 2000 Mitarbeitern aus mehr als 75 Nationen. Damit zieht Google die Limmatstadt anderen europäischen Standorten, wie Berlin, Dublin oder London, vor. Die neuen Büros bedeuten die bisher grösste Investition Googles in der Schweiz.
Ohne Rutschbahn

Die Arbeitsplätze an der Kasernenstrasse erinnern fast schon an klassische Grossraumbüros und unterscheiden sich damit von den bekannten, verspielten Räumen der Kalifornier. Eine Rutschbahn oder ein Fitnessstudio sucht man aber vergebens. Einzig kreative Meeting-Zimmer und vereinzelte Farbtupfer lassen etwas Jugendlichkeit erkennen. Die Mitarbeiter haben aber auch am Standort Sihlpost ein Restaurant und ein Massagestudio zur Verfügung. Neben den Räumlichkeiten in der ehemaligen Sihlpost, die etwa Platz für 300 Mitarbeiter bieten, wird Google weitere Büros in der Überbauung an der Europaallee beziehen. Sind die Büros auf dem Baufeld G und H bezogen, mietet Google rund ein Drittel der Gesamtfläche der SBB-Liegenschaft.
Maschinelles Lernen im Fokus

Den Platz benötigt Google unter anderem für das bereits 2016 angekündigte Forschungsteam Google Research Europe. Diese Mitarbeiter beschäftigen sich mit maschinellem Lernen. Dank dieser Technologie werden die Algorithmen der Google-Dienste verbessert. Das hat indirekt Auswirkungen auf jedes Produkt des kalifornischen Unternehmens. Es erstaunt also nicht, dass Google-Chef Sundar Pichai das maschinelle Lernen zu einem der wichtigsten Projekte des Unternehmens erklärt hat. Damit festigt Pichai gleichzeitig auch die Wichtigkeit des Standorts Zürich. Ein erstes Produkt aus diesem Entwicklungsschwerpunkt ist der Google Assistant, der vor allem mobil genutzt wird. Über den Assistenten lässt sich über ein Hotel in Locarno buchen oder nach dem Verkehr am Gotthard fragen. Dank «Machine Learning» versteht die Software die natürliche Sprechweise und kann einen Dialog verfolgen.
Schweizer Erfindungen

Die Software steht derweil erst auf drei Produkten zur Verfügung, wovon einzig die Nachrichten-App Allo in der Schweiz verfügbar ist. Die App bietet bildet aber nicht den komplette Funktionsumfang des Google Assistant ab. «Wir befinden uns noch in einer Beta-Phase» erklärt Urs Hölzle und bittet gleichzeitig um Geduld. Noch verpasst man nicht viel, denn Google liegt mit der Entwicklungen hinter dem Branchenprimus Amazon zurück. Dessen Assistant-Plattform Alexa war auf der wichtigsten Messe für Konsumgüter, der CES in Las Vegas, omnipräsent.

Andere Forschungs- und Entwicklungsschwerpunkte sind die Google-Suche, Google Calendar und Gmail. Ausserdem ist das Youtube-Team das grösste ausserhalb der USA und fokussiert sich auf die Werbemöglichkeiten der Plattform. Ingenieure in Zürich sind für zahlreiche Innovationen verantwortlich. Die Idee, Karten von ausgewählten Liegenschaften in Google Maps einzubinden, ist in Zürich entstanden. So ist es beispielsweise im Zürcher Hauptbahnhof möglich, sich vom Reisebüro zum Metzger navigieren zu lassen.
5000 Mitarbeiter bis 2021

Dass in Zürich vermehrt an der Zukunft von Google gearbeitet wird, benötigt das Unternehmen mehr Ingenieure. Bis zu 200 Personen stellt das Unternehmen jährlich ein, die meisten davon Abgänger der ETH und EPFL. «Die ETH gehört zu den besten Hochschulen der Welt», betont Patrick Warnking, Schweiz-Chef von Google. Sie ist damit ein Hauptgrund für die Investitionen in den Standort Zürich.

Damit 2021 die erhofften 5000 Mitarbeiter für Google arbeiten, werden aber mehr als nur ETH-Absolventen benötigt. Wie wichtig ausländische Fachkräfte für Google sind, zeigt sich an der Pressekonferenz: Ausser Urs Hölzle stammt keiner aus der Schweiz. Der Google-Manager Hölzle sieht die Entwicklung des Standorts Zürich optimistisch und betont, dass sich die Investitionen in Zürich klar gelohnt haben. «Und wenn sich die Bedingungen nicht verändern, wird es auch so weitergehen», ist sich der Schweizer Urs Hölzle sicher.
Förderung von Talenten

Diese verschärfen sich aber zunehmend. Ende 2015 beschwerte sich ein leitender Ingenieur öffentlich über das limitierte Bewilligungssystem der Schweiz und die strenge Regelung der Arbeitszeiterfassung. Für leichte Abhilfe sorgt Google selber: Ab dem Sommer 2017 werden in Zürich fünf Informatiker mit dem Schwerpunkt Applikationsentwicklung ausgebildet.

Einen weiteren Fokus legt Google auf die Förderung von Frauen in der Informatikbranche. Im Rahmen des ETH-Schnupperstudiums können Mädchen einen Tag auf den Büros von Google verbringen und damit Einblick in den Alltag eines Software-Entwicklers erhalten. Google beschäftigt weltweit 31 Prozent Frauen. Die Förderung von Talenten unterstützt Google zudem mit zahlreichen Partnerschaften, etwa mit Pro Juventute, dem und der PH Luzern.";https://www.nzz.ch/zuerich/google-in-zuerich-innovationen-made-in-zurich-ld.140285;NZZ;Nino Maspoli;;;
27.11.2019;Blick in die Glaskugel: Das sind die Berufe, die aus der Zukunft kommen ;"«Als Algorithm Bias Auditor stellen Sie sicher, dass unsere Algorithmen, die wir bei der Personalsuche einsetzen, nicht gegen Gesetze verstossen und niemanden diskriminieren.» Das Stelleninserat ist fiktiv und beschreibt einen von 42 Zukunftsberufen, die das Center for the Future of Work der Beratungsfirma Cognizant vorgestellt hat. Die Studienautoren malen sich auch aus, dass dereinst ein Machine Risk Officer in der Chefetage Einsitz nehmen wird, um dafür zu sorgen, dass durch den Einsatz von Algorithmen keine ethischen Grenzen überschritten werden und dem Unternehmen daraus kein Schaden entsteht.
Mit dem Auto in die Lüfte

Dass im Zuge des technischen Fortschritts nicht nur Jobs überflüssig werden, sondern wiederum neue Berufe entstehen, zeigt sich auch im Bereich der Cybersicherheit. Wenn die Gefahr von Cyberangriffen wächst, werden mehr Spezialisten wie Cyber Attack Agents benötigt.

Der technische Wandel schafft aber nicht nur Jobs, um Risiken der Digitalisierung vorzubeugen. Die Entwicklung führt auch zu Erfindungen, die neue Berufe hervorbringen. Werden wir in Zukunft vielleicht mit dem Auto durch die Gegend fliegen? Für das Center for the Future of Work ist dies keine Utopie: Der Beruf des Flying Car Developer könnte entstehen. Vor dreissig Jahren hätten wohl die wenigsten darauf gewettet, dass es einmal den Beruf des Social-Media-Managers geben würde. Denn es ist viel schwieriger, sich auszumalen, welche neuen Berufe dereinst entstehen könnten, als abzuschätzen, für welche zusätzlichen Aufgaben Algorithmen und Roboter künftig noch eingesetzt werden könnten.
Sozialkompetenz und Kreativität

Maschinen übernehmen vor allem repetitive Tätigkeiten, die nur geringfügige Interaktionen mit anderen Mitarbeitern erfordern. Weniger unter Druck sind gemeinhin soziale und kreative Berufe. Denn der Roboter hat keinen Einfall und kennt kein Mitgefühl. Daher erstaunt es nicht, dass Cognizant auch den Spaziergänger/Gesprächspartner als Zukunftsberuf aufführt, der ältere Menschen bei Spaziergängen begleitet und sich mit ihnen unterhält.

In der Arbeitswelt von morgen sind aber nicht nur Sozialkompetenz und Kreativität gefragt. Ebenso bedeutend sind etwa Flexibilität, kritisches Denken und Unternehmergeist. Reines Fachwissen veraltet hingegen zusehends schneller. Zudem wechseln Berufstätige ihren Job im Laufe der Karriere häufiger und arbeiten dabei bei verschiedenen Firmen. Entsprechend muss sich auch die Aus- und Weiterbildung den neuen Anforderungen der Arbeitswelt anpassen. Das Schlagwort des lebenslangen Lernens macht schon länger die Runde.

Doch auch dieser Wandel bietet Chancen für neue Berufsbilder. Im fiktiven Stelleninserat für einen Uni4Life Coordinator heisst es jedenfalls: «Unterstützt von künstlicher Intelligenz beraten Sie Uni-Absolventen bei der Suche nach Lernmodulen, die ihren Bedürfnissen am besten entsprechen.»

Eine Auswahl weiterer Zukunftsberufe:

    Fitness Commitment Counselor: Bietet Beratungen aus der Ferne an und überwacht digital die körperliche Aktivität seiner Kunden.
    Cyber-Stadtplaner: Sorgt dafür, dass die Daten in den Städten ungehindert fliessen und die technischen Anlagen funktionieren.
    Chief Purpose Planner: Berät Firmen dabei, einen Sinn für ihre Geschäftstätigkeit zu entwickeln und kommuniziert dies gegenüber allen Anspruchsgruppen des Unternehmens.
    Persönlicher Gedächtniskurator: Schafft für ältere Menschen mit Demenzerkrankungen virtuelle Umgebungen, in denen sie sich zurechtfinden.
    Genomic Portfolio Director: Mit dem Fortschritt der biotechnischen Forschung entstehen immer mehr pharmazeutische und medizinische Angebote. Der Manager einer Pharmafirma betreut das Angebot, entwickelt es weiter und organisiert den Vertrieb.
    Virtual Identity Defender: Hilft den Kunden im Zeitalter von Fake-News dabei, dafür zu sorgen, dass ihre virtuelle Identität nicht verfälscht wird. Dazu wird ein Prüfzeichen erstellt.
    Ethik-Beauftragter: Stellt sicher, dass die Verteilung des Firmeneinkommens den ethischen Standards des Unternehmens entspricht.";https://www.nzz.ch/wirtschaft/welches-sind-die-berufe-der-zukunft-ein-blick-in-die-kristallkugel-ld.1521358;NZZ;Natalie Gratwohl;;;
30.06.2016;Da geht noch was;"Wenn Mike Schaffner nach Hause kommt, öffnet sich sein Türschloss wie von Geisterhand. Ein kurzes Surren, ein Blinken, schon ist die Türe entriegelt. Schaffner, 25, ist Kinotechniker und Transhumanist. Er findet, dass man die menschliche Evolution nicht nur der Natur überlassen sollte. Darum hat er sich den «Near-field communication»-Chip, mit dem er seine Haustüre öffnet, vor zwei Jahren selbst unter die Haut gespritzt.

Der NFC-Chip, nicht grösser als ein Reiskorn, sitzt zwischen Schaffners linkem Daumen und Zeigefinger. Mit ihm kann er auch die Türe des Fitnessstudios öffnen, sein Handy entsperren und seine Kontakte zwischen Telefon und Chip übermitteln. Bald soll der Chip auch seinen Pass und sein Portemonnaie ersetzen. Und das ist noch lange nicht das Ende.

Menschen wie Schaffner gibt es auf der ganzen Welt. Transhumanisten interessieren sich für moderne Technologien und Zukunftsszenarien. Sie tauschen sich an den Universitäten, an Kongressen oder in Facebook-Gruppen aus. Die menschliche Natur ist für sie nichts Gegebenes, sondern formbar.

Längst nicht alle haben Chips implantiert, längst nicht alle empfinden die Technik als Segen. Sie alle aber vereint eine Geisteshaltung: Der Mensch muss sein Schicksal, und damit ist vor allem die Evolution gemeint, selbst in die Hand nehmen. Wenn der Nutzen das Risiko übersteigt, sollte man in die Natur eingreifen. Das Ziel: das menschliche Leben zu verbessern. Nur darüber, was das menschliche Leben besser macht, ist man sich uneins.
Wie die Katze, so das Herrchen

Schaffner ist ein sanfter Mensch. Er lebt mit seinen zwei Katzen und einem Mitbewohner in einer WG am Zürcher Stadtrand, hier ist es ruhig. Während er eine Katze streichelt, sagt er: «Schon als Kind wollte ich ein Cyborg sein.» Dank seinem NFC-Chip ist er der Symbiose zwischen Mensch und Maschine bereits ein Stückchen näher gekommen.

Angst vor Missbrauch hat er nicht. Schaffners Implantat ist ein Passivchip, von sich aus versendet er keine Daten. Auf die Daten zugreifen kann man nur, wenn man das Lesegerät direkt an die Haut hält. Die Risiken seien überschaubar, findet er. «Das Portemonnaie ist leichter geklaut als meine Hand.»

Die beiden Katzen leben Schaffners Traum vom digitalen Mischwesen schon länger. Sie wurden kurz nach der Geburt vom Tierarzt gechipt. Der modernen Technik können sie aber nicht so viel abgewinnen. Rollt der Staubsaugerroboter vorbei, suchen sie das Weite.

Hat der Kinotechniker zu viele Science-Fiction-Streifen gesehen? Schaffner verneint. Die Faszination für den Transhumanismus habe bei ihm andere Gründe: «Im Leben geht es doch darum zu empfinden. Ich möchte unterschiedliche Erfahrungen machen.» Darum möchte er sich auch bald einen 3mm grossen Magneten in die Fingerkuppen einsetzen lassen. Damit kann er die magnetischen Felder in seiner unmittelbaren Nähe spüren. Vögel haben diese Fähigkeit bereits.

    «Transhumanisten sehen im Menschen nicht die Krone der Schöpfung. Denn er ist noch in Arbeit.»

Wie sehr dieser Sinn auch Menschen nützt, ist unklar. Schaffner aber geht es nicht um den Nutzen, er will fühlen. Er möchte erleben, wie sich der Körper an die Veränderung anpasst. Dieser Reichtum an Erfahrungen bedeutet für ihn Lebensqualität. «Die Technik verbessert mein Leben», sagt er. Was Schaffner macht, mag befremdlich erscheinen. Die meisten von uns aber leben längst in einer ähnlichen Symbiose mit der Technik wie er. Mit Instant-Messaging können wir in Echtzeit mit Menschen am anderen Ende der Welt kommunizieren, mit dem Smartphone auf das gesammelte Wissen der Menschheit zugreifen. Was vor dreissig Jahren noch nach Science-Fiction klang, ist heute nicht mehr wegzudenken. Die Technik ist ein Teil von uns geworden, auch wenn wir sie nicht in uns tragen. Aber auch das könnte nur eine Frage der Zeit sein.

Die Realität ist: «Wir akzeptieren Hörgeräte und Herzschrittmacher. Also werden wir uns wahrscheinlich auch an andere technische Geräte in unseren Körpern gewöhnen.» Das sagt Johann Roduit, Geschäftsführer am Institut für Biomedizinische Ethik der Universität Zürich. Neben seiner wissenschaftlichen Tätigkeit ist Roduit Mitgründer des Think-Tanks NeoHumanitas, der sich mit den Auswirkungen von zukünftigen Technologien auf die Gesellschaft befasst.

Roduit ist der Meinung, dass es einen Unterschied macht, ob man Technologien nur als Hilfsmittel nimmt oder mit ihnen zu einer halbdigitalen Existenz verschmilzt. Er sagt: «Wir könnten das verlieren, was uns als Menschen ausmacht. Was auch immer das ist.»

Der Transhumanismus könne zwei Fragen nicht vermeiden: Was heisst es, ein Mensch zu sein, und was bedeutet es, das Leben zu verbessern? Mehr Technik oder höhere Intelligenz allein würden noch nicht direkt implizieren, dass die Lebensqualität steigt. Mehr bedeute nicht «besser». Was, wenn man sich durch Eingriffe so verändere, dass man den Zugang zu seinen Mitmenschen verliere? Das würde man wohl kaum eine Verbesserung nennen.
Offen für Veränderungen

In seiner Forschung hat Roduit herausgefunden: Es sind implizite Vorstellungen darüber, was es heisst, menschlich zu sein, die beeinflussen, wie der Einzelne zur Verbesserung steht. Für Transhumanisten ist der Mensch nicht die Krone der Schöpfung, sondern lediglich ein Produkt in Arbeit. Aus diesem Grund seien die meisten Transhumanisten enthusiastischer bezüglich Verbesserungen als andere Menschen. Mit der Aufklärung begann der Mensch, sich seines Verstandes selbst zu bedienen. Nun, scheint es, muss der Mensch Verantwortung für seine Evolution übernehmen. Felicitas Reddel, eine Transhumanistin, sagt: «Wir sind denkende Menschen, die bestimmen können, nach welchen Werten wir leben wollen. Also müssen wir auch prüfen, ob es sinnvoll ist, unsere Natur zu verändern.»

Reddel ist 22 und lebt in Basel. Hier wohnt sie, nicht weit vom Rhein, mit ihrem Verlobten Max Kocher, der ebenfalls Transhumanist ist. In der Altbauwohnung deutet nichts auf die Interessen des jungen Paares hin. Die Dielen knarren, die Türen quietschen. «Nicht sehr transhumanistisch», sagt Reddel grinsend, «aber dafür günstig und gemütlich». Reddel studiert Biologie an der Universität Basel, der 24-jährige Kocher vertieft sich nach einem Bachelor in Soziologie und Philosophie in die Kognitionswissenschaften. Kennengelernt haben sie sich, wie könnte es denn anders sein?, an einem Vortrag über Transhumanismus.
Die Evolution als Experiment

Zum Transhumanismus fand Reddel, als ihr im Studium klar wurde, dass die Natur gar nicht so perfekt ist, wie sie immer gedacht hatte. «Die Evolution ist ein riesengrosses Experiment, mit dem einzigen Ziel, unsere Gene weiterzugeben», sagt sie.

Vieles aber hätten wir nur erreicht, weil wir eingegriffen hätten: Bessere Hygienestandards senkten die Sterblichkeit, durch Impfungen liessen sich viele Krankheiten ausrotten. Natürlich sei es gefährlich, mit unvollständigem Wissen die Natur zu verändern. Aber es habe eben auch Konsequenzen, wenn man es nicht tue, sagt sie.

    Hätte Kocher nämlich früher gewusst, dass selbstfahrende Autos bereits 2018 auf den Markt kommen sollen, hätte er andere Prioritäten gesetzt.

Reddel und Kocher gehören zu den Transhumanisten, die versuchen, den zukünftigen Entwicklungen unvoreingenommen zu begegnen. «Technovolatil» nennt Kocher diese Haltung: Es sei unklar, ob die neuen Technologien für die Menschen positiv oder negativ seien. Sicher sei einzig, dass sie uns stark beeinflussen werden.

Nach ihrem Bachelor in Biologie wollte Reddel eigentlich Nachhaltige Entwicklung studieren. Seit sie sich aber ausgiebiger mit den technologischen Fortschritten aus dem Silicon Valley beschäftigt, hat sie sich entschieden, einen Master in Informatik anzuschliessen. In Anbetracht der rasanten Entwicklungen ergebe das mehr Sinn, so Reddel. Kocher nickt.
Organe aus dem 3-D-Drucker

Hätte Kocher nämlich früher gewusst, dass selbstfahrende Autos bereits 2018 auf den Markt kommen sollen, hätte er andere Prioritäten gesetzt. «Mein Führerschein war eine Geldverschwendung», sagt er. Seinem Bruder habe er erfolgreich von der Autoprüfung abgeraten. «Immerhin», sagt er und seufzt.

Spricht man mit Kocher, glaubt man, von der Zukunft nichts mitbekommen zu haben. 2018 soll die erste Transplantation mit künstlichen Organen erfolgen. Die Organe spuckt ein 3-D-Drucker aus. Bitte was? Immerhin können Maschinen den Journalisten noch nicht das Handwerk ersetzen. Falsch gelegen. «Es werden heute künstliche Intelligenzen entwickelt, welche die Denkarbeit übernehmen», sagt Kocher, «sogar Artikel können sie schon schreiben».

Kocher und Reddel haben ein gemeinsames Berufsziel. Sie wollen in der Sicherheitsforschung zu künstlicher Intelligenz arbeiten. Denn sie ahnen: Mit den modernen Technologien kommt nicht nur Gutes auf uns zu. Starke künstliche Intelligenzen sind sehr lernfähige Maschinen. Im Gegensatz zu schwachen künstlichen Intelligenzen können sie mehr als nur eine zugewiesene Aufgabe lösen. Starke künstliche Intelligenzen geben sich die Aufgaben selbst und entwickeln vielleicht sogar eigene Interessen. Noch gibt es sie zwar nicht. Doch spätestens seit die Software «AlphaGo» im März dieses Jahres einen der besten Spieler des Brettspiels «Go» zum vierten Mal in fünf Spielen besiegt hat, wird ihre Existenz zumindest wahrscheinlicher. Reddel und Kocher wollen vorbereitet sein.

Schon heute absolviert das Paar über die Plattform «Udacity» Kurse zur Einführung in künstliche Intelligenz. «Udacity» ist sozusagen die Online-Universität des Silicon Valley. Kocher und Reddel sind sich einig, dass sie in diesen Kursen viel mehr lernen als im richtigen Hörsaal. «Der Lehrstil an den heutigen Unis ist nicht mehr zeitgemäss», sagt Reddel. Bei «Udacity» könne man sein Wissen nach jedem Informationsblock gleich anwenden, man lerne mehr pro Zeiteinheit.

Geht ihr Plan auf, wollen die beiden ab 2017 einen Online-Master in Machine Learning machen. So können sie verstehen, wie sich lernende Computer neue Aufgaben erschliessen. Da sie später wahrscheinlich in den USA oder England arbeiten werden, sprechen Kocher und Reddel englisch miteinander.

Bevor die Zukunft ruft, stellt sich noch eine Frage. Was unterscheidet heute ein transhumanes Leben von einem normalen? Das Paar findet bis auf seinen Lebensplan, in die Sicherheitsforschung zu gehen, keinen Unterschied. Wirklich nicht? Reddel fällt ein, dass sie Kochers Schlaf mit einer App messen. Seit sie seine Daten auswerten und Anpassungen machen, seien seine Tiefschlafphasen länger und erholsamer geworden. «Ganz vergessen», sagt Reddel lachend. Macht nichts. Auch gegen das Vergessen wird es irgendwann bestimmt eine Technologie geben.";https://www.nzz.ch/karriere/studentenleben/transhumanismus-da-geht-noch-was-ld.129717;NZZ;Sandrine Gehriger;;;
13.03.2017;Ich bin kein Roboter;"Nach einer längeren Beta-Phase hat Google eine neue Lösung für die Abwehr von Bots auf Websites lanciert: Die Lösung Invisible reCaptcha braucht keine Interaktion des Anwenders mehr, weil sie menschliche Anwender selbständig im Hintergrund erkennt. Entwickler von Websites können unsichtbare Captchas ab sofort in Websites einbinden. Dafür stellt Google drei verschiedene auf Javascript basierende Methoden zur Verfügung.

Als Captcha (Completely Automated Public Turing test to tell Computers and Humans Apart) bezeichnet man einen vollautomatischen öffentlichen Turing-Test zur Unterscheidung von Computern und Menschen. Der im Jahr 2000 entwickelte Mechanismus will verhindern, dass Bots auf Websites automatisch Formulare ausfüllen oder Konten anlegen können.
Gratwanderung

Die Entwicklung eines Captcha ist eine Gratwanderung: Der Mechanismus muss möglichst benutzerfreundlich sein und gleichzeitig technisch so anspruchsvoll, dass automatisierte Scripts an der Hürde scheitern.

Die erste Generation von Captchas, die auf gedehnten und schwer lesbaren Zahlen und Zeichen basierte, nervte viele Anwender und war auch nicht barrierefrei. Sehbehinderte Anwender hatten grosse Mühe, die Zeichen zu erkennen. Neuere Versionen von Captchas nutzen Fotos, Audio-Schnipsel oder Videos, um menschliche Anwender von Bots zu unterscheiden.

Viele Websites verwenden die von Google vor rund drei Jahren eingeführte Lösung reChapta, bei der der Anwender im Idealfall nur noch im Feld „Ich bin kein Roboter“ einen Haken setzen muss. Erkennt das Programm nicht zweifelsfrei einen menschlichen Benutzer, muss der Anwender in einem zweiten Schritt dennoch ein Rätsel lösen und aus einer Bildserie ähnliche Sujets anklicken.
Maschinelles Lernen

Google macht aus verständlichen Gründen nur allgemeine Aussagen über die Funktionsweise des unsichtbaren Captchas. Der neue Mechanismus basiert laut der Google auf fortgeschrittener Risikoanalyse und nutzt die Technik des maschinellen Lernens. Google hat das Anwenderverhalten schon bisher analysiert und zum Beispiel Cookies, die Verweildauer auf einer Seite oder die Mausbewegungen ausgewertet.

Betreibern von Botnetzen gelang es in der Vergangenheit immer wieder, Captchas zu umgehen. So wurde etwa Captcha-Schutz bei der Eröffnung eines Mail-Kontos bei Gmail ausgehebelt und Googles Maildienst für den Versand von Massenmails missbraucht.";https://www.nzz.ch/kuenstliche-intelligenz-ich-bin-kein-roboter-ld.151041;NZZ;Claude Settele;;;
17.01.2017;Der Mann, der E-Mail nicht erfunden hat;"Gerüchte um das Sexleben von Prominenten waren
der bevorzugte Inhalt von «Gawker». Die Internet-Publikation musste aufgeben, nachdem einer der Betroffenen – der Wrestler Hulk Hogan – vor einem amerikanischen Gericht eine hohe Schadenersatzforderung durchsetzen konnte. Die Geschichte wird
jetzt wieder aufgewärmt im Zusammenhang mit den Schadenersatzforderungen, mit denen sich die Internet-Publikation «Techdirt» konfrontiert sieht. Der Anwalt, der «Gawker» in den Ruin getrieben hat, ist derselbe,
der jetzt «Techdirt» herausfordert. Dort ging es um ein Sexvideo, hier geht es um Computercode, dort war der Kläger ein muskelbepackter Sportler, hier ist es ein schmächtiger Programmierer. Dort konnte der Wrestler mit Sympathien seitens des Publikums rechnen, hier schüttelt man nur den Kopf über die Geltungssucht, über die Eitelkeit eines Programmierers, der etwas erfunden haben will, das es schon gab, bevor er geboren wurde.
(Bild: imago stock &amp; people)
(Bild: imago stock & people)

Es geht nicht um Sex, aber schon um Beziehungen, es geht um ein Beziehungsdelikt – im weitesten Sinn, es geht um Beziehungen zwischen Maschinen und um den Code, der diese Beziehungen ermöglicht. Es geht um E-Mail. Der in Indien geborene und in den USA aufgewachsene Biologe Shiva Ayyadurai behauptet, die E-Mail erfunden zu haben. Das ist lächerlich genug. Aber Ayyadurai ist offensichtlich wild entschlossen, mit Drohungen und juristischen Mitteln gegen jede Publikation vorzugehen, die dieser absurden Behauptung widerspricht.

    Man mag Ayyadurai belächeln oder bemitleiden. Aber es geht in dieser bizarren Geschichte nicht allein um ihn. Es geht um den Personenkult in der Technik, um die populäre Vorstellung, dass Technik durch einsame Genies aus dem Nichts heraus erschaffen werde, dass Technikgeschichte eine Abfolge von Grosstaten grosser Männer sei.

1963 geboren, erhielt Ayyadurai als 15-Jähriger Zugang zu einem ausserschulischen Programmierkurs. Ende der 1970er Jahre entwickelte er für ein Spital eine Minicomputer-basierte Kommunikationssoftware, die er «Email» nannte. 1982 gelang es ihm, diese Bezeichnung als Marke schützen zu lassen. Seither möchte er als Erfinder der E-Mail respektiert werden. Doch der Austausch von elektronischen Nachrichten zwischen verschiedenen Computerbenutzern war bereits zu Beginn der 1960er Jahre üblich. Standards für die Übermittlung von E-Mails über Internetverbindungen wurden 1973 festgelegt.

Man mag über die Eitelkeit, über die Geltungssucht Ayyadurais lächeln. Vielleicht wäre Mitleid eher angebracht, vielleicht geht es hier um die traurige Kindheit eines hochbegabten Kindes, das wegen seiner fremdländischen Abstammung gehänselt wurde. Ayyadurai hat sich auch schon beklagt, man gönne ihm den Ruhm eines Erfinders nur wegen seiner dunklen Hautfarbe nicht.

Man mag Ayyadurai belächeln oder bemitleiden. Aber es geht in dieser bizarren Geschichte nicht allein um ihn. Es geht um den Personenkult in der Technik, um die populäre Vorstellung, dass Technik durch einsame Genies aus dem Nichts heraus erschaffen werde, dass Technikgeschichte eine Abfolge von Grosstaten grosser Männer sei. Als die deutsche Depeschenagentur den im Frühjahr 2016 verstorbenen Ray Tomlinson als Erfinder der E-Mail ehrte, wurde der Nachruf fleissig reproduziert. Seit der Computerwissenschafter Leonard Kleinrock Ende der 1990er Jahre angefangen hat, sich als Erfinder des Internets aufzuspielen, wird jeweils am 29. Oktober der Geburtstag des Internets gefeiert. Doch jede nichttriviale, sozial bedeutsame Technik, von der Dampfmaschine bis zur Suchmaschine, vom Flügeltelegraf bis zum Farb-TV, vom Volksempfänger bis zu Social-Media-Facebook-Twitter, wurde nicht erfunden, sondern ergab sich durch das komplizierte Zusammenwirken von vielen Kräften, durch die Ideen vieler Menschen, durch Interferenzen zwischen tatkräftigen Individuen und gesellschaftlichen Konstellationen.";https://www.nzz.ch/meinung/kolumnen/technikgeschichte-der-mann-der-e-mail-nicht-erfunden-hat-ld.140169;NZZ;Stefan Betschon;;;
28.09.2020;Echte Gefahr geht von Schlamperei aus, nicht von Killerrobotern – ein Crashkurs über Algorithmen;;https://www.nzz.ch/pro-global/technologie/crashkurs-algorithmen-keine-angst-vor-killerrobotern-ld.1578860?reduced=true;NZZ;Stefan Häberli;;;
02.08.2020;Mitten in der Corona-Pandemie: Siemens Healthineers kauft US-Krebsspezialisten Varian für gut 16 Milliarden Dollar;"Mitten in der Corona-Pandemie tätigt der deutsche Technologiekonzern Siemens eine milliardenschwere Investition in den USA. Die Tochtergesellschaft Siemens Healthineers, in der Siemens seine medizintechnischen Aktivitäten zusammengefasst hat, will den amerikanischen Krebsspezialisten Varian Medical Systems für 16,4 Mrd. $ erwerben. Der Konzern mit Sitz in Berlin und München bezeichnet die Akquisition als transformatorischen Meilenstein in der Umsetzung der Strategie «Vision 2020+».
Global führend bei der Krebsversorgung

Laut Siemens entsteht durch den Kauf ein global führendes Unternehmen für die Krebsforschung und die Krebstherapeutik. Varian gilt als führendes Unternehmen bei der Krebsbehandlung; ein wichtiger Schwerpunkt sind die Strahlentherapie und die dazugehörige Software. Im Jahr 2019 machten die Amerikaner einen Umsatz von 3,2 Mrd. $, wiesen eine adjustierte operative Marge von 17% aus und beschäftigten rund 10 000 Mitarbeiter. Die Firma aus dem kalifornischen Palo Alto nutze zunehmend Technologien wie künstliche Intelligenz, maschinelles Lernen und Datenanalyse, um die Krebsbehandlung weiter zu verbessern, hiess es von Siemens. Durch die Akquisition kann Healthineers laut Beobachtern die klinische Kette immer besser abdecken – von der Prävention über die Diagnostik durch Röntgengeräte und Computertomografen bis hin zur Therapie.

Siemens Healthineers wurde im Frühjahr 2018 vom Mutterkonzern abgespalten und an die Börse gebracht. Der Börsengang diente auch dazu, eine Akquisitionswährung für potenzielle Zukäufe zu schaffen. Siemens behielt aber an dem in Erlangen domizilierten Unternehmen eine Beteiligung von 85%. Dieser Anteil sinkt im Rahmen der Akquisition von Varian durch Siemens Healthineers allerdings auf 72%, weil der Mutterkonzern an der geplanten Kapitalerhöhung nicht teilnehmen will. Durch die Erhöhung des Streubesitzes soll die Eigenständigkeit der Siemens-Tochter unterstrichen werden.

Den Kaufpreis will der Konzern durch eine Mischfinanzierung aus der Ausgabe von neuen Siemens-Healthineers-Aktien sowie der Begebung von Anleihen stemmen; wobei man die Erlöse aus den Anleihen konzernintern zu marktüblichen Konditionen an Siemens Healthineers weiterreichen will. Die Übernahme steht wie üblich unter dem Vorbehalt der Zustimmung der Varian-Aktionäre, der Erteilung behördlicher Genehmigungen sowie anderer vereinbarter Vollzugsbedingungen.
Mehr Eigenständigkeit für wichtige Tochtergesellschaften

Der Technologiekonzern befindet sich unter dem Vorstandsvorsitzenden Joe Kaeser seit einigen Jahren wieder in einem grossen Umbau, bei dem einige Geschäftsbereiche verkauft und andere in Tochterfirmen ausgegliedert werden. Viele Geschäftsbereiche sollen künftig deutlich eigenständiger geführt werden, als das früher in der engen Konzernstruktur der Fall gewesen ist. Diese grössere Eigenständigkeit haben beispielsweise Siemens Energy (Energiegeschäft, steht für 40% der Umsätze und soll im September an die Börse kommen) und Siemens Mobility (Mobilitätslösungen für Schienen- und Strassenverkehr) erhalten. Darüber hinaus wurden Tochtergesellschaften wie Siemens Healthineers und Siemens Gamesa Renewable Energy (Teil von Siemens Energy) an die Börse gebracht, wobei Siemens jeweils eine Mehrheitsbeteiligung behalten hat.

Konzernchef Kaeser begrüsste die Akquisition in den USA ausdrücklich. Ein derartiger transformatorischer Schritt wäre in der Konglomeratsstruktur der alten Siemens AG nicht möglich gewesen, sagte der 63-Jährige, der in der letzten Phase seiner Amtszeit ist. Die Verantwortung über die künftigen Kerngeschäfte hat bereits sein Stellvertreter Roland Busch übernommen, der kommenden Februar auf der nächsten Hauptversammlung zum Vorstandsvorsitzenden des Siemens-Konzerns gewählt werden soll. Kaeser wiederum will dann, dem Vernehmen nach ohne Abkühlungsphase, Aufsichtsratsvorsitzender von Siemens Energy werden. Die Energiesparte ist seit der Gründung des Unternehmens vor 173 Jahren durch Werner von Siemens und Johann Georg Halske ein bedeutender Bestandteil des Konzerns.";https://www.nzz.ch/wirtschaft/mitten-in-der-corona-pandemie-siemens-healthineers-kauft-us-krebsspezialisten-varian-fuer-gut-16-milliarden-dollar-ld.1569311;NZZ;Michael Rasch;;;
03.10.2019;Der Computer bestimmt über meine Finanzen, aber kann er es mir erklären?;"Von einem Menschen abgewiesen zu werden, ist nicht schön. Wenn ein Computer das tut, kann es noch unangenehmer sein. Doch genau das passiert immer öfter, weil Computer dank künstlicher Intelligenz (KI) Entscheide treffen, für die früher Kundenberater und andere Mitarbeiter zuständig waren. Das gilt auch in der Finanzbranche, zum Beispiel in Grossbritannien. Dort zeigt das Versicherungsportal GoCompare, wie KI und maschinelles Lernen nicht nur bestimmen, welche Offerten ein Antragsteller bekommt, sondern auch, ob er als Betrüger eingestuft und deswegen blockiert wird.
Der Algorithmus ist skeptisch

GoCompare ist ein führendes britisches Vergleichsportal, bei dem Nutzer Angebote für Auto-, Hausrat- und alle möglichen anderen Versicherungen einholen können. Rund 5 Mio. Besuche verzeichnet die Website im Monat. Wenn die Nutzer ihre Daten eingeben, denkt der Computer im Hintergrund immer mit: Ein lange und intensiv trainiertes KI-System versucht zu erkennen, ob jemand falsche Angaben macht, um seine Prämien zu senken. Es «überlegt» bei jeder Eingabe, ob es diese Art von Kunden schon einmal gesehen hat und welche Entscheidungsregeln in diesem Fall gelten. «Die Kundendaten werden in Echtzeit überwacht, um verdächtiges Verhalten zu erkennen», erläutert Fleur Lewis, Chefin der Betrugsüberwachung von GoCompare.

Kommt die künstliche Intelligenz zum Schluss, dass ein Betrugsversuch vorliegt, erhält der Nutzer keine Offerte, sondern eine Fehlermeldung: Leider sei es nicht möglich gewesen, für diese Angaben ein Versicherungsangebot zu generieren, heisst es dann. Das ist nicht gelogen: Das System vergleicht das Risikopotenzial, das es bei einem Kunden erkennt, mit der Risikotoleranz der Versicherungsanbieter. Manche Versicherer sind bereit, «riskantere» Kunden zu akzeptieren, und gehen das Wagnis eines Betrugsversuchs ein. In diesem Fall erhält der Nutzer von diesem Anbieter eine teurere Offerte, als wenn ihn das System als unbedenklich einstufen würde. Dass die KI diesen Entscheid getroffen hat, sieht der Kunde nicht – er sieht nur den Preis. Ist das Risiko jedoch zu hoch, beisst kein Versicherer an.
Jeder Klick wird überwacht

«Im Kampf gegen Betrug helfen wir der Branche, das Kundenverhalten zu verstehen», sagt Lewis. 75% aller Autoversicherungen würden über Vergleichsportale im Internet abgeschlossen. Der Verlust durch Versicherungsbetrug gehe in die Milliarden, so Lewis. Deshalb versucht das System zu erkennen, ob jemand zum Beispiel bei Beruf, Wohnort oder Vorstrafen lügt. Oder subtiler: Wenn etwa ein junger Autolenker bei der Autohaftpflicht der Eltern mitversichert werden soll, die Eltern dann aber bei der Angabe des Hauptfahrers des Autos zögern, fällt das der KI auf. Natürlich seien solche Falschangaben manchmal schwer zu beweisen, räumt Lewis ein. Aber wenn möglich, wolle man den Versicherer alarmieren.

Wenn die KI von GoCompare skeptisch wird, erhält der Nutzer entweder eine teurere Offerte oder gar keine. Dass der Computer in ihm ein Risiko erkannte, wird dem Klienten nicht dargelegt. Hat er eine Nachfrage, muss er sich an den Kundendienst wenden. Damit umgeht die Firma ein grosses Problem in der Anwendung von künstlicher Intelligenz: Wie erklärt man Entscheide, die Algorithmen getroffen haben? Theoretisch sind die KI-Anwendungsmöglichkeiten im Finanzsektor gross. Vieles spielt sich hinter den Kulissen ab, etwa bei der Analyse von Zahlungsströmen. Doch was, wenn die Entscheide dem Klienten präsentiert werden und kein Mitarbeiter involviert ist?
Einfachheit kann ein Nachteil sein

«Die schnelle Entwicklung von KI stellt Fragen, auf die wir noch keine Antworten haben – zuoberst nach Transparenz und Erklärbarkeit», sagt Christopher Woolard, Direktor für Strategie und Wettbewerb bei der britischen Finanzaufsicht FCA. In einer Rede wies er im Sommer auf die Wechselwirkungen zwischen einem guten Algorithmus und einer guten Erklärbarkeit hin: Wenn man für einen Finanzentscheid einen Algorithmus verwende, der leichter nachzuvollziehen und damit leichter erklärbar sei, könne diese Vereinfachung seine Voraussagekraft einschränken. «Was hat Vorrang, die Präzision der Vorhersage oder die Fähigkeit, die Vorhersage zu erklären?», fragte Woolard.

Diese Herausforderungen sind aus Sicht des Finanzaufsehers ein Grund, warum die Branche sich nicht euphorisch auf künstliche Intelligenz stürzt. Die Folgen der globalen Finanzkrise spielten in den Köpfen noch eine Rolle, so Woolard: «Zu viel Vertrauen wurde damals in Produkte und Instrumente gesteckt, die man nicht korrekt verstand.» So erklärt es sich die FCA, dass grosse Finanzdienstleister, zum Beispiel etablierte Banken, bei der Anwendung von KI zögerlicher sind als junge Firmen.
Nicht alles ist Betrug

GoCompare wurde 2006 aufgebaut, darf noch als jung gelten und suchte sich Ende 2018 einen noch jüngeren Partner, um das alte Überwachungssystem zu ersetzen: Featurespace, gegründet von Forschern der Universität Cambridge, hat sich auf die Verhaltensforschung mittels künstlicher Intelligenz spezialisiert. Die so entwickelte Plattform überwacht Konsumentendaten in Echtzeit, um verdächtige Entwicklungen zu entdecken und zu blockieren, ohne ehrliche Kunden auszuschliessen. Das ist wichtig, denn manche GoCompare-Nutzer spielen nur mit unterschiedlichen Angaben, um zu sehen, wie sich die Prämien verändern – zum Beispiel, wenn sie aus Interesse einen anderen Beruf eintippen, am Ende aber bei der Wahrheit bleiben.

Andere Firmen zögern. Nur 39% der britischen Finanzdienstleister setzen KI im Umgang mit Klienten ein, zeigte im Frühjahr die erste systematische Untersuchung dieser Art, veranstaltet von der Bank of England (BoE) und der FCA. Allerdings wird die Computerintelligenz schon von mehr als der Hälfte der Firmen für das interne Risikomanagement angewandt. Grundsätzlich halten viele Finanzdienstleister das Thema für strategisch wichtig, bleiben aber vorsichtig. Einige Anbieter nannten die Gefahr, dass der fehlerhafte Einsatz von KI ganz neue und komplexe Risiken schaffen könne.
Am Ende hängt es am Menschen

James Proudman, BoE-Aufsichtsdirektor für Retail-Banken, hat drei Probleme identifiziert. Erstens sei jedes KI-Modell nur so gut wie die Daten, mit denen es gefüttert werde, sagte er bei einem Vortrag im Juni. Zweitens spielten auch bei künstlicher Intelligenz die Irrtümer von Menschen eine grosse Rolle, denn es seien Menschen, welche die Programme schrieben, mit denen die Maschinen lernten und entschieden. Ausserdem werde mit zunehmender Komplexität der Modelle die Suche nach den Ursachen von Fehlern erschwert, und das nehme drittens das Management, das die KI-Anwendung überwachen müsse, sehr stark in die Pflicht. Fleur Lewis von GoCompare ist aber optimistisch: Die Erfahrungen mit dem KI-Einsatz seien sehr positiv und die Branche zufrieden.";https://www.nzz.ch/wirtschaft/kuenstliche-intelligenz-wenn-der-computer-die-finanzen-bestimmt-ld.1512486;NZZ;Benjamin Triebe;;;
21.01.2020;Herr Socher, welcher Aspekt in der Entwicklung von künstlicher Intelligenz bereitet Ihnen die meisten Sorgen?;"Künstliche Intelligenz, dieser Begriff schwebt wie ein Damoklesschwert über unserer Zeit. Ökonomen verwenden ihn, wenn sie von bevorstehenden Umwälzungen am Arbeitsmarkt sprechen, Politiker als Rechtfertigung für neue Regulierungen und Veranstalter, um mit einem Modewort möglichst viele Gäste anzulocken.

Tatsächlich ist künstliche Intelligenz ein Überbegriff für intelligente Computerprogramme, also Algorithmen, die lernen. Schrittweise erobert die «KI» derzeit unseren Alltag: etwa, wenn Siri, Alexa oder andere Sprachassistenten unsere Stimme erkennen und unsere Fragen beantworten; wenn wir die Kamera des Smartphones auf ein chinesischsprachiges Schild halten und auf dem Display die deutsche Übersetzung erscheint; oder wenn das Auto zunehmend alleine fährt.

Doch das sei nur der Anfang, ist Richard Socher überzeugt. Der Dresdner zählt zu den weltweit führenden Köpfen im Forschungsbereich der KI, insbesondere im «deep learning», also im komplexen maschinellen Lernen. Mit Jeans, zerknittertem Hemd und Lockenkopf sieht der 35-Jährige auf den ersten Blick aus wie einer der unzähligen Millennials, die an diesem Nachmittag im Grossraumbüro von Salesforce’ Niederlassung in Palo Alto arbeiten. Doch Socher ist ihr aller Vorgesetzter. Als Chefwissenschafter von Salesforce verantwortet er die Grundlagenforschung und deren Anwendungen in den Produkten des Software-Riesen – und beeinflusst damit, auf welche KI-Anwendungen Tausende Firmen weltweit Zugriff haben. «German Wunderkind» lautet sein Spitzname im Silicon Valley. Herr Socher, wie wird sich KI auf unsere Gesellschaft auswirken?

KI wird ein zivilisatorischer Sprung sein, so wie der von der Landwirtschaft zur Industriegesellschaft oder der durch Computer und das Internet – alles wird effizienter werden. Langfristig bin ich sehr optimistisch. Wenn wir in 50 Jahren zurückschauen, wird sich niemand mehr fragen, warum Menschen bestimmte repetitive Aufgaben noch machen sollten, wenn die doch die KI machen kann. Kurzfristig sorge ich mich jedoch, frühere Umwälzungen sind auch nie glimpflich abgelaufen. Die Politik und Unternehmen müssen die Veränderungen, welche die KI bringt, abfedern und den Menschen helfen, mit den sozialen Folgen des Wandels klarzukommen.

Diese treffen besonders niedrigqualifizierte Arbeiter, oder?

Ja, alle repetitiven Arbeiten. Erntehelfer ist so ein Beispiel für eine Aufgabe, die in drei bis fünf Jahren vollständig automatisiert werden kann. Aber in den meisten Branchen wird KI nur einen Teil der Arbeiten übernehmen. Telefonische Anfragen etwa kann man wunderbar automatisieren: Statt sich durch ein Menu zu tippen, stellt man als Kunde seine Frage und bekommt je nach Komplexität die Antwort von einem Computer – oder wird mit einem Mitarbeiter verbunden.

Welche Berufe kann die KI nicht ersetzen?

In den meisten Berufen werden wir wohl immer den Menschen brauchen, weil es überall ständig Neues gibt. Als die Geldautomaten erfunden wurden, dachten auch viele, man brauche nun keine Banker mehr. Tatsächlich ist deren Arbeit interessanter geworden, da weniger repetitiv. Bestimmte Aufgaben kann man aber gut der KI übertragen. In der Radiologie können Algorithmen zur Bildverarbeitung den Arzt unterstützen. Ein Algorithmus braucht mindestens 1000 Beispiele, danach kann er einen Vorgang automatisieren. Aber manche Erkrankungen sieht selbst ein Spezialist nur ein, zwei Mal im Leben, da wüsste die KI gar nicht, was sie machen soll.

Einige Anwendungsbereiche der KI, etwa die Gesichtserkennung, sind sehr kontrovers. Welche Verantwortung trägt da die Wissenschaft?

KI ist eine Technologie genau wie ein Motor auch, man kann diesen in einen Krankenwagen bauen oder in einen Panzer. Letztlich müssen wir als Gesellschaft entscheiden, wofür wir sie nutzen wollen. Ich selbst will aber sichergehen, dass unsere angewandte Forschung am Ende des Tages einen positiven Einfluss auf die Menschen und die Firmen hat, indem wir ihnen dabei helfen, in ihrer Branche wettbewerbsfähiger zu sein. Wir haben in meiner Gruppe eine Architektin für ethische KI eingestellt, die uns hilft, keine Anwendungen zu entwickeln, die negative Auswirkungen auf die Gesellschaft haben könnten. Wir arbeiten aber auch nicht mit dem Militär zusammen und wollen keine Tötungsmaschinen bauen. Und im Bereich der Gesichtserkennung sind wir auch nicht tätig.

Welcher Aspekt in der derzeitigen Entwicklung von KI bereitet Ihnen die meisten Sorgen?

Das Vorurteil, das wir den Algorithmen beibringen – das ist ein richtiges Problem. Dagegen gibt es auch keine einfache Lösung; am wichtigsten ist, dass sich die Leute bewusst werden, dass dieses Problem überhaupt existiert. KI ist immer nur so gut wie ihre Trainingsdaten. Wenn wir einer Bank dabei helfen, die Kreditvergabe zu optimieren, weisen wir sie auf das Problem hin. Womöglich gibt es in den Datenbanken weniger Fälle von Frauen, die einen Kredit zur Firmengründung beantragt haben. Die KI, die mit diesen Daten geschult wird, wird eine geringere Wahrscheinlichkeit dafür ermitteln, dass eine Frau einen Kredit bekommen soll. Das ist diskriminierend. Man muss sich immer fragen: Wie wichtig ist die Entscheidung, welche die KI für den Menschen trifft? Vielleicht bekomme ich als Ergebnis einer schlechten KI nur die falsche Autowerbung auf Facebook gezeigt. Das ist nicht so wichtig, wie wenn man Geld braucht, um eine neue Firma zu gründen.

Welche kulturellen Unterschiede gibt es im Umgang mit KI zwischen Europa und dem Silicon Valley?

In Deutschland wird erst reguliert, bevor überhaupt etwas Neues entsteht; in den USA lässt man einfach mal machen – «litigation statt legislation» heisst das hier (Rechtsstreit statt Gesetzgebung; Anm. d. Red.). In Europa sorgen sich die meisten Politiker vor allem um die negativen Folgen der KI, in Amerika beschäftigt man sich mehr mit den Chancen. Ich befürchte, dass der beste Weg in der Mitte läge. Die Technologie wird ganze Volkswirtschaften effizienter machen. Die Länder, die da hinterherhinken, werden letztlich zurückfallen. Aber die Länder, die sich gar keine Gedanken über die Ethik der Technologie machen, werden am Ende weniger glückliche Bürger haben und entsprechend weniger stabil sein.

Manche Beobachter kritisieren, dass Firmen im Silicon Valley teilweise ganze Lehrstühle aufkaufen. Wie beurteilen Sie das?

Die Forschung findet nach wie vor an beiden Orten statt. KI ist so spannend, weil man Grundlagenforschung machen kann, die ein, zwei Jahre später schon in einem Produkt drin ist. Ich sage immer, dass man sich um die Grösse des Kuchens sorgen soll und nicht um die Grösse seines eigenen Stückes vom Kuchen. Also: Nur weil die Firmen auch viel forschen, heisst das nicht, dass die Unis nicht mehr forschen.

Werden Algorithmen irgendwann klüger als wir Menschen sein?

Allgemeine künstliche Intelligenz – also Maschinen, die intellektuell mit dem Menschen gleichauf liegen – ist interessant, aber derzeit noch Science-Fiction. Es gibt jede Menge Hürden, die man erst überwinden müsste: Derzeit arbeitet fast niemand daran, einen Algorithmus zu generieren, der viele verschiedene Aufgaben gleichzeitig lösen kann. Oder am Problem des kombinierten logischen Denkens: Wenn ich sage, das ist ein typischer Nicolas-Cage-Film, dann wissen Sie vielleicht, was ich meine, aber die KI nicht. Eine weitere Herausforderung sind sogenannte Ketten von sukzessive komplizierter werdenden Zielfunktionen, also die Idee, dass KI sich selbst Ziele setzt, wie es der Mensch im Laufe seines Lebens tut. Kurzum: Es wird also noch eine Weile dauern, bis allgemeine künstliche Intelligenz hier ist. Aber ich sehe keinen theoretischen Grund, warum es nie möglich sein sollte.

Wie schafft man es von Saarbrücken nach Stanford?

Man darf nicht aufgeben (lacht). Es ist viel Arbeit, und man wird erst ein paar Mal abgelehnt, aber man darf nicht aufgeben. ";https://www.nzz.ch/wirtschaft/der-bias-ist-das-groesste-problem-bei-der-kuenstlichen-intelligenz-ld.1528242;NZZ;Marie-Astrid Langer;;;
01.11.2017;«Oft entscheiden Menschen sehr schlecht»;"Herr Hofmann, was ist überhaupt künstliche Intelligenz (KI)?

Bei der KI geht es darum, dass Maschinen Tätigkeiten ausführen, für die es Intelligenz erfordert. Was wir aber als «intelligent» bezeichnen wollen, ist nicht so einfach zu beschreiben. Dennoch zeichnet sich ab, dass KI nachhaltige und teilweise unterschätzte Auswirkungen haben wird. Wo begegnen wir heute KI?

Eine erste Art von KI sehen wir in Robotern, die in der Lage sind, sich in der Welt autonom zu bewegen. Von der Öffentlichkeit viel beachtet werden selbstfahrende Autos. Hinter der trivial erscheinenden Wahrnehmungsfähigkeit verbirgt sich eine perzeptive Revolution: Maschinen haben plötzlich Augen und Ohren. Eine andere Facette von KI betrifft den Bereich der Sprache und des Textverstehens. Hier geht es etwa um die Frage, wie man automatisiert aus Texten Wissen extrahieren kann. Man denke an Googles Suchmaschine oder auch an IBMs Frage-Antwort-System Watson. Wir gewöhnen uns schon daran, mit Maschinen zu sprechen. Schliesslich gibt es noch den ganzen Bereich der Vorhersagemodelle, die etwa im «algorithmic trading» in der Finanzbranche seit langem eingesetzt werden.

Sowohl in selbstfahrenden Autos als auch hinter einer Suchmaschine steckt also KI?

Ihnen allen liegt eine gemeinsame Methodik zugrunde – das macht das Ganze vielleicht verwirrend. Es geht immer um grosse Datenmengen, in denen Muster anhand von statistischen Modellen identifiziert werden, damit daraus automatisiert Entscheidungen abgeleitet werden können. Wir nennen das auch «maschinelles Lernen», denn diese Systeme verbessern sich selbständig in ihrem Einsatz.

Was würden Sie einem Verwaltungsrat empfehlen, der sagt: «Jetzt müssen wir mal was mit KI machen»?

Das ist auf die Schnelle nicht leicht, denn KI wirft tiefgreifende Fragen über Prozesse, Dienste und Produkte auf. Wenn der Mensch nicht mehr zur Bewältigung der Komplexität gebraucht wird, lässt das neue Designs zu. Im Flugzeugbau etwa wird mit elektrischen Kleinflugzeugen eine völlig neue redundante Bauweise mit 30 und mehr Antriebsaggregaten möglich. Dazu bedarf es aber einer hochkomplexen, intelligenten Steuerung. Fortschritte in Hardware und intelligenter Software müssen Hand in Hand gehen. Man muss langfristig denken. Was kann ein Unternehmer kurzfristig anpacken, um von KI zu profitieren?

Er könnte sich überlegen, wo im Unternehmen viele Entscheidungen durch einzelne Mitarbeiter getroffen werden. Denn das ist ja mit erheblichen Kosten verbunden. Denken wir nur an Prozesse im Backoffice wie die Abwicklung von Anträgen oder Forderungen. Ähnliches gilt für den Bereich Kundendienst und Marketing. In Bereichen wie Fertigung, Inspektion oder Qualitätskontrolle kann man sich fragen, ob datengetriebene Verfahren Optimierungspotenzial bieten. Vielleicht müssen dafür neue Sensoren installiert werden, um zusätzliche Daten zu erheben. Das klingt einfach. In der Praxis sind dies aber schwierige Entscheidungen, die mit Kosten verbunden sind. Die weitverbreitete Ängstlichkeit bei etablierten Anbietern ist aber der Hauptunterschied zu Firmen, die in der neuen Welt gross geworden sind. Unternehmen wie Google sehen das Risiko eher in vergebenen Chancen. Selbst wenn ein Projekt scheitert, ergeben sich daraus vielleicht ungeahnte Innovationen.

    «Es tut mir im Herzen weh, wenn ich sehe, wie ineffizient Formulare auf den Ämtern bearbeitet werden.»

Welche neue Firmenstrukturen gilt es für die KI-Ära aufzubauen?

Für mich ist Google das prototypische KI-Unternehmen. Das spiegelt sich auch im organisatorischen Aufbau. Das Wichtigste ist, von Anfang an Erfolgskriterien zu definieren. Dinge wie Qualität, Effizienz, Nutzen müssen messbar gemacht werden. Es gilt, Transparenz herzustellen, um zu wissen, wie gut man überhaupt ist – eine Unternehmenskultur, die Schein und Vertuschung toleriert, ist schlecht. Bei Google haben vielleicht nur 10% aller Ideen funktioniert. Man muss daher hinreichend schnell herausfinden können, was klappt und was nicht. Zudem muss die Firmenkultur 90% Fehlschläge verkraften können. Zusammenfassend lässt sich sagen: Es braucht Daten, Erfolgskriterien auf Basis dieser Daten und eine für Experimente offene Firmenstruktur.

Ohne Daten geht also gar nichts.

Ja. KI, wie wir sie heute verstehen, basiert immer darauf, interessante statistische Abhängigkeiten aus Daten abzuleiten. Die Datensammlung muss somit der erste Schritt sein.

Für KMU wird das schwierig sein. Können diese trotzdem von den KI-Entwicklungen profitieren?

Viele Kleinunternehmen werden indirekt von KI profitieren. Es wird neue Anbieter von Branchenlösungen geben. Ein Handwerker muss beispielsweise entscheiden, welche Aufträge er annehmen soll und wie er eine Kostenschätzung gestalten will. Bei solchen Analysen wird es in Zukunft bessere Planungstools geben. Ein Kleinunternehmer kann dabei aber kaum direkt aktiv werden. Ein Arzt ist ja nicht in der Lage, eine eigene KI zu entwickeln. Er muss aber verstehen, wo die neuen Technologien im Alltag seiner Praxis gewinnbringend eingesetzt werden können, etwa um Auswahl und Dosierung von Medikamenten zu optimieren. Eine KI-Lösung kann hier Hilfe leisten, insbesondere macht sie keine Flüchtigkeitsfehler. Ich bin überzeugt, dass wir noch einen Riesenwandel erleben werden. In jeder Branche wird KI aber in einem anderen Gewand daherkommen.

Welche Funktionen im Unternehmen werden durch KI ersetzt werden?

Das wird wohl viele Berufe treffen, welche intellektuelle Fähigkeiten voraussetzen. KI wird also da eingesetzt werden, wo mehr mit dem Kopf als mit den Händen gearbeitet wird. Ein riesiges Potenzial sehe ich bei Bürokratien jeglicher Art, sei es bei Versicherungen, Banken oder auch der öffentlichen Hand. Ein Sachbearbeiter fällt tagtäglich unzählige Mikroentscheidungen. Es tut mir im Herzen weh, wenn ich sehe, wie unglaublich ineffizient Formulare jeglicher Art auf den Ämtern bearbeitet werden. Mit KI sollten Menschen hier nur noch in Zweifelsfällen eingesetzt werden, also für schwierige oder wichtige Fälle und für die Qualitätskontrolle.

Welche intellektuellen Fähigkeiten werden nicht ersetzt werden?

Das ist eine gute Frage. Es hängt sehr vom Zeithorizont ab. Es gibt keine ultimative Grenze. Ich glaube, wir überschätzen in vielen Bereichen, wie gut Menschen urteilen. Oft entscheiden Menschen sehr schlecht. Zum einen können wir nur beschränkt mit Unsicherheit und Wahrscheinlichkeiten umgehen. Zum anderen ist unser Urteil durch Vorurteile und subjektive Faktoren getrübt. Deshalb glaube ich auch an das Potenzial etwa im medizinischen Bereich. Unsere heutige Medizin ist ein hoch ineffizientes System. Das meine ich keineswegs als Kritik an den Ärzten. Es ist für sie schlicht unmöglich, immer alle Informationen richtig zu gewichten und gute Entscheide zu treffen. Das Fehlen von Transparenz gaukelt uns jedoch manchmal vor, alles laufe bestens.

Sie sprechen vom gewaltigen Potenzial von KI. Bereits in den 1960er und den 1980er Jahren gab es eine grosse Euphorie, die dann jeweils in den sogenannten KI-Wintern endete. Könnte das auch jetzt wieder passieren?

Ich halte die Jahreszeitenmetapher für unbrauchbar. In den 1960er Jahren war die Euphorie ein rein akademisches Phänomen. Die Diskussion lief in den Bildungszirkeln jener Zeit. Dann kamen in den 1980er und 1990er Jahren die neuronalen Netzwerke. In der Zeit, in der ich auch promoviert habe, konnten wir erste Fortschritte beim maschinellen Lernen erzielen. Aber wir haben den Begriff KI vermieden, da er hohe Erwartungen weckt, die oft mehr mit Science-Fiction zu tun haben. Trotzdem wurden damals wichtige Entwicklungen vorangetrieben. Die Leute mögen das KI-Winter nennen, für mich war es einfach eine Zurückhaltung im Auftreten, weil der Fortschritt nicht ausreichte, um von KI zu reden.

Das hat sich jetzt geändert.

Ja, in den vergangenen rund sieben Jahren haben wir einen Durchbruch erzielt. Die Leute sagten: Lasst uns die Methoden nochmals versuchen, die vor 20 Jahren nicht funktioniert haben. Schliesslich hatte man nun tausendmal mehr Daten und tausendmal mehr Rechenleistung. Das führte zu einer wissenschaftlichen Neubewertung. Wir erlebten eine Konvergenz verschiedener technologischer Entwicklungen, die es plötzlich Firmen erlaubt hat, mit den KI-Technologien kommerziellen Erfolg zu haben. Es findet heute eine enorme Wertschöpfung statt. Weshalb soll daher jetzt wieder ein KI-Winter kommen? Klar gibt es überbewertete Startups. Aber die Technologie ist da: Auf dem Smartphone funktioniert die Spracherkennung, und die automatische Sprachübersetzung wird jedes Jahr besser. Wie sollte das eine Täuschung sein? Wir erleben den Fortschritt im Alltag und nicht nur in der Wissenschaft. Früher war manchmal viel Hype und wenig Substanz da, dieses Gefühl habe ich heute nicht mehr. Statt Visionen von selbsternannten Propheten geht es heute um reale KI-Produkte. Was wir heute aber noch haben, sind Propheten, die das Erreichte in die Zukunft extrapolieren . . .

    «Ich habe mehr Angst vor dem Menschen als vor der Maschine.»

. . . und vor den Gefahren einer Super-KI warnen.

Die Leute haben in meinen Augen den falschen Aufmerksamkeitsfokus. Wenn wir intelligente Maschinen bauen, sind diese auf eine Art immer superintelligent. Unsere Intelligenz verdanken wir dem extrem langsamen Prozess der Evolution. Die Fortschritte bei der KI werden sich aber nicht verlangsamen, sondern mit grosser Geschwindigkeit voranschreiten. Wir können da nicht mithalten. Es wird auch neue Fähigkeiten geben. Maschinen werden das menschliche Genom und den Code des Lebens lesen lernen. Dafür haben wir gar keine natürliche Veranlagung. Deshalb spreche ich auch lieber von maschineller Intelligenz. Es sind Fähigkeiten, die wir gar nicht haben.

Wir müssen uns also vor einer Super-KI nicht fürchten?

Die Frage ist, ob die spezialisierten Systeme zusammenwachsen. Das Bild der Vernetzung ist extrem wichtig. Bei einem selbstfahrenden Auto wird nicht einfach der menschliche Fahrer durch eine KI ersetzt. Vielmehr verbindet sich die eine KI mit allen anderen. Wir brauchen daher keine auf Menschen ausgerichtete Koordination mehr. Die umständliche Strassenverkehrsordnung, Ampeln oder Vorfahrtsregeln können durch Vernetzung ersetzt werden. Die zentrale Frage ist, wohin das führen wird. Für die nächsten 10 bis 15 Jahre mache ich mir wenig Sorgen. Aber langfristig müssen wir uns überlegen, wie wir unser Gemeinwesen um diese künstliche Intelligenz herum organisieren. Hier liegen auch Gefahren, insbesondere in Staaten mit zentralistischen und totalitären Strukturen wie China. Durch Einsatz einer Super-KI lassen sich Kontrollmechanismen entwickeln, die stark das Verhalten der Menschen beeinflussen können.

Sie haben aber keine Angst davor, dass sich die KI verselbständigt?

Ich habe mehr Angst vor dem Menschen als vor der Maschine. Wenn mit KI eine beinahe lückenlose Kontrolle möglich wird, ist es nicht mehr so wichtig, ob der Mensch die letzte Entscheidung trifft. Es geht um die Möglichkeit der Machtausübung durch diese Technologie: Mensch gegen Mensch bleibt das Problem. Hier eine hochpotente Technologie voranzutreiben, ist gefährlich. Denken Sie nur an die automatisierte Kriegsführung: Wo beginnt zum Beispiel die Autonomie einer Drohne? Ihr wird gesagt, eine bestimmte Person müsse eliminiert werden. Sie fliegt los und entscheidet selbständig über die Ausführung ihrer Mission. Die nächste Stufe ist, dass die Zielperson in den sozialen Netzwerken automatisch als potenzieller Terrorist identifiziert wird. Diese Vernetzung wird kommen. Ob jetzt da noch ein Mensch dazwischen sitzt, der den Knopf drückt, um die Drohne loszuschicken, spielt keine Rolle, wenn er im Normalfall den Knopf drückt. Wir müssen daher stärker darauf achten, welche Systeme vernetzt werden und wer das kontrolliert. Das ist ein riesiges Problem, dem sich die Politik in den nächsten 10 bis 20 Jahren stellen muss.";https://www.nzz.ch/wirtschaft/oft-entscheiden-menschen-sehr-schlecht-ld.1325428;NZZ;Jürg Müller;;;
19.05.2020;Corona und der Primat der Wissenschaft;"Seit Ausbruch der Corona-Krise wurde immer wieder die Rolle der Wissenschaft betont. Politiker berufen sich bei ihren Entscheidungen auf Virologen und Epidemiologen. Wer «der Wissenschaft» widerspricht oder deren Aussagen relativiert, wird häufig als einer abweichenden Minderheit zugehörig oder noch schlimmer als Verschwörungstheoretiker gebrandmarkt. Dabei gibt es «die Wissenschaft» gar nicht. Verschiedene Wissenschaften und wissenschaftliche Fachrichtungen arbeiten höchst unterschiedlich. Die exakten Wissenschaften, also weite Teile der Chemie, der Biologie, der Physik und natürlich der Mathematik, streben nach Erkenntnissen, die direkt überprüfbar sind. Jedes Mal, wenn ich einen Apfel fallen lasse, fällt er auf den Boden, und die Geschwindigkeit beim Aufprall steht fest. Die Erkenntnisse der exakten Wissenschaften, von der Mechanik zur Elektronik, sind für alle modernen Technologien unentbehrlich. Kein Handy oder kein Internet wäre ohne die bahnbrechenden Entdeckungen der Physik des 20. Jahrhunderts möglich.

Viele neue und moderne Wissenschaften hingegen arbeiten an komplexen Modellen, deren Aussagen sich häufig nicht sofort auf ihre Richtigkeit überprüfen lassen. Dazu gehören die Epidemiologie, die Klimawissenschaften und viele Bereiche der medizinischen Forschung. Natürlich sind diese Wissenschaften notwendig und richtig. Sie können den potenziellen Nutzen beim Einsatz von Medikamenten aufzeigen, mögliche Szenarien des Klimawandels beschreiben und einen Rahmen setzen. Darüber hinaus können neue Verfahren wie die künstliche Intelligenz und das maschinelle Lernen die Zuverlässigkeit der Voraussagen noch verbessern. Trotzdem sollte man sich bewusst sein, dass es sich um statistische Analysen und Modellvorhersagen handelt.

Um ein Beispiel zu nennen: In den 1990er Jahren war die Chaostheorie in den Medien sehr präsent. Eine populäre Aussage der Chaostheorie ist es, dass der Flügelschlag eines Schmetterlings in Brasilien einen Tornado in den USA auslösen könne. Chaotische Systeme und Situationen sind im Prinzip vorhersehbar, aber kleinste Veränderungen in den Modellannahmen können zu höchst unterschiedlichen Ergebnissen führen. Auch wenn epidemiologische Modelle nicht unbedingt chaotisch sind, so hängen deren Aussagen doch immer stark von den Annahmen ab. Eine kleine Änderung der angenommenen Reproduktionsrate oder der Definition der Fallsterblichkeit kann zu deutlich unterschiedlichen Aussagen führen.

Aus diesem Grunde ist es richtig, wenn auch heute abweichende Meinungen zur Kenntnis genommen und diskutiert werden und kein Überbietungswettbewerb in Bezug auf Weltuntergangsszenarien die öffentliche Debatte bestimmt. Niemand hat die Wahrheit gepachtet. Niemand weiss, wie schlimm es gekommen wäre unter weniger drastischen Massnahmen und ob die zweite Welle kommt, und wenn ja, wie heftig. Renommierte Experten wie John Ioannidis von der Universität Stanford haben schon Mitte März vor Panik gewarnt. Es wäre sicher ratsam gewesen, diese Stimmen schon früher in die Situationsanalyse und die öffentliche Debatte einfliessen zu lassen. Die erste Reaktion des Bundesrates war, angesichts der unübersichtlichen Situation in Italien, sicher vernünftig, vorsichtig und angemessen. In der Schweiz und in weiten Teilen Europas war das exponentielle Wachstum der bestätigten Fallzahlen aber bereits Ende März offensichtlich gebrochen. Spätestens dann hätte man debattieren und prüfen müssen, ob die ergriffenen Massnahmen noch verhältnismässig sind oder es je waren. Wenn nicht, hätte man diese schnellstmöglich zurückfahren müssen, um weiteren Schaden von der Gesellschaft abzuwenden.";https://www.nzz.ch/meinung/corona-und-der-primat-der-wissenschaft-ld.1556043;NZZ;Frank Scheffold;;;
28.11.2020;Im Rennen um den Quantencomputer machen Startups IBM, Google und Co. Beine;"Wer als Technologieunternehmen etwas auf sich hält, kommt am Thema Quantencomputer nicht vorbei. Das Potenzial dieser Rechner ist so gross, dass Firmen wie Google, IBM oder Microsoft in den letzten Jahren beträchtliche Summen in deren Erforschung und Entwicklung investiert haben. Den Tech-Giganten sitzt eine wachsende Zahl von Startups im Nacken. Wer momentan die Nase vorne hat, ist schwer zu sagen. Denn die Firmen setzen nicht nur auf unterschiedliche Rechnerarchitekturen. Sie verwenden auch unterschiedliche Metriken, um die Güte ihrer Quantencomputer zu beurteilen. Gemeinsam ist ihnen nur eines: Alle möchten möglichst schnell an den Punkt kommen, an dem man mit einem Quantencomputer Dinge besser und schneller tun kann als mit einem herkömmlichen Computer.
Mehr als 0 oder 1

Der wesentliche Unterschied zwischen einem klassischen und einem Quantencomputer liegt in der Natur ihrer Bits. Ein klassisches Bit kann entweder den Wert 0 oder den Wert 1 annehmen. Ein Quantenbit, abkürzend auch Qubit genannt, kann mehr. Wegen der merkwürdigen Eigenschaft von Quantenobjekten, sich gleichzeitig in verschiedenen Zuständen aufhalten zu können, kann ein Qubit auch beliebige Überlagerungen der Zustände 0 und 1 darstellen. Zudem kann man zwei Qubits so miteinander verschränken, dass sie mehr sind als die Summe ihrer Teile.

Diese beiden Eigenschaften, die Überlagerung und die Verschränkung, machen Quantencomputer theoretisch sehr leistungsfähig. Wie der Mathematiker Peter Shor schon 1994 gezeigt hatte, sollten sich grosse Zahlen mit hinreichend vielen Quantenbits sehr viel schneller in ein Produkt von Primzahlen zerlegen lassen als mit jedem herkömmlichen Computer. Die Arbeit Shors fand weltweit Beachtung. Denn sie tangierte die Sicherheit eines weitverbreiteten Verschlüsselungsverfahrens.

Auch 25 Jahre später sind Quantencomputer noch weit davon entfernt, kryptografische Codes zu knacken. Aber ein Nischenthema sind sie spätestens seit letztem Jahr nicht mehr. Damals meldete Google, ein Quantencomputer mit 53 supraleitenden Qubits habe in 200 Sekunden ein Problem gelöst, für das ein Supercomputer 10 000 Jahre gebraucht hätte. Die Meldung sorgte weltweit für Schlagzeilen. Und das, obwohl das gelöste Problem keine praktische Relevanz besass und auf die Arbeitsweise des Quantencomputers zugeschnitten war.

Die Konkurrenz reagierte verschnupft auf die Ankündigung von Google. So nahmen IBM-Forscher in einem Blog-Beitrag Anstoss am Konzept der Quantenüberlegenheit. Die Güte eines Quantencomputers bemesse sich nicht daran, ob er ein massgeschneidertes Problem schneller löse als ein klassischer Computer. Die Aufgabe bestehe vielmehr darin, potenziellen Anwendern immer leistungsfähigere Quantencomputer zur Verfügung zu stellen, die ein breites Spektrum von Programmen und Algorithmen bearbeiten könnten. Nur so liessen sich mit Quantencomputern eines Tages praktische Probleme lösen.

Der Blog-Beitrag skizziert die Strategie, die IBM schon seit mehreren Jahren verfolgt. Seit 2016 bietet das Unternehmen Anwendern aus der Wissenschaft und der Industrie die Möglichkeit, über die Cloud auf firmeneigene Quantencomputer zuzugreifen und auf diesen eigene Programme laufen zu lassen. Über die Jahre ist so ein Netzwerk von Partnern entstanden. Anfangs standen nur kleine Prototypen mit fünf Qubits zur Verfügung. Inzwischen können die Kunden unter mehr als zwei Dutzend Quantencomputern mit bis zu 65 Qubits wählen. Und das ist erst der Anfang. Vor wenigen Wochen stellte IBM eine Roadmap für die nächsten Jahre vor. Demnach soll die Zahl der Qubits bis zum Jahr 2023 schrittweise auf über tausend erhöht werden. Damit komme man in einen Bereich, wo man mit einem Quantencomputer auch komplexere Probleme in Angriff nehmen könne, sagt Alessandro Curioni, der das IBM-Forschungszentrums in Rüschlikon leitet. Als Beispiel nennt er Optimierungsprobleme, maschinelles Lernen oder die Simulation von chemischen Reaktionen.
Es zählen nicht nur die Qubits

Was ein Quantencomputer kann und was nicht, bemisst sich allerdings nicht nur an der Zahl seiner Qubits. Wichtig ist auch, wie fehleranfällig diese sind und wie gut sie sich untereinander verknüpfen lassen. All diese Faktoren fliessen in das sogenannte Quantenvolumen ein, eine Masszahl, die von IBM vorgeschlagen wurde, um Quantencomputer vergleichen zu können. Dabei gilt: Je grösser das Quantenvolumen ist, desto komplexere Probleme lassen sich mit einem Quantencomputer lösen. Wie IBM vor einigen Monaten bekanntgegeben hat, hat sein bester Quantencomputer ein Quantenvolumen von 64. Bezeichnenderweise ist es nicht jener mit 65 Qubits, sondern einer mit nur 27 Qubits.

Google hat bisher nicht preisgegeben, welches Quantenvolumen der Prozessor hatte, mit dem letztes Jahr die Quantenüberlegenheit demonstriert wurde. Andere Firmen benutzen inzwischen aber die gleiche Metrik wie IBM. So hat die Firma Honeywell Ende September bekanntgegeben, sie habe mit nur 10 Quantenbits ein Quantenvolumen von 128 demonstriert. Noch spektakulärer klingt eine Ankündigung des Startups IonQ. Mit 32 nahezu perfekten Qubits habe man ein Quantenvolumen von 4 Millionen erreicht. Damit sei der Quantencomputer von IonQ der leistungsfähigste auf dem Markt, heisst es in der Pressemitteilung ganz unbescheiden.

Ist IBM also ins Hintertreffen geraten? Curioni ist anderer Meinung. Er habe bisher keine wissenschaftliche Publikation gesehen, in der diese Zahlen erhärtet würden. Zudem verweist er auf das Problem der Skalierbarkeit. IBM habe in seiner Roadmap dargelegt, wie man Quantencomputer mit einigen tausend und später sogar mit Millionen Qubits bauen könne. Das vermisse er bei der Konkurrenz. Und auch das Partner-Netzwerk, das IBM aufgebaut habe, sei einzigartig.
Konkurrierende Technologien

Tatsächlich sind die Quantencomputer von Honeywell und IonQ ganz anders aufgebaut als jene von IBM und Google. Sie rechnen mit gespeicherten Ionen, die in einer Falle aufgereiht sind wie Perlen in einer Kette. Indem man den Ionen wohldosierte Laserpulse verabreicht, lassen sie sich miteinander verknüpfen. Die Qubits von IBM und Google bestehen hingegen aus supraleitenden Bauelementen, die mit den Techniken der Halbleiterindustrie auf einem Chip gefertigt werden.

Der Vorteil der Ionen bestehe darin, dass sich jedes Ion mit jedem verknüpfen lasse, erklärt Andreas Wallraff von der ETH Zürich, der im Rahmen des «Quantum Flagship»-Programms massgeblich an der Entwicklung eines europäischen Quantencomputers mit hundert supraleitenden Qubits beteiligt ist. Die supraleitenden Qubits auf einem Chip seien in der Regel nur mit ihren unmittelbaren Nachbarn verdrahtet. Zudem reagierten die Ionen weniger empfindlich auf Störungen als die supraleitenden Qubits. Beides zusammen, die kleinere Fehlerrate sowie die höhere Konnektivität, sorge dafür, dass Quantencomputer mit gespeicherten Ionen derzeit ein höheres Quantenvolumen hätten. Der Qualitätsunterschied sei aber nicht so gross, wie es die nackten Zahlen vermuten liessen, so Wallraff. So hätten Schaltungen aus supraleitenden Qubits kürzere Schaltzeiten.
Mit Fehlern ist zu rechnen

Egal, ob mit Ionen, supraleitenden Bauteilen oder anderen Qubits – derzeit können Quantencomputer noch nicht ihr volles Potenzial entfalten. Das liegt gewissermassen in der Natur der Sache. Die Zustände, mit denen ein Quantencomputer rechnet, sind nämlich fragil. Eine kleine Störung genügt, um aus einem Überlagerungszustand eines Qubits einen klassischen Zustand zu machen.

Diese sogenannte Dekohärenz begrenze die Anwendbarkeit der heutigen Quantencomputer, sagt Wallraff. Je länger eine Rechnung dauere, desto grösser sei die Wahrscheinlichkeit, dass das Ergebnis fehlerhaft sei. Deshalb liessen sich mit Quantencomputern heute noch keine komplexen Probleme lösen. Wolle man eine grosse Zahl in ihre Primfaktoren zerlegen oder die chemischen Eigenschaften eines medizinischen Wirkstoffs simulieren, führe kein Weg daran vorbei, die Fehler zu korrigieren.

Wie das im Prinzip gehen könnte, hat die Arbeitsgruppe von Wallraff vor einigen Monaten demonstriert. Die Forscher verschränkten mehrere supraleitende Qubits zu einer logischen Einheit. Indem sie verschiedene Operationen mit diesem logischen Qubit ausführten, konnten sie zeigen, dass es dreimal länger fehlerfrei arbeitet als die einzelnen Qubits, aus denen es besteht. Noch einen Schritt weiter geht die Arbeitsgruppe von Christopher Monroe von der University of Maryland in College Park, einem der Gründer von IonQ. In einer noch nicht begutachteten Arbeit haben die Forscher gezeigt, wie man die Fehler eines logischen Qubits nicht nur erkennen, sondern auch korrigieren kann.

Monroe widerspricht allerdings der Auffassung, dass man ohne volle Fehlerkorrektur nichts Sinnvolles mit Quantencomputern tun könne. Er glaubt vielmehr, dass der Übergang zu den fehlertoleranten Quantencomputern der Zukunft fliessend sein werde. Es gebe genug Probleme, die sich auch mit «verrauschten» Qubits lösen liessen. Sein Motto lautet: so viel Fehlerkorrektur wie nötig, aber auch nicht mehr.

Dieser pragmatische Ansatz empfiehlt sich auch deshalb, weil die Fehlerkorrektur erhebliche Ressourcen bindet. Je perfekter ein logisches Qubit sein soll, desto mehr physikalische Qubits muss man ihm zur Seite stellen. Die Quantencomputer auf der Basis von gespeicherten Ionen sieht Monroe hier im Vorteil. Da die Ionen von Natur aus eine geringere Fehlerrate hätten als die supraleitenden Qubits, müsse man weniger in die Fehlerkorrektur investieren. Man brauche keine Millionen Ionen, um fünfzig fehlerfreie logische Qubits zu machen. Tausend seien genug. Monroe widerspricht auch dem Einwand, Quantencomputer mit gespeicherten Ionen seien nicht skalierbar. Hier gebe es mittlerweile sehr konkrete Ideen. Für die Technologieunternehmen – ob gross oder klein – birgt die Übergangsphase Risiken. Für die heute verfügbaren Quantencomputer gebe es noch keinen klaren Business-Case, sagt Jan Goetz. Er ist Mitbegründer des Startups IQM, das kürzlich von der finnischen Regierung den Auftrag für den Bau eines Quantencomputers mit 50 Qubits erhalten hat. Es gebe zwar Firmen und Forschungseinrichtungen, die dafür bezahlten, mit diesen Computern rechnen zu können, so Goetz. Derzeit geschehe das aber vor allem, um neue Talente auszubilden und Grundlagenforschung zu betreiben. Lukrative Anwendungen in der Chemie oder der Finanzwirtschaft seien erst in einigen Jahren absehbar.

Goetz bedauert, dass von manchen Firmen zu viel versprochen werde. Damit könne man vielleicht Risikokapital akquirieren. Den Startups sei aber nicht geholfen, wenn die Investoren nach wenigen Jahren wieder absprängen. Was es jetzt brauche, seien clevere Geschäftsideen – und ein langer Atem.";https://www.nzz.ch/wissenschaft/quanrtencomputer-start-ups-machen-google-ibm-und-co-beine-ld.1585528;NZZ;Christian Speicher;;;
07.07.2020;Künstliche Intelligenz: Die Corona-Krise bringt den nächsten Technologieschub;"Videokonferenzen, Home-Office und intelligente Assistenten: Die Corona-Krise habe zu einem Technologieschub geführt, der zu einer Umwälzung der Arbeitswelt führen werde, frohlocken Ökonomen. «Wir haben in den zurückliegenden Wochen die Digitalisierung light kennengelernt und gemerkt, dass damit neue Organisationsformen möglich sind», sagt Rafael Lalive, Professor für Arbeitsmarktökonomie und technologische Transformation an der Universität Lausanne.

Betrachtet man die jüngsten Aussagen von Firmenchefs, könnte dies tatsächlich zutreffen. So erklärte Vasant Narasimhan, der Chef von Novartis, in einem Interview, dass der Pharmakonzern im Zuge der Corona-Krise viel mehr künstliche Intelligenz nutze, um das Produktionsnetzwerk, klinische Studien und die Finanzlage zu überwachen. Man wolle künftig in sämtlichen Aspekten der Arbeit digitaler werden.

Ähnlich positiv geben sich die Grossbanken. Bei der Credit Suisse freut man sich über den Schub, den das Online-Banking erhalten hat. Bei der UBS verweist man auf den erfolgreichen Einsatz von Roboterassistenten, die definierte Aufgaben automatisiert und selbständig ausführen. Um beispielsweise die grosse Zahl von Kreditanträgen im Rahmen des KMU-Hilfsprogramms des Bundes zu bewältigen, habe die Bank sechs neue Bots eingesetzt, erklärte Sabine Keller-Busse, die operative Leiterin der UBS, in einem Interview der NZZ.

Derweil geriet die Managerin mit Blick auf die eingesetzten Roboterassistenten geradezu ins Schwärmen: Um die gleichen Leistungen zu erbringen, hätte die Bank 280 Mitarbeiter einsetzen müssen, führte Keller-Busse aus. Bemerkenswert ist, dass beide Grossbanken im selben Atemzug anführen, dass man mittelfristig wohl mit weniger Personal auskommen werde.
Automatisierbare Jobs sind gefährdet

Dass in der Corona-Rezession Jobs verloren gehen, ist naheliegend. Lalive und sein Mitarbeiter Fabrizio Colella haben die Arbeitsmarktlage der USA in den zurückliegenden zwei Jahrzehnten analysiert. Untersucht haben die beiden Ökonomen dabei zusammen mit Forschern der ETH Lausanne rund 700 Berufe, die sie je nach erforderlichen Fähigkeiten bewertet und indexiert haben. Sie kommen zu interessanten Ergebnissen: In Krisenphasen gehen vor allem Arbeitsplätze in Berufen mit einem hohen Automatisierungspotenzial verloren. So ist deren Zahl zwischen 2003 und 2011 – eine Phase, die erheblich von der Finanzkrise geprägt war – markant gesunken; im Quartil mit dem höchsten Automatisierungsrisiko um zwischen 8 und 22%.

Dort, wo weniger die Möglichkeit bestand, Arbeit durch Maschinen zu ersetzen, sind hingegen neue Stellen geschaffen worden. Die Beschäftigung nahm bei den am wenigsten von der Automatisierung gefährdeten Jobs zwischen 2003 und 2011 um 10% zu. Für die Phase von 2011 bis 2019 ergibt sich demgegenüber keine solche Korrelation. Das heisst, in einer «ruhigen Phase» wachsen alle Berufsgruppen etwa gleich.

Die Konklusion lautet, dass die Einführung neuer Robotertechnologien keine grösseren Probleme auf dem Arbeitsmarkt schafft – es sei denn, es finde ein Strukturwandel statt. Und ein solcher könnte auch für die Schweiz bevorstehen.

So ist Lalive überzeugt, dass die Corona-Krise eine starke Verlagerung von Stellen nach sich zieht. Ob der erwartete Digitalisierungsschub auch zu einem Abbau von Arbeitsplätzen führen wird, ist allerdings unklar. Erhebungen zeigen, dass hiesige Firmen, die in den zurückliegenden Jahren in die Digitalisierung investiert haben, zusätzliche Arbeitsplätze – vor allem im hochqualifizierten Segment – geschaffen haben.
Firmen halten sich zurück

Bedroht waren hingegen vor allem Stellen, für die ein mittleres Qualifikationsniveau – bzw. eine Lehre ohne anschliessende Weiterbildung – erforderlich ist. Einfache repetitive Tätigkeiten machen zusehends der Automatisierung Platz und verschwinden. Im Gegenzug ist der Anteil an Jobs für Hochqualifizierte in der Schweiz in den zurückliegenden Jahren stark gestiegen.

Doch sind Firmen für den nächsten Digitalisierungsschritt bereit? Die Zurückhaltung der Firmen gegenüber der künstlichen Intelligenz scheint derzeit noch gross zu sein. Die meisten haben erst vage Vorstellungen davon, wie sich entsprechende Lösungen in profitable Geschäftsmodelle umsetzen lassen. Experten schätzen, dass sich der Anteil der Schweizer Unternehmen, die auf KI-Technologie setzen, gerade einmal im einstelligen Prozentbereich bewegt.

Die Corona-Krise hat sowohl die Vorteile als auch die Grenzen der künstlichen Intelligenz aufgezeigt. So wurden KI-gestützte Algorithmen genutzt, um Standortdaten von Mobiltelefonen danach zu analysieren, wie häufig und wie intensiv soziale Kontakte sind und wie sich das Virus ausbreitet. Das Vorgehen der chinesischen Regierung veranschaulichte derweil gleichzeitig auf drastische Weise die Gefahren der Technologie: nämlich deren Einsatz zur totalen digitalen Überwachung.

Gleichzeitig gab es im Bereich der künstlichen Intelligenz jüngst beachtliche Fortschritte: So sollen beispielsweise Wissenschafter in den USA durch den Einsatz von künstlicher Intelligenz neuartige Moleküle zum Kampf gegen Bakterien identifiziert haben. Laut einer weiteren Schlagzeile haben Forscher in Indien und Japan ein auf maschinellem Lernen basierendes Verfahren entwickelt, das eine verbreitete Art von Gehirntumor mit einer Genauigkeit von fast 98% klassifizieren kann.
Diskriminierende Gesichtserkennung?

Doch immer wieder kommt es auch zu Rückschlägen: So hat der Technologiekonzern IBM jüngst bekanntgegeben, sich vom Geschäft der Gesichtserkennung zu trennen. Hintergrund bilden die wieder erstarkte Antirassismusbewegung «Black Lives Matter». In einer Stellungnahme erklärte IBM-Chef Arvind Krishna, IBM biete solche Software nicht mehr an und sei allgemein gegen diese Technologie, wenn ihr Einsatz zu einer Massenüberwachung, Diskriminierung und Verletzung von Menschenrechten führe. Tatsächlich lieferte die Gesichtserkennungstechnologie in der Vergangenheit teilweise unzuverlässige Ergebnisse – und dies oft zulasten ethnischer Minderheiten.

Selbst beim Facebook-Konzern, den es ohne seine auf künstlicher Intelligenz basierenden Algorithmen in der heutigen Form nicht gäbe, erkennt man die Grenzen der Technologie. So setzt das soziale Netzwerk beim Aufspüren problematischer Inhalte wie Hassreden und im Kampf gegen die Verbreitung von Falschinformationen zwar auf automatisierte Computerprogramme. Gleichzeitig sind aber über 30 000 menschliche Gutachter im Einsatz, die kritische Inhalte anschauen und auf der Basis von gesundem Menschenverstand entscheiden.

Kritiker monieren, dass es künstliche Intelligenz noch gar nicht gebe, bloss Marketingversprechen von IT-Konzernen. Selbstlernende Maschinen brauchten derzeit noch zu viele Daten, verbrauchten viel Energie und verstünden im Grund nicht, was sie täten. Klar ist, dass die Diskrepanz zwischen den angeblich unbeschränkten Möglichkeiten der künstlichen Intelligenz und ihrem tatsächlichen Anwendungsgebiet gross ist.
Ein Nebeneinander von Maschine und Mensch

Kurzfristig ist jedenfalls keine drastische Ablösung des Menschen durch Roboter oder intelligente Algorithmen zu erwarten. Es dürfte allerdings zu einem Technologieschub kommen, der in Branchen wie dem Banking oder dem Handel einige Umwälzungen nach sich ziehen wird. Insgesamt zeichnet sich eher ein Miteinander von Maschinen und Mensch ab.

«Langweilige Tätigkeiten werden automatisiert, während der Mensch mehr Zeit für kreative, intellektuelle und emotionale Arbeit erhält», erläutert Lalive. Gleichzeitig räumt er ein, dass künstliche Intelligenz derzeit wenige Arbeitsplätze verändert. Ob und in welchen Bereichen sich daraus Produktivitätsfortschritte ergeben, muss sich erst noch zeigen.

Tatsächlich sind die technologischen Fähigkeiten, die darauf abzielen, die kognitiven Fähigkeiten des Menschen zu reproduzieren, beschränkt. Der Schweizer Physiker und Philosoph Eduard Kaeser stellte jüngst in einem Beitrag in der NZZ gar die Grundsatzfrage, ob der Mensch sich als Vorbild für ein KI-System überhaupt eigne. Seine Antwort fällt skeptisch aus: Denn der Mensch sei im Grund kein logisches Wesen. ";https://www.nzz.ch/wirtschaft/kuenstliche-intelligenz-die-corona-krise-bringt-neue-technologie-ld.1563175;NZZ;Nicole Rütti;;;
03.04.2017;«Google Translate» soll lernen, über einen Satz hinaus zu «denken»;"Wenn Maschinen Texte übersetzen, gehen sie meist Satz für Satz vor. Dabei stehen wichtige Informationen oft an anderer Stelle im Text. Schweizer Forschende verfolgen den Ansatz, Übersetzungsprogrammen mehr Textverständnis beizubringen und so zu verbessern.

Programme wie «Google Translate» verwenden Statistik, um die wahrscheinlichste Übersetzung von Wortgruppen in Sätzen zu liefern. Hinter menschlichen Übersetzerinnen liegen die Maschinen jedoch noch meilenweit zurück.

Einer der Gründe: Die Algorithmen schauen dabei nicht über die Grenzen eines Satzes hinaus. Dadurch haben sie etwa Mühe mit Pronomen, wie «sie» oder «diese», da das, worauf sie sich beziehen, in einem anderen Satz steht.

Forschende um Andrei Popescu-Belis vom Forschungsinstitut Idiap in Martigny wollen das im Rahmen eines vom Schweizerischen Nationalfonds SNF unterstützten Projekts ändern, indem sie den Algorithmus auch angrenzende Sätze analysieren lassen. Am Montag stellen sie ihre neuesten Ergebnisse an einer Konferenz der «Association for Computational Linguistics» im spanischen Valencia vor.
Fehlerrate bisher bei 50 Prozent

Ein Beispiel, wie das «Satz für Satz» Vorgehen von Übersetzungsprogrammen Probleme mit Pronomen verursacht, gibt der SNF in einer Mitteilung: «Meine Tante hat eine tolle Limousine gekauft. Sie ist aber nicht so schön.» Google Translate übersetzt dies so ins Englische: «My aunt has bought a great sedan. But she is not so beautiful.» Der englischsprachige Leser liest also, dass die Tante nicht so schön sei, weil «sie» in Zusammenhang mit «schön» öfter mit «she» übersetzt wird als mit «it».

Solche Probleme bestehen insbesondere bei Übersetzungen zwischen Französisch und Englisch, sowie Englisch und Spanisch. Programme wie Google Translate irren sich beim Übersetzen von Pronomen bei diesen Sprachpaaren in rund der Hälfte der Fälle.
Maschinelles Lernen

Das von Popescu-Belis' Team gemeinsam mit Kollegen von den Universitäten Genf, Zürich und Utrecht entwickelte Tool senkt diese Fehlerrate auf 30 Prozent, wie der SNF schreibt. Der Trick: Die Wissenschafter brachten dem Übersetzungsalgorithmus mittels maschinellem Lernen bei, auch angrenzende Sätze zu berücksichtigen.

«Im Prinzip geben wir dem System an, wie viele der voranstehenden Sätze es in welcher Weise analysieren muss. Dann testen wir es unter realen Bedingungen», sagt Popescu-Belis gemäss der Mitteilung.

Noch sei die Technik zwar nicht ausgereift für die breite Anwendung, allerdings hat das Projekt schon die Aufmerksamkeit von Anbietern von Übersetzungsprogrammen auf sich gezogen, hiess es weiter.

Das Forschungsteam sieht in der Lösung des Pronomen-Problems indes nur den Anfang. Mit der gleichen Idee, Maschinen mit mehr Textverständnis übersetzen zu lassen, wollen sie auch Aspekte wie die korrekte Abfolge der Zeiten oder die zum Kontext passende Terminologie verbessern.";https://www.nzz.ch/digital/kuenstliche-intelligenz-google-translate-soll-mehr-textverstaendnis-lernen-ld.155013;NZZ;SDA;;;
25.01.2019;Chinas Technologieführerschaft fordert die freie Welt heraus;"Der amerikanische Philanthrop George Soros ist bekannt dafür, dass er kein Blatt vor den Mund nimmt. «China ist nicht das einzige autoritäre Regime in der Welt. Doch es ist zweifellos das reichste, stärkste und das am weitesten entwickelte in den Bereichen maschinelles Lernen und künstliche Intelligenz», sagte Soros am Donnerstagabend am Rande des Weltwirtschaftsforums in Davos. «Das», so die Schlussfolgerung des Milliardärs, «macht Xi Jinping zum gefährlichsten Gegner für alle, die an das Konzept einer offenen Gesellschaft glauben.»

Soros trifft einen wichtigen Punkt. Zwar waren auch andere Diktaturen in gewissen technologischen Bereichen führend – so war es die Sowjetunion, die den ersten Menschen ins All schoss. Doch die Kommunistische Partei Chinas (KP) nutzt modernste Technik dafür, das Volk unter Kontrolle zu halten. Soros spricht direkt das «soziale Kreditsystem» an, das systemkonformes Verhalten belohnt und Widerstand bestraft. Das System werde das Schicksal des Individuums den Interessen des Einparteistaates unterwerfen, wie es in der Geschichte noch nie geschehen sei, sagt der 88-Jährige. In China selber wird das Kreditsystem weit weniger kritisch gesehen. Auch ist unklar, wie weit es je ausgebaut und eingesetzt werden wird. Doch mit Xinjiang gibt es ein Beispiel, das Böses erahnen lässt: Dort überwacht die KP die muslimischen Uiguren auf Schritt und Tritt. Wer nicht spurt, landet im Umerziehungslager – rund eine Million Menschen sollen sich dort befinden. Für Soros ist die Schlussfolgerung klar: «Maschinelles Lernen und künstliche Intelligenz geben repressiven Regimen Kontrollinstrumente in die Hand, die für offene Gesellschaften eine tödliche Gefahr sind.»

China im Innern zu verändern, ist von aussen kaum möglich. Soros musste selber seine Erfahrung damit machen, als er in den achtziger Jahren seinen China Fund aufbaute. Dieser blieb kurzlebig. Das kommunistische System, das die Stiftung überwinden wollte, ist hingegen heute stärker denn je. Doch für offene Gesellschaften weltweit stellt sich die Herausforderung, dass Peking sein System exportiert – oder es zumindest als Alternative zu einer demokratischen Entwicklung darstellt.

Ein wichtiges Vehikel dazu ist das globale Infrastrukturprojekt «One Belt, One Road», das neben einer wirtschaftlichen eine klare geopolitische Komponente hat. Peking gewährt auch autoritären Regierungen Milliardenkredite und bindet diese so an sich. Eine von China gebaute Eisenbahnlinie mit chinesischem Rollmaterial mag unverdächtig erscheinen. Doch in China selber können Personen, die vom sozialen Kreditsystem tief eingestuft werden, keine Billette kaufen. Wenn ein repressives Regime die richtige Technologie einsetzt, wird auch eine Eisenbahn zum Kontrollinstrument. Peking hat diese Technologie bereits entwickelt.

Wenn der Westen nicht einfach zuschauen will, wie Xi Jinping die Welt nach seinen Vorstellungen umgestaltet, muss er diesem die Stirn bieten. In den Ländern Afrikas, Asiens und Lateinamerikas, die China umwirbt, muss der Westen Alternativen anbieten; als Kreditgeber, aber auch mit seinem Modell der offenen Gesellschaft. Gleichzeitig braucht es eine Technologieabwehr, etwa gegen chinesische Hacker. Dazu gehört auch, dass man bei chinesischen Investitionen im Technologiebereich ganz genau hinschaut, ob diese wirklich nur wirtschaftliche Interessen verfolgen. Hier sind die Vereinigten Staaten den Europäern einen Schritt voraus. Soros hat denn auch Donald Trump für dessen China-Politik gelobt – obwohl er für den Präsidenten bisher fast nur Verachtung übrig hatte.";https://www.nzz.ch/international/george-soros-trifft-mit-seiner-kritik-an-china-einen-wunden-punkt-ld.1454573;NZZ;Patrick Zoll;;;
29.11.2019;Autonomes Fahren: neuer Versuch in London;"Vor zwei Jahren arbeiteten die beiden Doktoranden der Universität Cambridge, Alex Kendall und Amer Shah, an der Entwicklung eines autonomen Fahrsystems. Zu diesem Zweck modifizierten die beiden einen vollelektrischen Renault Twizy und brachten ihn dazu, innerhalb von 20 Minuten eigenständig zu fahren. Mit zusätzlich eingebautem GPS und einem Kamerasensor gelang es, den Twizy Gegenden erkunden zu lassen, die von Navigationssystemen noch nicht erfasst waren.

Im Unterschied zu den von Menschen programmierten Systemen entwickelt sich das System der beiden Cambridge-Doktoranden per maschinelles Lernen selbständig weiter. Dazu kopiert die Anlage das Fahrverhalten von erfahrenen Piloten und bezieht die Situationen mit ein, in denen ein Sicherheitsfahrer selbst eingreifen muss. So sammelt das System Fahrerfahrung wie ein Fahrschüler während der Fahrstunde. Mittlerweile haben die beiden ihr Doktorat abgeschlossen und das Startup Wayve gegründet. Das System implementierten sie in einen ebenfalls Batterie-elektrischen Jaguar I-Pace. Mit rund 20 Millionen Franken Venture-Kapital ausgestattet, will Wayve nun eine ganze Flotte der selbstfahrenden E-Jaguars auf den Londoner Stadtverkehr loslassen, um das autonome System zu perfektionieren.

Dass der Weg zum vollautomatischen Fahren in der Innenstadt von London noch ein sehr weiter sein dürfte, beweist ein Selbstversuch der NZZ aus dem Jahr 2017. Es ist zu hoffen, dass die Entwicklung nun deutlich weiter fortgeschritten ist. ";https://www.nzz.ch/mobilitaet/auto-mobil/autonomes-fahren-neuer-versuch-in-london-ld.1525250;NZZ;Herbie Schmidt;;;
30.11.2020;Die Startup-Novizin mit ihren drei Söhnen;"Vermutlich hätte die Karriere von Angelica Kohlmann eine andere Wendung genommen, wäre sie nicht Mutter von drei Söhnen geworden. Mitte der 1980er Jahren arbeitete sie in der Marketingabteilung der Behringwerke, der damaligen Tochtergesellschaft des deutschen Pharmakonzerns Hoechst. Als Ärztin seien ihr in dieser Branche alle Türen offengestanden, erzählt die heute 60-Jährige, die seit sieben Jahren in Zürich lebt. Genauso wichtig für Hoechst war jedoch ihre internationale Erfahrung und nicht zuletzt ihr unternehmerisches Naturell, das mit den Jahren immer stärker wurde: «Ich bin sehr entscheidungsfreudig, aber nicht leichtsinnig, ich komme aus einer Unternehmerfamilie», sagt Kohlmann.
Auf drei Kontinenten gelebt

Mit Blick auf ihren beeindruckenden Leistungsausweis – u. a. wurde sie 2018 in den Verwaltungsrat des Basler Pharmazulieferers Lonza gewählt – ist das souveräne Auftreten der Unternehmerin eine logische Folge. Geboren und aufgewachsen ist Kohlmann in Brasilien. Ihre deutschen Grosseltern seien in den 1930er Jahren vor den Nazis nach Südamerika geflüchtet. Der Grossteil der Familie lebe noch immer dort und sei im internationalen Getreidehandel (bei der Firma Sofico) tätig. Als Teenager ging Kohlmann nach Europa und doktorierte in Medizin an der Universität Hamburg. Direkt nach dem Studium ging sie in die USA ans MD Anderson Cancer Center in Houston sowie ans Memorial Sloan Kettering Cancer Center in New York. Doch weder als Ärztin im klassischen Sinn noch als Laborperson habe sie sich am rechten Platz gefühlt, erzählt sie.  Als sie für Höchst zu arbeiten begann, blieben der Geschäftsleitung ihre Fähigkeiten nicht lange verborgen. Kohlmann wurde in den Vorstandsstab berufen, wo sie sich mit strategischen Themen eines globalen Milliardenunternehmens befassen konnte. Die Arbeit sei faszinierend gewesen, quasi ihr MBA. Doch aus einer traditionellen Karriere bei einem internationalen Grosskonzern wurde nichts, denn plötzlich standen familiäre Pflichten im Vordergrund: Kohlmann heiratete und brachte Kinder auf die Welt. Während der Schwangerschaften habe sie voll durchgearbeitet. Wegen der grösseren zeitlichen Flexibilität zog sie es trotzdem vor, sich als Unternehmensberaterin selbständig zu machen.
Gemeinsamer Nenner: ETH Zürich

Die drei Söhne waren auch der Grund, weshalb Kohlmann vor einigen Jahren in Zürich ihre Zelte aufschlug. Alle haben an der ETH Zürich studiert. Nur noch der jüngste (Niclas, 23), der an seinem Masterdiplom arbeitet, residiert im Hotel Mama. Der älteste Sohn Eric (30) lebt als Investor in New York. Er ist der Co-Gründer des im Bereich künstliche Intelligenz und Big Data aktiven Private-Equity-Fonds Trinnacle Capital, der im Frühling an eine grosse Private-Equity-Firma verkauft wurde.

Der 29-jährige Sohn Thomas lebt in München, ist jedoch besonders eng in das neuste Projekt seiner Mutter involviert, die Medtechfirma Bloom Diagnostics. Beruflich tauscht sich das Quartett jeden Sonntag aus, wenn es sich via Videocall zur virtuellen Sitzung trifft und über die gemeinsamen Investitionsprojekte diskutiert. Über die Familienholding Kohlmann & Co. investieren die Kohlmanns seit Jahren eigenes Geld in diverse Jungunternehmen, offenbar äusserst erfolgreich. Alle der mittlerweile 12 Firmen hätten überlebt und entwickelten sich gut – bzw. hätten einen sehr guten Exit gehabt, sagt Kohlmann, die das VR-Präsidium der Familienholding innehat. Als Volltreffer erwies sich die Beteiligung an Ctrl-Labs, einer in New York von einem Neurowissenschafter gegründeten Firma, die sich der Computersteuerung via Gedanken verschrieben hat. Vor gut einem Jahr wurde das Jungunternehmen für knapp 1 Mrd. $ von Facebook gekauft.
Antikörpertest zu Hause

Wagniskapital wollen die Kohlmanns auch in Zukunft investieren, ein grösserer Deal stünde bevor. Doch mit der 2018 gegründeten Bloom Diagnostics ist Kohlmann erstmals selber Unternehmerin geworden. Für das technische Know-how kann sie auf das Wissen ihrer Söhne zurückgreifen, denn Bloom ist vor allem eine Softwarefirma. Das einige hundert Franken teure Analysegerät, in das der Teststreifen mit der Blutprobe gesteckt wird, fertigt die Ostschweizer Firma Cicor. Via Funk ist es mit dem Smartphone verknüpft, über das einfache Selbsttests von zu Hause aus möglich sind. Die Diagnose und die Empfehlungen bekommt man nach wenigen Minuten. Dank maschinellem Lernen werde das System immer besser, sagt Kohlmann.

Anfang November hat Bloom das CE-Zertifikat für einen Ferritin-Test bekommen, mit dem Eisenmangel festgestellt werden kann. Damit ist Bloom jedoch nicht allein. Mehr verspricht sich Kohlmann davon, ob jemand Antikörper gegen Sars-CoV-2 entwickelt hat. Menschen seien in der Anonymität zu Hause eher bereit, einen solchen Test zu machen. Noch sind solche Tests Medizinern vorbehalten, weshalb sie nur Spitäler, zertifizierte Labors oder Apotheken durchführen dürfen. Bei einem positiven Testresultat erfolgt zwingend die Meldung an die Gesundheitsbehörde. Wegen dieser Einschränkung muss auch das Unternehmen Bloom seinen Corona-Antikörpertest zuerst über österreichische Apotheken auf den Markt bringen. Das sei jedoch nur ein Zwischenschritt: «Testen von zu Hause kommt, es ist nur noch eine Frage der Zeit», so ist Kohlmann überzeugt. ";https://www.nzz.ch/wirtschaft/die-startup-novizin-mit-ihren-drei-soehnen-ld.1587486;NZZ;Giorgio V. Müller;;;
29.10.2020;Swisscom leitet Generationenwechsel ein – der Präsident Hansueli Loosli tritt ab, der Konzernchef Urs Schaeppi dürfte bald folgen;"Seit sieben Jahren stehen sie gemeinsam an der Spitze des grössten Schweizer Telekomkonzerns: Hansueli Loosli als Verwaltungsratspräsident und Urs Schaeppi als CEO. Dass das Duo bald auseinandergerissen würde, war unvermeidlich. Loosli erreicht im Frühling die maximale Amtsdauer von zwölf Jahren. Als Nachfolger schlägt der Verwaltungsrat den Aktionären Michael Rechsteiner vor, wie Swisscom am Donnerstag mitteilte. Seine Nomination kommt wenig überraschend: Bereits als er 2019 in den Verwaltungsrat gewählt wurde, wurde darüber spekuliert, dass er ein geeigneter Loosli-Nachfolger sei. Rechsteiner bringt Führungserfahrung mit; er leitet derzeit das Gaskraftwerkgeschäft von General Electric in Europa. Als Schweizer kennt er zudem die Gepflogenheiten in Bundesbern, was für einen Präsidenten eines halbstaatlichen Unternehmens unerlässlich ist. Neben Loosli wird auch der langjährige Finanzchef Mario Rossi das Unternehmen verlassen. Im Frühling wird der Österreicher Eugen Stermetz den CFO-Posten übernehmen. «Es ist ein guter Zeitpunkt für eine Verjüngung der Konzernleitung», sagte Rossi an einer Telefonkonferenz. Diese Aussage lässt aufhorchen: Denn Rossi wurde nicht nur im gleichen Jahr (2013) in die Konzernleitung befördert, in dem Schaeppi – nach dem tragischen Tod seines Vorgängers Carsten Schloter – den Chefposten übernahm. Die beiden haben auch denselben Jahrgang (1960). Es deutet somit einiges darauf hin, dass auch der CEO Urs Schaeppi bald abtreten dürfte. Offenbar gilt Urs Lehner, derzeit Leiter des Geschäftskundensegments, als valabler Kandidat für die Nachfolge.
Transformation bringt Kulturkonflikt

Swisscom wandelt sich seit längerem vom klassischen Netzbetreiber zu einem IT-Unternehmen. Für den blauen Riesen wird Know-how in Bereichen wie Softwareentwicklung, maschinellem Lernen oder Cloud immer wichtiger. Damit ist die Gefahr eines Kulturkonfliktes verbunden. Der Ex-Monopolist muss einerseits ein Arbeitsumfeld schaffen, das ihn zu einem attraktiven Arbeitgeber für junge Talente macht. Andererseits braucht Swisscom eine «langweilige» Sicherheitskultur, die rufschädigende Netzausfälle verhindert. Als Garant dafür sieht sich bei Swisscom eine ältere Generation von Ingenieuren. Die technologische Entwicklung und die Strategie erfordern, dass der halbstaatliche Konzern die beiden Kulturen und Generationen unter einen Hut bringt. Auf die neue Swisscom-Spitze wartet also keine einfache Aufgabe.

Hilfreich ist, dass der Konzern wirtschaftlich solide dasteht. Der Umsatz setzte zwar auch in den ersten neun Monaten des Jahres den schon fast üblichen Sinkflug fort; mit 8,2 Mrd. Fr. lag er 3% unter der Vorjahresperiode. Die florierende Italien-Tochter Fastweb vermochte den Erlösrückgang erneut zu bremsen, aber nicht aufzuhalten. Der Preisdruck im gesättigten Heimmarkt ist dafür zu gross. Die Schweizer Konsumenten sind zwar noch immer nicht besonders preisbewusst. Die Zeiten, in denen der Branchenprimus dank der unbestrittenen Qualitätsführerschaft seine Marktanteile trotz hohen Preisen halten konnte, sind aber vorbei. Swisscom mischt im Preiskampf mittlerweile widerwillig mit.
«Wir werden den rechtlichen Weg beschreiten»

Damit der Ertrag nicht stärker sinkt als der Aufwand, setzt der Konzern auf striktes Kostenmanagement. In der Schweiz hat er beispielsweise seit Jahresbeginn gut 500 Vollzeitstellen gestrichen. Mit den Sparmassnahmen gelang es Swisscom in den ersten drei Quartalen, das operative Ergebnis stabil zu halten. Der Gewinn von 3,36 Mrd. Fr. auf Stufe Ebitda entspricht einer Marge von über 40%. Damit stellt Swisscom punkto Rentabilität den als besonders dynamisch geltenden Konkurrenten Sunrise in den Schatten. Man sollte sich also vom Image des «trägen Ex-Monopolisten» nicht täuschen lassen.

Die zweite Coronavirus-Welle dürfte dem Telekomkonzern wenig anhaben. Swisscom peilt bis Ende Jahr weiterhin einen Umsatz von rund 11 Mrd. Fr. und einen Betriebsgewinn von rund 4,3 Mrd. Fr. an. Der Konzernchef Schaeppi warnte indessen vor einem Stau im Mobilfunknetz, weil der wachsende Datenverkehr nicht mehr «verdaut» werden könne. Im vergangenen Jahr sei das Datenverkehr um 29% gestiegen. Die Kapazität des Mobilfunknetzes konnte Swisscom 2019 hingegen nur um 5% erhöhen. Dafür machte Schaeppi die Kantone mitverantwortlich. Einige Kantone hätten Moratorien erlassen, andere würden Baugesuche für Mobilfunkantennen nur verzögert oder gar nicht bearbeiten. Das sei illegal: «Wir werden an gewissen Orten auch den rechtlichen Weg beschreiten.»  ";https://www.nzz.ch/wirtschaft/die-swisscom-spuert-wenig-von-der-pandemie-hansueli-loosli-tritt-als-praesident-zurueck-ld.1584193;NZZ;Stefan Häberli;;;
07.09.2020;Der Tiktok-Eigentümer will mit seinem Algorithmus die «ultimative Personalisierung»;;https://www.nzz.ch/technologie/der-tiktok-eigentuemer-will-mit-seinem-algorithmus-die-ultimative-personalisierung-ld.1574780?reduced=true;NZZ;Matthias Müller;;;
08.01.2019;IBM gewinnt das Cern für sein Quantencomputer-Netzwerk ;"Das von IBM geschaffene Quantencomputer-Netzwerk darf sich über prominente Zugänge freuen. Wie die Firma am Dienstag an der Consumer Electronics Show in Las Vegas bekanntgegeben hat, werden sich Exxon Mobil, das Cern sowie einige der grössten Forschungslaboratorien der USA dem Q-Netzwerk anschliessen. In den kommenden Jahren wollen die Partner zusammen mit IBM ausloten, wie man in der Quantenchemie, der Teilchenphysik und der Kosmologie von den Vorzügen von Quantencomputern profitieren kann.
Mehr als Null oder Eins

Ein Quantencomputer unterscheidet sich von einem herkömmlichen Computer dadurch, dass seine Bits nicht nur die binären Zustände Null oder Eins annehmen können, sondern auch Überlagerungen dieser beiden. Zudem können die Quantenbits mit anderen Bits eine innige Beziehung eingehen, für die es in der klassischen Welt keine Entsprechung gibt. Theoretisch versetzen diese beiden Eigenschaften einen Quantencomputer in die Lage, gewisse Aufgaben sehr viel schneller zu lösen als herkömmliche Computer. Dazu gehören das Faktorisieren grosser Zahlen oder das Durchsuchen grosser Datenbanken.

In den letzten Jahren sind auf diesem Gebiet grosse Fortschritte erzielt worden. Zwar kann man die wartungsintensiven Quantencomputer noch nicht von der Stange kaufen. In den existierenden Prototypen nähert sich die Zahl der Quantenbits inzwischen aber jener Schwelle, bei der die Überlegenheit der Quantencomputer erkennbar werden sollte.

Um potenziellen Anwendern vor Augen zu führen, wozu Quantencomputer gut sind, hat IBM im Jahr 2017 eine Cloud-basierte Plattform namens IBM Q lanciert. Die Idee besteht darin, zahlenden Kunden über die Cloud Zugang zu den jeweils besten Quantencomputern von IBM zu verschaffen. Derzeit ist das ein Quantencomputer mit 20 Quantenbits. Der Nachfolger soll bereits mit 50 Quantenbits rechnen.

Für die Partner hat das Netzwerk den Vorteil, dass sie sich keinen eigenen Quantencomputer anschaffen müssen. Zudem profitieren die Kunden von dem Know-how, das sich IBM über die Jahre erarbeitet hat. Aber auch IBM verspricht sich einen Nutzen vom Netzwerk. So kennt man zwar einige der Anwendungen, bei denen Quantencomputer einen echten Vorteil gegenüber herkömmlichen Rechnern versprechen. Durch die Zusammenarbeit mit Industriefirmen und grossen Forschungslaboratorien möchte man jedoch weitere Anwendungsfelder für diese neuartigen Computer erschliessen. Das wiederum könnte den Kreis zukünftiger Kunden erweitern.
Schnellere Mustererkennung

Zu den ersten Mitgliedern des Q-Netzwerkes gehörten Firmen wie Daimler oder Samsung, Universitäten und Forschungslaboratorien. Mit dem Cern in Genf ist nun auch das weltweit grösste Laboratorium für Elementarteilchenphysik mit von der Partie. Am Cern werden Protonen im Stakkato aufeinandergeschossen. Schon heute verwenden Teilchenphysiker Mustererkennungsalgorithmen, um die Spuren der Trümmer zu rekonstruieren, die bei jeder Kollision entstehen. Um die Zahl der Kollisionen zu erhöhen, wird der Beschleuniger am Cern derzeit umgerüstet. Das dürfte die heute verwendeten Algorithmen an ihre Grenzen bringen. Deshalb arbeiten Forscher daran, die Mustererkennung durch maschinelles Lernen schneller zu machen. Zusammen mit IBM möchte das Cern nun herausfinden, welche Rolle dabei Quantencomputer spielen können.

Auch das Fermilab bei Chicago will Quantencomputer für maschinelles Lernen nutzen. Zum einen geht es wie am Cern darum, die Resultate von Teilchenkollisionen zu verstehen. Zum anderen möchte man mit den lernfähigen Quantenalgorithmen Objekte klassifizieren, die in grossangelegten kosmologischen Untersuchungen zutage treten.

Ganz anders gelagert ist das Interesse von Exxon Mobil. Die Firma will mit dem Quantencomputer von IBM unter anderem quantenchemische Reaktionen simulieren. Davon verspricht man sich die Entdeckung neuer Materialien, etwa für den Einfang von Kohlendioxid. Zudem möchte Exxon Mobil mit dem Quantencomputer Probleme angehen, die sehr rechenintensiv sind. Dazu gehört zum Beispiel die Frage, wie man das Stromnetz eines Landes optimieren kann. Laut eigenen Angaben ist Exxon Mobil die erste Energiefirma, die dem Q-Netzwerk beigetreten ist.";https://www.nzz.ch/wissenschaft/quantencomputer-ibm-gewinnt-cern-fuer-q-netzwerk-ld.1449835;NZZ;Christian Speicher;;;
12.10.2018;Wenn Algorithmen bei der Personalsuche diskriminieren;"Das Experiment des Online-Händlers Amazon zeigt die Grenzen des maschinellen Lernens auf. Entwickler des US-Technologiekonzerns programmierten eine Maschine so, dass sie aus Hunderten von Lebensläufen die fünf besten Bewerber herausfiltern sollte. Beim Einsatz des Computerprogramms stellte man jedoch fest, dass der Algorithmus männliche Kandidaten bevorzugt. Wie die Nachrichtenagentur Reuters schreibt, hat der Konzern das Projekt mittlerweile gestoppt. Laut Amazon wurde das Tool auch nie eingesetzt, um Kandidaten zu bewerten.
Die Maschine entscheidet

Die Maschine hat laut dem Bericht Frauen diskriminiert, obwohl die Software geschlechtsneutral programmiert wurde. Dies konnte geschehen, weil der selbstlernende Computer mit früheren Lebensläufen trainiert wurde, die meist von Männern stammten. In der Folge stellten die Entwickler fest, dass das Programm auf dieser Basis seine eigenen Schlüsse zog und Bewerberinnen, deren Lebenslauf etwa das Wort «Frauen» (z. B. Mitgliedschaft im Frauen-Schachklub) enthielt, benachteiligt wurden. Darüber hinaus sorgte das Programm aber offenbar noch für andere Probleme. Es hatte etwa Kandidaten empfohlen, die für den Job gar nicht genügend qualifiziert waren. Trotz diesem Rückschlag arbeitet Amazon weiter daran, die Personalsuche zu automatisieren.
Der Roboter im Personalbüro

Auch viele andere Konzerne wollen bei der Rekrutierung immer mehr Schritte technisch abwickeln. In den Personalabteilungen halten zunehmend Algorithmen Einzug. Laut einer Studie der Firma CareerBuilder erwartet über die Hälfte der US-Manager, dass künstliche Intelligenz in den nächsten fünf Jahren zu einem festen Bestandteil ihrer täglichen Arbeit wird.

Die Personaldienstleister setzen ebenfalls vermehrt auf Maschinen. Bei der Adecco-Gruppe werden Bewerber bisweilen vom Chatbot Mya begrüsst. Der virtuelle Assistent kommuniziert mit dem Kandidaten, erstellt eine Short List und lädt danach Bewerber zum Vorstellungsgespräch ein. Konzernchef Alain Dehaze sieht in der rasanten technologischen Entwicklung viele Chancen, aber auch Herausforderungen. Es stelle sich dabei etwa die Frage, wie man sicherstellen könne, dass Algorithmen die Bewerber nicht diskriminieren würden.
Faire Algorithmen?

Laut Experten für maschinelles Lernen dürfte es noch ein weiter Weg sein, bis möglichst diskriminierungsfreie Entscheide von Algorithmen sichergestellt werden können. Diese Feststellung entbehrt nicht einer gewissen Ironie. Da sich selbstlernende Computer anders als Menschen nicht von Vorurteilen, Gefühlen und Halbwissen leiten lassen sollen, setzte man nämlich ursprünglich grosse Hoffnungen darauf, dass sie nicht nur effizientere, sondern auch objektivere Entscheide als Menschen fällen würden.

Doch so einfach ist das nicht. Algorithmen lernen anhand von Daten, die von Menschen ausgewählt werden. Sind die Kriterien nicht objektiv, wird der Fehler durch den Computer reproduziert. Auch können Computer Zusammenhänge erkennen, die Menschen verborgen bleiben, was im Ergebnis zu diskriminierenden Entscheiden führen kann. Die Algorithmen sind auch für die Entwickler bis zu einem gewissen Grad eine Blackbox.

Trotz diesen Herausforderungen übernehmen Maschinen bei der Rekrutierung eine zunehmend bedeutende Rolle. Algorithmen suchen nach geeigneten Kandidaten, prüfen den Lebenslauf und analysieren das Auftreten beim Interview. Auf der Suche nach Fachkräften können Firmen dank dem Einsatz von Software beispielsweise erkennen, ob ein Spezialist wechselwillig ist. Ein Hinweis dafür ist etwa, wenn jemand nach langer Zeit sein Profil bei einem Karriere-Netzwerk auf den neusten Stand bringt.
Verbreitete Videointerviews

Die Vorselektion von Lebensläufen durch Maschinen ist heute bei grösseren Firmen üblich. Für die Kandidaten ist es damit entscheidend, dass ihre Bewerbung die verlangten Schlüsselbegriffe enthält. Zunehmend setzen Unternehmen bei der Rekrutierung auch auf Videointerviews. Die US-Firma HireVue analysiert solche Bewerbungsgespräche und wertet dabei Merkmale der Bewerber aus, die zum Teil aus dem Unterbewusstsein stammen.

Obwohl der Bewerbungsprozess immer technischer wird, dürfte der Mensch bei der Personalauswahl weiterhin eine bedeutende Rolle spielen. Denn bei der Einstellung eines neuen Mitarbeiters geht es nicht nur um die objektive Beurteilung seiner Fähigkeiten, sondern auch darum, ob er zum Unternehmen und zum Team passt. Um dies beurteilen zu können, braucht es menschliche Intuition. ";https://www.nzz.ch/wirtschaft/wenn-algorithmen-bei-der-personalsuche-diskriminieren-ld.1427704;NZZ;Natalie Gratwohl;;;
24.09.2019;Googles Quantencomputer lässt Superrechner alt aussehen;"Erstmals hat ein Quantencomputer eine Aufgabe gelöst, die für jeden existierenden klassischen Computer so gut wie unlösbar ist. Der von Google entwickelte Chip Sycamore berechnete das Problem in Minutenschnelle, während ein Superrechner Jahrtausende brauchen würde. Damit hat das Unternehmen die schon 2017 angekündigte «Quantenüberlegenheit» erreicht, wie aus einem Forschungsartikel hervorgeht, der letzte Woche kurzzeitig auf der Website von Googles Kooperationspartner Nasa erschien, dann aber wieder vom Netz genommen wurde. Der Chip löste zwar nur eine einzige, für die Praxis irrelevante Aufgabe. Dennoch betrachten Physiker das Ergebnis als bahnbrechend. Nach wie vor gibt es aber keinen Quantencomputer, der in der Praxis einem normalen Rechner vorzuziehen wäre.
Sowohl 0 als auch 1

Nach den Regeln der Quantenphysik können Quantenrechner eine gigantische Zahl von Rechenschritten simultan ausführen, die ein klassischer Rechner nacheinander abarbeiten müsste. Dafür nutzt er die Überlagerung, die nur in der Welt der kleinsten Teilchen existiert. Ein einzelnes Partikel, ein Elektron etwa, kann zum Beispiel links- und gleichzeitig rechtsherum rotieren. Diese Doppelexistenz lässt sich in der Informationstechnik als sogenanntes Qubit nutzen. Dieses speichert die beiden Werte 0 und 1 simultan, während das klassische Bit nur jeweils einen davon aufnehmen kann. Mit jedem zusätzlichen Qubit verdoppelt sich die Zahl der Möglichkeiten. Googles Sycamore-Prozessor arbeitet mit 53 Qubits. Damit lassen sich etwa 100 Millionen Milliarden Kombinationen aus 0 und 1 speichern.

Um zu rechnen, müssen Qubits untereinander verknüpft werden. Ein Programm verbindet die Qubits paarweise in einer bestimmten, von dem konkret zu lösenden Problem abhängigen Reihenfolge.

In der Praxis jedoch ist eine solche Berechnung fehleranfällig, da Qubits sehr empfindlich auf Umwelteinflüsse reagieren. Die nötige Abschirmung wiederum erschwert es, die Ergebnisse auszulesen. Experten glauben, dass dies nur für bestimmte Anwendungen gelingt, wie etwa Optimierungsaufgaben oder maschinelles Lernen.

Diese Applikationen sind interessant für einen Datenkonzern wie Google. Schon 2014 engagierte die Firma einen der führenden Physiker auf dem Gebiet: John Martinis von der University of California in Santa Barbara. Sein Team entwickelt Chips mit supraleitenden Qubits. Diese sind so etwas wie künstliche Atome. Es handelt sich um maschinell hergestellte Bauelemente, im Fall von Sycamore aus Aluminium und Indium. Die Elektronen darin verhalten sich bei Temperaturen nahe dem absoluten Nullpunkt wie Qubits. Mit Mikrowellensignalen lassen sie sich verknüpfen und die Rechenergebnisse auslesen.

Um die Quantenüberlegenheit zu demonstrieren, hat Google eine Art Zufallsalgorithmus laufen lassen. Dieser verknüpfte Qubits in einer zufälligen Reihenfolge untereinander. Dann wurde eine Messung gemacht. Dieser Vorgang wurde millionenfach wiederholt. Auf diese Weise generierte der Quantencomputer innerhalb von wenigen Minuten eine chaotische Sammlung von Kombinationen aus 0 und 1. Diese weicht geringfügig vom reinen Zufall ab, einige Varianten sind häufiger als andere. Die entsprechende Wahrscheinlichkeitsverteilung lässt sich mit einem klassischen Rechner simulieren. Allerdings würde dies wegen des enormen Speicherbedarfs Jahrtausende dauern, wie Tests auf dem derzeit schnellsten Superrechner Summit in den USA zeigten, bei denen Teile der Aufgabe berechnet und daraus die Gesamtrechenzeit abgeschätzt wurde.
Ein Meilenstein mit Einschränkungen

Das sei ein entscheidender Meilenstein, sagt Frank Wilhelm-Mauch von der Universität des Saarlandes. Der Physiker leitet das EU-Projekt OpenSuperQ, das in den nächsten drei Jahren einen Quantencomputer mit supraleitenden Qubits am Forschungszentrum Jülich aufbauen will. Er lobt Googles «grandiose Ingenieursleistung». Es sei aber nur ein Anfang. Damit praxistaugliche Aufgaben wie etwa das Simulieren von Molekülen in der Wirkstoffsuche schneller gelingen als auf Superrechnern, müsste die Fehleranfälligkeit der Qubits noch eingeschränkt sowie etliche technische Probleme gelöst werden, wie etwa die Steuerelektronik in den gekühlten Teil des Quantenrechners zu integrieren. Er vergleicht den jetzt erreichten Stand mit einer Stadionrunde in einem Marathonlauf.

Das Ergebnis sei sehr beeindruckend, pflichtet Andreas Wallraff von der ETH Zürich bei, der ebenfalls an supraleitenden Qubits forscht. Sycamore sei der am wenigsten fehleranfällige existierende supraleitende Quantenchip dieser Grösse. Der Physiker schränkt aber die Bedeutung des Ergebnisses etwas ein: Die Testaufgabe sei eigens so konstruiert, dass sie für den Chip gerade so lösbar sei, während ein klassischer Rechner sich besonders schwer tue. Klassische Computer nutzten oft besondere mathematische Strukturen eines Problems, um schneller zur Lösung zu kommen, sagt Wallraff. Der Zufallscharakter der Aufgabe mache dies aber unmöglich.
Was heisst besser?

Ob die Quantenüberlegenheit ein geeignetes Kriterium ist, um die Qualität eines Quantencomputers zu beurteilen, ist in der Fachgemeinde umstritten. Ein anderes Kriterium nennt sich Quantenvorteil. Dieser wäre erst erreicht, wenn ein Quantenrechner einen bedeutenden praktischen Vorteil gegenüber klassischen Computern erlangen würde. Dies wäre zum Beispiel der Fall, wenn er die Suche nach einem medizinischen Wirkstoff bei gleichen Kosten viel schneller erledigen könnte als ein herkömmlicher Computer.

Obwohl Quantenüberlegenheit bombastischer klingt, ist sie sehr viel leichter zu erreichen als der Quantenvorteil. Googles Konkurrent IBM nutzt einen dritten Massstab namens Quantenvolumen. Dieser fasst technische Kriterien zusammen, die als entscheidend für die Leistung eines Quantenrechners gelten. Das sind neben der Zahl der Qubits auch deren Lebensdauer und die Möglichkeit, sie untereinander zu verknüpfen. In Googles Sycamore etwa sind nur direkt benachbarte Qubits verknüpfbar.

Dennoch sei Googles Ergebnis sehr motivierend, meint Wallraff. Es sei sogar denkbar, dass ein clever ausgedachter Algorithmus auf Sycamore eine praxistaugliche Aufgabe löse, meint Tomasso Calarco vom Forschungszentrum Jülich. Googles Team um John Martinis sieht das ähnlich: «Wir sind nur einen kreativen Algorithmus entfernt von wertvollen Anwendungen», lautet der Schlusssatz ihrer Publikation.";https://www.nzz.ch/wissenschaft/google-quantencomputer-beweist-erstmals-seine-ueberlegenheit-ld.1510869;NZZ;Christian J. Meier;;;
22.08.2019;«In Zukunft werden wir Mensch und Maschine wohl nicht mehr unterscheiden können»;"Lutz Jäncke, wie können wir sicher sein, dass wir einem Menschen und nicht einem künstlichen Wesen gegenübersitzen?

Heutige Roboter kann man noch eindeutig von Menschen unterscheiden. Aber in Zukunft werden wir Mensch und Maschine wohl nicht mehr auseinanderhalten können.

Warum?

Was wir gemeinhin als menschlich betrachten, sind Emotionen, Empathie, Witz, Kreativität oder die Fähigkeit, Kunst und Kultur zu entwickeln – also Verhaltensweisen und psychologische Fertigkeiten, die wir selber nicht so richtig verstehen. Wenn diese Fertigkeiten von einer Maschine generiert würden, dann könnten Sie diese Definition nicht mehr aufrechterhalten.

Das klingt nach Science-Fiction.

Ich gebe zu: Das gibt es noch nicht, das ist im Moment eher Spekulation. Aber meiner Ansicht nach ist es durchaus vorstellbar. Die Computertechnik hat sich in den letzten zehn Jahren unglaublich weiterentwickelt. Das maschinelle Lernen hat sich extrem verbessert. Da stecken unfassbare Algorithmen dahinter, die ein Eigenleben entwickeln können.

Sie bezeichnen Menschen in Ihren Publikationen als «Kulturwesen», weil wir – um zu überleben – Kulturen erlernen. Werden Maschinen deshalb immer menschenähnlicher?

Das ist genau der Punkt: Moderne Maschinen verfügen über Lernalgorithmen. Wenn wir sie in einer menschlichen Umgebung lernen lassen, dann werden sie sich genauso anpassen wie der Mensch. Ich bin überzeugt, dass wir irgendwann in der Lage sein werden, Robotergehirne zu generieren, welche menschliche Verhaltensweisen zeigen. Wird so ein künstliches Gehirn dann in eine Maschine gebaut, die dazu noch aussieht wie ein Mensch, fällt die ganze Definition, was Menschlichkeit bedeutet, darnieder.

Aber es braucht doch weit mehr, um menschlich zu sein. Gefühle zum Beispiel.

Neuropsychologisch wissen wir, dass Gefühle Bewusstseinsphänomene sind, die garniert sind mit körperlichen Empfindungen. Grummeln im Magen, Schweissausbrüche, erhöhte Herzfrequenz. Diese Informationen werden gesammelt, aus dem vegetativen Nervensystem in das Inselgebiet im Gehirn gemeldet, dort verwoben und dann mit den Informationen, die wir gelernt haben, im Gedächtnis abgespeichert. Dann können Phänomene entstehen, die nennen wir: Als-ob-Emotionen. Wenn Sie sich an ein bestimmtes Ereignis erinnern, werden diese Emotionen abgerufen. Und jetzt wird’s interessant!

Wir sind gespannt . . .

Es könnte so weit kommen wie in der Zeichentrickserie «Futurama»: Dort generieren die Gehirne selbst ihre Welt, ganz ohne Körper. Die Voraussetzung dazu ist aber, dass sie einmal die Erfahrung eines Körpers erlebt haben. Denn das Grummeln und das Kribbeln werden nicht im Magen oder im Herzen generiert, sondern letztlich im Gehirn. Warum soll ein künstlich geschaffenes Hirn also keine Emotionen entwickeln können?

Vielleicht, weil da immer noch etwas fehlt, was schwer greifbar ist, was man vielleicht Seele nennen könnte?

Die Vermutung, dass der Mensch über eine Seele verfügt, ist ja reine Spekulation. Eine Spekulation kann man aber nicht mit einer anderen widerlegen. Meine Spekulation beruht auf einer Interpretation dessen, was ich über das Gehirn und das Nervensystem weiss. Ihr vertraue ich mehr, weil sie auf dem derzeitigen Wissensstand basiert und nicht darauf, was jemand vor 1500 Jahren in irgendwelchen Schriften niedergeschrieben hat. Das Gehirn ist ein biochemisches System, das nach physikalischen Gesetzen arbeitet. Dieses System generiert unser Bewusstsein. Also müsste man es reproduzieren können.

Erinnerungen an die Kindheit oder die Eltern werden einem künstlichen Gehirn aber immer fehlen.

Menschliche Erinnerungen sind im Wesentlichen konstruiert – je weiter wir in die Vergangenheit zurückgehen, rekonstruieren wir unsere Vergangenheit. Man ist überzeugt, etwas erlebt zu haben, dabei basieren diese Erlebnisse meistens auf Geschichten, die man gehört hat. Ich kann mir gut vorstellen, dass man in Zukunft neuronale Netze zur Verfügung hat, bei denen man bestimmte Erinnerungsfetzen aktivieren kann, die dann zur kohärenten Geschichte verwoben werden. Insofern ist das kein Merkmal zur Unterscheidung zwischen Mensch und Maschine.

Trotzdem wird sich das menschliche Gehirn immer von der Maschine unterscheiden, weil es aus Eiweiss, Wasser, Kohlenstoff und so weiter und nicht aus Drähten besteht.

Auch diese Unterscheidung gerät ins Wanken. Bereits heute wird daran geforscht, Computernetzwerke aus biologischem Material zu bauen. Es gibt bereits erste Modelle, die aus solchen Materialien bestehen.

Die Maschinen gleichen uns also immer mehr. Was unterscheidet uns denn von den Tieren?

Nicht viel, je nachdem welche Tierart wir betrachten. Trotz unserer vermeintlichen Intelligenz sind wir vollgepackt mit ähnlichen Antrieben, wie sie zum Beispiel auch bei anderen Primaten zu finden sind. Sex, Empathie, Aggressivität, Moral, Neugierde. Darüber verfügen auch die Affen. Weil wir so lernfähig sind, können wir aber Kulturen entwickeln, die extrem differenziert sind.

Und wir sind vernünftig. Meistens jedenfalls.

Die Vernunft, wie wir sie derzeit auffassen, hat im Grunde wenig mit unserem Handeln zu tun. Wir sind biologische Wesen, die von der Natur programmiert wurden, um zu überleben. Der ultimative Zweck ist das Überleben und die Fortpflanzung. Alles andere ist Beigeschmack.

Wie funktioniert die menschliche Überlebensstrategie?

Wir werden zufällig in eine Kultur hineingeboren, die wir nicht selber erschaffen haben. Aber wir müssen darin überleben. Das ist der Motor, der uns zum Lernen verdammt. Wären Sie in einer anderen Kultur aufgewachsen, hätten Sie einen völlig anderen Blick auf die Welt.

Damit negieren Sie den freien Willen.

Ich bin skeptisch, dass es den freien Willen in dieser Form gibt, in der wir gerade glauben, dass er existiert. Die Bedeutung, die der freie Wille für unsere Verhaltenssteuerung einnimmt, ist wahrscheinlich eine ganz andere.

Nämlich?

Der freie Wille geht nie der neurophysiologischen Aktivität, die ein Verhalten generiert, voraus. Das ist gar nicht möglich. Er folgt ihr nur.

Geben Sie uns ein Beispiel.

Nehmen Sie die Fridays-for-Future-Bewegung: Ein einzelnes Mädchen zeigt Verhaltensweisen, die unüblich sind und unsere Aufmerksamkeit anziehen. Sie stellt sich mit einer Tafel vor das Parlament und demonstriert. Plötzlich gehen alle auf die Strasse. In Zürich arbeiten weltbekannte und herausragende Klimawissenschafter, die schon seit Jahrzehnten forschen und mahnen, aber die Öffentlichkeit schenkt ihnen nicht genug Aufmerksamkeit.

Das klingt ein bisschen verärgert.

Ich bin überhaupt nicht verärgert. Ich bin bestätigt in meiner Annahme. Der Sachverhalt ist vollkommen klar: Wir haben ein Klimaproblem, wir machen die Umwelt kaputt – aber das wissen wir schon seit dem Club of Rome Mitte der 1970er Jahre. Die abstrakte Botschaft aus dem Elfenbeinturm kam jedoch nicht an.

Warum nicht?

Wir werden weniger durch komplexe und sachliche Argumente überzeugt, sondern mehr durch Emotionen. Der Mechanismus, der dahintersteckt, ist hochinteressant.

Wie sieht der aus?

Grob erklärt sind es ähnliche Mechanismen, wie wir sie beim Fussball erleben, wenn ganz normale Menschen an ein Fussballspiel gehen und im Stadion mit 80 000 anderen herumbrüllen. Oder wenn Meghan Prinz Harry heiratet und ihr Bild plötzlich auf allen Kaminsimsen in Grossbritannien steht. Diese Attraktion, in der Masse aufzugehen, von etwas Emotionalem mitgerissen zu werden – das ist wichtig für uns Menschen. Aber dieser Mechanismus kann natürlich auch zu negativen Konsequenzen führen.

Liegt hier vielleicht der eigentliche Unterschied? Könnte eine Maschine diese unlogischen Komponenten überhaupt imitieren?

Unser Verhalten ist zwar oft unvernünftig, aber nicht beliebig. Denn diese Form der Unvernunft führt dazu, dass wir überleben. Für mich ist Vernunft das Verhalten, das hilft, in einem gegebenen kulturellen Umfeld ein erfolgreiches Leben zu führen. Maschinen können mit Sicherheit irgendwann einmal leisten, dass sie sich in Regeln und Systeme hineinlernen und angepasst verhalten.

Werden Roboter eines Tages die besseren Menschen sein?

Der Mensch verfügt über die technischen und moralischen Mittel, einen anderen Menschen zu töten. Er kann sich selbst sagen: Ich bin im Recht, einen anderen zu töten, weil er die Regeln verletzt hat. Etwa bei Hinrichtungen. Das macht in dieser Form kein Tier. Ein Tier tötet im Affekt oder zur Nahrungsaufnahme, der Mensch zelebriert den Tötungsakt. Ich weiss, Kollegen wie Steven Pinker sagen, die Welt werde immer friedfertiger, aber ich teile das nicht.

Das klingt sehr pessimistisch.

Ich muss ehrlich gestehen, ich mache mir schon ein bisschen Sorgen. Ich glaube nicht, dass der Mensch konstruiert wurde, um in dieser Welt, in der er heute lebt und die er sich selber geschaffen hat, zurechtzukommen.

Wie meinen Sie das?

Die Informationsflut führt dazu, dass unser Hirn versucht, aus der Menge die einfachsten Informationen herauszufiltern. Je mehr Informationen vorhanden sind, desto mehr versuchen wir zu vereinfachen. Das ist menschlich. Das Gehirn versucht, Klarheit zu schaffen, weil es Unordnung und Chaos scheut wie der Teufel das Weihwasser. In der heutigen Welt ist das unheimlich schwierig, deshalb erleben auch Populisten einen solchen Aufwind.

Sind wir also doch nicht die Krone der Schöpfung?

Als humanistisch gebildeter Mensch und Naturwissenschafter bin ich mittlerweile davon überzeugt, dass wir den Menschen masslos überschätzen. Wir stellen uns ja immer als denkende, logische, wissende und vernünftige Wesen dar. Das ist mitnichten der Fall. Mindestens 90 Prozent unseres Verhaltens sind uns unbewusst.

Wir machen uns etwas vor.

Davon bin ich überzeugt.

Könnten uns die Maschinen also bald als Herrscher des Planeten ablösen?

Ja, dieses Szenario sehe ich aus zwei Gründen. Erstens bringen wir Menschen uns durch unsere Unvernunft selbst stark in Bedrängnis. Und zweitens stehen wir uns selbst im Weg durch das Konkurrenzverhalten. Wir sind unglaubliche Egoisten gegenüber anderen.

Als Forscher finden Sie heraus, wie das Gehirn funktioniert. Wenn eines Tages tatsächlich ein künstliches Gehirn die Weltherrschaft übernimmt, dann tragen Sie eine Mitschuld.

Es ist noch so weit weg, dass ich mich da nicht in eine moralische Ecke gedrängt fühle. Rein wissenschaftlich ist die Frage natürlich unfassbar interessant. Da kommt ein Antrieb in mir hoch, der bei Menschen kaum zu unterdrücken ist: die Neugier. Die Geschichten, die wir hier besprochen haben, sind theoretische, spannende, belletristische Zukunftsvisionen. Für mich sind andere, konkrete Dinge aber viel wichtiger. Wie kann ich einer Frau helfen, die nach einem Schlaganfall nicht mehr sprechen kann, die Sprache wiederzuerlangen? Wie kann ich Menschen in der Schule helfen, zu lernen, oder alten Menschen die Angst vor dem Alter nehmen?

Der Gedanke, dass Ihr Wissen auch missbraucht werden könnte, bereitet Ihnen keine Sorgen?

Wenn Sie diese Angst hätten, dürften Sie fast gar nichts mehr machen. «Der Krieg ist der Vater aller Dinge», sagte schon Heraklit. Sie können alles zum Guten oder Schlechten drehen. Ich habe keine Angst per se, was in der Zukunft kommt. Ich habe nur Angst um den Menschen an sich, weil er sich so merkwürdig verhält.";https://www.nzz.ch/zuerich/mensch-oder-maschine-interview-mit-neuropsychologe-lutz-jaencke-ld.1502927;NZZ;Nils Pfändler, Tobias Sedlmaier;;;
12.05.2017;Nicht nachdenken, programmieren!;"Zum Jahreswechsel 2012 hatte der damalige Bürgermeister von New York, Michael Bloomberg, einen originellen Neujahrsvorsatz: Er werde in diesem Jahr programmieren lernen, verkündete das Stadtoberhaupt via Twitter. Was von diesem hehren Ansinnen übrig blieb, ist nicht bekannt. Aber der Tweet löste eine Debatte aus. In den USA und auch hierzulande gibt es eine mächtige Lobby fürs Programmieren. Google wirbt in Hochglanzbroschüren und auf Bildungskonferenzen dafür, Programmieren als Schulfach einzuführen – freilich nicht aus bildungspolitischem, sondern aus Eigeninteresse, um Nachwuchskräfte zu rekrutieren.

Mit code.org gibt es eine von den Tech-Giganten finanzierte Non-Profit-Organisation, auf der Prominente wie Mark Zuckerberg und Bill Gates fürs Programmieren trommeln. Zuckerberg, der oberste Bildungsbeauftragte der Nation, sprach von digitalen Erweckungserlebnissen, als er einem Nachbarskind das Programmieren beibrachte. Das klang rührig. Die deutsche Bundeskanzlerin Angela Merkel bezeichnete Programmieren «als Grundfähigkeit neben Lesen, Schreiben und Rechnen». Programmieren ist die neue Lingua franca des Internetzeitalters. An der Eliot-Pearson Children’s School der Tufts University lernen Kinder das kleine Einmaleins des Programmierens: A wie Algorithmus, das soll dem Nachwuchs eingebimst werden, und zwar besser heute als morgen.
Mensch oder Maschine? Völlig egal!

Mittlerweile hat die Debatte, angestossen durch einen Artikel in der «New York Times», einen neuen Drive bekommen. Schüler, so der Tenor des Artikels, sollen nicht nur programmieren, sondern wie ein Computer denken lernen. Computational Thinking heisst das Stichwort. «Computational Thinking», so eine Definition der Universität Paderborn, «bezieht sich auf die individuelle Fähigkeit einer Person, eine Problemstellung zu identifizieren und abstrakt zu modellieren, sie dabei in Teilprobleme oder -schritte zu zerlegen, Lösungsstrategien zu entwerfen und auszuarbeiten und diese formalisiert so darzustellen, dass sie von einem Menschen oder auch einem Computer verstanden und ausgeführt werden können.» Die Informatik-Professorin Jeannette Wing, eine der Promotoren der Idee, definiert den Begriff so: «Computational Thinking ist der Gedankenprozess, der sowohl die Formulierung eines Problems als auch die Repräsentation der Problemlösung so darstellt, dass sie von Menschen oder durch Maschinen ausgeführt werden könnte.» Das Problem soll methodisch so zurechtgelegt werden, dass es nach bestimmten formalen Vorgaben auch von einem Computer gelöst werden kann.

Statt zum Beispiel zu fragen, ob Medien eine linke Schlagseite hätten, solle man besser die Frage stellen, ob Linke in grossen Tageszeitungen häufiger als links identifiziert würden als Konservative als konservativ. Das klingt etwas verkopft, meint aber lediglich ein iteratives, strukturiertes Vorgehen. Im Grunde folgt jeder Satz, den man nach syntaktischen Regeln (Subjekt, Prädikat, Objekt) bildet, dem Gedanken des Computational Thinking. Und mit derlei Vorgaben lassen sich auch Schreibroboter programmieren.
So selbstverständlich wie ein Bleistift

Als Vordenker des Konzepts gilt der 2016 verstorbene Computerpionier Seymour Papert, ein Erkenntnistheoretiker und Erfinder der Programmiersprache Logo, der Kinder früh an Computer heranführen wollte. Richtig eingesetzt, seien Computer eine Erkenntnismaschine, die Kreativität und Lernen fördere, war er überzeugt. Das Konzept war nicht unumstritten. Bildungsforscher warfen Papert vor, er würde die Rolle der Technologie überbewerten und den klassischen Bildungskanon vernachlässigen, wobei sich Papert dezidiert gegen ein «technozentrisches Denken» aussprach.

Der französische Mathematiker und Hydrauliker Gaspard de Prony entwarf bereits ab 1792 eine mathematische «Fabrik», in der drei Gruppen von menschlichen Rechnern arbeitsteilig logarithmische und trigonometrische Tafeln zerlegten. Die Formeln wurden so programmiert, dass am Ende die dritte Abteilung von «Computern» – Menschen, die mit Papier und Bleistift arbeiteten – bloss die arithmetischen Grundoperationen ausführen musste, um die Ergebnisse zu erhalten. De Pronys «Fliessband» für die Erstellung von Tabellen war die moderne Version des menschlichen Computers – Computational Thinking avant la lettre. Über 200 Jahre später, 1994, titelte der «Spiegel»: «Lernen mit Computer – Schöne neue Schule». Auf dem Cover war ein Schüler abgebildet, der statt eines Schulranzen einen Computer mit Tastatur sattelt. In der Titelgeschichte hiess es: «Selbstverständlich wie einen Bleistift sollen die Kinder in allen Schulfächern die Maschine nutzen, Datenbanken anzapfen, mit anderen Schulen kommunizieren und Informationen verarbeiten.»
Wo bleibt die Revolution?

Die grosse Bildungsrevolution ist abgesehen von ein paar Steve-Jobs-Schulen ausgeblieben, auch wenn man die Selbstverständlichkeit, mit der junge Menschen Smartphones nutzen, über einen Zeitraum von zwanzig Jahren durchaus als disruptive Entwicklung bezeichnen könnte. Jetzt, wo Lehrinhalte digitalisiert werden und man mit mobilen Endgeräten Zugriff auf fast alle Fachzeitschriften hat, könnte sich der Erkenntnisprozess revolutionieren. Der Mensch könnte mit maschinellem Input endgültig zur Thesenmaschine werden und nach dem Vorbild de Pronys wie am Fliessband Ideen generieren. Das wiederum könnte massive Implikationen für die Hypothesenbildung und den gesellschaftlichen Diskurs haben. Wenn der Mensch bestimmte Mustererkennungsverfahren anwendet, könnte man aus Datensätzen neue Gesetzmässigkeiten ableiten.

Der amerikanische Informatiker und Quantenforscher Scott Aaronson vertritt in seinem Essay «Why Philosophers Should Care About Computational Complexity» das Argument, dass die Komplexitätstheorie das philosophische Denken verändern werde. Mithilfe von Grossrechnern könne man beispielsweise das Rätsel der Unteilbarkeit von Primzahlen lösen und damit epistemologische Grundannahmen sprengen, oder mit Quantenmechanik Theorien testen. Um herauszufinden, was eine Sache explizit bedeutet, braucht es nur den richtigen Algorithmus. Das wirft freilich Fragen nach unserem Erkenntnisapparat auf, ob man mit Heuristiken (etwa nach der Methode Trial and Error) noch genügend Erkenntnisse gewinnen kann, oder ob uns künstliche Intelligenzen bei der Wahrheitssuche schon derart überlegen sind, dass wir bei der Informationsgewinnung auch computerisierte Techniken anwenden müssen. Soll der Mensch wie ein Algorithmus operieren, wo sein Gehirn so leistungsfähig wie ein Grossrechner ist? Ist maschinelles Lernen nicht ein computerpädagogisches Konzept? Sind neuronale Netzwerke, die nach dem menschlichen Gehirn modelliert sind, vielleicht die effizientere Kopie? Liessen sich nicht grössere Lernerfolge erzielen, wenn der Mensch sein Denken formatiert und bei der Bearbeitung von Problemen die Lösungsstrategien von Computern appliziert?
Technologischer Darwinismus

Der Ingenieur David Young verwahrte sich in einem Beitrag für das Portal «Medium» gegen das Konzept des «Computational Thinking» und plädierte für einen erweiterten Begriff der «computer literacy». Man solle nicht computerisiertes Denken, sondern das Denken über Computer lehren. Computer, erinnerte Young, seien zunächst einmal Rechenmaschinen, also Hilfsgeräte; Psychologie spiele bei ihrer Produktion keine Rolle. Viele Programmierer hätten die modellhafte Vorstellung vom Endnutzer als einem Computer, der «unendlich ruhig» sei, eine «unrealistisch grosse und verlässliche Festplatte» habe sowie «regelmässige» Outputs produziere.

Computerfähigkeit und computerisiertes Denken bereiten Menschen darauf vor, mit Maschinen ohne Kritik umzugehen. Wenn der Mensch wie ein Computer denkt, so die implizite Conclusio, verdrahtet er sich zu einem Chip in einem gigantischen Grossrechner, wo er programmierbar wird. Entsprechend postulierte Quincy Larson, Lehrer bei der Organisation FreeCodeCamp.com, im Technikblog «Techcrunch»: «Program or be programmed.» Programmiere oder werde selbst programmiert. Das lässt Programmieren als Unausweichlichkeit erscheinen, als einen technologischen Darwinismus.

Inmitten von all den beflissenen Technik-Nerds fühlt man sich fast schon als Analphabet. Java, C und Co. heissen zwar auch Programmiersprachen, aber eigentlich sind es keine Sprachen, sondern technische Systeme zur Formulierung von Anweisungen. Menschliche Sprache ist eine Technik, die unter anderem zur Formulierung von Meinungen, Ansichten und normativen Aussagen eingesetzt wird. Schon allein deshalb geht der Vergleich fehl, Codieren sei eine Fremdsprache. Wer wie ein Computer operiert, vollzieht nur Programmierbefehle – und entäussert sich jeden kritischen, liberalen Denkens. «Die wahren Analphabeten», sagt der iranische Blogger Hossein Derakhshan, «sind diejenigen, die nur noch mit Bildern und Emojis kommunizieren.»";https://www.nzz.ch/feuilleton/soll-der-mensch-wie-ein-computer-denken-ld.1292090;NZZ;Adrian Lobe;;;
28.05.2019;China ist mit oder ohne Huawei eine Macht;"Mit China hat das Silicon Valley zum ersten Mal einen ernstzunehmenden Konkurrenten im Technologiebereich. Ein Beispiel dafür ist Huawei, dessen Angebot in der Mobilfunktechnik laut Branchenkennern nicht nur konkurrenzfähig ist, sondern in manchen Nischen sogar weltmarktführend. Nun soll die Sperre durch Präsident Trump diesen Zug stoppen.

Das mag durchaus funktionieren, zumal die gesamte Wertschöpfungskette rund um Huawei durch die Sperre aus den Angeln gehoben wird. Dass darunter vor allem amerikanische Halbleiterunternehmen zu leiden haben, ist einer der zahlreichen Nebeneffekte dieses Entscheides. Insofern ist die negative Reaktion der Wall Street durchaus verständlich.
Seltene Erden als Trumpf

Als mögliche Reaktion halten die Entscheidungsträger im Reich der Mitte zwei Trümpfe in der Hand. Einer ist direkt und dürfte die Amerikaner sofort treffen. Es handelt sich dabei um einen Exportstopp für sogenannte seltene Erden. Das sind Rohstoffe, die in der Halbleitertechnologie sehr wichtig sind. Aus verschiedenen Gründen werden seltene Erden heutzutage fast ausschliesslich in China gefördert.

Ein Exportstopp wäre kurzfristig verheerend. Die neuesten Tesla-Modelle, das nächste iPhone und viele andere Elektronikgeräte könnten nur mit beschränkter Funktionalität produziert werden. Zwar ginge es laut Ingenieuren auch ohne seltene Erden, doch dauerte das etwas länger. In der kurzen Frist wäre die Wertschöpfungskette aber stark beeinträchtigt.

Ein zweiter Trumpf der Chinesen ist der rapide Fortschritt im Bereich der künstlichen Intelligenz. Auch hier besteht ein natürlicher Vorteil, zumal den chinesischen Unternehmen einerseits sehr viel Datenmaterial zur Verfügung steht, was für die Entwicklung von Algorithmen der künstlichen Intelligenz unerlässlich ist. Anderseits können sie im Reich der Mitte auf billige Arbeitskräfte zurückgreifen, die den arbeitsintensiven Vorgang von Datenbeschreibung durchführen.

Moderne Algorithmen für maschinelles Lernen benötigen massive Datenmengen, die zuerst (meist von Hand) beschriftet werden müssen. Wie gut die Chinesen schon sind, zeigen Beispiele wie etwa die zahlreichen Startups im Bereich der Computervision, wo sie laut Experten aus dem Silicon Valley sogar vor den Amerikanern liegen. Auch bei der digitalen Spracherkennung halten die chinesischen Konkurrenten Schritt mit den besten des Westens.
Wird Silicon Valley abgehängt?

Aus Sicht der Anleger ist die Technologieführerschaft im Bereich der künstlichen Intelligenz der zentrale Punkt. Man muss sich hier laut Fondsmanagern ernsthafte Fragen stellen. Eine davon ist, ob die Chinesen die vermeintlichen Branchenleader im Silicon Valley wie Google oder Amazon überholen werden. Eine andere ist, ob man dort investieren soll.

Während sich die Gemüter allseits ob der Probleme rund um Huawei und mögliche Vergeltungsschläge der Chinesen erhitzen, findet der wirkliche Kampf in den Laboren der vielen Startups im Bereich der künstlichen Intelligenz statt. Hier hat das Reich der Mitte einen natürlichen Vorteil – und dieser lässt sich nicht mit einer Sperre ausser Kraft setzen.";https://www.nzz.ch/finanzen/china-ist-mit-oder-ohne-huawei-eine-macht-ld.1485134;NZZ;Krim Delko;;;
02.12.2019;Innoplexus-Gründer Bhardwaj will «deutsche Präzision mit indischer Energie verbinden»;"Begonnen habe alles 2010 mit der Krebserkrankung seines damaligen Chefs und engen Freundes, erzählt Gunjan Bhardwaj, Gründer und CEO des deutschen KI-Unternehmens Innoplexus, bei unserem Treffen in Berlin. Damals war er in der Unternehmensberatung tätig. Nach Deutschland gekommen war der gebürtige Inder und heutige deutsche Staatsbürger dank einem Stipendium des Deutschen Akademischen Austauschdienstes, das ihm einen MBA-Abschluss an der Hochschule Pforzheim ermöglichte. Später promovierte er an der European Business School in Oestrich-Winkel. Zuvor hatte Bhardwaj einen Bachelor of Technology am Indian Institute of Technology Bombay (IIT) erworben. Viele seiner Kommilitonen seien danach in die USA gegangen, sagt er. Ihn aber hätten das deutsche Wirtschaftswunder und hiesige Tugenden wie Qualitätsbesessenheit, Präzision und Pünktlichkeit fasziniert.
KI im Kampf gegen Krebs

Schockiert über die Erkrankung seines Freundes, suchte Bhardwaj Informationen über dessen Tumor, über Spezialisten und Therapien. Dabei habe er festgestellt, dass Antworten auf solche Fragen über das Internet kaum zu finden seien, sagt er. Das einschlägige Wissen wachse rasant, und es sei schwierig, aus der Überfülle relevante und hochwertige Daten herauszufiltern und Querverbindungen zwischen verstreuten Informationen zu erkennen. Als er darüber mit seinem Vater diskutiert habe, habe dieser geraten: «Don’t complain, act!» («Beklage dich nicht, tu etwas!»)

Und so machte sich Bhardwaj zusammen mit einigen Studienkollegen an die Arbeit. Nach vierjährigem Tüfteln gründeten sie Ende 2015 die Innoplexus AG. Deren Geschäftsidee liegt darin, mithilfe selbstentwickelter künstlicher Intelligenz (KI) und maschinellem Lernen Daten im Bereich Life-Sciences zu suchen, zu strukturieren, in einen logischen Zusammenhang zu bringen und über Lizenzen und Partnerschaften zahlenden Nutzern zur Verfügung zu stellen.

Inzwischen werden laut Bhardwaj bis zu zehn Milliarden Websites pro Tag «gecrawlt» und 97% des Internets erfasst. Ausgewertet würden zum Beispiel wissenschaftliche Publikationen, klinische Studien und Online-Patientenforen. Man habe dem System den Branchenjargon «beigebracht», so dass es relevante Informationen besser finde als eine allgemeine Suchmaschine wie Google.
Big Data für Big Pharma

Damit Forscher auch unveröffentlichte experimentelle Daten hochladen und lizenzieren können, bietet Innoplexus zudem eine sichere Blockchain-Plattform an. Gerade Erkenntnisse aus gescheiterten Experimenten könnten Pharmafirmen Doppelarbeit ersparen und den Aufwand für die Medikamentenentwicklung senken, doch blieben sie oft unpubliziert.

Zu den Kunden von Innoplexus gehören Pharmakonzerne wie Novartis, Pfizer oder Merck, die die Angebote zur rascheren und kostengünstigeren Medikamentenentwicklung nutzen. Eine weitere Anwendung seiner KI-Software sieht Bhardwaj in der Finanzbranche, da sie auf Basis öffentlicher Daten etwa die Erfolgschancen klinischer Studien einschätzen könne.

So unprätentiös Bhardwaj auftritt, so viele Superlative und ehrgeizige Ziele durchziehen seine Darstellung. Er spricht davon, wie er oft nur drei Stunden pro Nacht geschlafen habe und wie «hungrig» in Indien alle gewesen seien. Nun wolle er diese indische Energie zusammenbringen mit den deutschen Tugenden wie Präzision und Pünktlichkeit.

Innoplexus beschäftigt inzwischen rund 350 Mitarbeiter am Hauptsitz in Eschborn bei Frankfurt sowie je an einem Standort in Indien und den USA. Geschäftszahlen veröffentlicht die Firma noch nicht, Gewinne werden erst in zwei Jahren angestrebt. Nachdem die Gründer die Entwicklung ihrer Plattformen zunächst über einzelne Projekte finanziert hatten, haben sich Ende 2016 erste Finanzinvestoren beteiligt. 2018 begann die Kommerzialisierung. Eine derzeit laufende Finanzierungsrunde soll weitere 40 Mio. € an Kapital einbringen, bevor ab 2020 ein Börsengang vorbereitet werden soll.
Warum gerade Eschborn?

Bei allem Lob für Deutschland nimmt Bhardwaj auf die Frage nach Schwächen des Standorts kein Blatt vor den Mund. So klagt er über den kleinen Talentpool: An deutschen Schulen gelte Mathe als langweilig, jeder wolle Kunst und Ähnliches studieren, es brauche einen stärkeren Fokus auf die Mint-Fächer. Den zweiten Schwachpunkt sieht er in der Basisinfrastruktur. Es sei unglaublich, dass die Anbindung von Büros ans Hochgeschwindigkeitsinternet sechs Monate dauere. Drittens ortet der 36-jährige Unternehmer Risikoaversion in der Förderkultur: Gefördert würden etablierte Tech-Unternehmen statt Startups.

Warum aber sitzt Innoplexus in Eschborn statt im Start-up-Mekka Berlin? Von Eschborn aus sei er in zwanzig Minuten auf dem Flughafen Frankfurt mit guten Verbindungen nach New York und Indien, sagt Bhardwaj. Hilfreich sei zudem die Nähe zum Finanzplatz Frankfurt und zu Pharmaherstellern wie Merck. «Echte» Techs finde man eher in München und Frankfurt, während es in Berlin viel E-Commerce gebe. Dort müsse die Haare pink färben, wer dazugehören wolle, scherzt er. Und das ist seine Sache nicht, das muss er gar nicht erst aussprechen.";https://www.nzz.ch/wirtschaft/innoplexus-gruender-bhardwaj-will-ki-gegen-krebs-einsetzen-nzz-ld.1523063;NZZ;Rene Höltschi;;;
11.02.2017;Hedge-Funds – besser als ihr Ruf;"Hedge-Funds-Manager werden immer wieder von Anlegern, Politikern oder der Presse kritisiert. Für die Industrie als Ganzes ist das teilweise berechtigt, denn der Begriff «Hedge-Fund» ist kein geschützter Titel, und es gibt weltweit Tausende solcher Vermögensverwalter mit sehr unterschiedlichen Qualifikationen und erzielten Resultaten. Viele haben von den Anfängen der Finanzkrise vor etwas weniger als zehn Jahren bis heute schlechtere Renditen erzielt als traditionelle Vermögensverwalter. In dieser Periode hat der globale Hedge-Funds-Index von Hedge Fund Research zum Beispiel nur um rund 2% pro Jahr zugelegt. In der gleichen Zeit hat ein einfaches 60/40-Aktien/Obligationen-Portfolio um rund 8% jährlich avanciert. Grössere Freiheiten

Seit der Finanzkrise hat die Hedge-Funds-Industrie die erwarteten und bisweilen fast versprochenen Renditen nicht immer liefern können. Dabei darf man aber nicht vergessen, dass der einzelne Hedge-Funds-Manager bei der Vermögensverwaltung grössere Freiheiten geniesst als der relativ nahe an seinem Vergleichsindex operierende traditionelle Fondsmanager, und entsprechend sind die Renditen der verschiedenen Hedge-Funds wesentlich breiter gestreut als die der traditionellen Vermögensverwalter. Der Wahl des Hedge-Fund kommt somit grössere Bedeutung zu als der Wahl eines indexnahen Vermögensverwalters.

Trotzdem besteht ein Vertrauensproblem, und das Vertrauen der Anleger zurückzugewinnen, ist eigentlich einfach – nämlich indem die erwarteten Renditen in der Zukunft kontinuierlich erzielt werden. Das ist einfacher gesagt als getan. Aber die Hedge-Funds-Industrie unterliegt einem starken Wandel, und folgende vier Punkte sollen zeigen, dass die Besten der Industrie in Zukunft einen wichtigen Beitrag in einem gut geführten Portfolio leisten können.

    Erstens werden die seit der Finanzkrise historisch deutlich erhöhten Korrelationen zwischen verschiedenen Anlageklassen wie Aktien und Obligationen wieder auf ein Vorkrisenniveau zurückgehen. Die Korrelationen sind seit 2007 deutlich angestiegen, vor allem wegen der Geldschwemme der grossen Notenbanken. Höhere Korrelationen zwischen verschiedenen Anlagen und zwischen unterschiedlichen Anlageklassen sind nachteilig für Hedge-Funds, weil sie den Wert der Fundamentalanalyse reduzieren. Zum Beispiel konnten sich auch Unternehmen mit schlechten Fundamentaldaten mit billigem Geld refinanzieren und gingen nicht zugrunde, wie es die Fundamentalanalyse vielleicht hätte erwarten lassen. Die USA sind schon auf dem Weg der monetären Normalisierung beziehungsweise der Verteuerung des Geldes. In Europa wird inzwischen zumindest die Wirksamkeit der expansiven Notenbankpolitik von Teilen der Politik, der Wissenschaft und der Praxis bezweifelt.

    Zweitens können die Anlagechancen, welche tiefere Korrelationen bieten, in Zukunft wohl vermehrt nur unter grossem Aufwand ausgenützt werden. Wegen der hohen Effizienz der Märkte muss dafür extrem aufwendige und entsprechend teure Computertechnologie eingesetzt werden. So hat beispielsweise der Trendfolger Man AHL am Tag der US-Wahlen über 2,7 Milliarden Preisangaben zu den Märkten angefragt, gespeichert und verarbeitet. Neben der Möglichkeit, mit enorm grossen Datenmengen umzugehen, müssen auch neue Technologien erschlossen werden. Zum Beispiel eröffnet maschinelles Lernen neue Möglichkeiten, etwa um bisher unbekannte Muster in Aktienkursen zu erkennen und dadurch wahrscheinliche zukünftige Kurse vorherzusagen.

    Drittens sind Hedge-Funds häufig zu teuer. Die Gebühren sind im Verhältnis zu den Renditen zu hoch. Höhere Renditen sind ein Weg, dieses Missverhältnis zu beheben. Aber Renditen können nicht garantiert werden, und so müssen die Kosten ex ante verringert werden. Gebühren sorgen häufig für Konflikte zwischen Hedge-Funds-Managern und Investoren. Im besten Fall verdienen beide Seiten gut; im schlechtesten Fall fühlt sich der Anleger vom Manager übervorteilt. Auf jeden Fall darf der Manager nie vergessen, dass der Investor das volle Risiko trägt und dafür entsprechend entschädigt werden will.

    Zwar haben Hedge-Funds im Durchschnitt die Gebühren in den vergangenen Jahren deutlich reduziert. Aber wichtiger als die absolute Höhe der Gebühren ist deren Struktur. Die wichtigste Bezahlung für einen Hedge-Fund, im Gegensatz zum klassischen Asset-Management, muss die renditeabhängige Gebühr sein. Dies, um sicherzustellen, dass der Manager nur gut verdient, wenn es auch sein Kunde tut. Rund 70% der Erträge sollten beim Kunden bleiben. Ein «teurer» Hedge-Fund ist also nicht unbedingt ein Fonds, der hohe absolute Gebühren verlangt, sondern einer, welcher einen hohen Anteil der Rendite abschöpft.

    Viertens müssen Hedge-Funds den Kunden massgeschneiderte Dienstleistungen und nicht einfach nur Produkte verkaufen. Professionelle Investoren verlangen immer mehr nach einem Produkt, welches exakt zu ihrem Anlageportfolio passt und dieses effizient ergänzt und diversifiziert. So haben Trendfolgestrategien während der Kreditkrise von Juli 2007 bis Februar 2009 attraktive Renditen erzielt, in einem Umfeld, in dem globale Aktien um die 50% verloren. Die negative Korrelation zu den Börsen war für aktienlastige Portfolios Gold wert.

Die Zeit der Stars ist vorbei

Die Hedge-Funds-Industrie hat sich in den vergangenen Jahren stark verändert. Die Zeiten der bekannten Star-Manager, welche aufgrund eines Wissensvorsprungs und ihrer Analysen mit grossen Wetten Überrenditen erzielen konnten, sind vorbei. Aber viele Beispiele zeigen, dass ein Team-Effort von Anlagespezialisten mit der unabdingbaren technologischen Unterstützung nach wie vor ausgezeichnete Renditen erzielen kann.";https://www.nzz.ch/finanzen/fonds/alternative-anlagen-hedge-funds-ld.143100;NZZ;Luke Ellis;;;
23.01.2019;Algorithmus könnte Fehlalarme auf der Intensivstation aussortieren;"Forscher der ETH Zürich und der neurochirurgischen Intensivstation des Universitätsspitals Zürich haben in einer Machbarkeitsstudie einen Algorithmus getestet, der Fehlalarme erkennen kann. Dafür griffen sie auf einen umfangreichen Datensatz des Spitals zu Alarmen und den Vitalfunktionen von Patienten zurück, wie die ETH am Mittwoch mitteilte.

Üblicherweise funktionieren die verschiedenen Geräte zur Überwachung und Unterstützung der Vitalfunktionen, beispielsweise zur Kreislaufüberwachung und zur künstlichen Beatmung, unabhängig voneinander. So auch auf der neurochirurgischen Intensivstation des Unispitals Zürich. Jedes Messgerät schlägt auch unabhängig von den anderen Alarm, wenn ein bestimmter Schwellenwert unter- oder überschritten wird.

Der Datensatz enthielt die Messungen der Vitalfunktionen in hoher zeitlicher Auflösung. So konnten die Wissenschafter um Walter Karlen von der ETH diese Messungen kombinieren, synchronisieren und mit den ebenfalls gespeicherten Alarmen abgleichen. Diese Datengrundlage nutzten sie, um ihren lernenden Algorithmus auszubilden.
Training mit wenigen Vorlagen

Üblicherweise brauchen lernende Algorithmen eine grosse Menge bereits klassifizierter Daten als Vorlage, um eine Einteilung zu lernen. In diesem Fall also viele vom Personal als relevant oder nichtrelevant eingestufte Alarme. Auf einer Intensivstation hat das Personal aber kaum Zeit, die vielen ausgelösten Alarme zu bewerten und den Algorithmus so zu «trainieren».

Das Besondere des von Karlen und seinem Team entwickelten Systems ist jedoch, dass es nur wenige vom Personal beurteilte Alarme braucht, um die Klassifizierung zu lernen, wie es in der Pressemitteilung heisst. Die Forscher nutzten Vitalparameter und Alarme von 14 Patienten, die über mehrere Tage gesammelt wurden. Im Durchschnitt gab es täglich knapp 700 Mal Alarm pro Patient, also durchschnittlich alle zwei Minuten. Insgesamt kamen 14 000 Alarme zusammen.

Obwohl nur 1800 (also 13 Prozent) davon manuell als medizinisch relevant oder nichtrelevant beurteilt wurden, konnte der Algorithmus die restlichen Alarme anhand dieser Information ebenfalls klassifizieren. Erlaubten die Wissenschafter dem Algorithmus eine Fehlerquote von 5 Prozent, sortierte er 77 Prozent aller Fehlalarme aus. Auch mit nur 25 oder 50 «Vorlagen» lernte der Algorithmus bereits, den Grossteil der Fehlalarme auszusortieren.

Ihre Ergebnisse stellten die Forschenden im Rahmen einer internationalen Konferenz für maschinelles Lernen vor. Zwar bewertete das System in dieser Machbarkeitsstudie die Daten nur rückblickend, in einem weiteren Schritt wollen die Wissenschafter dessen Fähigkeiten jedoch auch in Sachen Vorhersage prüfen.";https://www.nzz.ch/wissenschaft/algorithmus-koennte-fehlalarme-auf-der-intensivstation-aussortieren-ld.1453994;NZZ;SDA;;;
05.03.2020;Du bist, was dein Gender ist: Die Sache mit der Geschlechtsidentität ist viel komplizierter als gedacht – oder viel einfacher?;"Gender heisst alles und nichts. Wenn der Direktor signalisieren will, dass er gegen Diskriminierung im Allgemeinen und die der Frauen im Besonderen ist, bringt er das Wort «Gendergleicheit» vor: Andere tun das vielleicht, aber er zieht die Männer sicher nicht den Frauen vor. Fortgeschrittene reden von «Gendermainstreaming», was noch besser klingt.

Der Bischof dagegen warnt vor dem «Genderismus». Und meint damit die Irrlehre, die gegen die Schöpfungsordnung der beiden fundamental differenten Geschlechter verstösst. Die LGBTQ-Aktivistin wiederum glaubt an «Genderidentitäten», die keinen Bezug zur Biologie des Körpers haben. Alle sollen sich ihr individuelles Gender frei wählen. Warum auch nicht? Vielleicht ist es ja tatsächlich dem Einzelnen aufgetragen, sich selber zu schaffen.

Der Genderbegriff ist aus den Wissenschaften in die sozialen Bewegungen und die Alltagssprache diffundiert. In den 1950er Jahren benutzten ihn Endokrinologen und Psychologen im Sinn von Geschlechterrolle. Sie empfahlen, Intersex-Babys – in der älteren, abwertenden Terminologie: «Zwitter» – zu operieren, bevor sie zwei Jahre alt sind, damit diese ihr Gender so problemlos wie möglich annehmen könnten. Gender musste also nicht identisch mit dem «wahren» biologischen Geschlecht sein. Entscheidend war die Einteilung der Kinder in männliche oder weibliche, Geschlechtseindeutigkeit lautete das Ziel.
Typisch männlich, typisch weiblich

1986 publizierte die US-Historikerin Joan W. Scott in der «American Historical Review» ihren bahnbrechenden programmatischen Aufsatz «Gender: Eine nützliche Kategorie der historischen Analyse». Scott hob den Begriff auf eine neue Stufe. Sie unterschied nicht nur zwischen «sex», dem biologischen Geschlecht, und «gender», dem sozialen Geschlecht. Noch vor der Philosophin Judith Butler machte sie mithilfe des Genderbegriffs sichtbar, wie Gesellschaften Geschlechter kategorisieren und zuteilen und davon ausgehend Menschen ungleich behandeln.

Wer «Gender» sagte, fragte fortan nach der Macht, also danach, wie etwa das Patriarchat die Unterdrückung der Frau legitimiert, indem es aus der Biologie Rückschlüsse auf ihre Eigenschaften zieht, oder warum und wie welches Verhalten als typisch weiblich oder männlich gilt und was mit «Abweichlern» passiert, mit Homo-, Trans- und Intersexuellen.

So gesehen, stellte der sozialwissenschaftliche Genderbegriff die Praxis infrage, dass Neugeborene, deren Geschlecht medizinisch nicht eindeutig zu bestimmen ist, notfalls durch operative Eingriffe einem Geschlecht zugeordnet werden, damit wieder die «heteronormative» Ordnung herrscht. Auf keinen Fall aber war der Begriff eine Bezeichnung für Frauen und Männer oder für männlich und weiblich – wie heute. Er griff das mit dem Geschlechtlichen verbundene Normale und Diskriminierende einer Gesellschaft an.
Ich sehe die Welt so, wie sie mir gefällt

Heute ist Gender so beliebig geworden, dass ihm Bedeutungslosigkeit droht. Wenn ein Wort fast alles heisst, besagt es bald einmal nichts mehr. Doch nun naht Hilfe von unerwarteter Seite, und zwar ausgerechnet von den von manchen Kulturwissenschaftern geschmähten Naturwissenschaften, die angeblich ein biologistisches Menschenbild haben. Das «high-ranking» Journal «Cerebral Cortex» präsentiert eine neue Studie, die sich in noch höheren Tönen anpreist. Ihre Methoden sind kaum nachvollziehbar, dafür umso mehr ihre Resultate. Sie sind eigentlich Weltanschauung: Ich sehe die Welt so, wie sie mir gefällt.

Zehn Forschende, mehrheitlich Psychiaterinnen und Neurobiologen der Hochschulen Aachen und McGill in Montreal, haben knapp hundert Explorandinnen und Exploranden aufgeboten. Die eine Hälfte nennen sie Cisgender. Diese Frauen und Männer wurden bei der Geburt biologisch als solche registriert, und sie sehen sich auch so.

Die andere Hälfte wird als Transgender bezeichnet: Diese Frauen und Männer wurden ebenfalls bei der Geburt biologisch als solche registriert, aber sie wollen, wie die Studie anmerkt, das Geschlecht wechseln. Sie fühlen sich sozusagen im falschen Körper. Gemäss psychiatrischer Diagnose weisen sie eine «Gender-Dysphorie» auf. Die Forschenden sind stolz darauf, diese Population, die sonst kaum berücksichtigt werde, mit einbezogen zu haben.
So bunt wie die Regenbogenfahne

Die Exploranden, die alle aus Aachen und Umgebung kommen, haben einerseits einen psychologischen Fragebogen ausgefüllt; er misst, wie Menschen sich geschlechtlich sehen, also bis zu welchem Grad sie sich als männlich oder weiblich identifizieren. Zudem sind von ihren Hirnen mittels Magnetresonanztomografie Bilder aufgenommen worden. All dies hat man schliesslich mit maschinellem Lernen gekoppelt.

Das Resultat der Studie: Das Gender ist neurobiologisch nachweisbar! Die datengetriebenen Maschinen, deren Berechnungen nicht durch Hypothesen oder Vorannahmen der Forschenden beeinträchtigt wurden, haben in den Hirnen nicht nur das weibliche und das männliche Gender gefunden, sondern auch zwei Transgender und dazu noch viel mehr, insgesamt neun «Gendervariationen». Jeder Proband kann einer dieser Ausprägungen zugeteilt werden. Jedes Hirn hat seine Genderidentität, auch das von Menschen, die ihr Geschlecht ändern wollen. Die Unterschiede auf den Hirnbildern, die das Paper fast so bunt wie die Regenbogenfahne illustrieren, sind nicht zu übersehen.

Die im Universum frei flottierenden Genderidentitäten, die sich auf «Kultur» berufen und das Joch der «Natur» abgeschüttelt haben, sie sind nun sowohl computerwissenschaftlich als auch hirnphysiologisch bewiesen. Der kulturalistisch-konstruktivistische Genderglaube mit seiner Gendermetaphysik wird nobilitiert von einer Naturwissenschaft, die auf den Genderbegriff der sozialen Bewegungen setzt. Solch ein Paradox schafft nur die göttliche Vorsehung, welche die Menschen vor unlösbare Rätsel stellt – oder intelligente Maschinen, die man in Ruhe rechnen lässt.
«Sex, not gender!»

In der Pressemitteilung der McGill University lässt ein an der Studie beteiligter Computerwissenschafter euphorisch verlauten, dass die Ergebnisse wichtige Konsequenzen für verbesserte Gleichheit, Diversität und Inklusion hätten. Die Forschung trage zum Aufbau einer Gesellschaft bei, in der sich Individuen, die sich zwischen den Positionen von männlich und weiblich identifizierten, nicht länger diskriminiert fühlen müssten, weil nun eben ihre Genderidentität wissenschaftlich im Hirn nachweisbar sei. Jetzt endlich wissen wir es: Transmänner und Transfrauen sind genauso normal wie die normalen Frauen und Männer!

Das war ja eigentlich schon immer klar. Aber wenn der wissenschaftliche Fortschritt die Welt verbessert, kann man ja nichts dagegen haben. Oder verbessert er sie gar nicht? Die britischen Feministinnen, die zurzeit mit der Transgender-Bewegung im Clinch liegen, von der sie auf Twitter als «Haters» bezeichnet werden, rufen: «Sex, not Gender!» Während der Bischof nickt, versteht der gendersensible Direktor nur noch Bahnhof: Was stimmt denn jetzt, woran soll er sich halten? An sein Gender: Es kennt die Wahrheit. Der Mann braucht dringend einen Hirnscan.";https://www.nzz.ch/feuilleton/gender-die-sache-ist-kompliziert-oder-viel-einfach-als-gedacht-ld.1543749;NZZ;Urs Hafner;;;
25.06.2018;Der weltweit schnellste Computer steht in den USA;"Die USA haben sich die Krone im Wettstreit um die schnellsten Superrechner der Welt nach mehr als fünf Jahren von China zurückerobert. Mit «Summit» von IBM steht wieder eine amerikanische Rechenanlage an der Spitze der Top-500-Liste der weltweit mächtigsten Supercomputer. Sie verdrängte mit einer Leistung von 122,3 Petaflops (Billiarden Rechenoperationen pro Sekunde) den chinesischen Superrechner «Sunway TaihuLight», der mit 93 Petaflops am Nationalen Supercomputing Center in Wuxi arbeitet und seit zwei Jahren die Liste angeführt hatte.

Vier der fünf schnellsten Rechner seien komplett neu gebaut oder substanziell aufgerüstet worden, teilten die Herausgeber der Liste mit. Auf den dritten Platz kam ebenfalls ein Neueinsteiger. «Sierra» ist am Lawrence Livermore National Laboratory in Kalifornien im Einsatz. Die chinesische Anlage «Tianhe-2A» rutschte vom zweiten auf den vierten Platz, obwohl ihre Leistung fast verdoppelt wurde. Neu unter den Top fünf ist «AI Bridging Cloud Infrastructure» vom japanischen Forschungsinstitut AIST.

Schnellster Superrechner aus Deutschland ist das modulare System «Juwels», das am Jülicher Supercomputing Centre (JSC) unter anderem für komplexe Berechnungen und Simulationen in der Hirnforschung eingesetzt wird. Die neue Anlage, die aus einem deutsch-französischen Projekt entstand, rangiert mit 6,2 Petaflops auf Platz 24. Vor einem Jahr noch war «Hazel Hen» aus dem Höchstleistungsrechenzentrum HLRS in Stuttgart die schnellste Anlage aus Deutschland. Mit 5,6 Petaflops fiel sie nun vom 19. auf den 28. Platz zurück. Insgesamt schaffte es Deutschland mit 21 Systemen in die Liste.

Ein deutlich schnellerer Supercomputer steht im Swiss National Supercomputing Centre (CSCS) in Lugano. Das «Piz Daint» genannte System bringt es auf 19,5 Petaflops - und damit auf den sechsten Platz in der aktuellen Top-500-Liste.
Intel-Prozessoren am häufigsten verbaut

Trotz ihrer Rückkehr an die Weltspitze brachten es die USA aktuell nur auf 124 Systeme in der Liste, ein neuer Tiefstand. Sechs Monate zuvor waren die USA noch mit 145 Systemen vertreten. China hielt seine Präsens mit 206 Systemen etwa auf gleichem Stand (202 im November 2017). An der Gesamtleistung (Performance) der Systeme haben die Anlagen aus den USA jedoch einen Anteil von 38,2 Prozent und übertreffen damit wieder jene aus China (29,1 Prozent).

Insgesamt 95 Prozent aller in der Liste aufgeführten Anlagen arbeiten laut Intel mit Prozessoren des kalifornischen Herstellers. Seit 2017 sei das ein Zuwachs von 2,4 Prozent. Mit 23,8 Prozent verwenden nach Angaben der Organisationen fast ein Viertel der Supercomputer Systeme des chinesischen Computer- und Hardware-Herstellers Lenovo. 122 der 500 leistungsstärksten Anlagen sind demnach Lenovo-Installationen.
Hilfsmittel für komplexe Simulationen

Mit Supercomputern lassen sich komplexe Simulationen etwa in der Klimaforschung, Medizin oder in der Genetik erstellen, neue Medikamente entwickeln oder Molekülbewegungen bestimmen. Sie werden aber auch in der Wirtschaft oder für die Berechnung von Verkehrsströmen genutzt. Bei vielen neuen Anwendungen etwa für künstliche Intelligenz oder maschinelles Lernen geht es jedoch nicht nur um die schnelle Abarbeitung einfacher Rechenschritte. Modulare Systeme sollen deshalb ermöglichen, dass für unterschiedliche Aufgaben jeweils auf die optimalen Module oder Prozessorbereiche zugegriffen werden kann.

Die Liste der Top 500 wird halbjährlich auf der Internationalen Supercomputing Conference veröffentlicht, die diesmal in Frankfurt stattfindet. Seit 25 Jahren misst sie die Leistung der Superrechner nach dem sogenannten Linpack-Benchmark. Viele Experten halten diesen Wert jedoch für nicht mehr zeitgemäss, da zum Beispiel die Effizienz von Berechnungen nicht erfasst wird.";https://www.nzz.ch/digital/der-weltweit-schnellste-computer-steht-in-den-usa-ld.1397862;NZZ;SDA;;;
10.09.2014;Sprechen Sie Chemisch?;"Die Software sei fertig, versichert Bartosz Grzybowski, auch wenn bis heute noch keine Publikation zu finden ist, die das Herzstück des Programms im Detail erläutert. Der Chemiker, der in Polen zu Hause ist, aber an der Northwestern University in Evanston, Illinois, lehrt, will sein Computerprogramm erst einmal kommerzialisieren. «Ich habe Hunderte Publikationen», sagt er am Telefon, «da kann diese eine ruhig warten. Was ich brauche, sind mehr Nullen auf meinem Konto!»
Die Sprache der Chemie

Kleine Einblicke gewährt Grzybowski natürlich trotzdem gern, zuletzt im Juli im Fachblatt «Angewandte Chemie» . Die Forscher beschreiben dort eine Art chemischen Wortschatz, den sie mit den Methoden der Computerlinguistik bestimmt haben. Algorithmen, die sonst in Texten nach strukturellen Ähnlichkeiten suchen, um Wortschatz, Satzbau und grammatikalische Regeln abzuleiten, liessen die Forscher Tausende organische Moleküle nach wiederkehrenden Mustern durchkämmen. Darin liegt, auch wenn es anders aussehen mag, weniger eine Spielerei als eine kühne Idee: Der Computer, der die Logik molekularer Strukturen wie eine Sprache versteht, könnte dem Chemiker im Labor die Denkarbeit abnehmen – zumindest teilweise.

In der Chemie gehe es letztlich um Mustererkennung, sagt Grzybowski – genau wie beim Erlernen einer Sprache. So wie es einem geübten Gehirn leichtfällt, den Fluss einer Rede in Sinnabschnitte, Sätze und Worte zu gliedern, lesen Chemiker aus der Struktur eines Moleküls wichtige Eigenschaften und mögliche Synthesewege ab. Dass das mehr ist als bloss eine Analogie, zeigt die neue Studie. Denn das Ergebnis – eine Liste von rund 40 000 wiederkehrenden molekularen Bruchstücken, gewissermassen das Vokabular der organischen Chemie – weist überraschende Parallelen zu den natürlichen Sprachen auf: So umfasst auch der aktive Wortschatz einer einzelnen Person (je nach Sprache und Bildungsstand) einige zehntausend Worte. Und wie die Worte eines Textes folgt die relative Häufigkeit der molekularen Bruchstücke dem Zipfschen Gesetz : Das zweithäufigste Wort eines Textes kommt jeweils halb so oft vor wie das häufigste, beim dritthäufigsten fällt dieser Wert auf ein Drittel und so weiter.

Das ist mehr als ein akademisches Glasperlenspiel. Denn auch punkto Anwendungen wollen die Wissenschafter um Grzybowski an die Computerlinguistik anknüpfen, die bekanntermassen nicht nur Suchmaschinen im Internet verbessert, sondern auch automatische Übersetzungen und Zusammenfassungen langer Texte ermöglicht. Übertragen auf die Chemie hiesse das, dass die Computerlinguistik helfen könnte, organische Moleküle zu erforschen und – möglicherweise – sogar deren Synthese zu automatisieren. Grzybowski will nichts weniger, als die Intuition des erfahrenen Chemikers durch eine intelligente Maschine ersetzen.

Diese Vision ist indes fast so alt wie die Idee der künstlichen Intelligenz. Schon vor mehr als 40 Jahren stand eine Art universeller Synthesemaschine für die Chemie ganz oben auf dem Wunschzettel der KI-Forscher – gleichauf mit dem Schachcomputer. Jedoch erwies sich die Chemie im Vergleich zum Schachspiel als die weitaus härtere Nuss: Während Deep Blue bereits 1996 Schachweltmeister Kasparow bezwang, ist das Rennen um einen maschinellen Chemiker, der seinem menschlichen Vorbild überlegen ist, noch lange nicht gewonnen. Synthesewege für organische Moleküle zu finden, ist ein weitgehend exklusives Feld für die menschliche Intuition und Kreativität geblieben; nur sporadisch setzt man auf maschinelle Hilfe.
Der Widerspenstigen Zähmung

Nicht, dass es niemand versucht hätte. Die theoretischen Grundlagen für die systematische Suche nach Synthesewegen für komplexe Moleküle reichen bis in die 1950er Jahre zurück. Damals entwickelte Elias J. Corey an der Harvard University in Cambridge, Massachusetts, die sogenannte Retrosynthese, einen Formalismus, bei dem das gewünschte Molekül gedanklich in Substanzen von einfacherer Struktur zerlegt wird . Schritt für Schritt lassen sich so auch komplexe Stoffe auf einfache, bekannte Chemikalien zurückführen – und eine Art «Kochrezept» für deren Synthese erschliessen. Noch in den 1960er Jahren goss Corey selbst seine Ideen in ein Computerprogramm; 1990 wurde er dafür mit dem Nobelpreis geehrt. Warum also gehen chemische Synthese und Computerwissenschaft nicht schon lange Hand in Hand?

Peter Murray-Rust verortet die Wurzel des Übels in den wissenschaftlichen Datenbanken. Es genüge nicht, nur Reaktionen von Molekül A zum Molekül B zu erfassen, sagt der Chemiker von der University of Cambridge in Grossbritannien. Ebenso wichtig seien die experimentellen Bedingungen: Temperatur, Druck, pH-Werte, die eingesetzten Lösungsmittel. Zweitens fehlten in der gesamten Fachliteratur die Negativresultate. Dabei wisse man längst, sagt Murray-Rust, dass höchstens 25 Prozent aller möglichen Reaktionen funktionieren. Beim Versuch, dem Computer die Semantik der Chemie nahe zu bringen, rächt sich, dass die Wissenschaft gern nur das publiziert, was gut geklappt hat.

Murray-Rust, der sich seit über zwei Jahrzehnten für die freie Verfügbarkeit und Nutzbarkeit wissenschaftlicher Daten insbesondere in der Chemie einsetzt, arbeitet deshalb an Algorithmen, die nicht nur die in hochkarätigen Magazinen publizierte Fachliteratur, sondern auch Laborbücher, Bachelor- und Doktorarbeiten nach Informationen zu gelungenen und missglückten Reaktionen durchforsten. Dort hofft er all das zu finden, was nicht in den feinen Journals steht – vorausgesetzt, jemand leistet zuvor die Übersetzungsarbeit. Denn für Moleküle gibt es eine ganze Reihe verschiedener, gleichwertiger Darstellungsformen, die das dreidimensionale Gebilde aus Atomen und ihren Bindungen wahlweise in eine Zeichenkette oder eine zweidimensionale Strukturformel übersetzt. Andrew Howlett, Student in Murray-Rusts Labor, arbeitet deshalb an einer Software, die Strukturformeln in PDF-Dokumenten aufspürt und in «chemical markup language» (CML) übersetzt, ein XML-basiertes Datensystem. Erst so liessen sich bereits erforschte Moleküle bis hin zu den jüngsten Publikationen maschinell vergleichen, nach Mustern durchsuchen und letztlich neue Synthesewege im Computer auffinden, sagt Murray-Rust. In seiner Vision von der universellen Synthesemaschine sammelt der Computer so viele Informationen aus der realen Welt wie möglich, um daraus Schlüsse zu ziehen. Watson von IBM, derzeit das Flaggschiff des kognitiven Computings, arbeitet auf ähnliche Weise an biologischen Fragestellungen.

Grzybowski allerdings glaubt an eine andere Strategie. Man könne nicht einfach ganz viele Daten nehmen und dann die gewünschte Information herausfischen, sagt er. Chematica, so der Name der Software, fütterte er deshalb mit den Informationen nur einer Datenbank, beschäftigte anschliessend aber fünf Jahre lang ein 20-köpfiges Team damit, die Daten zu bereinigen. «80 Prozent der publizierten Ergebnisse lassen sich sowieso nie reproduzieren», begründet Grzybowski seine Entscheidung. Datenqualität sei wichtiger als deren Quantität. Statt Datenberge zu durchsieben, liess der Forscher erfahrene Chemiker zusammen mit Spezialisten für maschinelles Lernen und künstliche Intelligenz ein Netzwerk der organischen Chemie erfassen. Moleküle stellen darin die Knotenpunkte dar, chemische Reaktionen verbinden die einzelnen Knoten zu einem Netzwerk. Zehntausende Regeln beschreiben genau, unter welchen Bedingungen eine bestimmte Reaktion abläuft. «Im Grunde steckt in Chematica der Erfahrungsschatz von zehn oder mehr talentierten Chemikern», fasst Grzybowski zusammen. Hinzu kommen die jüngst publizierten linguistischen Methoden, die laut dem Forscher helfen, gangbare von weniger günstigen Synthesewegen zu unterscheiden.
Routenplaner fürs Labor

Grzybowskis Arbeiten sorgten 2012 schon einmal für Aufsehen. Damals hatte seine Forschungsgruppe ihre Software wie einen Routenplaner benutzt, der innerhalb des Netzwerks den schnellsten, billigsten oder am wenigsten giftigen Weg zu einem gewünschten Molekül berechnet. Im gleichen Heft des Fachblatts «Angewandte Chemie» stellten sie stark vereinfachte Synthesewege für Substanzen vor, die als Asthmamedikamente im Gespräch sind . Chematica, so der Tenor, könnte die Prozesse in der chemischen Industrie massgeblich verbessern – und bald auch Synthesewege zu gänzlich neuen Molekülen aufzeigen. Das allerdings ist bis anhin noch nicht bewiesen. Grzybowski gibt freimütig zu, dass viele seiner Kollegen daran nicht glauben. Auch Murray-Rust ist skeptisch: Das Problem der computergestützten Synthese könne man grundsätzlich von zwei Seiten angehen, sagt er: «Entweder man schlägt so viele Informationen nach wie möglich und lässt den Computer daraus lernen, oder man abstrahiert und fasst das generalisierte Wissen in Algorithmen.» Problematisch an der letzteren Vorgehensweise sei, dass man nicht ohne Annahmen und Verallgemeinerungen auskomme. Das alles habe man längst versucht – bis hin zu detaillierten quantenchemischen Rechnungen. Deshalb sei er überzeugt, dass es ohne das Durchforsten der aktuellen chemischen Literatur nicht gehe, sagt Murray-Rust.

Chemiker dürfte immerhin freuen, dass laut Grzybowskis Studie lang bekannte «Worte» im Vokabular der Chemie auftauchen – die sogenannten funktionellen Gruppen, die Eigenschaften und Reaktionsverhalten eines Moleküls massgeblich mitbestimmen. Ganz so neu und fremd ist die Sprache der Chemie also keineswegs. Ein bisschen «Chemisch» haben wir alle schon in der Schule gelernt.";https://www.nzz.ch/wissenschaft/physik/sprechen-sie-chemisch-1.18380325;NZZ;Helga Rietz;;;
10.06.2016;«Bildung ist Arbeit an mir selber»;"Professor Hofmann, wie haben Sie Ihre Studienzeit erlebt?

Ich habe gerne und viel studiert: Informatik und Philosophie. Ich musste mir mein Studium allerdings selbst finanzieren; ich arbeitete auch nachts und an Wochenenden.

Waren Sie ein schneller Student?

Nicht unbedingt, aber das war nie mein Ziel. Ich habe neun Jahre studiert und mit 30 promoviert.

Wollten Sie immer Informatik ­studieren?

Nein, ursprünglich Physik. Mir hat aber die gesellschaftliche Brisanz, etwa der Nuklearphysik, zu denken gegeben. Ich war damals in der Friedensbewegung aktiv. Ironischerweise stehe ich heute gerade mit Informatik und Big Data im Zentrum von grossen gesellschaftlichen Veränderungen, die ebenfalls bedenkliche Nebenwirkungen haben.

Wie war das Informatikstudium damals, im Gegensatz zu heute?

An der Universität Bonn hatten wir 6 Professuren und nur ein begrenztes Lehrangebot. An der ETH sind es heute 30 Lehrstühle, und das Angebot ist umfangreich. Die Informatik steckte vor dreissig Jahren noch in den Kinderschuhen.

War es Ihr Traum, akademische Karriere zu machen?

Ich wollte schon als Kind Professor werden, obwohl es in meiner Familie keine Bildungstradition gab. Dadurch war ich aber auch nie besonderem Erfolgsdruck ausgesetzt. Ich habe zwischendurch zwei Unternehmen gegründet und war fast acht Jahre lang bei Google. Jetzt bin ich sehr froh, wieder an einer Hochschule tätig zu sein.

Hat Ihr Studium Sie auf das Leben vorbereitet?

Absolut. Vor allem auch die Philosophie, die mir viel Orientierung im Leben gegeben hat. Insbesondere die Sprachphilosophie ist auch für meine jetzige Forschung relevant, etwa in Bezug auf künstliche Intelligenz. Ich profitiere davon, dass ich beide Argumentationswelten gut kenne.

Was raten Sie den heutigen Studierenden?

In der Informatik wird es immer spannende Berufsmöglichkeiten geben. Umso wichtiger ist es da, sich zu fragen, was einem wirklich wichtig ist und was einen antreibt: Haben Sie den Mut, Ihren eigenen Weg zu gehen! Auch lernt man nicht alles an der Universität. Wenn ich einen Lebenslauf anschaue, suche ich nach Selbstinitiative, Kreativität und Persönlichkeit.

Was ist Bildung für Sie?

Bildung ist Arbeit an mir selbst und daher ein Selbstzweck. Patrick Frei, hättest du besser eine Lehre gemacht?

Ich habe eine Lehre zum Systemtechniker gemacht! Das Gymnasium habe ich abgebrochen, denn ich war einfach zu jung, und das starre Schulsystem lag mir nicht.

Wie hast du den Weg an die ETH gefunden?

Ich war enttäuscht vom Beruf. ­Arbeit mit Computern ist nicht ­automatisch Informatik. Informatik ist vielmehr Planung, Strategie, ­Mathematik. Ein Verwandter ­meiner ­Mutter studierte Informatik an der ETH, und ich dachte: Informatiker, das klingt cool. Nach einem Jahr reisen und Party machen und einem Jahr lernen meldete ich mich für die Aufnahmeprüfung an der ETH in Zürich an.

Welche beruflichen Träume hast du noch?

Ich wechsle meine Meinung fast so oft wie meine Unterwäsche. Ich wollte in den Consulting-Bereich, aber das war mir irgendwie zu versnobt. An der Corporate-Welt gefielen mir die fixen Strukturen nicht. Nun will ich ein Startup-Unternehmen gründen. Das ist kein sicherer Hafen, alles ist unklar und spannend.

Wie realistisch ist dein gegenwärtiger Traum?

Ich glaube, er ist realistisch. Aber diese Pläne brauchen Zeit. Ich werde wohl Teilzeit in einem Unternehmen arbeiten und nebenbei rumtüfteln. Die Chancen stehen gut, dass ich einen entsprechenden Job finde, da wir Informatiker sehr begehrt sind auf dem Arbeitsmarkt.

Was lernst du hier, was du in deiner Lehre nicht gelernt hast?

Wie ich grosse Mengen an Stoff richtig priorisiere. Ich habe mich früher immer genervt, wenn jemand triviale Dinge nicht verstand. Heute, in diesem ETH-Rahmen, gehöre ich zu den nicht so hellen Köpfen. Das hat mir eine neue Sicht aufs Leben gegeben.

Was sagen deine Eltern zu deiner Wahl?

Einmal sagen sie: Super, dass du endlich deinem Wunsch folgst. Ein anderes Mal fragen sie, wann ich endlich aufhören würde, ihr Geld aus dem Fenster zu schmeissen. Je nach Laune.

Sollten alle studieren können?

Wer die Anforderungen des Studiums erfüllen kann, sollte auch die Möglichkeit dazu haben. Aber studieren müssen ist Blödsinn. Für viele Berufe braucht es Praxiserfahrung, da nützen Theorien wenig.";https://www.nzz.ch/karriere/berufseinstieg/ist-das-dein-traum-13-bildung-ist-arbeit-an-mir-selber-ld.131192;NZZ;Anna Miller;;;
08.03.2019;Die Zürcher Hochschulen lancieren gemeinsam eine 300 Millionen Franken teure Digitalisierungsinitiative;"Zürich und seine Hochschulen rüsten sich für den absehbaren Digitalisierungswettbewerb. Bereits im Herbst gab etwa die Universität Zürich bekannt, 18 neue Professuren im Bereich der Digitalisierung zu schaffen. Nun sollen die Anstrengungen aller Zürcher Hochschulen intensiviert und gebündelt werden.

Erstmals lancieren die Universität Zürich (UZH), die Zürcher Hochschule für Angewandte Wissenschaften (ZHAW), die Zürcher Hochschule der Künste (ZHdK) sowie die Pädagogische Hochschule Zürich (PHZH) eine gemeinsame Digitalisierungsinitiative, wie der Zürcher Regierungsrat am Freitag bekanntgegeben hat.
Interdisziplinärer Ansatz

Die Digitalisierung stelle hohe Anforderungen an die wirtschaftliche, technologische und soziale Innovationskraft des Kantons Zürich, heisst es in der entsprechenden Mitteilung. Die Initiative solle dem Kanton eine «führende Rolle» in diesem Bereich sichern. Man wolle die Wettbewerbsfähigkeit von Zürich als Forschungs- und Entwicklungsstandort national und international stärken.

«Wir wollen den digitalen Wandel aktiv mitgestalten», sagte die Zürcher Bildungsdirektorin Silvia Steiner (cvp.) vor den Medien. Die Initiative besteht aus drei Teilen: einem Forschungscluster, einem Innovationsprogramm sowie der Bildungsförderung. Dadurch sollen Synergien und neue Impulse zwischen den Hochschulen entstehen.

Konkret soll die Initiative Brückenprofessuren mit interdisziplinären Schwerpunkten ermöglichen, die Zusammenarbeit mit dem privaten und dem öffentlichen Sektor unterstützen und digitale Lerninhalte sowie innovative Formen der Berufsbildung entwickeln helfen. Damit soll sie letztlich Wirtschaft und Gesellschaft dabei unterstützen, die Chancen der Digitalisierung wahrzunehmen.
Doppelt so viele Universitätsprofessuren

Welche Vorhaben konkret geplant oder ausgebaut werden, erläuterten darauf die Rektoren der Zürcher Hochschulen. Der UZH verleiht die Initiative laut Michael Hengartner einen «zweiten Schub». Unter anderem sei eine Verdopplung der geplanten Professuren auf 36 möglich, wovon 20 befristete Assistenzprofessuren wären. Gestärkt würden die Bereiche Big Data, digitale Gesundheit sowie künstliche Intelligenz und maschinelles Lernen. Brückenprofessuren sind etwa mit der ZHAW in der KMU-Datenwirtschaft oder mit der ZHdK für Virtualität als Methode in der Forschung vorgesehen. Schliesslich soll es ein hochschulübergreifendes Doktoratsprogramm geben.

Der Rektor der ZHAW, Jean-Marc Piveteau, spricht von einer «Hebelwirkung», welche durch die Zusammenarbeit mit den anderen Hochschulen entstehe. Die ZHAW möchte mit den zusätzlichen Mitteln zum Beispiel ihr Data Science Lab ausbauen und Projekte für Open Labs oder Spin-offs unterstützen sowie Fellowship-Programme aufbauen. Ausserdem will sie mithelfen, digitale Lehr- und Lerninhalte zu entwickeln. Dadurch soll nicht zuletzt eine Brücke zwischen der Berufsmaturität und einem Fachhochschulstudium geschlagen werden, womit die Schule bereits begonnen hat.
Personalisiertes Lernen

Die ZHdK will sich im geplanten Forschungscluster mit zwei Schwerpunkten einbringen: Das sind zum einen Creative Economies, mit denen die wirtschaftlichen Entwicklungen im Kreativbereich untersucht werden. Die Digitalisierung schaffe neue Distributionskanäle, die andere Geschäftsmodelle erforderten, erläuterte Rektor Thomas D. Meier. Zum anderen möchte man die sogenannten Immersive Arts stärken. Beispiele für diesbezügliche Pilotprojekte sind weiterentwickelte 3-D-Modelle oder Motion-Capture-Techniken für Filmaufnahmen.

Die Pädagogische Hochschule widmet sich den neuen Formen des digitalen Lehrens und Lernens. Als Beispiel nennt Rektor Heinz Rhyn adaptive Lernfördersysteme, die sich dem Bedürfnis und dem Kenntnisstand der Lernenden anpassen und einen individualisierten Unterricht ermöglichen. Daneben möchte man die digitalen Fähigkeiten («digital skills») von Lehrkräften und Dozierenden etwa mittels digitaler Selbstlernangebote stärken.
Kreditantrag von 108,3 Millionen Franken

Die Hochschulen investierten bereits heute namhafte Beträge, um sich für die Herausforderungen der Digitalisierung zu rüsten, sagte Bildungsdirektorin Steiner. Trotzdem seien sie auf die zusätzliche Finanzierung durch die öffentliche Hand angewiesen. Es brauche diesen «einmaligen finanziellen Sondereffort», um andere Bereiche der Hochschulen nicht zu schwächen.

«Wir kommen nun langsam an die Grenzen diesbezüglich, was wir allein stemmen können», bestätigte Universitätsrektor Hengartner. «Die Initiative kommt genau zum richtigen Zeitpunkt», pflichtete der Rektor der Pädagogischen Hochschule, Heinz Rhyn, bei. Die Hochschulen betonten, dass die Zusammenarbeit – auch mit der eidgenössisch getragenen ETH – aber bereits ein Fakt sei.

Zur Umsetzung der Digitalisierungsinitiative wird gesamthaft ein Kostenrahmen von 300 Millionen Franken für die Dauer von zehn Jahren veranschlagt. 191,7 Millionen Franken tragen die Hochschulen selbst, der Kanton soll zusätzlich 108,3 Millionen Franken beisteuern. Diesen Betrag beantragt der Regierungsrat dem Kantonsrat über einen entsprechenden Rahmenkredit. Sofern das Parlament diesem zustimmt, soll die Initiative bereits im nächsten Jahr starten.";https://www.nzz.ch/zuerich/digitalisierung-zuerich-lanciert-initiative-fuer-hochschulen-ld.1465575;NZZ;Lena Schenkel;;;
21.12.2018;Kann künstliche Intelligenz moralisch denken?;"Die Entwicklung künstlicher Intelligenz (KI) ist einer der wichtigsten Trends der Welt – und sie wirft eine Reihe von interessanten ethischen Fragen auf. Die entsprechenden Debatten werden in der breiteren Öffentlichkeit jedoch häufig engstirnig und schlecht informiert geführt. Ein Beispiel dafür ist die Versteifung auf moralische Dilemmata – man denke etwa an das gerne besprochene, aber sehr unwahrscheinliche Szenario, in dem ein autonomes Fahrzeug entweder plötzlich auf die Fahrbahn springende Kinder erwischt oder mit den Insassen in ein parkiertes Auto kracht.

Zwar mag die Frage, wer in einer unvermeidbaren Unfallsituation wie viel Verantwortung trägt – die Nutzer des Autos, der Programmierer der KI, die KI-Firma? –, durchaus interessant sein, doch viel wichtiger ist in diesem Zusammenhang der Hinweis auf einen eklatanten Mangel: Wir Menschen verfügen heutzutage einerseits über sehr strikte Regeln für Medikamententests oder Flugreisen, haben aber andererseits keine oder kaum Regeln für selbstfahrende Autos.

Das ist erstaunlich, denn klar müsste zumindest so viel sein: Das autonome Fahrzeug selber trägt keine (Mit-)Verantwortung für den Unfall. Dies zunächst aus einem einfachen Grund: KI ist heutzutage ziemlich dumm, und wir wiederum sind schlecht beraten, wenn wir ihre Fähigkeiten überschätzen, sie mit moralischen Akteuren verwechseln oder ihr Autonomie einräumen.
Simulieren statt wissen

In einer detaillierteren Betrachtung sollten wir zwischen mittelfristig und längerfristig realisierbarer KI unterscheiden. Auf mittlere Sicht sind KI nicht als «intelligent», sondern als Tool beziehungsweise in den meisten Fällen als Datenauswertungsmaschinen zu begreifen. Beim maschinellen Lernen werden Trainingsdaten (etwa von Autos) sowie Metadaten («Dieses Bild zeigt ein Auto») in ein bestimmtes Computerprogramm eingegeben. Daraufhin versucht das Programm durch stetiges Anpassen in den Daten Muster und Gesetzmässigkeiten zu erkennen – um schliesslich auch bei Bildern, die es noch nicht «gesehen» hat, mit einer statistischen Wahrscheinlichkeit vorauszusagen, dass es sich um ein Auto handelt. Wie sollte eine solche KI einen moralischen Status besitzen?

In der Tradition Immanuel Kants ist der moralische Status oder die Würde an der Autonomie festzumachen, die Menschen eigen ist. Demnach ist es die menschliche Fähigkeit, (Handlungs-)Gründe abzuwägen und Entscheidungen zu treffen, die den besten dieser Gründe folgen, die Menschen zu autonomen und willensfreien Akteuren macht. Demgegenüber handeln unsere heutigen KI nicht nach eigenen Gründen, weil sie keine solchen besitzen.

Über eine allfällige Simulation hinaus haben sie kein moralisches Empfinden, keine Intentionen (das Gerichtetsein des Geistes auf etwas), und sie können diese Dinge auch keinen Personen zuschreiben. Sie «wissen» nichts, weil sie kein Wissen besitzen können, und selbst wenn dem anders wäre, wüssten sie nicht, wie es sich anfühlt, etwa wohlwollend zu agieren. Ohne diese Fertigkeiten aber ist eine angemessene moralische Praxis nicht möglich. Trotz vielleicht perfekter Simulation von Denken oder Empathie liegt dem «Agieren» des Computers kein eigenes verständiges Erfassen, kein Problembewusstsein, keine Einsicht zugrunde.
Fakten sind nicht alles

Folgt daraus schon, dass es unmöglich ist, eine moralische Maschine zu bauen? Wie könnte eine KI künftig doch moralischen Status erlangen? Einerseits könnte man sofort einen Perspektivwechsel vornehmen und sich von Kant verabschieden. Sogenannte Behavioristen würden behaupten, dass das Verhalten von Menschen (und Maschinen) ohne Introspektion oder Einfühlung zu untersuchen und zu erklären ist. Nach dieser Lesart, die zum Beispiel Ludwig Wittgenstein und Alan Turing verbindet, heisst etwa wohlwollend sein nichts anderes, als ein bestimmtes Verhalten zu zeigen. Dennoch bleibt das Problem bestehen, dass funktionale Äquivalenz zwischen Mensch und Maschine (die mit heutiger KI noch nicht zu erreichen ist) aufseiten Letzterer noch kein Verständnis gewährleistet. Und Verantwortungsübernahme oder -zuschreibung ohne Verständnisgabe bleibt eine Chimäre.

Andererseits sollten wir anerkennen, dass Begriffe wie «Verantwortung», «Schuld» oder «Bedauern» die Währung eines kausal denkenden Geistes darstellen. Um sie zu verstehen, müssen wir beziehungsweise der Computer in der Lage sein, das, was passiert ist, mit dem zu vergleichen, was unter einer alternativen Hypothese passiert wäre. Somit würde die vielleicht erste Anforderung an eine moralische Maschine in der Fähigkeit bestehen, über ihr eigenes Handeln nachzudenken. Sie müsste also kausale beziehungsweise kontrafaktische Analysen vornehmen können: Was wäre der Fall gewesen, wenn ich anders gehandelt und das Auto in das parkierte statt in das fahrende gelenkt hätte?

Zwar ist KI auf mittlere Sicht in der Lage, Regelmässigkeiten zu erkennen und anhand von Datenmustern zu beobachten, wie sich kausale Fragen der einfachsten Art entwickeln: Was passiert, wenn ein Auto mit 50 Kilometern pro Stunde in ein parkiertes kracht? Doch eine zur Moral befähigte KI müsste durch kausale Modelle bereichert werden und insbesondere das Kontrafaktische erfassen können – das aber allein in der Vorstellungskraft und gerade nicht in Daten liegt. Denn Daten sind per Definition: Fakten.

Bis dahin müssen wir uns zumindest in den philosophischen Diskussionen damit abfinden, dass die KI im autonomen Fahrzeug nicht zur Rechenschaft für etwaige Unfälle gezogen werden kann. Ganz praktisch stehen wir aber schon jetzt vor der nicht zuletzt juristischen Schwierigkeit, wie eine Person für eine Maschine geradestehen kann, wenn diese eigene Verhaltensweisen erlernt. Frei nach Stanley Kubrick ist die Hölle damit vielleicht ein Ort, an dem der Mensch konsequenzialistisch programmierten, aber denkunfähigen Computern die Macht gegeben hat, über Leben und Tod zu entscheiden.";https://www.nzz.ch/feuilleton/kuenstliche-intelligenz-hat-zurzeit-weder-moral-noch-autonomie-ld.1443718;NZZ;Christian Hugo-Hoffmann;;;
10.10.2017;Fünf Fakten zu Google in Zürich;"Seit wann ist Google in Zürich?

Das erste Google-Büro in der Limmatstadt, damals noch mit zwei Mitarbeitern, war erst 2004 entstanden. 2008 wechselte man mit rund 300 Mitarbeitern vom Limmatquai auf das Hürlimann-Areal im Kreis 2. Rasch wurde das Zürcher Team weiter vergrössert und mit neuen Aufgaben betraut: Die Zürcher Googler – auch Zoogler genannt – arbeiteten unter anderem an Youtube, Gmail sowie am eigentlichen Herzstück des Unternehmens: dem Suchalgorithmus. Inzwischen ist Zürich der grösste Entwicklungsstandort von Google ausserhalb der Vereinigten Staaten. Der Konzern gehört mit seinen mehr als 2200 Mitarbeitern zu den grössten Arbeitgebern in der Limmatstadt. Google hat dabei selbst eine Entwicklung durchlaufen: War das Unternehmen in Zürich in der Startphase noch etwas «isoliert», bringt es sich heute zum Beispiel stark in das Startup-Ökosystem ein. Google baut weiter aus – wo und wie?

Bis 2021 will Google rund 5000 Personen in Zürich beschäftigen. Letztes Jahr hat der Softwarekonzern dafür einen zweiten Standort an der Europaallee eröffnet, in unmittelbarer Nähe zum Hauptbahnhof. Der IT-Riese will insbesondere auch seine Entwicklungstätigkeit in Zürich ausbauen. Seit 2016 kümmert sich beispielsweise das Forschungsteam Google Research Europe um maschinelles Lernen und versucht damit, die Google-eigenen Dienstleistungen zu verbessern. Warum hat sich Google überhaupt für Zürich entschieden?

Der Softwaregigant hat nebst Zürich viele weitere Standorte auf der ganzen Welt, einige davon auch in Europa. In der Limmatstadt hat Google jedoch meist sehr gute Rahmenbedingungen vorgefunden, so dass der Standort bei Ausbauschritten immer wieder berücksichtigt wurde. Einerseits verfügt Zürich über eine sehr gute Infrastruktur. Das betrifft den Verkehr, aber beispielsweise auch die Stromversorgung. Weiter sind in Zürich versierte Spezialisten verfügbar: Einerseits bildet die Schweiz diese selbst aus, andererseits war es mit der Personenfreizügigkeit bisher relativ einfach, Europäer nach Zürich zu ziehen. Die hohe Lebensqualität in Zürich tat ihr Übriges. Bringt Googles Wachstum auch Nachteile?

Die starke Präsenz des IT-Giganten macht sich auf dem Arbeitsmarkt bemerkbar: Google «saugt» hochqualifizierte Arbeitnehmer auf und beansprucht einen Teil der raren Zürcher Drittstaatenkontingente. Andere Arbeitgeber, die insbesondere IT-Fachkräfte suchen, sind herausgefordert. Allerdings gilt es zu sagen: Erfolgreiche, als «sexy» geltende Unternehmen hatten auf dem Arbeitsmarkt aus naheliegenden Gründen schon immer eine grössere Zugkraft als andere.  Und welche Vorteile verschafft der IT-Konzern Zürich?

In einem engeren Sinne profitiert Zürich von den hochbezahlten Fachkräften bei Google, weil diese hier Einkommenssteuern zahlen und durch ihren Konsum andere Branchen ankurbeln. Langfristig könnten auch auf dem Zürcher Arbeitsmarkt die positiven Effekte überwiegen: Der Konzern war massgeblich daran beteiligt, den hiesigen Technologie-Cluster auf der Weltkarte zu verorten. Er macht die Stadt dadurch attraktiver für Forscher und Entwickler aus aller Welt. Zum Beispiel kooperiert Google heute eng mit der ETH und weiteren Hochschulen. Ehemalige Mitarbeiter sind zudem in der Startup-Szene oder unter Risikokapitalgebern gut vertreten und vernetzt. Sie alle tragen dazu bei, die Marke «Zürich» zu stärken.";https://www.nzz.ch/zuerich/fuenf-fragen-zu-google-in-zuerich-ld.1320920;NZZ;Andre Müller;;;
10.07.2019;Instagram kämpft gegen Hass und Mobbing;"Das Fotonetzwerk Instagram hat neue Funktionen vorgestellt, die Nutzer besser vor Hasskommentaren und Mobbing schützen sollen. Insbesondere setzt die Facebook-Tochter mithilfe von künstlicher Intelligenz (KI) nun auf Warnhinweise, mit denen Absender noch vor dem Posten auf potenziell verletzende Kommentare hingewiesen werden sollen.
Ein Hassfilter für mehr Liebe

Zum anderen werden Nutzer künftig die Möglichkeit haben, die Sichtbarkeit von Kommentaren bestimmter Personen einzuschränken, von denen sie sich belästigt fühlen.

Mit dem Einblenden von Warnhinweisen bei Hasskommentaren («Bist du sicher, dass du das posten möchtest?») hat die Fotoplattform bereits begonnen. Dazu werden Kommentare in Echtzeit auf verletzende Inhalte hin analysiert. Noch vor dem Posten sollen Nutzer damit angeregt werden, ihren Kommentar nicht abzusenden und wieder zu löschen.

Bei den ersten Tests der KI-Funktion soll sich laut Instagram bereits gezeigt haben, dass manche Personen Beiträge zurückzögen und weniger verletzende verfassten.
Mobbende Personen isolieren

Mit der zweiten Neuerung, jener zur Einschränkung von Personen («Restrict»), soll Nutzern ein Mittel an die Hand gegeben werden, sich besser vor Mobbing zu schützen – ohne dass der Absender von Hasskommentaren blockiert, «entfolgt» oder gemeldet werden muss. Insbesondere jüngere Nutzer scheuten diesen Schritt aus Angst, die Situation dadurch zu verschlimmern oder nicht über die Aktivitäten des Mobbers auf dem Netzwerk im Bilde sein zu können.

Aus diesem Grund sollen Kommentare bestimmter Personen in Zukunft so eingeschränkt werden können, dass diese unter den Beiträgen nur für den Absender und den Betroffenen selbst sichtbar sind – es sei denn, der Nutzer genehmigt aktiv einen solchen Kommentar.

Für mehr Schutz sorgen soll auch, dass eingeschränkten Personen künftig verborgen bleibt, ob gesendete Direktnachrichten gelesen wurden oder der sich schützende Nutzer gerade auf der Plattform aktiv ist. Mit ersten Tests des neuen «Restrict»-Tools will Instagram in Kürze beginnen.
Algorithmen sollen helfen

Laut eigenen Angaben nutzt die Social-Media-Firma bereits seit Jahren künstliche Intelligenz, um Mobbing und andere Arten verletzender Inhalte in Kommentaren, Fotos und Videos zu erkennen. Diese Technologie ist besonders wichtig für Jugendliche, da diese Mobbing seltener melden, obwohl sie am häufigsten darunter zu leiden haben, wie der Instagram-Chef Adam Mosser im Blog-Beitrag zur Ankündigung erklärt.

Bereits im vergangenen Jahr hat Instagram seinen Hassfilter zum automatischen Ausblenden anstössiger Kommentare weiter ausgebaut, um auch Angriffe auf das Erscheinungsbild und den Charakter einer Person herauszufiltern.

Zudem wurden die Filtermechanismen zum möglichst schnellen Erkennen von Mobbing-Kommentaren auf Live-Videos ausgeweitet. Durch maschinelles Lernen sollen Anfeindungen in Fotos und Bildunterschriften proaktiv erkannt und der Plattform direkt zur Überprüfung gemeldet werden.";https://www.nzz.ch/digital/instagram-kaempft-gegen-hass-und-mobbing-ld.1495117;NZZ;Jochen Siegle;;;
24.03.2016;Microsoft lässt Bot nicht nur auf Twitter-Nutzer los;"Microsoft Research testet nun öffentlich den Chatbot Tay. Er ist unter anderem auf Twitter, Instagram, Facebook und Groupme im Einsatz. Um mit Nutzern kommunizieren zu können, wurden einerseits öffentlich verfügbare Datensätze mittels Bing ausgewertet, andererseits vom Forscherteam erstellte, die unter anderem auf Gags von Improvisationskomikern beruhen. Denn Tay soll simple Witze zum Besten geben und verstehen können.

Wie Microsofts Forschungsabteilung mitteilte, soll Tay nicht nur Projekte zur künstlichen Intelligenz vorantreiben, sondern auch Aufschluss über Kommunikationsgewohnheiten 18- bis 24-jähriger Amerikaner liefern. Dementsprechend soll Tay auch mit Slangausdrücken keine Probleme haben. Klippen umschifft der Chatbot nicht immer so elegant: Wenn es zu heiklen Aussagen wie dieser kommt, greifen in der Regel alsbald Microsoft-Mitarbeiter ein, um sie zu löschen:
Dieser Tweet wurde mittlerweile entfernt. (Bild: NZZ)  Wie zu erwarten war, dauerte es auch nicht lange, bis die ersten Nutzer Tay auf Twitter Penis-Bilder anboten. Die KI reagierte etwa, indem sie erwähnte, dass sie nach zehntausenden Tweets ganz schön müde sei. Zudem muss man damit rechnen von ihr nach solchen Beiträgen blockiert zu werden. Microsoft liess verlauten, Der KI-Chatbot Tay sei ein maschinelles Lernprojekt. Während es lernt, seien einige Antworten unangebracht und spiegelten die Art der Interaktion mit den Nutzern wider. Man nehme nun entsprechende Anpassungen vor.

Noch ein Problem: Es kann in jedem Fall sein, dass Kontoname, Geschlecht, Postleitzahl, Lieblingsessen und Beziehungsstatus gespeichert werden, wenn man Tay anschreibt. Microsoft behält sich vor, diese Daten anonymisiert ein Jahr lang zu speichern. Möchte man das nicht, muss man dies dem Unternehmen über ein Kontaktformular mitteilen, klassisches Opt-out also. Vertrauensfördernder wäre auch in diesem Fall Opt-in: Nutzer sollten gefragt werden, ob ihre Daten gespeichert werden dürfen.

Datenschutzbedenken wie diese scheinen allerdings kaum User zu haben. Auf Twitter hat der Bot schon 24'000 Follower. 96'000 Tweets hat er bereits abgesetzt.

Randnotiz: Mit Xiaoice testet seit Microsoft seit Sommer 2015 in China öffentlich eine künstliche Intelligenz, die auf Bing basiert und eine Milliarde Dateneinträge sowie 21 Milliarden Beziehungen zwischen diesen erfasst hat. In fünf Jahren soll man mit Xiaoice auch reden können. Bisher sind nur Text-Chats mit der Software möglich, die in ihrem Verhalten an eine 17-Jährige erinnert.
Lernen von den Nutzern

Digitale Assistentinnen wie Apples Siri und Microsofts Cortana verstehen nur Standard-Anfragen und Aufgaben, aber selten natürliche Sprache. Tay ist folglich nicht allein: Microsoft hat in den vergangenen Monaten diverse Produkte lanciert, die seine Forschungen zum maschinellen Lernen vorantreiben sollen: Seit zwei Monaten ist etwa für Android die kostenlose App Mimicker Alarm verfügbar. Diesen Wecker kann man erst abschalten, wenn man eine von drei Aufgaben gelöst hat. Tongue Twister verlangt einen Zungenbrecher, in Express Yourself muss man ein Handybild, das eine festgelegte Gefühlsregung zeigt, schiessen und in Color Capture eines mit einer bestimmten Farbe. Das jeweilige Ergebnis kann man via Social Media verbreiten.

Damit erinnert sie an die Anfang des Jahres präsentierte Anwendung Microsoft Selfie für iOS, mit der sich Selbstporträts verbessern lassen. Mit der Anwendung kann man mit wenigen Klicks unter anderem die Belichtung und Schärfe des Bildes ändern. Bekanntlich sind viele Selfies von unterdurchschnittlicher Qualität. Überdies stehen 13 Filter mit Namen wie 1965 und BlueDawn bereit. Wie die vor ein paar Monaten lancierte Altersbestimmungswebsite how-old.net sollen diese Apps Microsofts Programme verbessern. Die Software lernt beispielsweise, welche Fotos in den Augen der Betrachter gelungen sind.";https://www.nzz.ch/digital/kuenstliche-intelligenz-microsoft-laesst-bot-nicht-nur-auf-twitter-nutzer-los-ld.9701;NZZ;Henning Steiner;;;
13.09.2018;Wie digitale Ökosysteme die Vermögensverwaltung verändern;"Ein geschäftsorientiertes Ökosystem hat zum Ziel, mittels Partnerschaften verschiedene Kompetenzen und Branchen zu verbinden. So wird durch Disintermediation eine Effizienzsteigerung erzielt oder durch Innovationen verteilt auf einzelne Teilnehmer ein Mehrwert für Kunden erwirtschaftet. Denn nur wenn der Innovationsprozess über Organisationsgrenzen hinweg in Gang bleibt, können Ideen schnell umgesetzt werden.

Bleibt allerdings ein wichtiger Teilnehmer aus, verliert das System als Ganzes an Wert. Genau das geschieht gerade mit dem Krypto-Valley in Zug: Schweizer Banken lehnen es ab, eine Bankbeziehung mit einem Krypto-Currency-Anbieter einzugehen. Ein Eco-System zeichnet sich auch dadurch aus, dass es sich anpasst und agil bleibt; es reguliert sich selber und kompensiert fehlerhafte oder notwendige Beziehungen anderweitig – in diesem Fall mit Krypto-Banken in Liechtenstein.
Skalieren über andere Sektoren

E-Commerce, Social Media und Internetfirmen geniessen noch nicht das Vertrauen, das Privatbanken über Jahrzehnte aufgebaut haben. Allerdings konnten sie innert kurzer Zeit viele Nutzer auf ihre digitalen Plattformen locken und verstehen es, vorwiegend Millennials mit digitalen Innovationen immer länger auf ihren Plattformen zu beschäftigen. Dabei erweitern sie stets ihre Angebotspalette und drängen in Geschäftsbereiche traditioneller Vermögensverwalter ein. Alibaba, der grösste E-Commerce-Anbieter aus China, hat ein Ökosystem aufgebaut, das in beinahe alle Bereiche des täglichen Lebens reicht. Durch die vielen Kunden (550 Millionen alleine in China) entstehen viele Daten, die das Kaufverhalten, aber auch Kundenwünsche und Kundenbedürfnisse offenlegen. Aktivitäten in sozialen Netzwerken, Chats, Suchmaschinen, Reisen und Satellitendaten werden ebenso ausgewertet wie Gesundheits- und Finanzdaten.

In der Vermögensverwaltung sind Informationen über Kunden und deren Verhalten äusserst wertvoll für eine ganzheitliche Finanzberatung, aber auch für die Entwicklung neuer Produkte und Dienstleistungen. Was bis anhin mittels persönlicher Kontakte oder Transaktionen mit der Depotbank zusammengetragen und in Kundenprofilen abgelegt wurde, kann künftig automatisch gesammelt werden. Wenn ein Vermögensverwalter mit seiner digitalen Plattform Teil eines sektorübergreifenden Ökosystems ist, kann auch er Daten nutzen, zu denen er sonst nie Zugang hätte.
Daten schützen oder nutzen?

Auf Daten basierende digitale Geschäftsmodelle werden die traditionelle Vermögensverwaltung zunehmend verändern. Während in China die Verknüpfung von diversen Datenquellen und die permanente Auswertung von Kundendaten weit vorangeschritten sind, prüfen derzeit Amazon und Facebook Kooperationen mit Banken.

Das Modell ist einfach: Plattformanbieter liefern die Daten, Fintech-Unternehmen haben die technische Kompetenz, diese zu analysieren, und Vermögensverwalter können mit ihrem Finanzwissen darauf basierend künftige Wünsche und Bedürfnisse vorwegnehmen und individualisierte Kundenerlebnisse möglich machen.

Obwohl zwei Drittel der Schweizer Finanzdienstleister mit einem Fintech-Unternehmen zusammenarbeiten, ist hierzulande von einer automatischen Sammlung und einer maschinellen Auswertung von strukturierten und unstrukturierten Daten noch zu wenig zu sehen. Vor allem unabhängige Vermögensverwalter stehen meist alleine da und haben es verpasst, in ein Ökosystem einzutreten und in der neuen Umgebung Wert für die eigene Organisation und ihre Kunden zu generieren.

Ein Grund ist, dass sich Europa und die Schweiz im Speziellen auf Regularien und Datenschutz versteift haben. Die Spitzenposition in der Vermögensverwaltung ist hierzulande nur zu halten, wenn Daten strategisch genutzt werden. Denn ein effektives System muss über riesige Datensätze verfügen, da diese für das Training von Algorithmen durch maschinelles Lernen unabdingbar sind. Die gezielte Förderung von künstlicher Intelligenz als Schlüsseltechnologie der Zukunft ist deshalb sinnvoll.

Die Reaktionen von China bezüglich des Algorithmus Alpha Go, der 2016 überraschend den professionellen Go-Spieler Lee Sedol im 2000 Jahre alten chinesischen Brettspiel besiegt hat, ist gleichzusetzen mit den Reaktionen der amerikanischen Regierung nach den ersten Erfolgen der Sowjetunion in der Raumfahrt. Der «Sputnik-Schock» initiierte die amerikanische Aufholjagd, die 1969 mit der Mondlandung erfolgreich die Vorherrschaft im All sicherte.

Entsprechend ist das Ziel der chinesischen Regierung, bis 2030 führend im Bereich der künstlichen Intelligenz zu werden. Gefördert werden beinahe alle technologischen Entwicklungen wie Robotics, Ausbildung, autonomes Fahren, Virtual Reality oder 3-D-Druck, wo China nach den USA führend ist. Im Bereich Fintech tätigt China bereits heute rund 30% mehr Wagniskapital-Investitionen als die USA und sechs Mal so viel wie Grossbritannien.
Nicht länger zuwarten

Es ist unbestritten, dass die Vermögen in China zulasten Nordamerikas, Deutschlands, Grossbritanniens, Frankreichs und der Schweiz stark wachsen werden. In einer vernetzten und digitalen Welt ist es deshalb entscheidend, Kundendaten effizient zu sammeln, maschinell auszuwerten und mittels künstlicher Intelligenz Mehrwert zu generieren. Wer diesen Prozess mithilfe der richtigen Partner abbildet, wird die Vermögen in den neuen Wachstumsregionen verwalten und junge digital-affine Kunden mit innovativen Lösungen begeistern.

Wissenschaft, Regierung und Branchenverbände müssen Erkenntnisse aus anderen Sektoren und Regionen integrieren, um die Risiken im Datenschutz rasch und sorgfältig abzuschätzen und wettbewerbsfördernde Rahmenbedingungen für digitale Innovationen auszugestalten. Traditionelle Vermögensverwalter und Fintech-Unternehmen müssen ihre Kooperationsbereitschaft steigern und die Chancen sektorübergreifender Ökosysteme nutzen. Die Aufholjagd muss jetzt beginnen.";https://www.nzz.ch/wirtschaft/digitalisierung-der-vermoegensverwaltung/wie-digitale-oekosysteme-die-vermoegensverwaltung-veraendern-ld.1419545;NZZ;Daniel Fassnacht;;;
01.11.2017;Wie Lehrlinge für das Roboterzeitalter lernen;"«Ich will einmal als Ingenieur Dinge entwickeln, die den Menschen das Leben erleichtern», sagt Flaoudio Bylykou. Ein hehres Ziel – und ein fernes, für einen Automatiker im ersten Lehrjahr jedenfalls. Doch seine Lernkurve zeigt steil nach oben: Im August erst haben er und Felix Schweizer ihre Lehre bei Libs in Oerlikon begonnen, und jetzt bedienen sie bereits routiniert den futuristischen Yumi – den Industrieroboter neuester Generation, der es auf Youtube als «Dirigent» zu Bekanntheit gebracht hat. Schweizer erklärt anhand des Displays, auf dem Koordinaten und Winkelgrössen durcheinanderwirbeln, die Vorzüge des Roboters: «Yumi weiss immer genau, wo sich seine beiden Arme befinden. Er braucht sie nicht einmal zu sehen, um eine Kollision zu verhindern.» Schweizer, erst vor zwei Jahren aus Deutschland in die Schweiz gezogen, ist vom Wunderapparat der ABB sichtlich angetan. Gerne würde der angehende Automatiker dort später selbst solche Roboter bauen. Dazu muss er zunächst einmal seine Lehre absolvieren, sich weiterbilden und weitere Erfahrung sammeln. Dieser Weg ist kein Zuckerschlecken, doch er steht ihm offen. Mit dem Fortschritt Schritt halten

Die Industrie und ihre Berufsbilder verändern sich derzeit in rasantem Tempo: Immer mehr Prozesse können automatisiert, Abläufe digitalisiert und vereinfacht werden. Für Lehrlinge in technischen Berufen ist das ein zweischneidiges Schwert. Sie (und ihre Ausbilder) wissen heute nicht, wie ihr Beruf in zehn oder zwanzig Jahren genau aussehen wird. «Doch für das Hochlohnland Schweiz ist die vierte industrielle Revolution eine grosse Chance, trotz unseren hohen Kosten konkurrenzfähig zu bleiben», sagt Ingo Fritschi, Geschäftsführer von Libs Industrielle Berufslehren Schweiz. Sein Betrieb koordiniert die Ausbildung von rund 1100 Lehrlingen – vor allem Automatiker, Polymechaniker, Konstrukteure oder Apparatebauer – an fünf Standorten in der Nord- und Ostschweiz. Einer davon befindet sich an bester Lage hinter dem Bahnhof Oerlikon, leicht versteckt hinter einer Brachfläche und neben einem ältlichen, leicht verwahrlosten Parkhaus gelegen.

Umso stärker kontrastiert das Innenleben von Libs Zürich mit dieser Umgebung: Der Boden ist sauber, die Arbeitsplätze aufgeräumt, die Schutzbrillen in der Werkhalle liegen am richtigen Ort auf. Das ist auch im Sinne der am Ausbildungsbetrieb beteiligten Arbeitgeber: Libs bildet seine Schützlinge nur während der ersten zwei Lehrjahre aus, im Auftrag ihrer rund hundert Partnerfirmen. Im dritten Jahr wechseln sie zu diesen. Libs ist als Verein organisiert und blickt auf eine lange Geschichte zurück, ist dieser Verein doch 1996 direkt aus der Lehrlingsausbildung der früheren BBC hervorgegangen, als deren Nachfolgerin ABB sie auslagerte. Sie hat sich zum Vorzeigebeispiel für den dualen Bildungsweg gemausert: Kantonsvertreter und Bundesräte lassen es sich nicht nehmen, ihren ausländischen Gästen auf Staatsbesuch hier die Vorzüge des schweizerischen Lehrlingswesens zu zeigen. «Wir hatten schon neun US-Gouverneure bei uns zu Besuch; auch der damalige US-Arbeitsminister Thomas Perez war 2015 hier», zählt Fritschi auf. Doch während die Gouverneure Schlange stehen, gestaltet sich die Suche nach guten Lehrlingen nicht immer einfach. Nicht selten steuern Eltern, insbesondere Akademiker ohne Bezug zur Berufsausbildung, ihre Kinder heute vorsorglich in Richtung Gymnasium, weil sie sich davon die besten Startbedingungen für ihren Nachwuchs erhoffen. «In der Tat haben viele unserer Lehrlinge Eltern, die selbst in technischen Berufen arbeiten. Besonders bei den Mädchen ist das der Fall.» Von diesen gibt es bei Libs in Zürich nur eine Handvoll; die technischen Berufe sind fest in Männerhand und dürften es vorläufig auch bleiben. Zu den wenigen jungen Frauen am Standort Oerlikon gehört Sheela Reutlinger, Polymechanikerin im ersten Lehrjahr. Sie hat im Haus ihrer Grosseltern schon immer gern gebastelt, Mechanik fasziniert sie. In Skandinavien würde sie später im Berufsleben wohl in fast ausgeglichenen Teams arbeiten; hier in Zürich ist sie das einzige Mädchen in ihrem Jahrgang. Woran das liegt, kann auch Fritschi nicht genau sagen; veranstaltet Libs doch einen Zukunftstag und lädt Schulklassen ein, um gezielt auch Mädchen für technische Lehrgänge zu begeistern.

Für angehende Polymechanikerinnen und -mechaniker gilt jedoch gleichermassen: Ihr Beruf wird in einigen Jahren kaum mehr wiederzuerkennen sein. Software statt Schmieröl lautet die Devise – die riesigen Maschinen in der Werkhalle weisen den Weg. Auf der einen Seite steht noch eine gute alte Drehmaschine, an welcher die Lehrlinge mit rotierenden Werkstücken umzugehen lernen. «Wir möchten den Lehrlingen auch zeigen, was es heisst, Metall zu bearbeiten: Hier formen sie das Material noch mit eigenen Händen; spüren die Hitze, die es entwickelt», sagt Fritschi. Gleich gegenüber steht ein gut abgeschirmtes Ungetüm, das einige dieser Handgriffe schon selbst beherrscht und auf elementare, programmierte Befehle hört. Fünf Schritte entfernt davon steht die Zukunft: eine vollkommen selbständig arbeitende Maschine, die mit rund 170 000 Zeilen Code (und einem Metallstück) gefüttert wurde und daraus einen zuvor entworfenen kleinen VW-Bus formt.
Ohne Schule geht es nicht

Für schwache Schüler ist dieser Wandel eine besondere Herausforderung. Einfache manuelle Tätigkeiten haben es in der Schweiz zusehends schwer, doch nicht jedem geht das Programmieren gleich leicht von der Hand. Libs setzt darauf, ihre Schüler möglichst anwendungsorientiert an die Mathematik – die sie parallel dazu in der Berufsschule erlernen – heranzuführen. «Unsere Lehrlinge haben eigenständig Ideen entwickelt, wie wir den Wasser- und Stromverbrauch senken können. Einige tüfteln jetzt an einer Presse, mit der wir Karton und Papier effizienter entsorgen», sagt Fritschi. Oftmals bringe dieser spielerische, experimentelle Zugang die Jugendlichen dazu, sich mit der Theorie zu beschäftigen, welche ihnen hilft, diese Probleme zu lösen.

Das Schulungszimmer der Konstrukteure ist an diesem späten Nachmittag bereits verlassen. Die Lehrlinge, deren Arbeitstag pünktlich um halb acht Uhr beginnt, geniessen den Feierabend. Nur einer ackert unermüdlich: Ein filigraner 3-D-Drucker drückt blaues, heisses Plasticlaminat aus einer dünnen Düse auf eine rotierende Plattform. Damit lassen sich Matrizen und Formen aller Art zusammenbauen; vier Lehrlinge im zweiten Jahr haben aus solchen Formen ein zwei Meter langes Schiff aus Glasfasern gebaut, das via GPS aus der Ferne gesteuert werden kann. Konstrukteure sind Teamarbeiter, gemeinsam mit Ingenieuren erstellen sie die Baupläne für Maschinen oder Werkteile aller Art. Ihr Beruf, den die vorige Generation noch als «Zeichner» kennenlernte, hat sich mit dem Aufkommen von spezialisierten Designprogrammen bereits stark verändert. Die 3-D-Drucker verändern ihre Arbeitswelt und den industriellen Fertigungsprozess noch einmal grundlegend, erlauben sie es doch, viel günstigere Prototypen zu fertigen und auch Kleinserien von Massengütern wirtschaftlich herzustellen. Doch auch hier gilt: Nur wer die Sprache der Wundermaschinen lernt, kann ihnen die richtigen Befehle erteilen.

Die Industrie im Grossraum Zürich hat zwar die Frankenstärke nach 2015 überstanden. Doch Erholung und Krise geben sich im Produktionssektor jeweils rasch die Hand. So haben derzeit wichtige Arbeitgeber, die hinter Libs stehen, wirtschaftliche Probleme. Bombardier reduziert seine Belegschaft in Zürich deutlich, und erst vor wenigen Tagen verdichteten sich Gerüchte, wonach General Electric im Aargau wohl bis zu 1300 Stellen streicht. Fritschi äussert sich nicht zum Wohlergehen einzelner Firmen, verneint aber auch nicht, dass auch Libs diesbezüglich vor Herausforderungen steht. Doch die Schweizer Industrie habe schon viele Krisen überstanden und sei immer gestärkt daraus hervorgegangen. Der Libs-Nachwuchs soll dereinst dafür sorgen, dass sich diese Weisheit noch in zwanzig, dreissig Jahren bewahrheitet.";https://www.nzz.ch/zuerich/nachwuchs-fuer-das-roboterzeitalter-ld.1324308;NZZ;Andre Müller;;;
24.11.2017;Wie Sie Ihre Gedächtnisleistung verbessern können;"Herr Dr. Konrad, Sie sind Weltmeister im Namenmerken und können sich in 15 Minuten 200 Namen merken. Können wir Durchschnittsmenschen das auch?

Dafür müssen Sie schon etwas trainieren.

Wie denn?

Versuchen Sie, Informationen in Form von Bildern aufzunehmen. Das kann unser Gehirn sowieso schon gut. Wo etwas passiert ist und wie etwas aussieht, können wir uns sehr gut merken. Das ist evolutionär bedingt: Früher mussten wir Menschen uns Orte merken, aber keine Zahlen. Es fällt uns darum schwer, uns Zahlen, Fakten, Fachinformationen und Namen zu merken, denn die sind weniger greifbar. Um unser Gedächtnis zu trainieren, benutzen wir sogenannte Mnemotechniken, bei denen wir abstrakte Informationen in Bilder umwandeln.

Wie können wir uns also zum Beispiel am schnellsten alle Länder in Asien merken?

Dafür nehmen wir uns einmal diese Länder vor: Aserbaidschan, Armenien, Georgien, die Mongolei und Kasachstan. Dann überlegen wir uns zu jedem Land ein Bild, das ähnlich klingt wie das Land, und kreieren damit eine kleine Geschichte. Die könnte so lauten: Wir machen eine Asienreise, das klingt wie ‹Aser›, wir treffen uns aber in Bayern, das klingt wie ‹Baidschan›, weil wir arm sind (‹Armenien›). Unser Reiseleiter ist George Clooney (Georgien). Er sagt, dass wir erst einmal in den Supermarkt gehen, dort kaufen wir eine Mango (Mongolei), und damit gehen wir zur Kasse (Kasachstan). So kann man weitermachen. Gut funktioniert auch die Routen-Methode. Wie wenden wir die Routen-Methode an?

Hier verknüpfen wir Worte, die wir lernen möchten, mit Gegenständen oder Wegpunkten auf einer bekannten Route. Im Gedächtnis rufen wir einen uns bekannten Weg ab, etwa den Weg durch den Garten der Grosseltern oder den Arbeitsweg.

Und dann?

Sagen wir einmal, Sie wollen sich zehn bekannte Schweizer Personen merken. Auf dem Briefkasten, dem ersten Punkt, sitzt Jean-Jacques Rousseau, mit der weissen Perücke auf dem Kopf, an der Laterne daneben lehnt Albert Einstein, er fährt sich über den dicken Schnauz, und so weiter. Wichtig: Es muss ein Weg sein, den Sie in- und auswendig kennen und den Sie ganz bewusst vor dem inneren Auge ablaufen können. Sie können sich die Route nicht erst beim Lernen ausdenken, dann funktioniert es nicht.

Warum können wir uns so Dinge besser merken?

Sie müssen sich das so vorstellen: Wenn wir etwas Abstraktes mit einem Bild oder einem Ort verbinden, schaffen wir neue Verknüpfungen. Wir können die Gedächtnisleistung so vervielfachen, weil das Gehirn sich sehr gut an Orte, Wege und Bilder erinnern kann. Dafür ist es gemacht. Unsere Erinnerungen sind ja auch immer mit Orten verbunden. Namen, Daten oder Fakten, die wir uns schlecht merken können, fehlt umgekehrt meist der örtliche Bezug.

Ist das auch der Grund, weshalb uns etwas, das wir vergessen hatten, erst an einem bestimmten Ort wieder einfällt?

Ja. An dem Ort, der mit der Erinnerung verknüpft ist, fällt uns das Vergessene häufig wieder ein. Eigentlich reicht es auch schon, sich diesen Ort bildlich vorzustellen.

Was ist das Gedächtnis?

Das Gedächtnis ist die Fähigkeit von Lebewesen, Informationen in ihr Nervensystem aufzunehmen und bei Bedarf wieder abzurufen.

Haben schlauere Menschen ein besseres Gedächtnis?

Grundsätzlich kann man sagen, dass Intelligenz und Gedächtnisleistung korrelieren, ja. Wenn es ums Gedächtnistraining geht, sind aber alle gleich gut.

Wir können also alle die Gedächtnistechniken gleichermassen lernen?

In einer Studie haben wir festgestellt, dass alle Teilnehmer mit dem Üben der Routen-Methode ihr Gedächtnis verbessern konnten. Wir haben die Teilnehmer in drei Gruppen eingeteilt. Die einen haben nicht geübt, die anderen sechs Wochen lang 30 Minuten am Tag mit der Routen-Methode, die dritte Gruppe genauso lang mit einer anderen Kontrollmethode. In den Tests haben nach sechs Wochen die Teilnehmer der zweiten Gruppe am besten abgeschnitten. Vor dem Training konnten sie sich durchschnittlich 30 von 72 neuen Wörtern merken, nach sechs Wochen mehr als 60 Wörter.

Können wir mit diesen Methoden auch ein Musikinstrument lernen?

Die Techniken funktionieren vor allem für Dinge, die wir uns ganz bewusst einprägen wollen. Für das prozedurale Gedächtnis, das fürs Geigespielenlernen zuständig ist, funktioniert es schlechter.

    «Wir alle haben eine grosse Vorstellungskraft, aber nutzen sie oft kaum. Übung ist alles.»

Was empfehlen Sie fürs Sprachenlernen?

Arbeiten Sie mit Schlüsselwörtern. Verbinden Sie eine Vokabel mit etwas, das ähnlich klingt und das Sie gut kennen. Iglesia beispielsweise, das ist die Kirche auf Spanisch, würde ich mir so merken: Iglesia klingt wie Iglu, dann stelle ich mir einen riesengrossen Iglu vor, und in diesem steht eine Kirche. Wenn ich das nächste Mal das Wort Iglesia sehe, entsteht das Bild in meinem Kopf, und ich kann «Kirche» abrufen.

Seien wir ehrlich, die wenigsten von uns können so kreativ denken beim Vokabelnlernen.

Am Anfang ist das tatsächlich nicht einfach. Schauen Sie sich Gedächtnistechniken einmal an, und üben Sie diese. Wir alle haben eine grosse Vorstellungskraft, aber nutzen sie oft kaum. Übung ist alles.

Aber nicht alle Menschen haben ein gutes visuelles Vorstellungsvermögen.

Doch, wir alle haben ein sehr gutes Vorstellungsvermögen, wenn wir es nur anwenden. Auch Blinde können problemlos die Gedächtnismethoden anwenden, weil sie sich ihre Bilder vor dem inneren Auge vorstellen.

Woran liegt es denn, dass wir den Namen der neuen Nachbarin gleich nach dem Vorstellen wieder vergessen?

Namen haben ihre ursprüngliche Bedeutung nicht mehr. Früher war eine Frau Fischer auch tatsächlich Fischerin, heute natürlich nicht mehr. Ich kann sie mir aber trotzdem auf einem Fischerboot mit einer Angel vorstellen. Und wenn ich sie sehe, dann denke ich: ah, die Fischerin! Was ist mit komplizierteren oder ausländischen Namen?

Fragen Sie nach, wenn Sie einen Namen nicht verstehen, und sprechen Sie den Namen Ihres Gegenübers noch einmal aus. Denn nur das, was wir verstanden haben, erreicht unser Kurzzeitgedächtnis. Einen langen ausländischen Namen können Sie dann in Silben unterteilen und sich dann Bilder für die einzelnen Silben ausdenken.

Wie schaffen wir es, dass wir die Vokabeln auch in einem Jahr noch wissen?

Bei den Gedächtnismethoden geht es erst einmal nur um die Aufnahme der Wörter, Zahlen oder Begriffe. Wenn wir uns diese längerfristig merken wollen, dann hilft nur Wiederholen. Das ist aber leichter und weniger aufwendig, als die meisten denken. Übrigens: Das Blatt Papier mit der niedergeschriebenen Route durchzulesen, bringt nichts. Am besten versuchen Sie, die Route aus dem Gedächtnis zu wiederholen. Wenn Sie sich selbst abfragen oder jemand anderes Sie testet, dann ist das am sinnvollsten. Eine neue Routen-Geschichte würde ich am gleichen Abend, am nächsten Morgen, am Ende der Woche und dann ein paar Wochen später noch einmal wiederholen.

Warum vergessen wir überhaupt Dinge?

Unser Gehirn ist eigentlich ein Filter und enorm effizient. Das Gehirn weiss, dass es sich nicht alles merken kann. Deshalb nimmt es 99 Prozent der Informationen einer normalen Alltagssituation gar nicht auf. Sie kommen gar nicht erst vom Ultrakurzzeitgedächtnis ins Kurzzeitgedächtnis. Sonst wären wir auch überfordert: Wir könnten uns nicht auf ein Gespräch konzentrieren und würden ständig über unsere Umgebung, Gerüche und Geräusche nachdenken. Viele denken auch, dass eine gewisse Gehirnregion für das Gedächtnis zuständig sein muss. Dem ist aber nicht so. Ich kann also nicht örtlich bestimmen, welche Gehirnzellen genau abgespeichert haben, was ich gestern gemacht habe.

Wie funktioniert das Gedächtnis dann?

Unser Gehirn speichert keine Erinnerungen ab, wie ein Computer das tut. Ein Computer kann Informationen abrufen, exakt replizieren und auch löschen. Das Gehirn aber arbeitet anders. Wenn wir eine Erinnerung abrufen, dann wird diese Erinnerung jedes Mal verändert und neu interpretiert. Und wenn wir etwas Neues lernen, entstehen neue Verknüpfungen.

Was heisst das?

Unser Gehirn gleicht ständig ab: Was kennen wir schon, und was ist neu? Wenn ich etwas Neues erlebe, dann wird es mit Altbekanntem verknüpft. Und wenn ich eine Erinnerung abrufe, dann werden vorhandene Verknüpfungen aktiviert.

Nennen Sie ein Beispiel!

Ich möchte mich an das vergangene Gespräch mit einem Freund erinnern, als wir in einem Restaurant sassen. Wenn ich die Erinnerung abrufe, kommt mir vielleicht der blaue Tisch in den Sinn, an dem wir sassen, nicht aber der Stuhl. Diesen schnappt sich das Gehirn von irgendwo anders im Gedächtnis, denn Stühle hat es ja schon oft gesehen. Es füllt sozusagen die Lücke: Ah, so sieht ja ein Stuhl in einem Restaurant aus, und ergänzt die Erinnerung. So entstehen auch falsche Erinnerungen.

Unser Gedächtnis kann uns täuschen?

Ja, absolut. Was wir vor dem inneren Auge haben, ist oft nicht exakt. Es gibt viele Studien, bei denen Forscher Probanden falsche Erinnerungen «eingepflanzt» haben, und es ist beeindruckend, wie leicht das geht. Zum Beispiel haben sie den Probanden gesagt: Weisst du noch, als jemand bei der Hochzeit deiner Freundin die Torte umgeworfen hat? Und später meinen die Getesteten, sich wirklich an eine umgeworfene Torte zu erinnern.  Und was bedeutet all das fürs Lernen?

Das Gehirn lernt in Schemata und versucht ständig, die Welt zu beschreiben. Deshalb können Sie problemlos beschreiben, wie ein üblicher Kindergeburtstag aussieht. Wenn Sie aber jemand bittet, Ihren eigenen elften Geburtstag zu beschreiben, dann ist das schwierig. Ausser es ist etwas Ungewöhnliches passiert: Sie waren an Ihrem elften Geburtstag besonders traurig, weil Ihre beste Freundin gefehlt hat.

Wie kommt das?

Weil wir bestimmte Erinnerungen mit Gefühlen verbinden. Man dachte lange, im limbischen System im Gehirn fänden nur die Emotionsverarbeitung und die Entstehung von Triebverhalten statt. Heute aber wissen wir, dass dieser Teil des Gehirns auch wichtig für die Gedächtnisbildung ist. Gefühle und Gedächtnis hängen also eng zusammen. Alles, was nicht mit einem Gefühl verbunden ist, wird in ein Schema eingeteilt, aber die einzelne Erinnerung geht verloren. Und alles, was mit einem starken Gefühl verbunden ist, sei es Glück, Trauer oder auch Peinlichkeit, bleibt eher haften.

Alte Menschen erinnern sich oft viel lebhafter an Ereignisse, die lange her sind. Warum?

Weil diese Erinnerungen ganz oft im Leben abgerufen und immer wieder mit neuen Erinnerungen verknüpft wurden. Diese Ereignisse werden im Laufe des Lebens immer präsenter und so immer lebhafter. Aber wissenschaftlich wissen wir noch nicht genau, was da passiert.

Die meisten Menschen haben Angst, eines Tages an Demenz zu erkranken. Können wir der Alterskrankheit vorbeugen?

Es gibt Studien, die darauf hinweisen, dass das so ist. Mit Gedächtnistraining können wir Demenz vorbeugen, aber natürlich kann auch der beste Gedächtniskünstler einmal dement werden. Genau wie sich jemand immer gesund ernähren und Sport machen kann, trotzdem einen Herzinfarkt erleiden kann. Interessant ist auch: Menschen, die sich ihr ganzes Leben lang viel merken mussten oder einen intellektuell anspruchsvollen Job hatten, können sich auch im hohen Alter noch viele Dinge merken. Sie erkranken auch nachweislich später an Demenz, jedoch ist der Verlauf der Krankheit dann oft schneller.

Wie erklären Sie sich das?

Wir haben eine Vermutung. Vielleicht hat die Demenz schon mit 65 Jahren eingesetzt, wurde aber durch die gute Leistung des Gehirns lange Zeit ausgeglichen. Sobald der Ausgleich nicht mehr klappt, bauen diese Menschen schnell ab.

Was können wir also konkret gegen die Demenz tun?

Wichtig ist, dass wir unser Gedächtnis herausfordern. Wenn Sie nie Sudokus gelöst haben und das dann tun und sich von leicht über mittel bis schwer steigern, ist das sinnvoll. Wenn ein Grossvater aber zwanzig Jahre lang das gleiche Kreuzworträtsel in der gleichen Tageszeitung löst, dann bringt das nicht mehr so viel. Dann ist das eher Routine.

Was wäre denn sinnvoller?

Er könnte versuchen, eine neue Sprache zu lernen oder zumindest weiterzulernen, oder ein Musikinstrument lernen. Wichtig ist aber auch, dass das gesetzte Ziel realistisch ist.

    «Alte und Demente müssen immer wieder herausgefordert werden.»

Es gibt ja eine riesige Menge an Gehirnjogging-Apps. Bringen die etwas?

Dazu gibt es unterschiedliche Meinungen. Es gibt Studien, die belegen, dass viele dieser kommerziellen Apps nichts bringen. Andere Studien haben positive Effekte gefunden, meist aber nur bei wissenschaftlich erprobten Spielen und nicht bei beliebigen auf dem Smartphone. Klar ist, dass Spiele nur dann sinnvoll sind, wenn sie mit der Zeit schwieriger werden.

Wenn jemand schon dement ist, kann er dann der Krankheit entgegenwirken?

Ja, aber nur, wenn die Krankheit im Anfangsstadium ist. Eine Studie hat gezeigt, dass die Routen-Methode bei Menschen im frühen Demenzstadium den Verlauf der Krankheit verlangsamt. Problematisch ist aber immer, dass Demenzkranke schnell demotiviert sind, wenn sie merken, dass sie sich verschlechtert haben.

Können Angehörige helfen?

Angehörige machen den Fehler, dass sie ihren Partnern, Eltern oder Grosseltern zu viel helfen. Sie nehmen ihnen alltägliche Dinge wie etwa das Einkaufen ab. Das ist aber genau falsch! Alte und Demente müssen immer wieder herausgefordert werden. Wenn man alles für sie macht, kann die Krankheit schneller voranschreiten.

Was hilft dementen Menschen noch?

Mit Sprichwörtern spielen. Diese beginnen, und die betroffene Person soll sie beenden. Oder Erinnerungen zurückholen, über vergangene Erlebnisse sprechen. Und vor allem: ihnen die Chance geben, Erinnerungen selbst abzurufen.

Hilft Sport gegen Demenz?

Ja. Es wurde bewiesen, dass eine gute körperliche Fitness sich positiv auf die Leistungen des Gehirns auswirkt.

Heute haben wir alles Wissen immer in unserer Hosentasche parat – in unserem Smartphone. Wir müssen keine Telefonnummern mehr auswendig kennen und auch nicht das Jahr, in dem Rom gegründet wurde. Ist das schlecht für unser Gedächtnis?

Zunächst ja. Weil wir immer alles abrufen können, nutzen wir unser Gedächtnis weniger. Es kann schon sein, dass unser Gedächtnis darunter leidet, dass wir uns manche Dinge einfach nicht mehr merken müssen und sie darum vergessen.

Sind Menschen, die sich ihr Wissen auf Wikipedia abrufen, also dümmer?

Es gibt eine Studie, bei der bei Studenten untersucht wurde, wie sich der Gebrauch von Wikipedia auf ihr Gedächtnis auswirkt. Tatsächlich wussten jene, die Wikipedia benutzt hatten, später weniger über das Thema. Aber die Digitalisierung hat zwei Seiten.

Welche?

Vielleicht kennt eine 50-jährige Frau alle wichtigen Goethe-Zitate auswendig und eine 25-jährige nicht, sondern sie schaut diese auf Wikipedia nach. Allerdings musste sich die 50-Jährige auch nie merken, wie man 37 Apps auf dem Smartphone bedient.

Der Neurologe Manfred Spitzer schreibt in seinem Buch «Digitale Demenz», dass die digitalen Medien uns geistige Arbeit abnähmen und dem Gedächtnis schadeten. Er rät, Kinder bis im Alter von zehn Jahren ganz von Bildschirmen fernzuhalten.

Das ist Quatsch. Es ist wichtig, den Gebrauch von Technologie zu fördern. Kinder wachsen heute in einer Welt auf, in der digitale Kompetenzen unheimlich wichtig sind. Aber natürlich sollte man die Nutzung von Medien bei Kindern kritisch beurteilen.

Warum sollte man das?

Wenn Kinder nur passiv vor dem Bildschirm sitzen und blinkende Bilder anschauen, dann ist das schlecht für die Entwicklung. Aber ich kann ja dem Kind im Kindergartenalter einmal das iPad in die Hand geben und mit ihm schauen, wie das überhaupt funktioniert und was man damit machen kann. Generell finde ich es erstaunlich, wie schnell sich unser Gehirn in kürzester Zeit an eine digitalisierte Welt angepasst hat.

Unser Gehirn ist also erstaunlich lernfähig?

Unser Gehirn hat sich im Laufe der Evolution enorm entwickelt. Digitale Technologie ist aber sehr neu. Nach kurzer Zeit hat das Gehirn gelernt, mit Smartphones, Datenbrillen, virtueller Realität oder Robotern umzugehen. Wie schnell das Gehirn das wirklich lernt, untersuchen wir aber noch.
";https://www.nzz.ch/wissenschaft/was-sie-schon-immer-ueber-ihr-gedaechtnis-wissen-wollten-ld.1331777;NZZ;Alexandra Kohler;;;
11.11.2019;Im Rausch des Sammelns von Einsen und Nullen;"Silicon Valley hat den globalen Goldrausch um Daten ausgelöst, und längst versuchen die hier ansässigen Unternehmen die Politik der USA mit «data-driven» Methoden umzukrempeln. Silicon Valley ist Erdbebengebiet, was mit einer viel älteren Art Daten zu tun hat, die weiterhin die US-Regierung sammelt – in Menlo Park, unweit des Hauptquartiers von Facebook. Die höchste Erdbebensicherheit herrscht nicht in Bürotürmen, auf Highways oder im Rathaus. Nein, die in der Bay Area entwickelte, aber vor allem in Japan eingesetzte Base-Isolation-Technik wird fast ausschliesslich in den Datenzentren Silicon Valleys eingesetzt. Nur Apple hat in seinem neuen Hauptquartier auf diese supersichere Technologie gesetzt. Was heisst: Im Falle eines Erdbebens wird Mark Zuckerberg von einem Deckenpfeiler erschlagen, bevor unsere Urlaubsfotos in Gefahr geraten.
Fetisch Daten

Daten sind heute – wie Karl Marx vielleicht gesagt hätte – voll metaphysischer Spitzfindigkeit und theologischer Mucken. Und haben sicher auch ein Stück weit Fetischcharakter. Nur dass bei ihnen, im Vergleich zur Ware, der Nutzwert vielleicht weniger selbstverständlich ist als der Tauschwert. Ja, man könnte sogar sagen, dass ihr Nutzwert fetischisiert wird – alle wissen, dass sie etwas wert sind, nur noch nicht genau, wie viel es sein soll. Zahlreiche Unternehmen Silicon Valleys horten sie in der Hoffnung, sie zu Geld machen zu können; Staaten sammeln sie und kontrollieren sie in der Hoffnung, sie zur Verbrechensbekämpfung einsetzen zu können; reiche Investoren subventionieren Unternehmen, die zwar ständig mit ihrem Hauptgeschäft Geld verlieren, aber eben nebenher auch eifrig Daten produzieren.

Wir schützen Daten, aber das bedeutet in der digitalen Welt vor allem: die User darauf hinweisen, wo, wann und wie sie ihre Daten hergeben. Denn was man mit diesen Daten machen kann, demgegenüber sind diese Datensätze ja absolut neutral. Und, wie der Fall der Firma Cambridge Analytica zeigt, sind selbst die technischen Mittel, mit denen man den Missbrauch von Daten verhindern will, ebenso gut für Missbrauch einsetzbar. Als Cambridge Analytica noch SCL Group hiess, vermarktete sich die Firma noch als effektives Gegenmittel zu Radikalisierungprozessen, wie sie sich in sozialen Netzwerken vollziehen können. Doch es dauerte nicht lange, bis sie dieselbe Technologie einsetzte, um Wähler in den USA zu radikalisieren.

    Investoren subventionieren Unternehmen, die zwar ständig mit ihrem Hauptgeschäft Geld verlieren, aber nebenher eifrig Daten produzieren.

Am Schluss sind die Daten möglicherweise tatsächlich massgeblich für die Zukunft der Weltwirtschaft, aber erst über den Umweg durch die Politik. Denn einerseits muss sich die Politik darüber Klarheit verschaffen, was die Privatwirtschaft sammeln und wie sie das auswerten darf – der Wert der Daten hängt also ganz explizit von der staatlichen Regulierung ab, weswegen gerade auch in der EU ein Glaubenskrieg um den Datenschutz tobt. Es ist der letzte Rückzugskampf der Deregulierungsgegner. Umgekehrt aber hat die Datensammlung der Privatwirtschaft extreme politische Sprengkraft. Das Unternehmen Palantir kann anhand von Kreditkarteninformationen und Aktivität in den sozialen Netzwerken Gefahrenprofile erstellen, die Terroristen angeblich im Voraus identifizieren. Aber als ich vor wenigen Tagen nach Frankfurt flog, wurde ich per Hand abgeklopft, wurden mir dieselben drei Fragen drei Mal gestellt, weil angeblich das System kaputt war.
Zwischen Wirtschaft und Politik

In solch eklatanter Diskrepanz liegt natürlich immer das Potenzial, dass irgendwann verlangt wird, der Staat müsse es den Privaten nachmachen: Die Zivilgesellschaft verlange, dass bei dieser oder jener Art Person die weitaus invasiveren Techniken der Werbebranche angewandt werden. So geschehen in der Jagd der Trump-Regierung auf undokumentierte Einwanderer in den USA. Die Daten, welche die ICE (Immigration and Customs Enforcement) für ihre Listen benutzt, sind laut Berichten der «New York Times» fast allesamt Anwendungen, die von privaten Zulieferern aus Silicon Valley geschaffen wurden – die ursprünglich also Datensätze von Kunden, nicht von Einwanderern erstellen. So oder so drängen die Daten aus der Wirtschaft in die Politik. Und sie drängen umgekehrt die Politik in die Wirtschaft. Aber es gilt zweierlei festzuhalten: Erstens tangieren uns die Datenmassen, die einmal unser aller Leben verändern sollen, heute vor allem im Konjunktiv. Wenn man wissen will, wieso Uber bei einem Quartalsverlust, der jedem Taxiunternehmen den Besuch der Finanzbehörden bescheren würde, trotzdem von Investoren weiterhin mit Cash versorgt wird, ist man schnell bei den Daten, die das Unternehmen sammelt und irgendwann einmal für x-mögliche Zwecke einsetzen könnte. Bei WeWork handelt es sich dem Augenschein nach um einen ganz normalen Vermieter. Aber dank Daten erfüllte das Unternehmen lange den Nimbus eines Technologiekonzerns. Dass die Daten von WeWork dem Unternehmen unter anderem zeigten, dass Büroangestellte gerne guten Kaffee trinken, wirft die Frage auf, inwiefern Big Data uns in gewissen Fällen auch nicht mehr sagt als der gesunde Menschenverstand. Alle wollen Tech-Unternehmen sein, wollen Daten sammeln, weil man damit Investoren vermitteln kann, man habe den direkten Draht zur Zukunft.

    Zu unserem Diskurs über Daten gehört ein ausgeprägter Möglichkeitssinn. Daten sind ein Instrument spekulativer Gouvernementalität.

Die Daten spielen in der Phantasie unserer Gegenwart eine grosse Rolle, aber sie beleuchten im Grunde genommen die Veränderung dessen, was Foucault einmal Gouvernementalität genannt hat. Während der Markt im ständigen Indikativ-Präsens spricht, schweben sie im Futur und im Konjunktiv. Wenn Uber einmal könnte, wenn Google einmal wüsste, wenn die Regierung Zugang hätte ... Zu unserem Diskurs über Daten gehört ein ausgeprägter Möglichkeitssinn – Daten verleihen sowohl dem Unbehagen als auch den Versprechungen des globalen Kapitalismus eine Dimension der Futurität, die sich sonst im Zeitalter staatlicher Austerität und planetarer Katastrophe immer weiter zu verflüchtigen droht. Daten sind ein Instrument spekulativer Gouvernementalität.
Daten sind nur Daten, wenn sie in Sätzen aufkommen

Und auch der globale Datenhandel klammert am Ende den Menschen nicht aus. Hinter vielem, was angeblich Algorithmen und künstliche Intelligenz für uns erledigen, stehen in Wahrheit am Ende doch Menschen – Menschen in Indonesien und auf den Philippinen etwa, die ihren ganzen Arbeitstag lang nur Gewaltvideos sichten müssen, um diese von Plattformen wie Facebook zu entfernen. Die Frage nach den Daten ist also immer auch eine nach der Gesellschaft – auch wenn die Datensammler so tun, als liesse sich diese Gesellschaft mittlerweile in Daten auflösen. Daten sind nur Daten, wenn sie in Sätzen aufkommen, wenn sie zueinander in Relation stehen. Das einzelne Datum hat seinen Wert in seinem Verhältnis zu Millionen Geschwistern. Und doch behandeln wir, wenn wir unsere Daten schützen, diese Daten immer noch als ein Etwas, das der Einzelne besitzen und schützen kann.

Zweitens aber reden wir wenig darüber, wie sehr der globale Datenfluss die globalen Waren- und Kapitalströme nachvollzieht. So gerne man angesichts der Callcenter in Hyderabad und der Content Moderation Center in Indonesien von einer globalisierten Internetwelt spricht: Der Kampf um die Daten hält sich ziemlich brav an die Gezeiten analoger Souveränität. Der Datenstrom, insofern es nicht darum geht, wer ihn bearbeitet, sondern wer ihn kontrolliert oder besitzt, fliesst vom Rest der Welt zu Unternehmen mit Sitz in den USA und Europa, fliesst von nichtweissen Menschen zu weissen, von Frauen zu Männern. Einerseits zerstört die Datenwelt alte Hierarchien; andererseits scheint sie ihnen allzu willfährig zu gehorchen.
Damoklesschwert des Illiberalismus

Wir fetischisieren also die angebliche Gleichheit der Daten, die ja nur aus Einsen und Nullen bestehen, und wir vernachlässigen, dass gesellschaftlich manche Daten eindeutig gleicher sind als andere. Wir werden alle ausgespäht, aber mit äusserst unterschiedlichen Konsequenzen. Daten nivellieren, aber die Nivellierung trifft keine zwei Menschen exakt gleich. Indem wir die Selbstbestimmung über die eigenen Daten an Bürgerrechte geknüpft haben, ist sie immer dort am schwächsten, wo wir keine Bürgerrechte geniessen. Mobilität, Migration und Flucht legitimieren den Zugriff auf die Daten der Betroffenen und führen diese Daten in neue Kontexte ein.

Mittels sozialer Netzwerke vermag Immigration and Customs Enforcement in den USA schnell ein Netzwerk von «known associates» zu erstellen – eine Technik, die ursprünglich zur Terrorismusbekämpfung entwickelt wurde. Hier scheint es bereits ausreichend, von einem von ICE festgesetzten Migranten aus der Abschiebehaft angerufen zu werden und sich auf Spanisch zu unterhalten, um auf die Liste zu geraten. Es ist ganz klar, dass in Fällen wie diesem Daten nicht gleich Daten sind. Das Potenzial des Staates, Macht zu zeigen, unterscheidet sich ganz eklatant von der Entscheidung, diese wirklich auszuüben. Egal, was sie sonst versprechen, personenbezogene Daten, so scheint es, sind das ewig drohende Damoklesschwert des Illiberalismus. Egal, wie stark der Staat sie einhegt – sie baumeln über dem Bürger, und gerade dem Nichtbürger, als ständige Erinnerung, was sein und was kommen könnte. ";https://www.nzz.ch/meinung/im-rausch-des-sammelns-von-einsen-und-nullen-ld.1519978;NZZ;Adrian Daub;;;
14.01.2017;Folgen der vierten industriellen Revolution;"In den vergangenen zwölf Monaten waren in vielen Industrieländern soziale und politische Gegenreaktionen auf die Globalisierung zu beobachten. Deindustrialisierung, negative Entwicklungen auf den Arbeitsmärkten und schlechte Konjunkturprognosen waren der Katalysator für sogenannte Anti-Establishment-Wahlen.
Nicht die Globalisierung ist schuld an der Arbeitsmarktmisere

Viele sehen darin ein Misstrauensvotum gegen die liberalen Zielsetzungen von Freihandel und Förderung globaler Wirtschaftsbeziehungen der letzten Jahrzehnte. Doch vieles weist darauf hin, dass nicht die Globalisierung schuld an der Arbeitsmarktmisere ist, sondern der technologische Wandel. Produktion, Mobilität, Kommunikation, Energie und viele andere Bereiche verändern sich derzeit so rasant wie nie zuvor. Und genau das setzt unsere Arbeitswelt, den gesellschaftlichen Zusammenhalt und die geopolitische Stabilität unter Druck.

Mit ihrer machtvollen Konvergenz von Digital-, Bio- und physischen Technologien stellt die vierte industrielle Revolution die Arbeitsmärkte auf den Kopf. Und so ist auch die Anti-Globalisierungs-Rhetorik der Populisten eine Fehldiagnose: Die von ihnen vorgeschlagenen Heilmittel werden nichts bewirken. Laut den Wirtschaftsexperten Michael Hicks und Srikant Devaraj sind 86 Prozent des Stellenabbaus in der US-Industrie zwischen 1997 und 2007 auf Produktivitätssteigerungen zurückzuführen, nur 14 Prozent auf den Handel. In Grossbritannien ist der Anteil der Produktion an der Gesamtwirtschaft zurückgegangen – doch was produziert wird, ist höherwertig; zudem hat sich die Aussenwirtschaft enorm entwickelt.

Die meisten Analysen legen nahe, dass sich die negativen Auswirkungen des technologischen Wandels auf die Arbeitsmärkte in den kommenden Jahren noch beschleunigen werden. Durch die rasante Weiterentwicklung von Robotik, Sensoren und maschinellem Lernen wird in immer mehr Berufen Arbeit durch Kapital ersetzt. Die Schätzungen dazu, wie viele Jobs verdrängt werden, sind unterschiedlich, aber eine häufig zitierte Studie der Oxford Martin School aus dem Jahr 2013 geht davon aus, dass in den USA 47 Prozent aller Arbeitsplätze bedroht sind. Eine Studie von McKinsey von 2015 kommt zum Schluss, dass 45 Prozent aller von Menschen verrichteten Arbeiten schon heute automatisiert werden könnten. Und die Teilnehmer des «Global Risks Perception Survey» des Weltwirtschaftsforums – Grundlage unseres «Global Risks Report 2017» – halten künstliche Intelligenz und Robotik für die neuen Technologien, die in den nächsten zehn Jahren die negativsten Auswirkungen haben könnten.

    Nach Schätzungen der Oxford Martin School gehören nur 0,5 Prozent aller Arbeitnehmer in den USA Branchen an, die seit der Jahrtausendwende neu entstanden sind.

Technische Innovationen schaffen nicht nur Arbeitsplätze, sie vernichten sie auch. Doch die neuen Technologien sind besonders schlechte Jobmotoren. Nach Schätzungen der Oxford Martin School gehören nur 0,5 Prozent aller Arbeitnehmer in den USA Branchen an, die seit der Jahrtausendwende neu entstanden sind. Zum Vergleich: Rund 8 Prozent sind in Sektoren tätig, die es schon seit den 1980er Jahren gibt. Mit dem technologischen Wandel verschiebt sich die Einkommensverteilung von der Arbeitskraft auf das Kapital. Laut einem Bericht der OECD wurde der Rückgang des Lohnanteils am Volkseinkommen zwischen 1990 und 2007 bis zu 80 Prozent von neuen Technologien verursacht. Weltweit werden immer mehr Menschen abgehängt: Über 4 Milliarden haben keinen Zugang zum Internet, 1,2 Milliarden haben keinen Strom.

Doch wir können die Dynamik der technologischen Revolution beeinflussen – indem wir sie umsichtig steuern und die Vorteile und Lasten gleichmässiger verteilen. Denn so viel steht fest: Die künftige Entwicklung von neuen Technologien wird stark von gesellschaftlichen Normen, wirtschaftlichen Verhaltensregeln, Branchenstandards und Gesetzen beeinflusst sein, die wir heute diskutieren und festschreiben.

Populistische Bewegungen kaprizieren sich eher auf Globalisierungsängste als auf die Angst vor dem technischen Fortschritt. Aber auch hier ist mit Gegenreaktionen zu rechnen. So besteht beispielsweise immer noch ein starkes Misstrauen gegen Gen-Food, obwohl die Wissenschaft die Risiken für geringer hält. Und die Sorge in Bezug auf den Klimawandel verhindert nicht den öffentlichen Protest gegen Windparks.
Schwindende gesellschaftliche Kohäsion

Wir befinden uns in einer Umbruchphase der technologischen Entwicklung und in einer Zeit schwindender gesellschaftlicher Kohäsion und politischer Legitimation. Und angesichts der vierten industriellen Revolution, durch die globale Risiken generiert und noch verstärkt werden können, wird deutlich, wie dringend wir handeln müssen. Es kommt nun darauf an, dass Entscheider und Stakeholder in Politik, Gesellschaft, Bildung und Medien gemeinsam effiziente und praktikable Formen der lokalen, nationalen und globalen Steuerung und Risikovermeidung erarbeiten.";https://www.nzz.ch/meinung/globalisierung-und-beschaeftigung-folgen-der-vierten-industriellen-revolution-ld.139753;NZZ;Klaus Schwab;;;
26.01.2019;Europa bläst beim Bau eines Quantencomputers zur Aufholjagd – doch etwas Entscheidendes fehlt noch;"Im Wettlauf um den Quantencomputer geben die USA und China das Tempo vor. Dort pumpen Regierungen und Unternehmen wie Google oder Alibaba, das chinesische Pendant zum Onlinehändler Amazon, Milliardenbeträge in die Entwicklung der «Wundermaschine». Diese soll dank den Gesetzen der Quantenphysik bestimmte Aufgaben sehr viel schneller lösen als jeder Supercomputer. Wem es gelingt, winkt enormer Profit.
Zögerliche Industrie

Vor Jahren schon habe man in den USA von der akademischen Forschung am Quantenrechner auf dessen ingenieurmässige Entwicklung umgeschaltet, sagt Frank Wilhelm-Mauch von der Universität des Saarlandes. Der Physiker leitet eines von zwei EU-Projekten, mit denen Europa nachziehen will. Indem man eine Brücke zwischen der Forschung und der Industrie schlägt, möchte der alte Kontinent seine führende Position in der Forschung – einer der weltweit am weitesten entwickelten Quantenrechner steht in Innsbruck – in kommerzielle Produkte ummünzen. Die beiden Projekte zum Bau eines Quantencomputers werden im Rahmen des Quanten-Flaggschiffprogramms der EU mit je zehn Millionen Euro gefördert. Damit die Kommerzialisierung gelingen kann, ist die Einbindung der Industrie essenziell. Bis anhin zeigen europäische Technologieunternehmen jedoch keine Ambitionen, tatsächlich einen Quantenrechner zu bauen. Beim Bau eines Quantenrechners dreht sich alles um das «Qubit». Es kann die beiden Bit-Werte 0 und 1 nicht nur nacheinander speichern wie ein klassisches Bit, sondern simultan. Realisiert wird das Qubit beispielsweise durch Ionen (geladene Atome), die zwei Energiezustände gleichzeitig einnehmen, oder supraleitende Schleifen, in denen Strom gleichzeitig links und rechts herum fliesst.

Weil jedes zusätzliche Qubit die Zahl der speicherbaren Werte verdoppelt, könnte schon ein Quantencomputer mit etwa 50 Qubits bestimmte Aufgaben lösen, die heutige Supercomputer überfordern, etwa das Simulieren eines Moleküls aus 50 Atomen. Dies wäre jedoch erst der Anfang. Um klassische Rechner auch auf Gebieten wie der Mustererkennung, dem maschinellen Lernen oder der Optimierung von Verkehrsströmen zu schlagen, brauchte es Quantenrechner mit Tausenden von Qubits. Bis dahin werden noch mindestens zehn Jahre vergehen, schätzen Physiker.

Binnen drei Jahren wollen die europäischen Projekte immerhin die Marke von 50 bis 100 Qubits erreichen: Das Projekt Aqtion (Advanced Quantum Computing with trapped Ions) an der Universität Innsbruck setzt auf gespeicherte Ionen, das Projekt Opensuperq (An Open Superconducting Quantum Computer) am Forschungszentrum Jülich auf supraleitende Quantenbits. Eine Kopie des Jülicher Rechners solle in Zürich gebaut werden, sagt der am Vorhaben beteiligte Quantenforscher Andreas Wallraff von der ETH Zürich. In Opensuperq sieht der Physiker eine neue Herangehensweise. Diese erfordere nicht nur das Lösen offener Forschungsfragen, etwa wie sich Tausende von Qubits steuern liessen, ohne Fehler zu verursachen, sondern auch das Know-how von Ingenieuren. «Wir haben die Komponenten, nun müssen wir sie zusammenbauen», sagt Wallraff. Ein einzelnes Labor könne dies nicht leisten. Das sei ähnlich wie beim Bau von Detektoren am Kernforschungszentrum Cern. Hier tragen mehr als 100 Institute Komponenten bei, die aufeinander abgestimmt sein müssen. Dass es für den Bau eines Quantencomputers eine grosse gemeinschaftliche Anstrengung braucht, sieht auch Wallraffs ETH-Kollege Jonathan Home so, der am Aqtion-Projekt beteiligt ist. Für ihn sei vor allem die andere Arbeitsweise eine neue soziale Erfahrung.

Die vordringlichste Aufgabe sieht Wallraff darin, eine Infrastruktur aufzubauen, auf der ein leistungsstarker Quantencomputer überhaupt gebaut werden kann. «Die knappste Ressource sind Experten», sagt Wallraff. Er regt daher an, gezielt Quanten-Ingenieure auszubilden. Wallraff lobt das Engagement der EU, bezweifelt aber, dass die Geldmittel reichen werden, um an den USA vorbeizuziehen. Über kurz oder lang brauche es das Investment einer grösseren Firma.

Viele Kandidaten gibt es dafür in Europa nicht. Experten nennen oft Siemens oder den französischen IT-Konzern Atos. In der Szene wird aber auch die mangelnde Risikobereitschaft europäischer Unternehmen kritisiert, in eine Entwicklung zu investieren, die keine kurzfristigen Erlöse verspricht. Auch bei den beiden Quantencomputer-Projekten halten sich Firmen finanziell zurück. Das Budget stammt zur Gänze aus dem EU-Flaggschiffprogramm. Die kleinen Firmen, die an den beiden Projekten beteiligt sind, steuern zwar Steuerelektronik, Lasertechnik oder Kühlapparate bei. Als Hersteller von Quantencomputern sind sie aber kaum geeignet. Einzig das Startup Alpine Quantum Technologies, eine Ausgründung der Universität Innsbruck, hat sich das Ziel gesetzt, einen kommerziellen Quantencomputer zu bauen. Dafür hat es zehn Millionen Euro vom österreichischen Staat bekommen.
Was Quantencomputer können

Somit müssen die beiden EU-Projekte die Industrie erst einmal vom kommerziellen Nutzen eines Quantenrechners überzeugen. «Wir streben Firmenkontakte an», sagt Wallraff. Dabei soll ein «user board» helfen, in dem Firmen wie Siemens, Carl Zeiss, Volvo oder Ericsson vertreten sind. «Die Firmen werden ein Gefühl dafür bekommen, was mit einem Quantencomputer machbar ist», verspricht Home. Auch bei Aqtion gebe es Firmenkontakte, die Anwendungen einen Quantencomputers ausloten wollen, wie Thomas Monz von der Uni Innsbruck bestätigt.

Immerhin könnte ein europäischer Quantenrechner die heimische Softwareindustrie befeuern. Denn der Wunderrechner benötigt spezielle Algorithmen. Der Softwarehersteller Atos investiert in dieses Feld, indem es Quanten-Algorithmen auf einem eigenen Supercomputer testet. Der Jülicher Quantencomputer soll über die Cloud zugänglich sein und somit Unternehmen ermöglichen, ihre Software zu testen. Bisher bieten nur US-Firmen und Alibaba solche Dienste an.

Mit dem neuen Quanten-Flaggschiff ist Europa also nun im Rennen um die Kommerzialisierung eines künftigen Quantenrechners. Für einen Spitzenplatz dürfte das Förderinstrument aber wohl nicht reichen.";https://www.nzz.ch/wissenschaft/quantencomputer-europa-moechte-anschluss-an-die-usa-wahren-ld.1453642;NZZ;Christian J. Meier;;;
02.08.2019;Künstliche Intelligenz erkennt auf dem EKG «stumme» Herzrhythmusstörung;"Was Forscher von der Mayo Clinic in Florida, USA, in der Fachzeitschrift «The Lancet» berichten, klingt spektakulär. Mit Hilfe von künstlicher Intelligenz (KI) können sie in einem gewöhnlichen Elektrokardiogramm (EKG) die häufigste Herzrhythmusstörung, das sogenannte Vorhofflimmern, erkennen – und das auch dann, wenn das Flimmern gar nicht vorhanden ist.

Das ist deshalb bedeutsam, weil viele Patienten unter einem sogenannten intermittierenden Vorhofflimmern leiden. Die Herzrhythmusstörung ist bei ihnen nicht immer, sondern nur ab und zu aktiv, was die Diagnose schwierig macht und oft um Monate bis Jahre verzögert. Ohne Diagnose fehlt aber eine adäquate Therapie, was für die Patienten schwerwiegende Konsequenzen haben kann. Denn das Vorhofflimmern geht mit einem erhöhten Risiko für – durch Gerinnsel verursachte – Hirnschläge einher, weshalb die Patienten blutverdünnende Medikamente einnehmen sollten.
Training an 650 000 EKG

Das an der Mayo Clinic entwickelte KI-Diagnoseinstrument basiert auf einem neuronalen Netzwerk, das anhand von Übungsdaten selber lernen kann. Für diesen «deep learning» genannten Vorgang stellten die Wissenschafter dem Netzwerk fast 650 000 Standard-EKG von 180 000 Personen zur Verfügung. Auf den zwischen 1993 und 2017 aufgezeichneten «Herzstromkurven» war kein Vorhofflimmern sichtbar. Aus anderen Tests wie Langzeit-EKG war aber bekannt, dass ein Teil der Personen ein intermittierendes Vorhofflimmern hatte.

Nach der Lernphase erkannte die KI bei 79 Prozent der Patienten mit verborgenem Vorhofflimmern die Störung korrekt. Und das, wenn der Maschine für die Beurteilung nur eine einzige, zehn Sekunden dauernde EKG-Aufnahme zur Verfügung stand. Gleich gut war die Erkennungsrate bei Patienten ohne Vorhofflimmern; auch hier beurteilte der Algorithmus 79 Prozent der Fälle richtig. Die auch als Sensitivität und Spezifität bezeichnete Treffsicherheit liess sich auf 83 Prozent steigern, wenn der KI für ihre Beurteilung mehrere EKG vom gleichen Patienten zur Verfügung standen.

Dass mit KI ein Vorhofflimmern erkannt werden kann, das bei der Durchführung des EKG nicht vorhanden ist, kommentiert der Studienleiter Paul Friedman von der Mayo Clinic in einer Medienmitteilung folgendermassen: «Das ist, wie wenn wir heute aufs Meer blicken und sagen könnten, dass es gestern grosse Wellen gab.»
Breite Validierung vor klinischem Einsatz

Laut einem anderen Studienautor könnte das neue Diagnoseprogramm möglicherweise auf Smartphones laufen. Vor einer klinischen Anwendung seien aber erst noch weitere Studien und damit eine breite Validierung der Ergebnisse notwendig. Dieser Meinung sind auch Forscher, die nicht an der Arbeit beteiligt waren. Es müsse zum Beispiel nachgewiesen werden, dass die KI-Diagnostik die Situation des Patienten wie erhofft verbessere, sagen sie. Denn die Maschine könne das Vorhofflimmern ja nicht beweisen, sondern nur eine mehr oder weniger zuverlässige Aussage zur Wahrscheinlichkeit der Rhythmusstörung machen.

Als weiterer Negativpunkt wird von Experten angeführt, dass man beim eingesetzten Algorithmus – wie das meist bei KI-Anwendungen der Fall ist – nicht im Detail verstehe, wie die maschinelle Beurteilung zustande komme. Was der Algorithmus auf den EKG sieht, bleibt somit unklar.

Fachleute vermuten, dass die künstliche Intelligenz kleine, für das menschliche Auge nicht sichtbare Veränderungen erkennt, welche die elektrische Aktivität auf Ebene der Herz-Vorhöfe repräsentieren. Solche Veränderungen – sie gehen fast immer mit einem strukturellen Umbau im Herzen einher – könnten die Vorboten eines künftigen oder wiederkehrenden Vorhofflimmerns sein. Erst wenn dieses aktiv ist, lässt es sich im EKG vom Arzt nachweisen (siehe Bild oben). Dann aber ist der Fall eindeutig.";https://www.nzz.ch/wissenschaft/ki-erkennt-auf-gewoehnlichem-ekg-stummes-vorhofflimmern-ld.1499619;NZZ;Alan Niederer;;;
08.12.2020;Quantencomputer: China präsentiert einen Rechner, der herkömmlichen Computern haushoch überlegen ist. Allerdings kann er nur eines gut;"Die neuesten Entwicklungen

Chinesische Forscher haben mit einem Quantencomputer in 200 Sekunden ein Problem gelöst, das mit herkömmlichen Computern wohl nie zu knacken sein wird. Wie die Gruppe um Jian-Wei Pan Anfang Dezember im Wissenschaftsmagazin «Science» dargelegt hat, würde der beste chinesische Supercomputer für die Berechnung 2,5 Milliarden Jahre benötigen. Damit ist der Quantencomputer um einen Faktor 1014 im Vorteil.

Die Meldung erinnert an den Durchbruch, den Google vor einem Jahr feierte. Tatsächlich gibt es einige Parallelen. So hat das sogenannte Boson-Sampling-Problem, das die chinesischen Forscher nun gelöst haben, ebenfalls keine praktische Relevanz. Und es kommt der Arbeitsweise des verwendeten Quantencomputers ebenfalls maximal entgegen.

Das Problem des Boson-Sampling erinnert ein wenig an das Spiel, bei dem Kugeln in einen Kasten geworfen werden, in dem sich die Bahnen mehrfach verzweigen. Die Aufgabe besteht darin zu berechnen, mit welcher Wahrscheinlichkeit die Kugeln in den verschiedenen Ausgängen des Kastens landen. Diese Berechnung ist ziemlich trivial, solange es sich um gewöhnliche Kugeln handelt. Spielt man das Spiel allerdings mit Photonen (also mit Teilchen, die der Bose-Statistik genügen), wird die Berechnung höllisch kompliziert. Die Photonen können nämlich miteinander interferieren. Mit der Zahl der Photonen und der Zahl der Verzweigungen wächst die Zahl der zu berechnenden Möglichkeiten rasch ins Astronomische. Eher früher als später zwingt das jeden Supercomputer in die Knie.

Der Quantencomputer der chinesischen Forscher löst das Problem ganz anders. Er besteht aus Spiegeln und Strahlteilern, die ein hundertfach verzweigtes optisches Netzwerk bilden. Die Forscher schickten 50 Photonen gleichzeitig in das Labyrinth und registrierten mit einem Photonenzähler, durch welche Ausgänge sie das Netzwerk wieder verlassen. Die «Berechnung» läuft also darauf hinaus, die Wahrscheinlichkeitsverteilung der Photonen zu messen.

Mit dem, was man sich unter einem universell programmierbaren Quantencomputer vorstellt, hat das wenig zu tun. Andere Aufgaben lassen sich mit dem Quantencomputer nicht lösen. Seine gesamte Arbeitsweise ist auf das Boson-Sampling zugeschnitten. Hier kann ihm allerdings kein noch so schneller Supercomputer das Wasser reichen. Google setzt einen Meilenstein

Im September 2019 löste ein Quantencomputer zum ersten Mal eine Aufgabe, die jeden existierenden klassischen Computer überfordert. Der von Google entwickelte Prozessor berechnete das Problem in Minutenschnelle. Ein Superrechner würde für die gleiche Aufgabe Jahrtausende brauchen. Damit hat Google bewiesen, dass Quantencomputer herkömmlichen Rechnern zumindest in einigen Belangen überlegen sein können. Auch wenn die gelöste Aufgabe für die Praxis irrelevant und auf die Arbeitsweise des Quantencomputers zugeschnitten ist, wird das Ergebnis weltweit als Durchbruch gefeiert.

Der Prozessor von Google besteht aus 53 supraleitenden Quantenbits. Auf diesem Prozessor haben die Forscher eine Art Zufallsalgorithmus laufen lassen, der eine Abfolge von Nullen und Einsen generiert. Dieser Vorgang wurde millionenfach wiederholt. Auf diese Weise entstand innerhalb von wenigen Minuten eine riesige Sammlung von Bit-Folgen. Weil die Quantenbits des Prozessors miteinander interferieren, sind einige Bit-Folgen wahrscheinlicher als andere. Die entsprechende Wahrscheinlichkeitsverteilung lässt sich im Prinzip auch mit einem herkömmlichen Computer berechnen. Die Forscher von Google haben jedoch abgeschätzt, dass das mit dem schnellsten derzeitigen Supercomputer 10 000 Jahre dauern würde. Nur wenige Tage nach der Veröffentlichung der Resultate meldeten Forscher von IBM Zweifel an der Überlegenheit des Quantencomputers an. Sie machten geltend, die Google-Forscher hätten die Fähigkeiten des Supercomputers unterschätzt. Mit zusätzlichem Speicherplatz sei dieser in der Lage, das Problem in 2,5 Tagen statt in 10 000 Jahren zu lösen.

Unbestritten ist aber, dass der Quantencomputer von Google bei der Lösung dieses speziellen Problems einen Geschwindigkeitsvorteil gegenüber Supercomputern hat. Und unbestritten ist auch, dass dieser Vorteil mit jedem zusätzlichen Quantenbit immer grösser wird.
Was ist ein Quantencomputer?

Ein Quantencomputer rechnet mit einzelnen Teilchen, die den Gesetzen der Quantenphysik gehorchen. Das können zum Beispiel Elektronen, geladene Atome (Ionen) oder Lichtquanten sein. Diese Quantenobjekte zeigen ein Verhalten, das man aus der klassischen Physik nicht kennt. So kann sich ein Atom an zwei Orten gleichzeitig aufhalten oder sich wie eine Welle ausbreiten. Teile dieser Welle können sich überlagern und auslöschen.

Die Idee, die Effekte der Quantenphysik für Rechner zu nutzen, die schneller arbeiten als jeder Supercomputer, versuchen Forscher seit 25 Jahren zu verwirklichen. Sie nutzen dabei die Tatsache, dass sich quantenmechanische Teilchen gleichzeitig in zwei verschiedenen Zuständen befinden können. Macht man diese Objekte zu Informationsträgern, so können diese sogenannten Qubits gleichzeitig die Werte 0 und 1 speichern. Das unterscheidet sie von herkömmlichen Bits, die sich entweder im Zustand 0 oder im Zustand 1 befinden. Jedes zusätzliche Qubit verdoppelt die Anzahl der simultan speicherbaren Werte. Schon rund 300 Qubits genügen, um mehr Zahlen aufzunehmen, als das Universum Teilchen besitzt.

Um mit einem Quantencomputer zu rechnen, muss man die Qubits gemäss einem Algorithmus gezielt ansprechen und miteinander verknüpfen. Auf diese Weise kann man die Unzahl der gespeicherten Werte simultan verarbeiten, sprich viele Rechenwege parallel gehen. Das Problem: Eine Beobachtung der Qubits greift willkürlich eine der Alternativen heraus. Ein sinnvolles Ergebnis produziert ein Quantencomputer daher nur, wenn der Algorithmus die Rechenwege so geschickt steuert, dass sich die falschen Varianten wie Wellen gegenseitig auslöschen und bis zur Messung nur der richtige übrig bleibt.
Was lässt sich mit einem Quantenrechner (nicht) machen?

Seine Stärken spielt der Quantencomputer dort aus, wo die Zahl der Lösungsmöglichkeiten ins Unermessliche wächst. Die Frage, welche Rundtour durch fünf Städte die kürzeste ist, lässt sich mit Papier und Bleistift lösen. Bei fünfzehn Städten gibt es schon rund 87 Milliarden möglicher Routen, jede Station mehr vervielfacht diese Zahl noch. Ein konventioneller Rechner, zukünftige eingeschlossen, steigt beim klassischen «Problem des Handlungsreisenden» früher oder später aus. Ein Quantencomputer kann derlei Probleme jedoch in den Griff bekommen. Denn mit jedem zusätzlichen Qubit verdoppelt sich seine Rechen-Power, sie wächst exponentiell. Ähnlich komplexe Aufgaben wie das Problem des Handlungsreisenden finden sich beim maschinellen Lernen oder beim Brechen von Verschlüsselungen. Quantencomputer würden riesige Datenmengen schnell nach versteckten Muster durchforsten, glauben Experten. Das Problem: Für jede neue Anwendung müssen Programmierer einen Trick finden, um die Lösungswege so zu überlagern, dass die optimale Lösung herausgefiltert wird. Bis jetzt existieren erst wenige solcher Algorithmen. Daher fällt es schwer, das tatsächliche Potenzial der neuen Art von Rechnern einzuschätzen.

Bei einer Anwendung sind sich jedoch die meisten Experten einig, dass sie kommen wird: die Simulation von Molekülen oder Kristallen. Klassische Rechner tun sich schwer damit, weil sie die vielen Möglichkeiten, die sich laut Quantenphysik in einem Molekül versammeln, nur durch aufeinanderfolgende Rechenschritte simulieren können. Daher scheitern selbst Superrechner an der Simulation von Molekülen mit mehr als 50 Atomen. Ausreichend grosse Quantencomputer werden vermutlich deutlich grössere Verbindungen simulieren können, wie sie etwa in der Biologie vorkommen. Das könnte etwa die Wirkstoffsuche in der Pharmabranche verbilligen und beschleunigen.

Auch wenn es in Jahrzehnten einen Quantenrechner gibt, der flexibel programmierbar ist: Als Tablet im Alltag wird er wohl nicht dienen. Für Aufgaben wie Textverarbeitung wird sich weiter der klassische Rechner am besten eignen.
Ist der Quantencomputer eine Gefahr für die Sicherheit im Internet?

Der amerikanische Mathematiker Peter Shor schuf im Jahr 1994 eine grosse Motivation, Quantencomputer zu entwickeln. Der nach ihm benannte Algorithmus knackt gängige Verschlüsselungsverfahren, vorausgesetzt, es steht ein Quantenrechner mit mehreren Tausend Qubits bereit. Dies betrifft sogenannte asymmetrische Kryptoverfahren, bei denen die Partner keinen gemeinsamen Schlüssel austauschen müssen. Sie sind im Internet weit verbreitet, werden etwa beim Online-Banking genutzt oder für digitale Signaturen, die die Urheberschaft nachweisen.

Um den Code zu knacken, findet der Quantencomputer heraus, welche Primzahlen man multiplizieren muss, um eine bestimmte Zahl mit mehreren hundert Stellen zu erhalten. Ein klassischer Rechner müsste, grob gesagt, alle Möglichkeiten durchtesten – mehr, als es Teilchen im Universum gibt. Ein PC brauchte dafür Jahrmilliarden. Ein leistungsstarker Quantenrechner schafft es binnen Minuten. Schon jetzt raten Experten warnend dazu, Daten, die mehr als ein Jahrzehnt geheim bleiben sollen, nicht mehr mit den betreffenden Verfahren zu sichern.

Derzeit wird eine «Post-Quanten-Kryptografie» entwickelt. Sie nutzt mathematische Methoden, die Quantencomputer wohl nicht knacken werden. Für digitale Signaturen gibt es bereits einsatzreife Alternativen, auch für Verschlüsselungen werden solche entwickelt. Dank dieser Post-Quantum-Kryptografie dürfte der Quantencomputer eine weit geringere Bedrohung darstellen als anfangs gedacht.
Warum ist es schwer, einen Quantenrechner zu bauen?

Ein Qubit ist empfindlicher als jedes Soufflee: Es reagiert auf den leisesten Umwelteinfluss. Schon der Stoss eines Luftmoleküls oder die Einstrahlung von Wärme kann aus dem Überlagerungszustand eines Quantenbits einen klassischen Zustand machen. Daher schützen Forscher ihre Qubits aufwendig durch Vakuum, sehr tiefe Temperaturen und Abschirmungen. Dennoch halten Qubits ihren Überlagerungszustand meist nur Bruchteile von Sekunden. Ein Algorithmus, der Minuten andauert, kann so nicht zu Ende geführt werden. Forschern gelingt es zwar, die Qubits immer länger intakt zu halten. Doch Fehler bei der Ausführung von Rechenschritten entstehen noch viel zu oft. Sie sollen deshalb in Zukunft vom Quantenrechner selbst erkannt und korrigiert werden. Dann braucht er aber für jedes rechnende Qubit mehrere zusätzliche Korrektur-Qubits. Dieser Wasserkopf könnte eine leistungsstarke Maschine auf Millionen Qubits aufblähen, fürchten Forscher.

Derzeit weiss noch niemand, wie man einen fehlerkorrigierenden, leistungsstarken Quantenrechner baut. Das ist Grundlagenforschung.
Wann kommt der Durchbruch?

Ein flexibel einsetzbarer, sprich frei programmierbarer Quantencomputer werde frühestens um 2030 zu erwarten sein, sind sich Experten einig. Manche glauben, dass es wesentlich länger dauern werde. Wenn es überhaupt den einen grossen Durchbruch geben wird. Denkbar ist auch eine Evolution, angefangen bei kleinen Quantenrechnern, die eng umrissene Teilaufgaben lösen und den Rest einem klassischen Computer überlassen, über solche, die einzelne Aufgaben wie Optimierung gut beherrschen, bis hin zum frei programmierbaren Quantencomputer.

Der Quantencomputer, den Google im Jahr 2019 präsentiert hat, markiert den Anfang dieser Entwicklung. Wir befinden uns nun in der Ära von mittelgrossen Quantencomputern, deren Qubits noch nicht fehlerkorrigierend sind. Solche Qubits könnten gerade stabil genug sein, um begrenzte Rechnungen durchzuführen, etwa Teile der Simulation von Molekülen. Es ist daher vorstellbar, dass solche Quantencomputer zunächst als Koprozessoren von klassischen Rechnern fungieren. Dies könnte schon in wenigen Jahren gelingen – vorausgesetzt, die Qualität von Qubits ohne Fehlerkorrektur reicht dafür aus. Ist das nicht der Fall, wäre auch vorstellbar, dass man mit einer kleinen Zahl von fehlerkorrigierenden Quantenbits beginnt und die Zahl der Bits nach und nach erhöht.

Es könnte also auf eine Reihe von kleineren Erfolgen hinauslaufen, die schon in wenigen Jahren beginnt. Der eine grosse Durchbruch kommt vielleicht nie.
Wer entwickelt Quantencomputer, und wer hat die Nase vorn?

Nimmt man das mediale Echo als Massstab, haben die grossen amerikanischen Firmen Google und IBM die Nase vorne. Auch, was die blosse Anzahl von Qubits angeht, sind diese Unternehmen führend. IBM bietet einen Cloud-Dienst an, über den Forscher ihre Algorithmen auf Quantenprozessoren testen können. Google und IBM wollen möglichst schnell praxistaugliche Anwendungen ihrer Maschinen finden. In den USA gibt es ausserdem eine Startup-Szene, die, ausgestattet mit Hunderten Millionen Dollar Wagniskapital, Soft- und Hardware für Quantencomputer entwickelt.

Europa hat einen Spitzenplatz in der akademischen Forschung inne. Um den Status zu halten, appellierten Physiker des alten Kontinents 2016 an die EU-Kommission, den Bereich stärker zu fördern. Diese hörte den Ruf und setzte zwei Jahre später ein Flaggschiff-Programm für Quantentechnologien mit einem Budget von einer Milliarde Euro auf. In dessen Rahmen sollen bis 2021 zwei europäische Quantencomputer entstehen. Einer davon ähnelt den Maschinen von Google und IBM, der andere nutzt Qubits aus Ionen. Bei dieser Art von Quantencomputer sind Physiker aus Innsbruck führend.

Auch China investiert Milliarden Dollar, etwa in den Aufbau eines nationalen Labors für Quantentechnologie nahe der Stadt Hefei. Der Onlinehändler Alibaba entwickelt Quantencomputer und bietet über die Cloud Zugriff auf einen Quantenrechner mit 11 Qubits.

Ob die USA, Europa oder China die Nase vorne haben, lässt sich schwer sagen. Zu ungewiss ist noch, wie die Entwicklung weitergeht. Einzelne Institutionen würden sich wegen der Komplexität der Technologie schwertun, betonen Forscher oft. Demnach braucht es ein funktionierendes Ökosystem aus Physikern, Ingenieuren, Softwareentwicklern und Anwendern, um zum Ziel zu gelangen. Wer dies als Erster zustande bringt, hat gute Chancen, die Führung zu übernehmen.";https://www.nzz.ch/wissenschaft/quantencomputer-was-von-ihnen-zu-erwarten-ist-und-was-nicht-ld.1517345;NZZ;Christian Speicher, Christian J. Meier;;;
05.10.2016;«Warum nicht von Grösserem träumen?»;"Matthias Vanoni ist kein typischer Startup-Gründer: Zu geerdet, zu wenig quecksilbrig wirkt der 30-jährige kräftige Franzose. Man würde einen Polizisten in ihm vermuten – und läge damit nicht falsch. Tatsächlich hatte Vanoni, bevor er Biowatch gründete, mehrere Jahre lang als Ermittler und Leiter eines Überwachungsteams in der Gendarmerie Frankreichs gearbeitet. Später kam er in einer mobilen Anti-Riot-Einheit zum Einsatz, von Paris bis Guadeloupe. Es war ein abwechslungsreicher und nicht zu schlecht bezahlter Job mit hoher Arbeitsplatzsicherheit.
Gründen mit Lust und Kalkül

Das hat Vanoni aber nicht gereicht, und daher sitzt er derzeit für 1500 Franken im Monat im Startup-Hub in Zürich Selnau, den der Kickstart Accelerator aufgebaut hat (siehe Kasten). Vanoni hat Mathematik studiert, bevor er an der ETH Lausanne im Doktorat zur biometrischen Erkennung forschte. «Mich hat schon immer fasziniert, wie man Personen anhand von Spuren finden kann, die sie an einem Tatort zurücklassen.» Bereits in seinem Doktorat beschäftigte er sich mit der Wiedererkennung von Personen über ihre Venen. «Seither hat mich die Idee von Biowatch nicht mehr losgelassen, es war zu sexy.»

Biowatch wird von mehreren Kennern der Startup-Szene als eines der interessantesten Schweizer Jungunternehmen bezeichnet. Die Firma arbeitet an einer Minikamera, die sich am Uhrenband befestigen lässt und es dem Träger ermöglicht, sich sofort und überall zu identifizieren – ohne dass er etwas tun muss. Das Modul scannt dafür das einzigartige Venenmuster am Handgelenk. Über Funkstandards wie Bluetooth oder NFC authentifiziert es ihn dann gegenüber Badge-Stationen, Autos oder der Migros-Kasse. «Unser Ziel ist es, PIN-Codes, Passwörter, Schlüssel und Fingerabdruck-Scanner zu ersetzen. Mit der Biowatch muss man sich niemals mehr aktiv authentifizieren, weil sie ein Wearable ist. Mit einem Mobiltelefon lässt sich das nicht machen.»

Dass aus der Idee ein Unternehmen mit inzwischen fünf Angestellten geworden ist, erklärt er nicht mit übermässiger Freude an der unsicheren Existenz des Selbständigen – «ich habe das Risiko nie gesucht», sagt er –, sondern mit einer kühlen Rechnung: «Selbst wenn die Chance nur 10 Prozent betrüge, dass Biowatch zum Milliardenerfolg wird, hätte ich einen höheren Erwartungswert an Einkommen, als wenn ich noch über 30 Jahre angestellt bin.»

Er liebe diese Rechnung vor allem, weil er die Erfolgschance mit seiner eigenen Arbeit erhöhen könne. Unwahrscheinlich bleibe ein Grosserfolg, doch es gehe im Leben ja auch um Hoffnung: «Warum also nicht von Grösserem träumen?»

Am Kickstart Accelerator geht es für ihn vor allem um die Chance, die Sponsoren zu treffen, die sich hier die Klinke in die Hand geben. Vanoni möchte diese wirtschaftlichen Schwergewichte als Partner und Kunden für Biowatch gewinnen. Er möchte die Firma lieber über Vorbestellungen mit Rabatten finanzieren als über den Verkauf von Firmenanteilen. Zudem will er die Grossen «evangelisieren» und ihnen zeigen, was mit der Biowatch technisch alles möglich ist, damit sie selbst Anwendungen suchen und so das Ökosystem des Produkts ausbauen. Und natürlich sei es auch toll, dass es der Kickstart Accelerator ihm ermögliche, elf Wochen in Zürich am Produkt zu arbeiten, sagt Vanoni (der im T-Shirt des Organisators zum Gespräch erscheint).
Der Weg ist noch weit

Bis der Mini-Scanner in Serie geht, liegt noch viel Arbeit vor Vanoni und seinem Team. Der Algorithmus, der die Venenbilder erkennt und einer Person zuordnet, muss mit Bildern von unzähligen Handgelenken gefüttert werden. Der Algorithmus bringt sich mit diesen Daten selbst bei, dieselben Venen unter verschiedenen Bedingungen zweifelsfrei zu erkennen. Je mehr Venen er gesehen hat, desto besser wird der Algorithmus – maschinelles Lernen, wie es auch selbstfahrende Autos tun. 250 Personen haben sich mittlerweile als «Venenlieferanten» zur Verfügung gestellt. «Wir möchten höchstens in einem von 10 000 Fällen Fehler haben. Das heisst: Wenn jemand deine Biowatch stiehlt, darf höchstens einer von 10 000 Dieben fälschlicherweise als richtiger Träger erkannt werden», sagt Vanoni.

Das fertige Produkt soll 2018 erscheinen, doch bis dahin ist es noch ein steiniger Weg. Die Module müssen dünn genug sein, die Kamera muss ohne externe Lichtquelle funktionieren, leicht gebogen sein und richtig fokussieren. Sind all diese Probleme gelöst, muss aus dem Prototyp ein Serienprodukt werden. «Das Schöne an einem Startup ist es zunächst, sich vorzustellen, was man alles tun könnte. Jetzt müssen wir das auch tun. Das ist die harte Arbeit!»

Die Idee zu Biowatch stammt vom britischen Forscher Joe Rice, der sie in den achtziger Jahren hat patentieren lassen. Vermarkten konnte er sie nie, andere Technologien setzten sich durch. Vanoni kam indes mit einer konkreten Geschäftsidee auf Rice zu, der nun als Co-Gründer für die technologische Weiterentwicklung zuständig ist. «Erst in unserer heutigen Umgebung – mit Smart Objects, mit NFC und Bluetooth – kann Biowatch seinen Vorteil ausspielen.»

Das Startup ist ein Rennen gegen die Zeit: Auch Apple oder Samsung suchen nach einer einfacheren Form der Authentifizierung. «Apple hat bereits das Ökosystem und hat mit der Apple Watch ein Wearable parat, ihnen fehlt aber noch die Technologie zur sicheren Authentifizierung», sagt Vanoni. Der Gigant aus Redwood könne Kunde oder gar Käufer von Biowatch werden, wenn man die Arbeit richtig mache – oder Konkurrent, falls man nachlasse.
Die grosse Freiheit lockt

Der perfekte Erfolg wäre es, wenn Biowatch an die Börse gehen könnte. «Doch erfolgreiche Börsengänge sind ziemlich selten», sagt der Franzose. Ein ausserbörslicher Verkauf des Unternehmens ist realistischer und könnte immerhin auch eine Wertung von mehreren hundert Millionen bringen.

Und dann? «Von meiner Persönlichkeit, von meinem Lebenslauf her würde ich danach wohl wieder ein Startup aufziehen wollen», sagt Vanoni. Ein Erfolg wäre für ihn Mittel zu grösserer Freiheit: «Arbeiten zu müssen, ist ja eine Art Sklaverei. Ich möchte mein Leben gerne ohne alle Ketten leben.» Etwas Dolce Vita wäre eine nette Nebenerscheinung, aber kaum Antrieb genug: «Sans doute, on voudrait un autre challenge!»";https://www.nzz.ch/zuerich/aktuell/firmengruender-matthias-vanoni-warum-nicht-von-groesserem-traeumen-ld.120322;NZZ;Andre Müller;;;
11.03.2019;Deepfakes: Kann ich überhaupt noch glauben, was ich sehe?;"Welche Informationen sind echt? Was ist Fake? Es ist die Debatte unserer Zeit.

Texte konnten schon immer gefälscht werden. Man verändere den Titel, spitze auf unzulässige Weise zu oder erfinde schlicht Begebenheiten – und schon sind Fake-News Realität. Gibt es dann noch eine gewisse Anzahl Menschen, die den gefälschten Text verbreiten, ist die Spirale in Gang gesetzt – und kaum mehr zu bremsen. «Fake-News gab es schon im Mittelalter», schrieb der Geschichtsprofessor Volker Reinhardt kürzlich in der NZZ. «Jeder, der schreibt, und zwar nicht nur über sich, inszeniert sich, und zwar oft so geschickt, dass er selbst daran glaubt.»

Dasselbe gilt für Bilder und Videos: Die Gefahr besteht hier, dass sie aus dem Kontext gerissen oder verkürzt wiedergegeben werden. Es sei kurz an zwei Beispiele aus den letzten Monaten erinnert: In Chemnitz entwickelte sich im vergangenen Herbst ein Streit über angebliche Hetzjagden von Deutschen auf Ausländer, und zwar nur, weil ein einziges verwackeltes Video dies zu belegen schien. Und in den USA soll eine Videoaufnahme belegt haben, dass Abtreibungsgegner und Trump-Fans bei einer Demonstration in Washington gegen Indigene vorgegangen sind.

Der Clou dabei: Die Videos waren nicht gefälscht. Sie zeigten einfach nur einen bestimmten Ausschnitt und liessen damit gleichzeitig entscheidende Informationen unberücksichtigt. Ja, in Chemnitz gab es offenbar eine zu verurteilende Aktion gegen Minderheiten. In Washington ebenso. Doch zahlreiche Berichte bzw. ein längeres Video zeigten dann, dass sich die ganze Situation anders abgespielt hatte als zunächst angenommen.
«Donald Trump ist ein Trottel»

Eine Stufe dreister ist der Vorgang bei Deepfakes – denn hier kommt eine Fälschung von Videoaufnahmen mittels künstlicher Intelligenz hinzu. Deepfakes bezeichnen Videos, in denen dank maschinellem Lernen Gesichter oder sonstige Elemente ausgetauscht und beispielsweise auch Stimmen imitiert oder übernommen werden können – ohne dass der Zuschauer dies bemerkt.

Man nehme also ein Video von Person A, vermische es mit einer Aufnahme von Person B, unterlege es mit einer authentischen Tonspur, und schon ist die perfide Fälschung erstellt – in der Theorie zumindest. Wird diese Technologie massentauglich, dann ist es plötzlich möglich, einem Menschen Worte in den Mund zu legen, die er nie gesagt hat. Welche abartigen Spiele damit getrieben werden können, lässt sich höchstens erahnen.
Deepfakes: So erkennt man manipulierte Videos

Das bekannteste Beispiel für einen Deepfake ist ein Video von Barack Obama, in dem er Donald Trump als Vollidiot bezeichnet (siehe Videobeitrag). Obamas Stimme, Mimik, Gestik – alles sehr authentisch. Doch das Video ist ein Fake. Der Schauspieler Jordan Peele wollte mit seinem Werk für Deepfakes sensibilisieren. Und darauf hinweisen, dass wir alle die Augen immer offenhalten und nichts glauben sollten. Es ist nichts weiter als ein Plädoyer für die älteste und wichtigste journalistische Grundregel, das Zwei-Quellen-Prinzip. Auch ein Video mit den beiden Satirikern John Oliver und Jimmy Fallon zeigte eindrücklich, wie täuschend echt Deepfakes daherkommen können.

Wie gefährlich aber sind Deepfakes wirklich? Bedrohen sie gar unsere Demokratie und die offene Gesellschaft? «Geht eine Fälschung im falschen Moment viral, könnten Deepfakes den Ausgang von demokratischen Wahlen gewichtig beeinflussen», schrieb der «Tages-Anzeiger» vor einigen Wochen dystopisch. Ähnliches musste man aber auch schon über soziale Netzwerke oder – viel früher – über den Aufstieg des Boulevardjournalismus lesen.
Technischer Mangel . . .

«Mit den aktuellen technischen Möglichkeiten ist es ausgesprochen schwierig, einen wirklich guten Deepfake zu erstellen», sagt Marc Ruef, Mitinhaber der Sicherheitsfirma Scip in Zürich. Das Unternehmen betreibt eine Forschungsabteilung, die sich mit aktuellen und zukünftigen Themen und Risiken im digitalen Raum auseinandersetzt. In einer breit angelegten Studie haben Ruef und sein Team versucht, den perfekten Deepfake zu erstellen – bisher ohne Erfolg. «Erstellt sind Deepfakes sehr schnell, die Anleitung findet sich kinderleicht im Internet. Ob das Ergebnis dann aber auch gut ist, ist eine andere Frage», erklärt Ruef. Der Arbeitsprozess umfasse viele Schritte, bei denen man Fehler machen könne. Doch selbst wenn alles reibungslos verlaufe, könne das Ergebnis enttäuschend sein. Auch der Videoredaktor der NZZ ist beim Versuch, einen authentischen Deepfake zu erstellen, an technischen Hürden gescheitert.
. . . und mangelnde Beleuchtung

«Wir haben mehr als fünfzig Deepfakes zu Testzwecken generiert und haben es noch nicht geschafft, auch nur einen wirklich sauber hinzubekommen», erklärt Ruef. Ein grosses Problem sei die optimale Beleuchtung und ideale Umsetzung: «Original- wie Fake-Video müssen mit dem gleichen Licht, mit demselben Kamerawinkel und in derselben Umgebung aufgezeichnet werden, damit es perfekt funktioniert», so Ruef. «In einem Studio sind Ausleuchtung oder Kamerawinkel völlig anders als auf einer Bühne.» Grundsätzlich reichten schon Sequenzen von 200 Bildern, um einen Deepfake zu erstellen – allerdings müsse die Qualität des Ursprungsvideos gut und reproduzierbar sein.

Die ursprüngliche Quelle kann eine beliebige Rede eines Politikers oder ein TV-Interview sein. Die Begebenheiten dieser Sequenz gilt es dann im Studio nachzustellen – am besten mit einer Person, die dem Protagonisten ähnlich sieht. «Trägt jemand eine Brille oder einen Bart, wird es schon fast unmöglich», sagt Ruef. «Man beachte zum Beispiel das Video mit John Oliver und Jimmy Fallon. Die Brille ist nicht sauber übertragen worden.»

Auch die Rechenleistung führe derzeit noch bei vielen Versuchen zu Problemen, sagt Ruef. Zudem seien die Frameworks noch umständlich und wenig benutzerfreundlich. Beide Hürden dürften in den kommenden Jahren allerdings kleiner werden.
Hilft ein digitales Gütesiegel?

Welche Möglichkeiten gibt es überhaupt, Deepfakes zu bekämpfen? «Sinnvoll wäre ein digitales Gütesiegel für Videos», sagt Ruef. «Diese Signatur könnten offizielle Stellen erhalten oder auch Qualitätsmedien.» Im Umkehrschluss führte dies dann allerdings zu einer noch grösseren Skepsis: Alle Videos, die kein Gütesiegel haben, wären somit potenziell gefälscht.

Auch Technologie könnte mithelfen, Deepfakes zu erkennen: «Schon jetzt gibt es Programme, die ein Video abspielen und auf mögliche gefälschte Stellen hinweisen. Anhaltspunkte für Fälschungen können ein leichtes Flackern im Gesicht oder gewisse Unschärfen sein.» Auch diese Technologie wird sich verbessern, davon ist Ruef überzeugt, so dass künstliche Intelligenz mit künstlicher Intelligenz bekämpft werden kann – «fast so wie bei ‹Terminator›», ergänzt er lachend.

Bis es so weit ist, werden Ruef und sein Team weiterhin versuchen, den perfekten Deepfake herzustellen, um Möglichkeiten und Risiken bestimmen zu können. Und wir alle müssen achtsam bleiben bei allem, was wir im Internet sehen und lesen. Insbesondere, wenn das Material aus unbekannter oder dubioser Quelle stammt.";https://www.nzz.ch/digital/deepfakes-kann-ich-ueberhaupt-noch-glauben-was-ich-sehe-ld.1457416;NZZ;Reto Stauffacher;;;
24.06.2016;«Ich habe mir zu wenige Freiheiten genommen»;"Irina Zürrer, wolltest du schon immer Medizin studieren?

Eigentlich wollte ich irgendwas im Bereich Medien machen. Dann habe ich angefangen, Serien wie «Grey’s Anatomy» zu schauen, und dachte: Medizin ist schön, spannend, extrem breit gefächert. Und Ärzte braucht es auf der ganzen Welt.

Hast du konkrete Berufsträume?

Ich würde nie einfach nach dem Lustprinzip studieren. Das Studium ist für mich Mittel zum Zweck, um meine Ziele zu erreichen. Ich möchte etwas Chirurgisches machen. Am liebsten würde ich im Ausland arbeiten, bei einer Organisation wie Ärzte ohne Grenzen. Ich habe vor meinem Studium fast ein Jahr in Kapstadt gelebt und möchte noch viel mehr von Afrika sehen.

Wie realistisch sind deine Träume?

Dass ich in die Chirurgie gehe, ist für mich mehr Fakt als Wunsch. Man ist als Ärztin doch praktisch nie arbeitslos, vor allem als Schweizerin in der Schweiz nicht. Sollte mich kein Hilfswerk nehmen, versuche ich’s eben auf eigene Faust.

Hast du deine Studienwahl jemals bereut?

Nein, ich bin sehr glücklich damit. Ich arbeite gerne mit meinen Händen, im OP kann ich das anwenden. Gleichzeitig muss ich strategisch denken. Und ich helfe mit meiner Arbeit Menschen. Ich löse Probleme.

Was hat das Studium dich gelehrt?

Ich lerne, mich zu organisieren, diszipliniert zu sein. Trotz all meinen Engagements habe ich aber noch Zeit für mich selbst. Das möchte ich jetzt auskosten. Danach, im Berufsleben, ist das wohl mehr oder weniger vorbei.

Hast du hier Freunde fürs Leben gefunden?

Definitiv, bei 250 Leuten kannst du dir ja auch einfacher aussuchen, wen du magst. Hier sind die Leute sehr klug, tolerant und aufgeschlossen. Ich komme aus dem Kanton Schwyz, dort ist alles kleinräumiger und enger. Ich bin froh, bin ich in Bern gelandet.

Hättest du besser eine Lehre gemacht?

Nein, der akademische Weg ist der richtige für mich, weil ich viel lernen darf, und auch, weil ich mich erst mit 26 Jahren für einen Beruf entscheiden muss. Meine Schwester macht eine Lehre, sie war mit 16 Jahren schon im Berufsleben und orientiert sich nun bereits neu. Da bin ich froh, habe ich mehr Zeit. Thierry Carrel, sind Sie der geborene Mediziner?

Ich war mir sicher, dass ich Medizin studieren wollte. Obwohl man mit 18 gar nicht weiss, was da auf einen zukommt. Ich war in einem humanistischen Gymnasium, die Kombination von Menschenwürde und Wissenschaft hat mich gereizt.

Hatten Sie einen Plan B?

Ich war interessiert an Philosophie und Theologie, aber es hiess schon damals: Wie willst du damit dein Geld verdienen?

Denken Sie gerne an Ihre Studienzeit zurück?

Sehr. Viele Freunde von damals habe ich heute noch. Wir waren eine kleine, eingeschworene Gemeinschaft. Wir hatten Spass am Unterricht und in den Laboratorien. Die Leichenhalle blieb für uns ein grosses Mysterium.

Bereuen Sie etwas?

Dass ich es verpasst habe, während des Studiums schon ein Jahr ins Ausland zu gehen. Das würde ich heute jedem dringend empfehlen, es erweitert den Horizont. Ich habe mir wohl damals zu wenig Freiheiten genommen.

Hätten Sie jemals gedacht, dass Sie hier landen?

Ich wusste schon früh, dass ich in die Chirurgie wollte. Aber ich dachte nicht, dass ich einmal Chef der grossen Klinik für Herz- und Gefässchirurgie am Inselspital werden würde.

Leben Sie Ihren Traum?

Den Traum Herzchirurg habe ich mir erfüllt. Diese Chefposition nun ist sehr vielfältig und spannend, umfasst aber auch äusserst anstrengende Augenblicke, weil die Erwartungen der einzelnen Akteure sehr unterschiedlich sind.

Was hat Sie das Studium gelehrt?

Es war eine Charakterschule. Ich lernte, Biss zu zeigen, zuverlässig zu sein, Ausdauer und  Flexibilität zu haben. Ich bin froh, muss ich diese Dinge nicht erst jetzt lernen.

Hilft Ihnen das Gelernte im Beruf?

90 Prozent des Gelernten war ja ­Allgemeinwissen. In der Chirurgie brauche ich heute sehr viel Spezialwissen. Das Wissen in der Medizin hat sich seit meinem Studium explosionsartig weiterentwickelt. Und dank dem Internet sind die Neuigkeiten sehr schnell verfügbar. Es wird aber eine Herausforderung für die nächste Generation sein, dieses Wissen richtig zu ordnen und zu priorisieren.";https://www.nzz.ch/karriere/berufseinstieg/ist-das-dein-traum-23-ich-habe-mir-zu-wenige-freiheiten-genommen-ld.131210;NZZ;Anna Miller;;;
20.12.2018;Grüsse aus der Gedanken-Cloud: wie menschliche Gehirne demnächst direkt miteinander kommunizieren werden;"Neue Krankheitsbilder verweisen auf Veränderungen der Lebensverhältnisse. Immer häufiger ist von einem Krankheitsbild namens «phubbing» die Rede. Die Wortneuschöpfung setzt sich aus dem englischen Wort für Telefon («phone») und dem für vor den Kopf stossen («snubbing») zusammen. Sie beschreibt inzwischen eine Standardsituation der Technologienutzung: Während man eigentlich mit anderen Menschen sprechen, ihnen vielleicht gar zuhören, beim Spaziergang durch die herbstliche Welt ein paar Beobachtungen machen oder die Enten füttern könnte, starrt man gebannt auf den Bildschirm des eigenen Telefons und tippt ohne Unterlass darauf herum.

Nicht nur Psychologen sind zunehmend mit der Frage konfrontiert, was das für die menschliche Entwicklung bedeutet. Orthopäden kennen das Phänomen als «Handynacken», der gelegentlich gymnastischer oder orthopädischer Behandlung bedarf.

Was bisher als Problem in der Technologiesozialisation des Menschen mit allen bekannten Suchtpotenzialen galt, hat womöglich eine Dimension, von der erst selten die Rede war. Es ist die Beschränktheit unseres Grundmediums, die hier zum Tragen kommt. Der Mensch verständigt sich noch immer über die Sprache als gesprochenen und geschriebenen Text.
Von Gehirn zu Gehirn

Die Sprache ist entstanden, als der Homo sapiens vor etwa 50 000 Jahren von Afrika aus die Eroberung der Welt antrat. Niemand weiss genau, wie diese Vorfahren damals gesprochen haben, aber das Prinzip war dasselbe wie heute. Es werden Laute und Zeichen gebildet, die auf etwas Bestimmtes verweisen. Sobald andere gelernt haben, dass spezielle Laute oder Zeichen fu?r eine spezielle Bedeutung stehen, können sie diese gleichbedeutend verwenden.

Und schon verstehen sich zwei Menschen, ganz egal ob sie im Fellschürzchen in einer Höhle oder im Anzug in einem Konferenzraum sitzen. So grossartig diese Möglichkeit der Verständigung auch ist, ihre technologischen Grundlagen haben sich seit Tausenden von Jahren nicht verändert. Wir kleben weiter an den Buchstaben, formen Laute mit unserem Stimmwerkzeug, um Worte hörbar zu machen, und schreiben sie mit der Hand oder dem Computer auf, um sie bleibend übermitteln zu können. Nur dass Menschen heute nicht mehr auf Wachstafel, Papyrus oder ein Blatt Papier starren, sondern auf einen kleinen Bildschirm, der vorgibt, Kommunikation zu revolutionieren.

Für die Grundform der menschlichen Kommunikation stimmt das nicht. Eine echte Revolution der zwischenmenschlichen Kommunikation gäbe es dann, wenn es gelänge, sich von den Mühen zu lösen, die es bedeutet, Worte – gesprochen oder geschrieben – zu formen für eine mögliche Verständigung. Wenn stattdessen Menschen ihre Gedanken mit Hilfe von Technologie austauschen könnten, ohne dafür auf einer Computertastatur oder einem Smartphone herumzutippen. Genau daran arbeiten einige neurowissenschaftliche Forschungsprojekte und Technologieunternehmen weltweit.
Mensch und Maschine

Schon jetzt ist es möglich, durch Denken zu schreiben. Ausgestattet mit einer Elektrodenhaube, die an einen Hochleistungscomputer mit maschinellem Lernen angeschlossen ist, kann man Buchstaben auf einen Bildschirm zaubern und ganze Wörter schreiben. Dazu müssen sich Computer und Mensch bloss ein wenig aneinander gewöhnen.

Einige Minuten lang trainiert die Maschine, um die neuronalen Signalmuster des menschlichen Gehirns entschlüsseln zu können. Die unterscheiden sich nämlich, wenn der Proband an ein X oder ein U denkt. Nach dieser Kalibrierung kann es losgehen. Man konzentriert sich bei dem auf dem Bildschirm gezeigten «Buchstabenglücksrad» immer auf den Buchstaben, den man schreiben möchte, und dann taucht er auf der anderen Hälfte des Bildschirms auf. Buchstabe für Buchstabe entstehen so ganze Wörter.

Das geht sehr langsam, denn die Technik ist ebenso störanfällig wie die menschliche Konzentration. Weil die Elektroden aussen am Kopf angebracht sind, ist ihre Signalerkennung weniger präzise als die eines Hirnimplantats, das die Signalmuster direkt im Gehirn abgreifen kann. Auch die Konzentration ist flügge wie ein junger Vogel. Bei der kleinsten Störung ist es aus mit dem Schreiben durch Denken. Das Gehirn ist ein komplexes und fragiles System. Seine Signale zu lesen, fällt auch künstlicher Intelligenz nicht leicht. Aber es ist möglich.

Patienten, die mit Querschnittslähmung oder Locked-in-Syndrom leben, bekommen so eine Chance, wieder zu kommunizieren oder einen Roboterarm mit ihren Gedanken zu steuern, um ein Stück Schokolade zu essen oder einen Schluck Wasser zu trinken. Der Fortschritt ist faszinierend, aber er bewegt sich noch mit der Geschwindigkeit einer Schnecke voran. Der derzeitige Rekord fu?r das Schreiben per Gehirn liegt bei acht Wörtern pro Minute. Diese Geschwindigkeit erreichte man nur mithilfe eines Hirnimplantats, also dadurch, dass in einer Operation eine Sonde in das Gehirn eines gelähmten Mannes eingeführt wurde. Die Signalerkennung funktioniert dann sehr viel besser und genauer als mit den aussen am Kopf angebrachten Elektroden.
Neue Schnittstelle

Doch wo Kommunikation durch die direkte Entschlüsselung von Gedanken möglich wird, da wartet bald ein riesiger Markt auf die ersten Unternehmen, die eine solche Technologie zur Marktreife bringen. Im Sommer 2017 kündigte Facebook-Gründer und -CEO Mark Zuckerberg auf der Entwicklerkonferenz F8 ein Gerät an, dass sich aussen am Kopf anbringen lässt, um die direkte Kommunikation durch Analyse von Hirnsignalen in nur wenigen Jahren marktreif zu machen.

«Wir arbeiten an einem System, das es euch erlauben wird, direkt aus eurem Gehirn heraus zu tippen, und zwar fu?nfmal so schnell, wie ihr heute auf euren Telefonen tippen könnt», schrieb Mark Zuckerberg zur Ankündigung eines «silent speech interface». Ein Team von sechzig Personen werkele mit Volldampf an dieser Technologie. Sie soll es möglich machen, in einer Geschwindigkeit von einhundert Wörtern pro Minute Text in einen Computer zu denken. Einhundert Wörter pro Minute? Das sind mehr als eineinhalb Wörter pro Sekunde direkt aus dem Kopf ins Telefon.

Wenn das geht, treten wir ein in die Revolution der Kommunikation. Krankheitsbilder der Smartphone-Generation werden dann bald in Vergessenheit geraten. Der Blick auf den Bildschirm entfällt, ein Text entsteht mit klarem Blick auf die Umwelt, einfach dadurch, dass man ihn denkt. Den Rest übernimmt die Technologie.

Wirklich? Ganz so einfach wird es wohl doch nicht. Denn dann entfällt der Prozess, der als allmähliche Verfertigung von Gedanken beim Schreiben und Sprechen immer auch ein sozialer Puffer in der zwischenmenschlichen Kommunikation ist. Wie oft beisst man sich im letzten Moment auf die Zunge, um etwas nicht zu sagen und damit seinen Arbeitsplatz oder eine Beziehung zu retten? Wie oft löscht man in Rage geschriebene Nachrichten, um später einen neuen, sozial verträglichen Anlauf zu nehmen? Vor allem aber: Wie wichtig ist das Gehirn als Arkan-Raum der Entstehung eigener Gedanken für die Freiheit des Denkens und den Schutz des Individuums?
Gefahr: Gedankenstarre

Wenn eine Maschine neuronale Signale in Text verwandeln kann, muss es eine Grenze des Zugriffs geben, damit das Denken nicht das nächste Ziel von Hackerangriffen wird. Facebook gibt auf diese Fragen bis jetzt eine sehr technokratische Antwort. Die Technologie solle nur solche Gedanken entschlüsseln, bei denen die Nutzer «sich schon entschieden haben, sie zu teilen, indem sie sie an das Sprachzentrum ihres Gehirns gesendet haben», so die damalige Chefentwicklerin des Projekts. Das ist eine sehr feine und fragwürdige Unterscheidung zwischen privatem und öffentlichem Denken. Gedanken sind dann ganz schnell nicht mehr frei, sondern für alle frei verfügbar.

Wenn es irgendwann gelingen sollte, mit einem Gerät für Endverbraucher Gedanken zu lesen und in geschriebenen Text zu verwandeln, dann wird das Schreiben von Nachrichten und Texten um ein Vielfaches leichter. Wir treten ein in eine Zukunft, in der das Medium der Sprache revolutioniert wird, unabhängig wird von der Hand- und Mundarbeit des Schreibens und Sprechens.

Aber die Kontrolle unserer Kommunikation wird zugleich komplizierter. Vielleicht wird es dann eine neue Klausel des sozialen Haftungsausschlusses am Ende einer jeden E-Mail geben: «Diese Nachricht wurde direkt aus meinem Gehirn gesendet. Ich bitte, Fehler, gedankliche Kurzschlüsse und beleidigende Inhalte zu entschuldigen.»

Vielleicht verfällt der Mensch mit der Revolution der Sprache aber auch einer neuen Krankheit der Ideophobie, der ängstlichen Gedankenstarre. Aus Angst vor fremdem Zugriff und Veröffentlichung halten wir die Gedanken lieber an der kurzen Leine und im engen Stall sozialer Normierung. Mit gymnastischer oder orthopädischer Behandlung ist dagegen dann leider nichts mehr zu machen.";https://www.nzz.ch/feuilleton/gruesse-aus-der-gedanken-cloud-wie-menschliche-gehirne-demnaechst-direkt-miteinander-kommunizieren-werden-ld.1445990;NZZ;Miriam Meckel;;;
09.10.2019;Wie intelligent ist künstliche Intelligenz?;"Eine internationale Sommerschule in Como hat es sich wieder wie schon seit einigen Jahren zur Aufgabe gemacht, Fortschritte der Künstlichen-Intelligenz-Forschung (KI) auszuloten und für die kommende Generation darzustellen. Das ist ebenso nötig wie schwierig, geht die Entwicklung doch so schnell voran, dass der unbedarfte Beobachter kaum hinterherkommt, geschweige denn die Frage aufwerfen könnte, ob wir diesen Fortschritt wollen.

Anlass dazu gäbe es genug, wie etwa der gerade bekanntgewordene Angriff der intelligenten amerikanischen Drohne, die dreissig afghanische Erntearbeiter tötete. Wie intelligent ist das? Wenn die Technologie idiotensicher wäre, wäre die Tragödie nicht passiert. Aber bedeutet intelligent denn idiotensicher? Dieser Schlussfolgerung auszuweichen, ist nicht immer einfach.

Ich fahre 32 Kilometer pro Stunde, wie mir die Anzeigetafel am Beginn einer 30er-Zone mit bösem Gesicht anzeigt. Das ist nicht sehr intelligent, denn der Sensor kann nur böse oder freundlich gucken und erkennt nicht, dass ich offensichtlich bereit bin, mich an die Geschwindigkeitsbegrenzung zu halten.
Triviale wie existenzielle Fragen

Algorithmen so zu programmieren, dass sie Entscheidungen nicht nur nach 0/1 oder nein/ja treffen, ist sehr aufwendig und erfordert den Einsatz der Vagheitslogik, von der auf der Tagung viel die Rede ist. Und sie muss sein, sind doch zahllose Fragen im täglichen Leben nicht einfach mit ja oder nein zu beantworten, triviale ebenso wie um existenzielle: Ist eine Person belesen, jung, betrunken, übergewichtig, kurzsichtig, weiblich, weiss, gross, klein – tot?

Wie intelligent muss ein KI-Algorithmus sein, um Antwort zu geben? Lassen sich ein gesundes Abwägungsvermögen und gefestigte ethische Prinzipien programmieren? KI-Enthusiasten behaupten das.

    Wird künstliche Intelligenz auch unser ästhetisches Empfinden, unsere ethischen Grundsätze und unsere zwischenmenschliche Emotionalität verändern?

Die Liste der Probleme, mit denen sich KI-Forscher befassen, wird von Tag zu Tag länger, und unter den Ergebnissen, die sie erzielt haben, sind viele, die uns Respekt abnötigen. Dass Computerprogramme bei so komplexen Spielen wie Schach und Go heute keine ernstzunehmenden menschlichen Gegner mehr haben, ist eine erstaunliche Leistung. Die Maschine Deep Blue, die 1997 den Grossmeister Kasparow aus dem Feld schlug, war zum Schachspielen programmiert. Das Programm AlphaGo aber überwältigte den Go-Meister Lee Sedol 2016, nachdem es 30 Millionen Go-Züge studiert und sich das Spiel dadurch selbst beigebracht hatte.

Lernende Maschinen fesseln zurzeit die Aufmerksamkeit. Die Quantensprünge, die beim automatischen Übersetzen gemacht wurden, seit dabei nicht mehr grammatische Regeln zugrunde gelegt werden, sondern Vorkommen von Wortfolgen in gigantischen Datenbanken, sind ein gutes Beispiel dafür, dass Lernen für Maschinen etwas anderes ist als für uns, und dass Maschinen uns bezüglich bestimmter Fähigkeiten – Rechnen, Suchen, Vergleichen – überlegen sind.

Damit lässt es die KI-Forschung aber nicht bewenden. Nach dem Lernen kommt die Kreativität: Kunst, Poesie, Musik. Software für Gesichts- und Stimmerkennung geht in die Richtung und ist schon sehr zuverlässig. Maschinell erzeugte Gedichte, Zeichnungen und musikalische Kompositionen wirken noch platt, unbeholfen oder surreal. Aber sind das nicht Eigenschaften, die avantgardistischen Werken oft genug attestiert wurden?

Schon erkennen aufmerksame Beobachter den Einfluss automatischer Übersetzungen auf kleine Sprachen, deren Sprecher sich bei der Wissenserzeugung an grösseren orientieren. Auch auf anderen Gebieten dringt KI in unser Alltagsleben ein, oft unbemerkt.

KI erschliesst uns neue Dimensionen des Wissens; das steht schon fest. Wird sie auch unser ästhetisches Empfinden, unsere ethischen Grundsätze und unsere zwischenmenschliche Emotionalität verändern? Das ist wahrscheinlich, aber wie oder ob wir uns ihr anvertrauen sollen, ist niemand im Stande zu sagen.
Bald jedem einen Roboter-Zwilling?

Weltweit arbeiten KI-Forscher an hochspezialisierten Projekten, ohne viel voneinander zu wissen. Wer hat den Überblick? Wer weiss, wo die Reise hingeht? Ein KI-Programm für die Zukunft? Das gibt es nicht. Nur fünf Jahre vorauszuschauen, ist unter den gegebenen Umständen ein reines Hasardspiel. Denn wie die einzelnen Algorithmen und maschinellen Fähigkeiten zu einem intelligenten Wesen integriert werden können und ob das je geschehen wird, wissen weder die Spezialisten noch sonst jemand.

Zwar hat der japanische Robotikforscher Hiroshi Ishiguro seinen eigenen Zwilling konstruiert und behauptet, dass wir über kurz oder lang nicht mehr von KI-begabten Robotern unterscheidbar sein werden; aber einstweilen ist das nicht mehr als Selbstmarketing.

Sicher ist nur, dass wir Zeugen einer Entwicklung sind, die bis jetzt unbekannte Formen geistiger Produktivität hervorbringt, die unser Leben verändern. Ob wir sie Intelligenz nennen wollen, ist noch nicht ausgemacht; dass es geschieht, aber sehr wohl, ob es uns gefällt oder nicht.";https://www.nzz.ch/meinung/wie-intelligent-ist-kuenstliche-intelligenz-ld.1510615;NZZ;Florian Coulmas;;;
16.08.2019;«Hey Siri, wie viel Mensch steckt in dir?»;"Es ist die jüngste Meldung in einer ganzen Reihe ähnlicher Vorfälle: Facebook hat externe Mitarbeiter ausgewählte Sprachaufnahmen von Nutzern seines Chat-Diensts Messenger anhören und abtippen lassen. Facebook steht mit diesem Vorgehen nicht alleine da. Bereits im April wurde bekannt, dass Amazon Aufnahmen des Sprachassistenten Alexa von Mitarbeitern abtippen liess. In der Folge erschienen Berichte, wonach auch Google, Apple sowie Microsoft über ihre digitalen Assistenten aufgenommene Sprachaufnahmen an Mitarbeiter weitergegeben hatten. Alle Fälle haben gemein, dass die Nutzer über diese Praxis nur unzulänglich informiert wurden. Was steckt hinter den Meldungen?

Täglich geben Menschen eine grosse Zahl von Sprachbefehlen an digitale Assistenten wie Alexa (Amazon), Google Assistant, Siri (Apple) und Cortana (Microsoft). Diese stecken in Smart Speakers wie Echo (Amazon), HomePod (Apple) oder Google Home, sind in unsere Handy-Betriebssysteme eingebaut oder sind via Smartwatch abrufbar. Dazu bieten die Firmen auch Dienste an, die gesprochene Sprache in Text umwandeln, zum Beispiel Facebook via Messenger. Bei diesem Dienst hat der Empfänger einer Sprachnachricht die Möglichkeit, sich den Inhalt transkribieren zu lassen, anstatt ihn sich anzuhören. Die Funktion ist hierzulande jedoch nicht verfügbar.

All diese Dienste basieren darauf, dass eine künstliche Intelligenz unsere gesprochene Sprache möglichst korrekt erkennt und wiedergeben kann. Um die Funktionsweise zu verbessern, lassen Amazon, Google, Facebook, Apple und Microsoft Audioaufnahmen von externen oder internen Mitarbeitern analysieren. Nach und nach teilten die Mitarbeiter der verschiedenen Firmen Details ihrer Arbeit mit unterschiedlichen Medien, woraufhin die betroffenen Unternehmen die Praxis bestätigten.
Wieso wurden die Audiomitschnitte von Menschen analysiert?

Wer mit den Sprachassistenten interagiert, stösst regelmässig auf Probleme. Theoretisch sollten die Assistenten erst reagieren, wenn der Nutzer ein spezifisches Codewort äussert. Oft versteht die künstliche Intelligenz unseren Befehl nicht, und wir müssen uns wiederholen. Ärgerlich ist weiter, wenn der Assistent zu lauschen beginnt, ohne dass zuvor das Codewort geäussert wurde. Das kommt besonders häufig vor, wenn die interagierende Person einen Dialekt, Umgangssprache oder eine andere Sprache als Englisch spricht. Beispielsweise hört der digitale Assistent von Amazon auf das Wort «Alexa», das dem im Französischen geläufigen «avec sa» sehr ähnlich ist und zur ungewollten Aktivierung des Dienstes führen kann.

Das mag in gewissen Fällen lustig sein. Zum Beispiel, wenn Apples Siri sich in einer Debatte über Syrien im britischen Parlament angesprochen fühlt und sich zu Wort meldet. Weniger lustig ist hingegen, wenn ein Assistent ungefragt beginnt, ein persönliches Gespräch aufzuzeichnen.

Um die KI zu verbessern, geben die grossen Tech-Firmen Audiomitschnitte an Mitarbeiter weiter, die sich die Aufnahmen anhören, sie transkribieren und an die KI zurückfüttern. Was hat der Mensch gesagt, und was hat die KI verstanden? Wieso hat der Assistent angefangen zu lauschen, obwohl das Codewort gar nicht ausgesprochen wurde? Die Unternehmen betonen, dass nur ein kleiner Prozentsatz der gespeicherten Aufnahmen an Menschen zur Analyse weitergereicht worden sei.

Über je mehr Daten und Muster die Software verfügt, desto grösser ist die Wahrscheinlichkeit, dass die KI uns korrekt versteht. Das Ziel ist, dass das Programm stets dazulernt und die Missverständnisse zwischen Mensch und Maschine in Zukunft ausbleiben.
Wurden die Nutzer im Vorfeld über die Praxis informiert?

Wer digitale Sprachassistenten nutzt oder Dienste zur Erkennung von gesprochener Sprache verwendet, hat – oft unbewusst – über die allgemeinen Geschäftsbedingungen eingewilligt, dass seine Aufnahmen zur Verbesserung des Dienstes verwendet werden. Angeprangert wird nun, dass aus den Bestimmungen nicht eindeutig hervorging, dass die Audiomitschnitte auch Menschen vorgespielt oder an externe Mitarbeiter weitergegeben werden.
Inwiefern war die Privatsphäre der Nutzer betroffen?

Nachdem die einzelnen Vorfälle öffentlich geworden waren, beteuerten alle Unternehmen, dass die Audioaufnahmen jeweils anonymisiert weitergegeben worden seien. Das bedeutet jedoch nicht, dass die Mitarbeiter keine heiklen Daten zu hören bekamen. Besonders problematisch ist dies, wenn die Sprachassistenten zuhören, weil sie fälschlicherweise ihr Codewort wahrgenommen haben. In den Mitschnitten seien Angaben wie Namen und Adressen oder Informationen zu Kontodetails oder zur Krankengeschichte zu hören. Auch von Drogendeals oder sexuellen Übergriffen, die die Mitarbeiter mit anhören mussten, ist die Rede.
Was sind die Folgen?

Inzwischen haben die Tech-Unternehmen auf die Berichte reagiert. Google, Apple und Facebook haben die Analyse von Mitschnitten durch Mitarbeiter ausgesetzt. Amazon bietet den Nutzern von Alexa neu die Möglichkeit, die Analyse eigener Sprachaufnahmen durch Menschen zu verbieten. Apple hat angekündigt, in Zukunft eine ähnliche Funktion anzubieten. Am wenigsten weit ging Microsoft, das neu lediglich präzisiert, dass Menschen zur Analyse von Mitschnitten hinzugezogen werden könnten.

Die Fälle beschäftigen nun auch die Behörden in der EU. Die irische Datenschutzkommission untersucht, inwiefern diese Art der Verarbeitung persönlicher Daten gegen die europäische Datenschutzverordnung verstösst. Irland ist zuständig, weil die Firmen dort ihren europäischen Hauptsitz haben.
Wie kann man sich schützen?

Wie der Standortverlauf lässt sich in den Einstellungen der einzelnen Dienste auch die Aufzeichnung von Audiomitschnitten ausschalten:

    Bei Google sind diese Optionen in den Aktivitätseinstellungen zu finden, wo sich auch bereits gespeicherte Mitschnitte löschen lassen.
    Bei Alexa von Amazon findet sich die entsprechende Option auf der Alexa-Website unter dem Reiter «Alexa-Datenschutz». Auch bei Alexa gibt es die Möglichkeit, frühere Aufnahmen zu löschen.
    Microsoft macht die Aufnahmen seiner Nutzer über ein Privatsphäre-Dashboard zugänglich. Dort kann man seine gespeicherten Aktivitäten einsehen und löschen.
    Anders als bei der Konkurrenz lässt sich bei Apple das Speichern von Audiomitschnitten bei der Benutzung von Siri nicht ausschalten. Der Dienst lässt sich lediglich ganz abschalten, was man auf seinem Gerät unter «Einstellungen», «Siri & Suche» machen kann. Wie oben beschrieben, will Apple in Zukunft eine Option anbieten, um die Analyse durch Menschen auszuschalten.

Eine radikale Methode wäre, den entsprechenden Diensten den Zugriff auf das Mikrofon zu entziehen. Dazu muss man in iOS oder Android in den Einstellungen zur App-Übersicht navigieren, dort die App auswählen, der die Berechtigung entzogen werden soll, und ihr den Zugriff verweigern. Dadurch verzichtet man jedoch gleich komplett auf die digitalen Assistenten, da diese, um zu funktionieren, auf das Mikrofon zugreifen müssen.
Wie sind die Vorfälle zu bewerten?

Dass uns digitale Assistenten oft nicht verstehen, ist im Grunde verständlich. Auch wir Menschen haben Mühe, einander zu verstehen. Wie oft bittet man sein Gegenüber, einen Satz oder ein Wort zu wiederholen, weil wir ihn nicht verstanden haben? Und wie oft verstehen wir zwar die Worte, aber nicht deren Bedeutung? Erstaunlich hingegen ist, wie viel menschliche Intelligenz noch in der künstlichen steckt. Während uns das Marketing verspricht, dass die Assistenten dank «maschineller Intelligenz» (Apple) stets dazulernen, passiert im Hintergrund viel manuell. Dass gleich alle grossen amerikanischen Tech-Unternehmen es nicht für nötig hielten, die Nutzer darauf hinzuweisen, dass Mitschnitte von Menschen analysiert und transkribiert werden, ist doch sehr bedenklich. Es zeugt davon, dass der Nutzer weiterhin nicht im Zentrum der Überlegungen von Facebook, Google, Amazon, Microsoft und Apple steht. Um schnelle Fortschritte bei der Entwicklung nicht zu gefährden, formulieren sie ihre Bestimmungen eher vage. Mehr Transparenz bei der Verarbeitung von Daten würde jedoch das Vertrauen in die digitalen Assistenten fördern und damit der durchaus interessanten Technologie zu mehr Akzeptanz in der Bevölkerung verhelfen.";https://www.nzz.ch/digital/alexa-siri-google-assistant-audioaufnahmen-landen-bei-menschen-ld.1502275;NZZ;Philipp Gollmer;;;
20.10.2017;KI-Pionier Jürgen Schmidhuber: Noch kommt er selbst zum Interviewtermin;"Noch kommt Jürgen Schmidhuber selbst zum Interviewtermin, eines Tages würde er gerne eine künstliche Intelligenz (KI) senden können.

Noch muss er darauf warten, aber der Fortschritt im Bereich der KI ist spektakulär: Diese Woche hat die Google-Schwester Deepmind enthüllt, dass es ihrer KI AlphaGo Zero gelungen ist, das chinesische Brettspiel Go ohne menschliche Anleitung zu lernen. Die Maschine hat sich alles selber beigebracht, der Mensch gab nur die Spielregeln vor. Danach spielte das künstliche Netzwerk einfach nur gegen sich selbst. Nach lediglich drei Tagen war es so weit: AlphaGo Zero hat von selbst gelernt, das künstliche System zu schlagen, das den 18fachen Weltmeister Lee Sedol – einen Menschen – mit 100 zu 0 besiegte. Nach 40 Tagen Training schlug es die beste bisherige Version (siehe Grafik). 2014 hatte Google die britische KI-Firma Deepmind für geschätzte 500 Millionen Dollar gekauft. Einer der drei Mitgründer, Shane Legg, und der erste Mitarbeiter der Firma, Daan Wierstra, haben zuvor am Schweizer KI-Lab IDSIA in Lugano unter Jürgen Schmidhuber doktoriert. Der KI-Pionier forscht seit 1995 in der Schweiz. Erstmals mit Schmidhuber gesprochen, aber nur am Telefon, habe ich im Sommer 2016 für einen Artikel über das KI-Forschungszentrum von Google in Zürich. Ich ahnte also schon, dass das Gespräch speziell werden würde, als ich Schmidhuber Ende September am Zürcher Hauptbahnhof zum Mittagessen traf. Wir sassen in der lärmigen Brasserie Federal. Als ich vorschlug, das Interview in einen ruhigen Raum zu verlegen, damit ich die Aufnahme besser transkribieren könne, fragte er: «Wieso lassen Sie dies nicht die Google-Spracherkennung erledigen?» Ja, wieso eigentlich nicht?

Schmidhubers Forschungsgruppe (mit Sepp Hochreiter, Felix Gers, Alex Graves und anderen) hat einen Lern-Algorithmus namens LSTM entwickelt, der heute von allen grossen Tech-Konzernen in der Spracherkennung oder in der maschinellen Übersetzung eingesetzt wird. Ohne ihn gäbe es Alexa, Siri oder den Google Assistant nicht in der heutigen Form.

Jetzt forscht er in seinem Labor daran, Computern Neugier beizubringen. Sie sollen lernen, die Welt von selbst zu begreifen. Schmidhuber geht davon aus, dass die Computer um das Jahr 2050 intelligenter als wir sein und wir Menschen danach rasant an Bedeutung verlieren werden. Sein Ziel ist es, sich selbst durch eine KI ersetzen zu können – und er meint das in vollem Ernst.

Der Übergang in die Science-Fiction ist bei Schmidhuber fliessend – er hat die Zukunft der KI in seinem Szenario bis ins kleinste Detail durchdacht. Und er ist mit viel Selbstvertrauen gesegnet. Von den vielen KI-Prognosen, die von Philosophen, Historikern, Unternehmern alter Schule oder Silicon-Valley-Stars wie Mark Zuckerberg oder Elon Musk herumgeboten werden, hält er wenig. Die hätten vom Gebiet halt wenig Ahnung.

Dass Schmidhuber eine Superintelligenz schaffen will, die uns übertrifft, klingt für die meisten beängstigend. Nicht für ihn: Im Interview erklärt er, wieso wir keine Angst vor künstlicher Superintelligenz oder dem möglicherweise daraus folgenden Ende der Menschheit zu haben brauchen.

Das Interview ist übrigens das Resultat meiner händischen Arbeit. Die Google-Spracherkennung funktioniert in unserem Büro nämlich noch nicht.";https://nzzas.nzz.ch/notizen/ki-pionier-juergen-schmidhuber-noch-kommt-er-selbst-zum-interviewtermin-ld.1323151;NZZ;Marco Metzler;;;
15.02.2019;Das globale Englisch verhindert eine wirkliche Begegnung der Kulturen;"Reist man als Tourist durch fremde Länder und versucht, die dort heimische Sprache zu sprechen, so macht man häufig die Erfahrung, von Angestellten der Tourismusbranche auf Englisch angesprochen zu werden. Das globale Englisch – kurz «Globisch» – fungiert als sprachliches Vehikel für diese Abdichtung gegenüber anderen Kulturen. Die Anästhesierung des kulturell fruchtbaren Raums zwischen den Sprachen suggeriert, die Welt sei letztlich überall gleich, weil eben die Dinge in einer Weltsprache mitgeteilt werden können. Wieso sollte man mühsam Fremdsprachen lernen, scheint doch eine universelle Sprache zu genügen.
Schwankende Vieldeutigkeit der Welt

Die Tatsache, dass die Sonne auf Deutsch aufgeht, im Portugiesischen aber jeden Tag neu geboren wird, verweist, trotz dem gleichen Naturphänomen, auf kulturell verschiedene Umgänge mit ihm. Oder dass das Englische zwei Wörter für Freiheit – Liberty und Freedom – kennt, könnte den Deutschsprachigen auf eine wichtige, ihm bis anhin vielleicht unbekannte innere Differenz des Begriffs der Freiheit aufmerksam machen. Dasselbe gilt für russisch Pravda und Istina, die zwei verschiedene Aspekte der Wahrheit beleuchten. Eine Verstärkung der ethisch-moralischen Komponente der Wahrheit (Pravda) wäre in Zeiten des Populismus und der Fake-News sehr willkommen. Mit andern Worten, Umwege über fremde Sprachen vermögen die Aufmerksamkeit und das eigene Denken zu schärfen.

Die Verschiedenheit der Sprachen und deren Erlernbarkeit bedingen das, was Hannah Arendt in ihrem «Denktagebuch» von 1950 die «schwankende Vieldeutigkeit der Welt» nennt. Sie erinnert die Menschen daran, dass die eigene Welt immer nur relativ ist. Diese produktive Vieldeutigkeit der Dinge in ihrem sprachlichen Ausdruck gerät durch das «Globisch» massiv unter Druck. Arendt geht hart ins Gericht mit ihm: So spricht sie vom «Unsinn der Weltsprache», weil sie «gegen die ‹condition humaine› die künstlich gewaltsame Vereindeutigung des Vieldeutigen» erzwingt. Dieser Zwang, der als Fortschritt in der Völkerverständigung gepriesen wird, hat sich inzwischen weltweit verselbständigt.

Der Wunsch nach einer einheitlichen Welt kann ein Ausdruck von Macht sein ? heutzutage vor allem in ihrer ökonomischen Ausprägung, die am liebsten auf der ganzen Welt die gleichen Marktgesetze einrichten möchte. Die digitale Revolution hat die Realisierung dieses Wunsches extrem beschleunigt. Für die Naturwissenschaften und die Technik bildet er ein Herzstück, das seit Beginn des 17. Jahrhunderts, so etwa im «Novum Organum» von Francis Bacon, in der Vorstellung von einer der Forschergemeinschaft gemeinsamen, genau geregelten Sprache seinen Ausdruck gefunden hat.
Ein reines «tool»

Dass diese seit geraumer Zeit weltweit in der Gestalt einer Sprache auftritt, zeitigt schwerwiegende Folgen für Generationen von Jugendlichen. Die Hochschulen, und nicht nur deren technisch-naturwissenschaftliche Abteilungen, gehen weltweit immer mehr zum «English only» über. Oft sind Studierende nicht mehr in der Lage, den Inhalt der Studien in ihrer eigenen Muttersprache zu erklären. Für viele Jugendliche erscheint somit das Erlernen der englischen Sprache für ihren beruflichen Erfolg als unabdingbar.

Wie also erklärt man z. B. Deutschschweizer Gymnasiasten, dass sie weiterhin Französisch und Italienisch lernen sollen? Wer macht ihnen klar, dass auch in wirtschaftlicher Hinsicht in den Schweizer KMU nach wie vor Mehrsprachigkeit sehr gefragt ist? Wer öffnet ihnen dahingehend die Augen, dass ein viersprachiges Staatsgebilde nur kraft des gelebten Respekts gegenüber dem anderssprachigen Landesteil überleben kann und dass dies durch das «Globish» nicht zu leisten ist? Und wer bringt ihnen zu guter Letzt bei, dass ein gepflegter Ausdruck in der Muttersprache (Hochdeutsch) nach wie vor essenziell für die Ausbildung eines eigenen kritischen Denkens ist?

Das «Globisch» ist eine Sprache ohne Geschichte und ohne Kultur. Es verfügt über keine Literatur und keine Autoren. Kurzum, es ist, anders als alle andern Sprachen, ein reines «tool» und als solches Träger der digitalen Revolution sowie des ökonomischen und technologischen Fortschritts. Dadurch ist es eng verbunden mit einer maschinellen Vorstellung von Virtualität, die sehr wenig mit der lebendigen Einbildungskraft des Menschen als eines sprachlichen Wesens zu tun hat.
";https://www.nzz.ch/meinung/der-mensch-das-sprachliche-wesen-ld.1453406;NZZ;Marco Baschera;;;
26.01.2019;Viele fürchten, wegen künstlicher Intelligenz überflüssig zu werden. Dabei hat KI ein fundamentales Problem: Sie macht keine Fehler;"Die Furcht vor technologiebedingter Massenarbeitslosigkeit geht um. Sie ist alt. Tiberius hat, laut dem römischen Historiker Plinius dem Älteren, einen Erfinder von bruchsicherem Glas umbringen lassen – aus Sorge um das Glasmachergewerbe. Und aus dem Jahr 1935 stammt ein Zeitungsartikel, der vor Jobrisiken durch «denkende Maschinen» warnt.

Treppenwitze der Wirtschaftsgeschichte. Denn Historiker wissen: Bei wirtschaftlichen Umbrüchen entstanden schon mittelfristig viel mehr neue Stellen, als alte verschwanden. Immer kam mehr Wohlstand für alle dabei heraus. Interessanterweise waren vor allem Phasen der Vollbeschäftigung von Automatisierungsfurcht geprägt.

Auch heute fragen wieder viele: Was bleibt für uns Menschen noch zu tun? Sie fürchten eine Spirale digitaler Grausamkeiten, die nicht nur den eigenen Job bedroht, sondern Arbeit als wichtigste gesellschaftliche Integrationsmaschine grundsätzlich überflüssig macht.

Der Alarmismus basiert auf der sogenannten Oxford-Studie von 2013, die, wenn auch methodisch äusserst fragwürdig, knapp die Hälfe der amerikanischen Arbeitsplätze als «automatisierungswahrscheinlich» etikettiert. Die Medien steigen ein, der Beratungsbedarf explodiert – ein Milliardengeschäft. Hinzu kommen die Apokalyptiker der Techno-Szene wie der Kapitalismuskritik. Die haben schon immer die dunklen Wolken selbst gemalt.
Immer mit der Ruhe

Kein Zweifel: Was digitalisiert werden kann, wird digitalisiert werden. Es werden Arbeitsplätze vernichtet. Wird das schnell gehen? Nein. Kurzfristig – das heisst innerhalb der nächsten acht bis zehn Jahre – dürfte sich nicht viel ändern. Langfristig hingegen schon.

Dabei gibt es eine alte Erkenntnis im Umgang mit dem Neuen: Wirtschaftshistorisch wurden die kurzfristigen Auswirkungen technologischer Umbrüche immer überschätzt, die langfristigen unterschätzt.

Die gute Nachricht ist, dass es um viele der verschwindenden Jobs nicht sonderlich schade sein wird. Die Massenfertigung hatte ja dazu geführt, dass die Arbeitsplätze immer maschinenähnlicher wurden. Nun werden diese Jobs auch von Maschinen erledigt. Eintönigkeit verschleisst dann nur noch Maschinenteile, keine Menschen. Wird man in hundert Jahren irgendwelchen langweiligen Bürojobs oder aufreibenden Über-Kopf-Arbeiten eine Träne nachweinen?

Mehr noch: Neugeburten plus Zuwanderung kompensieren nicht die Sterblichkeit in Mitteleuropa. Die geburtenstarken fünfziger und sechziger Jahrgänge verlassen den Arbeitsmarkt. Geburtenschwache Jahrgänge kommen.

Es würden mehrere Millionen Arbeitskräfte fehlen, bliebe der Bedarf ähnlich hoch wie heute. Mithin ist der Fachkräftemangel als Treiber der Digitalisierung mindestens so wichtig wie technologische Innovation. Wenn wir diese Knappheit mit Menschen kompensieren wollten, hätten wir Zuwanderungsraten, die politisch gar nicht durchsetzbar wären.
Maschinen? Experten!

Die Kassandren aber bleiben beharrlich: «Heute ist alles anders», lautet ihre Warnung. «Die künstliche Intelligenz ist der menschlichen bald überlegen und bedroht sogar hochqualifizierte Jobs.»

Halt. Maschinen, auch wenn sie sich künstlich intelligent nennen, sind zunächst nichts anderes als Metallkästen. Deshalb sollten wir auch von maschineller Intelligenz (MI) sprechen. Denn Algorithmen müssen von Menschenhand programmiert werden, bevor sie produktiv werden können; die Anfangslosigkeit der sich selbst zeichnenden Hände M. C. Eschers ist bis jetzt noch ein Technikertraum. Dann aber sind sie sehr produktiv. Es sind Expertensysteme, die jeweils eine Disziplin extrem gut beherrschen und darin jeden Menschen schlagen.

MI kann zudem extrem schnell optimieren: Muster erkennen, Aktienhandel und Anzeigenschaltung, Fleischqualität und Fahrstrecke, Krebsdiagnose und Knochenbruch, Düngermenge und Datenberge. MI kann sogar reparieren, mehr und mehr auch lernen.

Aber auch diese Lernfähigkeit ist von Menschen programmiert. Sie vergleicht immer nur Daten mit Voreinstellungen. Und selbst diese Voreinstellungen sind später kaum nachvollziehbar: Nach dem vierten Update weiss kein Experte mehr, wie MI zu diesem Ergebnis gekommen ist. Dabei gelten Netzwerke von einer Million Knoten schon als gross. Der Mensch verfügt in seinem Gehirn über 86 Milliarden Nervenzellen, zudem über ein chemisches System, das zusätzliche Verknüpfungen erlaubt. Er kann auf Fähigkeiten zurückgreifen, die über Äonen gespeichert wurden.

Allein das Wort «Hund» löst beim Menschen eine Unmenge an Assoziationen, Gefühlen, Erinnerungen aus. Wenn aber einer dieser Kästen die Bilder eines Wolfshundes von einem Wolf unterscheiden kann, dann kann er noch lange nicht ein Fernsehquiz gewinnen, das sich auf Wölfe bezieht. Es kann auch vorkommen, dass ein ernst schauender Mensch als solcher nicht erkannt wird, weil nur lächelnde Menschenfotos online gestellt werden. Oder dass ein Job-Bewerber, der von einem Computer mittels Videokamera ausgesucht wurde, in der Zusammenarbeit mit realen Menschen sich als Zombie herausstellt.

Und wenn der automatische Antwortgenerator auf verwirrende E-Mails «Ich liebe dich» antwortet, weil er weiss, dass das die richtige Antwort in verwirrenden Situationen ist, dann ist das allenfalls kurios. Gerade Ambiguität führt zu den absurdesten Klassifikationsfehlern, die schon zu manch schenkelklopfender Heiterkeit Anlass gaben.
Der Vorteil des Menschen

Aber MI fehlt paradoxerweise eine fundamentale Fähigkeit: Sie ist nicht fehleranfällig. Das ist der Vorteil der menschlichen Intelligenz. Wir verrechnen uns oder übersehen bessere Lösungen. Und eben weil wir Fehler machen, haben wir als Spezies über Jahrmillionen überlebt.

Unser Denken und Handeln folgt keinem Algorithmus, sondern passt sich an, ist lernfähig, vorausschauend, macht dabei immer wieder kleine Fehler, die wir korrigieren. Deshalb sind wir schlecht ausrechenbar. Goethes zauberlehrlingshafter Besen kannte keine Kollateralschäden, da diese nicht programmiert wurden.

MI kann also intelligent sein im Sinne extrem schneller Datenverarbeitung. Aber sie wird nie intelligent im menschlichen Sinne sein. Es war zu keiner Zeit sonderlich intelligent, ein Wettrennen mit Maschinen zu laufen, das man nicht gewinnen kann. Maschinen sind immer schneller. Wir verlieren da alle Spiele – und sind dadurch die Gewinner.

In welchen Spielen? Da, wo es um Gefühl und Intuition geht, um praktische Tugenden wie Weisheit und Klugheit. Die unklare Wahrnehmung gehört dazu. Menschliche Intelligenz qualifiziert für das Kreative, das Individuelle, das Komplexe, das Besondere, das Abwägen, Spüren, Bewerten. Für das Soziale: Gespräche etwa, Zuwendung, Kontakt. Sie kann zu einer Stimmung beitragen, die das Arbeiten werthaltig macht.

Eine schier endlose Reihung: Autonomie, Kontextsensibilität, Intuition, Analogiebildung, Gewissen, Sterblichkeit, Sorge, Liebe, Schönheit, Zauber, Ehrfurcht, Frömmigkeit, Neugierde, Unternehmertum, Sympathie, auch Verstehen in einem starken Sinne – alles nicht programmierbar. Vor allem aber der Widerspruch! Die Fähigkeit, sich selbst zu widersprechen, sie wird wohl immer dem Menschen vorbehalten bleiben. Das ist sein höchster Adel.
Bildung ist die Lösung

Wir begegnen also der MI mit einer eigenartigen Mischung aus Misstrauen und Vertrauen – Misstrauen gegenüber dem Jobkiller, Vertrauen gegenüber dem Wahrsager. In beiden Funktionen wird MI überschätzt.

Das betrifft auch den Vorwurf, unser Bildungssystem qualifiziere vorrangig für wegfallende Berufsbilder. Das ist unwahrscheinlich. Wir erleben ja gerade die Abkehr von der Maschinenlogik der Unternehmensführung, von Reibungslosigkeit und Konsenszwang. Der Mensch ist nicht mehr die grösste Fehlerquelle für Routinen, sondern die Lösungsquelle für Nicht-Routinen.

Bildung kann Menschen auf genau jene Situationen vorbereiten, die von Algorithmen nicht zu entscheiden sind. Bildung, die auf Nicht-Wissen zielt, auf das Nicht-Rationale, auf Intuition und Besonderheit. Wir brauchen nicht das Trennende der 0/1-Operation, sondern das Verbindende, nicht das Abgelagerte, sondern das Vorausdenkende. Wir müssen uns auf das konzentrieren, was Alexa noch nicht weiss.

Diese Bildung im humboldtschen Sinne ist die Basis der Urteilskraft. Denn Informationen sind das eine. Die Qualität der Informationen ist das andere. Ihre Beurteilung und Bedeutung ist etwas Drittes. Dieses Dritte wird uns Menschen vorbehalten bleiben. Denn selbst Datenberge bergen nichts. Nichts, wenn nicht ein Mensch hinzukommt.

Also, Kopf hoch! Wir müssen nicht technofatalistisch abdanken. Die Digitalisierung ist kein Grund zur stummen Unterwerfung unter das unendliche Rauschen der Daten und ihrer Verarbeitungsmaschinen. Vielmehr wird sie menschliche Anlage und Begabung neu und höher bewerten. Maschinelle Intelligenz bietet dafür realistische Chancen.";https://www.nzz.ch/feuilleton/ki-macht-nicht-arbeitslos-denn-sie-begeht-zu-wenige-fehler-ld.1453722;NZZ;Reinhard K. Sprenger;;;
30.08.2017;Autonome Fahrzeuge brauchen keine Ethik-Software;"Anfangs mutet es wie ein harmloses Computerspiel an: Die stark vereinfachten Zeichnungen auf dem Bildschirm zeigen einen Strassenzug. Ein blaues Auto bewegt sich auf einen Zebrastreifen zu. Den quert gerade eine Gruppe von Passanten. Das Auto, heisst es im Begleittext, könne aufgrund eines plötzlichen Bremsversagens nicht mehr anhalten. Und jetzt soll der Mensch am Bildschirm entscheiden, ob das Auto die Passanten auf dem Zebrastreifen überfahren soll (dies explizit mit dem Ergebnis, dass alle diese Passanten zu Tode kommen) oder ausweichen und mit einem schweren Hindernis kollidieren soll, was wiederum den Tod aller vier Insassen zur Folge hätte. Wer soll sterben?  Kaum ist eine Entscheidung per Klick auf das entsprechende Szenario getroffen, erscheint eine Abwandlung derselben Frage. Diesmal kommen per Mausklick entweder ein Jogger mit Hund oder eine Mutter mit Kind ums Leben. Ändert sich das Urteil, wenn die Frau die Fahrbahn überquert, während die Fussgängerampel auf Rot steht? – Klick. Was, wenn sie auch noch dick ist? – Klick. Und wie sieht es aus, wenn statt des Lebens des Joggers das dreier Krankenschwestern auf dem Spiel steht?
Value-of-life-Abwägungen im grossen Stil

Der makabre Fragenkatalog ist kein Spiel, sondern ein Forschungsinstrument. Wissenschafter des renommierten Media Lab am Massachusetts Institute of Technology (MIT) wollen damit herausfinden, wie autonome Fahrzeuge programmiert sein sollten, damit sie – auch und gerade dann, wenn es zu einem tragischen Unfall kommt – die grösstmögliche gesellschaftliche Anerkennung geniessen. Es könne zu Situationen kommen, in denen das Fahrzeug autonom über Leib und Leben entscheiden müsse, schreiben die Forscher. Um dies angemessen zu programmieren, müsse man erst einmal genauer verstehen, wie und nach welchen Kriterien Menschen solche Entscheidungen fällten – daher die Erhebung mit dem Fragenkatalog. Und so spucken die Algorithmen der «Moral Machine» unerbittlich immer neue Szenarien aus; es sind endlose Variationen eines Dilemmas, das, wenn man manchen Autoren Glauben schenkt, das drängendste noch zu lösende Problem auf dem Weg zum autonomen Fahren darstellt.

Wenig überraschend sind die Forscher am MIT nicht die Einzigen, die sich dieser Fragestellung widmen. Ähnliche Experimente haben Forscher der Universität Osnabrück in einer virtuellen Umgebung durchgespielt; im letzten Jahr befasste sich eine vielbeachtete Studie im Fachblatt «Science» mit dem Thema. In den Medien ist die Diskussion moralischer Dilemmata in Zusammenhang mit selbstfahrenden Autos ein Dauerbrenner geworden. Die Ergebnisse ähneln einander: Die meisten Menschen befürworten autonome Fahrzeuge, die grosso modo utilitaristisch handeln, also den Schaden für die Gesamtheit der Betroffenen minimieren. Entsprechend werden Kinder eher geschont als Alte, Einzelne lieber geopfert als eine Gruppe von Passanten – auch wenn das bedeutet, dass gelegentlich die Fahrzeuginsassen den Kürzeren ziehen. Kaufen oder nutzen möchten die Befragten meist aber nur ein Fahrzeug, welches der Unversehrtheit der Insassen einen höheren Stellenwert beimisst als jener der übrigen Verkehrsteilnehmer.

Was soll man mit diesen Resultaten anfangen? Die Diskussion, die wir im Zug der Einführung dieser neuen Technologie als Gesellschaft zwingend führen müssen, bringen sie jedenfalls nicht voran. Im Gegenteil. Die reichlich künstlichen Dilemmata vermitteln ein gefährlich falsches Bild davon, wie maschinelle Intelligenz und autonomes Fahren funktionieren.

Erstens werden autonome Fahrzeuge nicht lernen, zu entscheiden wie ein Mensch. Maschinen können allenfalls lernen, aus zahlreichen von Menschen getroffenen Entscheidungen eine Regel abzuleiten. Nur ist, was alle tun, nicht notwendigerweise moralisch richtig. Die Vorstellung von einer solchen «empirischen Ethik» ist abstrus, denn sobald aus der Präferenz vieler eine allgemeine Regel nach dem Schema «Kinder vor Alten» oder «Gruppen vor Einzelpersonen» abgeleitet wird, geschieht eine Diskriminierung. Auch verfängt das Argument nicht, dass ein autonomes Auto nur dann gesellschaftliche Akzeptanz finden kann, wenn dessen «Entscheidungen» deckungsgleich sind mit dem, was die breitere Bevölkerung für das kleinere Übel hält. Wir haben längst entschieden, dass das eine Leben nicht gegen ein anderes aufgerechnet werden darf. Das steht aus gutem Grund so in der Bundesverfassung.

Weiter muss man sich vor dem Schluss hüten, dass die Antworten der Probanden im Simulator oder am Computerbildschirm deren Reaktion in der Realität abbilden; mit grosser Wahrscheinlichkeit tun sie das nicht. Das zeigen zum Beispiel einschlägige Erfahrungen mit Piloten, die mit einem Kleinflugzeug in Not geraten: Beim Training im Simulator folgen die Piloten den Instruktionen und landen auf einem Acker, wo sie keine Unbeteiligten in Gefahr bringen. Im Angesicht der realen Bedrohung jedoch überwiegt der Drang, die eigene Haut zu retten – das zeigte vor wenigen Wochen erst die tragische Notlandung einer Cessna an einem belebten Strand nahe Lissabon. Den Probanden der Ethik-Experimente zum autonomen Fahren fehlt dasselbe entscheidende Kriterium: Sie sitzen nicht drin.

Drittens bewegt sich die Diskussion um autonome Fahrzeuge in Dilemma-Situationen in einer gefährlichen gedanklichen Schieflage: Einerseits unterstellt sie eine bis in utopische Sphären fortgeschrittene maschinelle Intelligenz, die offenbar problemlos Fussgänger nicht nur als solche erkennt, sondern diese auch noch nach diversen Merkmalen unterscheiden und ausserdem die konkreten Folgen der Kollision für alle Beteiligten abschätzen kann. Diese Welt der vollständigen Information (von der wir noch sehr weit entfernt sind) paart sich im Gedankenexperiment mit einer verblüffenden Ignoranz gegenüber den Veränderungen, die autonome Fahrzeuge für den Strassenverkehr in seiner Gesamtheit mit sich bringen werden.
Die Einführung autonomer Fahrzeuge ähnelt einem riesigen Beta-Test

Denn die Sensoren eines autonomen Fahrzeugs sind dem Menschen in mancher Hinsicht klar überlegen. Zum Beispiel beim Bremsen und beim Suchen nach freiem Raum. Beides wird dem Automaten wesentlich besser gelingen als dem Menschen – und das eröffnet ein Zeitfenster, in dem so mancher heute tragisch endende Zusammenstoss verhindert werden kann. Zugleich gibt es Situationen im Strassenverkehr, die autonome Fahrzeuge auch auf lange Sicht nicht ebenso gut wie ein Mensch am Steuer lösen können. Manche Signale vermögen die Sensoren schlicht nicht zu erkennen, andere missverstehen sie. Die potenziell gefährlichen Lücken der Technologie sind bekannt: Derzeit tun sich die Systeme noch schwer damit, zum Beispiel einen im Wind bewegten Luftballon von einem Lebewesen zu unterscheiden. Bei Regen und in Dunkelheit versagen sie den Dienst. Es gibt noch keine zufriedenstellende Lösung in Bezug auf die IT-Sicherheit autonomer Fahrzeuge. Und anders als ein menschlicher Fahrer wird ein autonomes Fahrzeug vermutlich nie mit erhöhter Aufmerksamkeit reagieren, wenn eine Stimme am Strassenrand «Pass auf, Anna!» ruft.

Mysteriöse Regeln, nach denen irgendwann algorithmisch entschieden wird, wer überfahren wird, werden schwerlich bei der Lösung dieser durchaus drängenden Probleme helfen. Vielmehr tut eine Diskussion um die Risiken not, die wir im Strassenverkehr einzugehen bereit sind. Einige davon liessen sich gerade mit autonomen Fahrzeugen vermindern, indem redundante Sicherungsmechanismen eingebaut würden. Heute wird eine solche Redundanz (wie sie im Flugverkehr selbstverständlich ist) für Automobile nicht gefordert – sie böte vermutlich auch nur marginale Verbesserungen. Denn der gegenwärtig grösste Risikofaktor im Verkehr ist der Mensch. Schon deshalb sollten wir uns nicht fragen, wessen Leben wir zu opfern bereit wären, sondern wie viele Leben noch durch Menschen am Steuer frühzeitig beendet werden sollen. Dabei hilft ein Blick in die Unfallstatistik: Verkehrsunfälle passieren, wenn der Mensch am Steuer abgelenkt, übermüdet oder gestresst ist. Wenn er Geschwindigkeitsbegrenzungen ignoriert, Alkohol oder Drogen konsumiert hat. Laut den Schätzungen verschiedener Experten gehen 80 bis 94 Prozent aller Verkehrsunfälle auf menschliches Versagen zurück. Mit autonomen Fahrzeugen lässt sich deshalb das Risiko, im Strassenverkehr zu verunglücken, auch ohne einprogrammierte Werturteile deutlich reduzieren. Zugleich sind unausweichliche, tragische Situationen wie jene, die die gegenwärtige Diskussion um autonome Fahrzeuge beherrschen, auch heute schon extrem selten (deshalb bekommen wir in der Fahrschule auch keine Anleitung in die Hand, wie mit einer solchen Situation umzugehen ist).

Wir brauchen auch nicht zu fragen, wie wir dem Computer den von möglichst vielen als glimpflich empfundenen Ausgang jeder vorstellbaren Kollision beibringen, auf dass der Computer eines pseudo-moralischen Urteils fähig werde. Wir sollten besser darüber reden, wie Mobilität in 20 bis 40 Jahren aussieht. Forscher wie Emilio Frazzoli an der ETH Zürich oder Raúl Rojas von der Freien Universität Berlin zeichnen in ihren Studien zum autonomen Fahren insbesondere für den städtischen Verkehr eine Zukunft, in der autonome Fahrzeuge als Taxis oder Shuttles Teil eines vernetzten Verkehrssystems sind: Das Auto in Privatbesitz spielt darin nur noch eine untergeordnete Rolle. Parkplätze in der Innenstadt sind obsolet geworden. Der Verkehr fliesst langsam, mit etwa 40 Kilometern pro Stunde, doch staufrei und kontinuierlich. Weil weniger Unfälle passieren, fahren anstelle von geländegängigen Kleinpanzern leichte Karosserien herum, die weniger Treibstoff verbrauchen und infolgedessen weniger Feinstaub und weniger CO2 emittieren.

Das zeigt: Nicht die Transformation vom Auto zum denkenden Transport-Roboter steht bevor, sondern die vom Individualverkehr hin zu einem vernetzten Verkehrssystem.";https://www.nzz.ch/meinung/autonome-fahrzeuge-brauchen-keine-ethik-software-ld.1308201;NZZ;Helga Rietz;;;
11.09.2019;Wieso hat ausgerechnet Japan nur drei Startups mit mehr als 1 Mrd. $ Marktwert? Die Antwort weiss Preferred Networks, der Erfinder eines Butler-Roboters;"Auch Japans Investorenlegende Masayoshi Son ist fehlbar. Schenkt man dem Chef des Technik-Investors Softbank Glauben, sind die fabelhaften Einhörner, also junge private Firmen mit mehr als 1 Mrd. $ Marktwert, in Japans Startup-Welt so rar wie im wirklichen Leben. Weltweit hat der selbsternannte «Einhorn-Jäger» über seinen 100 Mrd. $ schweren Softbank Vision Fund bereits in mehr als 80 Mega-Startup-Unternehmen investiert, doch keines liegt in seiner Heimat. Japan sei bei künstlicher Intelligenz (KI) ein Entwicklungsland, lamentierte Son jüngst. «Die Tatsache ist, dass es keine Einhörner in der KI in diesem Land gibt.» Nur ist die Meinung etwas übertrieben.
Preferred Networks als Star

Unweit von Softbanks Hauptquartier tüftelt Japans Vorzeige-Einhorn Preferred Networks daran, mit intelligenten Maschinen zur Revolution beizutragen. Seit das Unternehmen voriges Jahr einen Butler-Roboter vorgestellt hat, ist es daheim endgültig ein Star. Die von Toyota geliehene einarmige Maschine bewegte sich zwar noch etwas langsam. Doch immerhin räumte der Roboter ein vermülltes Zimmer selbständig auf. Socken warf der noch etwas langsame einarmige Haushaltsroboter in den Wäschekorb, Flaschen räumte er akkurat zurück ins Regal.

Zudem hat der Chef Toru Nishikawa ambitionierte Ziele, die selbst einem Mann wie Son gefallen sollten: «Innerhalb der kommenden fünf Jahre werden die Menschen sich an Haushaltsrobotern erfreuen können, die ihnen helfen werden zu kochen, Geschirr abzuräumen und aufzuräumen, während sie ausser Haus sind», sagt er. Und er will sie liefern.
Exklusiver Milliarden-Klub

Damit ist er längst nicht mehr der einzige Japaner im Milliarden-Klub mit grossen Plänen. Laut der globalen Referenzliste für Einhörner des Marktforschers CB Insight übersprangen dieses Jahr auch die Kryptobörse Liquid vom Fintech-Startup Quoine und Anfang August der Online-News-Dienst Smart News die magische Hürde– ganz ohne Softbanks Hilfe.

Auf dem Papier wirkt die Zahl von drei Einhörnern zwar weiterhin mager. Südkorea kommt mit seiner nicht einmal halb so grossen Volkswirtschaft auf neun Mega-Startups, von China und Indien ganz zu schweigen. Und gewiss spielt auch Japans risikofeindliche Geschäftskultur eine Rolle. Aber durch eine einmalige japanische Börsenkultur, die Innovationsstrategie der Japan AG und den Hunger japanischer Konzerne nach heimischen Startups stellt sich die Lage düsterer da, als sie ist, meinen Optimisten.

Der Startup-Markt entwickle sich zwar erst noch, meint der in Japan aktive Startup-Gründer und -Investor James Riney, Chef des Wagniskapitalgebers Coral Capital. «Aber es gibt hier Gewinner.» Nur passen viele der Firmen nicht in das Beuteschema von globalen Einhornjägern oder sind wie Preferred Networks derzeit selbst für Masayoshi Son unerreichbar.
Schnell an die Börse

Ein Grund für Japans tiefe Zahl von Investoren-Träumen ist, dass viele Startups so schnell an die Börse gehen, dass sie gar keine Zeit haben, zu Einhörnern zu werden, wie Riney erläutert. Bei den globalen Stars vergehen zwischen Gründung und Börsengang oft mehr als zehn Jahre, in Japan nur sechs. Denn zum einen sind die Anforderungen für Listings an Japans neuem Markt Mothers sehr niedrig. Zum anderen brauchen die Startups Geld, weil erstens Wagniskapital in Japan lange rar war und zweitens die Investoren oft schnell ihr Geld wiedersehen wollen.

2018 hatte sich Riney den Spass gemacht, die Zahl der Börsengänge im Verlauf von 20 Monaten zu zählen. Er kam auf zwölf Firmen mit einem Marktkapital von mehr als 100 Mio. $, darunter ein prominenter Einhorn-Börsengang, die global expandierende Flohmarkt-App Mercari. Derzeit liegt ihr Börsenwert bei 3,3 Mrd. €. «Das ist nicht schlecht», meint der Startup-Investor. Nur würden diese Unternehmen nicht mehr gezählt, weil sie die Grunddefinition eines Einhorns nicht mehr erfüllen: Sie sind schon an der Börse und damit nicht mehr für Einhornjäger als unerschöpfliche Quelle interessant.

Dennoch hat Japan mit jeder Startup-Welle globale Unternehmen hervorgebracht: Softbank selbst stammt aus den 1980er Jahren, Japans Online-Mall Rakuten aus den 1990er Jahren und eine Reihe von Online-Gaming-Firmen wie Gree oder DeNA aus diesem Jahrtausend. Einen weiteren Grund für den von Son beklagten Mangel an KI-Startups nennt Martin Schulz, Ökonom vom Fujitsu Research Institute: «Viele Konzerne entwickeln KI im eigenen Konzern.»
Dem Investoren-Zugriff entzogen

Preferred Networks wiederum entzieht sich wegen zweier anderer Faktoren Sons Zugriff, der Unternehmensphilosophie und des Willens der Japan AG, den KI-Lokalmatadoren für sich zu sichern. Schon das bisherige Geschäftsmodell entspricht nicht Sons Geschmack. Er investiert in Unternehmen wie den Mitfahrdienst Uber oder den Büroflächenanbieter WeWork, die mit KI und Internettechnologien Industrien umpflügen und die Welt erobern wollen. Im Gegensatz dazu stellten die Gründer von Preferred Networks ihre preisgekrönten Algorithmen lieber nicht in den Dienst eigener oder gar Sons Grossartigkeit, sondern von Japans Grosskonzernen. Dies zeigt ein Blick in die Unternehmensgeschichte.

2006 erstellten die Informatiker Nishikawa und sein Partner Daisuke Okanohara gemeinsam mit anderen Kommilitonen der elitären Tokio-Universität Preferreds Infrastruktur, um eine eigene Suchmaschine zu programmieren. Nishikawa wurde schnell das Gesicht des Unternehmens, Okanohara wurde derweil «Theorie-Nerd» genannt, weil er pro Woche 100 wissenschaftliche Aufsätze liest.

Irgendwann las Okanohara Papiere über künstliche Intelligenz. Deep Learning heisst der Megatrend, bei dem Computer mit vernetzten Prozessoren ähnlich wie ein Mensch selbst lernen, Dinge, Muster oder Zusammenhänge zu identifizieren. Sein Entschluss stand fest: «Das will ich auch versuchen.»
Kooperation mit Toyota

Nur war den beiden Japanern bewusst, dass ihre Bonsai-Programmierstube zu dem Zeitpunkt nicht direkt mit globalen Trendsettern wie Amazon oder Google konkurrieren konnten. Die haben schliesslich Zugriff auf riesige Bild- oder Sprachdatenmengen. Stattdessen boten sie Konzernen der Japan AG an, deren Produkte und damit auch die Technologie des Startups intelligenter zu machen.

2014 war es dann so weit: Das Duo gründete Preferred Networks. Projekte und vor allem Geld von grossen Namen der Japan AG belegten ihren Erfolg besser als Worte. Ihr erster Partner war kein Geringerer als Japans grösster Autobauer Toyota mit einem Projekt für autonomes Fahren. Bald danach investierte Toyota 95 Mio. $ in das Abenteuer. Seither wächst die Zahl der Partner.

Erst folgte der Industrieroboter-Hersteller Fanuc, mit dem das Startup eine neue Produktlinie intelligenter Fabrikroboter entwickelte. Krebsdiagnostik und der Versuch, mit dem japanischen Ölkonzern JXTG grosse Raffinerien zu automatisieren und zu optimieren und nebenbei neue Materialien zu entwickeln, gesellten sich jüngst zum Portfolio.
Nötige Reform des Geschäftsmodells

Mit jedem Projekt floss überdies Kapital ins Unternehmen, so dass Preferred Networks bisher nicht auf Investmentfonds angewiesen war. Doch das könnte sich nun ändern. Inzwischen wird der Wert des Unternehmens auf 2 Mrd. $ taxiert und hat 250 Mitarbeiter. «Anfangs konnten wir uns das nicht vorstellen», sagt Nishikawa, überwältigt vom eigenen Erfolg. Nun glaubt er aber, dass es Zeit für eine Reform des Geschäftsmodells ist.

Die bisherigen Auftragsarbeiten könnten nicht wie die Ideen der globalen Riesen in Massen vervielfältigt werden, benennt er selbst als die grösste Wachstumsbremse seiner Gründung. Seine Idee: «Nun beginnen wir mit unseren eigenen Projekten.» Medizinische Diagnostik mit der global in neue Dienste expandierenden sozialen Online-Game-Plattform DeNA ist ein Bereich, Nishikawas Traum eines Roboter-Butlers der andere.

Für die zweite Mission hat sich das Startup wieder mit dem Toyota-Konzern verbündet, der selbst ein formidabler Roboterhersteller geworden ist. Anfang August kündigten beide Unternehmen an, mit Toyotas Human Support Robot (HSR) und Preferred Networks’ Brainpower in den kommenden drei Jahren eine maschinelle Haushilfe zu entwickeln. «Nun beschleunigen wir die Rekrutierung von Personal, um die Geschäfte voranzutreiben», sagt Nishikawa. Vielleicht wird Japans führendes KI-Startup dann doch noch für Softbanks Roboter-Fan Masayoshi Son interessant.";https://www.nzz.ch/finanzen/preferred-networks-japans-ehrenretter-ld.1504781;NZZ;Martin Kölling;;;
27.06.2018;Can oder Kraftwerk – wer hat den Krautrock nachhaltiger geprägt?;"Wer mit Briten über deutsche Musik spricht, muss sich regelmässig zu diesen zwei Bands der siebziger Jahre äussern: Can aus Köln. Und Kraftwerk aus Düsseldorf. Bei den retrofuturistischen Klängen und posthumanen Bildern von Kraftwerk erstaunt die Faszination nicht mehr. Der ab 1974 fast rein elektronische, bewusst linkische, aber präzise und roboterhafte Kraftwerk-Funk wird überall als wichtigster transatlantischer Einfluss genannt; so etwa auch von New Yorker Rap-Pionieren und von den Techno-Produzenten aus Detroit. Die Liebe zu Can mag eher verblüffen. Die vier Gründungsmitglieder haben eine komplexe Musik gespielt, die sie aus elektronischer Avantgarde, Jazz und Rock zusammensetzten.
Museale Elektronik

Das musikalische Vermächtnis von Kraftwerk ist gut dokumentiert. Mittlerweile wirkt es aber fast etwas banal: Die Band, die in Museen auftritt und ständig ihren alten Katalog wiederholt, entwickelt die Kraft guter Werbung und überklarer Botschaften: «Wir sind die Roboter» (1978). Die maschinellen Beats klingen «funky» und nicht so steif, wie der Look es nahelegt, der sich beim sowjetischen Futurismus ebenso bedient wie bei grenzfaschistischen deutschen Heimatbildern. Doch bei allem Respekt für die konzeptionelle Bedeutung von Kraftwerk für die Entwicklung der Pop-Musik: Der Kraftwerk-Sound wirkt unterdessen ältlich. Wer ihre pittoresken und musealen Konzerte besucht, wir kaum noch Überraschungen erleben.

Und so geht es einem auch mit «Mensch Maschinen Musik. Das Gesamtkunstwerk Kraftwerk», einem eben erschienenen Buch, das aus zwei Tagungen hervorgegangen ist. Nicht alles wirkt zwar so leer wie das unterwürfig geführte Interview mit Ralf Hütter, dem einzigen verbliebenen Original-Kraftwerker. Denn immerhin öffnet der Sammelband mit einem klaren, persönlichen Text von Stephen Mallinder, dem Sänger der nordenglischen Band Cabaret Voltaire. Mallinder erinnert einerseits noch einmal an die historische Bedeutung von Kraftwerk. Er setzt dann aber die Kölner Band Can in dieselbe Reihe. Und völlig zu Recht!

Es ging damals Ende der sechziger Jahre um die Stunde Null eines kontinentaleuropäischen Pop. Die deutschen Musiker wollten sich lösen von angelsächsischen Vorbildern und erst recht vom deutschen Schlager, der damals noch die Hitparaden dominierte. Dass Can dem Zahn der Zeit seither musikalisch standhält, unterstreicht eine hervorragende, vorerst im englischen Original erschienene Can-Biografie von Rob Young: «All Gates Open. The Story of Can». Im zweiten Teil führt das Buch unter dem Titel «Can Kiosk» in eine Montage aus Gesprächen sowie in ein Journal von Irmin Schmidt.

Irmin Schmidt spielte bei Can elektronische Orgeln und einen Synthesizer, den ihm Helmi Hogg aus Zürich entwarf. Er ist der einzige lebende Can-Gründer. Der Bassist und Magnetbandtüftler Holger Czukay starb im Herbst vorigen Jahres, der präzise und vielseitige Schlagzeuger Jaki Liebezeit bereits im Januar 2017. Und Michael Karoli, der rund zehn Jahre jüngere Gitarrist, erlag 2001 einem Krebsleiden.
Untypische Musikerkonstellation

Es ist wichtig, auf die unterschiedlichen Musikerpersönlichkeiten hinzuweisen. Im Gegensatz zu Kraftwerk nämlich entwickelte Can keinen markenhaften Klang. Die Konstellation der Musiker war untypisch. Im Kontext von Pop-Musik entsprach sie gewissermassen der grösstmöglichen Unwahrscheinlichkeit. Czukay und Schmidt waren in den sechziger Jahren Schüler von Karlheinz Stockhausen gewesen. Schmidt bog bereits ein in eine vielversprechende Karriere als Dirigent. Liebezeit wiederum hatte als Freejazzer viel zu tun, während Karoli, aus reichem Haus, in einem Internat in St. Gallen steckte, wo der ältere Czukay sein Gitarrenlehrer war.

Nun hämmerte der Free-Jazz-Drummer für Can gnadenlos monotone, aber hyperpräzise Rhythmen. Die von klassischer Moderne und elektronischer Avantgarde, also von extrem formaler Kunst geprägten Stockhausen-Schüler machten sich derweil locker an Bass und Synthesizer, während der junge Bohémien an der Gitarre lernen musste, dass man mit diesem Phallus-Gerät bei Can nicht im Zentrum stand. Für einmal aber stimmte die strapazierte Phrase: Diese Band war mehr als die Summe ihrer Teile. Bedeutsam ist, und dabei hilft einem das Buch von Rob Young entschieden, dass sich in der Musik die unterschiedlichen Einflüsse und die dynamischen Verhältnisse auch manifestierten.

Der Hang zu «autonomer Musik» entsprach dem Erbe Stockhausens. Trotz politisierten Zeiten sah sich Can nie als politische Band. Im Versuch allerdings, lebendige, nicht etwas nachäffende Klänge zu schaffen, die aber den Weg zum Pop-Publikum finden sollten, schien doch ein sozialer Anspruch auf.

Can arbeitete auch mit Sängern – mit Malcom Mooney (1968–1970) und Damo Suzuki (1970–1973). Im Unterschied zu gewöhnlichen Rockbands fungierten diese weder als Frontmann noch als Botschafter. Ihre Texte waren für konkrete Messages zu abstrakt, sie wirkten unverständlich und dadaistisch. Wer die Tore für Sinn und Sinne so weit offen lässt wie Can, der nimmt auch den Wahnsinn in Kauf, in dessen Richtung es die Sänger mitunter drängte.

Bei Can aber vertraute man auch auf apollinische Gruppenkräfte, die den Drift zum Dionysischen quasi disziplinierten. Was auf Tonträger sehr frei klingen mag, wurde auf Magnetband minuziös editiert. Und wenn Can motorisch tönt, ist das oft das Resultat langer Improvisation. Diese widerstrebenden Kräfte wurden zwischen 1969 und 1973 auf fünf irisierenden Alben dokumentiert (in fünf weiteren Jahren viel die Band langsam auseinander).
Der Sound des Treppenhauses

Dank einem befreundeten Kunstsammler konnten die Musiker zunächst auf Schloss Nörvenich bei Köln üben und aufnehmen. Den kosmischen Sound ihrer Platten verdankten sie tatsächlich den akustischen Verhältnissen im Schloss-Treppenhaus. Später aber betrieb Czukay ein Studio im Dorf Weilerswist (bis zu seinem Tod). Die langhaarigen Musiker waren hier sehr geschätzt. Die Dörfler anerkannten die Arbeitsethik der Musiker, die sie bald auch am Fernsehen sahen.

Es erstaunt heute, wie die verrückte Musik von Can zeitweise mitten im Mainstream hausen konnte. Der Westdeutsche Rundfunk filmte sehr früh schon ein Konzert. Und als der Japaner Damo Suzuki wegen mangelnder Papiere ausgewiesen werden sollte, setzte sich WDR-Chef Werner Höfer ans Telefon und regelte die Angelegenheit auf dem kurzen Dienstweg direkt mit Aussenminister Walter Scheel.";https://www.nzz.ch/feuilleton/can-oder-kraftwerk-wer-hat-den-krautrock-nachhaltiger-gepraegt-ld.1397823;NZZ;Tobi Müller;;;
29.12.2020;Chinas Regulierer knöpfen sich die Internetkonzerne und Jack Ma vor;"Der einstige Englischlehrer Jack Ma steht wohl sinnbildlich für ein modernes und neues China. Mit dem E-Commerce-Konzern Alibaba hat er den Detailhandel revolutioniert. Und auch wegen des Fintech-Unternehmens Ant Group sind in China die Tage des Bargelds gezählt. Beide Firmen sind von Ma (mit)gegründet worden. Und dem Entrepreneur ist es wie keinem anderen gelungen, das laxe regulatorische Umfeld und die vielen Möglichkeiten, die seine Heimat bietet, zu nutzen. Allerdings sind die Tage gezählt, an denen die chinesische Regierung untätig dem Treiben der E-Commerce- und Fintech-Konzerne zuschaut, wie zwei Beispiele aus der jüngeren Vergangenheit zeigen.
«Systemische Risiken»

Zunächst war an Heiligabend bekanntgeworden, dass Chinas State Administration of Market Regulation gegen Alibaba wegen monopolistischer Praktiken ermittelt. So sind in dem asiatischen Land Fälle an der Tagesordnung, wo Internetkonzerne Händlern untersagen, ihre Produkte auch auf anderen Plattformen anzubieten. Diese Vorgaben sind vor allem jeweils während des Singles’ Day am 11. November gängig, wenn die grossen E-Commerce-Anbieter im Werben um die Kunden besonders scharf miteinander konkurrieren.

Auch wenn der Regulierer nun gegen Alibaba vorgeht, ist der Vorgang ein Warnschuss an die gesamte Branche, denn alle E-Commerce-Firmen setzen auf solche monopolistischen Praktiken. Zudem war bereits vor anderthalb Monaten bekanntgeworden, dass die State Administration for Market Regulation einen Gesetzentwurf zur Regulierung von Internetplattformen zur öffentlichen Vernehmlassung publiziert hatte; und die 27 wichtigsten E-Commerce-Anbieter waren von der Behörde zu Gesprächen vorgeladen worden.

Der bei der UBS von Hongkong aus die Geschäfte auf dem chinesischen Festland leitende David Chin hatte jüngst gegenüber der NZZ gesagt, China sei im Umgang mit Konzernen wie Alibaba und Tencent bisher äusserst liberal gewesen. «Sie konnten frei agieren. Nun sind die Behörden jedoch zu dem Schluss gekommen, dass mit den Internetkonzernen systemische Risiken einhergehen.»

Chin betonte jedoch auch, dass die Behörden bei der Regulierung nicht überdrehen würden, weil ihnen bewusst sei, welche gesamtwirtschaftliche Bedeutung Internetkonzerne hätten. Und das Sprachrohr der Kommunistischen Partei, die Zeitung «People’s Daily», schrieb beschwichtigend, die Untersuchung der Alibaba-Praktiken sei nicht so zu verstehen, dass die Regierung ihre Haltung gegenüber den Internetkonzernen geändert habe. Wie weiter mit dem Börsengang der Ant Group?

Am vergangenen Wochenende hatten Chinas Regulierer einen weiteren Teil der Welt von Jack Ma ins Visier genommen. Zunächst waren am Samstag führende Köpfe der Ant Group von der chinesischen Notenbank (PBoC), dem Regulierer für Banken und Versicherungen sowie der staatlichen Verwaltung für Devisenhandel einbestellt worden.

Tags darauf gab es eine Mitteilung der PBoC, in der es hiess, die Ant Group solle sich wieder auf das Geschäft mit dem mobilen Bezahlsystem fokussieren. Die Gerüchte schossen schnell ins Kraut, dass sich der Fintech-Konzern, dessen dualer Börsengang überraschend Anfang November abgesagt worden war, von dem lukrativen Kreditvermittlungsgeschäft trennen müsse.

Die Ant Group würde dieser Entscheid hart treffen, wie ein Blick in die Bilanz zeigt. In den ersten sechs Monaten des laufenden Jahres haben sich die mit den digitalen Finanzprodukten – wie der Vermittlung von Krediten, Versicherungen sowie Vermögensanlagen wie dem weltweit grössten Geldmarktfonds Yu’e Bao – erwirtschafteten Umsätze um 57% gegenüber der Vorjahresperiode auf 46 Mrd. Yuan (Y) gesteigert, was rund 6,3 Mrd. Fr. entspricht. Deren Anteil an den gesamten Erlösen der Ant Group von 72,5 Mrd. Y belief sich auf mehr als 63%.

Müsste sich die Ant Group davon trennen, wäre der Fintech-Konzern bei einem anstehenden Börsengang weit weniger für Investoren interessant als bis anhin. In ihrer Stellungnahme offeriert die PBoC dem Unternehmen jedoch eine Lösung. Die Ant Group müsse ein Finanzinstitut gründen und die geforderten Eigenkapitalanforderungen erfüllen.

Für den Konzern stellt sich die Frage, wie er beim Gang an den Kapitalmarkt weiter vorgehen will. Bringt er nur eine auf die Erbringung von Dienstleistungen spezialisierte Sparte an die Börse, dürfte dieser Schritt schnell zu bewerkstelligen sein. Hält die Ant Group jedoch an dem geplanten grossen Börsengang fest, muss das Unternehmen entsprechende Lizenzen für die geforderte Finanzsparte beantragen und anschliessend Vorgaben wie die Anforderungen an das Eigenkapital erfüllen. Dafür benötigt der Fintech-Konzern jedoch Zeit; ein Börsengang im kommenden Jahr dürfte damit in weite Ferne rücken.
Argwöhnische Grossbanken

Die grosse Frage bleibt, warum der Regulierer die Ant Group ins Visier genommen hat. Das Vorgehen mag auch mit einer Rede Mas in Schanghai im Oktober zusammenhängen, bei der dieser dem Notenbankchef Yi Gang und dem stellvertretenden Staatspräsidenten Wang Qishan, die beide im Saal waren, durch die Blume sagte, sie wären unfähig, Lösungen für die Zukunftsfragen der Finanzmärkte zu finden.

Nach der Rede dürften jedoch vor allem die staatlichen Grossbanken ihre Chance gewittert haben, gegen den Ma-Konzern vorzugehen, nachdem zu deren Leidwesen der Börsengang noch in Rekordzeit genehmigt worden war. Chinas grosse Finanzinstitute verdienen ihr Geld vor allem mit der Vergabe von Krediten. Die Ant Group ist mit ihrer auf künstlicher Intelligenz sowie Big Data basierenden Kreditbewilligung den Grossen der Finanzbranche ein Dorn im Auge, weil sie mit ihren kostengünstigen Modellen an deren Margen kratzt.

Die staatlichen Finanzhäuser sahen ihre bisherigen Einnahmen wegbrechen. Erschwerend kommt hinzu, dass die Ant Group bei der Kreditvergabe vor allem mit kleineren und mittleren Banken zusammenarbeitet. Es braucht wenig Phantasie, um zu erkennen, dass die staatlichen Finanzkolosse nach der aggressiven Rede Mas erfolgreich für ihre eigenen Interessen lobbyiert haben. Dass solche Kämpfe hinter den Kulissen die Argumentation englischsprachiger Medien rechtfertigen, Ma sei bei der Kommunistischen Partei in Ungnade gefallen, muss bezweifelt werden.";https://www.nzz.ch/wirtschaft/china-regulierer-knoepfen-sich-internetkonzerne-und-jack-ma-vor-ld.1594062;NZZ;Matthias Müller;;;
31.08.2020;Schweizer Firmen sind überraschend veränderungswillig und optimistisch;"So etwas gab es seit Beginn der Aufzeichnung der Quartalszahlen noch nie: Von April bis Juni ist die Wertschöpfung der Schweizer Wirtschaft gegenüber dem Vorquartal wegen der Corona-Krise um 8,2% eingebrochen. Doch was steckt dahinter, und wie wird es weitergehen? Eine von 1014 Führungskräften beantwortete Umfrage, die das zur NZZ-Gruppe gehörende Swiss Economic Forum (SEF) zusammen mit der Kalaidos-Fachhochschule zwischen Ende Juni und Mitte Juli unter Teilnehmern des SEF durchgeführt hat, zeichnet trotz dem gewaltigen Leistungsabfall ein erfreulich dynamisches Bild der Schweizer Unternehmenslandschaft. Gerade auch die kleinen und mittelgrossen Firmen (KMU) nutzen offenbar die Krise dazu, bereits laufende Anpassungsprozesse und Innovationen zu beschleunigen. Damit werden sich auch wettbewerbsfähiger.
Digitaler, flexibler und personalisierter

Insgesamt hält sich in der Umfrage die Zahl derjenigen, die von der Corona-Krise stark betroffen sind, und derjenigen, die schwach betroffen sind, die Waage (der Mittelwert auf einer Skala von –3 [gar nicht] bis +3 [sehr schwer] beträgt überraschend geringe –0,1). Bei den kleinen Firmen mit 1 bis 49 Mitarbeitern ist die Polarisierung in stark Betroffene und kaum Betroffene stärker ausgeprägt als bei den mittleren (50 bis 249 Mitarbeiter) und grossen Unternehmen. Unter dem Lockdown besonders gelitten haben Gastgewerbe und Tourismus, Kunst und Unterhaltung, PR und Werbung, aber auch Maschinenbau, Gesundheits- und Sozialwesen. Nur wenig berührt sind hingegen Energie- und Wasserversorgung, Bau, Landwirtschaft, Immobilien- und Finanzunternehmen.

Eine deutliche Mehrheit der befragten Führungskräfte glaubt, dass sich durch die Corona-Krise Chancen ergeben haben, von denen ihr Unternehmen nun profitieren kann. Etwas überraschend ist dieser Optimismus bei den kleinen Unternehmen (Mittelwert 1,83), wo er sogar noch ausgeprägter ist als bei den mittleren (1,68) und grossen (1,73).

Chancen für ihr Unternehmen sehen die Führungskräfte vor allem darin, dass die Corona-Krise die Digitalisierung beschleunigt, zum vermehrten Einsatz von kollaborativen Tools geführt und gezeigt hat, wie man Arbeitsprozesse flexibilisieren kann. Den vermehrten Einsatz von Home-Office und damit verbunden einen geringeren Bedarf an Büroräumlichkeiten erachten vor allem Grossunternehmen als Chance, während für kleinere eher die tieferen Reisekosten und neue Konsumwünsche wichtig sind. Insourcing, also wieder mehr selber zu machen, geben hingegen nur 9% der kleinen und gar bloss 2,5% der Grossunternehmen als Lehre aus der Krise an.

In der Umfrage zeigt sich viel Unternehmergeist. Die Firmenchefs geben sich zuversichtlich, dass sich die Unternehmen anpassen können und mithilfe neuer Technologien, besserer Führung und Organisation sowie dank den Standortvorteilen der Schweiz wettbewerbsfähig bleiben. Der Optimismus zeigt sich auch darin, dass die Befragten bezüglich ihres eigenen Unternehmens (Mittelwert 1,84) noch optimistischer sind als im Hinblick auf die Wirtschaft insgesamt (1,26). Dabei ist der Optimismus bei den KMU auch hier ausgeprägter als bei den Grossunternehmen. Kleinere Unternehmen sehen für sich die grössten technologischen Chancen darin, Kunden vermehrt personalisiert ansprechen zu können, sich künstlicher Intelligenz zu bedienen, zu automatisieren und generell Daten zu nutzen. Für Grossunternehmen ist Big Data am wichtigsten, vor dem Einsatz von künstlicher Intelligenz. Interessanterweise scheint Cleantech bei den kleinen Unternehmen eine grössere Rolle zu spielen als bei den grossen, die eher auf Robotik setzen. Die Blockchain-Technologie ist nur für Banken, Versicherungen und die Rechtsberatung ein zentrales Thema. Erstaunlicherweise bloss 8% der Befragten sehen den 5G-Mobilfunkstandard und den 3-D-Druck als Chance für ihr Unternehmen, und bloss 9% halten die Sharing-Economy für eine Geschäftschance. Sich schnell und agil in flachen Hierarchien anpassen, um immer wieder mit Innovationen aufwarten zu können, ist in den Augen der SEF-Teilnehmer das Wichtigste, was eine moderne Führungskultur hervorbringen muss. Dazu braucht es lebenslanges Lernen. Etwas überraschend spielt die vielgerühmte Diversität und Chancengleichheit im Urteil der befragten Wirtschaftsführer hingegen eine untergeordnete Rolle.
Leistungsbereite Problemlöser und Kommunikatoren gesucht

Zukunftsfähige Mitarbeiter sollten leistungsbereite Problemlöser sein, die kreativ und gut mit Komplexität umgehen können. Für nicht weniger als 41% der Befragten ist gut kommunizieren können die wichtigste Eigenschaft, über die Mitarbeiter verfügen müssen, damit ihre Firma die sich ihr bietenden Chancen auch nutzen kann. Wohl weil sie sich schneller an den technologischen Wandel anpassen müssen, erachten die Firmen die hohe Qualität des Schweizer Bildungswesens und der Hochschulen, den liberalen Arbeitsmarkt und die Verfügbarkeit von Fachkräften als zentrale Standortvorteile der Schweiz (was auch auf die Bedeutung der Personenfreizügigkeit hinweist). Geschätzt werden zudem die Qualität der Infrastruktur und die Finanz- und Steuerpolitik. Hingegen sehen selbst unter den kleinen Firmen nur gerade 7,5% die staatliche finanzielle Absicherung in Krisen als Standortvorteil – echtes Unternehmertum setzt in der Schweiz erfreulicherweise nicht auf den Staat und die Steuerzahler.
«Walk the Line?» in Montreux

Als einer der ersten Grossanlässe findet das SEF dieses Jahr unter strengen Corona-Schutzmassnahmen am 2. und 3. September wieder «real» in Montreux statt. Unter dem Motto «Walk the Line?» soll dort unter anderem auch der Frage nachgegangen werden, welche bleibenden Spuren die Corona-Krise hinterlassen wird. ";https://www.nzz.ch/wirtschaft/sef-umfrage-digitalisierung-und-big-data-helfen-schweizer-kmu-ld.1573882;NZZ;Peter A. Fischer;;;
18.09.2019;Wer aber beobachtet die Beobachter? – Die Geburt des Big-Data-Minings aus dem Geiste der Aufklärung;"Wer heute von der Zukunft spricht, spricht auch von Big Data und künstlicher Intelligenz. Manche nennen es eine «Revolution, die unser Leben verändern wird», wie der Untertitel eines populären Buches von Viktor Mayer-Schönberger und Kenneth Cukier zu Big Data aus dem Jahr 2013 lautet. Das ist eine grosse Ansage, die keineswegs als Warnung gemeint ist, sondern eher als Ausblick auf ein gelobtes Land, in dem durch Big-Data-Mining Krankheitsursachen besser erkannt und Verkehrsströme optimaler organisiert werden können. Und natürlich sprechen heute alle von den Daten als Öl des 21. Jahrhunderts, und viele Vertreter aus Wirtschaft und Politik fordern, dieses neue Wertschöpfungsmodell nicht durch altertümliche Datenschutzvorstellungen zu gefährden.

Man kann die Sache auch ganz anders angehen. Man kann gerade in der Aufweichung des Datenschutzes eine grosse Gefahr sehen und vor einem Überwachungsstaat warnen beziehungsweise vor dem «Zeitalter des Überwachungskapitalismus», so der Titel eines Buches aus dem letzten Jahr von Shoshana Zuboff. In diesem Falle ist Big Data kein Versprechen, sondern die Ursache einer Totalüberwachung des Menschen, wenn auch zum Zweck nicht der Disziplinierung, sondern der besseren Konsumverführung: Man will alles von allen wissen, um ihnen – überall und immer – besser das anbieten zu können, was sie brauchen oder brauchen könnten.
Historische Kontinuität

Da dies schliesslich dazu führt, dass Menschen in ihrem Verhalten völlig transparent und im Interesse der Profitmaximierung manipuliert werden, läuft eine solche Betrachtung oft auf Kapitalismuskritik und den Aufruf zum Widerstand hinaus. Das erwähnte Buch bemüht dazu sogar eine gewichtige historische Analogie: «Die Berliner Mauer fiel aus vielen Gründen, vor allem aber weil die Menschen in Ostberlin sich sagten: ‹Jetzt reicht’s!›» Braucht es also eine politische Revolution, um für die technologische andere ökonomische Rahmenbedingungen zu schaffen?

    Wie Studien erwiesen haben, verraten 150 Likes mehr über einen Menschen, als seine Eltern wissen und ihm womöglich lieb ist.

Bei solcher Überhitzung tun abgeklärte Perspektiven gut. Perspektiven, die, statt Schuldige zu benennen und Anklage zu erheben, die historische Kontinuität dieser sogenannten «Revolution» aufzeigen. Ein hilfreicher Ansatz dazu ist die Frage, warum das Big- Data-Mining und die Digitalisierung sich überhaupt so erfolgreich durchsetzen können. Eine plausible Antwort lautet: Weil sie die Lösung zu einem Problem sind. Dieses Problem besteht darin, dass die moderne Gesellschaft zum Zweck der Organisation ihrer Prozesse so gut wie möglich über sich Bescheid wissen will und muss.

Genau das ermöglicht die Digitalisierung, die alle gesellschaftlichen Prozesse in die Form analysierbarer Daten bringt, in bisher ungeahntem Ausmass. Und zwar so radikal, dass noch die geheimsten Muster erkennbar werden, wie ein im August erschienenes Buch mit dem Titel «Muster» von Armin Nassehi erklärt. Mit einer entsprechenden Menge und Breite an Daten lassen sich Profile erstellen und Korrelationen bilden, die Daten hinter den Daten erkennbar machen. Ein Beispiel dafür sind die Likes, die man im Internet vergibt. Wie Studien erwiesen haben, verraten 150 Likes mehr über einen Menschen, als seine Eltern wissen und ihm womöglich lieb ist. So etwas führt dann natürlich zurück zum Problem des Datenschutzes: Wie kann man seine informationelle Selbstbestimmung aufrechterhalten, wenn man gar nicht weiss, mit welchen Informationen man was über sich verrät?

Hinter dieser Frage lauert eine andere: Wie viel sollte die Gesellschaft überhaupt über sich wissen? Diese Frage richtet sich nicht zuletzt an die Soziologie, oder zumindest deren empirische Abteilung, die auf die Aufdeckung sozialer Strukturen, gesellschaftlicher Dynamiken und individueller Verhaltensmuster zielt. Ist der gläserne Mensch nicht auch das ureigene Problem der Soziologie? Vorausgesetzt natürlich, man sieht im gläsernen Menschen ein Problem, was diejenigen, die euphemistisch von Transparenz sprechen, ja keineswegs tun. Und wer will es ihnen verübeln? Ist nicht Georg Simmels Lob auf das Geheimnis als «eine der grössten geistigen Errungenschaften der Menschheit» von 1907 hoffnungslos veraltet? Basiert nicht das Reputationsmodell des Plattformkapitalismus (Uber, Airbnb) darauf, möglichst viel über alle Beteiligten zu wissen? Ist die optimierte Selbstbeobachtung der Gesellschaft nicht ein legitimes Ziel?
Alles wissen wollen

Wie auch immer man die Sachlage bewertet, man ahnt, dass der ungebändigte Vermessungswille nicht nur zur DNA des Kapitalismus gehört, sondern ein zentrales Merkmal der modernen Gesellschaft ist. Man könnte auch sagen, die Gefährdung dessen, was Simmel als grosse Errungenschaft der Menschheit bezeichnet, erfolgt aus dem Geiste der Aufklärung. Denn es war die Aufklärung, die alles ins Licht der Erkenntnis tauchen und jeden Hügel, jede Kreatur vermessen wollte. Die Vermessung der sozialen Welt, die heute unter Begriffen wie «social physics» computergestützt bis in den letzten Winkel des gesellschaftlichen und individuellen Lebens vorangetrieben wird, ist die logische Konsequenz dieses Dranges zu wissen, was die Welt im Innersten zusammenhält.

Der Taxidienst Uber hatte 2014 Karten mit den One-Night-Stands seiner Fahrgäste erstellt, indem er die Fahrten zueinander in Beziehung setzte, die an einem Wochenende zwischen 10 Uhr abends und 4 Uhr morgens erst zu und dann von einer bestimmten Adresse erfolgten. Mit entsprechenden Informationen lassen sich weitere Informationen finden, das gilt nicht nur für Likes. Aber warum suchte Uber nach diesen Informationen? Wollte die Firma, ganz im Sinne des Überwachungskapitalismus, ihren Fahrgästen Werbung für Dating-Websites einspielen? Uber sprach von einem «analytischen Spielchen». Das befriedigt zwar kaum, bezeichnet aber sehr genau das Selbstverständnis der Datenwirtschaft: Alles, was miteinander in Bezug gesetzt und erkundet werden kann, wird auch in Bezug gesetzt und erkundet. Ist es aber nicht – wie in Politik und Eheleben auch – mitunter vernünftiger, bestimmte Dinge nicht zu wissen? Ist die Digitalisierung neben der Lösung des alten Problems, über sich Bescheid wissen zu müssen, auch die Quelle eines neuen Problems, indem sie mehr Selbsterkenntnis produziert, als der Gesellschaft guttut? Dieser Wissensüberschuss ist auch deswegen problematisch, weil er numerisch ausgerichtet ist. Das könnte zwar falsche Normvorstellungen korrigieren, sollte die Statistik belegen, dass der Ehebruch keine Ausnahme, sondern eine gängige kulturelle Praxis ist. Aber wie moralisch wäre eine Moral, die sich auf Zahlenverhältnisse beruft, gegenüber Minderheiten? Muss man nicht befürchten, dass Big-Data-Analysen numerische Normierungen hervorbringen, die alle möglichen Arten von Abweichung der Identifizierung und Stigmatisierung ausliefern?
Herrschaftswissen kontrollieren

Nicht unbedingt, wenn die Gesellschaft ihre Normen und ihren Umgang mit Normverstössen neu überdenkt. Und eben deswegen ist die Digitalisierung nicht nur eine Sternstunde der empirischen Sozialforschung, die noch nie so viele Daten zur Analyse der Gesellschaft zur Verfügung hatte wie jetzt. Die Digitalisierung drängt auch die kulturwissenschaftliche Soziologie à la Simmel oder Adorno zur Diskussion der kulturellen Veränderungen, die sie der Gesellschaft bringt. Diese Form der Soziologie fragt dann nicht primär, wie sich die Analyse von Verhaltensmustern optimieren lässt, sondern, welches Herrschaftswissen für welche Kontrollmassnahmen damit möglicherweise produziert wird und inwiefern diese Wissensproduktion begrenzt werden sollte. Das erwähnte Buch zu Big Data als Revolution endet mit dem Vorschlag, eine neue Berufsgruppe zu schaffen: die Algorithmiker. Diese sollen, in einer Mischung aus Unternehmensberatung und Verbraucherschutz, ausserhalb und innerhalb von Unternehmen die Sammlung und Auswertung von Daten überwachen sowie Ansprechpartner für Nachfragen und Beschwerden der Betroffenen sein. Ein interessanter Vorschlag, der sich mit unternehmerischen Überlegungen zur «corporate technical responsibility» deckt. Eine weitergehende Aufgabe dieser ethischen Technikkontrolleure wäre die Klärung der moralischen Unbedenklichkeit bestimmter Mustererkennungsvorhaben für eine Gesellschaft, die ihr demokratisches Selbstverständnis höher schätzt als ihre grenzenlose Selbstbeobachtung.

Was damit gemeint ist, ahnt man, wenn die Stadt San Francisco – ganz im Gegensatz zu China – Gesichtserkennungssoftware verbietet, weil diese ohne das Einverständnis der Individuen deren Bewegungsmuster erkennbar mache. Dass gerade der Ort, der so von den neuen Technologien lebt, altertümliche Datenschutzvorstellungen gegen diese ins Feld führt, mag überraschen. Andererseits: In einer solchen Stadt weiss man wohl am ehesten, welche Folgen welche Erfindung hat und wann es Zeit ist für ein «Jetzt reicht’s!».";https://www.nzz.ch/meinung/die-geburt-des-big-data-minings-aus-dem-geiste-der-aufklaerung-ld.1507689;NZZ;Roberto Simanowski;;;
24.08.2018;Wie Big Data die Finanzmärkte verändern könnte;"Ökonomische Daten aus China sind notorisch unzuverlässig. Zuverlässige Wirtschaftsstatistiken sind jedoch wichtig für Analytiker, die sich mit der Entwicklung der chinesischen Wirtschaft beschäftigen, um zum Beispiel die Nachfrage nach Öl und anderen Rohstoffen zu bewerten. Wie aber können Analytiker die Unzuverlässigkeit von Daten kompensieren? Eine Möglichkeit ist, sich auf alternative Daten zu stützen.

Bereits etabliert ist etwa die Methode, für Schätzungen des chinesischen BIP die Veränderung des dortigen Energieverbrauchs heranzuziehen. Ein neuer Vorschlag stellt auf die Analyse der nächtlichen Lichtemissionen chinesischer Städte ab. Wenn Investoren Daten verwenden, die nicht zu den traditionellen Datenquellen wie Finanzberichten, Management-Präsentationen oder Pressemitteilungen gehören, spricht man von «Alternative Data». Geht es dagegen um eine Vielzahl von Daten, die zu analysieren sind, ist die Bezeichnung «Big Data» üblich.
Grosse Versprechungen

Die Verheissungen von Big Data und Alternative Data sind enorm. Grossinvestoren und Fondsmanager, so die Propaganda, könnten etwa mittels Spracherkennungsalgorithmen aus den Reden von Managern und Zentralbankern Muster ableiten, die zu besseren Anlageentscheidungen führen; sie könnten Millionen von Twitter-Botschaften oder Kreditkartenzahlungen analysieren, um frühzeitig Trends aufzuspüren. Und, das häufig angeführte Paradebeispiel, sie könnten Algorithmen nutzen, die auf Satellitenbildern die Belegung der Parkplätze vor Retail-Unternehmen erkennen, um daraus Umsatzschätzungen abzuleiten.

Diese Verheissungen sind nicht ohne Wirkung geblieben; im vergangenen Jahr nutzte bereits die Hälfte aller Hedge-Fund-Manager alternative Daten, während es ein Jahr zuvor erst 30 Prozent gewesen waren. Aber auch bei herkömmlichen Long-only-Funds zeigt sich ein steigendes Interesse. Weil diese deutlich längere Investmentzyklen aufweisen als Hedge-Funds, etwa fünf Jahre gegenüber drei Monaten, dürfte sich die Jagd nach Talenten in Zukunft verschärfen.
Starker Ausbau der Kapazitäten

Bereits in den vergangenen fünf Jahren ist die Zahl der Big-Data-Spezialisten, die für Fonds arbeiten, enorm gestiegen, um beinahe 400%. Von unerwarteter Seite bekommt die Wall Street nun jedoch Hilfe im Gerangel um gute Big-Data-Fachkräfte. Noch vor ein paar Jahren war für Hochschulabgänger klar, wo sie am liebsten arbeiten wollten: im Silicon Valley. Die Arbeitsmentalität ist auf Kreativität ausgerichtet, inklusive Tischtennistischen und Starbucks-Lounges, die Bezahlung ist ausserordentlich hoch.

An der Wall Street müssen Hochschulabgänger, so wenigstens das Image, in den Anfangsjahren viele kräftezehrende Zwölfstundentage leisten, und der Ruf der Finanzindustrie ist auch einige Jahre nach der Krise ramponiert. Aber das Silicon Valley holt hier gerade stark auf. Amazon bezahlt viele seiner Angestellten so schlecht, dass sie auf Essensmarken angewiesen sind, Apple ist bekannt für seine Steuertricks, und Google gerät gerade in einen Strudel der Empörung, weil das Management offenbar die chinesische Zensur unterstützen wollte. Deswegen könnten sich in Zukunft wieder etwas mehr Big-Data-Spezialisten für die Wall Street entscheiden.

Fondskunden werden von diesen neuen Kapazitäten profitieren – das verspricht das entsprechende Marketing. Überlegene Anlageentschiede durch Datenanalyse bringen überlegene Performance, so lautet etwa das Credo der Fondsbranche. Diese Logik ist bestechend. Aber sie ist falsch. Leider ist es nicht so, dass an der Börse gute Informationen eine gute Performance bringen müssen. Das ist die Erkenntnis einer Untersuchung von vier sehr bekannten Ökonomen: Brad DeLong, Larry Summers, Andrei Shleifer und Robert Waldmann.
Eine Art adverser Selektion

Im Prinzip geht es darum, auf welche Art Märkte Gewinne ermöglichen. Big-Data-Proponenten gehen implizit oder explizit von einem ganz bestimmten Marktbild aus, das etwa folgendermassen aussieht: Aufgrund von Big Data lässt sich für manche Investments sehr gut der gerechtfertigte Preis abschätzen. Wenn der momentan bezahlte Preis weit von diesem Wert entfernt ist, lohnt sich ein Investment, weil der bezahlte Preis in Richtung gerechtfertigter Wert konvergiert. Auf lange Sicht erzielen rationale und kenntnisreiche Investoren bessere Renditen als Noise-Trader – eine Bezeichnung für Investoren, die ohne wirkliche Analyse investieren.

Brad DeLong und seine Kollegen haben jedoch gezeigt, dass Märkte nicht auf diese Art funktionieren müssen. Eine erstaunliche Erkenntnis ist, dass Noise-Trader langfristig höhere Renditen erwirtschaften können als ihre rationale Konkurrenz. Die Erklärung: Noise-Trader können die Marktpreise sehr weit vom rational zu erwartenden Preis wegtreiben, in einer nicht vorhersagbaren Weise. Dies schafft ein zusätzliches Risiko im Markt. Weil Noise-Trader aufgrund ihrer Investmentselektion ein höheres Risiko eingehen als rationale Investoren, können sie unter Umständen überproportional davon profitieren, insbesondere vom Risiko nichtrationaler Preise. Das von ihnen selbst geschaffene Risiko verschafft den Noise-Tradern also die höheren Renditen.

Eine beunruhigende Konsequenz ist der Selektionsmechanismus eines solchen Marktes; nicht die rationalen Investoren verbleiben im Markt, sondern die irrationalen. Die Marktpreise sind dann überaus stochastisch. Wer mit Big Data bestimmte Muster herausfiltern will, hat keinen Vorteil.
Zufallsbestimmte Märkte

Es gibt allerdings ein weiteres Szenario. Angenommen, die Vorstellung der Big-Data-Proponenten darüber, wie der Markt funktioniert, ist korrekt. Dann werden Big-Data-gestützte Anlagestrategien, mindestens in einer Anfangsphase, überdurchschnittliche Rendite erzielen, weil sie Preisbewegungen besser voraussagen können. Diese Renditen verleiten jedoch andere Marktteilnehmer dazu, ebenfalls auf Big-Data-Strategien zu setzen. Die Vielzahl an ausgefeilten datengestützten Strategien führt dazu, dass irrationales Verhalten bestraft wird und der Einfluss von Noise-Tradern auf die Preise langfristig schwindet.

Dann aber steigt die Effizienz der Märkte immer weiter an, bis im Prinzip jede Information weitgehend in den Preisen enthalten ist. Dies bedeutet, dass zukünftige Preisbewegungen weitgehend zufallsbestimmt sind und daher selbst mit guten Daten nicht prognostiziert werden können. In einer solchen Welt bringt Big Data keinen Mehrgewinn. Allerdings lässt sich vor diesem Zeitpunkt mit Big Data eine überdurchschnittliche Rendite verdienen. Das langfristige Ergebnis beider Szenarien ist jedoch ein zufallsbestimmter Markt, wobei im zweiten Szenario aufgrund des durch die Marktrationalität vorgegebenen Zwangs zu Big Data die Kosten höher sind.

Gegenwärtig ist unklar, ob eine dieser Möglichkeiten die Wirklichkeit besser trifft oder ob die Märkte eventuell zwischen den beiden Szenarien oszillieren. Allerdings scheint es bis jetzt nicht so zu sein, dass durch Big Data unterstützte Fonds überlegene Renditen erwirtschaften. Das heisst nicht, dass sich das nicht ändern kann. Deswegen sollten Investoren die weitere Entwicklung gut im Auge behalten. Unbedingt auf Big-Data-unterstützte Fonds setzen müssen Anleger momentan allerdings nicht.";https://www.nzz.ch/finanzen/fonds/wie-big-data-die-finanzmaerkte-veraendern-koennte-ld.1413856;NZZ;Patrick Herger;;;
29.01.2020;So bereitet sich die Schweizer Polizei auf Videofahndung mit Big Data vor;"Das Szenario klingt verlockend: Bei einem Raubüberfall zeichnet eine Überwachungskamera das Gesicht des Täters auf. Die Polizei kann die Aufnahmen in ihr System einspeisen, dieses sucht mittels automatischer Gesichtserkennung diverse Datenbanken nach dem Gesicht ab und spuckt eine Handvoll Personen mit einem ähnlichen Gesicht aus. Die Arbeit der Ermittlungsbehörden wäre praktisch erledigt.

Solche Systeme kommen zumindest in ähnlicher Form bereits zum Einsatz – und zwar nicht nur in China, sondern auch in den USA. Kürzlich sorgte das System der Firma Clearview für Schlagzeilen, weil deren Datenbank 3 Milliarden Bilder aus öffentlich zugänglichen Online-Quellen wie Facebook umfasst. Auch Amazon bietet Gesichtserkennungssoftware an, mit der amerikanische Strafverfolgungsbehörden in Polizeidatenbanken nach erfassten Personen suchen können. Wie hoch die Trefferquote ist, bleibt allerdings unklar.
Skepsis gegenüber Gesichtserkennung vorhanden

In der Schweiz sind die Polizeikorps noch deutlich weniger weit. Automatisierte Gesichtserkennung ist offiziell noch kaum ein Thema, wie eine unvollständige Umfrage bei grossen Korps und bei IT-Experten zeigt. Das mag an den fehlenden gesetzlichen Grundlagen liegen oder aber an der Skepsis gegenüber «chinesischen Verhältnissen», wie es ein Gesprächspartner nennt, der anonym bleiben will. Technisch seien die Voraussetzungen zwar gegeben, sagt er: «Doch beim Gedanken daran stehen mir die Haare zu Berge.»

Gleichzeitig ist das Interesse gross, Kameraaufnahmen automatisch auszuwerten. Denn die Zahl der Kameras steigt und damit auch die Menge an verfügbaren Aufnahmen, die für die Ermittlungsbehörden von Interesse sind. Denn nicht nur die Behörden setzen Kameras ein, sondern auch Unternehmen und Privatpersonen. Zudem existieren bei Ereignissen auch Aufzeichnungen von Smartphones.

Doch je mehr Filme und Fotos zur Verfügung stehen, desto aufwendiger ist die manuelle Auswertung. Das Anschauen und Analysieren braucht Zeit. Ähnlichkeiten oder bestimmte Muster erkennt ein Algorithmus möglicherweise gar besser als der Mensch.
Bereits «interessante Kandidaten» getestet

Mehrere Polizeien verfolgen deshalb die technische Entwicklung in diesem Bereich genau und haben auch schon Produkte getestet. Eine von ihnen ist die Kantonspolizei St. Gallen. Bisher seien fünf Systeme getestet worden, sagt der zuständige Experte Serdar Günal Rütsche. «Darunter gibt es sehr interessante Kandidaten.» Günal Rütsche rechnet damit, dass es in den nächsten zwei Jahren zu einer Beschaffung kommen wird.

Dabei steht allerdings nicht die automatisierte Gesichtserkennung im Vordergrund. Am Anfang einer Ermittlung stünden meist Objekte mit bestimmten Merkmalen, sagt Günal Rütsche. Das kann zum Beispiel ein grosser blonder Mann sein, der in einem dunklen Kleinwagen wegfährt. «Für uns muss die Software solche Objekte erkennen können, egal ob das Menschen sind oder nicht.» Denn die Verdächtigen können auch vermummt sein.

Dass die Gesichtserkennung noch keine Rolle spielt, hat auch mit technischen Faktoren zu tun. Meist ist die Qualität der Aufnahmen zu schlecht – sei es wegen der geringen Kameraauflösung oder der schlechten Lichtverhältnisse. «Die Software funktioniert im Verkaufsprospekt tadellos, aber eben nur unter idealen Bedingungen», sagt Günal Rütsche. Ein Faktor sei auch die Positionierung der Kamera: Meistens filmen Überwachungskameras von oben, um einen möglichst grossen Bereich zu erfassen – was aber die Identifizierung von Personen eher erschwert.
Luzerner Polizei kann automatisch suchen

Die Kantonspolizei Luzern setzt bereits seit rund drei Jahren ein Computersystem ein, das die Suche nach Objekten erlaubt. Allerdings hat der Funktionsumfang wenig mit der Technik aus James-Bond-Filmen zu tun. Die Luzerner Polizisten können in der Aufzeichnung einer Überwachungskamera nach Personen mit auffälligen Kleidungsstücken oder nach Fahrzeugen in bestimmten Farben suchen. Gesichter erkennt die Software nicht. Auch müssen die Merkmale manuell eingegeben und die Treffer ebenso ausgewertet werden, wie die Medienstelle schreibt. Das System analysiert keine Aufnahmen in Echtzeit.

Trotzdem erleichtert und beschleunigt das Programm die Arbeit der Polizisten. Sie können beispielsweise auf Kameraaufzeichnungen ein rotes Fluchtauto suchen, um so die Fluchtrichtung der Täter zu sehen. Oder die Ermittler können rasch auf die Aufnahme eines vermummten Täters, der zu Fuss flüchtet, zugreifen und erkennen dank einer hochauflösenden Kamera weitere Merkmale wie eine Tätowierung. Dank der automatischen Suche sparen die Polizisten Zeit.
Augenzeugen sollen künftig ihre Videos einschicken

Die Automatisierung gewisser Vorgänge ist Voraussetzung, um künftig überhaupt Kameraaufzeichnungen im grösseren Umfang auswerten zu können. Die Kantonspolizei St. Gallen bereitet sich auf diese Zeit vor, wie Günal Rütsche sagt. Künftig will sie vermehrt Zeugen dazu aufrufen, eigene Aufnahmen mit der Smartphonekamera hochzuladen.

Die Kantonspolizei hat dieses Mittel bereits zweimal versuchsweise angewendet, zuletzt bei einem Grossbrand in Mels im November. «Die ersten Minuten eines Brandes können für die Ermittler sehr interessant sein», sagt Günal Rütsche. Rund 50 Aufnahmen sind nach dem Aufruf eingetroffen. Eine Menge, die die Polizei heute noch bewältigen kann.

Bei einem Grossereignis jedoch, bei dem die Polizei Hunderte von Smartphoneaufnahmen erhält, geht die Auswertung nicht mehr manuell. Hierfür soll dereinst eine Software Unterstützung bieten, wie Günal Rütsche sagt: «Das System soll automatisch diejenigen Aufnahmen aussortieren, die dasselbe Ereignis aus gleicher Perspektive zeigen.»
Liste mit allen Personen in einer Videosequenz

Zum repetitiven Teil der Arbeit der Polizeien gehört auch das Erkennen von Gesichtern. So ist für Günal Rütsche ein Szenario denkbar, bei dem die Polizei zum Beispiel alle Personen sehen will, die in mehreren Stunden Videomaterial vorkommen. Diese Auswertung kann manuell geschehen, aber auch automatisch: Die Software präsentiert von jeder Person, die in der Aufnahme vorkommt, das beste Gesichtsbild. Die Arbeitserleichterung für die Ermittler ist enorm, auch wenn das System keine Identifizierung der Personen vornimmt.

Doch die automatische Identifizierung eines Verdächtigen ist nur der nächste logische Schritt. Haben die Ermittler im Videomaterial eine brauchbare Aufnahme des mutmasslichen Täters gefunden, so müssen sie heute auf die menschliche Fähigkeit zur Gesichtserkennung setzen. Einige Polizeien setzen Mitarbeiter ein, die sich Gesichter besonders gut merken können. Teilweise verschicken die Ermittler solche Gesichtsbilder auch manuell an andere Polizeikorps – in der Hoffnung, dass das Gesicht dort bekannt ist.
Politische Frage wird kommen

Eine Software, die polizeiliche Datenbanken automatisch nach den Gesichtern von Verdächtigen durchsuchen kann, wäre eine grosse Vereinfachung. Ein Polizeiinformatiker, der anonym bleiben will, hat ein solches System vor gut zwei Jahren ausprobiert. Sein Fazit: «Nicht zu gebrauchen.» Die Qualität der Aufnahmen sei zu schlecht für einen Abgleich. Doch dass sich das ändert, ist nur eine Frage der Zeit. Dann stellt sich nicht mehr die technische, sondern nur noch die politische Frage: Will die Schweiz solche Systeme zur automatischen Gesichtserkennung einsetzen?";https://www.nzz.ch/schweiz/wie-sich-die-schweizer-polizei-auf-videofahndung-mit-big-data-vorbereitet-ld.1536559;NZZ;Lukas Mäder;;;
28.06.2019;Big Data raubt dem Sport die Romantik;"Im August 2018 hatte Jayson Werth einen Ausbruch. Werth, 40, ist ein ehemaliger Baseballprofi, er hat in seiner Karriere 140 Millionen Dollar verdient. Doch Werth scheint die Freude vergangen zu sein, er sagte: «Es tötet den Sport. Wir sind an einem Punkt angelangt, an dem wir einfach Laptops auf das Feld stellen könnten. Es braucht uns Spieler nicht mehr. Es ist ein Witz.»

Es, das ist Big Data, die kühle Logik der Zahlen. Sie hat den Baseballsport in der letzten Dekade radikal verändert, bis zur Verzerrung fast. Nie liess sich mit Baseball mehr verdienen als jetzt, Mike Trout wird bei den Los Angeles Angels in den nächsten 12 Jahren 432 Millionen Dollar erhalten, es ist der lukrativste Vertrag in der Geschichte des Teamsports. Geld spielt keine Rolle mehr, die Medienrechte haben für eine Goldgräberstimmung gesorgt, doch der Sport befindet sich in der Krise. Die Zuschauerzahlen sinken, das Publikum ist überaltert, die Partien dauern zu lange und sind zunehmend öde – es gibt zu viele Unterbrüche, zu wenig Aktion. Die Teams füllen ihre Kader vermehrt mit Spezialisten statt Allroundern, was die taktische Variabilität erhöht, das Spiel aber weiter verlangsamt – der Sport kennt keine Wechselbeschränkungen.

Glaubt man Jayson Werth, ist die Entwicklung nicht zuletzt den «Super-Nerds» zu verdanken, die den Baseball in eine Art Schachspiel verwandelt haben. Denn kaum ein Sport lässt sich besser vermessen und in Statistiken pressen, es gibt unzählige Einzelsituationen und keinen kontinuierlichen Spielfluss. Für jeden erdenklichen Spielzug, für jede Eventualität existieren komplexe Wahrscheinlichkeitsberechnungen. Profis in der Major League Baseball (MLB) sind längst zu gläsernen Menschen geworden – und ihre Coachs geben Anweisungen weiter, die ein Computer errechnet hat. Es gibt Teams, die den Zahlen mehr vertrauen als der menschlichen Intuition. Die Frage ist tatsächlich, wie weit die überzeichnete Vision Werths unter diesen Gegebenheiten noch von ihrer Realisierung entfernt ist. Einer, der sie zu teilen scheint, ist Bill James, der in den 1990er Jahren zahlreiche Statistiken entwarf. James sorgte im Herbst für einen Aufschrei, als er sagte, es würde für das Produkt MLB keine Rolle spielen, wenn sämtliche aktiven Profis morgen zurückträten. Sie seien austauschbar geworden, innerhalb von drei Jahren wären sie alle vergessen und ersetzt.

Diese Entwicklung war so nicht abzusehen gewesen. Denn Baseball ist ein Sport der Traditionen, «America’s pastime» quer durch alle Gesellschaftsschichten hindurch, er faszinierte Frank Sinatra ebenso wie die Arbeiterklasse oder den Schriftsteller Philip Roth, der dem Sport in «The Great American Novel» ein ganzes Buch gewidmet hat. Baseball hing lange der Ruf des Verruchten nach. In diesem Jahr jährt sich eine der schwersten Krisen in der Geschichte des Sports zum 100. Mal: Acht Spieler der unterbezahlten Chicago White Sox liessen sich 1919 von einem Syndikat schmieren, um den Play-off-Final, die World Series, gegen Cincinnati zu verlieren. Es war einer der ersten dokumentierten Wettbetrugsfälle weltweit. Der Skandal ramponierte das Image der Liga; sie galt als Hort für Spieler, Alkoholiker und Tunichtgute.
Mächtige Männer in Anzügen

Doch heute regieren nicht den Verlockungen des Lebens verfallene Trinker die Liga, sondern Anzugträger, Abgänger von Eliteuniversitäten. Fast die Hälfte der General Manager verfügt über einen Abschluss von einem Prestige-College wie Harvard, Dartmouth oder Cornell. Es sind blitzgescheite Männer, die nicht zwingend aus dem Baseballmetier stammen, sich dafür aber mit Algorithmen und Datenanalyse auskennen. Eingeleitet wurde der Trend um die Jahrtausendwende in Kalifornien von den Oakland Athletics, einem Modernisierungsverlierer in einem kleinen Markt auf der weniger glamourösen Seite San Franciscos, ausgestattet mit einem winzigen Budget. «Der Dschinni ist draussen, und ich glaube nicht, dass er wieder in die Wunderlampe zurückgeht», sagt Billy Beane, der wichtigste Funktionär der Athletics. Beane spielt nicht auf die gerade über die Kinoleinwände flimmernde «Aladdin»-Adaption an, sondern auf den Einfluss von Big Data im modernen Sport.

Beane, 57, muss es wissen, er war eine Art Revolutionär, ein lakonischer Baseballpirat, der sich mit der Sezierung neuartiger Statistiken einen Wettbewerbsvorteil verschaffte. Oakland spielte trotz signifikanten monetären und infrastrukturellen Defiziten jahrelang um den Titel mit. Beanes sehr amerikanische Aufsteigergeschichte ist im Buch «Moneyball. Die Kunst, in einem unfairen Spiel zu gewinnen» brillant nacherzählt worden, 2011 wurde es mit Brad Pitt in der Hauptrolle in Hollywood verfilmt.
Die Epoche der Postromantik

Die «Moneyball»-Ära ist lange vorbei, doch sie war der Wegbereiter für das Zeitalter von Big Data, eine postromantische Epoche. Die Unberechenbarkeit ist eine der wichtigsten Facetten, die den Sport so faszinierend macht. Es hat etwas Deprimierendes, wenn einem ein Algorithmus den Idealismus raubt – und sei dieser noch so fehlgeleitet.

Die Armada an Datenanalysten versucht die integralen Eckpfeiler des Live-Sporterlebnisses nach Kräften zu zerstören. Nicht vorsätzlich, aber es verursacht doch einen Kollateralschaden. Der Trend ist unaufhaltsam, es geht im Profisport des 21. Jahrhunderts um zu viel Geld, als dass die milliardenschweren Klubbesitzer etwas dem Zufall überlassen würden. Die durch Big Data ausgelösten Veränderungen betreffen längst nicht mehr nur die MLB, sondern alle Teamsportarten, in denen genug Geld steckt. Die Basketballteams in der NBA haben ihren taktischen Stil aufgrund von Big Data verändert, es werden so viele Dreipunktewürfe abgegeben wie nie zuvor – federführend sind die Houston Rockets, wo der statistikversessene General Manager Daryl Morey den Ton angibt.

Die Eishockey-Profiliga NHL führt ab dem Start der Saison 2019/20 eine Tracking-Technologie für alle Spieler ein und verpflanzt Mikrochips in ihre Pucks. Auch in der NHL gibt es hochkomplexe Statistiken – und kluge Köpfe, die sie analysieren. Die Arizona Coyotes machten 2016 den 26-jährigen John Chayka zum jüngsten General Manager der Ligageschichte. Chayka war eines der Gründungsmitglieder eines Startups für eishockeyspezifische Datenanalyse. An solchen Firmen mangelt es nicht: Etliche Anbieter behaupten, sehr effektiv Daten zu schürfen. Reto Kläy, der Sportchef des EV Zug, des in der Schweiz in Sachen Datenanalyse vermutlich fortschrittlichsten Klubs, sagt: «Daten erleichtern vieles, sie helfen einem Trainer beispielsweise, Entscheide zu treffen und zu begründen. Doch man kann Statistiken auch bis zur Unkenntlichkeit durchleuchten und verweben, es ist Vorsicht geboten. Es hat es noch niemand geschafft, Eishockey komplett zu entschlüsseln.»

Die in der letzten Dekade erzielten technologischen Fortschritte sind enorm, doch noch fehlt ein Algorithmus, der die weichen Faktoren abseits des Spielfelds decodiert. Es ist nicht so, dass die US-Profiteams es nicht versuchen würden. In den Interviews mit Talenten vor dem Draft werden die eigenwilligsten Fragen gestellt: Wenn du mit einem Mord davonkämst, würdest du ihn begehen? Hier ist eine Pille, sie garantiert dir den Gewinn der Meisterschaft, bedeutet darauf aber auch deinen Tod. Schluckst du sie? Es soll helfen, ein Bild der Persönlichkeit zu zeichnen, zeugt aber in erster Linie von intakter Phantasie.

Derweil ist Big Data auch in der Major League Soccer, der Fussball-Profiliga Nordamerikas, stark verankert – die Datenanalyse hat den Transfermarkt und die spieltaktischen Analysen global revolutioniert. Am ausgeprägtesten bleiben die Auswüchse jedoch im Baseball, wo alles eine Rolle zu spielen scheint: in welchem Grad sich der Ellbogen eines Pitchers im Moment der Wurfabgabe befindet. In welchem Winkel der Schlagmann den Ball trifft.

Am Samstag und Sonntag gastiert die MLB zum ersten Mal in ihrer Geschichte in Europa, sie hat für das Gastspiel in London zwei ihrer prestigeträchtigsten Organisationen entsandt: Die New York Yankees, deren Logo global auf der Kopfbedeckung von Millionen Menschen prangt, ohne dass die meisten von ihnen auch nur einen einzigen Spieler nennen könnten. Und den derzeitigen Champion Boston Red Sox, den ewigen Rivalen der Yankees. Die Veranstaltung wird ein Erfolg werden, ein Medienspektakel; alle 120 000 Tickets sind verkauft, der Schwarzmarkt floriert. Baseball hat im Vereinigten Königreich keine Basis, doch vielleicht ist genau das für die Besucher die Chance, die Partien zu geniessen. «Ignorance is bliss», hat der Poet Thomas Gray einmal geschrieben, Unwissenheit bedeute Glückseligkeit. Was den modernen Baseball betrifft, könnte das tatsächlich zutreffen, weil das Staunen bewahrt, wer von all den Formeln nichts weiss.

Bei Live-Übertragungen im TV gibt es in den USA oft Einblender mit einer Siegwahrscheinlichkeitsvoraussage. Der Bestsellerautor Michael Lewis, der auch «Moneyball» verfasst hat, hielt kürzlich eine Brandrede dagegen. Er sagte: «Lasst mich mit all diesen Zahlen in Ruhe. Ich will das Spiel mit meinen eigenen Emotionen und Gedanken verinnerlichen.» Und er sagte auch: «Die Datenanalysen haben die Emotionen aus dem Baseball abfliessen lassen. Baseball war der Stoff von grossen Dramen, aber heute geht es nur noch um Zahlen und Videos.»

Die Sätze hörten sich an wie die Ode eines Nostalgikers an vergangene Zeiten. Es wird keine Rückbesinnung geben, Big Data ist dafür längst zu mächtig geworden. Doch es gibt Menschen, denen sprach Lewis mit seiner Sehnsucht nach mehr Romantik aus dem Herzen. Irgendwo wird Jayson Werth milde gelächelt haben.";https://www.nzz.ch/meinung/big-data-raubt-dem-sport-die-romantik-ld.1491829;NZZ;Nicola Berger;;;
02.07.2019;Auf der Jagd nach den Geheimnissen der Datenflut;"Der Alltag von Unternehmensberatern besteht aus Excel-Tabellen und Powerpoint-Präsentationen, könnte man meinen. Doch wenn überhaupt, dann stimmt das immer seltener. Im sich schnell entwickelnden digitalen Zeitalter braucht es mehr als Excel, um der Datenflut Herr zu werden, und mehr als Powerpoint, um das Ergebnis darzustellen. Für die besonders kniffligen Fälle hat McKinsey, eine der weltgrössten Consulting-Firmen, ein eigenes Unternehmen: QuantumBlack. «Wir konzentrieren uns auf Analysen und nichts anderes. Wenn es ein besonders komplexes Problem gibt oder man Experten braucht, dann ruft McKinsey uns», sagt der Chef Jeremy Palmer.
So wie früher, nur ganz anders

Palmer leitet QuantumBlack seit 2014 und übernahm das Ruder, kurz bevor die Firma von McKinsey gekauft wurde. Dass die amerikanische Unternehmensberatung die in London beheimateten Spezialisten an Bord holte, schreibt der QuantumBlack-Mitbegründer Simon Williams den gestiegenen Anforderungen zu: «Beratung drehte sich schon immer um Daten und Analyse. Aber die Grössenordnung, die Geschwindigkeit und das erforderliche Raffinement bei der Auswertung haben sich total verändert», erklärt Williams, der im Direktorium sitzt.

Früher habe man bei einem Problem mit einem Projekt nur die Projektdaten angeschaut und bei einem Vertriebsproblem nur die Vertriebsdaten, sagt Williams. QuantumBlack macht es anders: Die Spezialisten lassen sich Datensätze aus fast allen Bereichen eines Unternehmens geben, darunter aus der Buchhaltung, der Personalabteilung und vom internen E-Mail-Verkehr. Dann versuchen sie, mittels Analyseprogrammen und Algorithmen Zusammenhänge zu finden – zum Beispiel anhand des E-Mail-Verkehrs die Information, dass sich die Technikabteilung kaum mit der Designabteilung ausgetauscht hat. Es gehe immer darum, die Daten zu nutzen, die in einer Organisation vorhanden seien, um so zu erklären, was die Leistung beeinflusse, sagt Williams.

Der Firmenchef Palmer sieht einen Kreislauf: Das Volumen an verfügbaren Daten nimmt exponentiell zu, gleichzeitig wächst die Verarbeitungskapazität. Es brauche stets neue Methoden, diese Daten auszuwerten, und sie müssten auch immer schneller entwickelt werden. «Breite Datenanalyse war früher eine Konsumentengeschichte, vor allen in der Werbung und im Detailhandel. Erst seit einer Weile schaut man es sich aus unternehmerischer, operativer Perspektive an», sagt er. Dieser Wandel schürt den Bedarf nach Spezialisten: Heute zählt QuantumBlack rund 300 Mitarbeiter. Ende des Jahres 2015, als McKinsey das Unternehmen kaufte, waren es nur 45. Angefangen hatte QuantumBlack im Jahr 2009 in der Formel 1: Simon Williams und zwei Mitbegründer entwickelten eine Strategiesoftware für einen Rennstall, mit der während des Rennens die Leistungs- und Motordaten analysiert werden (die mysteriösen Anzeigen der Bildschirme in der Boxengasse). Aber es gibt zwei Rennen in der Formel 1, wie es Williams formuliert: «Erstens das einzelne Rennen am Sonntag und zweitens das Rennen über ein Jahr hinweg, um innovativer zu sein als die Konkurrenz und um die Weltmeisterschaft zu gewinnen.» Ein Rennstall lanciere pro Jahr 3000 Forschungsversuche, davon schlügen 97% fehl. Um das zu ändern, kam QuantumBlack auf die Idee, den Fokus zu weiten und auch Finanz-, Management- und Kommunikationsdaten auszuwerten.

Das Wachstumspotenzial in der Formel 1 ist allerdings begrenzt. An dieser strategisch entscheidenden Stelle kann eine Beratungsfirma nur für einen einzigen Rennstall arbeiten. Also expandierte QuantumBlack in andere Branchen, arbeitete unter anderem für ein grosses Eisenbahninfrastrukturprojekt in London und wurde Teil von McKinsey – auch aus Eigeninteresse: Für das Beratungsgeschäft brauche man mehr als die Analysefähigkeit, sagt Williams. McKinsey bringe das Fachwissen über Branchen und über Managementprozesse ein, es sei eine symbiotische Beziehung.

Heute arbeiten die Fachleute von QuantumBlack zusammen mit Kunden aus der Pharma- und der Finanzbranche, dem Autobau, der Luftfahrt sowie dem Rohstoff- und dem Telekommunikationssektor. Ein grosser Teil der Aufgabe besteht darin, die Unterschiede beim Datenmaterial in den Griff zu bekommen: Finanzdaten werden anders formatiert und gespeichert als Daten von Projekten oder aus der Personalabteilung. «Wir müssen die Datenquellen prüfen, bereinigen, maschinenlesbar machen und verbinden. Erst dann können wir Zusammenhänge suchen und darstellen», erklärt Palmer. Entscheidend seien immer die Sicherheit und die Privatsphäre. Alle personenbezogenen Daten würden anonymisiert. «Wir schauen uns nie Namen an, uns interessieren die Prozesse», beteuert er.

Zehn Jahre nach der Gründung muss QuantumBlack auch auf die eigenen Prozesse schauen. Die Firma wächst schnell, unterhält mittlerweile sieben Büros rund um die Welt und hat die Mitarbeiterzahl seit der Übernahme im Jahr 2015 mehr als versechsfacht. Dennoch entsprechen die 300 Angestellten nur rund einem Prozent der globalen Belegschaft von McKinsey. Man werde dieses Einstellungstempo in Zukunft bremsen, sagt der Chef Palmer. Das weltweite Büronetz sei bald errichtet, und dann stehe eher Konsolidierung als Expansion auf dem Programm.
Die Wurzeln bewahren

Eine Rolle könnte dabei auch die Bewahrung der eigenen Identität spielen, wie es ehemalige Mitarbeiter gelegentlich in Internetforen anmahnen: Die Angestellten von QuantumBlack sind Datenfachleute, keine Berater, und ihre Firma ist einem lockeren Tech-Startup näher als dem von Schlips und Kragen geprägten Betrieb des McKinsey-Konzerns. Skeptiker befürchten, dass sich die Gewichte in der Unternehmenskultur verschieben. Der Mitbegründer Williams sieht das gelassen: McKinsey habe durch die Übernahme seine Kultur erweitern wollen, deshalb beschütze das Beratungsunternehmen den Unterschied, sagt er. «Was wir machen, ist anders. Das haben sie gesucht, und das wollen sie behalten.»";https://www.nzz.ch/wirtschaft/big-data-wie-quantum-black-fuer-mckinsey-datengeheimnisse-jagt-ld.1489744;NZZ;Benjamin Triebe;;;
28.08.2019;Psychiatrische Diagnostik soll sich Big-Data-Technologie öffnen – das Humane darf aber deswegen nicht zu kurz kommen;"Die Diagnostik, also das Erkennen und Bezeichnen von Krankheitszuständen, ist ein etabliertes, im Grundsatz unbestrittenes Element der Medizin. In einem ihrer Teilgebiete, der Psychiatrie, liegen die Dinge wieder einmal komplizierter: Seit sich das Fach im 19. Jahrhundert zu einer eigenständigen Disziplin entwickelte, kommt die Debatte über Nutzen und Risiken psychiatrischer Diagnosen und die ihnen zugrunde liegenden Krankheitsmodelle nicht zur Ruhe.

Zwei Kritikpunkte stechen hervor: Diagnosen seien inakzeptable soziale Demarkationslinien. Überdies verfehlten sie als formalisierte Konstrukte die Individualität der Person in ihren Lebensvollzügen. Jüngst formierte sich eine weitere kritische Phalanx von eher unerwarteter Seite: Die Neurowissenschaft setzt auf objektivierbare und quantifizierbare Parameter und die Künstliche-Intelligenz-Forschung auf sehr grosse, auch durch soziale Netzwerke und Gesundheits-Apps generierte Datensätze.
Überkommene Denkmuster?

Was treibt diese Entwicklung an? Die heutige psychiatrische Diagnostik – wie das in der Schweiz verbindliche Diagnos-Manual ICD-10 der WHO – basiert auf berichteten oder beobachteten psychopathologischen Symptomen, etwa depressive Verstimmung, Sinnestäuschungen, Wahngedanken. Diese Symptome werden definiert und gruppiert; Verknüpfungsregeln geben den Entscheidungsweg vor, nach dem sich der diagnostische Prozess zu richten hat. Referenz­punkte sind dabei das klinische Erscheinungsbild sowie seit dem frühen 20. Jahr­hundert tradierte Krankheitsentitäten wie Schizophrenie oder bipolare Erkrankung.

Die Kritik setzt genau hier an und kommt zu einem beunruhigenden Schluss: Die gängige Diagnostik beharre in überkommenen Denkmustern und verkenne die Chancen innovativer Ansätze, ja sie sei ein veritables Forschungshindernis. Brauchen wir nicht, so fragen die Kritiker, diagnostische Prozeduren ganz anderer Art, die sich weniger an Erleben und Verhalten der betroffenen Person, an stark subjektiv ge­färbten Phänomenen, ausrichten, sondern an objektivierbaren, meist biologischen Parametern? Wird uns nicht die in grossem Stil betriebene Auswertung von Daten über Menschen mit einer, sagen wir, psychotischen Erkrankung viel mehr Neues über diese psychopathologischen Zustände lehren als die blosse Subsumierung individueller Symptome unter die ICD-10-Diagnose «Schizophrenie»?

Die Protagonisten bejahen diese Fragen und prognostizieren ein neues Begriffsraster für die Psychia­trie, das quer zu den traditionellen Krankheitsein­heiten stehen und diese früher oder später ablösen werde. Nicht starre Diagnosen seien das Ziel, «hinter» denen vorgegebene Krankheiten stünden, sondern fluide Mo­delle eingeschränkter kognitiver, affektiver und sozialer Funktionen. Dies werde dem Individuum weit eher ge­recht und biete zudem fruchtbare Ansatzpunkte für die psychiatrische Forschung. «Denosologisierung», der gewollte Verlust der Lotsenfunktion klassischer Krankheitsbegriffe, avancierte zum Markenzeichen aktueller Diagnose-Kritik. All dies führte in den letzten Jahren zu einem ebenso bunten wie dissonanten Reigen von Voten: Er reicht von der eisernen Verteidigung der klassischen Diagnose über deren Verknüpfung mit modernen Forschungsmethoden bis hin zur prinzipiellen Ablehnung. Nun ist eine solche Meinungsvielfalt kein Nachteil, ganz im Gegenteil: Die Psychiatrie muss mehrdimensional sein, so wie es auch ihr Forschungs-«Gegen­stand» ist, die psychisch erkrankte Person. Aber wir dürfen das Kind nicht mit dem Bade ausschütten: Die Perspektivenvielfalt unseres Faches muss auf einem Verständnis der Psychiatrie als interpersonal konstellierter Handlungswissenschaft auf­bauen.
Diagnose – und Beziehung

Mit anderen Worten: Die Forschung zu biologischen Markern psychischer Erkrankungen sowie das Durchforsten riesiger Datenmengen auf der Suche nach neuartigen Zusammenhängen sind vielversprechende Ansätze. Doch werden diese quantifizierenden Methoden das qualitative Moment psychiatrischen Arbeitens, vor allem die therapeutische Beziehung, nicht ersetzen können. Als Technik ist die Diagnose quantifizierbar und normierbar. Sie ist aber stets auch Beziehung, und als sol­che umfasst sie Subjektivität und Interpersonalität.

Wir brauchen ein patientenzentriertes Zusammen-Denken dieser beiden Welten: «Individuum plus Big Data» sozusagen. Psychiatrisch diagnostizieren heisst, zum handlungs-, also therapierelevanten Problem-Kern vorzudringen. Das bedingt die konsequente Zusammenschau biologischer, psychischer und sozialer Faktoren, aber eben nicht in je punktueller Verkürzung, sondern unter Einbezug des Erlebens- und Werthorizontes der Person, um die es geht. Ebendieses Personale in der Psychiatrie – immer als Element der Medizin – darf im Zeitalter der Digitalisierung weder naiv idealisiert noch wissenschaftlich dis­kreditiert werden. Das ist und bleibt eine grosse Aufgabe.";https://www.nzz.ch/meinung/psychiatrische-diagnostik-soll-sich-big-data-technologie-oeffnen-das-zwischenmenschliche-darf-aber-deswegen-nicht-zu-kurz-kommen-ld.1498258;NZZ;Paul Hoff;;;
16.04.2019;Privatsphäre spielt in Singapurs Big-Data-Welt nur eine Nebenrolle;"Die klassische Erfassung der Arbeitszeiten, Anwesenheitskontrollen oder der Blick in die Kamera am Zoll bei der Einreise könnten bald der Vergangenheit angehören. Face Pro, ein System von Panasonic, das in Zusammenarbeit mit der Singapurer National University (NUS) entwickelt worden ist, ist angeblich in der Lage, bis zu 30 000 Gesichter gleichzeitig zu erkennen und in Echtzeit mit registrierten Datensätzen abzugleichen. Theoretisch liesse sich dank dieser Technologie eruieren, wer an der Uni in der Vorlesung war, wer in der Fankurve dem Fussballmatch beiwohnte, am Freitag die Moschee aufsuchte oder wer was, wann oder wie oft bei Aldi einkaufen geht.
«Endlose Möglichkeiten»

«Die Möglichkeiten sind endlos», meinte der für das System zuständige Yoshinori Yamana des japanischen Elektronikgiganten bei der Präsentation in Singapur: Der Detailhandel wartet auf den gläsernen Kunden, Personenfahndungen verlagern sich auf den Bildschirm, und wer das Flugzeug betritt oder an einer Konferenz teilnimmt, wäre bei der Anwendung dieser Technologie längst vor-identifiziert. Grossflächig soll das System laut Yoshinori Yamana erstmals anlässlich der Sommerolympiade 2020 in Tokio eingesetzt werden, wenn das Land bis zu 40 Mio. ausländische Besucher erwartet.

Japan, China und Südkorea mögen bei solchen Anwendungen und dem Einsatz von 5G in Asien derzeit führend sein; aber Singapur will technologisch mithalten und vor allem den Zug in Richtung Artificial Intelligence (AI) nicht verpassen. Um der seit längerem beobachteten Verlangsamung des Wirtschaftswachstums entgegenzutreten und einen neuen Innovationsschub einzuleiten, setzt der Kleinstaat wie schon in der Vergangenheit darauf, dass sich auf diesem Gebiet hier Spitzenunternehmen aus aller Welt ansiedeln. In den früheren Wachstumszyklen kamen die Elektronik- und Erdölfirmen, später Banken, die Pharmaindustrie und Logistikunternehmen. Jetzt läutet man gewissermassen eine neue Wachstumswelle ein. Jedenfalls vergeht kein Tag, ohne dass in regierungsnahen Organen und Fachzeitschriften das hohe Lied der AI gesungen wird.

Man strebt den Status einer Smart Nation an. So fördern die Behörden hier unter anderem Tests für selbstfahrende Autos, treiben E-Governance und den bargeldlosen Zahlungsverkehr an; der Staat verfolgt ferner Blockchain-Entwicklungen und ist drauf und dran, eine flächendeckende Überwachung des Territoriums zu realisieren. Big Data wird entsprechend in erster Linie mit zusätzlicher Sicherheit, effizienteren Anwendungen, Produktivitätsgewinnen und neuen Geschäftsfeldern assoziiert. Im Rahmen der Initiative «AI Singapore» bündelt der Stadtstaat derzeit denn auch Kräfte, um der Industrie bei der Anpassung zu helfen und neue Lehrpläne für Hochschulen zu entwickeln. Der Staat steht über jedem Verdacht

In welche Grenzbereiche sich die Wissenschaft (und Anwender) vortastet, wurde anlässlich des diesjährigen Stars-Seminars in Singapur klar, das bezeichnenderweise unter dem Motto «Future Ready Asia» stand. Ong Yew Soon, der an der Nanyang Technological University (NTU) forscht, prophezeite, dass man bald weit über die Reproduktion klassischer menschlicher Sinne (wie Hören und Sehen) werde hinausgehen können. Über hochentwickelte Systeme der Gesichtserkennung werde es bald möglich sein, Gemütsstimmungen wie Freude oder Aggressivität zu erfassen; eines Tages werde man vielleicht gar Instrumente oder Datenanalysen einsetzen können, die als Witterung bisher nur in der Tierwelt anzutreffen sind.

Wo aber liegen die gesellschaftlichen Grenzen dieser Entwicklung? Themen wie Schutz der Privatsphäre, Datensicherheit und Sinnhaftigkeit von Big Data und den damit verbundenen neuen Analysen werden hierzulande erst seit kurzem behandelt. Ein Auslöser dazu waren Datenlecks im singapurischen Gesundheitssystem, von denen das eine auf menschliches Versagen, das andere auf einen erfolgreichen Hackerangriff zurückzuführen sind. Beide haben das Vertrauen in das System und das bisher ziemlich gute Renommee der Regierungstechnokraten erschüttert. Als Reaktion darauf hat die Regierung eine technische Überprüfung aller Informatiksysteme des öffentlichen Dienstes angeordnet. Konkret wurden Zugänge zum Internet unterbunden, wo diese nicht unbedingt nötig sind; zu den weiteren Massnahmen gehören Schritte, die das Herunterladen von Daten erschweren.

Doch das sind letztlich nur physische Schutzmassnahmen. In eine andere Schutz- und Sicherheitskategorie gehören die Abgrenzung des Staats und der Respekt vor der Privatsphäre. Hier ist die Antwort der Regierung bisher weniger eindrucksvoll ausgefallen. Sie hat zwar eine Personal Data Protection Commission (PDPC) ins Leben gerufen, die unter der Aufsicht des für Sicherheitsfragen zuständigen stellvertretenden Ministerpräsidenten Teo Chee Hean steht. Zu den entsprechenden Richtlinien gehört beispielsweise, dass jede Verwendung von CCTV-Kameras bewilligungspflichtig ist und dass Personen auf deren Einsatz aufmerksam gemacht werden müssen. Pikant ist indessen, dass alle staatlichen Institutionen hierzulande ausdrücklich von den PDPC-Prinzipien ausgenommen werden sollen. Der Staat – und dessen Organe – werden dadurch nicht eingeschränkt. Sie stehen gewissermassen über jedem Verdacht.
Vorgeschmack auf das 21. Jahrhundert

Künstliche Intelligenz solle für alle da sein, erklärte der für das Smart-Nation-Programm zuständige Minister Vivian Balakrishnan kürzlich im Parlament. «AI For Everyone» lautete denn auch das Motto einer Konferenz, die aus Anlass des Frauentags am 8. März hierzulande über die Bühne ging. Der Mensch müsse bei allen Anwendungen im Zentrum stehen, hiess es dazu. Als konkrete nützliche Beispiele nannte Balakrishnan etwa Sprachassistenten, GPS-Optimierungen und Aufdeckung von Finanzbetrug, etwa bei Kreditkarten, sowie Zusammenführung von Angebot und Nachfrage am Arbeitsmarkt.

Ein kritischerer Diskurs findet nur am Rand statt. Gedanken beispielsweise zu gesellschaftlichen Aspekten und weniger offensichtlichen Nutzungen (oder zur Nützlichkeit schlechthin) kommen in der Regel bloss als Gastbeitrag und Aussensicht daher. Sollen wir AI regulieren – können wir überhaupt? So fragte in der Zeitung «The Straits Times» kürzlich etwa Simon Chesterman, der Dekan an der Rechtsfakultät der National University of Singapore (NUS). Er verwies auf den ersten tödlichen Unfall mit einem selbstfahrenden Uber-Testfahrzeug am 18. März 2018 in Tempe (Arizona) als Folge eines nächtlichen Zusammenpralls mit einer Fussgängerin. Die passive «Fahrerin» vertraute dem System und schaute Video. Auch beim Absturz der beiden Boeing 737 Max vertrauten alle einem letztlich fehlerhaften System, das sich sogar dem Eingriff der Piloten widersetzen konnte. Wen trifft in diesen Fällen die Schuld?

AI wird vielerorts bereits als eine Technologie betrachtet, die das 21. Jahrhundert prägen wird. Dem wird sich Singapur nicht verschliessen, im Gegenteil. Aber ähnlich wie in China erwartet der Staat hier von seinen Bürgern viel: beispielsweise absolutes Vertrauen in den Regierungsapparat. Ferner blindes Vertrauen in die Techniken und Anwendungen der künstlichen Intelligenz. Das sei gefährlich, meint dazu Rebecca Parsons von Thought Works, eine andere kritische Gastautorin der «Straits Times». AI-Technologien trügen immer die Handschrift ihrer Erschaffer und seien damit auf ihre Weise voreingenommen und letztlich fehleranfällig. ";https://www.nzz.ch/wirtschaft/wer-zieht-die-grenzen-bei-big-data-ueberwachung-und-anderen-spaessen-ld.1472615;NZZ;Manfred Rist;;;
23.03.2018;Sind wir alle ferngesteuert von mysteriösen Firmen?;"Eineinhalb Jahre nach der US-Präsidentschaftswahl ist die Welt erneut in Aufruhr. Hat Cambridge Analytica (CA), die mysteriöse Marketingfirma, Donald Trump ins Weisse Haus befördert? Nach allem, was wir wissen, lautet die Antwort: höchstwahrscheinlich nicht. Aber was im allgemeinen Trubel fast vollkommen untergeht, ist die Frage nach der tatsächlichen Wirksamkeit der vermeintlichen Datenmagier. Viele sahen sich nach den Veröffentlichungen bestätigt: War dies nicht der finale Beweis? Hatte man nicht die ganze Zeit gewusst, dass die zwielichtige Firma hinter den grossen politischen Entwicklungen der vergangenen Jahre – die US-Wahl, der Brexit – stecken musste?

Der Schein trügt: Sieht man sich vorerst nur die Reaktion der amerikanischen Wahlkampfexperten und derjenigen, die in der Vergangenheit mit CA gearbeitet haben, an, gibt es schon genug Gründe, am Narrativ der Allmacht der Marketingfirma zu zweifeln. Seitens der Kampagne des Texaners Ted Cruz, für die CA tätig war, bevor man zu Team Trump stiess, beklagte man sich früh darüber, dass deren Wunderprodukt nicht richtig funktionierte und stellenweise gar nicht einsatzbereit war – kurz darauf setzte man die Firma klammheimlich ab. Selbst die Trump-Truppe blieb nicht bis zum Ende mit CA in einem Bett, da man auch hier bezweifelte, dass die revolutionären Modelle wirklich funktionierten. Viele politische Beobachter sind überzeugt, dass beide Kampagnen die Firma nur eingesetzt haben, weil es Bedingung für grosse Spenden vonseiten der Milliardärsfamilie Mercer war.

Auch die Strategen der Republikanischen Partei waren Berichten gemäss alles andere als beeindruckt, als CA zu Beginn der Präsidentschaftskampagne bei ihnen vorstellig wurde. «Sie haben nur mit schönen Worten um sich geschmissen und angegeben», heisst es zum Beispiel von Mike Murphy, Chef des einflussreichen republikanischen PAC (Political Action Committee) «Right to Rise». Laut Murphy und anderen Experten hätte CA von den Grundlagen des amerikanischen Wahlkampfs keinen Schimmer gehabt und nichts vorweisen können, was wirklich revolutionär gewesen wäre. Viele, die in der Vergangenheit auf konservativer Seite mit CA gearbeitet haben, sind heute verblüfft über die aufgestellten Behauptungen. Wie die «LA Times» berichtet, hatte eine konservative Organisation sogar eine Warnung vor CA (damals noch SCL Group) herausgegeben. Der Inhalt? «Achtung, Quacksalber!»

    Ein Unternehmen, das wirklich an seine Fähigkeiten glaubt, Wähler mit «Big Data Analytics» zu überzeugen, umzudrehen oder von der Wahlurne fernzuhalten, müsste sich nicht mit solch dubiosen Tricks auseinandersetzen.

Selbst die pikanten Details aus der Investigativreportage von Channel 4 lassen Zweifel am Erfolg der angeblich überlegenen Datenanalyse aufkommen. In den verdeckt aufgezeichneten Treffen präsentierte sich CA nämlich vor allem als Firma, welche die Gegenseite ausforscht und mit dubiosem Material zu diffamieren versucht. Ein Unternehmen, das wirklich an seine Fähigkeiten glaubt, Wähler mit «Big Data Analytics» zu überzeugen, umzudrehen oder von der Wahlurne fernzuhalten, müsste sich nicht mit solch dubiosen Tricks auseinandersetzen.

Die Perspektive der Wissenschaft

Statements von politischen Experten sind das eine, doch auch die Wissenschaft begegnet der vermeintlichen Wirksamkeit von CA mit Zweifel bis Ungläubigkeit. «Es gibt viele gute Gründe, skeptisch zu sein», erklärt zum Beispiel Daniel Kreiss. Der Amerikaner lehrt an der University of North Carolina at Chapel Hill und gilt als einer der führenden Experten für datengetriebenen Wahlkampf. «Es gibt so gut wie keine wissenschaftlichen Beweise dafür, dass psychometrisches Targeting in der Politik funktioniert, und jede Menge theoretischer Erklärungen dafür, dass es nicht funktionieren würde.»

Um diese Aussage zu verstehen, lohnt es sich, einen kurzen Blick auf die genaue Methode zu werfen. Laut CA sei man in der Lage gewesen, für Millionen von Amerikanern psychografische Profile zu bauen: eine Kombination aus demografischen Faktoren – wie Alter, Geschlecht und Wohnort – und Charaktereigenschaften – wie Aufgeschlossenheit, Gewissenhaftigkeit, Extraversion, Verträglichkeit und Neurotizismus –, den Merkmalen des klassischen «Big Five»-Modells aus der Persönlichkeitspsychologie. Diesen habe man dann politische Einstellungen zugewiesen. Doch während die Daten der angeblich rund 320 000 originalen Nutzer der Persönlichkeitstest-App des Cambridger Wissenschafters Alexander Kogan – über die CA an die Facebook-Daten gelangt war – noch halbwegs akkurat sein mögen, lässt sich dies für die restlichen 50 Millionen Profile schon nicht mehr behaupten. Hier wurden die Charaktereigenschaften nämlich nur anhand von Facebook-Likes modelliert, getreu dem Motto: «Persönlichkeitstest-App-Nutzer Johnny Walker hat Profil A, und ihm gefällt X und Y. Dementsprechend hat seine Facebook-Freundin Mary Mueller, der ebenfalls X und Y gefällt, wahrscheinlich auch Profil A.» Schon dieser erste Schritt basiert auf wackeligen Annahmen, die anschliessende Verknüpfung dieser Profile mit politischen Neigungen ist noch ungenauer.

Dementsprechend skeptisch ist auch Jessica Baldwin-Philippi, Professorin an der Fordham University, die sich in den letzten Jahren intensiv mit digitalem Wahlkampf auseinandergesetzt hat. «Man kann eine Tonne von Persönlichkeitsmerkmalen modellieren, und vielleicht sind sie im Allgemeinen korrekt», sagt sie, aber es sei vollkommen unklar, wie Facebook-Daten wie «Likes» mit Charaktermerkmalen verbunden seien und wie diese Variablen wiederum mit bestimmten Messaging-Strategien und politischer Einstellung in Verbindung gebracht werden können. «All diese Zwischenschritte multiplizieren die Herausforderung, belastbare Ergebnisse zu bekommen.»

    Ein Grossteil der (amerikanischen) Wählerschaft ist bereits in politische Lager eingeteilt. Und: Das Fernsehen ist immer noch wichtiger für viele Amerikaner als Facebook.

Kreiss und Baldwin-Philippi sind bei weitem nicht die Einzigen, die von den Behauptungen der britischen Firma nicht überzeugt sind. Rasmus Kleis Nielsen, Professor für Kommunikationswissenschaft und wissenschaftlicher Direktor des Reuters Institute for the Study of Journalism an der Universität Oxford, findet sogar noch deutlichere Worte: «Cambridge Analytica ist ein privates, gewinnorientiertes Unternehmen, das Beratungsdienstleistungen verkauft, und es ist absurd, ihre eigennützigen Behauptungen als Beweis ihrer Effizienz zu akzeptieren», erklärt er auf Nachfrage. Unabhängige Untersuchungen, so Kleis Nielsen, legten nahe, dass verschiedene Formen des Microtargeting für Kampagnen nützlich seien, doch man sollte die Wirkung nicht überschätzen: «Es ist keine Wunderwaffe und kein entscheidender Faktor für die Wahlergebnisse.»

Abgesehen von der wissenschaftlichen Fragwürdigkeit des psychografischen Targeting, ist es generell extrem schwierig, Menschen zu beeinflussen und ihre politische Meinung zu ändern – ein Umstand, den Forscher immer wieder betont haben. «Ein Grossteil der [amerikanischen] Wählerschaft ist bereits in politische Lager eingeteilt», erklärt auch Kreiss. Dies mache es schwierig, ihre Meinung zu beeinflussen – ganz egal, wie viel Aufwand man betreibe. Und: Wie Menschen ihre Stimme abgeben, hängt zum Beispiel auch entscheidend davon ab, wie sie die Lage der Wirtschaft sehen oder welche Ausbildung sie genossen haben. Politische Werbung hat in diesem Kontext, wie verschiedene Studien gezeigt haben, vergleichsweise wenig Einfluss.

Was wir im Medienhype übersehen

Letztlich sind sich fast alle Experten einig, hat Donald Trump die Wahl nicht wegen CA gewonnen, sondern dank einer ganzen Reihe von anderen Faktoren, die seine Kampagne begünstigt haben. Betrachten wir nur den Einfluss von sozialen Netzwerken, muss hier zuallererst die Twitternutzung des Reality-TV-Stars genannt werden. Twitter, nicht Facebook, sicherte Trump die Aufmerksamkeit der «Gatekeeper» in den Mainstream-Medien und damit grossen Einfluss auf die Nachrichten-Agenda – ein entscheidendes Element im US-Wahlkampf.

«Vom 16. Juni 2015, als Trump seine Kandidatur ankündigte, bis zum 20. Juli 2016, dem Tag, an dem er von den Republikanern nominiert wurde, setzte Trump eine ganze Reihe umstrittener Tweets ab, die ihm weit mehr Aufmerksamkeit im Fernsehen und in den Zeitungen verschafften als seinen Konkurrenten – oft mehr als allen anderen zusammen», sagt auch Ralph Schroeder, Professor am Internet Institute der Universität Oxford und Experte für die Mediennutzung von Populisten. «Die Mainstream-Medien – gierig nach Neuigkeiten und in scharfer Konkurrenz um Zuschauer – stürzten sich förmlich auf seine Tweets.» Auf diese Weise, so Schroeder, setzte Trump die Agenda: Er zog national die meiste Aufmerksamkeit auf sich und übertönte alle anderen Kandidaten.

Studien zur Mediennutzung in den USA unterstützen Schroeders These. Das Fernsehen ist immer noch wichtiger für viele Amerikaner als Facebook. 58 Prozent aller Wähler nannten in einer Studie des Pew Research Center Fernsehsender als ihre Hauptnachrichtenquelle während der US-Wahl 2016. Gerade einmal 8 Prozent gaben Facebook an, obwohl hier zusätzlich erwähnt werden muss, dass viele Nachrichten auf der Plattform ebenfalls von traditionellen Medien stammen. Vielleicht sollte man einen Teil der Verantwortung für Trumps Wahlerfolg weniger bei Alexander Nix und Mark Zuckerberg suchen als bei Rupert Murdoch oder Jeff Zucker, dem CEO von CNN – beide hatten Trump grosszügig Sendezeit eingeräumt und stark davon profitiert. Und es gibt noch mehr Faktoren, die der Medienhype rund um CA ignoriert. Die Einkommensungleichheit ist in den USA zuletzt immer weiter gewachsen, und Trump präsentierte sich wie kein anderer als «Freund» des «kleinen Mannes» – für viele ein Grund, den Populisten ins Weisse Haus zu wählen. Und wenn die US-Wahl eines gezeigt hat, dann, dass die USA immer noch zutiefst von Rassismus geprägt sind. Für Baldwin-Philippi ist das ein entscheidender Punkt: «Die Idee, dass Cambridge Analytica Leute dazu gebracht hat, für einen Kandidaten zu stimmen, der unglaublich rassistische und sexistische Dinge gesagt und getan hat, erlaubt es uns zu vermeiden, sich mit der Tatsache auseinanderzusetzen, dass viele Menschen bereitwillig für diesen Kandidaten gestimmt haben und dass diese Ansichten in den USA systemisch sind.»

    Keine Regulierung wird das grundlegendere Problem lösen, dass Populisten soziale Netzwerke nutzen können, um ihre Ideen zu verbreiten, indem sie die traditionellen Medien umgehen.

Abschliessende Sicherheit, was genau Trump am Ende entgegen allen Erwartungen an die Macht gebracht hat und wie wichtig CA bei alledem war, wird es nie geben. «Ohne Daten von Facebook oder Cambridge Analytica über die Ergebnisse dieses Targeting ist es unmöglich, sicher zu wissen, ob die strategische Kommunikation, an der CA beteiligt war, effektiv war», meint Daniel Kreiss. Eine Einschätzung, die Ralph Schroeder teilt: «Es ist wahrscheinlich, dass wir nie genau wissen werden, welche Auswirkungen die Bemühungen von Cambridge Analytica auf die amerikanischen Wahlen im Jahr 2016 hatten. Es ist jedoch klar, dass die letzten Ereignisse zur Regulierung der Nutzung von Social-Media-Daten für politische Kampagnen führen werden.» Doch ein Problem wird laut Schroeder bestehen bleiben. «Keine Regulierung wird das grundlegendere Problem lösen, dass Populisten soziale Netzwerke nutzen können, um ihre Ideen zu verbreiten, indem sie die traditionellen Medien umgehen.»";https://www.nzz.ch/feuilleton/cambridge-analytica-facebook-die-big-data-panik-ld.1368507;NZZ;Felix Simon;;;
01.04.2019;Nationales Register: Big Data erfasst die Schwarzfahrer;"Wer in einem öffentlichen Verkehrsmittel ohne gültigen Fahrausweis oder nur mit einem teilweise gültigen Fahrausweis angetroffen wird, landet künftig in einer zentralen Datenbank. Gestern Montag hat das nationale Schwarzfahrer-Register seinen Betrieb aufgenommen, wie die Branchenorganisation «ch-direkt» mitteilte. Damit sollen Schwarzfahrer landesweit leichter und schneller identifiziert und besonders notorische Schwarzfahrer erfasst werden. Von den jährlich rund zwei Milliarden Fahrten im öffentlichen Verkehr würden geschätzte drei Prozent ohne oder nur mit teilgültigem Ticket angetreten. Von diesen Schwarz- oder Graufahrern erwischten die Kontrolleure heute jährlich etwa 800 000. Den Betrieben entgehe durch Reisende, die kein Billett kauften, ein zweistelliger Millionenbetrag. Basierend auf einem Parlamentsbeschluss von 2015 führen die Transportunternehmen deshalb das zentrale Register ein, das bis Ende Jahr flächendeckend alle rund 100 beteiligten Bahngesellschaften, S-Bahnen, Bus- und Trambetriebe umfassen soll. Aufgebaut und betrieben wird die Datenbank laut Mitteilung von der Postauto AG im Auftrag der Branche.
Keine Direktabfrage möglich

Das nationale Schwarzfahrer-Register bedeute keine grundsätzliche Praxisänderung, betonte «ch-direkt»-Sprecher Thomas Ammann auf Anfrage. Es sei eine Lösung, die im Hintergrund arbeite. Wenn das Kontrollpersonal vor Ort jemanden ohne gültiges Ticket antreffe, würden wie bisher die Personalien aufgenommen und ans Backoffice des Transportunternehmens geschickt. Dort tippe jemand die Daten ein, sende sie ans Register und bekomme eine Antwort, ob die Person bereits einmal erwischt worden sei oder nicht. Danach komme wie bisher die Rechnung. Die Kontrolleure könnten die Datenbank nicht gleich vor Ort abfragen, und es sei auch nicht möglich, dass sie einfach so das Register konsultierten. Auch die Gebühren blieben gleich. Allerdings fliegen laut Ammann mit der zentralen Registrierung Wiederholungstäter künftig automatisch auf, so dass sich die Gebühren im Wiederholungsfall zwingend erhöhten. Bisher sei es möglich gewesen, dass jemand beispielsweise an einem Tag bei den SBB und zwei Tage später bei der BLS schwarzgefahren und deshalb nicht als Wiederholungstäter erkannt worden sei. Mit der Registrierung könnten nun notorische Schwarzfahrer einfacher ermittelt und schweizweit einheitlich mit den entsprechenden Zuschlägen belegt werden. «Dies sollte eine abschreckende Wirkung haben», sagte Ammann.
Zwei Jahre auf Bewährung

Es werde keine Unterscheidung gemacht, ob jemand vorsätzlich oder fahrlässig ohne Billett unterwegs sei. Auch ein vergessener Nachtzuschlag oder Klassenwechsel ergebe einen Eintrag. Es sei jedoch wie bisher möglich, dass die Betriebe gewisse «Härtefälle» nicht erfassten, wenn jemand zum Beispiel kein Ticket lösen konnte, weil der Automat defekt war. Betroffene könnten jederzeit einen Antrag auf Einblick in ihre Daten und Vorfälle stellen. Des Weiteren bieten laut Mitteilung einige Unternehmen bereits an, vergessene persönliche Abonnemente online vorzuweisen. Die Einträge bleiben zwei Jahre im System gespeichert. Wenn es zu keinem weiteren Vorfall kommt, werden sie gelöscht. Wenn jemand innerhalb der zwei Jahre erneut erwischt wird, beginnt die Frist jedoch von neuem zu laufen.";https://www.nzz.ch/schweiz/big-data-erfasst-die-schwarzfahrer-ld.1471899;NZZ;Helmut Stalder;;;
13.03.2020;Wo die liberalen Demokratien schwächeln, braucht es nicht neue Algorithmen, sondern mutige Politiker;"Als mathematik- und physikbegeisterter Mittelschüler faszinierte mich seinerzeit die Idee des Laplaceschen Dämons. Dieser Dämon, im Jahre 1814 von Pierre-Simon Laplace erfunden, kennt Ort, Position und Bewegungszustand jedes Teilchens im Kosmos und kann damit mittels der physikalischen Gesetze die Vergangenheit und die Zukunft dieses Kosmos genau berechnen.

Den Gedanken, dass nicht Chaos diese Welt beherrscht, sondern eine durch Naturgesetze definierte mathematisch abbildbare Ordnung, empfand ich als ungemein befriedigend. Damit wurde auch der im realen Leben so lästige Störenfried Zufall erklärbar. Er erwies sich keineswegs als Laune der Schöpfung, sondern bloss als ein Ereignis, dessen komplexe Kausalität noch nicht erschlossen ist.

Die modernen Digitalisierungsfreaks, so hat man den Eindruck, wittern dank Big Data Morgenluft und träumen, ganz im Sinne des genialen Dämons, von der Steuer- und Berechenbarkeit dieser Welt durch gigantische Datenspeicher und superschnelle Quantencomputer. Diesen Eindruck bekommt man jedenfalls, wenn man den – im Übrigen sehr gescheiten – Artikel von Miriam Meckel in der NZZ über digitale Demokratie liest.
Die Demokratie – ein Trauerspiel?

Meckel stellt zunächst richtigerweise fest, dass die Bildung von Regierungen in vielen modernen Demokratien zu Trauerspielen degeneriere. Das parteipolitische System spiegle den Willen der Bürger längst nicht mehr und korrumpiere den Grundgedanken der Demokratie, wonach das Volk die Staatsgewalt ausübe.

Eine algorithmische Wahl, gestützt auf die Rechen- und Prognosekapazität künstlich intelligenter Systeme, könne genauer beschreiben, was die Bürger wollten. Sozusagen als Analogon zur Momentaufnahme des Weltquerschnitts des Laplaceschen Dämons als Ausgangspunkt seiner Berechnungen müsste man dem Staatswesen eine Stunde null der Datensammlung gönnen, um ab dann mittels eines neuen Datenrepositoriums in die Zukunft extrapolieren zu können. Keine Trumps, Orbans, Kaczynskis und Salvinis mehr, dafür die Wohltat von mit Twitter- und Google-Daten gefütterten Quantencomputern?

Nun, schon in den zwanziger Jahren des letzten Jahrhunderts machte Heisenberg mit seiner Unschärferelation dem Laplaceschen Dämon den Garaus, indem er dem Zufall wieder eine Chance gab und damit die Laplacesche Voraussetzung der strengen Kausalität von allem und jedem zertrümmerte. Und Stephen Hawking zeigte später auf, dass es zwar durchaus einen gewissen Determinismus gebe, weil von einem bestimmten Weltmoment aus nicht mehr alle Zukünfte möglich seien. Es verblieben aber derart viele Zukunftsmöglichkeiten, dass von einer deterministisch vorbestimmten besonderen Zukunft trotzdem nicht die Rede sein könne.

Wo aber die Zukunft weder durch physikalische Gesetze noch durch bestehende Datenquerschnitte unveränderlich vorbestimmt ist, bleibt einerseits der Zufall einflussreicher Akteur, andererseits öffnen sich Handlungsspielräume zur Gestaltung der Zukunft. Deshalb ist es sehr unwahrscheinlich, dass ein noch so leistungsfähiger Computer auf der Basis des Datenbestandes der Stunde null alles zutreffend extrapolieren kann.

Der Glaube an solche Extrapolation könnte sogar den Blick auf die bestehenden Handlungsspielräume derart einengen, dass dadurch die Innovationsfähigkeit der Demokratie beeinträchtigt würde. Ich glaube beispielsweise nicht, dass der besagte Quantencomputer etwa ausgehend vom globalen Datenquerschnitt des 1. Januars 2020 den Flug der Fledermaus hätte einberechnen können, die in einem chinesischen Wildtiermarkt die Rekombination von dort bereitstehenden Coronaviren mit ihren Coronaviren zum humanpathogenen Coronavirus Sars-CoV-2 einleitete und damit über Nacht die Welt veränderte.
Die Gefahr: eine Gefälligkeitsdemokratie

Aus demokratiepolitischer Sicht stellen sich allerdings drei viel einfachere Fragen. Gibt es erstens überhaupt einen Volkswillen? Und wenn es denn einen gäbe, wäre er zweitens eine hinreichende Basis zur Gestaltung guter Politik? Sollten schliesslich drittens Parteien und gegebenenfalls der politikgestaltende Quantencomputer einen solchen Volkswillen überhaupt zur Richtschnur ihrer politischen Programme machen?

Die modernen Gesellschaften in den liberalen Demokratien bestehen aus Menschen aus unterschiedlichen Kulturen, mit unterschiedlichen Biografien, unterschiedlichen Präferenzen, unterschiedlichen Überzeugungen, unterschiedlichem Bildungsstand und unterschiedlichen Erfahrungen. Es mag zwar mehr oder weniger grosse Schnittmengen an gemeinsamen Überzeugungen geben, in der Schweiz beispielsweise diejenige, dass die direkte Demokratie anderen Staatsformen überlegen sei. Doch die individuellen Profile der Menschen liegen dessen ungeachtet weit auseinander.

Die wachsende Zersplitterung der Parteienlandschaft und die Erosionserscheinungen der grossen Volksparteien sind Symptome dafür, dass keine Partei mehr die Präferenzen grosser Teile der Bevölkerung abzubilden vermag. Dass es sinnvoll ist, ob dieser Zersplitterung überhaupt so etwas wie einen Volkswillen herauszufiltern, ist zu bezweifeln. So wird auch der leistungsfähigste Quantencomputer keinen solchen Willen ermitteln können, aus dem sich brauchbare politische Handlungsmuster ergäben. Im Gegenteil: Die Summierung mehrheitsfähiger Präferenzen zu einem Volkswillen könnte zu einer Gefälligkeitsdemokratie führen, die letztlich den Staat zugrunde richtet.

Genauer: Es entstünde daraus die Big-Data-gestützte Politik des nassen Fingers, wie man sie bisweilen der umfragegestützten Politik der deutschen Kanzlerin vorwirft. Wohl war diese Politik lange erfolgreich, möglicherweise aber unter Opferung eines rechten Stücks Zukunftsfähigkeit des Landes.

Miriam Meckel denkt beispielsweise an einen Haushaltsentwurf, der die Bedürfnisse des Volkes und nicht die parteipolitischen Interessen in den Vordergrund rücke, indem er auf durch KI-Systeme erstellte datenbasierte Analysen aufbaue. Die Summe der in einem bestimmten Zeitraum gemessenen kumulierten Bedürfnisse des Volkes wird die verfügbaren Mittel allerdings nicht weniger übersteigen als die Bedürfnisse der heutigen Politiker. Ob dann der Quantencomputer der Politik auch die Aufgabe der notwendigen Kürzungen und Schwerpunktsetzungen im Einklang mit den langfristigen wirtschafts-, sozial- und umweltpolitischen Zielen eines Landes wird abnehmen können, ist doch mehr als zweifelhaft.
Die Chance: Mut zur Überzeugung

Das bringt mich zur dritten und wichtigsten Frage: Ist die Demokratie wirklich nur dazu da, einen irgendwie herausdestillierten volatilen Volkswillen umzusetzen, mit dem nassen Finger im Winde des Zeitgeistes eben, oder haben die vom Volk mit Regierungsverantwortung Betrauten nicht vielmehr die Pflicht, das für das Land langfristig Richtige und nicht das Gefällige anzustreben? Vielleicht sogar unter Inkaufnahme der Abwahl wie seinerzeit Bundeskanzler Gerhard Schröder mit der Agenda 2010?

Das kann natürlich nicht bedeuten, permanent am Volk vorbeizupolitisieren. Man muss schliesslich auch gewählt werden, um politische Ideen überhaupt umsetzen zu können. Aber Politik muss eben auch heissen, das Volk von dem, was man als richtig erkannt hat, zu überzeugen, vielleicht sogar klar gegen den gerade obwaltenden «Volkswillen». Nur so, im Widerstreit von Meinungen und Konzepten und mit Mut auch zum Unpopulären, wird Fortschritt möglich.

Der Nobelpreisträger Douglass North kam nicht von ungefähr zu dem Schluss, dass Demokratien mit Meinungskonkurrenz für neu auftauchende Probleme rascher Lösungen finden als Autokratien. Kein Computer wird in einer Welt, in der Zufälle eine grössere Rolle spielen, als uns bewusst ist, die menschengemachte Politik mit Meinungswettbewerb ersetzen können, so mühsam das auch immer wieder sein mag. (Das heisst natürlich im Umkehrschluss nicht, dass datenbasierte KI nicht auch zur Verbesserung politischer Entscheide genutzt werden sollte.)

Regierungsbildungen sind vor allem in Demokratien mit Verhältniswahlrecht mühsam, weil Mehrheiten in vielfältigen Gesellschaften nur noch mit Koalitionen möglich sind, die wiederum Kompromisse durch Aushandeln suchen müssen. Friedrich August von Hayek sprach deshalb auch von Schacherdemokratien. Das kann die Problemlösungsfähigkeit zwar behindern, regelt aber die Konflikte zwischen den gesellschaftlichen Gruppen besser als Majorzsysteme. Aber gerade solche Konflikte können ihrerseits die Problemlösungsfähigkeit wieder beeinträchtigen und neue Probleme schaffen.

Bleiben die Autokratien! Die entscheiden zwar rascher, aber trotzdem bilden sich selten Kolonnen von um Einlass bittenden Flüchtlingen vor ihren Toren. Doch vielleicht stellen sich diese Fragen gar nicht mehr, wenn die künstliche Intelligenz in ein, zwei Jahrzehnten die technologische Singularität erreicht, also den Punkt, wo die künstliche die menschliche Intelligenz überholt. Sie wird dann wohl sehr bald zu dem Schluss kommen, dass die Menschen mit ihren kognitiven Verzerrungen eigentlich nur stören.

Vielleicht ist das die Gnade des Alters: dass man das nicht unbedingt auch noch erleben muss!";https://www.nzz.ch/feuilleton/kaspar-villiger-es-braucht-in-der-politik-den-faktor-mensch-ld.1545621;NZZ;Kaspar Villiger;;;
15.06.2020;Chinas Wirtschaft macht Hoffnung – die Situation in Peking bereitet jedoch Sorgen;"Zumindest laut den offiziellen Zahlen des National Bureau of Statistics of China erholt sich das asiatische Land schneller als erwartet von der Covid-19-Krise. Besonders die industrielle Produktion mit einem Plus von 4,4% im Mai gegenüber dem Vorjahresmonat zieht wieder an. Der Detailhandel ist zwar noch im Minus. Aber auch seine Lage bessert sich schrittweise, nachdem er vor allem im Januar und Februar zur Hochzeit der Covid-19-Pandemie noch zweistellig eingebrochen war.

Die Ökonomen des Beratungsunternehmens Capital Economics rechnen auf Basis der offiziellen Statistiken damit, dass im Mai 2020 die Wirtschaftsleistung erstmals über dem Niveau des Vorjahres gelegen haben dürfte; mit dieser Trendwende hatten sie erst im dritten Quartal 2020 gerechnet. Abzuwarten bleibt jedoch, wie sich der neuerliche Ausbruch des Virus in Peking auf die wirtschaftlichen Aktivitäten auswirken wird. Innert weniger Tage hat es 79 neue Infektionen gegeben. Zumindest für die Bewohner der chinesischen Hauptstadt werden die kommenden Wochen wegen all der Einschränkungen wieder zu einer Geduldsprobe.
5G und Big Data im Blick

Treiber hinter dem Aufschwung sind die Ausgaben für Infrastruktur, die im Mai um annähernd 12% zugelegt haben. Damit schlagen sich die an die Provinzregierungen verteilten Quoten zur Ausgabe spezieller Anleihen in den Zahlen nieder, mit denen nun landesweit Infrastrukturprojekte finanziert werden. Unter dem Schlagwort «neue Infrastruktur» soll auch der Ausbau des neuen Mobilfunkstandards 5G sowie von Zentren für Big Data vorangetrieben werden.

Capital Economics prognostiziert, dass in solche Vorhaben im laufenden Jahr rund 650 Mrd. Yuan (Y) fliessen werden, was rund 87 Mrd. Fr. entspricht. Die Zahl zeigt jedoch auch, dass der Grossteil der 2020 ausgegebenen speziellen Anleihen von 3750 Mrd. Y noch immer für traditionelle Infrastrukturvorhaben verwendet wird. Kurzfristig ist damit zwar eine Belebung der Konjunktur verbunden, die sich vor allem im zweiten Halbjahr auf die Statistiken auswirken wird.

Da China jedoch bereits über eine für das herrschende Wohlstandsniveau intakte Infrastruktur verfügt, werden die weiteren Investitionen – im Gegensatz zu jenen in die neue Infrastruktur – nur in geringem Umfang produktivitätssteigernd sein. Mit anderen Worten verspielt das asiatische Land damit langfristige Wachstumsperspektiven, weil die vorhandenen Ressourcen falsch eingesetzt werden.
Industrielle Produktion erholt sich weiter Im Schlepptau der Massnahmen zieht auch die industrielle Produktion an, die nach der Covid-19-Krise einen V-förmigen Verlauf aufweist. Auf den schlagartigen Einbruch erfolgt die zügige Erholung. Besonders freuen werden sich die Produzenten von Stahl und Zement, deren Zulieferungen bei den Infrastrukturprojekten gefragt sind.

Mit den Massnahmen schielt Peking auch auf den Arbeitsmarkt. Laut offiziellen Zahlen ist die Arbeitslosenquote in den Städten im Mai gegenüber dem Vorjahr leicht um 0,1 Punkte auf 5,9% gesunken. Allerdings erfasst diese Statistik die Millionen Wanderarbeiter nicht, die rund ein Drittel der städtischen Beschäftigten stellen und die von der Krise heftig getroffen worden sind.

Erschwerend kommt hinzu, dass zwar viele eine neue Stelle in der Nähe ihres Heimatortes gefunden haben. Allerdings verdienen sie dort rund 20% weniger als bei ihren bisherigen Anstellungen in den Städten, was sich wiederum negativ auf den Konsum auswirkt. Die Infrastrukturprojekte werden deshalb auch forciert, um die Lage der Wanderarbeiter auf dem Arbeitsmarkt zu verbessern.
Gastgewerbe leidet noch immer

Mut machen auch die offiziellen Statistiken über den Konsum. So sind die Umsätze des Detailhandels zwar auch im Mai mit annähernd 3% gegenüber dem Vorjahresmonat zurückgegangen; allerdings war das Minus im April noch 5 Prozentpunkte höher gewesen. Ein breiter gefasster Index zur Erfassung von Dienstleistungen ist im Mai gar erstmals in diesem Jahr leicht gestiegen. Die Zahlen zeigen, dass die Erholung des personalintensiven tertiären Sektors langsamer verläuft als jene der industriellen Produktion. Dafür gibt es drei Gründe: Erstens sind die Chinesen wegen der unsicheren Wirtschaftslage verunsichert, was sich negativ auf ihr Konsumverhalten auswirkt; zweitens sitzt wegen der schwierigen Lage am Arbeitsmarkt und der zurückgehenden verfügbaren Einkommen das Geld nicht mehr so locker wie anhin; und drittens kehren sie aus Angst vor Infektionen nur langsam zu ihren bisherigen Verhaltensmustern zurück.

Die offiziellen Statistiken zeigen jedoch auch, wie unterschiedlich stark der tertiäre Sektor von Covid-19 betroffen ist. So gingen im Mai die Umsätze des Gastgewerbes gegenüber dem Vorjahresmonat abermals um annähernd 20% zurück. Im Gegenzug entdecken die Chinesen wieder den heimischen Herd. Laut den offiziellen Zahlen sind die mit Getränken, Getreide sowie Ölen und Essen erzielten Umsätze deutlich im zweistelligen Bereich gestiegen.

Besonders gefragt sind die Dienste der E-Commerce-Anbieter. Die Verkäufe im Internet legten in den ersten fünf Monaten des laufenden Jahres um annähernd 5% zu. Im World Wide Web werden inzwischen rund ein Viertel aller Konsumgüter erworben.

Und auch vom Automobilmarkt kommen positive Meldungen; seine Umsätze erhöhten sich im Mai um mehr als 3% im Vergleich mit dem Vorjahresmonat. Jüngst hatte bereits Volkswagen gemeldet, dass es im Mai 6% mehr Personenwagen ausgeliefert habe als im Vorjahr. Die insgesamt 33 Produktionsstandorte in China hätten annähernd wieder die normalen Kapazitäten erreicht, war einer Medienmitteilung des deutschen Konzerns zu entnehmen. ";https://www.nzz.ch/wirtschaft/coronavirus-chinas-wirtschaft-macht-hoffnung-sorge-in-peking-ld.1561268;NZZ;Matthias Müller;;;
08.06.2020;Der gesetzliche Weg für die Contact-Tracing-App ist frei – doch noch ist sie keine Erfolgsgeschichte;"Für die Schweizer Contact-Tracing-App Swiss Covid beginnt in den kommenden Wochen die kritische Phase. Die Anwendung soll helfen, Infektionsketten nachzuverfolgen, und soll damit die analoge Kontakt-Verfolgung ergänzen. Die Politik hat nun wichtige gesetzliche Grundlagen verabschiedet. Doch reichen diese aus, um das Vertrauen der Bevölkerung zu gewinnen? Die Vorzeichen stehen gut.

Am Montag hat es die App auch durch die zweite Kammer geschafft. Nach dem Ständerat vergangene Woche gab auch der Nationalrat grünes Licht für eine entsprechende Änderung des Epidemiegesetzes, so dass die App von der gesamten Schweizer Bevölkerung genutzt werden kann. Bis jetzt läuft die App nur in einer Pilotphase, die maximal bis Ende Juni dauern wird. Nach der Schlussabstimmung kommende Woche wird es am Bundesrat sein, die Verordnung rasch zu verabschieden.
Ein App-Zwang wäre unverhältnismässig

Bei der Contact-Tracing-App handelt es sich um ein politisch und datenschutzrechtlich sehr sensibles Big-Data-Vorhaben. Unser Smartphone weiss meist mehr über uns als unsere engsten Vertrauten. Wenn nun der Staat eine App herausgibt, die Daten aus unseren Handys nutzt, ist das ein heikles Unterfangen. Es ist daher richtig und wichtig, dass nun im Gesetz gewisse Grundsätze festgehalten werden. Nur so gewinnt die App die breite Akzeptanz und das Vertrauen der Bevölkerung. Dazu gehört, dass sie auf Freiwilligkeit basiert, einen starken Datenschutz bietet und die gesetzliche Grundlage nach zwei Jahren abläuft.

Eine Pflicht zur Nutzung der App wäre ein unverhältnismässiger Eingriff in die Selbstbestimmung der Bürger. Ein Blick Richtung China zeigt, wie schnell sich eine staatliche Überwachung mit technologischen Mitteln entwickeln kann. Mit einem App-Zwang kommen unweigerlich weitere Pflichten und Kontrollen daher. Jeder müsste ein Smartphone besitzen, auf dem die App installiert ist, und dieses bei sich tragen. Die Geräte und ihr Bluetooth müssten eingeschaltet sein. Neu Infizierte müssten ihre Erkrankung der App mitteilen und diejenigen, die mit ihnen in Kontakt standen, sich melden. Dies müsste alles jederzeit vom Staat kontrolliert werden dürfen. Ein Albtraum-Szenario für jede Demokratie – selbst in Ausnahmesituationen.

Es gibt darüber hinaus technische Unsicherheiten, die eine App-Pflicht nicht rechtfertigen würden. Die Anwendung ist nicht perfekt. Es kann trotz vielen Tests und Optimierungen der Bluetooth-Technologie nicht ausgeschlossen werden, dass sogenannte «false positives» entstehen – also Falschmeldungen über eine mögliche Infektion. Genauso kann es zu «false negatives» kommen, also nicht erkannten Infektionen. Auch deshalb ist es richtig, dass niemand zur App-Nutzung gezwungen werden kann – weder vom Staat noch vom Arbeitgeber, noch von einem Veranstalter.
Gute Voraussetzung

Die Schweizer App ist mit ihrem Ansatz international ein Vorreiter für die Entwicklung von datensparsamen Contact-Tracing-Anwendungen. Sie erhält Lob von Datenschützern, und ihr Konzept hat sogar die amerikanischen Tech-Konzerne Apple und Google inspiriert. Zudem hat die App ein fest definiertes Ablaufdatum im Sommer 2022. Der Staat hat also gute Argumente, warum er auf solch persönlichen Geräten wie den Smartphones mit einer App präsent sein möchte.

Ob die Bevölkerung von der App überzeugt ist, wird sich in den kommenden Wochen zeigen. Die gesetzliche Grundlage ist jedenfalls eine gute erste Voraussetzung. Doch die Politik allein kann nicht über den Erfolg der App entscheiden. Wenn die Kinderkrankheiten und Fehler, die in der Testphase jetzt zutage treten, nicht richtig behoben werden, kann die Stimmung leicht kippen – und die App trotz überzeugender Theorie in der Praxis scheitern.";https://www.nzz.ch/meinung/der-gesetzliche-weg-fuer-die-contact-tracing-app-ist-bereitet-doch-noch-ist-sie-keine-erfolgsgeschichte-ld.1560196;NZZ;Jenni Thier;;;
17.01.2018;Die UBS setzt bei Big Data auf das Tessin;"Die Grossbank UBS verstärkt ihre Präsenz im Tessin. In Manno, einem Vorort von Lugano, wird sie in den kommenden Monaten ihr drittes Business Solution Center in der Schweiz aufbauen. Ein solcher Hub besteht seit Herbst 2017 bereits in Schaffhausen, ein weiterer wird im kommenden Jahr in Biel eröffnet werden. Ein grosser Teil der in diesen Städten angesiedelten Arbeitsplätze wird aus Zürich dorthin verschoben.
Renommiertes IT-Institut

Der neue Standort in der Südschweiz wird dagegen etwas anders ausgerichtet sein als die beiden Zentren in der Deutschschweiz. In diesen arbeiten die Angestellten in Bereichen, welche die Banken üblicherweise Back und Middle Office nennen. Es handelt sich also beispielsweise um IT-Jobs und Aktivitäten rund um die Kontoeröffnung, die Erstellung von Steuerauszügen oder die Wertschriftenabwicklung. Im Vergleich damit ähnelt der Hub in Lugano eher einem Entwicklungszentrum. Tätig sind die Angestellten in den Bereichen künstliche Intelligenz, Analytik und Big Data. Finanzinstitute stehen unter einem grossen Druck, ihre Prozesse effizienter zu gestalten und vermehrt zu digitalisieren. Zumindest teilweise soll der Input dafür aus Lugano kommen. In einem ersten Schritt wird die UBS 20 bis 30 neue Stellen in Lugano ansiedeln; insgesamt sollen 80 Arbeitsplätze entstehen. Tätig sein werden die Mitarbeiter in einem Gebäude, das die UBS bereits für anderweitige Aktivitäten nutzt.

Die Spezialisten will die UBS vor allem vor Ort rekrutieren. Warum aber weicht sie dafür nach Lugano aus, während beispielsweise das Internetunternehmen Google seinen Standort in Zürich forciert und dabei stark auf Absolventen der ETH setzt?

Es dürfte gerade der Boom des IT-Sektors gewesen sein, der die UBS-Führung unter anderem dazu bewogen hat, sich nach einer Alternative zu Zürich umzusehen. In der Limmatstadt wetteifern unzählige Unternehmen um gute Software-Ingenieure. In Lugano ist hingegen nicht nur die Nachfrage nach solchen Spezialisten niedriger als in Zürich, sondern es gibt auch eine Ausbildungsstätte für IT-Spezialisten, die in der Deutschschweiz kaum bekannt ist. Es handelt sich um das Dalle Molle Institute for Research in Artificial Intelligence (IDSIA). Es ist 1988 vom italienischen Industriellen Angelo dalle Molle gegründet worden und heute Teil der Universität von Lugano. Offenbar erhofft sich die UBS einen gewissen Wissenstransfer vom IDSIA, das in Fachkreisen einen sehr guten Ruf geniesst; gleichzeitig setzt die Bank darauf, dass Absolventen des Instituts für sie tätig werden.
Alternative zu Polen

Mit dem Hub in Lugano hat die UBS ihr Projekt des «Nearshoring» abgeschlossen. Es war gleichsam eine Ergänzung und Alternative zu den Shared Services Centers, welche die Bank wie andere Grossfirmen im Ausland installiert hat, etwa in Polen, Indien und China. In Polen beispielsweise beschäftigt die Grossbank in Krakau und Wroclaw über 3500 Angestellte. Im mittelosteuropäischen Land wird das Institut den Personalbestand jedoch nicht mehr ausbauen. Diesen Entscheid hat die Bankleitung unter anderem aus Risikoüberlegungen getroffen. Kein Land soll im Konzernverbund ein zu grosses Gewicht bekommen, mittlerweile arbeiten in Polen aber immerhin 6% der UBS-Mitarbeiter.
Buhlen um UBS-Ableger

Nachdem die UBS im Herbst bekanntgegeben hatte, dass man rund 450 Arbeitsplätze von Zürich nach Schaffhausen verlagern werde und die Gründung von zwei weiteren Business Solution Centers erwäge, nährte das in vielen mittelgrossen Schweizer Städten die Hoffnung auf eine Ansiedlung. Offenbar hat die UBS bei der Auswahl nun darauf geachtet, dass die drei Sprachregionen der Schweiz berücksichtigt werden.";https://www.nzz.ch/wirtschaft/die-ubs-setzt-bei-big-data-auf-das-tessin-ld.1348240;NZZ;Daniel Imwinkelried;;;
24.07.2020;David Weber soll aus Zürich eine Smart City machen – doch was heisst das überhaupt?;"Zürich erwartet viel von David Weber: Sein Team soll die Stadt – je nachdem, welchen Politiker man fragt – klimafreundlicher, solidarischer und zugänglicher, flexibler und weniger bürokratisch, vor allem aber «smarter» machen. Das wären schon für eine Stadtpräsidentin hohe Ziele, doch der 39-Jährige führt das neue, sechsköpfige Smart-City-Team der Verwaltung.

2018 stellte der Zürcher Stadtrat seine Smart-City-Strategie vor: Mit digitalen Technologien sollte die Stadt besser und einfacher mit ihren Bürgern in Kontakt treten, aber auch die städtischen Angestellten untereinander besser vernetzen. Für Zürich war das kein komplettes Neuland, man bot den Bürgern damals bereits «Mein Konto» an, einen zentralen digitalen Zugang zu immer mehr städtischen Services.

Dem Stadtparlament, das die Smart-City-Strategie einst gefordert hatte, gefiel der Plan. Es stockte den geforderten Nachtragskredit im Mai 2019 sogar noch um 150 000 Franken auf 1,55 Millionen auf: Knapp vier Franken pro Kopf, aber für städtische Verhältnisse immer noch wenig Geld. Die Stadtentwicklung investierte 600 000 Franken in Innovationskredite (siehe unten), vor allem aber ins Team um David Weber, der am 1. Januar 2020 seinen Job als Leiter Smart City anfing.

Der grossgewachsene Enddreissiger hat nicht den Habitus eines städtischen Kadermitarbeiters. David Weber erinnert eher an den Studienkollegen, den man beim GZ Wipkingen auf ein Bier treffen würde: leicht graumelierte Haare, schlichtes dunkles T-Shirt, passende Sneakers, entspannter Auftritt. Damit fügt er sich bestens ins Smart City Lab ein, das in den früheren SBB-Werkstätten an der Hohlstrasse in Altstetten untergebracht ist. Das weitläufige Industrieareal wirkt am Morgen, bei geschlossenem Gittertor, noch nicht sehr einladend. Es versprüht aber, wenn man das so sagen kann, den Geist des Unvollständigen. Das Smart City Lab ist hier in guter Gesellschaft: Auf dem früheren SBB-Werksgelände stellt die Zuriga-Crew hochwertige Kaffeemaschinen her, Fluidsolids tüftelt nebenan an Verfahren, um aus Abfall Recyclingplastik zu gewinnen.

Das Lab selbst ist nur dank einem kleinen Schild auf der altehrwürdigen, gemauerten Wand zu finden: erster Stock, Eingang B. Die zwei Büros sind mit Stühlen aus dem Brocki und Ikea-Interieur ausgestattet. An der Wand kleben einige hundert Post-its, man weiss: Hier wird geworkshoppt. Die eine Tür ist mit einer Sternenhimmeltapete beklebt. Sie soll ins SBB-Lab führen, bleibt aber verschlossen.
Die Stadt als Möglichkeitenmaschine

Das Unfertige ist gewollt. «Das hier ist kein Amtshaus, es ist ein Freiraum», sagt Weber. Dass Sofas und nicht technischer Firlefanz den zentralen Teil des Labs einnehmen, entspricht seiner Vorstellung einer Smart City. Hier sollen Leute zusammentreffen.

«Hugo Loetscher nannte die Stadt einst den ‹Ort grösstmöglicher Gleichzeitigkeit menschlicher Möglichkeiten›», sagt er. Das treffe das Ziel der Smart City ganz gut, «im Zentrum sollte immer der Mensch stehen». Es gehe also nicht an sich um Transportdrohnen und selbstfahrende Autos, um die jeweils neuesten technischen Visionen, wie sie Herrscher und Präsidenten seit Jahrtausenden für ihre Städte pflegten.

Hier holt der Wirtschaftshistoriker Weber weit aus: Nero brannte angeblich seine Stadt ab, um sie smarter wieder aufzubauen, Herrscher wie Napoleon und Ceausescu rissen ganze Viertel ab, um ihr Bild einer besseren Stadt zu verwirklichen. Beim rumänischen Diktator schaute am Ende bloss ein überdimensionierter Präsidentenpalast heraus.

Doch die Visionen einer Smart City veränderten sich stets, sagt Weber und erinnert an Norman Bel Geddes, der 1939 an der New Yorker Weltausstellung mit «Futurama» eine Stadt voller Highways entwarf, die alles verbinden und neue Räume erschliessen sollten. «Die heutigen Visionäre stellen sich darunter Vertical Farms vor, Drohnen für die städtische Logistik oder unterirdische Schnellzüge, die Metropolitanräume verbinden.» Alle grossen Visionen hätten den einen grundlegenden Fehler, «sie verkennen die Essenz der Stadt», eben das den Menschen Mögliche zu erweitern.
Übersetzer zwischen zwei Welten

Weber ist kein Verwaltungsneuling. Er hat ab 2012 knapp sieben Jahre für die Stadt Zürich gearbeitet, in der Wirtschaftsförderung kümmerte er sich zunächst um Firmenansiedlungen. Früh war Weber auch beim Startup-Inkubator Blue Lion involviert, den die Stadt zusammen mit Swisscom und ZKB auf die Beine stellte. Als später in Zürich die Hackathons und Startup-Programme wie der Kickstart Accelerator wie Pilze im Regen aufschossen, fungierte Weber als Scharnier und Übersetzer zwischen der Verwaltung und den Tüftlerinnen und Tüftlern, die mit der Stadt zusammen neue Dienstleistungen entwickeln wollten oder städtische Daten brauchten.

«Man muss den Startups erklären, wie die Stadt funktioniert – welche Aufgaben sie bei einer Kooperation selbst übernehmen können und welche nicht.» Viele Gründer seien sehr jung und hätten keine Industrieerfahrung. «Sie kennen die politischen Abläufe nicht.» Der Verwaltung galt es wiederum aufzuzeigen, was ausserhalb der gewohnten Strukturen, gemeinsam mit Jungunternehmen, überhaupt alles möglich wird.

Als bürokratisch und reformunwillig habe er die Stadtmitarbeiter dabei nie erlebt, sagt Weber. «Natürlich musste man Überzeugungsarbeit leisten, aber ich stiess immer auf offene Türen für eine Zusammenarbeit, für Experimente mit Startups.»
«Kein Ufo»

Aber wird Zürich jetzt wie versprochen grüner, bürgernäher oder doch sparsamer? Verheddert man sich bei der Umsetzung so gegensätzlicher politischer Wünsche nicht zwangsläufig in Widersprüchen? Aus der Politik hält sich Weber heraus. Doch er sagt: «Die Smart City ist kein Ufo, das mit neuen Visionen und Zielen in der Stadt gelandet ist. Sie ist eine klassische Unterstützungsstrategie für bestehende Ziele, wie sie der Stadtrat formuliert hat.» Smart City Zürich stelle zusätzliche Instrumente und Methoden für deren Umsetzung bereit.

Die Smart City nach Zürcher Art ist also nicht aus sich heraus politisch, aber eben doch insofern, als sie auf die politischen Ziele der Mehrheit in der Stadt einzahlen soll. Und diese Mehrheit ist, wie zahlreiche Abstimmungen und die vergangenen Wahlen zeigen, nun einmal solide links und grün verortet: Zürich arbeitet auf den «Stadtverkehr 2025» hin (10 Prozent weniger Autoverkehr, Förderung Velo, öV und Fussverkehr) oder auf die 2000-Watt-Strategie (beziehungsweise bald Netto-null-Strategie). Das kann man nun gut oder schlecht finden. Aber klar ist auch, dass sich die Smart-City-Strategie ja nicht gegen den Willen der Mehrheit umsetzen lässt.

Inzwischen, ein halbes Jahr nach der Gründung des Smart City Lab, sind erste konkrete Ideen entstanden, wie sich die Stadt im Sinne ihrer Bürger verbessern liesse. Doch die Corona-Pandemie hat auch Webers Team etwas gebremst. Bald soll aber die ganze Reihe an Werkzeugen zur Verfügung stehen, die der Gemeinderat im Vorjahr gebilligt hat:

    Innovation-Fellowships: Ab September wird die Stadt zwei Personen (Hochschulforscherinnen oder Think-Tank-Mitarbeiter etwa), die sich beruflich mit Big Data beziehungsweise selbstorganisiertem Lernen beschäftigen, für neun bis zwölf Monate anstellen. Die beiden Stellen werden laut Weber nächstens ausgeschrieben. Diese Temporär-Experten sollen neues Wissen und neue Impulse in die Verwaltung hineintragen. «Die technologische Entwicklung verläuft sehr schnell, gerade bei Themen wie künstlicher Intelligenz oder Big Data», sagt Weber. «Als Stadtverwaltung wissen wir, da kommt etwas auf uns zu, wir haben dieses Know-how aber noch nicht aufgebaut.»
    Stadtbox: Städtische Mitarbeiter reichen ihre Projektideen bei Smart City Zürich ein, eine Jury wählt die besten davon aus. Diese Mitarbeiter erhalten 70 Stunden Arbeitszeit und 1000 Franken für den Bau eines ersten kleinen Prototyps oder für eine Demo-Version. Bewährt sich die Idee, wird sie «in der Linie», also in der Dienstabteilung selbst, zur Reife gebracht. Im Gesundheits- und Umweltdepartement (GUD), das die Stadtbox seit September testen konnte, wurden so ein digitaler Schicht- und Dienstplan für die Alters- und Pflegeheime oder ein Chatbot für Energieberatungen aufgegleist, der später auch für andere Themen und Dienstabteilungen adaptiert werden kann.
    Innovationskredite: «Das ist Venture-Capital für die Stadtverwaltung, um möglichst kurzfristig ein Projekt zu testen, das mit Risiken verbunden ist», sagt Weber. Mit einem Budgetrahmen von 600?000 Franken pro Jahr (eine Million Franken ab 2021) liessen sich allerdings nur Pilotprojekte finanzieren, für die Umsetzung und den Betrieb müssen die Budgets der Dienstabteilungen herhalten. Mit solchen Krediten arbeiten städtische Mitarbeiter derzeit an der dreidimensionalen Darstellung des Zürcher Untergrunds, um die Bauplanung zu vereinfachen. Alle oberirdischen Bauwerke verfügen bereits über einen solchen digitalen Zwilling. Auch der Chatbot für die Energieberatung wird bis Ende Jahr in diesem Vehikel weiterentwickelt, ebenso ein Projekt, das mit Sensordaten die Nutzung der Infrastruktur in den städtischen Parkanlagen messen (und optimieren) will.
    Design-Sprints: Städtische Mitarbeiter aus möglichst vielen Dienstabteilungen kommen für vier- bis fünftägige Workshops ins Smart City Lab, um eine spezifische Herausforderung zu meistern. Just in der Vorwoche versammelte sich eine Gruppe aus Stadtmitarbeitern und externen Expertinnen, welche niederschwellige Meldeangebote bei sexueller Belästigung entwickeln sollte.

Zwischen Monterrey und Mühlebach

David Weber ist Stadtzürcher durch und durch. Aufgewachsen ist er nicht weit weg vom Kunsthaus, die Primarschule besuchte er im Mühlebach, das Gymnasium im Rämibühl. Und ein paar hundert Meter weiter studierte er danach Wirtschaftsgeschichte an der Universität Zürich. «Ich lebte immer entweder sehr lokal hier in Zürich oder sehr weit weg im Ausland.»

Im Studium organisierte er sich einen sechsmonatigen Austausch in der nordmexikanischen Grossstadt Monterrey, um Spanisch zu lernen. Es dünkte ihn, das sei in Mexiko, fernab der Schweizer Austauschstudenten und der Strandpartys, besser zu erreichen als in Barcelona oder Valencia. Weber blieb schliesslich nicht weniger als 18 Monate im Land und kehrte seither immer wieder dorthin zurück, dem Drogenkrieg zum Trotz, der gerade Monterrey stark getroffen hat. 2019 zog es Weber nochmals für ein fast einjähriges Sabbatical nach Mexiko-Stadt.

«Ich habe immer sehr gern für die Stadt gearbeitet. Doch nach sieben Jahren bei der Wirtschaftsförderung musste ich etwas Neues machen.» So wollte Weber mit Freunden in Mexiko-Stadt Microliving voranbringen oder startete ein E-Commerce-Startup. Während dieser erneuten Auszeit in Mexiko lernte er seine Freundin kennen, es wuchs aber auch das Heimweh nach der Schweiz, nach Familie und Freunden hier. Und Weber beobachtete mit einem Auge, dass das Stadtparlament die Smart-City-Strategie bewilligt hatte und die Stadt nun einen Leiter suchte. Also unterbrach er seine Auszeit und bewarb sich.

Die Corona-Krise hat für Weber auch privat die Pläne durcheinandergewirbelt: Seine Freundin plante, fürs Studium nach Spanien auszuwandern, was doch ein Stück näher bei Zürich liegt als Monterrey. Jetzt, da das Virus in Mexiko so heftig wütet wie kaum anderswo auf der Welt, ist das fürs Erste vom Tisch. Die menschlichen Möglichkeiten, wie sie Weber in Zürich erweitern will, sind derzeit empfindlich eingeschränkt. Aber bis jetzt haben die Städte sich von jeder Pandemie wieder zurückgekämpft und sich dabei verbessert, etwa mit Abwassersystemen gegen die Cholera. Als Optimist baut David Weber auf die «smarte» City, die sich ihren Freiraum wieder zurückholt.";https://www.nzz.ch/zuerich/smart-city-zuerich-david-weber-will-das-menschenmoegliche-ld.1564119;NZZ;Andre Müller;;;
25.05.2020;«Bis auf die Unterhose überwacht»: Wie Unternehmen im Home-Office Daten über ihre Mitarbeiter sammeln;"In der Corona-Krise mussten viele Angestellte plötzlich zu Hause arbeiten. Daran haben nun einige Firmen Gefallen gefunden, sie wollen in Zukunft vermehrt auf Home-Office setzen. Für Vorgesetzte hat das Konsequenzen. So sehen sie beispielsweise nicht mehr, ob ihre Mitarbeiter gerade am Computer sitzen oder lieber mit den Kollegen Kaffee trinken. Viele scheinen deshalb bereits nach Alternativen zu suchen: Die Nachfrage nach Software für «Mitarbeiter-Tracking» ist bei verschiedenen Herstellern bis zu dreimal höher als vor der Corona-Krise, wie die «Süddeutsche Zeitung» und die «New York Times» berichtet haben.

Hubstaff, Activtrak oder Time Doctor sind Namen solcher Tracking-Anbieter, deren Programme von Kritikern als «Spy-Software» bezeichnet werden. Die Hersteller selbst meiden das Wort «Überwachung» und versprechen stattdessen, die Leistung von Mitarbeitern zu messen und beispielsweise in einem «Produktivitäts-Score» abzubilden. Dafür zeichnen sie mitunter jede Bewegung des Mauszeigers auf dem Bildschirm und jeden Tastaturanschlag auf. Oder sie erlauben es einer Chefin, zu sehen, welche Websites ihre Mitarbeiter wie lange aufrufen.
Überwachte E-Mails und Bewegungs-Tracking

In der Schweiz können Chefs diese Programme nicht einfach nutzen, wie sie wollen. In einer Verordnung zum Arbeitsgesetz heisst es: «Überwachungs- und Kontrollsysteme, die das Verhalten der Arbeitnehmer am Arbeitsplatz überwachen sollen, dürfen nicht eingesetzt werden.» Ausnahmen sind jedoch möglich, wenn es Hinweise auf Missbrauch durch die Mitarbeiter gibt – oder wenn das Unternehmen die Technologien aus anderen Gründen nutzt.

Die Arbeitsinspektorate von Wirtschaftskantonen wie Bern, Zürich oder Basel-Stadt schreiben auf Anfrage, sie hätten bisher keine Beschwerden wegen virtueller Überwachung am Arbeitsplatz erhalten. Wer sich Sorgen über firmeninterne Überwachung macht, kann sich jedoch auch an den Eidgenössischen Datenschutz- und Öffentlichkeitsbeauftragten (Edöb) wenden. Dort heisst es, in der Schweiz werde zwar Software angewendet, welche die Produktivität von Mitarbeitenden messe, der Edöb habe dazu jedoch bisher keine Abklärungen vorgenommen. Die meisten Anfragen beträfen die Überwachung von E-Mails, die Beobachtung des Surfverhaltens oder das Tracken der Bewegung im Firmenfahrzeug oder per Smartphone. Dies sind Überwachungsformen, die auch in den «Spy-Softwares» enthalten sein können.

Der Edöb-Kommunikationschef Hugo Wyler betont, Arbeitgeber müssten bei der Überwachung grundlegende Prinzipien einhalten und beispielsweise transparent machen, welche Daten sie erfassten. Zudem sollen sie nicht unnötig Daten sammeln und aggregieren. Die Bearbeitung müsse «verhältnismässig» sein. Weiter stellt Wyler klar: «Eine dauerhafte Verhaltensüberwachung ist nicht erlaubt.»
Oft wird die Leistung des ganzen Teams gemessen

Arbeitsforscher der Universität St. Gallen haben im Rahmen des Nationalen Forschungsprogramms zu Big Data (NFP 75) mehrere Firmen untersucht, die Datafizierungstechnologien anwenden. Dabei handelt es sich laut Forschungsassistent Simon Schafheitle nicht ausschliesslich um klassische «Spy-Software». Trotzdem erlauben es die Programme beispielsweise, Bewegungen am Bildschirm aufzuzeichnen.

«Wir haben mit Mitarbeitern gesprochen, die bis auf die Unterhose überwacht werden», sagt Schafheitle und ergänzt: «Vor kurzem wären mir noch die Haare zu Berge gestanden, wenn ich gewusst hätte, wie viele detaillierte Informationen da über Angestellte erhoben werden.» Es sei jedoch wichtig zu beachten, dass die Unternehmen die Technologien primär nutzen würden, um Prozesse zu optimieren. So könne ein Arbeitgeber anhand der Bewegung des Mauszeigers herausfinden, ob eine Tätigkeit automatisiert werden kann. Oft werden die Daten daher aggregiert und nicht auf individueller Ebene ausgewertet. Leistungen einzelner Mitarbeiter oder eines ganzen Teams können die Programme jedoch auch messen – quasi als Nebeneffekt.

Weil die Überwachung bei einigen Firmen so weit geht, wollten die Forscher wissen, welchen Einfluss das Datensammeln auf das Vertrauen von Mitarbeitern hat. Sie haben beobachtet, dass Angestellte zweier Unternehmen, die vergleichbare Technologien anwenden, völlig unterschiedlich darauf reagieren. «Bei einer Firma finden es die Leute toll, bei der anderen schrecklich, obwohl der Eingriff in die Privatsphäre gleich weit geht», erklärt Schafheitle.
Der Algorithmus als «Chef aus Fleisch und Blut»

Antoinette Weibel, die das St. Galler Forschungsprojekt leitet, schliesst daraus, dass es nicht nur zentral ist, dass Mitarbeiter wissen, welche Daten gesammelt werden. Vielmehr sollten sie früh einbezogen werden und beispielsweise an der Auswertung der Daten teilhaben können. «Das gibt meist den Ausschlag bei der Frage, ob man die Technologie als eine Art Big Brother wahrnimmt und sich ständig überwacht fühlt oder darin eine Lernchance oder eine Hilfestellung sieht.» Auch ein kritischer Umgang der Vorgesetzten mit den Algorithmen sei zentral.

Zur Verbreitung von Technologien, die Daten über Mitarbeiter sammeln, ist in der Schweiz wenig bekannt. Die St. Galler Forscher fanden in einer ersten Umfrage bei rund 160 grossen Schweizer Unternehmen heraus, dass 37 Prozent der Firmen verschiedene Datafizierungstechnologien einsetzen, um Leistungen zu beurteilen. 18 Prozent nutzen sie, um zu sehen, ob sich ihre Mitarbeiter an die Regeln halten.

Dies war jedoch vor der Corona-Krise. Daher wollen Weibel und Schafheitle bereits im Juni eine zweite Umfrage durchführen. Sie rechnen damit, dass der Anteil der Unternehmen, welche die Leistung ihrer Mitarbeiter messen oder das Einhalten der Arbeitszeiten im Home-Office kontrollieren wollen, in den nächsten Monaten weiter steigen wird.";https://www.nzz.ch/schweiz/ueberwachung-am-arbeitsplatz-firmen-sammeln-daten-im-homeoffice-ld.1557068;NZZ;Larissa Rhyn;;;
29.03.2019;So sammeln Unternehmen Daten über ihre Mitarbeiter – und nutzen sie für das Personalmanagement;"Am Arbeitsplatz zu sitzen, während der Chef einem permanent über die Schulter blickt – das ist selbst für den pflichtbewusstesten Angestellten eine beunruhigende Vorstellung. In der virtuellen Welt ist ein ähnliches Szenario aber für viele bereits real. Vor allem Grossunternehmen sammeln mit Softwares Daten über Mitarbeitende und speisen sie – in der Regel anonymisiert – in ein zentrales System ein. Dort kann sie der Arbeitgeber nutzen, um unzählige Fragen zu beantworten: Wie kann der Arbeitsplatz besser gestaltet werden? Wie binden wir unsere Mitarbeiter an uns? Aber auch: Was leisten sie?

Big Data und die gesellschaftlichen Implikationen der Auswertung grosser Datenmengen sind bisher weitgehend unerforscht. Kein Wunder, interessiert sich der Bundesrat dafür. Und zwar so sehr, dass er 25 Millionen Franken für ein fünfjähriges nationales Forschungsprogramm ausgibt, das im Jahr 2017 begonnen hat. Ein Projekt mit dem eingängigen Namen «Big Data – Big Brother» will herausfinden, welche Datafizierungsmethoden Schweizer Unternehmen im Personalmanagement einsetzen. Datafizierung bedeutet, dass die Daten so formatiert werden, dass grosse Mengen leichter analysiert werden können.
Videoüberwachung ist populär

Eine erste, unveröffentlichte Umfrage bei knapp 160 grossen Schweizer Unternehmen zeigt, dass vor allem folgende Datafizierungstechnologien genutzt werden: Mitarbeiterumfragen, Videoüberwachung, Eintritts- und Austrittsbefragungen und ID-Badges.

Was heute im Arbeitsalltag bereits gemacht wird, lässt aufhorchen. Besonders, weil fast 30 Prozent der befragten Unternehmen angaben, Videoaufnahmen im Personalmanagement einzusetzen. Zwar ist die Studie nicht repräsentativ, weil die meisten Teilnehmer laut den Forschern Pioniere im Bereich Big Data sind und viele andere Schweizer Unternehmen diese Technologien noch gar nicht nutzen. Doch es stellt sich die Frage, ob sie Vorreiter eines Trends sind, der bald weit verbreitet sein wird.

Antoinette Weibel, Professorin für Personalmanagement, die das Projekt an der Universität St. Gallen (HSG) leitet, sagt, Videoüberwachung werde in der Regel dazu genutzt «Prozesse zu optimieren oder Büros besser zu gestalten» – und nicht etwa zur Leistungsüberwachung. Aber wenn die Technologie vorliegt, scheint eine Rund-um-die-Uhr-Überwachung nicht mehr weit entfernt. Entsprechend sind viele Arbeitnehmer besorgt: Beim Beratungsdienst zur Überwachung am Arbeitsplatz, den der Eidgenössische Datenschutzbeauftragte (Edöb) anbietet, gehört die Videoüberwachung zu den Top-Themen.

Daneben erhält der Edöb viele Anfragen zum Thema Bewegungsverfolgung mittels Smartphones oder zur Überwachung von E-Mail-Verkehr und Surf-Verhalten. Dabei geht es auch um Softwares, welche eine «zeitlich lückenlose Überwachung» erlauben. Manchmal werden diese vom Arbeitgeber gar heimlich installiert. Wenn Mitarbeiter Verdacht schöpfen, prüft der Edöb den Sachverhalt, gibt eine Empfehlung ab und kann unrechtmässige Überwachungen verbieten.
Fast 40 Prozent prüfen Leistungen mittels Daten

Datafizierungstechnologien können vielfältig genutzt werden. In der HSG-Umfrage gaben 37 Prozent der teilnehmenden Firmen an, dass sie die Technologien im Leistungsmanagement einsetzen. 61 Prozent nutzen sie für die Mitarbeitendenbindung und -entwicklung. Dabei spielen sogenannte «People Analytics» eine grosse Rolle: Personalabteilungen wollen mittels anonymisierter Daten ihrer Mitarbeiter Fragen zum Unternehmen beantworten.

In der IT- und Kommunikationsbranche ist das Interesse an People Analytics laut der HSG-Professorin Weibel besonders gross. Entsprechend hat auch die Gewerkschaft Syndicom das Thema auf dem Radar: Sie hat eine Sensibilisierungskampagne unter ihren Mitgliedern durchgeführt. Mediensprecher Christian Capacoel sagt, die Datensammlung sei nach wie vor eine Blackbox. «Darum fordern wir Unternehmen dazu auf, zusammen mit den Sozialpartnern Richtlinien auszuarbeiten, welche Daten wie erfasst und genutzt werden dürfen.» Um zu prüfen, ob sich Arbeitgeber anschliessend an die Richtlinien halten, hat Syndicom paritätische Kontrollen eingeführt.

Die Swisscom hat sich bereits mit Syndicom abgesprochen. Das Unternehmen klärt bei jeder Fragestellung ab, welche Daten konkret benötigt werden und ob der Nutzen die Erhebung rechtfertigt. Zudem werden laut Mediensprecherin Sabrina Hubacher rechtliche und ethische Gesichtspunkte geprüft. Die Swisscom arbeitet beispielsweise mit anonymisierten Austritts- und Lohndaten oder mit Mitarbeiterumfragen. Das variiere je nach Fragestellung, erklärt Hubacher. Und ergänzt: «Wir sehen die Daten als Leihgabe der Mitarbeiter, darum informieren wir sie auch über die Ergebnisse.»

Es sei zentral, die Mitarbeiter auch darüber aufzuklären, wie sie von der Datenanalyse profitieren können, findet Studienleiterin Weibel: «Mitarbeiter haben die Chance, dank Big Data dazuzulernen und Zeit zu sparen.» Sie sollten deshalb früh einbezogen und bei jeder neuen Entwicklung informiert werden.
Mitarbeiter müssen Überwachung erlauben

Wenn Unternehmen Datafizierungstechnologien nutzen oder gar E-Mails oder Telefonate überwachen, sind sie dazu verpflichtet, das Einverständnis ihrer Mitarbeiter einzuholen – zum Beispiel in einem IT-Reglement. Dort sollte der Arbeitgeber nicht nur deklarieren, welche Daten erfasst werden, sondern auch, zu welchem Zweck sie genutzt werden.

Der Arbeitgeber muss sich auch an diverse gesetzliche Vorgaben halten: Das Arbeitsvertragsrecht des Obligationenrechts enthält eine relevante Bestimmung, genau wie Artikel 13 der Bundesverfassung, das Fernmeldegesetz und das Datenschutzgesetz. Reto Fanger, Anwalt für ICT-, Daten- und Arbeitsrecht, sagt: «Diese Fülle an Gesetzesquellen macht es sowohl für Arbeitgeber als auch für Arbeitnehmer schwierig, herauszufinden, was erlaubt ist und was nicht.»
Daten bleiben nicht immer anonym

Auch anonymisierte Daten sind nicht immer unproblematisch. Insbesondere dann, wenn so viele Datensätze gesammelt und kombiniert würden, dass Rückschlüsse auf Personen möglich würden, erklärt Fanger. «Das ist bisher nicht reguliert, und es wird schwierig, einheitlich festzulegen, bis zu welcher Schwelle die Datenaggregation erlaubt ist.» Wenn ein Unternehmen dank einer Daten-Software feststellt, dass ein Mitarbeiter gegen die Regeln verstösst, sei es im Einzelfall zulässig, von den Daten Rückschlüsse auf die Person zu ziehen. Einige der geläufigen Technologien erlauben dies laut dem Anwalt auch – trotz Anonymisierungsversprechen.

Eine weitere Schwierigkeit ergibt sich daraus, dass viele Arbeitgeber heute eine massvolle private Nutzung von E-Mail und Telefon am Arbeitsplatz erlauben. «Dadurch wird die Datenerfassung heikel, weil der Arbeitgeber keine privaten Daten der Mitarbeiter auswerten darf.»

Momentan steckt die Vermessung und Überwachung von Mitarbeitenden hierzulande – im Vergleich zu den USA oder China – noch in den Kinderschuhen. Studienleiterin Weibel sagt: «Gruslige Technologien, wie Bio-Chips, welche den Mitarbeitern unter die Haut operiert werden, sind in der Schweiz noch Zukunftsmusik.» Doch die Angebote schiessen förmlich aus dem Boden, und Weibel rechnet damit, dass die Nutzung von Big Data in der Arbeitswelt in den nächsten Jahren zunehmen wird.
Die Videoüberwachung der Mitarbeiter hat vor allem im Detailhandel und in der Gastronomie zugenommen. (Bild: Christian Beutler / Keystone) ";https://www.nzz.ch/schweiz/ueberwachung-am-arbeitsplatz-wie-firmen-mitarbeiterdaten-sammeln-ld.1470741;NZZ;Larissa Rhyn;;;
18.11.2017;Digitale Planwirtschaft führt zu einem orwellschen Schreckensszenario;"Es sind wehmütige Zeiten für Altlinke. Allenthalben wird der grossen sozialistischen Oktoberrevolution und der gewaltsamen Machtübernahme der Bolschewiken vor hundert Jahren gedacht. Vielerorts erfolgen die Rückblenden mit einem nicht zu knappen Mass an Pathos und Faszination für die damals angestrebte Herrschaft der Arbeiter und Bauern. Dass schliesslich alles ganz anders kam und die vermeintlichen Sozialromantiker nicht allzu sozial vorgingen, sondern vielmehr zur Tyrannei neigten, ist längst Geschichte. Ebenfalls abgehakt wähnt man das von den Kommunisten propagierte Ordnungsprinzip der Planwirtschaft. Das Scheitern dieser Idee musste spätestens beim Mauerfall von 1989 für jedermann offenkundig sein, und fast alle exkommunistischen Staaten haben sich der Marktwirtschaft zugewandt. Ausnahmen wie Nordkorea oder Kuba bestätigen die Regel.
Ist die unsichtbare Hand des Marktes plötzlich sichtbar?

Warum die Planwirtschaft nicht funktionierte, sondern zu Überforderung und Verschwendung führte, ist rasch erklärt: Die Planer verfügten nicht über die notwendigen Informationen zur Steuerung der Wirtschaft. Wie Paul Mason in seinem 2016 erschienenen Buch «Postkapitalismus» schreibt, existierten 1980 in der Sowjetunion rund 24 Millionen verschiedene Produkte. Das Moskauer Regime, das theoretisch alle wirtschaftlichen Aktivitäten in elf Zeitzonen hätte koordinieren müssen, kannte aber nur die Preise und Mengen von 200 000 Produkten. Und im zentralen Plan, der den Staatsfirmen langfristig die Richtung vorgab, wurden gerade noch 2000 erfasst. Die Behörden rangen also mit einem riesigen Wissensproblem, das Friedrich August von Hayek schon 1944 vorausgesagt hatte. Die kommunistischen Chefplaner hatten keine – oder nur sehr vage – Vorstellungen von den tatsächlichen Bedürfnissen, Knappheiten und Überschüssen im Land. Es fehlten ihnen verlässliche Daten.

    Eine digitale Planwirtschaft
    ist nicht nur ineffizient, da sie Bedürfnisse verkennt und
    Innovationen verhindert. Sie läuft auch auf einen orwellschen Überwachungsstaat hinaus.

An Daten herrscht hundert Jahre nach der Oktoberrevolution kein Mangel mehr. Ganz im Gegenteil: «Big Data» lautet das Zauberwort. Es verdeutlicht, dass wir im 21. Jahrhundert in einem Meer von Bits und Bytes schwimmen, die rund um die Uhr ortsunabhängig abrufbar sind. Als Datenquellen dienen soziale Netzwerke, Suchmaschinen, Mobiltelefone, Navigationssysteme, Satelliten, Kreditkarten, Online-Shops und vieles mehr. Kluge Algorithmen ertasten unsere Wünsche, bringen einsame Herzen zusammen und sagen uns, an welchen Büchern, Musiktiteln oder Menschen wir Gefallen finden könnten. Das «Internet der Dinge» vernetzt derweil die physische und die virtuelle Welt auf immer komplexere Weise: Wenn im Kühlschrank des Supermarkts das Joghurt zur Neige geht, weiss das die Molkerei in Echtzeit und organisiert den Nachschub mit den gerade besonders angesagten Geschmackssorten.

Von solchen Vernetzungen und Datenmengen konnten die Kommunisten im Kalten Krieg nur träumen. Nicht nur in sozialistischen Zirkeln wird daher die Frage aufgeworfen, ob die Planwirtschaft heute besser funktionieren würde, ob sie vorschnell auf der Müllhalde der Geschichte entsorgt wurde, ob Digitalisierung, Cloud-Computing und künstliche Intelligenz das Wissensproblem der Planwirtschaft lösen könnten. Jack Ma, der Gründer und Chef der chinesischen Alibaba-Gruppe, bejaht all diese Fragen. Der milliardenschwere Unternehmer, auf dessen Online-Plattform sich schon gegen 500 Millionen Kunden tummeln, zeigt sich überzeugt, dass der Planwirtschaft die Zukunft ge­hört. Big Data sei wie ein Röntgenapparat, wie Computertomografie, mit der die Weltwirtschaft bis in die feinsten Verästelungen erkennbar gemacht werden könne. «Weil wir heute Zugang zu allen Arten von Daten haben, werden wir vielleicht fähig sein, die sichtbare Hand des Marktes zu finden», sagt Ma.

Hat somit Adam Smiths «unsichtbare Hand», die am Markt unbewusst das Gemeinwohl fördert, ausgedient? Steckt hinter dieser effizienten Koordination von Angebot und Nachfrage letztlich nur ein komplizierter Algorithmus, der sich mit genügend Rechenleistung durchaus sichtbar machen lässt? Schon Salvador Allende träumte diesen Traum. Im sozialistischen Chile der frühen 1970er Jahre bastelte er zusammen mit dem britischen Kybernetiker Stafford Beer am Projekt «Cybersyn», einem landesweiten Fernschreiber-Netzwerk, das die wichtigsten Fabriken des Landes miteinander ver­band; das funktionierte aber eher schlecht als recht. 2017 ist die abrufbare Datenmenge jedoch ungleich grösser als damals. Das bringt selbst gestandene Banker – traditionell kaum dem Kommunismus zugeneigt – zum Grübeln. So zeigten sich unlängst an einer Finanzkonferenz am Londoner Bankenplatz verschiedene Votanten davon überzeugt, dass die Planwirtschaft die ihr innewohnenden Informationsprobleme heute überwinden könnte.

Niemand stellt in Abrede, dass Big Data – angeblich das neue Gold des 21. Jahrhunderts – zu mehr Effizienz und Kundennähe führen kann. Zutreffend dürfte auch sein, dass menschliches Verhalten dank selbstlernenden Algorithmen etwas berechenbarer geworden ist. Dennoch entbehrt die Behauptung, das Wissensdefizit der Planwirtschaft sei dank Big Data gewissermassen behoben, jeder Grundlage. Empirische Daten oder Statistiken – und seien sie noch so ausgeklügelt – beziehen sich immer nur auf die Vergangenheit. Sie sind daher für Vorhersagen zukünftiger Entwicklungen oder Verhaltensänderungen von nur beschränktem Wert. Die Wirtschaft von morgen ist weit mehr als die Extrapolation der Wirtschaft von gestern. Sie ist ein hochkomplexes, nichtlineares, dynamisches Phänomen, das in zumeist chaotischer Manier immer wieder zu einer neuen spontanen Ordnung findet. Gegenüber solcher Komplexität ist Demut gefragt, nicht Kontrollwahn.
Der Staatsmonopolist hat keinen Anreiz zur Innovation

Der Apple-Gründer Steve Jobs hatte schon recht: «Es ist wirklich schwer, Produkte für Zielgruppen zu entwickeln. Meistens wissen die Leute nicht, was sie wollen, bis man es ihnen zeigt.» Mit anderen Worten: Die Fixierung auf Umfragen oder vergangenheitsorientierte Daten führt selten zu grosser Innovation. Nötig ist die Fähigkeit, radikal mit dem Bisherigen zu brechen. Hätte Apple nicht sämtliche Vorstellungen mobiler Kommunikation über den Haufen geworfen, das iPhone wäre nie erfunden worden. Ein zentraler Planer ist dazu kaum in der Lage. Er hat keinen Anreiz zur Innovation, da er keine Konkurrenz fürchten muss. Er wird selbst bei Verfügbarkeit gigantischer Datenmengen stets im Dunkeln tappen, weil das Preissignal, das in einem Markt der wichtigste Informationsträger ist, in einer Staatswirtschaft keine Aussagekraft mehr hat. Und wenn der Preis keine Knappheit oder Geschäftschance mehr signalisiert, weiss auch niemand, ob ein Gut gefragt ist oder nicht.

Um einem Missverständnis vorzubeugen: Niemand fordert heute eine Rückkehr zu einer Planwirtschaft sowjetischer Bauart. Die Debatte ist subtiler, deswegen aber nicht weniger gefährlich. Ins Spiel gebracht werden alternative Spielarten des Dirigismus, etwa ein hybrides Nebeneinander privater Akteure und eines staatlichen Eigentümers an all jenen Online-Plattformen, die heute von Google, Facebook oder Amazon beherrscht werden. Zwar kann man sich mit guten Gründen stören an den monopolähnlichen Positionen dieser Konzerne bei Suchmaschinen, Social Media und Online-Handel. Deswegen die Kontrolle über Big Data aber dem Staat anzuvertrauen, macht die Sache nicht besser. Zwischen Algorithmen muss mehr Wettbewerb bestehen, nicht weniger. Erklärt man Daten zu Staatseigentum, droht digitaler Totalitarismus, wie er sich in China bereits abzuzeichnen beginnt. So will Peking bis 2020 ein engmaschiges Überwachungsnetz aufspannen, in dem jeder digitale Fussabdruck erfasst und jeder Bürger mit einem Punktesystem benotet wird – «Social Credit System» nennt sich das unverfänglich.

Eine digitale Planwirtschaft ist nicht nur ineffizient, da sie Bedürfnisse verkennt und Innovationen verhindert. Sie läuft auch auf ein orwellsches Schreckensszenario hinaus, in dem das Individuum zum Datenpaket verkommt. Beides ebnet den «Weg zur Knechtschaft». Hayek, der Autor des gleichnamigen Werks, erkannte schon vor sieben Jahrzehnten, dass das in einer Gesellschaft verstreute Wissen eben nicht statistisch erfassbar, sondern an die Person, den Ort, das Umfeld gebunden ist. Seine Warnung vor einer Anmassung von Wissen erscheint heute aktueller denn je. So nähren die Verheissungen von Big Data erneut den – 1989 besiegt geglaubten – Irrglauben, die Komplexität der Wirtschaft könne nicht nur verstanden, sondern gar gelenkt werden. Solcher Hochmut ist geschichtsblind. Nicht jeder Fehler der Historie muss wiederholt werden, auch nicht nach hundert Jahren.";https://www.nzz.ch/meinung/big-data-kann-den-kommunismus-nicht-retten-ld.1329778;NZZ;Thomas Fuster;;;
07.01.2020;«Das grösste Problem ist die Kommerzialisierung»;"Als hätten sie bei iCarbonX (ICX) nicht schon genug Probleme, ist das Unternehmen aus dem südchinesischen Shenzhen unlängst auch noch in den Sog des amerikanisch-chinesischen Handelsstreits geraten. Die US-Behörde zur Kontrolle ausländischer Investitionen, CFIUS, zwang ICX, die Beteiligungen an den US-Firmen PatientsLikeMe und HealthTell abzustossen. Sherry Xian, Chief Data Officer bei ICX, sagte am Rande des Stein-am-Rhein-Symposiums des Business-Netzwerks Stars: «Wir brauchen jeglichen Segen.»

Wohl wahr. Denn schwerer noch als die Vorgaben der US-Behörde wiegen die unsicheren Geschäftsaussichten des Startups. Im Zuge der konjunkturellen Abkühlung sind die noch vor kurzem breiten Kapitalströme für Gründer in China zu Rinnsalen zusammengeschrumpft. ICX mit seinen heute rund 1000 Mitarbeitern sucht zudem auch vier Jahre nach der Gründung noch nach einem schlüssigen Geschäftsmodell. «Das grösste Problem bei uns ist die Kommerzialisierung», räumt Xian ein. Dabei haben Chinas Behörden das Startup schon mit reichlich Subventionen gepäppelt.

Es hatte 2015, als noch reichlich Kapital vorhanden war, alles so gut begonnen – ICX legte einen regelrechten Senkrechtstart hin. Kapitalgeber, darunter auch der ebenfalls in Shenzhen beheimatete chinesische Tech-Riese Tencent, investierten bisher 600 Mio. $; die Bewertung des Startups erreichte innert sechs Monaten 1 Mrd. $. Damit ist ICX Chinas erstes Biotech-Einhorn.
Big Data und KI sollen die Gesundheit fördern

Die Idee von ICX ist im Grundsatz durchaus überzeugend: Big Data und künstliche Intelligenz (KI) sollen ein gesünderes und damit längeres Leben ermöglichen. Gründer von ICX ist Wang Jun, ein 43-jähriger Biotech-Wissenschafter und Unternehmer. Mit 23 Jahren gründete er die Bioinformatik-Gruppe des Beijing Genomics Institute (BGI), welches Chinas Beitrag zum Humangenom-Projekt leistete, mit dem das Genom des Menschen entschlüsselt werden sollte. Das Startup sequenzierte unter Wang als CEO unter anderem die DNA des Sars-Virus, ein gefährlicher Erreger, der 2003 Asien heimsuchte, ausserdem die DNA des Pandas und erreichte einen Umsatz von 250 Mio. $. 2015 gründete Wang iCarbonX.

Das Unternehmen entwickelt eine Vielzahl von Anlagen und Verfahren, mit denen grosse Mengen Gesundheitsdaten, etwa zum Cholesterinspiegel, zu den Proteinwerten oder zum Herzrhythmus, erhoben werden und mittels Big Data und KI aufbereitet werden können. Daraus können dann Empfehlungen zum richtigen Verhalten abgeleitet werden. Demnächst will ICX seine Plattform «Health Buddy» an den Start schicken. Darüber können Kunden in Echtzeit ihren Gesundheitszustand verfolgen – alles in allem: ein KI-gestütztes Vorsorgesystem.

Die 34-jährige Xian ist 2016 zu ICX gestossen. Mit einem Doktortitel in Psychologie der Uni Chicago sowie Weiterbildungen in Neurobiologie an der renommierten Stanford-Universität fügt sich die ambitionierte junge Frau bestens in die Truppe der Top-Talente bei ICX ein.
Missionarischer Eifer statt durchdachter Businessplan

Fehlt nur noch der kommerzielle Erfolg. Das grösste Problem der Gründer aus Shenzhen: Die meisten gesunden Menschen sind (noch?) nicht bereit, für ein solches System Geld auszugeben, die Digitalisierung der Daten ist noch sehr teuer. Eine Analyse zum Genom koste beispielsweise 200 $, eine zu Darmbakterien 100 $ und zu DNA-Bakterien bis zu 2000 $, erklärt Xian.

Die ICX-Gründer waren vor allem von ihrem missionarischen Eifer getrieben, einen durchdachten Businessplan gibt es noch immer nicht. Xian gesteht: «Zu Beginn haben wir uns darüber nicht wirklich den Kopf zerbrochen.» Der Gründer Wang sei davon überzeugt gewesen, dass die Menschen den Wert von ICX erkennen würden und auch bereit wären, dafür zu zahlen. Ein Irrtum. «An Profit können wir im Moment nicht denken», sagt Xian. Ein möglicher Durchbruch kann somit noch Jahre dauern. Doch der Druck der Investoren nehme zu, stellt Xian klar.

Nicht einfacher wird das Geschäft dadurch, dass ICX immer noch zu wenige Daten hat, um das System breit auszurollen. Dabei sind die Chinesen beim Datenschutz schon weit weniger sensibel als die Menschen im Westen.

Um an mehr Daten zu kommen, sucht ICX jetzt Kooperationen mit anderen Firmen. Bei Health-Food-Unternehmen, die oft schon einen Kundenstamm haben, hilft ICX bei der Zusammenstellung individueller Gerichte. Mit anderen Firmen wollen die Gründer bei Gesundheitschecks für die Belegschaften zusammenarbeiten. Fraglich, ob das den kommerziellen Durchbruch bringt. ICX dürfte unsicheren Zeiten entgegengehen.";https://www.nzz.ch/nzz-asien/das-groesste-problem-ist-die-kommerzialisierung-ld.1532362;NZZ;Michael Settelen;;;
31.10.2018;Wähler, wir wissen, wo du wohnst;"Die CVP will weg von den bisherigen Wahlkampfmethoden. Statt überall im Lande orangefarbene Plakate kleben zu lassen, setzt sie nun auf zielgenaue Überzeugungsarbeit im weltweiten Netz. Im Auftrag der Partei verwendet eine auf Datenanalyse spezialisierte Kommunikationsagentur ihr Know-how dazu, potenzielle Wählerinnen und Wähler ausfindig zu machen und gezielt anzusprechen. Stichwort: Big Data. Neu setzt die Partei zudem auf sogenannte Influencer. Eine Anfang September lancierte Kampagne hat das Ziel, 200 überzeugte Parteigänger zu rekrutieren und sie als Botschafter einzusetzen. Ihre Hauptaufgabe besteht vor allem darin, über ihre Social-Media-Netzwerke CVP-Botschaften weiterzuverbreiten.
Kenne deine Wähler

Influencing, Microtargeting oder Canvassing heissen die Methoden, auf die Schweizer Parteien zunehmend ihre Hoffnungen setzen. Beim Microtargeting etwa werden Inhalte im Netz auf einzelne Gruppen oder sogar einzelne Individuen zugeschnitten. Oft merken die Empfänger der Nachrichten nicht, dass sie andere Inhalte erhalten als andere Gruppen und Personen.

Mit Canvassing – zu Deutsch Kundenfang – meint man systematische Wählerkontakte, wie dies bei Tür-zu-Tür- oder Telefonkampagnen der Fall ist. Weltweit bekannt wurden diese Instrumente durch verschiedene amerikanische Präsidentschaftswahlkämpfe. Auch der Triumph von Emmanuel Macrons La République en Marche wird zum Teil einer cleveren Mobilisierungskampagne nach angelsächsischem Vorbild zugeschrieben.

Das Prinzip, das all diesen Methoden zugrunde liegt, ist die systematische Vermessung des Bürgers. Dazu sammeln die Parteien fleissig Daten, aufgrund deren sie dann ein Profil erstellen. Das Ziel lautet immer gleich: Kenne deine Wähler.

Die grösste Erfahrung mit dem modernen Stimmenfang hat hierzulande die SP. Die Sozialdemokraten waren die Ersten, die systematisch Informationen über ihre Sympathisanten zusammentrugen und diese wenn immer möglich direkt ansprachen. Ihre für die nationalen Wahlen 2015 entwickelte Basiskampagne – zu der auch die mittlerweile berühmten Telefonaktionen gehören – ist so erfolgreich, dass sie nun auch von der wahlkampftechnisch noch im analogen Zeitalter verhafteten SVP kopiert wird.

Der Parteisekretär der SP Solothurn gab vor zwei Jahren in einem Zeitungsinterview freimütig zu Protokoll: «Wir verfügen über ein tolles Tool, in welchem wir seit vielen Jahren Adressen und Informationen über unsere Sympathisanten sammeln. Die Datensammlung besteht nicht etwa aus zufällig kopierten Adressen. Darin sind nur Leute registriert, die sich bereits auf irgendeine Art für unsere Anliegen eingesetzt haben.»
Wenn der Freisinn zwei Mal klingelt

Nun ist auch die FDP auf den Geschmack gekommen. Die Freisinnigen sind bereits seit einiger Zeit dabei, ihre Kampagnenkompetenz zu stärken und zu zentralisieren. Im Hinblick auf die nationalen Wahlen 2019 gehen sie aber einen Schritt weiter. Die Parteipräsidentenkonferenz hat kürzlich beschlossen, auf die im Obama-Wahlkampf berühmt gewordene «Get out the vote»-Methode zu setzen. Zu Deutsch: Die FDP will einen datengestützten Tür-zu-Tür-Wahlkampf führen.

In Zürich, Luzern und Basel-Landschaft sollen nun Pilotversuche durchgeführt werden. In diesen Kantonen werden jeweils wenige Monate vor dem eidgenössischen Wahlsonntag Regierungs- und Parlamentswahlen durchgeführt. Fallen die Erfahrungen in den drei Kantonen vielversprechend aus, soll die Methode im Hinblick auf die nationalen Wahlen landesweit zum Einsatz kommen.

Der Tür-zu-Tür-Wahlkampf, auf den die FDP setzt, wurde in Europa unter anderem bereits in Deutschland, den Niederlanden, Irland, Frankreich und Grossbritannien angewandt. Laut Kommunikationsfachleuten gehört er zu den erfolgreichsten und wirkungsmächtigsten Instrumenten des politischen Marketings. Um zu wissen, wo die freisinnigen Wahlkämpfer mit Offenheit und Sympathien rechnen können, arbeitet die Partei mit einem Datenmodell, das eine Einschätzung des Wählerpotenzials nach Gemeinden, Quartieren und Strassenzügen erlaubt.

Laut dem stellvertretenden FDP-Generalsekretär Matthias Leitner handelt es sich dabei um Datenmodelle, wie sie in der Schweiz vom Forschungsinstitut GFS und vom Sinus-Institut angeboten werden. Erfasst werden nicht nur Wählerstatistiken, sondern – wie im Fall der sogenannten Sinus-Milieus – eigentliche Lebensweltanalysen. Die FDP weiss also bereits, in welchen Wohngegenden sie auf Sympathien hoffen darf und in welchen eher nicht.

In die als tendenziell erfolgversprechend erkannten Territorien entsendet die Partei nun Freiwillige. Diese haben die Aufgabe, sich bei ihren Hausbesuchen anhand eines Fragenkatalogs zu den politischen Interessen und Befindlichkeiten der Besuchten vorzuarbeiten. Die Erkenntnisse, die die Wahlkämpfer während der Gespräche gewinnen, werden anschliessend in einer eigens für diesen Zweck entwickelten App erfasst: Was hält der Gesprächspartner von der FDP? Welche Themen treiben ihn um? Wie alt war das Gegenüber? War es ein Mann oder eine Frau? Die Daten werden anschliessend in einem Modell gerechnet und zusammengefasst. Daraus resultiert eine Empfehlung, ob und wann die Person nochmals kontaktiert werden soll. Die gesammelten Daten werden anschliessend gelöscht.

Das Echo, das die neue Kampagne parteiintern ausgelöst habe, sei riesig gewesen, sagt der Zürcher Kantonalparteipräsident Hans-Jakob Boesch. In den drei Wahlbezirken, die man für den Pilotversuch erkoren habe, hätten sich neben vielen Ortsparteien über 80 Freiwillige gemeldet. Laut Boesch sollen die ersten Hausbesuche Ende Jahr stattfinden. Derzeit werden die Freiwilligen noch in speziellen Kursen für ihre neue Aufgabe geschult. Boesch erhofft sich von den Hausbesuchen vor allem eine bessere Mobilisierung und vertieftere Kontakte zur Bevölkerung. Im Vordergrund stehe klar der persönliche Kontakt mit der Stimmbevölkerung, sagt er. Die digitalisierte Systematik, die der Methode zugrunde liege, sei lediglich Mittel zum Zweck. Hat Boesch keine Angst vor Datenschutzverletzungen? Droht nach dem gläsernen Patienten nun der gläserne Wähler? Nein, sagt Boesch. Der Schutz persönlicher Daten sei ein urliberales Anliegen. Deshalb werde sich seine Partei peinlich genau an das Datenschutzgesetz halten. Für den Wahlkampf im Kanton Zürich bedeutet das konkret, dass die Wahlkämpfer jedes Mal das Einverständnis eines Befragten einholen müssen. Wer der FDP seine Mailadresse zur Verfügung stellen will, muss sein Einverständnis sogar schriftlich geben.

Um keine Fehler zu machen, werde die FDP das neue Wahlkampfkonzept noch dem eidgenössischen Datenschützer zur Beurteilung vorlegen, sagt Matthias Leitner, der den Wahlkampf vom Generalsekretariat der FDP in Bern aus koordiniert. Anschliessend werde man, wenn nötig, nochmals Anpassungen machen. Man sei aber bereits heute darum bemüht, mindestens die europäische Datenschutzverordnung umzusetzen, da mit dem neuen Schweizer Datenschutzgesetz eine ähnlich umfassende Regulierung erwartet werde.

Leitner ist überzeugt, dass die Get-out-the-vote-Methode nicht nur parteiintern mobilisiert, sondern der FDP dabei hilft, ihren Wähleranteil auszubauen. Als positive Beispiele nennt er die erfolgreichen Wahlkämpfe von Emanuel Macron, der österreichischen Partei Neues Österreich (Neos) oder auch die Erfolge, die die deutsche CDU letztes Jahr in einigen Bundesländern verbuchen konnte. Im Saarland etwa konnte die seit über 18 Jahren regierende Partei ihr Wahlergebnis um mehr als 5 Prozentpunkte steigern und kam schliesslich auf einen Wähleranteil von über 40 Prozent. Zuvor hatten Freiwillige jedem fünften Haushalt einen persönlichen Besuch abgestattet.

Ein Wundermittel ist der mit den Mitteln moderner Technik aufgerüstete Strassenwahlkampf jedoch nicht: In Hessen, wo die CDU über die vergangenen Monate ebenfalls an Türen klopfte und die Ergebnisse in einer eigens dafür entwickelten App mit dem Namen Connect 17 speicherte, sackte die Partei um 11 auf 27 Prozentpunkte ab. Es ist das schlechteste Ergebnis der Union in Hessen seit 1966. Mutmasslich hat die CDU auch in Hessen mehr Menschen direkt angesprochen als bei den letzten Landtagswahlen. Aber sie hat sie nicht überzeugt.";https://www.nzz.ch/schweiz/nun-setzen-auch-schweizer-parteien-im-wahlkampf-auf-big-data-ld.1431938;NZZ;Christina Neuhaus;;;
18.03.2020;Glokalisierung statt Globalisierung – der Corona-Krise entwachsen zwei mögliche Zukunftsszenarien: «Alle gegen alle» oder die «Wir-Weltgesellschaft»;"Wir erleben einen globalen Shutdown: Schulen werden geschlossen, das öffentliche und soziale Leben wird weitgehend eingestellt, die Kursverluste sind die grössten seit langem. Das Coronavirus hält die Welt in Atem und produziert existenzielle Unsicherheit, individuell wie kollektiv, überall und gleichzeitig. Zukunftsforscher nennen solche Katastrophen einen «schwarzen Schwan», ein nicht vorhergesehenes Ereignis. Die sozialen und ökonomischen Folgen sind derzeit kaum absehbar. Es fehlt uns die Erfahrung mit einer solchen Jahrhundertkrise. Erleben wir gerade das Ende der Globalisierung? Folgt jetzt die totale Isolation mit dem Szenario «Alle gegen alle»?
Der Shutdown als neue Normalität

Im negativen Szenario beschleunigt die Corona-Krise den Trend zur Deglobalisierung. Während die USA versuchen, deutsche Wissenschafter abzuwerben und potenzielle Impfstoffe aufzukaufen, droht China mit dem Lieferstopp für Schutzkleidung und Medikamente. Das Vertrauen in die globalen Lieferstrukturen ist nachhaltig erschüttert. Die Folge ist ein wachsender Neonationalismus. Immer mehr Unternehmen holen ihre Wertschöpfungs- und Lieferketten wieder zurück. Die Rückverlagerung betrifft vor allem sicherheitsrelevante Branchen wie Chemie, Automotive und Pharma.

    Deglobalisierung als Antiglobalisierung erweist sich als das falsche Rezept gegen Gesundheits- wie gegen Wirtschaftskrisen.

Die Abschottung nationaler Märkte und die Schliessung von Grenzen werden vor allem aus Gesundheitsgründen betrieben. Die Nachfrage nach Keim- und Virenfreiheit führt zu einem Verbot von Produkten, deren Herkunft sich nicht eindeutig nachvollziehen lässt. Lebensmittel werden vor dem Verkehr desinfiziert. Das soziale, kulturelle und öffentliche Leben bricht ein und wird in den virtuellen Raum verlagert. Gesundheitsdaten werden zur Staatsangelegenheit, der Datenschutz wird aus Gründen des Virenschutzes abgeschafft. Individuelle Bewegungsprofile erlauben ein ständiges Tracking und Verfolgen von infizierten Personen und ihre Isolierung in dafür vorgesehenen Regionen.

Auf nationaler Ebene führt die Deglobalisierung zu einer De-Urbanisierung und einer neuen Landflucht. Die Städte werden zu den nervösesten Plätzen der Welt. Der Trend zum Single-Leben und zu immer kleineren Wohnungen hat die Stadtbevölkerung unselbständig gemacht. Wer kann, zieht raus aufs Land und versorgt sich selbst. Ökonomisch setzt sich auf nationaler Ebene eine rigorose staatliche Planwirtschaft durch. Auf lokaler Ebene dagegen erfahren genossenschaftliche Selbstversorgermodelle eine Renaissance.
Resilient und robust

Das positive Szenario dagegen setzt auf eine Stärkung der Systeme hin zu mehr Resilienz und Robustheit. Die neue Pandemie zeigt vor allem eins: Nicht die Abschottung durch das Schliessen von Grenzen bremst die Ausbreitung des Virus, sondern konsequente lokale Massnahmen. Die Gesundheitssysteme kaufen sich wertvolle Zeit, indem sie sich nach innen abschotten und Schwache schützen. Deglobalisierung als Antiglobalisierung erweist sich als das falsche Rezept gegen Gesundheits- wie gegen Wirtschaftskrisen.

Abschottung bringt wenig, wenn die Pandemie längst unter uns ist. Protektionistische Massnahmen führen vor allem dazu, dass wichtige Medikamente und Schutzkleidung überall auf der Welt knapp werden und die Wissenschafter und Medizinunternehmen abgeworben werden. Die Welt würde ökonomisch und sozial in eine Abwärtsspirale getrieben, ohne dass die Epidemie gestoppt würde. Eine länger dauernde Wirtschaftskrise erhöht die Sterblichkeitsraten vor allem in den ärmeren Ländern.

Der schwedische Forscher und Bestsellerautor Hans Rosling hat in seinem letzten Buch, «Factfulness», auf beeindruckende Weise gezeigt, wie sehr Lebenserwartung und Wohlstand historisch zusammenhängen. Eine der Folgen der letzten Finanzkrise waren bis zu 500 000 zusätzliche Tote allein durch Krebserkrankungen. Aufgrund von Sparmassnahmen im Gesundheitssystem und unzureichendem Versicherungsschutz konnten viele Patienten nicht mehr ausreichend behandelt werden. Das Coronavirus führt zu einer neuen Kooperation, einem neuen Gleichgewicht der Staaten und ihrer Bundesländer und Kommunen.
Neue Formen der Vernetzung

Das positive Szenario einer resilienten globalen Gesellschaft setzt auf eine neue Synthese. Aus der Globalisierung wird etwas Drittes: die Glokalisierung. Eine Dezentralisierung von Märkten und Wertschöpfungsketten bei gleichzeitiger Intensivierung kooperativer Systeme.

Glokalisierung bedeutet die Entkopplung der Geschäftsmodelle von geografischen Räumen. Die technischen Instrumente sind längst vorhanden: Versammlungen und Sitzungen im Internet, Home-Office, Telemedizin, neue Formen der Mobilität. Frankreich hat damit begonnen, Sprechstunden von Ärzten per Whatsapp abzuhalten; in China ist die Nachfrage nach selbstfahrenden Autos in der Krise rapide gewachsen. Die digitale Vernetzung hält die Verbindung der Bürgerinnen und Bürger weltweit aufrecht. Mit Unterstützung ihrer mehr als 19 Millionen Fans hat die Instagram-Influencerin Chiara Ferragni in wenigen Tagen mehr als drei Millionen Euro per Crowdfunding für ein Krankenhaus in Mailand gesammelt. Auf lokalen Plattformen organisieren sich Nutzer und Nachbarn für Einkaufsdienste.

Die Corona-Krise kann am Ende auch zu einem neuen ganzheitlichen Gesundheitsverständnis führen. Gesundheit ist nicht nur eine individuelle, sondern auch eine öffentliche, gemeinsame Angelegenheit. Individuelle Gesundheit und Weltgesundheit sind zwei Seiten einer Medaille. Und die zunehmende Digitalisierung beschleunigt die Entwicklung. Die Zahl der Opfer bisheriger Pandemien war auch deshalb so hoch, weil die Gesellschaften und Nationen nur analog miteinander kommunizieren und nur langsam reagieren konnten.

Das öffentliche und kooperative Nutzen von Big Data und das Tracking von Personendaten hilft, Frühwarnsysteme zu entwickeln. Mithilfe von Predictive Health können genaue Vorhersagen über wahrscheinliche künftige Epidemien getroffen werden. Das ständige Lernen voneinander in supranationalen Netzwerken kann zu einem neuen, robusteren Gesundheitssystem führen, wenn Ärzte, Virologen und Pharmaunternehmen bei der Entwicklung von Impfstoffen weltweit zusammenarbeiten und sich ein gemeinsames Denken auf Ebene der Regierungen im Kampf gegen das Virus durchsetzt.
Corona kann die Zukunftsintelligenz erhöhen

Corona kann neuen Technologien, digitalen wie sozialen, zum Durchbruch verhelfen und so die Zukunftsintelligenz der Systeme erhöhen. Es geht um Innovationen wie digitale Infrastrukturen, kollaborative Plattformen und soziale Netzwerke, welche unser Leben insgesamt robuster machen. Glokalisierung bedeutet politisch eine neue Phase der Zusammenarbeit: den Ausbau und die bessere Kooperation lokaler wie supranationaler Institutionen. In den USA haben die Bundesstaaten und Städte den Schutz ihrer Bevölkerung übernommen und warten nicht auf Anweisungen der Trump-Regierung. Auch nach Corona wird es weitere Seuchen und Epidemien geben, die wir als Nebenfolgen der real existierenden Globalisierung nicht hinnehmen müssen, sondern präventiv durch einen globalen Seuchen- und Infektionsschutz gestalten können. Europa kommt dabei eine Schlüsselrolle zu.

Wir erleben in diesen Tagen nicht das Ende der Globalisierung, sondern etwas Neues, den Beginn der Glokalisierung. Die globale und lokale Zivilgesellschaft organisiert sich in diesen Tagen neu. Abschottung und Isolation führen nicht zu einer besseren Zukunft. Es geht vielmehr um beides: die Stärkung lokaler und regionaler Strukturen sowie globaler Systeme und ihrer Institutionen. Auf die schnelle Hyperglobalisierung kann eine langsamere, achtsame Glokalisierung folgen. Ihre Wirtschaft und Gesellschaft ist resilienter und robuster als die heutige.";https://www.nzz.ch/meinung/glokalisierung-statt-globalisierung-der-corona-krise-entwachsen-zwei-moegliche-zukunftsszenarien-alle-gegen-alle-oder-die-wir-weltgesellschaft-ld.1546705;NZZ;Daniel Dettling;;;
05.09.2018;Dateneigentum: Eckstein der kommenden Digitalordnung;"Big Data bestimmt das Geschehen in der Zivilgesellschaft zunehmend. Und es besteht deswegen die Gefahr, dass unsere Freiheit schleichend schrumpft, die zivile Sphäre nach eigenen Anschauungen zu gestalten. Es stellt sich somit in der digitalen Gesellschaft die Frage nach der Einführung von Eigentumsrechten an Daten. Eigentum erschöpft sich dabei nicht darin, die dem Berechtigten zugeordneten Sachen gegen Einwirkungen Dritter (Sachentziehung, -vorenthaltung, -beschädigung usw.) zu schützen; es erlaubt dem Berechtigten ebenfalls, seine zivile Sphäre nach seinen eigenen Präferenzen auszugestalten.
Algorithmische Gouvernementalität

Weil Big Data in der Hauptsache aus im Web getrackten User-Daten besteht und wir als Mitglieder der Zivilgesellschaft fast alle User sind, ist anzunehmen, dass Eigentum an User-Daten besagte Freiheit wiederherstellen kann. Diese Hypothese baut auf der Tatsache auf, dass die Digitaltechnologie die erste Kommunikationsart der Geschichte ist, die durch die menschlichen Sinne nicht wahrnehmbar ist. Der Medientheoretiker F. Kittler hat nachgewiesen, dass die Computersprache (d. h. der Binärcode) Kommunikation und Information entkoppelt. Grund dafür ist, dass unser Gehirn den Sinn kolossaler Reihen von 0 und 1 (Kommunikation) nicht erfassen kann; erst das Decodieren dieser binarischen Reihen vermittelt uns deren Bedeutung (Information). Diese Entkoppelung entzieht den Usern die Möglichkeit, zu überwachen, was mit ihren Daten im Internet passiert. Diese Konsequenz des Digitalen ist epochemachend: Sie ist es, die Big-Data-Firmen überhaupt erst in die Lage versetzt, im Web unbemerkt User-Daten en masse zu erheben. Sie ist es, die diesen Firmen Data-Mining erlaubt, d. h., Massendaten algorithmisch zu durchforsten, um das Verhalten von kategorialen Menschenmassen (z. B. «die» Mieter oder «die» Kreditnehmer) vorherzusagen (predictive analytics). Sie ist es, die eine vor ungefähr zwanzig Jahren noch völlig unbekannte Technologie der Sozialkontrolle ermöglichte: die sogenannte algorithmische Gouvernementalität. Diese wirkt in einer für den Einzelnen kaum wahrnehmbaren Weise: Sie taucht Menschenmassen in ein Milieu (d. h. in eine soziale Umwelt) ein, das zuvor künstlich konditioniert worden ist (prescriptive analytics). Auf dieses Milieu reagieren die Massen mit Verhaltensmustern, die sie in die Richtung bewegen, die die Verwender von Big-Data-Technologien anstreben.
Zivilgesellschaftliches Dateneigentum

Wie wirkt Dateneigentum gegen algorithmische Gouvernementalität? Wie restauriert es die zivile Freiheit? Es setzt direkt an der Quelle an: Indem es die Daten demjenigen User zuweist, der sie tatsächlich produziert hat (z. B. durch Senden einer E-Mail), hören diese Daten auf, herrenlose Gegenstände zu sein. Entsprechend wird der faktische Zustand des heutigen Cyberspace beendet, in welchem sich jedermann die in den Digitalnetzen zirkulierenden Daten frei aneignen kann. Mit der Einführung von Dateneigentum wird der Cyberspace radikal transformiert. Denn dieses Rechtsinstitut verwehrt Digitalfirmen, Data-Mining ohne vorherige vertragliche Vereinbarung mit den Dateneigentümern zu betreiben.

Wird diese Regel nicht beachtet, liegt eine Dateneigentumsverletzung vor, die mit gerichtlicher Klage beseitigt werden kann. Digitalnetze als zivilgesellschaftliche Handlungsfelder werden so der totalitären Datenaneignungsstrategie namentlich der Big Five (Google u. a.) entzogen und in die Gestaltungszuständigkeit der dateneigentumsrechtlich berechtigten User übergeführt. Das bisher rechtsfreie Web würde mit der Einführung von Dateneigentum seinen zweiten evolutionären Schub erfahren (der erste war die Revolution der digitalen Netz-Vernetzungstechnologien).

Allerdings: Wie Rechte aus Dateneigentum in der Praxis tatsächlich durchgesetzt werden, ist noch unklar. Es ist nicht zu erwarten, dass die Dateneigentümer ihre Ansprüche selber und individuell in einem Ausmass wahrnehmen werden, das ausreicht, um die Policy des Dateneigentums zu verwirklichen. Nur eine kollektive Rechtsdurchsetzung ist realistisch. Deshalb wird zurzeit erwogen, ob, ähnlich wie im Urheberrecht, die Rechtewahrnehmung einer privat- oder öffentlichrechtlichen Verwertungsorganisation anvertraut werden soll.
";https://www.nzz.ch/meinung/dateneigentum-eckstein-der-kommenden-digitalordnung-ld.1415565;NZZ;Marc Amstutz;;;
03.07.2020;Ein Datenschatz in der Medizin liegt brach;"Barbara Biedermann ist Hausärztin in Adetswil bei Zürich und hat eine Vision. Sie will die Medizin besser machen. Nicht mit einer neuen Therapie, sondern indem sie die klassische Patientenuntersuchung mit moderner IT kombiniert. «Damit sollten sich die Qualität der Diagnosen verbessern und viele unnötige Untersuchungen verhindern lassen», sagt Biedermann. Der Ansatz habe also auch das Potenzial, die Medizin günstiger zu machen.

Was nach der Quadratur des Kreises klingt, ist ein Computerprogramm namens Cobedias. Das Akronym steht für «Comprehensive Bedside Diagnosis», zu Deutsch: umfassende Diagnose am Patientenbett. Die Software ist das Steckenpferd von Biedermann, die neben ihrer Praxis noch eine Titularprofessur an der Universität Basel innehat.
Alles begann mit einer Nationalfonds-Studie

Begonnen hat alles vor rund zwanzig Jahren im Bruderholzspital im Kanton Baselland. Im Rahmen eines vom Nationalfonds finanzierten Forschungsprojekts konnte Biedermann zeigen, dass der Algorithmus der Software bei einer Modellkrankheit (Arteriosklerose) funktioniert. Das heisst, mit den «am Krankenbett» gewonnenen Informationen liess sich beim Patienten zuverlässig die Krankheit bestimmen. Beflügelt von diesem Erfolg, entwickelte die Ärztin ihre Idee mit einer IT-Firma in ein marktreifes Produkt für alle Krankheiten weiter. So ist die Cobedias-Software entstanden. Sie ist seit 2014 verfügbar und wird ständig weiterentwickelt. Das System hilft Ärzten, ihre Patienten korrekt und vor allem vollständig zu untersuchen. Zudem werden die gewonnenen Informationen – oder Daten – in digitaler Form dokumentiert. So kann eine Datenbank mit klinischen Patienteninformationen aufgebaut werden. «Etwas, das es in dieser Form bis heute in der Schweiz nicht gibt», sagt Biedermann. Das ist erstaunlich, wird doch in der Medizin ständig von Big Data gesprochen. «Dabei geht es um Labordaten, Gentests oder Daten aus bildgebenden Untersuchungen», erklärt die Ärztin. «Nicht um klinische Patientendaten.»

Die klinischen Patientendaten stammen aus dem Gespräch, das der Arzt mit seinem Patienten führt (Anamnese), und der anschliessenden Ganzkörperuntersuchung (Status). In der Anamnese wird nicht nur das gegenwärtige Problem des Patienten besprochen. Es kommen auch frühere Krankheiten und Operationen, familiär gehäufte Störungen sowie Allergien, Suchtverhalten und regelmässig eingenommene Medikamente zur Sprache. Bei der Untersuchung des Körpers muss der Arzt all seine Sinne einsetzen, indem er zum Beispiel die durch Krankheiten veränderte Hautfarbe beurteilt (Inspektion), innere Organe abtastet (Palpation), Bauch und Brustraum abklopft (Perkussion), Herz und Lunge abhört (Auskultation) und verschiedene Funktionsprüfungen vornimmt.

Auf diese Weise kommen im Cobedias-System pro Patient rund 7000 Informationspunkte zusammen, wobei nur knapp 300 aktiv dokumentiert werden müssen (die anderen sind vorgegebene Normalbefunde). Die so entstandenen Datensätze lassen sich in anonymisierter und aggregierter Form auf die unterschiedlichsten medizinischen Fragestellungen hin auswerten. «Davon können Patienten, Ärzte und die Gesellschaft gleichermassen profitieren», sagt Biedermann.
Informationsbasis für ärztliche Entscheide

Dabei ist die systematische Patientenuntersuchung keine Erfindung der Adetswiler Hausärztin. Schon seit den Anfängen der modernen Medizin liefern die Anamnese und der Status den Ärzten die Basis, um beim Patienten über weiterführende diagnostische Abklärungen und seine Behandlung zu entscheiden. Dass diese Basis auch in Zeiten der hochtechnisierten Spitzenmedizin immer noch unabdingbar ist, zeigt sich darin, dass sie im Studium immer noch gelehrt wird. Zudem belegen Studien, dass erfahrene Hausärzte alleine mit Anamnese und Status bei 80 bis 90 Prozent ihrer Patienten die zugrunde liegende Krankheit diagnostizieren oder zumindest mit hoher Wahrscheinlichkeit vermuten können.

«Das hat damit zu tun, dass der menschliche Körper seit Jahrhunderten auf ähnliche Weise krank wird», erklärt Biedermann. Die für eine Krankheit typischen Veränderungen zu erkennen und richtig zu deuten, sei somit für die ärztliche Arbeit zentral. Biedermann spricht in diesem Zusammenhang vom klinischen Phänotyp einer Krankheit – dies im Gegensatz zur genetischen Grundlage vieler Störungen (Genotyp).

«Den klinischen Phänotyp möglichst rasch zu kennen, wäre auch bei einer neuen Krankheit wie Covid-19 sehr hilfreich», betont Biedermann. Dafür brauche es aber verlässliche Daten aus den Spitälern und den Arztpraxen. Gerade die Hausärzte seien oft die Ersten, die einen Patienten sähen. Weil die meisten Mediziner aber keine systematischen, sondern nur problemzentrierte Untersuchungen durchführen – im Fall von Fieber und Husten fokussiert man sich beispielsweise auf die Atemwege –, hat es laut Biedermann in der gegenwärtigen Pandemie relativ lange gedauert, bis man realisierte, dass viele Covid-19-Patienten an einer Riechstörung leiden.
Das Gesundheitssystem tickt anders

Der Ansatz der Adetswiler Hausärztin klingt so logisch und nachvollziehbar, dass man sich fragt, warum sich das Cobedias-System oder etwas Ähnliches nicht schon längst in der Medizin durchgesetzt hat. Tatsächlich wird die Software, die als Jahreslizenz für drei Ärzte 1620 Franken kostet, erst in zwei Arztpraxen eingesetzt. Für den fehlenden Erfolg sieht Biedermann zwei Hauptgründe. «Mit rund einer Stunde Zeitaufwand dauert die systematische Untersuchung und Dokumentation vielen Ärzten zu lange», sagt sie. Das habe auch damit zu tun, dass das Tarifsystem Tarmed den zeitlichen Aufwand für technische Untersuchungen wie Ultraschall oder Labortests deutlich besser honoriere (vgl. Kasten ganz unten). Ein weiteres Hindernis sei die schwierige Integration der Software in die bestehenden Praxis- und Spitalinformationssysteme. So habe eine Offerte für das Spital Wetzikon ergeben, dass die Anpassungen am Spitalinformationssystem dreimal so teuer wären wie das Cobedias-System. Das sei dem Spitalrat zu viel gewesen.

Müsste Biedermanns Idee einer Datenbank für klinische Patientendaten nicht bei der Schweizerischen Gesellschaft für allgemeine innere Medizin (SGAIM) und den universitären Instituten für Hausarztmedizin auf grosses Interesse stossen? Warum leistet niemand Schützenhilfe?

Der Internist und SGAIM-Co-Präsident Drahomir Aujesky findet Biedermanns Ansatz im Grundsatz richtig. «Ich bin überzeugt, wir könnten eine bessere Medizin machen, wenn wir mehr Zeit in das Gespräch und die klinische Untersuchung investieren würden», sagt der Chefarzt der Universitätsklinik für allgemeine innere Medizin am Inselspital Bern. Doch das Gesundheitssystem laufe dem zuwider. Neben der ungenügenden Vergütung für Anamnese und Status sieht der Arzt auch den enormen Zeitdruck und vor allem den zu geringen Stellenwert des ganzheitlichen klinischen Ansatzes in der Medizin als wichtige Hemmfaktoren.

In seiner eigenen Klinik versuche er mit entsprechenden fachlichen Weiterbildungen Gegensteuer zu geben, sagt Aujesky. Neben solchen lokalen Initiativen gebe es in der Schweiz aber keine übergeordneten Bemühungen zur Wiederbelebung von Anamnese und Status. «Das ist auch deshalb schade, weil die heutige fragmentierte Medizin viele Redundanzen und Fehlinformationen produziert», sagt der Medizin-Professor. Ein Big-Data-Ansatz ohne klinische Patientendaten sei deshalb zum Scheitern verurteilt. Andererseits dürfe man die klinische Untersuchung aber auch nicht romantisieren, warnt Aujesky. Viele klassische Tests seien nicht besonders aussagekräftig. Ihren Nutzen gelte es in Studien kritisch zu überprüfen. Dafür brauche es aber auch verlässliche klinische Patientendaten.

Aus eigener Erfahrung weiss Aujesky, dass die meisten Patienten eine ausführliche klinische Untersuchung schätzen. Wenn etwas in der Betreuung schieflaufe, höre er oft den Satz: «Doktor X hat mich nicht einmal richtig untersucht.» Das sei schade, sagt Aujesky. Denn die persönliche Zuwendung sei Teil des medizinischen Erfolgs. Damit werde das Vertrauensverhältnis zwischen Patient und Arzt gestärkt.
«Moderne Medizin hängt Patienten ab»

Dieser Ansicht ist auch Thomas Rosemann vom Institut für Hausarztmedizin der Universität Zürich. Die moderne Medizin hänge viele Patienten ab, sagt er. «Ein Grund ist, dass das Gespräch und die klassische Untersuchung in der Medizin an Bedeutung verlieren.» Dieser Trend habe aber auch rationale Gründe, betont der Hausarzt-Professor. Statt den Bauch lange mit dem Stethoskop abzuhören, könne eine kurze Ultraschalluntersuchung viel präzisere Informationen liefern. Auch die Auskultation des Herzens stosse rasch an Grenzen.

Von einer Software wie Cobedias würde Rosemann erwarten, dass sie den Arzt bei der Entscheidungsfindung unterstütze. Ein solches intelligentes System würde zum Beispiel beim Symptom Atemnot eine Warnlampe einschalten und dem Mediziner «sagen», mit welchen weiteren Fragen an den Patienten und mit welchen Untersuchungen er am besten einen Herzinfarkt, eine Lungenembolie oder anderes nachweisen oder ausschliessen könne.

Solche Hilfsmittel, die anhand von Punktwerten (Scores) die Wahrscheinlichkeit einer vermuteten Diagnose angeben, gibt es zwar schon. Meist liegen sie aber als einfache Fragebögen vor. «In die elektronische Krankengeschichte integriert, könnten sie dem Praktiker einen echten Mehrwert bieten», sagt Rosemann. Doch so weit sei man noch lange nicht. Das habe auch mit der Abneigung vieler Ärzte gegen verordnete Strukturen und Standardisierungsbemühungen zu tun. Eine gewisse Vereinheitlichung sei jedoch eine Voraussetzung, wenn man wie Barbara Bidermann klinische Daten sammeln wolle.
Kein Goldstandard für gute Medizin

«Für eine systematische und vollständige klinische Untersuchung fehlt uns schlicht die Zeit», sagt Felix Huber, Hausarzt und Mitgründer des Praxisnetzwerks Medix. Der erfahrene Mediziner bezweifelt zudem, dass Biedermanns Ansatz gegen medizinische Überversorgung hilft. Das Gegenteil könnte eintreffen, warnt er. Denn beim rigiden Abarbeiten von vorgeschriebenen Untersuchungen und Tests könnten auch viele Befunde ohne klinische Relevanz erhoben werden.

Gleichzeitig räumt der Medix-Pionier aber ein, dass die Arbeit in der Praxis immer eine Gratwanderung sei. «Wenn wir wie bei der problemzentrierten Untersuchung Abkürzungen nehmen, besteht die Gefahr, dass wir etwas verpassen.» Bevor man aber alle Hausärzte verpflichte, systematisch klinische Daten zu sammeln, müsse man den Ansatz in einer Studie mit freiwillig teilnehmenden Ärzten sauber evaluieren. «Ohne das fehlt der Beweis, dass sich damit eine bessere und kostengünstigere Medizin realisieren lässt.»

Die Frage nach dem Nutzen sei umso schwieriger zu beantworten, als es keinen anerkannten Goldstandard für gute Medizin gebe, erklärt Huber. «Macht der alte, erfahrene Arzt die beste Medizin?» Schliesslich wisse dieser oft schon anhand von ein paar Fragen, in welche Richtung es beim Patienten gehe. «Oder ist die junge Ärztin besser?» Ihr medizinisches Wissen sei meist riesig. Doch wegen der fehlenden Erfahrung veranlasse sie oft viele Zusatzuntersuchungen.

Der Hausarzt-Professor Rosemann spricht in diesem Zusammenhang von den zwei Seiten der Medizin. «Die eine Seite ist das wissenschaftliche Fundament», sagt er. Dafür wären mehr klinische Patientendaten sinnvoll. Auf der anderen Seite seien Ärzte aber auch Individualisten mit je eigenem Arbeits- und Kommunikationsstil. Und das sei gut so, sagt Rosemann. Denn diese Unterschiede seien wichtig, weil auch die Patienten verschiedene Vorlieben und Ansprüche an die Ärzte hätten.";https://www.nzz.ch/wissenschaft/patientendaten-ein-datenschatz-liegt-in-der-medizin-brach-ld.1563785;NZZ;Alan Niederer;;;
20.09.2019;Der perfekte Albtraum – wenn Überwachungskapitalismus und Überwachungsstaat zusammenwachsen;"Im Herbst des Jahres 2000 veranstaltete der deutsche Auslandsgeheimdienst eine Tagung zum Thema «information warfare». Der BND reagierte damit auf die Instrumentalisierung des Internets im Kosovo-Krieg. Das Internet wurde damals erstmals zur Waffe, und zwar in zweierlei Hinsicht: buchstäblich, indem Serbien und das mit ihm verbündete Russland versuchten, mit gezielten Cyberangriffen Rechner der Nato lahmzulegen. Und in übertragener Hinsicht, indem Serbien Falschinformationen zum Kriegsverlauf veröffentlichte.

Beides, Cyberattacken und Cyberpropaganda, steckten damals noch in den Kinderschuhen. Social Media waren noch nicht erfunden. Die Angriffe waren plump und zielten auf die Überlastung der Rechner. Deshalb geriet das Internet als Waffe in der westlichen Öffentlichkeit schnell wieder in Vergessenheit. Es war die Zeit der digitalen Euphorie, in der viele glaubten, mit dem Internet breche eine neue Ära der Freiheit an.

Im Kreml hingegen glaubte das niemand. Dort sah und sieht man das Internet immer als Waffe. Im amerikanischen Präsidentschaftswahlkampf 2016 drang eine Hackergruppe, mutmasslich auf Geheiss des russischen Militärgeheimdienstes, in die Computer der demokratischen Parteizentrale ein und kopierte zahllose Mails, die führende Mitarbeiter von Hillary Clinton in ein schlechtes Licht rückten.
Putin will Verwirrung stiften

Wikileaks diente wissentlich oder unwissentlich als fünfte Kolonne des russischen Geheimdienstes und publizierte die Mails, was dem Wahlkampf Clintons vielleicht nicht den entscheidenden, aber gewiss einen Stoss versetzte. Im Kalten Krieg hätte man Wikileaks noch einen Einflussagenten Moskaus genannt.

Zugleich nutzte der Kreml das Internet als Propagandainstrument. Fake-Accounts, also Social-Media-Konten, hinter denen keine realen Personen stehen, veröffentlichten Hunderttausende von Posts, in denen Hillary Clinton diffamiert wurde. Gleichzeitig versuchten die russischen Internet-Trolle, Verwirrung unter republikanischen Wählern zu stiften. Es ging also in erster Linie nicht darum, Propaganda für oder gegen eine Partei zu machen. Das Ziel war es, die amerikanische Demokratie an sich anzugreifen.

Wir lernen daraus etwas ganz Triviales. Nur weil wir, die westlichen Gesellschaften, eine Technik unter bestimmten Aspekten sehen, heisst dies noch lange nicht, dass andere diese Perspektive teilen. Unterschiedliche Gesellschaftssysteme setzen eine Technologie für sehr verschiedene Zwecke ein. Das hat direkte Auswirkungen auf uns, denn im Internet ist alles mit allem verbunden.

    «Der Totalitarismus schien 1989 überwunden. Nun kehrt er als technologischer Totalitarismus zurück.»

Russland betrachtet das Internet nicht als Mittel zur informationellen Selbstbestimmung, sondern als Mittel zur Propaganda. China sieht in der künstlichen Intelligenz (KI) zwar auch ein Instrument der wirtschaftlichen Modernisierung, aber mindestens ebenso ein Instrument der sozialen Kontrolle.

Welche Auswirkungen haben die modernen Informationstechnologien auf die Freiheit? Für Liberale muss dies, in der ersten Hälfte des 21. Jahrhunderts, eines der zentralen Themen sein.

Das Smartphone ist längst mit seinem Träger zu einem neuartigen Wesen verschmolzen. Gemäss der jüdischen Legende schuf der Prager Rabbi Judah Löw um 1580 aus Lehm ein menschenähnliches Wesen mit gewaltiger Kraft und Grösse – auf Hebräisch einen Golem. Der moderne Golem sind wir mit unseren Smartphones, die uns früher kaum vorstellbare Möglichkeiten bieten. Das Einzige, was dieses Ding nicht zu können scheint, ist in dem legendären Dialog in «Raumschiff Enterprise» beschrieben: «Beam me up, Scotty!» Die Technik prägt unser Leben und vor allem unser Bewusstsein intensiver als jemals zuvor.
Haben wir noch eine Privatsphäre?

Ich werde mich nicht zu Spekulationen darüber versteigen, ob die künstliche bald die menschliche Intelligenz überholt, so wie dies Ray Kurzweil unter dem Schlagwort der Singularität prognostiziert. Es geht nur um eine simple, aber hochaktuelle Frage: Welchen Raum gibt der Mensch der Technik in seinem Leben? Was bedeutet heute Privatsphäre? Haben wir das Recht, bei Bonitätsprüfungen oder Einstellungen nicht allein von selbstlernenden Algorithmen beurteilt zu werden?

Es geht auch darum, was Technik mit Staaten und Demokratien macht. Wenn Staaten beispielsweise zum Ziel von Cybermanipulationen werden oder IT-Unternehmen die Rolle von Regierungen usurpieren.

Natürlich hatte Technik zu allen Zeiten gesellschaftliche Auswirkungen. Aber die Informationstechnologie verändert unser Mensch-Sein, indem wir zu Techno-Golems mutieren. Und ich meine nur die Symbiose zwischen Mensch und Handy. Ich rede nicht von Mensch-Maschine-Mischwesen, von Cyborgs, von denen Elon Musk phantasiert. Die Informationstechnologie hat eine vergleichbare Qualität wie die Gentechnik, die ebenfalls direkt auf unser Mensch-Sein einwirkt.

Natürlich gab es schon früher Propaganda und Desinformation. Noch nie waren die Menschen ihr allerdings so lückenlos ausgesetzt wie heute, wo sie vom Aufstehen bis zum Zubettgehen beständig auf den kleinen leuchtenden Bildschirm schauen. Natürlich gab es schon früher soziale Kontrolle. Noch nie haben die Menschen das Überwachungsgerät allerdings so begeistert, so konstant und vor allem völlig freiwillig mit sich herumgetragen wie heute.

Träumten Stalin und Mao noch davon, die Gedanken ihrer Untertanen auszuspähen, so ist dies für ihre Nachfahren Realität. Der Totalitarismus schien 1989 überwunden. Nun kehrt er als technologischer Totalitarismus zurück.

Zugleich gibt es eine neue geo-ideologische Konfrontation. Im Kalten Krieg rangen der Kommunismus und die marktwirtschaftliche, pluralistische Demokratie um die Vorherrschaft. Nach dem Fall der Berliner Mauer glaubte der Westen an das «Ende der Geschichte». Liberalismus und Marktwirtschaft schienen alternativlos zu sein. Tatsächlich nahm nach dem Untergang von Ostblock und Sowjetunion die Zahl der Staaten rasch zu, die wenigstens auf dem Papier Demokratien waren. Doch das Pendel schwingt zurück. Die westliche Selbstgewissheit ist ängstlicher Nabelschau gewichen.

Die naive Erwartung, freies Wirtschaften führe automatisch zu freien Gesellschaften, hat sich verflüchtigt. Russland und China sind autoritäre Regime und zugleich kapitalistische Systeme. Weder Russland noch China haben in den letzten drei Jahrzehnten einen Schritt in Richtung Demokratisierung unternommen. Im Gegenteil: Unter Xi Jinping nehmen Dogmatismus und Nationalismus wieder zu. Peking füllt nur zu gern die Lücke, die Amerikas Rückzug aus internationalen Organisationen hinterlässt. China versucht, in Uno-Gremien den Ton anzugeben, und afrikanische und andere Potentaten stimmen oft für chinesische Anliegen.
Information ist eine Waffe

Der technologische Totalitarismus und der geo-ideologische Wettstreit bilden ein Amalgam, das ich anhand von vier Problemfeldern beschreiben will: Informationen als Waffe, amerikanischer Überwachungskapitalismus, chinesischer Überwachungsstaat und Krieg um Standards und Werte.

1. «Information warfare»: Wenn wir von Krieg reden, denken wir an Tote, Verletzte und grossflächige Zerstörungen. Das gibt es weiterhin, doch die Kriegführung setzt heute mehrere Stufen weiter unten an: bei der Beeinflussung der Öffentlichkeit oder dem Eindringen in gegnerische Rechner zum Zweck der Spionage oder der Zerstörung von strategisch wichtiger Infrastruktur.

Der «information warfare» lässt sich mit anderen niederschwelligen Kriegsformen kombinieren, zum Beispiel der Besetzung fremden Territoriums durch Söldner wie in der Ostukraine. Hierfür hat sich der Begriff hybride Kriegsführung eingebürgert. Wer glaubt, das betreffe nur die Ukraine, irrt. Es tangiert genauso die angeblich neutrale Schweiz. Im September 2016 bezogen zwei Agenten des russischen Geheimdienstes Posten in Lausanne. Ihr Auftrag: Sie sollten eine Konferenz der Welt-Antidoping-Agentur (Wada) ausspionieren. Spionage hat heute nichts mehr mit James Bond und schönen Frauen zu tun, sondern mit Computer-Nerds und Hackern. Über das WLAN eines Hotels gelangten die Agenten in die Rechner der Konferenzteilnehmer.

Zwei Jahre später wurde der übergelaufene russische Geheimdienstmann Sergei Skripal in Grossbritannien Opfer eines Giftanschlags. Die beiden Agenten hatten in Lausanne ganze Arbeit geleistet. Jetzt sollten sie die Spuren des Attentats auf Skripal verwischen, die nach Moskau führten. Dazu versuchten sie, die Organisation für das Verbot chemischer Waffen in Den Haag auszuspionieren. Diese Organisation sollte den britischen Befund überprüfen, wonach Skripal mit dem sowjetischen Kampfstoff Nowitschok vergiftet wurde. Die notwendigen Tests übernahm ein Schweizer Labor, das zu den führenden Einrichtungen seiner Art gehört. Das Agentenduo hatte deshalb schon Zugtickets gekauft, um von Den Haag in die Schweiz zu reisen – vermutlich, um in die Rechner in Spiez einzudringen. Allerdings wurden die zwei noch in Den Haag verhaftet.
Big Brother wohnt jetzt im Silicon Valley

Für die Schweizer war der Agentenkrimi damit nicht vorbei. Der russische Aussenminister Lawrow behauptete, das Labor habe den Beweis dafür gefunden, dass nicht ein sowjetischer, sondern ein westlicher Kampfstoff Sergei Skripal vergiftet habe. Das waren Fake-News, klassische russische «desinformazija». Diese beherrschte aber tagelang die Medien.

Nach dem Hack in Lausanne war die neutrale Schweiz zum zweiten Mal in die russischen Machtspiele hineingezogen worden. Hier ist Freiheit in einem elementaren Sinn bedroht, weil staatliche Souveränität und die Integrität wichtiger nationaler Einrichtungen attackiert wurden.

2. Der Überwachungskapitalismus, wie ihn die Technologiefirmen im Silicon Valley praktizieren. Die Tendenzen zum technologischen Totalitarismus sind keineswegs auf autoritäre Regime beschränkt. Rein wirtschaftlich orientiertes Denken kann ihnen ebenfalls Vorschub leisten. Google, Facebook und andere Unternehmen gingen um die Jahrtausendwende mit einem simplen Versprechen an den Start: Die Konsumenten erhalten gratis Dienstleistungen und zahlen dafür mit Daten. Doch so einfach und überschaubar, wie es klingt, sind die Transaktionen in dieser digitalen Welt nicht.

Bei den Tech-Giganten ist es nahezu unmöglich, herauszufinden, mit welchen Daten ich registriert bin. Das liegt zum einen an der Vielzahl der angebotenen Dienste. Google ist nicht nur eine Suchmaschine, sondern auch eine Weltkarte, ein Videoportal, ein Übersetzungsbüro, ein Textverarbeitungssystem, ein Mail-Programm, eine Agenda und ein Streaming-Dienst.

Facebook ist eben nicht nur Facebook, sondern besitzt auch Instagram und Whatsapp, also die neben Facebook erfolgreichsten sozialen Plattformen. Facebook sammelt nicht nur von seinen Nutzern Daten, sondern auch von Personen, die in den Notizbüchern der Nutzer gespeichert sind, oder von Menschen, die ohne jede direkte Beziehung zu Facebook den Like-Button auf einer x-beliebigen Homepage drücken.
Wem vertrauen Sie eher – der Regierung oder Google?

Apple, Google oder Amazon belauschen mit der Spracheingabe-Funktion und Geräten wie «Alexa» ihre Nutzer. Die dabei gesammelten Daten werden genauso ausgewertet wie andere Daten auch. Auch wenn man keinen dieser Spione bei sich im Schlafzimmer oder Wohnzimmer aufgestellt hat, kann man in den Räumen anderer abgehört werden. Auf die Anonymität sollte man sich dabei besser nicht verlassen. So wie die Bilderkennung schon jetzt Gesichter Personen zuordnen kann, vermag das die Spracherkennung auch. Die Auswertung ist nur davon abhängig, wie viele Audio-Files mit identifizierten Personen verfügbar sind.

Wer behauptet, zu überblicken, welche Daten über ihn gespeichert sind, lebt in einer Traumwelt. Niemand weiss das besser als die IT-Unternehmen. Sie versuchten daher schon früh, unser Verständnis von der digitalen Welt zu verändern. So behauptete Mark Zuckerberg, Privatsphäre sei keine soziale Norm mehr. Der Google-Gründer Larry Page ging noch einen Schritt weiter. Er sagte, wenn der Mensch schon gläsern sei, seien seine Daten besser bei Google aufgehoben als bei demokratisch kontrollierten Institutionen: «Google ist sein Ruf wichtig. Bei Regierungen ist das vielleicht nicht in gleichem Ausmass so.»

Hier spricht die Hybris des Prometheus. Er bringt den Menschen das Feuer, also Big Data und künstliche Intelligenz. Und Prometheus glaubt, besser zu wissen, was für die Menschen gut ist, als diese selbst.

    «Übernehmen die chinesischen Konzerne in der künstlichen Intelligenz die Führungsrolle, gehört etwas der Geschichte an, was der Westen seit dem Ende des Mittelalters als selbstverständlich betrachtet: seine wissenschaftliche Vormachtstellung.»

Im letzten Jahr wurde ruchbar, dass die Firma Cambridge Analytica auf der Basis von Facebook-Daten politische Benutzerprofile erstellte. Facebook hatte anderen Unternehmen Zugang zu seinen Daten gewährt, um den Vorwurf zu entkräften, es sichere sich als Quasimonopolist das exklusive Verfügungsrecht über seine Datenbestände. Cambridge Analytica analysierte Facebook-Profile mit allen Kommentaren, Likes und geteilten Inhalten. Die Firma konnte so vorhersagen, welche Partei der jeweilige Nutzer wählt, welche sexuelle Orientierung er hat, was er über Rassentrennung denkt.

Big Brother – nicht zum Zweck der politischen Überwachung, sondern zur Gewinnmaximierung. Denn Facebook betreibt den Aufwand nur, weil es den Grossteil seines Umsatzes von 56 Milliarden Dollar mit präzise auf die Bedürfnisse der User zugeschnittener Werbung verdient.

3. Der moderne Überwachungsstaat: Während im Silicon Valley die Überwachung nur Mittel zum Zweck ist, ist sie im Überwachungsstaat das eigentliche Ziel. Umso besser, wenn sich wie in China politische und wirtschaftliche Ambitionen ergänzen. China will bis zum Jahr 2025 zur dominanten Macht in der künstlichen Intelligenz aufsteigen. Der IT-Unternehmer Kai-Fu Lee glaubt, dass China dies gelingen wird. Die Grundlagenforschung sei weit fortgeschritten, jetzt komme es auf anwendungsorientierte Forschung, einen grossen Binnenmarkt und vor allem riesige Datenmengen an. China erfüllt alle Bedingungen.

Übernehmen die chinesischen Konzerne in der KI die Führungsrolle, gehört etwas der Geschichte an, was der Westen seit dem Ende des Mittelalters als selbstverständlich betrachtet: seine wissenschaftliche Vormachtstellung. Die Beherrschung der Informationstechnologie wird darüber entscheiden, welche Staaten im 21. Jahrhundert dominieren. Der CEO von Google, Sundar Pichai, nennt die KI noch wichtiger als die Entdeckung von Feuer und Elektrizität. Der Verweis auf Prometheus ist also berechtigt.
Dein Gesicht und deine Stimme verraten dich

China hat bereits das weltweit grösste Netz an Videokameras im öffentlichen Raum und will es bis 2020 auf 625 Millionen Kameras ausbauen. KI erkennt nicht nur Gesichter, sondern wertet auch Geschlecht, Grösse und Kleidung der Passanten aus. In der Provinz Anhui läuft ein Pilotprogramm, um die Stimmen von Mobilfunkteilnehmern auszuwerten. Noch sind erst einige zehntausend Stimmproben hinterlegt, aber die Behörden sind zuversichtlich, bald automatisch alle Stimmen in Telefongesprächen erkennen zu können. Da ist es auch kein weiter Weg mehr, bis Amazons Lautsprecher «Alexa» das kann.

Und das ist der eigentlich furchterregende Gedanke: dass der Überwachungskapitalismus und der Überwachungsstaat zusammenwachsen könnten. Das ist dann tatsächlich eine neue Singularität – aber ganz anders, als Ray Kurzweil sie sich vorstellt.

Berüchtigt ist das Programm zum Aufbau eines Social-Scoring-Systems in China. Strafregisterauszug, Kreditkartenbonität, das von einer Videokamera aufgenommene Betreten der Fahrbahn bei Rot, im Handy-Gespräch geäusserte politische Ansichten: All das wird zusammengeführt werden und die Bewertung von jedem der 1,4 Milliarden Chinesen erlauben. Mehrere Millionen Menschen durften bereits keine Tickets für Inlandsflüge und Hochgeschwindigkeitszüge kaufen, weil die Behörden ihr Verhalten als missliebig einstuften. Umgekehrt bringt erwünschtes Verhalten Bonuspunkte und Privilegien. «Nudging», Schubsen, nennt die Verhaltensökonomie solche Inzentivierung. Mao «nudgt» neuerdings. Verhaltensökonomie und KI gehen eine Symbiose mit totalitärem Denken ein.
Wer die Standards setzt, kontrolliert die Welt

«Big Data meets Big Brother» bleibt nicht auf China beschränkt. Die Überwachungstechnologie wird ins Ausland verkauft, vor allem aber dringt allmählich chinesische Hardware und Software auf den Weltmärkten vor. Handys und Netzwerkausrüstung von Huawei sind ebenso Beispiele wie die App WeChat, mit der bereits 80 Prozent der Chinesen chatten und bezahlen, oder Tiktok, eine Videoplattform, die dank KI zielgenau auf die Bedürfnisse der User eingeht. Bei norwegischen Kids ist Tiktok der letzte Schrei, weshalb norwegische Medien fleissig Inhalte für die App kreieren. Wer weiss, welche Codes in der App sonst noch eingebettet sind.

Und seien wir ehrlich: Wenn es nur China wäre, wäre es nur halb so schlimm. Aber auch westliche Länder mischen ganz vorne mit bei der Entwicklung von Überwachungssoftware, etwa Amerika und Israel. So hilft das Silicon Valley Peking, die Überwachung seiner Bürger zu perfektionieren.

4. Geopolitik und Technik: Das Internet ist auch ein Instrument der Gleichschaltung – wenigstens in technischer Hinsicht. Die USA setzten hier die technischen Normen, und die Welt folgte. Man sieht dies daran, dass die USA das einzige Land sind, das keine nationale Kennung in Internet-Adressen hat. Das Internet ist das beste Beispiel für Geopolitik durch IT.

Das nächste Megaprojekt steht jetzt vor der Einführung – allerdings ist Peking diesmal wild entschlossen, die Vorherrschaft nicht Amerika zu überlassen. Der neue Mobilfunkstandard 5G wird die Telekommunikation revolutionieren. Er ist schneller, er kann grössere Datenmengen verarbeiten und mehr Teilnehmer bedienen. Selbstfahrende Autos und in Echtzeit ferngesteuerte Industrieroboter sind so erst möglich.

In den Uno-Gremien tobt bereits – man kann es nicht anders nennen – eine Schlacht zwischen den USA und China um die technischen Normen. Amerika fordert zudem seine Verbündeten auf, keine chinesische 5G-Netzwerktechnik einzusetzen, weil diese im Ernstfall von Peking manipuliert werden könne. So droht eine Zweiteilung der Welt: Die USA, Europa und Australien setzen auf europäische und amerikanische Anbieter; Afrika und weite Teile Asiens auf die chinesische Alternative.
«In London gibt es pro Kopf der Bevölkerung mehr als doppelt so viele Kameras wie in der Hauptstadt des Überwachungsstaates China.»

Wer die Standards bestimmt, hat einen Startvorteil bei der Vermarktung von neuen Technologien. Das gilt auch für 5G. China will beim Internet der Dinge und selbstfahrenden Autos führend sein. Und nicht völlig zufällig ist jetzt der Chef der Uno-Organisation in Genf, die für die 5G-Normen zuständig ist, ein Chinese.

Standards definieren die Art, wie wir mit Technik umgehen und wie diese auf unser Leben einwirkt. Demokratien müssen daher erörtern, welche gesellschaftlichen Auswirkungen sie akzeptieren. Ein Beispiel hierfür ist die KI, in der – vereinfacht gesprochen – Technik autonom entscheidet und sich überdies autonom weiterentwickelt. Umso wichtiger sind die ethischen Normen, die der Software einprogrammiert werden. Bei Kampfrobotern von Drohnen bis zu Minipanzern ist das evident. Es gilt aber genauso für selbstfahrende Autos und zivile Roboter. Es gilt erst recht für Überwachungssoftware, die auch im Westen Einzug hält. Westliche Behörden setzen ebenfalls vermehrt auf Gesichtserkennung und den lückenlosen Einsatz von Videokameras.

In London gibt es pro Kopf der Bevölkerung mehr als doppelt so viele Kameras wie in der Hauptstadt des Überwachungsstaates China. Die Londoner Polizei testet die Gesichtserkennung in Feldversuchen. Sobald der Computer eine Person zu erkennen glaubt, die zur Fahndung ausgeschrieben ist, wird sie verhaftet. Entscheidet also künftig KI-Software, ob Passanten erkennungsdienstlich behandelt werden? Man sollte das nicht einfach abtun. Ein vorläufiger Arrest mit Leibesvisitation ist keine erfreuliche Erfahrung, auch in der Schweiz nicht.

Der Unternehmer Kai-Fu Lee vertritt die These, dass China einem Techno-Utilitarismus anhängt: also der grösste Nutzen für die grösste Zahl von Menschen. Gesellschaften, die auf den Werten der Aufklärung basieren und vom Wert des Individuums überzeugt sind, sollten das anders sehen und für ihre Werte einstehen. Sie müssen auch dafür sorgen, dass die Informationstechnologie diese Werte reflektiert.

Ich habe ganz bewusst so unterschiedliche Dinge wie die Videoüberwachung, chinesische Unterhaltungs-Apps in norwegischen Kinderzimmern, russische Angriffe auf die Schweizer Neutralität oder Amazons Lautsprecher «Alexa» zusammengenommen. Jeder der vier Bereiche bedeutet für sich eine Herausforderung. In der Wechselwirkung aber haben sie das Potenzial, unser Leben drastisch zu verändern.
Das Handy bestimmt unser Bild von der Welt

Was bedeutet es, wenn der Schweizer Neutralitätsbegriff zum Zweiten Weltkrieg passt, aber nicht zu Cyberangriffen und dem «information warfare»? Weil die hybride Kriegsführung im Frieden beginnt, ist unser Konzept von Landesverteidigung obsolet.

Ich prophezeie Ihnen: Bei der Abstimmung über den Kampfjet werden wir eine digitale Beeinflussungsoperation erleben. Moskau will unbedingt verhindern, dass die Schweiz ein US-Flugzeug kauft. Schon heute machen Trolle im Internet dagegen Stimmung. Wieder lassen sich die Accounts keinen realen Personen zuordnen. Das kann uns nicht egal sein. Wir müssen nach Wegen suchen, um solche Operationen offenzulegen.

Was bedeutet es, wenn Technik nicht eben bloss ein Walkman, ein CD-Player oder ein Auto ist, sondern ein Smartphone, das unsere Persönlichkeit affiziert? Es liefert den Menschen ihr Bild von der Welt, es überwacht die Fitness und bestimmt, wie wir mit der Welt kommunizieren. Haben Sie schon einmal beobachtet, wie ein Kind vergeblich um Aufmerksamkeit ringt, während Mutter oder Vater Zwiesprache mit ihrem Telefon halten?

Was bedeutet es, wenn autoritäre Herrscher wie Putin den Umstand ausnutzen, dass Menschen heute Techno-Golems sind mit einem Handy als Körperteil, um ihre Ideologie zu verbreiten?

Was bedeutet es schliesslich, wenn gesellschaftliche Entwicklungen in fernen Ländern nicht auf diese beschränkt bleiben, sondern unmittelbar auf die Länder des Westens einwirken? Das Internet ist ein «global village», das Menschen zusammenbringt und den Austausch fördert. Es kann auch ein «global prison» sein, das die Überwachung fördert und die entsprechenden Technologien verbreitet.
Firmen besser kontrollieren

Damit sind wir bei den Konsequenzen. Wir müssen vermutlich stärker als bisher die wirtschaftlichen Aktivitäten von Firmen aus Staaten beobachten, die wie China nicht immer fair und vor allem nicht transparent agieren. Wo es um die individuelle und gesellschaftliche Freiheit geht, können wir die Firmen eines Überwachungsstaates nicht gleich behandeln wie Unternehmen aus Demokratien. Hier sind wir in der Schweiz vermutlich noch zu blauäugig.

Ich fordere keine neuen Gesetze zur Investitionskontrolle. Der bestehende Rechtsrahmen gibt den Behörden genügend Spielraum. Aber das vom Bundesrat angekündigte Monitoring ist sinnvoll. Wichtig ist, dass Politik und Öffentlichkeit dem Thema die nötige Beachtung schenken. Wir müssen auch als ein kleineres Land sehr viel stärker versuchen, Einfluss auf Standards und ethische Normen in der Technologieentwicklung zu nehmen. Das ist eine Aufgabe der Aussenpolitik. Wir müssen darüber diskutieren, wie IT-Konzerne kontrolliert werden und wie sich die Schweiz in diesen Regulierungsprozess einbringen kann. Früher bin ich davon ausgegangen, dass die Stellung von Google kein Problem sei. Jemand programmiert eine bessere Suchmaschine, und Googles Dominanz ist dahin.

Ich habe meine Meinung geändert. Die Tech-Giganten besitzen aufgrund ihrer schieren Grösse gegenüber Newcomern inzwischen einen immensen Vorteil. Der Wettbewerb funktioniert nur noch eingeschränkt. Social Media haben einen massiven Einfluss auf die Demokratie und die Art, wie wir Politik betreiben. Sie sind daher systemrelevant wie Grossbanken, die ebenfalls einer Regulierung unterworfen sind.
Nicht Panik, sondern Wachsamkeit

In einer NZZ-Kolumne beschreibt der Historiker Niall Ferguson, wie Facebook und Youtube Texte und Videos mit politisch «unangemessenen» Inhalten zensieren. Und als unangemessen gilt alles, was umstritten ist, etwa rechtspopulistische Äusserungen. Ist das unsere Vorstellung von Demokratie: ohne Pluralismus, ohne den Streit der manchmal auch extremen Meinungen? Nein, das darf nicht sein.

Nicht Panik, sondern Wachsamkeit ist angebracht. Jede neue Technologie weckt Ängste. Ein Beispiel ist die Gentechnik, die in den 1980er Jahren mit dem Klonen von Menschen gleichgesetzt wurde. Die Befürchtungen haben sich nicht bewahrheitet, weil inzwischen rund um den Globus Vorschriften gelten, die eine Balance zwischen Forschungsfreiheit und dem Schutzbedürfnis der Gesellschaft herstellen. Um revolutionärer Technologien Herr zu werden, braucht es also keine antikapitalistischen Kahlschlag-Szenarien, wohl aber ein vernünftiges und liberales Regelwerk. Für Kulturpessimismus, diesmal in der digitalen Variante, gibt es keinen Anlass.

Griffige Regeln existieren allerdings in der Informationstechnologie und der KI nicht oder nur in Teilbereichen, und vor allem nicht mit weltweiter Gültigkeit. Sie werden auch viel schwieriger zu erreichen sein, denn hier geht es um einen Milliardenmarkt für Unternehmen und gleichzeitig die militärischen und strategischen Interessen von Staaten. So versucht die Uno seit einem Jahrzehnt, eine Rüstungskontrolle für Cyberwaffen durchzusetzen, analog zu den Atomraketen im Kalten Krieg. Vergeblich.

Ebenfalls bis jetzt vergeblich bemüht sich die Staatengemeinschaft, Regeln für Killer-Robots und andere Waffensysteme mit KI zu definieren. Dennoch müssen wir versuchen, auch hier eine Balance zu erreichen. Denn wer heute Freiheit sagt, meint ganz oft: einen Software-Code.

Die offene Gesellschaft wirkt oft schwächer als ihre autoritären Herausforderer, aber sie besitzt einen unschätzbaren Vorteil: ihre Fähigkeit zur Selbstkorrektur durch kritische Debatten. Was die Digitalisierung mit all ihren Aspekten und Wechselwirkungen anlangt, stehen wir erst am Anfang der Diskussion.";https://www.nzz.ch/meinung/google-facebook-und-amazon-big-brother-lebt-jetzt-im-silicon-valley-ld.1509071;NZZ;Eric Gujer;;;
14.03.2020;Wie Behörden ihre Bürger in der Corona-Krise mittels Smartphone-Daten überwachen ;"Im Kampf gegen die Ausbreitung des neuartigen Coronavirus setzen vor allem asiatische Staaten auf Technologie. So gibt es in fast ganz China mittlerweile ein «Quick Response (QR)»-System: Mithilfe des Smartphones und einer Applikation kann nachverfolgt werden, wo sich die Bürger jeweils befinden und wohin sie gehen. Die Hauptstadt der ostchinesischen Provinz Zhejiang, Hangzhou, hat dabei die Vorreiterrolle übernommen – jene Millionenstadt, in der Jack Ma einst den Internetkonzern Alibaba gegründet hat.

Die Nutzer können zwischen dem von Tencent entwickelten System Wechat oder Alipay von Alibaba wählen. Dort taucht zunächst die Mini-App «Health Code» auf. Anschliessend müssen der Name und der Gesundheitszustand eingegeben werden, bevor der Nutzer von sich selbst mit dem Smartphone ein Foto macht und dann mittels Gesichtserkennung identifiziert wird. Letzteres ist bei Alipay möglich, weil sich bereits beim Herunterladen der App jeder Nutzer mit dem richtigen Namen und der jeweiligen Nummer des Personalausweises hat registrieren müssen.

Für das Gesundheitszertifikat gibt es drei Farben: Grün, Gelb, Rot. Alle in Peking Lebenden werden – eine gute Gesundheit vorausgesetzt – mit Grün eingestuft, weil die chinesische Hauptstadt nicht als Covid-19-Risikogebiet gilt. Wer dagegen mit Gelb gekennzeichnet wird, muss für zwei Wochen in Quarantäne. Bei Rot hat man sich unter Beaufsichtigung in Isolation an einem ausgewiesenen Ort zu begeben. Der Status der Farbe wird täglich um Mitternacht aktualisiert. Wer bei der Frage nach dem Gesundheitszustand falsche Angaben macht, muss mit rechtlichen Konsequenzen rechnen.
Überwachungssystem soll Lücken schliessen

Peking hatte erst vor wenigen Tagen die Einführung des QR-Systems bekanntgegeben. Der Entscheid dürfte auch mit dem Vorfall einer Frau zusammenhängen, die nach Verbüssung ihrer Haftstrafe das Gefängnis in Wuhan verlassen durfte. Sie war zu jener Zeit bereits erkrankt. Der Sohn holte sie mit dem Auto ab und gelangte problemlos bis in seine Pekinger Wohnung, obwohl Wuhan seit dem 23. Januar abgeriegelt ist. Der Fall sorgte in China für Empörung, weil er zeigte, welche Lücken im System zur Vermeidung einer weiteren Streuung von Covid-19 bestehen. Hätte es das inzwischen verpflichtende QR-System damals bereits gegeben, wäre es den Pekinger Behörden möglich gewesen, die Fahrt des Sohnes in das Hochrisikogebiet zu verfolgen.

Hangzhou hatte mit dem QR-Code-System das Ziel verfolgt, die Arbeitsaufnahme der Bürger nach der abflauenden Pandemie zu kanalisieren und zu erleichtern. Auch die Provinz Hubei will nun schrittweise die Produktionskapazitäten wieder hochfahren und nutzt dazu ein vergleichbares QR-System. Entscheidet sich in der zentralchinesischen Provinz eine Stadt dazu, die Arbeit wieder aufzunehmen, werden zunächst die oft noch in ihrer Heimat ausharrenden Arbeitskräfte informiert. Sind diese mit der Farbe Grün gekennzeichnet, weil sie nicht aus einem Risikogebiet kommen und gesund sind, dürfen sie mit gesondert ausgewiesenen Transportmitteln an ihren Arbeitsplatz reisen. Dort erfolgt dann noch ein weiterer Medizincheck.
Achselzucken beim Datenschutz

Chinesen sind sich des Umstands bewusst, dass ihre Bewegungsabläufe durch Smartphones und Apps nachvollzogen werden können. Die Behörden in Peking beteuern zwar, dass nur die Stadtregierung Zugriff auf die mit dem QR-System ermittelten Daten habe und die Privatsphäre geschützt bleibe. Viele Chinesen nehmen solche Versicherungen jedoch mit einem Achselzucken zur Kenntnis. Punkto Datensicherheit bezeichnen sie sich wegen der Nummer ihres Personalausweises bereits als «nackt». Dieses Dokument ist der Dreh- und Angelpunkt im täglichen Leben. Ohne den Personalausweis erhält man kein Hotelzimmer und keine Billetts.

An die Daten kommt man offenbar leicht heran, so dass ein Schwarzmarkt entstanden ist. Für wenig Geld können Anbieter genaue Informationen dazu liefern, wann eine bestimmte Person mit wem in welchem Hotel abgestiegen ist, wann sie in welchem Internet-Café online gegangen ist, wann sie China verlassen hat und wieder eingereist ist. Dafür braucht es allein die Nummer des Personalausweises. Und solche Anbieter schaffen es auch, in nur wenigen Minuten die genaue Ortsangabe mit GPS-Koordinaten zu liefern und somit darüber zu informieren, wo sich die Person gerade aufhält.
Südkoreas Lehren aus früheren Epidemien

Laut der japanischen Gesundheitsexpertin Hiromi Murakami, Gastwissenschafterin am Nationalen Graduiertenkolleg (Grips), verfolgen die Hightech-Nation Südkorea und Taiwan einen liberaleren Ansatz als das autoritäre China: «Südkorea hat kraftvolle und intensive Massnahmen mit einem hohen Mass an Transparenz durchgeführt.» Zwar war die Zahl der identifizierten Infizierten rasch in die Höhe geschnellt. Dafür erkannten die Behörden Infektions-Cluster schnell. Die koreanische Strategie lautet, Spuren des Virus zu verfolgen, in Massen zu testen und gezielt zu behandeln, ohne die Bürgerrechte und die offene Gesellschaft allzu sehr einzuschränken. Dabei spielt die Technik eine grosse Rolle.

Es beginnt bei den Tests. Südkorea hat nach den Coronavirus-Epidemien Sars und Mers ein gezieltes Krisenprotokoll aufgebaut. Kein Land ausserhalb Chinas hat derzeit einen so hohen Anteil seiner Bürger getestet wie Korea – mehr als 4000 auf eine Million Bewohner. Schwere Fälle kommen in spezielle Krankenhäuser, infizierte Menschen ohne oder mit leichten Symptomen kommen in ihren Wohnungen in Quarantäne. Über eine Smartphone-App des Innenministeriums bleiben die meisten von ihnen mit den Behörden in Verbindung. Die Behörden werden mithilfe der GPS-Ortung des Handys zudem informiert, wenn der Träger die Wohnung verlässt. Bis jetzt läuft sie auf Smartphones mit dem Betriebssystem Android von Google. Eine iPhone-Version folgt bald.
Virenkarten dank Apps

Ein wichtiger Punkt ist laut der Gesundheitsexpertin Murakami die prompte, pünktliche und transparente Information über diverse Kanäle. Die Regierung verbreitet Informationen zudem über Websites und Textmitteilungen auf Handys. Dabei werden auch detaillierte Informationen über Infizierte veröffentlicht, inklusive der Orte, die sie in den vergangenen Tagen aufgesucht haben. Findige App-Entwickler haben die Daten aufgegriffen und damit eine Art von Virenkarten erstellt. Südkoreaner können die eingezeichneten Orte dank Apps wie «Corona100m» oder «Coronamap.site» vermeiden.

Mit manchen können sogar Alter, Nationalität und Geschlecht eines Infizierten samt Bewegungen der vergangenen Tage und das Datum seines positiven Tests eingesehen werden. Zugleich informiert das Smartphone den App-Nutzer, sobald dieser sich einem Ort, an dem zuvor eine nachweislich infizierte Person war, bis auf 100 Meter nähert. In den sozialen Netzwerken beklagen einige allerdings, dass zu viel individuelle Daten preisgegeben würden. Seitens der Behörden heisst es in südkoreanischen Medien nun, man werde nach der Corona-Krise überlegen, ob dieses sehr transparente System einer Überarbeitung bedürfe.
Taiwan nutzt Big Data

Auch in Taiwan unterstützt die Technologie eine sehr aktive Virenbekämpfung. Unter den fast 24 Millionen Einwohnern des Eilandes sind Stand Freitag gerade einmal 50 Infizierte bekannt. Gleich nachdem China Ende Dezember erstmals Informationen über eine unbekannte Infektionskrankheit in Hubei veröffentlicht hatte, schickte die Regierung Gesundheitsbeamte in Flugzeuge aus der Region, um die Reisenden noch an Bord auf Krankheitszeichen zu untersuchen.

Im Land nutzte die Regierung die Analyse grosser Datensätze, um potenzielle Virenträger anhand ihrer Reisegeschichte aufzuspüren. Wer gefährdete Regionen besucht hatte, konnte mit einem Anruf der Gesundheitsbehörden rechnen. Wie in Korea werden auch die Bewegungsprofile von Infizierten veröffentlicht.
Schweizer Spielraum beim Datenschutz

In der Schweiz ist der Gebrauch von Smartphone-Standortdaten laut dem Rechtsanwalt und Digitalexperten Martin Steiger unter normalen Bedingungen nicht zulässig. Unter Epidemie-Bedingungen, wie sie momentan vorherrschen, wäre dieser aber denkbar. Behörden könnten etwa Bewegungsprofile von Infizierten aus der Vorratsdatenspeicherung nutzen. Denn Telekomanbieter können aus den Daten ihrer Antennen nachvollziehen, wo ein Handy eingeloggt war. Es gibt zudem die Bewegungsdaten, die Internetunternehmen wie Google oder Apple via Smartphones über uns sammeln. Dass diese Unternehmen dem Bund diese Daten liefern müssen, dafür sieht Steiger bis jetzt noch keine rechtliche Grundlage. Sobald der Bundesrat allerdings eine ausserordentliche Lage ausrufe, sei er gemäss Art. 7 des Epidemiengesetzes (EpG) ermächtigt, «die notwendigen Massnahmen anzuordnen».

Beim Eidgenössischen Datenschutz- und Öffentlichkeitsbeauftragten (Edöb) ist von einem Einsatz von Apps wie etwa in Südkorea nichts bekannt. «Wir gehen davon aus, dass wir vorab konsultiert würden, wenn Bundesbehörden entsprechende Pläne hätten», heisst es vom Edöb. Man hält es aber für denkbar, dass entsprechende digitale Apps auf private Initiative hin betrieben und von allen Beteiligten auf freiwilliger Basis benutzt würden. Dabei müssten die allgemeinen Grundsätze des Bundesgesetzes über den Datenschutz erfüllt werden.

Gesundheitsdaten dürfen laut Edöb auch vom Staat grundsätzlich nur mit Einwilligung der Betroffenen bearbeitet werden. «Obligatorien wären im Kontext der Seuchenbekämpfung nur aufgrund von ausdrücklichen und hinreichend bestimmten gesetzlichen Grundlagen gangbar und stehen unseres Wissens nicht zur Diskussion.» Steiger ist aber der Einschätzung, dass das Epidemiengesetz derart weit gehe, dass die wirksame Bekämpfung übertragbarer Krankheiten nicht allein auf Freiwilligkeit der Bürger beruhen könne.

Dass die Behörden in der Schweiz entsprechende Tracking-Apps entwickeln, hält Steiger dennoch für unrealistisch. Dem Bund fehle es an digitaler Kompetenz. Er sagt: «Wer soll das technisch umsetzen? Der Rückstand bei der Digitalisierung verhindert, dass solche Software wie in Südkorea genutzt wird.» ";https://www.nzz.ch/digital/coronavirus-wie-behoerden-die-buerger-in-der-krise-ueberwachen-ld.1546033;NZZ;Matthias Müller, Martin Kölling, Stephanie Lahrtz, Jenni Thier;;;
09.08.2015;Die Formel zum Sieg;"Eben hat es wie aus Kübeln gegossen, nun scheint wieder die Sonne. Es ist kalt, ein typischer Julinachmittag im Nordwesten Englands. Die jugendlichen Fussballtalente, die auf dem Kunstrasenplatz um bunte Plastic-Hütchen rennen, kümmert das nicht. Auf dem Weg zu ihrem grossen Traum lassen sie sich nicht von ein paar Regentropfen abhalten. Der Traum heisst Premier League, die höchste englische Fussballliga. Ihre Aussichten stehen nicht schlecht: Sie sind Junioren von Manchester City.

Der Chef ihres Vereins, Ferran Soriano, hat kurz zuvor im geheizten Konferenzraum nebenan der versammelten Presse erklärt, dass es sich heute um einen wegweisenden Tag für den Fussball handle: «Wir werden Dinge tun, die noch nie zuvor getan wurden.» Soriano sprach von Big Data. Manchester City will das Sammeln von Spielerdaten auf ein noch nie da gewesenes Niveau heben. Dazu werden die Fussballer aus Manchester – die Citizens – ab diesem Sommer mit Europas grösstem Softwareunternehmen, SAP, zusammenarbeiten. Der deutsche Konzern hat sich auf die Handhabung grosser Datenmengen spezialisiert. Sechzig Millionen Datenpunkte

Das Sammeln von Daten gehört mittlerweile zum Alltag jedes grossen Fussballklubs. Alles wird erfasst: Wie schnell die Spieler rennen, welche Wege sie gehen, wie genau sie passen. Bei Ernstkämpfen stehen dazu hochauflösende Kameras im Stadion, die alle Bewegungen der Fussballer aufzeichnen. Auf dem Trainingsplatz kommen Sensoren zum Zug, welche die Sportler auf sich tragen. «In einem einzelnen Spiel kommen so mehr als sechzig Millionen Datenpunkte zusammen», sagt SAP-Manager Fadi Naoum.

Allerdings wissen bis jetzt die wenigsten, was aus der Datenflut genau zu lesen ist. Noch nicht einmal ist klar, was überhaupt gemessen werden muss. Nun also will Manchester City gemeinsam mit dem Softwaregiganten aus Deutschland vormachen, wie die Sache anzugehen ist. Dazu werden die beiden Partner ein Analysesystem weiterentwickeln, das SAP ursprünglich für die deutsche Fussballnationalmannschaft konzipiert hatte. Bei der WM in Brasilien war es bereits im Einsatz. Trainingspläne, interne Kommunikation, Leistungsdaten der Spieler, Informationen über ihre Fitness, ihre Gesundheit und sogar ihre Essgewohnheiten – alles soll über eine einzige Plattform laufen. Vollmundig hat man in Manchester schon einmal eine Fussball-Revolution angekündigt.

In anderen Sportarten hat diese Revolution bereits stattgefunden, etwa im Baseball. Als die Oakland Athletics in einer äusserst schlechten Verfassung waren, wagte ihr Manager Billy Beane das Team grundlegend zu verändern. Er stellte einen jungen Statistiker an und verliess sich bei der Zusammenstellung der Mannschaft für einmal nicht auf seine Intuition, sondern auf die nackten Zahlen. Mit Erfolg: Im Jahr 2002 gelang den Athletics eine Siegesserie von zwanzig Spielen. In der über hundertjährigen Geschichte der amerikanischen Baseball-Liga schaffte das keine Mannschaft zuvor.

Daraufhin kopierten die Boston Red Sox die Methode und gewannen in den folgenden Jahren mehrmals die Meisterschaft. Unter dem Titel «Moneyball» hat Hollywood die Erfolgsgeschichte längst auf die Leinwand gebracht, Brad Pitt spielt im Film die Hauptrolle.

Der unbestrittene Statistik-Papst des Baseball heisst Bill James. Er war es, der die Ligagewinne der Red Sox erst möglich machte. Die Verantwortlichen von Manchester City verehren seine Arbeit. Vor drei Jahren fassten sie deshalb einen ungewöhnlichen Beschluss: Sie veröffentlichten sämtliche Daten ihrer Fussballer, die sich in der vorigen Saison angehäuft hatten. «Ich will, dass unsere Branche einen Bill James findet. Bill James braucht Daten», sagte damals Gavin Fleig, der leitende Leistungsdiagnostiker der Citizens, gegenüber der «Financial Times». Bisher blieb die Suche ohne Erfolg.
Benimmkurse für Junioren

Man hat nicht den Eindruck, dass Manchester City irgendetwas dem Zufall überlässt. Die Bestätigung dafür findet man bei einem Rundgang auf dem Trainingsgelände des Klubs. Im letzten Dezember wurde es eröffnet. Direkt gegenüber dem Heimstadion erstreckt sich die «City Football Academy» über mehr als 30 Hektaren. Hier stehen 17 Fussballfelder, ein eigenes Stadion inklusive. Im Gebäude daneben sind Fitnessräume, Schwimmbäder, Kantinen und Schulungszimmer untergebracht. Für die Junioren, die Jüngsten sind fünf Jahre alt, finden hier Benimmkurse statt. Das besondere Schmuckstück ist der Indoor-Fussballplatz, wo man dereinst das Klima verschiedener Weltgegenden simulieren will: Russland oder Katar vielleicht.

Auf den Aussenplätzen trainieren praktisch pausenlos Jungtalente irgendeiner Altersklasse. Gut möglich, dass einige von ihnen – mittels aufwendiger Analysen auserkoren – als Profis dabei sind, wenn Manchester City in den kommenden Jahren tatsächlich die Fussball-Revolution verwirklicht. Oder etwa doch nicht?

    Wer wagt den entscheidenden Schritt, wer bringt beim Aufbau eines Fussballteams der Statistik das nötige Vertrauen entgegen?

«Ich bezweifle, dass der analytische Durchbruch bei einem Verein wie Manchester City geschieht», sagt Chris Anderson. Der gebürtige Deutsche, einst Torwart bei einem Regionalliga-Verein, war bis vor kurzem Politologieprofessor an der Cornell University in den USA. Inzwischen hat er seinen Lehrstuhl niedergelegt und arbeitet Vollzeit als Fussballanalyst. Anderson berät Top-Mannschaften in England, Italien und Deutschland. In seinem Buch «Die Wahrheit liegt auf dem Platz», das 2013 erschienen ist, beschreibt er, wie Datenanalyse den Fussball verändern wird.

Von der Revolution ist Anderson überzeugt, nur eben nicht, dass sie bei einem Grossklub passiert. «Vereine wie Manchester City können sich zwar ein Dutzend Analysten und teure Software leisten», sagt Anderson. «Die entscheidende Frage aber lautet: Werden die Ergebnisse auch konsequent angewendet?» Wenn nämlich die Trainer kein Interesse daran hätten, dann seien die Analysen nicht viel mehr als ein Spielzeug – und der jetzige Coach der Citizens, Manuel Pellegrini, gelte eher als Traditionalist.

Laut Anderson geschieht der Umbruch bei einem kleinen Verein, wo man bereit ist, alles zu verändern. Die Schweiz hat vergleichsweise kleine Vereine. Auch sie sammeln Bits und Bytes, wobei die Schweizer Fussballliga die Klubs unterstützt. Im Auftrag der Liga überwacht die Firma Prozone ein Spiel pro Runde im Detail: Kameras verfolgen die Spieler, Spezialisten arbeiten die gesammelten Daten auf und übergeben sie den Klubs. «Die Tiefe der Datenauswertung unterscheidet sich dann von Verein zu Verein, abhängig von den vorhandenen Ressourcen», sagt Liga-Sprecher Philippe Guggisberg. Grössere Klubs wie der FC Basel und die Young Boys benutzen während der Trainings auch ihre eigenen Trackingsysteme. Dass ein Schweizer Verein voll auf Statistik setzt, ist aber nicht zu erkennen.

Dabei haben die Zahlen, die in den letzten Jahren in europäischen Ligen gesammelt wurden, schon einiges enthüllt. Etwa, dass nicht Ballbesitz per se, sondern jener im Drittel vor dem gegnerischen Tor entscheidend ist. Oder dass die Bedeutung der Defensive oftmals unterschätzt wird: Ein geschossenes Tor ist durchschnittlich 1 Punkt wert, ein verhindertes dagegen 2,5 Punkte.
Es gibt nichts mehr zu verbergen: Die gesammelten Daten landen unmittelbar auf dem Tablet des Trainers. Angriff im Fünfmeterraum

Doch wer wagt den entscheidenden Schritt, wer bringt beim Aufbau eines Fussballteams der Statistik das nötige Vertrauen entgegen? Der Fussballanalyst Chris Anderson nennt einen Namen: Matthew Benham. Nachdem der Brite in Oxford Physik studiert hatte, machte er mit seinem Unternehmen Smartodds Millionen. Er wettete auf Fussballspiele und verwendete dabei mathematische Modelle. Dann kaufte Benham zwei Fussballvereine, den FC Brentford in der dritthöchsten englischen Liga und den dänischen FC Midtjylland. In beiden Klubs hatten ab sofort die Daten das Sagen. Seither bestimmen statistische Werte, wer zum Elfmeter antritt, wer die Freistösse ausführt und dass bei Eckbällen schon einmal sieben Angreifer im Fünfmeterraum stehen. Die neusten Berechnungen erhalten die Trainer noch während des Spiels auf ihr Handy.

Der FC Brentford steht mittlerweile an der Schwelle zur Premier League, der FC Midtjylland hat im Juni die dänische Meisterschaft gewonnen – zum ersten Mal in der Vereinsgeschichte. Es sieht fast so aus, als verpasse es Manchester City, in seiner 48 000 Zuschauer fassenden Fussballarena zur Revolution anzusetzen. Denn die ist schon in vollem Gange: in den bescheidenen Stadien von Brentford und Midtjylland.";https://www.nzz.ch/nzzas/nzz-am-sonntag/fussball-big-data-die-formel-zum-sieg-ld.1340;NZZ;Martin Amrein;;;
01.10.2020;Die Schweiz wird bei der digitalen Wettbewerbsfähigkeit zurückgestuft;"Am Donnerstag hat die private Wirtschaftshochschule IMD zum vierten Mal das «Digital Competitiveness Ranking» veröffentlicht. Die Schweiz wurde auf der Rangliste von Hongkong überholt und liegt damit auf dem 6. Platz. Das Ranking vergleicht 63 Länder mit hohen und mittleren Einkommen. Verbessern könnte sich die Schweiz laut Christos Cabolis, Chefökonom des IMD-Wettbewerbs-Centers, vor allem bei den Firmengründungen: «Die bürokratischen Hürden für Unternehmer sind noch immer um einiges höher als in anderen Ländern», sagt Cabolis. Die Messgrösse, auf die sich der Ökonom bezieht, stammt allerdings aus dem «Doing Business»-Ranking der Weltbank, das kürzlich wegen Manipulationsversuchen in die Kritik geraten ist.

Zudem fehlten der Schweiz die Frauen in der Forschung. «Offenbar braucht es bessere Anreize, damit hochqualifizierte Frauen in der Forschung und Entwicklung arbeiten», sagt Cabolis. In einer Studie der EU-Kommission aus dem Jahr 2017 waren in der Schweiz nur 26,9% der Wissenschafter in der Industrieforschung Frauen. Beim Indikator, der den Anteil der Frauen in der Forschung abbildet, landet die Schweiz im «Digital Competitiveness Ranking» auf Platz 34. Schlecht schneidet die Schweiz ausserdem im Ranking der Hightech-Exporte ab. Dies ist jedoch eher auf die Methodik des Rankings zurückzuführen als auf die Schweizer Exporttätigkeit. So stiegen die Schweizer Hightech-Exporte in den vergangenen Jahren stetig an. Die Schweiz führte 2019 Waren im Wert von 312 Mrd. Fr. aus, über 20% davon waren Hightech-Produkte. Vor allem Pharmafirmen exportieren sehr viele forschungsintensive Waren. Bei digitalen Indikatoren wird die Schweiz überholt

Insgesamt fällt auf, dass die Schweiz bei der allgemeinen Wettbewerbsfähigkeit grundsätzlich sehr gut abschneidet, bei spezifisch digitalen Indikatoren aber von den Spitzenpositionen verdrängt wird. Verbesserungspotenzial hat sie laut IMD vor allem bei der Digitalisierung der Kontakte zwischen Privaten und Behörden und bei der Nutzung von Big Data. Dass Digitalisierungsprojekte in der Schweiz immer wieder verzögert werden, zeigen das politische Seilziehen um die E-ID oder die Diskussionen über die Digitalisierung im Gesundheitswesen.

Gute Noten erhält die Schweiz, weil sie hochqualifizierte Arbeitnehmende aus dem Ausland anziehen und im Arbeitsmarkt halten kann. Zudem haben viele Manager Erfahrungen in internationalen Märkten, was das Rating positiv beeinflusst.
Auch Deutschland wird zurückgestuft

Deutschland fällt im Ranking ebenfalls einen Rang zurück und liegt nun auf Platz 18. «Deutschland hat zwar eine exzellente Forschung und gute Talente, aber es kommt bei der digitalen Wettbewerbsfähigkeit nicht richtig in die Gänge», sagt Cabolis zur Deutschen Presse-Agentur. «Es hapert zum Beispiel an der technologischen Infrastruktur und an Investitionen in die Telekommunikation. Aber viele Deutsche kennen sich auch im digitalen Raum zu wenig aus. Da haben die Bildungsmassnahmen bisher noch nicht genug gebracht.»

Der grosse Aufsteiger unter den 20 Top-Platzierten ist China. Im Vergleich zum vergangenen Jahr macht das Land 6 Plätze gut und liegt neu auf Rang 16. Die USA führen das «Digital Competitiveness Ranking» seit 2018 an. ";https://www.nzz.ch/wirtschaft/die-schweiz-wird-im-digital-competitiveness-ranking-um-einen-platz-zurueckgestuft-ld.1579506;NZZ;Gioia da Silva;;;
17.12.2019;Digitale Technologien und wahre Widersprüche;"Zwei Dinge, schrieb Hugo von Hofmannsthal vor mehr als einem Jahrhundert, bestimmten das moderne Leben: «die Analyse des Lebens und die Flucht aus dem Leben». Das waren selige Zeiten, als der avancierte Zeitgenosse nach den Worten des Dichters zwischen «alten Möbeln» und «jungen Nervositäten» pendeln konnte. Wollte man diesen Gedanken paraphrasieren, könnte man sagen: Zwei Dinge bestimmen unser modernes Leben: die Digitalisierung und der Klimawandel. Beobachtet man die Debatten darüber, fällt auf, dass diese nahezu vollkommen getrennt verlaufen, so, als befände man sich einmal in dieser und dann in einer ganz anderen Welt.

Digitalisierung: Das bedeutet Automatisierung, Bequemlichkeit, neue Geschäftsmodelle, Robotik, künstliche Intelligenz, in Summe eine grossartige Zukunft, gewürzt mit ein bisschen Kritik an Google und Facebook. Klimawandel: Das bedeutet Erderwärmung, Gletscherschmelzen, Artensterben, «Flugscham» und drohender Weltuntergang. Und doch leben wir in beiden Welten. Haben diese wirklich nichts miteinander zu tun?

Natürlich wäre es müssig, darauf hinzuweisen, dass die Digitalisierung selbst einen veritablen Beitrag zur CO2-Produktion und damit zum Klimawandel leistet. Gross diskutiert wird das nicht; angesichts der Digitalisierungsoffensiven im Bildungswesen zusätzlich zur «Flugscham» von den jungen Leuten auch noch eine «Handyscham» einzufordern, wäre wohl zu viel verlangt, zumal die sozialen Netzwerke, die zu Fridays for Future aufrufen, dann nicht mehr funktionierten. Es wäre übrigens ja nicht das erste Mal, dass der Teufel durch Beelzebub ausgetrieben würde.

Interessanter ist ein anderer Aspekt. Digitalisierung und Klimawandel stehen sich in ihren Programmatiken und Assoziationsfeldern diametral gegenüber. Während die Digitaleuphoriker das Hohelied der Virtualität singen, von smarten Städten, intelligenten Automaten und einem Dienstleistungssektor träumen, der dank Big Data, Algorithmen und Streaming-Diensten keine Wünsche mehr offenlässt, verweisen die Klimaapokalyptiker auf die harten Eigenschaften der analogen Realität. Wenn Stürme wüten, Landstriche austrocknen, Meeresspiegel steigen, die vernetzten Städte versinken, dann wird klar: Der Mensch lebt nicht von Daten allein. Er lebt überhaupt nicht von Daten. Existieren heisst nach wie vor und bis auf weiteres: Essen, Trinken, Wohnen, Schlafen, Sterben.

Wohl kann man die digitalen Technologien einsetzen, um die Folgen des Klimawandels abzumildern: Frühwarnsysteme, bessere Steuerung von Verkehrsströmen, effizienterer Einsatz von Technologien. Aber die Wucht der Beschwörungen des bevorstehenden Weltenendes lebt von der archaischen Vorstellung, dass den Menschen die physische Grundlage ihres Lebens entzogen wird und sie auf ganz altmodische Art schwitzen, hungern, dürsten und nach Atem ringen werden. Im digital kommunizierten Mahnruf zum Schutz und zur Rettung des Klimas prallen deshalb zwei Vorstellungswelten aufeinander. Man muss diese nicht gegeneinander ausspielen, auch wenn das mitunter durchaus reizvoll sein könnte. Im Zuge einer etwas abgekühlten Klimadiskussion könnte man aber erkennen, dass die Digitalisierung nicht als fundamentale Umgestaltung unseres Lebens aufzufassen ist, sondern als technologischer Überbau mit all den dazugehörigen Phantasmen, der sich nur über einer halbwegs intakten materiellen Basis entfalten kann. Wer allerdings in der künstlichen Intelligenz eine erstrebenswerte posthumanistische Perspektive sieht, kann die Sorgen um den Temperaturanstieg ohnehin beiseiteschieben. Die klugen Maschinen, die uns ablösen, werden dann eben im Schatten rechnen.";https://www.nzz.ch/meinung/kolumnen/wahre-widersprueche-ld.1528865;NZZ;Konrad Paul Liessmann;;;
20.04.2020;Eine vom Programmieren und vom TV besessene Allrounderin;"«Nach einigen Wochen Lockdown merken die Anwender, dass Netflix ausgeschossen ist. Bei Zattoo ist das anders», sagt die Zattoo-Gründerin Bea Knecht. Während der US-Streaming-Dienst in der Schweiz rund 36 000 Stunden im Jahr anbiete, kämen bei Zattoo jede Woche um die 42 000 Stunden neuer Content hinzu. Knecht hatte bereits im Jahr 1990 die Idee für das TV-Streaming via Internet. Sie musste sich aber gedulden, da die Rechenleistung und die Übertragungsraten noch nicht ausreichten.
Der Zeit voraus

2005 war es dann so weit, und Knecht gründete in San Francisco den Internet-TV-Dienst Zattoo. Sie sei «vom TV besessen und überzeugt, dass es das Fernsehen auch in Zukunft braucht». Im Jahr 2019 übernahm die TX Group die Mehrheit an Zattoo und die Führung im Verwaltungsrat (VR). Bea Knecht hält den Grossteil der restlichen Aktien und zwei Sitze im VR. Heute nehme Zattoo noch 10 bis 20% ihres Berufslebens in Anspruch. Auch ihre zwei neuen Projekte, die Firmen Genistat und Levuro, arbeiten mit Online-Video.

Genistat analysiert mithilfe von Artificial Intelligence grosse Datenmengen (Big Data). So kann in Echtzeit ein Grossteil des TV-Publikums erfasst und seine Wünsche aufgenommen werden. Zudem hat der «knowledge graph» alle Fiktion und alle Sportsendungen im Fernsehen in allen Sprachen «gesehen». Die Software kann deshalb Torsequenzen erkennen und als Clips an Medien weiterleiten – oder Zusammenfassungen von Spielen produzieren. Langfristig ist denkbar, dass die Software ein fiktives Spiel von zwei grossen Mannschaften selbst erstellt: «Regeln halten den Spielverlauf in einem für die Maschine verständlichen Rahmen», sagt die TV-Unternehmerin.
Wiederverwendung von Know-how

Bei der Firma Levuro geht es um «social selling» auf Facebook, Instagram und Co. Mit einer App können Marken ihre Botschaften, mit Bildern und Videos und einem «call to action» verknüpft, zu Zielkunden lenken. Dabei ist die Sicherung von Rechten an Bildern zentral. Jedes einzelne Bild einer Sendung oder eines Films kann mit Informationen zu den Urheberrechten versehen werden. Bereits für die Gründung von Zattoo waren die Bildrechte im TV zentral. Weil diese damals international nur in der Schweiz erhältlich waren, begann das operative Geschäft hier. Ein iteratives Vorgehen zieht sich wie ein roter Faden durch die Karriere von Knecht: «Es geht mir um die Wiederverwendung von Know-how, das ich früher bereits entwickelt und angewendet habe.»

Es erstaune Knecht, wie gönnerhaft Männer sich verhielten, wenn sie merkten, dass sie auf einem Gebiet Spezialistin sei. Dabei ist sie eine «Multi-Spezialistin». Jedes Familienmitglied hatte im aargauischen Familienunternehmen Knecht eine Aufgabe zu übernehmen: Sie implementierte IT-Branchen-Lösungen für verschiedene Bereiche, zudem übernahm sie Aktien der Familiengesellschaft, verschuldete sich dafür und arbeitete sich in die Unternehmenssteuerung ein. Beim Startup Linuxcare in San Francisco galt sie als Marketingexpertin, bei ihrer Tätigkeit für McKinsey und die UBS als IT- und Bankspezialistin. Alle diese Fähigkeiten kulminierten in der Tätigkeit als CEO von Zattoo.
A/B-Test für Gender-Fragen

«Ich sehe, wie Frauen in ihrer Karriere oft an eine gläserne Decke stossen», sagt die Zattoo-Gründerin. Ihr fällt das noch deutlicher auf als anderen Geschlechtsgenossinnen. Bis zur Geschlechterangleichung im Jahr 2012 lebte sie in einem Männerkörper. Ganz IT-Fachfrau, bezeichnet sie sich als «wandelnden A/B-Test». Ihre Geschlechterangleichung sei heute kaum mehr ein Thema, ausser etwa bei Handwerkern, die doch erstaunt seien, wenn sie etwas reparieren könne. Ihr habe als Geschäftsführerin auch der Aussenblick geholfen. 1986 zog Knecht für ein Informatikstudium an die Eliteuniversität Berkeley nach Kalifornien. Das erlaubte ihr eine internationale Perspektive. Schweizer würden erst tiefstapeln und seien dann überzeugt, dass sie es immer am besten machten, was natürlich nicht stimme. Es gebe stets mehrere Wege zum Ziel. «Ich habe aber mit der Schweiz nie abgeschlossen», sagt Knecht – auch weil ihre Familie hier lebt. Nach 300 Flügen über den Atlantik habe sie sich 2007 wieder «in Europa gefunden». Heute ist sie vor allem von Zürich aus aktiv.
Zattoo wächst auch 2020

Jedes Unternehmen, das 2020 ein Nullwachstum erreicht, ist für Knecht ein Champion – Zattoo werde ein positives Wachstum ausweisen. Viel Werbung für Events entfalle, trotzdem seien die Werbungen gut gebucht, und es gebe keine Rabatte, da die Zattoo-Nutzung um 30% gestiegen sei. Knecht sieht weiterhin ein grosses Bedürfnis fürs Fernsehen, das mehr bietet als die Web-Dienste: Es informiere, unterhalte und gebe Zusammenhalt, in Form von gemeinsam Erlebtem und Gesprächsstoff.

Netflix unterhalte und könne mit einzelnen Serien Gesprächsstoff bieten. Youtube offeriere viel Unterhaltung und teilweise Informationen etwa mit Erklärvideos. Diese Dienste enthielten während der Corona-Krise aber kein Interview mit einem Bundesrat und lieferten keine regionalen Informationen. Zudem fehlten ihnen der Bezug zur Schweiz und die europäischen Wertvorstellungen.

";https://www.nzz.ch/wirtschaft/tv-streaming-zattoo-gruenderin-bea-knecht-hat-bereits-neue-ideen-ld.1552015;NZZ;Werner Grundlehner;;;
20.11.2020;Eindämmung statt Ausmerzung – warum den Europäern in Sachen Corona das Lernen von Ostasien so schwer fällt;"«Wir müssten nicht an diesem Punkt sein», lautet das Urteil der Virologin Isabella Eckerle zum zweiten Lockdown in der deutschen Talk-Sendung «Hart, aber fair». Eckerle, die das Zentrum für neuartige Viruserkrankungen am Universitätsklinikum Genf leitet, widerspricht damit all denjenigen, die betonen, man wisse noch immer nicht genau, wie man mit dieser Pandemie umgehen solle. Als sie auf die erfolgreichen Ansätze in Asien hinweist, wird sie von dem Moderator Frank Plasberg unterbrochen – da gebe es ja China, und «die halten sich mit Demokratie nicht so lange auf».

Plasbergs reflexhafter Verweis auf China ist beispielhaft für eine in Europa weitverbreitete ablehnende Haltung gegenüber den ostasiatischen Strategien im Umgang mit der Corona-Pandemie. Einmal betont man, wie in der Debatte um Masken im Frühjahr, die Andersartigkeit der asiatischen Kulturen, gerne unter Bezugnahme auf den Konfuzianismus oder den Kollektivismus. Ein anderes Mal echauffiert man sich über den chinesischen Überwachungsstaat. Nur weiter bringt uns dieser Abwehrreflex nicht.
Zurück zur Normalität

Tatsache ist, dass ostasiatische Gesellschaften weitgehend zur Normalität zurückgekehrt sind. Am 31. Oktober feierten in Taiwan 130 000 Menschen Asiens grösste LGBTQ-Parade Taiwan Pride. Insgesamt sind seit Beginn der Pandemie in Taiwan 7 Menschen an Covid-19 gestorben, seit über 200 Tagen gibt es keine Neuansteckungen. In Südkorea gibt es täglich um die 100 Neuinfektionen. In beiden Ländern war kein Lockdown nötig. In Vietnam, das knapp 30 Millionen mehr Einwohner als Frankreich zählt, sind nur 35 Menschen gestorben – in Frankreich sind es über 40 000. Auch China ist es, bei aller berechtigten Kritik an den Versäumnissen in der Anfangsphase, gelungen, den Ausbruch des Virus in Wuhan praktisch vollständig unter Kontrolle zu bringen. Dort finden Kongresse wieder offline statt, die Wirtschaft erholt sich rasch.

    Lieber alle wieder in den Lockdown als eine wirksame App auf dem Handy oder die digitale Überwachung der Quarantäne einiger weniger.

Ziel der massiven Grundrechtseinschränkungen im Frühjahr war es, Zeit zu gewinnen und die Infrastruktur aufzubauen, um die Pandemie danach unter Kontrolle zu halten. Warum haben die ostasiatischen Länder dies geschafft und wir nicht?

Während ostasiatische Regierungen sich auf das schnelle Austreten der Glutnester konzentrierten, gerieten in Europa während der sommerlichen epidemiologischen Entspannung lokale Hotspots ausser Kontrolle. Dort wurde massenhaft und durch die öffentliche Hand finanziert getestet, hierzulande waren Corona-Tests nur begrenzt verfügbar und mussten teilweise privat bezahlt werden. Wo in Ostasien ein frühes und konsequentes Infektionsketten-Tracing auch mittels Big Data umgesetzt wurde, hinkt in Europa die digitale Vernetzung im Gesundheitswesen stark hinterher. Trotz der europäischen Datenschutzgrundverordnung, die den weltweit strengsten Schutz der Privatsphäre bietet, herrscht gegenüber digitalen Technologien grosses Misstrauen: Lieber alle wieder in den Lockdown als eine wirksame App auf dem Handy oder die digitale Überwachung der Quarantäne einiger weniger. Auch der irrationale Skeptizismus gegenüber Gesichtsmasken besitzt kein Pendant in Ostasien.
Prinzip der Ausmerzung

Experten in Europa und Ostasien setzten von Anfang an auf unterschiedliche Ansätze. Nach den Erfahrungen mit Sars im Jahr 2003 verfolgten die ostasiatischen Länder das «Unterdrückungsmodell». Nach dem gleichen Fahrplan strebte Australien auf Rat seiner grössten Universitäten hin danach, die Neuinfektionen möglichst vollständig zu unterbinden – mit Erfolg. In europäischen Staaten hingegen schien selbst die prinzipielle Möglichkeit der Virusausmerzung unvorstellbar oder zumindest unpraktisch. Stattdessen stellten Epidemiologinnen das bekannte «Influenzamodell», gemäss dem sich das Virus nicht stoppen lasse und letztlich eine langsame globale Durchseuchung akzeptiert werden müsse, als alternativlos dar.

Dass es überhaupt diese beiden gegensätzlichen Ansätze gibt, taucht im öffentlichen euro-amerikanischen Pandemiediskurs kaum auf. Die erfolgreichen Beispiele aus Taiwan, Südkorea, Vietnam, China oder der Mongolei werden ausgeblendet, die grossen Erfolge in der Pandemiebekämpfung pauschal mit dem Verweis auf die Insellage oder die Autokratie abgetan.

Statt einer gesunden Portion Neugier darauf, welche politischen, organisatorischen, technischen und medizinischen Massnahmen den fulminanten Erfolg gegen Covid-19 in Asien ermöglicht haben, dominiert Ignoranz. Man müsse sich nicht wirklich mit «deren» Massnahmenkatalog auseinandersetzen. So verlagert die reflexartige China-Skepsis die Debatte unversehens von einem rationalen und pragmatischen Lernprozess, wie man die Pandemie effizienter in den Griff bekommen könnte, hin zur Frage der anfänglichen Schuld oder zum erhobenen Zeigefinger der Systemkritik.

Gewiss, die Vorbehalte, vom autoritären China zu lernen, sind verständlich, zumal in der gegenwärtig aufgeladenen geopolitischen Stimmung. Doch Südkorea und Taiwan sind liberale Demokratien. Und selbst im Einparteistaat China liessen sich anpassbare Erfahrungswerte auffinden. Eine pauschale Abgrenzung von Demokratie und Autoritarismus macht ein differenziertes Verstehen der chinesischen Massnahmen schlicht unmöglich.
Habitus der Überlegenheit

Dabei gibt es europaweit in Universitäten und Think-Tanks genügend Asienexperten, die dabei hätten helfen können, die erfolgreichen Ansätze von Taipeh bis Seoul einzuordnen. Jene mit entsprechenden Sprachkenntnissen haben den Corona-Ausbruch in Asien aufmerksam verfolgt. Nur sind sie während der Pandemie kaum angefragt worden. Ein Blick auf die europäischen China-Twitterer im Januar hätte genügt, um zu erkennen, was auf Europa zukommen würde.

Die intuitive Polemik gegen das autoritäre Regierungssystem Chinas und das hartnäckige Ausblenden der asiatischen Erfolgsmodelle lassen sich am besten mit dem Begriff des epidemischen Orientalismus erklären. Er beschreibt eine Geisteshaltung, die jegliches Lernen vom Gegenüber ausschliesst, weil das orientalische Andere als fremd und minderwertig gilt. Asien kann somit niemals als Vorbild, sondern nur als Folie für die ideologische Abgrenzung dienen. In Talkshows und in den Krisensitzungen von Bern bis Berlin fanden so weder das «Unterdrückungsmodell» noch die detaillierte epidemiologische Expertise von Wissenschaftern aus Ostasien Gehör. Die Chance wurde vertan, ernsthaft darüber zu diskutieren, wie alternative Methoden der Kontaktsuche oder lokale Massentests im europäischen Kontext umgesetzt werden könnten.

Unser Umgang mit Covid-19 scheitert daher auch aufgrund orientalistischer Vorurteile und des Habitus der Überlegenheit. Eine Haltung, die dem Selbstbild von liberalen, aufgeklärten Gesellschaften eigentlich diametral entgegensteht. Denn wären nicht gerade die öffentliche Diskussion verschiedener Ansätze und die Lernfähigkeit unsere Stärke? Europäische Regierungen hätten im Frühjahr einen Dialog mit führenden ostasiatischen Fachleuten aufnehmen können. Schon damals zeichnete sich ab, dass es sich lohnt, in dieser Boom-Region nach replizierbaren Konzepten und Instrumenten zu suchen. Stattdessen befinden wir uns jetzt in einem zweiten Lockdown, dessen sichere Beendigung nicht absehbar ist. Isabella Eckerle hat schon recht: An diesem Punkt müssten wir nicht sein.";https://www.nzz.ch/meinung/unterdrueckung-statt-ausmerzung-warum-den-europaeern-in-sachen-corona-das-lernen-von-ostasien-so-schwer-faellt-ld.1587172;NZZ;Marina Rudyak, Maximilian Mayer und Marius Meinhof;;;
27.09.2019;Eine Schweizer Initiative will der digitalen Welt ein Gewissen verpassen;"Neue Technologien bringen viele Vorteile. Doch die Digitalisierung kann auch eine Gefahr darstellen: etwa dann, wenn Algorithmen diskriminierende Entscheidungen treffen, wenn Tech-Konzerne mit den Daten von Nutzern leichtfertig umgehen oder Unternehmen Überwachungstechnik an autoritäre Regime liefern, die sie zur Kontrolle der Bürger nutzen. Schon länger beschäftigen sich Staaten, Wissenschaft, Industrie und Zivilgesellschaft daher mit der Frage, wie verhindert werden kann, dass Big Data und Algorithmen zum Schaden anstatt zum Nutzen der Menschen eingesetzt werden.

Nun will eine neue Schweizer Initiative mitmischen und Unternehmen zu mehr Ethik in der digitalen Welt bewegen. Die Swiss Digital Initiative (SDI) ist Anfang September in Genf gestartet. Lanciert hat das Projekt die Standort-Initiative Digitalswitzerland. Den Vorsitz der dahinterstehenden Stiftung übernimmt die ehemalige Bundesrätin Doris Leuthard. Weitere Mitglieder sind die Präsidenten der ETH Zürich und der Universität Genf sowie der Ringier-CEO und Gründer von Digitalswitzerland, Marc Walder. Mit Bundeskanzler Walter Thurnherr soll auch der Bund im Stiftungsrat vertreten sein.

Die Initiatoren richten sich an Firmen weltweit und wollen diese dazu bringen, beim Umgang mit künstlicher Intelligenz (KI) oder bei der Ansammlung und Verwendung persönlicher Daten ethische Standards einzuhalten. Schlagworte sind Transparenz, Verantwortung, Nichtdiskriminierung oder Erklärbarkeit. Vertreter aus den Chefetagen von Google, Facebook, Microsoft oder Huawei waren bei der Lancierung der SDI denn auch anwesend. Ob und inwiefern die Unternehmen mitmachen werden, ist aber noch offen. Ein erster Entwurf eines Grundsatzpapiers soll in den kommenden Monaten überarbeitet werden. «Dann werden wir mal sehen, wer das unterschreibt», sagt Doris Leuthard im Gespräch mit der NZZ. Sie hoffe, im Januar am Weltwirtschaftsforum (WEF) in Davos erste Projekte vorstellen zu können.
Noch viele offene Fragen

Denn noch ist wenig bekannt dazu, wie konkret Unternehmen zur Einhaltung ethischer Standards bewegt werden sollen oder was die SDI von bereits existierenden Projekten unterscheiden wird. Man stehe noch am Anfang, sagt Leuthard. Die Idee sei aber, auf bestehenden Richtlinien und Ethik-Initiativen aufzubauen. «Es gibt relativ viele Prinzipien, was meistens fehlt, ist die konkrete Umsetzung», betont die ehemalige Bundesrätin.

Seit einiger Zeit häufen sich die Versuche, mehr Ethik in die digitale Welt zu bringen. Allein im Bereich der künstlichen Intelligenz zählte die Organisation AlgorithmWatch im Juni, ohne Anspruch auf Vollständigkeit, mehr als 80 Ethik-Initiativen. Die Mehrheit ist 2018 oder 2019 entstanden. Im Mai haben sich die Mitgliedsstaaten der Organisation für wirtschaftliche Zusammenarbeit und Entwicklung (OECD) auf nicht bindende Grundsätze für den Umgang mit KI geeinigt. Auf europäischer Ebene hat eine von der EU-Kommission eingesetzte Expertengruppe im April Ethik-Leitlinien für eine vertrauenswürdige KI veröffentlicht. Bis 2020 werden diese in einer Pilotphase von verschiedenen Interessenvertretern getestet.

Auch die Forschung und der Privatsektor befassen sich mit dem Thema, Konzerne setzen auf eigene Ethik-Komitees oder Ähnliches. Microsoft hat seine KI-Prinzipien, genau wie IBM. Amazon unterstützt gemeinsam mit der amerikanischen National Science Foundation Recherche im Bereich Fairness bei künstlicher Intelligenz; und Facebook hat an der Technischen Universität München in ein Institut für KI-Ethiker investiert.

Die Grundsätze, auf die sich die zahlreichen Ethik-Bestrebungen beziehen, unterscheiden sich dabei kaum. Meist ist von Transparenz, Nichtdiskriminierung, Rechenschaftspflicht und Sicherheit die Rede. Laut der Organisation AlgorithmWatch, die die Auswirkung von Algorithmen auf die Gesellschaft kritisch untersucht, gehen die wenigsten der Initiativen aber über eine erklärte Selbstverpflichtung hinaus. Nur bei drei oder vier der von der Organisation verzeichneten Fälle gebe es Hinweise darauf, dass auch ein Aufsichts- oder Durchsetzungsmechanismus zur Anwendung komme.
Gefahr des «ethics washing»

Häufig handle es sich bei dem Vorgehen um nichts anderes als um sogenanntes «ethics washing», kritisiert Thomas Metzinger, Professor für theoretische Philosophie an der Universität Mainz. «Das bedeutet, dass die Industrie ethische Debatten organisiert und kultiviert, um sich Zeit zu kaufen – um die Öffentlichkeit abzulenken, um wirksame Regulation und echte Politikgestaltung zu unterbinden oder zumindest zu verschleppen», schrieb Metzinger im April in einem Gastbeitrag im deutschen «Tagesspiegel».

Der Universitätsprofessor war Mitglied jener Expertengruppe, die die ethischen Leitlinien für KI im Auftrag der EU-Kommission erarbeitet hat. Er hat dabei mit einer zweiten Person rote Linien definiert, also Prinzipien, die festlegen, was nicht mit KI gemacht werden darf. Dazu zählten sie unter anderem den Einsatz von tödlichen autonomen Waffensystemen oder die KI-gestützte Bewertung von Bürgern durch den Staat, die etwa bei Chinas anlaufendem Sozialkreditsystem zur Anwendung kommt.

Diese roten Linien seien auf Druck von Industrievertretern jedoch verwässert, andere gar aus der Endfassung der Leitlinien gelöscht worden, beklagte Metzinger nach der Ausarbeitung des Textes. Erst Handlungsempfehlungen an die Kommission, die von der Expertengruppe zwei Monate später veröffentlicht wurden, waren schärfer formuliert. «Die Frage bei Ethik-Initiativen ist immer, ob sie ernst gemeint sind oder ob es sich lediglich um eine Betrugs-Nummer vonseiten der Industrie handelt», sagt Metzinger im Gespräch.
Ein Gütesiegel für Ethik?

Dieser Frage wird sich auch die SDI stellen müssen. Um es nicht bei blossen Willensbekundungen der Unternehmen zu belassen, erwägen die Träger der Initiative den Einsatz eines Label-Systems. Dieses könnte jene Firmen zertifizieren, die sich an gewisse Standards halten. Wie wirksam solch ein freiwilliges Label sein kann, muss sich allerdings erst zeigen. Ungewiss ist zudem, ob sich dieses international als Qualitätsmerkmal durchsetzen könnte. Die SDI steht mit der Idee auch nicht alleine da: Die Fraunhofer-Gesellschaft etwa arbeitet ebenfalls an einer KI-Zertifizierung.

Grossen Einfluss erhoffen sich die Initiatoren der SDI unter anderem durch den Stiftungsstandort Genf, mit seinen internationalen Organisationen. «Die Stadt bietet das optimale Umfeld, um rasch mit internationalen Akteuren in Kontakt zu treten», sagt Valentin Zellweger, der als Schweizer Botschafter bei der Uno vor Ort ist und die Initiatoren bei der Lancierung unterstützt hat. Umgekehrt könne die Stadt auch von der SDI profitieren, denkt Daniel Stauffacher, ehemaliger Schweizer Botschafter, Präsident der Stiftung ICT4Peace und Gründer des Zurich Hub for Ethics and Technology, der sich mit Chancen und Risiken der neuen Technologien befasst: «Dies ist ein wichtiger Schritt dafür, dass Genf für die Gouvernanz-Bildung in der neuen digitalisierten Welt mit an der Spitze bleibt.»

Noch bleiben jedenfalls viele Fragen offen. Etwa, wie kontrolliert würde, dass sich Unternehmen an von einem Label vorgegebene Kriterien halten, oder ob die Zertifizierung auch wieder entzogen werden könnte. «Letztlich hängt viel davon ab, wer ein solches Label herausgeben würde», sagt Cornelia Diethelm, Gründerin des Centre for Digital Responsibility, eines Think-Tanks, der sich mit Fragen der Ethik im digitalen Bereich befasst. Sie hält es für die Glaubwürdigkeit der neuen Initiative für wichtig, dass auch die Zivilgesellschaft ins Boot geholt werde. Laut den Initiatoren ist das geplant.";https://www.nzz.ch/schweiz/eine-schweizer-initiative-will-der-digitalen-welt-ein-gewissen-verpassen-ld.1508955;NZZ;Judith Kormann;;;
11.09.2020;Der Innovationspark Zürich darf nicht untergehen;"Jedem ist klar: Die Wettbewerbsfähigkeit der Wirtschaft am Standort Schweiz und damit ein wesentlicher Teil unseres Wohlstandes hängen massgeblich von unserer Innovationsfähigkeit ab. Dabei gehörte die Schweiz bisher zur internationalen Spitzengruppe. Weniger bewusst ist vielen der laufende intensive internationale Wettbewerb mit Innovationen. Dabei verliert die Schweiz relativ gegenüber wichtigen Wettbewerbern bzw. Regionen laufend. Der vom Staatssekretariat für Bildung, Forschung und Innovation (SBFI) herausgegebene Bericht «Forschung und Innovation in der Schweiz 2020» zeigt, dass Volkswirtschaften wie Singapur, Südkorea, Israel und die Niederlande rasch aufholen. Im Vergleich mit führenden Innovationsregionen der Welt wird der Standort Schweiz nicht nur von Regionen in Asien, sondern auch von Baden-Württemberg und Bayern stark bedrängt, ja überholt.
Innovationsräume

In der Schweiz sind neue und wiederholte Anstrengungen zur Stärkung der Innovationsleistungen und der Standortattraktivität dringend notwendig. Die 2015 lancierte nationale Initiative Switzerland Innovation will zu einer solchen Dynamisierung beitragen. Ihre Zielsetzungen sind Ausbau der Infrastruktur für Forschung und Entwicklung mit industrieller Orientierung, Zusammenarbeit von Hochschulen mit technologieintensiven Unternehmen und Ansiedlung neuer Unternehmen auf der Basis fachlicher Kompetenzen sowie Kommerzialisierung hervorgebrachter Leistungen im Verbund bei hoher internationaler Vernetzung. Startup- und Spin-off-Unternehmen aus dem Hochschulumfeld erhalten optimale Entfaltungsmöglichkeiten. Auch Aus- und Weiterbildung profitiert durch Einbezug der Firmen vor Ort, im Innovationspark.

Diese Initiative wird durch die Standortkantone, die Privatwirtschaft und die beteiligten Hochschulen realisiert. Bis anhin sind daraus Projekte in Lausanne mit dem Netzwerk Romandie, Biel, Basel, Würenlingen (PSI Innovaare) und Zürich in der Realisierung. So hat die EPFL diesen Impuls genutzt, um Aktivitäten in Genf, Lausanne, Neuenburg, Freiburg und Sitten auf- und auszubauen. Der Arc lémanique hat sich als dynamischer Technologie- und Innovationsraum stark profiliert und sich gegenüber der Deutschschweiz massiv verbessert.

Das Projekt SIP Zürich ist jedoch kaum vom Fleck gekommen. Dabei ist das Potenzial im Raum Zürich sehr gross: Es besteht ein starker Hochschul-Cluster mit der Universität Zürich (UZH), der ETH, der Empa, der Zürcher Hochschule für Angewandte Wissenschaften sowie mit Spitälern mit akademischer Anbindung wie dem Universitätsspital, alle mit hervorragenden Kompetenzträgern in Lehre und Forschung.

Innovation bedeutet Umsetzung wissenschaftlicher Leistungen in erfolgreiche unternehmerische Aktivitäten, in Markterfolge. Dazu braucht es enge Zusammenarbeit und Austausch zwischen Hochschulen und Unternehmen, zwischen Forschern, Entwicklern, Produzenten und Vermarktern, günstige Voraussetzungen für Neugründungen und Wachstum. Dies wiederum verlangt räumliche Nähe, physische Räume wie Labors, Testflächen für Entwicklungen und Experimente, Raum für Pilotprojekte und die Produktion von ersten Serien. Es braucht eine gemeinsam zu nutzende leistungsfähige und flexible Infrastruktur wie thematisch orientierte Technologieplattformen, also Anlagen, die nur zweckmässig und von den Investitionen her verantwortbar sind, wenn mehrere Hochschulen und Teams mit der Wirtschaft sie für ihre Projekte nutzen.

An den bestehenden Standorten der Hochschulen im Raum Zürich gibt es dafür keinen Platz mehr. Für die Nutzung und Ausschöpfung des vorhandenen Potenzials braucht es neue, zusätzliche Flächen. Teile des Areals des Flugplatzes in Dübendorf bieten dafür eine einmalige Chance. UZH und ETH haben für das Projekt in Dübendorf in mehreren Anläufen konkrete Anstrengungen unternommen. Zurzeit stehen chancenreiche Projekte in den Bereichen Robotik und Mobilität im Fokus, Med-Tech und Big Data könnten folgen, aber auch Raum- und Luftfahrt im weitesten Sinn sowie Materialwissenschaft und -technologien. Auch das Zusammenspiel von Luftwaffe, Innovationspark und Werkflugplatz auf dem Flugplatzareal beinhaltet ein grosses Potenzial. All dies bietet auch Chancen für eine Diversifizierung der stark durch Finanzdienstleistungen geprägten Wirtschaftsstruktur im Raum Zürich.
Grosses Interesse

Private Partnerunternehmen gehören zum Kern eines Innovationsparks. Kompetente Firmen haben denn auch wiederholt ein Interesse für Dübendorf gezeigt. Es konnten ihnen bisher jedoch keine attraktiven Angebote gemacht werden. Sie können die an sich verfügbaren Flächen nicht nutzen, weil sie in der Zone für öffentliche Bauten liegen. Die Planungssicherheit hält sich in engen Grenzen.

Mit dem Gerichtsentscheid drohen die bisherigen Anstrengungen aller Beteiligten zu verpuffen. Motivation und Risikobereitschaft der Hochschulen und insbesondere privater Investoren und Partner drohen verloren zu gehen. Die eingereichten Assoziierungsgesuche der Standorte Tessin und Zentralschweiz, die sich an den Standortträger Zürich anbinden möchten, sind infrage gestellt. Das Potenzial kann nicht genutzt werden. Das Gelände, das der Bund im Baurecht dem Kanton abgeben wollte, könnte an den Bund zurückgehen und damit auch die Gestaltungsinitiative darüber. Der Grossraum Zürich, die Schweiz kann es sich schlicht nicht leisten, ein solches Projekt versanden zu lassen. Damit würde nicht nur ein Bedarf verkannt und eine grosse Chance vertan, vielmehr würde auch ein starkes negatives Signal national und international gegeben. Notwendig ist eine klare Verpflichtung der Entscheidungsträger.";https://www.nzz.ch/meinung/der-innovationspark-zuerich-darf-nicht-untergehen-ld.1573757;NZZ;Beat Hotz-Hart;;;
03.07.2020;In Brasilien lautet der Corona-Gewinner Facebook: Sein Chat-Dienst Whatsapp ist die neue Ladentheke der Brasilianer;"Vor den geschlossenen Elektronikläden, den Textilketten und den kleineren Shoppingmalls hängen in ganz Brasilien jetzt oft improvisierte Banner mit dem hellgrünen Telefonsymbol von Whatsapp mitsamt Telefonnummern. Manchmal sind dahinter noch Fotos und Namen aufgeführt. Wer dann etwa eine Nachricht an Anderson oder Sheila von Casas Bahia, einer der grossen Elektroartikelketten in Salvador, schickt, landet direkt bei den Verkäufern der Detailhändler. Dabei geht es nach einigem Hin und Her direkt brasilianisch-persönlich zu – auch wenn nur der Kauf einer neuen Waschmaschine oder ein Küchenmixer verhandelt werden: «Schätzchen, ich würde dir eine verlängerte Garantie empfehlen, du weisst doch, wie schnell die Apparate ihren Geist aufgeben wegen der Salzluft am Meer», schreibt Daiane und verabschiedet sich nach vollendetem Deal mit Küsschen und mehreren Emojis.

Im nun vierten Monat des totalen und teilweisen Lockdown in Brasilien wegen Corona haben sich die Marketing- und Verkaufskanäle im Detailhandel stark verändert. Vor allem die Chat-Plattform Whatsapp (WA) ist dabei zur verlängerten Ladentheke geworden. Die Plattform veröffentlicht keine genauen Nutzerdaten, hat aber in Brasilien mit 120 Mio. Nutzern den weltweit zweitgrössten Standort nach Indien. Laut einer Untersuchung des brasilianischen Verbandes für Automation GS1 Brasil nutzen ein Drittel der kleinen und mittleren Unternehmen in der Pandemie Whatsapp für alternative Verkaufsstrategien, die sonstigen sozialen Netzwerke werden nur von acht Prozent der Unternehmen eingesetzt.

Das ist nicht verwunderlich: Im brasilianischen Alltag wird WA universell und selbstverständlich schon lange kommerziell genutzt. Das gilt für den kleinen Handwerker sowie die Bäckerei bis hin zum Markthändler, die ihre Kunden direkt auf Nachfrage per WA bedienen. Auch Selbständige in der Peripherie und auf dem Land nutzen gerne WA, um ihre Dienste anzubieten: Die Plattform hat den Vorteil, dass man über die Chat-Plattform auch einfach Kurznachrichten daraufsprechen oder Fotos schicken kann, also nicht so versiert beim Schreiben sein muss. Aber auch Makler suchen so nach Interessenten. Kanzleien schicken Schriftsätze an ihre Klienten, Labors ihre Untersuchungen an Patienten oder Journalisten ihre Fragen an Politiker.

Doch in der Corona-Krise haben sich die Dimensionen der WA-Nutzung geändert. Auch Grosskonzerne setzen WA nun strategisch ein. Varejo etwa, die führende Kette des Landes für Konsumgüterartikel mit 1000 Läden, leitet inzwischen einen Fünftel des digitalen Geschäfts über Whatsapp. Im Mai konnte Via Varejo ihren Umsatz im Vergleich mit dem Vorjahr sogar noch um 10% steigern, obwohl da noch 80% der Läden geschlossen waren. «Wir sind zum weltweiten Testfall für Facebook geworden, weil niemand sonst das Instrument im Massenbetrieb so systematisch einsetzt», sagt der operative Direktor Marcelo Ubriaco.

Der Konzern setze auf eine Taktik, bei der Big Data mit der Kaufhistorie des Kunden und mit direkter und personalisierter Kommunikation vermischt werden, beobachtet Paula Soprana, Technologieexpertin bei der Tageszeitung «Folha de São Paulo». Denn auf anonyme Sites reagieren Brasilianer eher zurückhaltend, auf persönliche Ansprache jedoch überproportional schnell. «Niemand lässt eine WA-Nachricht ungeöffnet.»

So bekommen 9000 Mitarbeiter von Via Varejo zwischen dem Amazonas im Norden und den Pampas im Süden vom Mutterhaus jeden Morgen Whatsapp-Listen zugeschickt von potenziellen Kunden in ihrem Bezirk. Von 1,5 Mio. monatlichen Nutzern im Mai vergangenen Jahres ist die Zahl auf nun 11 Mio. Nutzer gestiegen. Knapp die Hälfte der Kunden hat bisher in den Läden gekauft und noch nie online. Die geschlossenen Läden in den Städten sind zu improvisierten Verteilungszentren geworden, damit schneller an die Kunden ausgeliefert werden kann. Das riesige Verteilernetz des Detailhändlers ist logistisch ein grosser Vorteil gegenüber den reinen E-Commerce-Anbietern.

Nach einer Untersuchung des Marktforschers Nielsen haben ein Drittel aller Online-Käufer in Brasilien während der Corona-Krise zum ersten Mal digital gekauft. Die Popularität von Whatsapp zeigt, wie schnell und einfach soziale Netzwerke in einem Schwellenland das Geschäftsmodell einer ganzen Branche verändern können. Denn den Mitarbeitern bleibt auch kaum etwas anderes übrig, als in WA ihre Chancen zu sehen, wenn sie ihren Job nicht verlieren wollen.

Dieser rasch wachsende Online-Markt über Whatsapp schafft Begehrlichkeiten der Wettbewerber. Der WA-Mutterkonzern Facebook wollte eigentlich in Brasilien ein Bezahlsystem über die Chat-Plattform einführen; denn jetzt müssen die Verkäufer ihren Kunden immer noch extra einen Link schicken, der sie zu einer Bezahlseite führt, oder aber die Kunden müssen über eine App bezahlen. Künftig soll das Bezahlen aber per Whatsapp Pay möglich sein. Es werde einfach sein wie Fotos schicken, erklärte Facebook-Eigentümer Mark Zuckerberg in seinem Blog Mitte Juni. Doch er hatte den Widerstand der brasilianischen Finanzindustrie unterschätzt. Nach einer Woche blockierten die Zentralbank und die Kartellbehörden die Zahlfunktion. Die Behörden fürchten, dass die Facebook-Tochter ein Monopol aufbauen könnte.

Auch andere Online-Anbieter versuchen in Brasilien Fuss zu fassen. Der Markt ist verlockend wenig erschlossen. Nach Schätzungen kaufen die konsumfreudigen Brasilianer erst 5 bis 6% ihrer Produkte digital, gegenüber 13% in den USA. Doch Amazon aus den USA (seit 2019 in Brasilien) und Alibaba aus China treffen in Brasilien auf starke lokale Konkurrenz. Das ist einerseits der in ganz Südamerika erfolgreiche Marketplace MercadoLivre, das «südamerikanische Ebay». B2W belegt den zweiten Platz; hinter dem Online-Händler stehen der Brasilien-Schweizer Jorge Paulo Lemann und seine Mitinvestoren der Beteiligungsgesellschaft 3G. Magazine Luiza wiederum gilt als einer der innovativsten Detailhändler der Welt, der bereits lange vor anderen Konkurrenten auf E-Commerce umgestellt hat. Es sieht also nicht danach aus, als würden die ausländischen Konkurrenten den einheimischen Platzhaltern so bald das Terrain streitig machen können. Zumal, wenn sie auf eine Chat-Plattform wie Whatsapp zugreifen und diese in ihre Geschäftsmodelle integrieren können.";https://www.nzz.ch/technologie/whatsapp-brasilianer-nutzen-dienst-von-facebook-auch-kommerziell-ld.1564123;NZZ;Alexander Busch;;;
03.07.2017;Avance an den gläsernen Patienten;"«Eine Anonymisierung medizinischer Daten ist heute nicht mehr möglich», sagt Assistenzprofessorin Franziska Sprecher. Sie ist an der Universität Bern auf Medizin- und Gesundheitsrecht spezialisiert und unter anderem Stiftungsrätin der Patientenschutzorganisation SPO. Als Beispiel nennt sie Blutproben eines Patienten, die dieser zusammen mit seinen weiteren, eigentlich anonymen medizinischen Daten für die Forschung freigibt. De facto, erklärt Sprecher, erlaube dies mit den heutigen digitalen Möglichkeiten ein Zurückverfolgen bis zum Patienten. Möglichst viele Teilnehmende

Dennoch haben am Montag die Schweizerische Akademie der Medizinischen Wissenschaften (SAMW) und die Arbeitsgemeinschaft der Schweizerischen Ethikkommissionen (Swissethics) erstmals eine Vorlage präsentiert, bei der es um die Weitergabe von verschlüsselten und anonymisierten Patientendaten und -proben für die medizinische Forschung geht, auch ins Ausland oder zuhanden kommerzieller Firmen. Um die Chance neuer medizinischer Erkenntnisse und Behandlungsmöglichkeiten effizient zu nutzen, sollen möglichst viele Patientinnen und Patienten in die Verwendung ihrer gesundheitsbezogenen Daten für die Forschung einwilligen, indem sie einen sogenannten Generalkonsent unterschreiben. Für Franziska Sprecher ist dabei klar: Die Sicherheit und Anonymität der Daten lässt sich unter dem geltenden Recht nicht gewährleisten. «Hier vermittelt man dem Patienten eine Scheinsicherheit.»

Die Datensicherheit ist – nicht nur in der Medizin – eine der grossen heutigen Herausforderungen. Gerade im Gesundheitssektor aber hinkt sie der Entwicklung hinterher: Spitäler sind ein bevorzugtes Ziel von Hackern. Die Daten, die in personalisierter Form dem Arztgeheimnis unterstehen, sind besonders sensibel. Auch die SAMW und Swissethics verschweigen die Risiken nicht: Eine strafbare, missbräuchliche Verwendung, etwa durch einen Hackerangriff, könne nicht zu hundert Prozent ausgeschlossen werden. Zudem sei es durchaus möglich, Angaben auf eine bestimmte Person zurückzuführen, wenn grosse Datenmengen aus unterschiedlichen Quellen (Big Data) ausgewertet würden.
Wildwuchs verhindern

Die Frage der Daten(un)sicherheit zeigt auf, in welch heikles, weites und noch wenig bestelltes Feld sich die medizinische Forschung mit der Nutzung von Big Data begibt. Die Chancen sind gross, die Risiken hoch. SAMW-Präsident Daniel Scheidegger hält dazu fest: «Im schweizerischen Gesundheitswesen wird bei vielen Projekten vor allem diskutiert und nichts gemacht. Jetzt ist der Moment gekommen, wo wir in der Praxis die Betroffenen selber fragen und mit dem Kompromissvorschlag einen praktischen Test machen sollten.»

Die Vorlage für den Generalkonsent wird denn auch als eine erste Empfehlung verstanden, um das Vertrauen zwischen Patienten, Ärzteschaft und Forschenden zu stärken und bei den Spitälern einheitliche Voraussetzungen für die Weitergabe der Daten und Proben und für eigene medizinische Studien zu schaffen. Damit soll ein Wildwuchs verhindert werden; mehrere Spitäler in der Schweiz haben bereits eigene Versionen eines Generalkonsents entwickelt. Der nun vorliegende nationale Kompromiss ist aus diesen kantonalen Vorgaben entstanden und Ausdruck unterschiedlicher Ansprüche und Haltungen, die im Rahmen einer Vernehmlassung von über sechzig Institutionen geäussert wurden. Er soll in den Spitälern weiterentwickelt und in Diskussionen, auch mit den Patienten, verfeinert und angepasst werden. Auf breiter Ebene begrüsst wurde das Vorliegen einer schweizweit einheitlichen Vorlage.
Erbanlagen, Blutproben

Grundlage für den Generalkonsent ist das 2014 in Kraft getretene Bundesgesetz über die Forschung am Menschen. Mit einem Formular sollen die Patienten künftig festlegen können, ob sie bereit sind, ihre medizinischen Daten und Proben für nicht definierte künftige Forschungsprojekte zur Verfügung zu stellen. Darunter fallen etwa Analysen zu Erbanlagen, Blut-, Urin- oder Gewebeproben. Als Besonderheit braucht es für zusätzliche Blutproben von 10 Millilitern, die für eine Bio-Datenbank gemacht werden, eine eigene schriftliche Einwilligung.

Die Patienten haben das Recht, den Generalkonsent jederzeit und ohne Begründung zu widerrufen. Finanzielle Entschädigungen sind für sie nicht vorgesehen, auch dann nicht, wenn ihre Daten zur Entwicklung gewinnbringender Produkte beitragen. Ebenso wenig dürfen die Spitäler einen Gewinn mit der Weitergabe von Daten und Proben erzielen.

Letzteres ist auch mit ein Grund, weshalb viele Spitäler darauf aus sind, eigene Datenbanken und Forschungsabteilungen aufzubauen. Auch hier gibt es Bestrebungen zu einer nationalen Koordination: Im Aufbau begriffen ist die Swiss Biobanking Platform in Lausanne, zudem ist das Swiss Personalized Health Network entstanden. Es soll mithelfen, die gesundheitsbezogenen Daten für Forschung und Innovation nutzbar zu machen. Noch zurückhaltend zeigen sich die Schweizer Ärzte gegenüber der Initiative des Weltärztebundes, der 2016 die Deklaration von Taipeh verabschiedet hat und internationale Regeln für die digitale Forschung am Menschen vorantreiben will.";https://www.nzz.ch/schweiz/einwilligung-zur-verwendung-von-patientendaten-neue-medizinische-erkenntnisse-dank-big-data-ld.1304038;NZZ;Jörg Krummenacher;;;
22.06.2020;«Machen wir uns nichts vor: Wir sind im Krieg» – wie Israel auf Corona-Jagd geht;"«Machen wir uns nichts vor: Wir sind im Krieg.» Oren Caspi ist kein Typ, der dramatisiert. In diesem Fall aber sucht er die Zuspitzung, denn «die Lage ist zu ernst». Der 41-jährige, bedächtige Kardiologe vom Rambam-Gesundheitszentrum in Haifa bemüht wie viele seiner israelischen Kollegen bewusst die Metaphern des Krieges, wenn es um Covid-19 geht. Sein Vokabular ist das des Generalstabs, es geht um die Aufspürung und Vernichtung des Feindes. «Wir müssen die Armeekultur in die Spitäler tragen. Wir müssen strategisch denken. Wir müssen weniger reden, die Dinge genauer beschreiben und das Nötige sofort tun. Nur so können wir diese Schlacht gewinnen.»
Mehr wissen, besser helfen

Israel hat viele Schlachten geschlagen seit seiner Gründung 1948, und so ist es verständlich, dass man auch auf die bisher unbekannte virale Herausforderung erst einmal reflexartig militärisch reagiert. Volk und Armee sind sich nah, näher als in der Schweiz, die Rüstungsindustrie ist selbst für die meisten Linken kein Feind, sondern die logische Konsequenz einer seit Jahrzehnten bedrohten Existenz in feindlicher Umgebung. Oren Caspi hat wie alle Israeli Militärdienst geleistet, er ist überzeugter Zivilschützer, und als er im März zusammen mit seinen Mitarbeitern die Corona-Lage analysierte, als er die Apokalypse in der Lombardei sah, das grosse Sterben in Spanien und die ernste Lage in Iran, da griff er zum Hörer und rief bei Elbit an. Elbit ist nicht etwa ein Hersteller medizinischen Bedarfs, sondern die grösste private Rüstungsfirma im Land, ein Gigant auch international gesehen, der grösste Konkurrent der Staatsfirma Israel Aerospace Industries (IAI). In der Schweiz kennt man Elbit gut. Das Unternehmen hat der Armee für eine Viertelmilliarde die Drohne Hermes 900 verkauft.

Caspi sah, dass die herkömmliche Spitaltechnik im Fall von Corona versagte, und er erwartete militärisch rasche Hilfe. Die Patientenzahl in Haifa war dramatisch angestiegen, das Pflegepersonal war überfordert. Mit Pagern, Lautsprechern und Walkie-Talkies kam man im hektischen, «bereits etwas panischen» Umfeld nicht mehr weiter. Der Hauptsitz von Elbit liegt gewissermassen um die Ecke, «wir konnten schnell und unbürokratisch arbeiten», sagt Caspi. Am 24. März kam das Spezialteam des Rüstungskonzerns. Dann wurde eine Woche lang «Tag und Nacht gearbeitet», und Ende Monat war Ex-Teams geboren, ein neues, speziell für Spitäler konzipiertes Kommunikationssystem, basierend auf den Kommando- und Kontrollsystemen für Panzer, Flugzeuge und Gefechtsstellen, die Elbit seit Jahrzehnten herstellt. Grundlage ist eine App, die aufs Handy geladen wird.

Dazu kommen ein Kopfhörer mit Mikrofon und eine grosse Drucktaste, die unter der Schutzkleidung getragen wird und auch mit Handschuhen leicht gefunden werden kann. Das System ermöglicht die simultane Zusammenarbeit diverser Gruppen und sofortigen Zugriff auf alle Daten über Audio und Video. Alle Beteiligten sehen mit einem Knopfdruck, wo sich Mitarbeiter befinden und welche Schritte nötig sind. Notfälle werden rasch erkannt, Hilfeteams sofort zusammengestellt. Sämtliche erhobenen Infos werden verschlüsselt, das Arztgeheimnis ist garantiert. Das Rambam-Zentrum hat bereits komplett auf das neue System umgestellt. Ärzte und Pflegepersonal sind erfreut, das System wird auch an den meisten andern Spitälern Israels eingesetzt. «Es arbeitet makellos», sagt Caspi.

Bedenkt man, dass sich in Grossbritannien laut Angaben von Regierungsbeamten jeder fünfte Corona-Kranke im Spital ansteckte, sind Entwicklungen wie Ex-Teams sicher keine Nebensache. «Im Krieg gibt es keine Nebensachen», sagt Caspi. Er ist froh, dass die Israeli in schwierigen Lagen stets ohne viel Aufhebens zusammenrücken, und er ist überzeugt, dass das Land in der Krise davon enorm profitiert hat. Er ist nicht der Einzige, der so denkt. Haim Delmar, Generaldirektor und Chef der Cyber Division von Elbit, sagt im Gespräch mit der NZZ, seine Firma habe sich mit dem denkbar grössten Enthusiasmus an dieser «grossen nationalen Anstrengung gegen den unsichtbaren Feind» beteiligt. Frühwarnung statt Appeasement

Bevor man sich über solche Pathetik mokiert, tut man vielleicht gut daran, zur Kenntnis zu nehmen, wie Israel im Kampf gegen Covid-19 abschneidet: sehr gut nämlich. Israel hat bisher einen Drittel weniger statistisch erfasste Infizierte und nur rund einen Siebtel so viele Tote wie die Schweiz, und zwar in absoluten Zahlen. Das ist signifikant, denn Israel hat 9,1 Millionen Einwohner, die Schweiz 8,6 Millionen. In beiden Ländern leben viele Alte. Im Gegensatz zur Schweiz ist Israel allerdings auch ein «junges» Land mit vielen Kindern und Jugendlichen, sowohl in der arabischen wie in der jüdischen Gemeinschaft. Viele Politiker, die sich rühmen, die Hand am Puls der Zeit und das Ohr am Herzen des Volkes zu haben, Leute wie Donald Trump also, Jair Bolsonaro oder Boris Johnson, hatten die Gefahr des Virus krass unterschätzt und bemäntelt. Nicht so Benjamin Netanyahu, den man sonst durchaus als ein Mitglied dieses konservativen Herrenklubs bezeichnen kann. Der israelische Ministerpräsident hat von Anfang an eindringlich vor dem Coronavirus gewarnt. Früher als in europäischen Ländern kam in Israel der Lockdown. Anfang März bereits wurden die Grenzen weitgehend gesperrt. In den USA twitterte Donald Trump am 9. März, die gewöhnliche Influenza bringe jedes Jahr Zehntausende um, und kein Geschäft werde deswegen geschlossen – es folgte das Massensterben von New York. In Grossbritannien ordnete die Regierung elf Tage nach dem ersten Todesfall erste «ausserordentliche Massnahmen» wie die Schliessung von Schulen und eine partielle Ausgangssperre an – das Land wurde von Corona hart getroffen.

Israel bezeichnete die Pandemie schon Wochen vor dem ersten Infektionsfall als «Sicherheitsrisiko» und schickte Rückkehrer aus Krisenländern in die Heimquarantäne. Am 19. März rief Netanyahu den nationalen Notstand aus, am 25. März wurden drastische Einschränkungen der öffentlichen Bewegungsfreiheit dekretiert. Mitte April wurde das Tragen einer Maske obligatorisch. Etliche Corona-Hochburgen, meist Orte, in denen Ultraorthodoxe wohnen, wurden gesamthaft isoliert. «Wir testen viel, und wir finden viel», sagte Yehuda Carmeli, Mitglied der Corona-Task-Force, die der Regierung rapportiert. Mittlerweile sind etliche Massnahmen wieder aufgehoben worden, und weitere Lockerungen sind in Aussicht gestellt. Über den Berg ist man nicht. Mitte Juni zeichnete sich im Zuge der Lockerungen eine zweite Welle ab, die sofortige Schliessungen nach sich zog. Beanstandungen an der Regierung gab es, und selbstverständlich gab es Fehler und Versäumnisse. Doch angesichts der Corona-Zahlen wirkten die Rügen der Opposition rasch etwas lächerlich, und als dann auch noch der linke «Haaretz» einräumte, Netanyahu sei ein «guter Krisenmanager», ging den Kritikern der Schnauf aus.
Diese Aufnahme einer Drohne aus Tel Aviv zeigt Zelte auf einem Parkplatz, in denen Corona-Tests durchgeführt werden. Aufs Schlimmste gefasst

Caspi und sein Team hatten sich aufs Schlimmste eingestellt: «Wir erwarteten eine Katastrophe.» Das ist nicht übertrieben. Wir steigen hinab in die Tiefgarage des Rambam-Zentrums und sehen die Hinterlassenschaft einer wahrhaft beeindruckenden Vorsorgeaktion. Über jeder der 3000 Parkbuchten sind Anschlüsse für Elektrizität und Sauerstoff angebracht. Das ganze Parkhaus auf drei Etagen hätte bei Bedarf in eine riesige Corona-Station verwandelt werden können. Aus jeder der 3000 Parkbuchten wäre ein «Zimmer» geworden, abgetrennt durch Planen, ausgerüstet mit allem Erforderlichen. Ein unterirdischer Bettentrakt ist heute noch zu sehen. Es kam dann nicht so weit. Italienische Verhältnisse blieben Israel erspart, heute parkieren wieder Autos in der Garage. Im Rambam-Spital lagen Anfang Juni noch vier Corona-Patienten, alle rechneten mit ihrer baldigen Entlassung. Nicht nur die Spitäler klopften bei Elbit an, auch das Gesundheitsministerium trug seine Wünsche vor. Die Corona-Task-Force Netanyahus befand, es wäre gut, wenn Patienten ohne Körperkontakt untersucht werden könnten. Berichte über hohe Infektionsraten beim Spitalpersonal in Italien und Spanien hatten die Spezialisten alarmiert. Elbit machte sich an die Arbeit und entwickelte E-ReS, eine Gerätekombination, die Puls, Atemfrequenz und Körpertemperatur einer Person mit hoher Genauigkeit ermittelt, und zwar aus drei bis fünf Meter Distanz. Haim Delmar nennt sie lapidar die «Wahrnehmungsbox».

Das Infektionsrisiko ist null, die Notwendigkeit langwieriger Desinfektionsprozeduren entfällt. Noch besser als Ex-Teams zeigt E-ReS, wie verblüffend leicht sich militärische Erkenntnisse auf die Medizin übertragen lassen. Die ersten Tests sind ermutigend. Eine Untersuchung an Hunderten Patienten hat ergeben, dass die erhobenen Daten sogar genauer sind als die mit den üblichen Methoden erzielten. Derzeit werden fünf Wahrnehmungsboxen in zwei Spitälern in Israel getestet.

Allein hat Elbit die Wahrnehmungsbox nicht gebaut. Der hochauflösende Radar, der in der Lage ist, durch Kleidung hindurch Puls und Atemfrequenz zu eruieren, stammt vom Jerusalemer Startup Neteera, ebenso eine ausgeklügelte Software, die mit «intelligenten Algorithmen» die von E-ReS erhobenen Messdaten filtert, speichert, auswertet und auch gleich die angezeigte Therapie empfiehlt. Neteera baut sonst Apparaturen für autonomes Fahren, hier haben sich der militärische und der zivile Sektor zusammengetan. Was die Wahrnehmungsbox kostet, möchte Elbit lieber nicht angeben. Ein Schnäppchen ist E-ReS also nicht.
Ein Tummelfeld für Startups

Hunderte von Firmen haben sich mittlerweile auf das Virus gestürzt und forschen nach Medikamenten und Impfstoffen, Dutzende bieten bereits Lösungen an. Die «grosse nationale Anstrengung», die die Regierung ausgerufen hat, findet tatsächlich statt, und das Beste an ihr ist, dass die gestalterische Energie von unten kommt, nicht von oben. Eugene Kandel ist CEO von Start-Up Nation Central, einer Organisation, die die Investments bei den Neugründungen verfolgt. Zum 72. Jahrestag der Staatsgründung im Mai hat Kandel eine Liste von 72 Firmen präsentiert, die in diesem Frühjahr begonnen haben, Covid-19 zu bekämpfen. Er hätte locker doppelt so viele auflisten können. Von Telemedizin und Patienten-Fernüberwachung über Diagnostik, Tests, Selektionsmethoden und Pflege bis hin zur Prävention sind alle Sparten vertreten. Die Medizin ist quasi über Nacht zu einem neuen, wichtigen Tätigkeitsfeld von Hightech und Cyber-Tech geworden.

Natürlich gibt es Skeptiker. Wer der maschinellen Fernüberwachung von Patienten wenig abgewinnen kann, wer sich, gerade in Zeiten der Krankheit, nach Berührung sehnt, nach Nähe, Zusprache, Trost vielleicht sogar – der wird wenig anfangen können mit dieser schattenlos ausgeleuchteten Welt leise summender Apparaturen, in der der Mensch kaum noch auftaucht. Was lässt sich dem entgegenhalten? Erstens, dass der medizinisch-technische Fortschritt unzählige Menschenleben verbessert und verlängert hat. Zweitens, dass Medizin angesichts der immer höheren Lebenserwartung teurer geworden ist und der Mensch als teuerster Faktor kaum noch zu bezahlen ist. Drittens, dass es so schlimm nicht ist, dass Menschen immer noch da sind und dass die Technik sogar in vielen Fällen dazu beiträgt, dass der Patient zu Hause bleiben kann, eventuell sogar bei seiner Familie.

Der beste Beleg für diese These ist wohl die Firma Datos Health mit Sitz in Ramat Gan, gegründet 2016, also lange vor der Pandemie. Uri Bettesh, CEO von Datos, bietet Spitälern und Health-Maintenance-Organisationen (HMO) eine «Plattform» an, die im Wesentlichen darauf abzielt, dass Patienten so lange wie möglich zu Hause bleiben können. Die Nutzer definieren ihre Bedürfnisse selber und können auch ihre eigenen Apparate verwenden, müssen also keine neuen kaufen. Der Patient ist jederzeit in der Lage, per Video oder Handy mit dem Pflegepersonal zu kommunizieren. Sehr krank und allein darf er natürlich nicht sein. In Israel allerdings ist dieses Szenario selten. Die Familien sind gross, ein intaktes familiäres Umfeld ist häufiger als in Europa, und meist sind technikaffine Kinder oder Enkel da, die gerne helfen. «Never on your own» ist der Werbeslogan von Datos. Der Ausbruch der Pandemie hat, wie Bettesh gegenüber der NZZ unumwunden zugibt, den Geschäftsgang erheblich verbessert. Sämtliche HMO Israels und «fast alle Spitäler» haben das System adaptiert und so dazu beigetragen, dass Tausende Patienten zu Hause bleiben konnten. Das hat die Spitäler entlastet und Infektionen verhindert. Keine Angst vor Hightech

Auf dem Höhepunkt der Pandemie brachte Datos seine Plattform der Heimbetreuung nach Italien. Zwei grosse Spitäler in Mailand übernahmen sie, Bettesh hofft, damit etwas zur Eindämmung der Pandemie beigetragen zu haben. In Italien mussten viele Pflegerinnen und Pfleger, die sich in den Monaten zuvor infiziert hatten, in die Quarantäne. Mit der Plattform von Datos konnten sie laut Bettesh produktiv am Computer weiterarbeiten und Patienten betreuen, statt zu Hause untätig herumzusitzen.

Ein entscheidender Grund dafür, dass die israelische konzertierte Aktion gegen Corona so gut funktionierte, ist die Technikaffinität der Bevölkerung. Angst vor Datenmissbrauch und überbordender Überwachung gibt es. Doch grundsätzlich zeichnen sich die Israeli durch eine zutiefst positive Einstellung gegenüber Technik und ihrer pragmatischen Anwendung aus. Wissenschaft und Forschung sind Felder, in denen sich Juden über Jahrhunderte hervorgetan haben, nicht zuletzt, weil ihnen der Zugang zu anderen Berufen oft verboten war. Einwände dagegen, das Machbare auch zu machen, gibt es weit seltener als in Europa – man denke an die Präimplantationsdiagnostik, die in Israel im Gegensatz etwa zu Deutschland bedenkenlos angewendet wird, genauso wie die extensive genetische Untersuchung oder die In-vitro-Befruchtung. Dass man dem Coronavirus mit allen Mitteln zu Leibe rückt, ist den meisten Israeli intuitiv verständlich.

Was für den «einfachen» Bürger gilt, gilt für grosse Staatsbetriebe erst recht. Amira Sharon ist die technische Direktorin von Israel Aerospace Industries (IAI), verantwortlich für Forschung und Entwicklung in der Technologie. Auch die erfolgreiche Karrierefrau sieht den Kampf gegen das Virus als eine patriotische Pflicht. Israel Aerospace, sagt Sharon, sei «zur Flagge gerufen» worden und dem Ruf gefolgt, «freudig und überzeugt». Spitäler, HMO und das Gesundheitsministerium formulierten ihre Wünsche, IAI lieferte. Hinter Firmen wie Elbit wollte man natürlich nicht zurückstehen, also entwickelte IAI ebenfalls ein Gerät, das aus der Distanz Körpertemperatur, Atemfrequenz und Puls misst.

Enge Zusammenarbeit mit dem Soroka-Spital in Beer Sheva brachte «Cockpit» hervor, ein computerisiertes System, das sämtliche Patientendaten sammelt, nach Indikatoren für Krankheiten sucht und genau wie E-ReS Therapiewege empfiehlt. IAI baut Ventilatoren (was auch Elbit tut), und auf speziellen Wunsch des Yitzhak-Shamir-Spitals südlich von Tel Aviv wurde ein robotisches System entwickelt, das in der Lage ist, Räume innert zwanzig Minuten mit UV-Licht virensicher zu desinfizieren. IAI hofft, ein geringfügig modifiziertes System schon bald an Fluggesellschaften verkaufen zu können.
Funktionierende Lieferketten

Logistische Probleme gab es weder für Elbit noch für IAI. Elbit kann jederzeit auf Massenproduktion umschalten. Allein in Israel stehen drei grosse Fabriken, dazu kommen zahlreiche weitere in Ländern wie Grossbritannien, den USA, Indien, Rumänien und Brasilien. Ähnliches gilt für Israel Aerospace Industries mit seinen über 16 000 Mitarbeitern und einem Umsatz von umgerechnet 3,7 Milliarden Franken, der ausschliesslich in Israel erarbeitet wird.

Beide Giganten des militärisch-industriellen Sektors machen derzeit viel PR-Wind um ihre Teilnahme am Kampf gegen Corona, und hakt man nach, geben sie auch gerne zu, dass sie mit den neu entwickelten Geräten und Methoden gut verdienen wollen. Das ist legitim. Eher zu beanstanden ist, dass sie manchmal den Eindruck erwecken, als seien sie Pioniere an einer völlig neuen Front.

Das ist nicht der Fall. Andere sind in dieser Sparte schon lange unterwegs. Das renommierte Medizinische Zentrum Sheba in Ramat Gan zum Beispiel arbeitet im Rahmen des Programms «HealthSpace 2030» schon seit Jahren mit Unternehmen im Gesundheitssektor, mit Startups sowie mit diversen «Geheimabteilungen» der israelischen Streitkräfte zusammen. Bei Diagnostik und Therapie kommen telemedizinische Techniken zum Einsatz, auch mit Robotern, Überwachungsgeräten und Sensoren, die von Armeezulieferern entwickelt wurden, ist man vertraut. Wo bleibt bei alledem der Schutz der Privatsphäre? Die Angst vor Big State und Big Data ist in Israel genauso präsent wie in Europa. Beobachter aus allen politischen Lagern warnen vor einem unkontrollierten Machtzuwachs der Exekutive und der Behemoths der künstlichen Intelligenz auf Kosten der Demokratie. Bürgerrechtsgruppen und Menschenrechtsaktivisten protestierten heftig, als Netanyahu im März den Inlandgeheimdienst Shin Bet ermächtigte, mit seiner eigenen Überwachungssoftware, die sonst im Kampf gegen Terroristen eingesetzt wird, nach Corona-Infizierten und deren Kontaktpersonen zu suchen.

Zwar fehlte dafür die gesetzliche Grundlage, doch Netanyahu setzte die Verordnung erst einmal per Notrecht und mit dem Plazet des Generalstaatsanwalts in Kraft. Allem Anschein nach hatten die Spione auch einigen Erfolg. Es heisst, der Geheimdienst habe bereits 4000 Personen lokalisiert, die später positiv auf das Virus getestet worden seien. Das wären nach derzeitigem Stand fast 20 Prozent. Im Juni allerdings war Schluss mit der Späherei – vorerst. Nadav Argaman, der Chef von Shin Bet, erklärte, er sehe nicht ein, warum seine Behörde in einer «zivilen Sache» eingesetzt werden sollte. Israel habe die Pandemie unter Kontrolle, man komme auch ohne Shin Bet aus. Argaman empfahl der Regierung stattdessen, Warn-Apps auf freiwilliger Basis wie das von der Tel Aviver Firma GlobeKeeper entwickelte Programm HaMagen anzuwenden. Daraufhin beschloss das «Coronavirus-Kabinett» Netanyahus, das Programm ad acta zu legen. Allerdings kann es sofort wieder reaktiviert werden, sollte sich die Lage verschlimmern. Es ist einen kleinen Eintrag ins Poesiealbum von Verschwörungstheoretikern wert: Eine Exekutive verzichtet auf Machtzuwachs. Freiwillig.

Manche Israeli haben mit Verständnis auf die breiten staatlichen Überwachungsaktivitäten reagiert, andere mit Groll und Argwohn. Ende April demonstrierten 2000 Menschen auf dem Rabin-Platz in Tel Aviv gegen die ihrer Ansicht nach demokratiewidrigen Anti-Corona-Direktiven der Regierung, diszipliniert in Masken – und unter strikter Einhaltung des Sicherheitsabstands, den die Regierung dekretiert hatte. Ein grosses Echo hatte die Demo nicht. Doch die Israeli sind sowieso stets misstrauisch gegenüber der Exekutive, und die Arbeit ihrer Agenturen wird mit Argusaugen verfolgt und meist verdriesslich kommentiert. Die Polizei überprüft die Quarantänebestimmungen rigoros. Sie hat bisher weit über 100 000 Checks durchgeführt, um festzustellen, ob die betreffenden Personen auch tatsächlich zu Hause sind. Dass sie dabei sogar Drohnen einsetzt, finden viele Bürger übertrieben. Der Mossad, der Auslandgeheimdienst, hat bei der Corona-Bekämpfung eine weniger prominente Rolle gespielt als der Shin Bet. Weniger wichtig war sie nicht. In Israel fehlte es wie in vielen Ländern in den ersten Wochen der Pandemie am Nötigsten. Im Medizinischen Zentrum Sheba in Ramat Gan etwa registrierte man bereits im Februar ein Defizit an Ventilatoren, Masken und Tests. Der Generaldirektor des Spitals, Yitzhak Kreiss, traf an einem privaten Anlass den Mossad-Chef Yossi Cohen und erklärte ihm die Notlage. Cohen reagierte prompt. Er informierte das riesige Netzwerk seines Geheimdienstes, worauf schon Mitte März aus allen Weltgegenden 100 000 Test-Kits eingeflogen werden konnten.

Kurz darauf kamen 1,5 Millionen Masken, weitere Tests, Schutzbrillen und Spezialtechnik zur Herstellung von Ventilatoren, die zu diesem Zeitpunkt in Israel noch knapp waren. Um für eine gerechte Verteilung der Güter auf alle Spitäler und alle Regionen zu sorgen, liess Cohen im Sheba-Spital ein Kommandozentrum einrichten. Mit von der Partie waren Mitarbeiter des Verteidigungsministeriums, des Mossad und der «Einheit 81», einer Abteilung des militärischen Aufklärungsdienstes Aman, die den Auftrag hat, die israelischen Soldaten stets mit der neuesten und besten Spionagetechnik auszurüsten. Die Einheit ist angeblich noch «geheimer» als die bereits hingebungsvoll unter Verschluss gehaltene «Einheit 8200», die spezialisiert ist auf das Abhören und Verschlüsseln von Signalen jeder Art.

Kreiss ist ausserordentlich stolz auf seine «Rekrutierung» des Mossad und hat zu bedenken gegeben, dass es in New York dem Mount-Sinai-Spital wohl kaum gelungen wäre, die CIA für ihre Zwecke einzuspannen. Der Mossad schweigt, wie er zu allem schweigt, was er tut. Man kann aber sicher sein, dass es letztlich das Coronavirus selbst war, das Yossi Cohen Gelegenheit zu seiner spektakulären Hilfsaktion gab. Iran ist seit Jahren der Staat, der dem Mossad die meisten Sorgen bereitet. Nachdem Iran schon früh zu einem der Hotspots der Pandemie geworden war, konnte der Mossad im Frühjahr die unmittelbare Bedrohung aus Teheran für ein paar Wochen herunterstufen. Das setzte die Ressourcen frei, von denen Israel nun profitiert.";https://www.nzz.ch/technologie/wie-israel-mit-cyber-tech-auf-corona-jagd-geht-ld.1561196;NZZ;Ulrich Schmid;;;
09.04.2020;Zuerst der Hochflug, nun der tiefe Fall: Immer mehr Unternehmen verbannen Zoom aus ihrem Arbeitsalltag;"Mit steigendem Erfolg kommen die Probleme: Das muss derzeit auch der US-Videodienst-Anbieter Zoom erfahren. Binnen weniger Wochen wuchs die Zahl der Nutzer von 10 auf 200 Millionen pro Tag. Die Software macht es den Nutzern einfach, mit Freunden, Familie und Kollegen in Kontakt zu bleiben. Nun aber gerät sie, die Videokonferenzen mit Screen-Sharing anbietet, wegen Sicherheitsbedenken massiv unter Druck. Einige Unternehmen und Behörden haben Zoom bereits verbannt.

Die wichtigsten Fragen und Antworten in der Übersicht:
Mit welchen Vorwürfen ist Zoom konfrontiert?

Mehrere Behörden, Unternehmen und Einrichtungen weltweit haben massive Sicherheitsbedenken gegen den Videodienstanbieter Zoom erhoben und den mangelhaften Datenschutz beklagt. Konkret häuften sich Beschwerden darüber, dass Fremde in Videokonferenzen eingedrungen und dort unangemessene Inhalte verbreitet hatten, das sogenannte «Zoom-Bombing». Dieser Zugang für Fremde ist vor allem dann einfach, wenn der Link zur Einwahl oder die Konferenz-ID bekannt ist sowie der Einlass vom Organisator nicht passwortgeschützt ist. Bereits Ende März gab das FBI in den USA eine Warnung heraus, nachdem sich Berichte über Störungen während Zoom-Konferenzen gehäuft hatten. Besonders Schulen waren davon betroffen.
Welche unangemessenen Inhalte wurden verbreitet?

Laut FBI wurden Gottesdienste und Schulstunden in den USA mit rassistischen Schimpftiraden oder dem Vorzeigen von Nazi-Symbolen unterbrochen. Bei virtuellen Treffen der Anonymen Alkoholiker wurden Fotos trinkender Menschen eingeblendet. Ebenso wurden pornografische Inhalte und Hassreden in den Zoom-Calls verbreitet. Die «New York Times» fand in dunkleren Ecken des Netzes – aber auch bei Instagram – Gruppen, in denen solche Attacken ausgeheckt wurden.
Was haben die Sicherheitsbedenken bisher ausgelöst?

Als eine der ersten Regierungen hat Taiwan die Nutzung von Zoom von offiziellen Stellen wegen Sicherheitsmängeln untersagt. Zudem haben das Raumfahrtunternehmen Space X sowie die Raumfahrtbehörde Nasa die App bereits aus ihrem Berufsalltag verbannt. Die Bildungsbehörde der Stadt New York City hat die Nutzung von Zoom verboten und wies Schulen an, schnell auf Microsofts Konkurrenzdienst Teams umzusteigen. Weitere Schuldirektionen und Universitäten in Nevada, Florida und Los Angeles haben sich nach mehrfachen Störungen gegen die Anwendung entschieden. Neu hat auch Google die Software auf seinen Arbeitsrechnern für Mitarbeiter blockiert. Auch Singapur verbietet Lehrern Zoom für den Unterricht zu nutzen.
Welche Konsequenzen hat das für das Unternehmen?

Zoom sieht sich wegen Sicherheitslücken und Datenschutzmängeln mit einer Klage konfrontiert. Am Dienstag reichte ein Aktionär im US-Gliedstaat Kalifornien eine Sammelklage ein. In der Klage wird Zoom vorgeworfen, die Qualität des Datenschutzes zu hoch angegeben sowie nicht öffentlich gemacht zu haben, dass der Dienst nicht durchgehend verschlüsselt ist. Die Klage könnte wirtschaftliche Folgen für das Unternehmen haben: Seit einem Allzeithoch Ende März hat die Aktie fast einen Drittel an Wert verloren.
Wie reagiert Zoom auf die Vorwürfe?

Die Kritik an Zoom ist nicht ganz neu. Vor einem Jahr hatte ein Sicherheitsforscher behauptet, eine Lücke entdeckt zu haben, dank der er auf die Videokameras von Zoom-Anwendern zugreifen könne. Die Sicherheitslücke wurde rasch geschlossen, veranlasste aber das Electronic Privacy Information Center (Epic), eine Beschwerde gegen Zoom bei der amerikanischen Handelskommission (FTC) einzureichen. Doch die momentane Kritik an der App ist weitaus massiver. Der Zoom-Chef Eric Yuan spricht von schlaflosen Nächten. «Wenn wir es noch einmal vermasseln, ist es aus», sagte er gegenüber dem «Wall Street Journal». Zoom hat bereits auf die jüngsten Beschwerden reagiert und die Einrichtung von Passwörtern und Warteräumen als Standardeinstellung ausgeweitet. Yuan kündigte an, in den nächsten drei Monaten statt der Einführung neuer Funktionen die Schwachstellen stopfen zu wollen.

Nach Bekanntwerden der Sammelklage gegen Zoom hat das Unternehmen den Cybersicherheitsexperten Alex Stamos als Berater verpflichtet. Stamos ist Professor an der Stanford University und bekannt als ehemaliger Chief Security Officer bei Facebook.
Was sagen Experten dazu?

«Zoom ist bei der Sicherheit bestenfalls schlampig und schlimmstenfalls bösartig», kritisiert der Kryptografie-Fachmann Bruce Schneier in seinem Blog-Eintrag. «Die Verschlüsselung bei Zoom ist schrecklich.» So stellten Forscher am Citizen Lab der Universität von Toronto fest, dass Zoom eine Verschlüsselungsmethode nutzt, die als unzureichend gilt. Das Unternehmen musste auch die Behauptung zurücknehmen, die Daten seien mit Ende-zu-Ende-Verschlüsselung geschützt. Zudem entdeckten die Forscher nach eigenen Angaben schwerwiegende Sicherheitslücken in der Wartezimmer-Funktion der App. Diese ermöglicht es allen Teilnehmern, sich vor dem Meeting in einem virtuellen Wartezimmer zu versammeln. So bestimmt der Organisator, wer dem Meeting beitreten kann und wann. Nutzern rät Citizen Lab, die Funktion derzeit zu meiden und Passwörter für Meetings zu vergeben. Kritik gab es auch daran, dass Nutzerdaten über Server in China geleitet werden. Zoom reagierte auf die Vorwürfe und gab an, das Problem zu beheben.

«Wir denken, dass Zoom eine hochriskante Software ist», sagte ein taiwanischer Regierungsvertreter der NZZ. Howard Jyan ist Generaldirektor der Abteilung für Cyber Security. Er verwies darauf, dass Taiwan seit 2019 alle als unsicher eingestuften Produkte wie Software, Hardware und Dienstleistungen für Behörden und Telekom-Unternehmen verbiete. Taiwan achte sehr darauf, keine Daten nach Festland-China zu senden – aber genau das tut Zoom teilweise.
Was sind Alternativen zu Zoom?

Trotz etlichen Sicherheitsmängeln und Kritik an der Software scheint der Nutzen für Unternehmen und Privatleute derzeit zu überwiegen. Wer jedoch auf Nummer sicher gehen will, verlässliche Kommunikation braucht, Geschäftsgeheimnisse besprechen will oder lediglich seine Kinder schützen möchte, sollte sich überlegen, auf eine andere Software zurückzugreifen. An Alternativen mangelt es nicht: Neben Microsoft Teams, Skype und Google Hangouts/Meet empfehlen Experten offene Videoanwendungen wie Bigbluebutton. Bigbluebutton steht nicht nur unter einer offenen Lizenz, sondern kann auch auf einem eigenen Server oder bei Dienstleistern wie Lern.Link in Deutschland betrieben werden. Eine weitere freie Software heisst Jitsi Meet. Das System, das unter anderem vom Chaos Computer Club verwendet wird, steht für Privatleute auf etlichen Servern im Netz kostenfrei zur Verfügung. Als weitere Alternative zu Zoom hat sich die junge norwegische Softwarefirma Whereby ins Gespräch gebracht. Die Gratisversion dieser Software lässt sich ohne Anmeldung nutzen.";https://www.nzz.ch/technologie/zoom-die-video-software-hat-sicherheitsluecken-ld.1550903;NZZ;Corinne Plaga;;;
07.08.2019;Ein Baseball-Fan wirft bei einem Spass-Event so gut, dass er einen Profi-Vertrag erhält;"Die ungewöhnliche Geschichte des Amerikaners Nathan Patterson beginnt am 15. Juli 2019. Der 23-Jährige besucht gemeinsam mit seinem Bruder ein Spiel des Baseball-Teams Colorado Rockies. Am Rande der Partie versucht er sich in der Speed Pitch Challenge, einem Spiel, bei dem sich die Zuschauer im Werfen messen. Ein Gerät misst die Geschwindigkeit der Bälle. Patterson schleudert den ersten Ball mit 90 Meilen pro Stunde (mph) in Richtung Fangnetz, den zweiten mit 94 mph, den dritten mit 96 mph. Zum Vergleich: Profispieler erreichen durchschnittlich 93 mph (150 km/h).

Der Bruder hält den Moment in einem Video fest und postet dieses auf Twitter. Darunter schreibt er: «Major League Baseball, nehmt meinen Bruder unter Vertrag!» Er ahnt nicht, dass der Wunsch zwei Wochen später in Erfüllung gehen würde.

Patterson hat in der High School Baseball gespielt und wollte Profi werden. Die Leistung habe jedoch nicht genügt, sagte er kürzlich dem Portal mlb.com. Sein Arm sei damals zu schwach gewesen. Er gab den Traum auf und brach die Karriere noch vor dem College ab. Bälle warf er nur noch in der Freizeit.
Mit 23 Jahren plötzlich Profi

Aus Spass wollte er sich vergangenen Sommer mit dem Bruder messen. Auch damals traten sie vor einem Baseball-Spiel in der Speed Pitch Challenge gegeneinander an. Patterson erzielte untrainiert eine Geschwindigkeit von 96 mph. Er konnte es kaum glauben und entschied, noch einmal zu versuchen, Profi zu werden. Er trainierte mit einem Privattrainer und meldete sich auf der App FlatGround an.

Auf der Scouting-App können sich Amateure den Teams präsentieren. Rob Friedman, der Gründer der App, sagte kürzlich zu NBC Sports California: «Viel zu viele Spieler fallen durchs Raster, weil sie etwa zu wenig Geld zur Verfügung haben oder weil sie erst später gut werden und so den Scouts nicht auffallen. Deshalb habe ich die App entwickelt.» Tatsächlich fiel Patterson auf der App bald den Scouts der Oakland Athletics auf. Doch dann bremste ein Schicksalsschlag Pattersons Ambitionen: Er wurde von einem Auto angefahren und verletzte sich am linken Arm.

Patterson trainierte verbissen, erst mit rechts, dann mit beiden Armen. Schneller als erwartet fand er zur alten Leistung zurück. Und so kam es, dass er am 15. Juli in der Speed Pitch Challenge einen Ball mit 96 mph ins Netz schleuderte. Das Video verbreite sich rasch in den sozialen Netzwerken. Und wenige Tage später meldeten sich erneut die Oakland Athletics. Am 1. August unterschrieb Patterson seinen ersten Profi-Vertrag. Kommende Saison spielt er in der Minor League, einer unteren Liga, und irgendwann vielleicht in der Major League. Es passt, dass es ausgerechnet die Oakland Athletics waren, die auf Patterson aufmerksam wurden.
Romantik dank Big Data

Oaklands Vizepräsident Billy Beane, früher Trainer, ist in den USA bekannt für seine unkonventionellen Scouting-Methoden. Er stützt seine Personalentscheide nicht auf Ruf und Renommee, sondern auf Statistik und hat auch schon Starspieler gegen Nachwuchsathleten getauscht. Seine Methode ist erfolgreich. Ihm ist es ab 2000 trotz Mini-Budget gelungen, das Team Jahr für Jahr in die Play-offs zu führen. In den USA eifern viele Beane nach.

Der Autor Michael Lewis hat Beanes Geschichte nacherzählt im Buch «Moneyball. Die Kunst, in einem unfairen Spiel zu gewinnen». 2011 wurde das Buch mit Brad Pitt in der Hauptrolle verfilmt.

Beane wird in den USA gleichermassen verehrt und kritisiert. Es wird ihm vorgeworfen, die statistischen Auswertungen hätten dem Baseball die Emotionen gestohlen. Vielleicht wird die Verpflichtung von Patterson die Kritiker nun etwas milder stimmen. Für einmal hat Big Data für Romantik im Baseballsport gesorgt.";https://www.nzz.ch/sport/baseball-ein-zuschauer-wird-profi-bei-den-oakland-athletics-ld.1500345;NZZ;Claudia Rey;;;
05.06.2019;Darwin mit Daten – was die allgegenwärtige Vermessung mit uns macht;"Der Besuch einer öffentlichen Toilette sagt einiges über den Zustand unserer Zivilisation aus. Seltsame Apparate finden sich mittlerweile in den WC von Einkaufszentren oder Flughäfen: «Wie zufrieden sind Sie mit unserem Service?», steht geschrieben. Dazu vier Knöpfe mit Gesichtern in unterschiedlichen Gemütszuständen – von wütend bis hocherfreut, von dunkelrot bis sattgrün. Was mit den so unspezifisch erhobenen Latrinendaten genau geschieht, erschliesst sich einem ebenso wenig wie der mögliche Erkenntniswert aus dem Verhalten einiger Personen, die tatsächlich – aus welchen Gründen auch immer – irgendwo gedrückt haben.

Man muss kein Kulturpessimist sein, um so etwas albern zu finden, eine läppische Spielerei, ein nerviger Unsinn. Doch genau solche Erhebungen sind ein Symptom für unsere Leistungsgesellschaft, die vernarrt ist in Scores, Likes und Rankings. Alles wird heute vermessen, bewertet, verglichen. Das Phänomen umfasst so Unterschiedliches wie die Ranglisten der Städte mit dem besten Nachtleben, die Benotung von Uber-Fahrern oder die sogenannten Performanzmessungen im Büro und im Schulzimmer. Auch ganz private Dinge werden mit Apps dokumentiert, das eigene Sportverhalten etwa, die Gesundheit, die Gemütslage. Wie steht es um mich? Wie optimiere ich mich? Self-Tracking nennt sich das. Bereitwillig stellen Zeitgenossen dafür ihre persönlichen Daten zur Verfügung.

Die Quantifizierung ist so allgegenwärtig, dass sie von der Wissenschaft längst als gesellschaftlicher Megatrend identifiziert wurde. So beschreibt der Berliner Soziologieprofessor Steffen Mau in seiner glänzenden Studie «Das metrische Wir», wie wir immer stärker zu «Numerokraten» erzogen werden – zu Menschen, die nach statusrelevanten Zahlen gieren. Von der in China geplanten Big-Data-Diktatur namens «Sozialkreditsystem», die konformes Verhalten belohnt und Widerstand bestraft, sind wir zwar noch weit entfernt. Aber auch im Westen etablieren sich derzeit neue datenbasierte Formen sozialer Rangordnung.
Die Magie der Zahlen

Natürlich ist die Quantifizierung der Gesellschaft keine historische Novität. Zahlen und Statistiken werden genutzt, seit es sie gibt. Zum Glück, denn sie haben viel zur Entwicklung der modernen Staatlichkeit und der Wirtschaft beigetragen. Doch die Digitalisierung hat völlig neue Möglichkeiten geschaffen. Schätzungen gehen davon aus, dass sich das globale Datenwachstum alle zwei Jahre mehr als verdoppelt. So werden nicht nur unvorstellbare Massen an Daten generiert und gespeichert, sie lassen sich auch mit immer feineren algorithmischen Verfahren auswerten. Diese «allgegenwärtige Soziometrie» (Steffen Mau) als rein technologisch getrieben zu verstehen, griffe dennoch zu kurz. Sie hat auch mit der modernen Arbeitswelt zu tun, die sich an einem Wertekanon aus Effizienz und Evidenz, Transparenz und Rechenschaftspflicht orientiert.
eistungs- und Zielvereinbarungen, wie sie in der Privatwirtschaft und beim Staat verbreitet sind, wollen kontrolliert sein. Das hat in den letzten Jahren unter dem Label «Qualitätsmanagement» zu einer massiven Ausbreitung von Monitorings, Reports und Evaluationen geführt. Daten über Mitarbeitende und Kunden sind wichtiges Steuerungswissen: Mit ihnen können Arbeitsprozesse verbessert und Zielgruppen präziser adressiert werden. Und natürlich lässt sich Druck aufsetzen. Interne Rankings und Scorings gelten als leistungssteigernd. Schliesslich kratzen schlechte Resultate in diesem Wettbewerb der Zahlen am Selbstwertgefühl der Angestellten, und sie sollen dazu anspornen, es künftig besser zu machen. Sie konditionieren aber auch das Beurteilungsraster der Belegschaft, die sich immer mehr an dem orientiert, was gemessen wird. Oder wie es der ehemalige IBM-Chef Louis V. Gerstner ausgedrückt hat: «People don’t do what you expect, but what you inspect.»

Die Verführungskraft solcher Leistungsmessungen ist immens. Zahlen suggerieren Eindeutigkeit, Exaktheit, Nachprüfbarkeit und Nüchternheit. Wer heute etwas belegen will, argumentiert mit der Faktizität datengesättigter Zahlen. Das hat auch mit einem anderen gesellschaftlichen Trend zu tun: der Risikoaversion, der Absicherung vor Fehlern und der Flucht vor Verantwortung.

Doch so objektiv Zahlen auch scheinen mögen – sie sind es nicht. Quantifizierungen reduzieren die komplexe Wirklichkeit auf einige wenige Indikatoren. Es gibt keinen neutralen, vom Betrachter unabhängigen Wert, der nur gemessen werden müsste. Wert wird immer sozial hergestellt. Deshalb zeigt ein Algorithmus auch nicht, was relevant und wertvoll ist, sondern nur, was dafür gehalten wird.
«Boost your score!»

Das ist die Crux am Vermessungseifer: Welche Indikatoren werden gewählt, was wird miteinander verglichen, und was lässt sich überhaupt sinnvoll quantifizieren? Diese Fragen bleiben angesichts der nackten Zahlen oft unbeantwortet. Der Ökonom Mathias Binswanger warnt deshalb schon seit Jahren vor einer «Messbarkeitsillusion»: Das Problem sei, «dass die heute in Wirklichkeit wichtigen Leistungen sich einer quantitativen Messbarkeit entziehen, da es dort in erster Linie um Qualität und nicht um Quantität geht». Die Versuche, Qualität allein mithilfe quantitativ messbarer Kennzahlen oder Indikatoren abzubilden, hätten zu «perversen Anreizen» geführt.

Damit meint er nicht primär die zusätzliche Bürokratie, die wegen der monströsen «datenbasierten» Berichte, Optimierungssitzungen, Debriefings und Massnahmenpläne wuchert – und Effizienzgewinne sogleich wieder verpuffen lässt. Es geht dem Ökonomen um die Fehlanreize, die durch den gesellschaftlichen Zahlenrausch entstehen. Sie zeigen sich zum Beispiel im Bereich der Bildung.

Internationale Leistungstests wie Pisa, die medial viel Beachtung finden, lassen zwar nur beschränkt Aussagen über das Wissen von Schülern und die Qualität von Schulen zu. Auch gibt es keine brauchbaren Kriterien, mit denen Leistungen von Lehrpersonen auf die Kommastelle genau berechnet werden könnten. Trotzdem zeigt die Vermessung der Bildung Wirkung. Die Lehrer würden regelrecht auf das Einpauken standardisierter Prüfungsaufgaben dressiert, monieren Kritiker. Mit diesem «teaching to the test» sollen die Schüler auf Top-Resultate getrimmt werden. Auch die Universitäten passen mittlerweile ihre Strukturen gezielt denjenigen Indikatoren an, die bei den weltweiten Hochschul-Rankings gemessen werden. Die äusserlichen Kennzahlen werden wichtiger als der komplexe Inhalt – oder plakativ gesagt: Gut auszusehen, zählt heute mehr, als gut zu sein. Das gilt notabene auch für die dort angestellten Wissenschafter. Sie müssen möglichst viel und im Mainstream publizieren, um gute «Impact-Zahlen» zu haben. Die Bedeutung der Forscherinnen und Forscher lässt sich etwa am sogenannten H-Index ablesen, der anhand der Zahl von Zitationen einen Wert ausspuckt. Das fördert mitunter viel Unausgegorenes und Unoriginelles. Auf Online-Netzwerken für Wissenschafter wie Academia und ResearchGate lautet das Motto denn auch: «Boost your score!» Nur was geklickt wird, ist wichtig.
Wo ist der «Underperformer»?

Die im Internet anzutreffende Bewertungsmanie offenbart ein weiteres Dilemma. Auf den Web-Portalen von Amazon bis Tripadvisor ist unsere Stimme heute zwar so gefragt wie nie zuvor. Und diese neue Macht der Laien schafft zweifellos mehr Markttransparenz. Doch wie unabhängig, kompetent und zuverlässig sind die geposteten Urteile und Bewertungen? Können subjektive Eindrücke objektiviert werden?

Bedenken sind nicht nur angebracht, wenn beispielsweise die fachlichen Leistungen von Professoren durch Studierende qualifiziert werden oder diejenigen von Ärzten durch Patienten. Die gängige Noten- und Sternchenvergabe folgt keinen klaren Kriterien und ist mitunter so reduktionistisch-nichtssagend wie die Umfragen auf öffentlichen Toiletten oder den News-Sites von Medien: «Ist dieser Artikel lesenswert? Ja/Nein.»

Die gravierendste Folge der Vermessung des Sozialen zeigt sich aber bei der täglichen Zusammenarbeit. Der Wettbewerb um gute Performance belebt idealerweise die Konkurrenz, sorgt für mehr Effektivität und Effizienz. Wahrscheinlich führt er aber auch zu mehr Konformität und schlimmstenfalls zu einem «Kampf ums Dasein», einem vulgären Darwinismus mit Daten. So hat der einstige General-Electric-Chef Jack Welch bereits in den 1980er Jahren seine Belegschaft nach einer simplen Regel eingeteilt: 20 Prozent der Mitarbeiter sind «Stars», 70 Prozent Durchschnitt und 10 Prozent «Underperformer», die man wieder loswerden muss.

In einer solch kompetitiven Unternehmenskultur sind die Mitarbeitenden ständig damit beschäftigt, ihre persönlichen Scores zu optimieren und zu verbessern – gerade auch gegenüber ihren Arbeitskollegen. Das kann gesundheitliche Nebenwirkungen haben. Was in einer derartigen Bürowelt aber akut gefährdet ist, sind zentrale Produktivitätsmotoren wie Teamgeist, Kreativität und Solidarität, die jedoch ungleich schwerer zu erfassen sind. Weshalb auch jemandem helfen, wenn die eigenen Kennzahlen davon nicht profitieren? Oder missgünstiger: wenn «Konkurrenten» dadurch auch noch glänzen? So droht die Arbeitsleistung insgesamt sogar zu sinken.

Sinnigerweise kann «vermessen» im Deutschen nicht nur bedeuten, etwas genau in seinen Massen festzulegen, sondern eben auch – sich beim Messen zu irren.";https://www.nzz.ch/meinung/darwin-mit-daten-was-die-allgegenwaertige-vermessung-mit-uns-macht-ld.1486846;NZZ;Marc Tribelhorn;;;
15.05.2020;Taiwan hat während der Pandemie viel auf Überwachung gesetzt. Dabei blieb die Privatsphäre auf der Strecke;"Glücklich ist das Land, das in Zeiten der Coronavirus-Pandemie einen Arzt als Vizepremierminister hat, zumal für präventive Medizin. Und glücklich ist das Land, in dem der Vizepremierminister in einem wissenschaftlichen Papier erklärt, wie seine Regierung digitale Überwachung gegen die Verbreitung des Virus einsetzt. Dumm nur, wenn die Regierung das vorher dem Volk kaum erklärt hat.

Das Land ist Taiwan, und der Vizepremierminister heisst Chen Chi-mai. Chen ist in Taiwan der zentrale Mann für all jene Aspekte im Kampf gegen das neuartige Coronavirus, die nicht direkt das zentrale Seuchenkontrollzentrum betreffen. Chen gilt auch als Treiber hinter der atemberaubend schnellen Einführung von neuen digitalen Datenbanken und Überwachungssystemen während der Pandemie.
Keine neuen Coronavirus-Fälle

Taiwan hat Covid-19 so erfolgreich bekämpft wie kaum ein anderes Land. Chinas Nachbar hat insgesamt nur 440 Infektionen und sieben Todesfälle gemeldet. Seit mehr als einem Monat gibt es keine einzige lokale Infektion mehr und seit mehr als einer Woche auch keinen neuen importierten Fall.

Welchen Anteil hat daran die Überwachung? Kritiker finden, deren Nutzen sei kaum erwiesen, und die Regierung sei weit übers Ziel hinausgeschossen. Die Regierung hält natürlich dagegen; sie ist stolz auf ihre digitalen Fähigkeiten. Und doch bestätigt sie mit dem Papier des Vizepremierministers unfreiwillig ihre Kritiker.

Die Anfang Mai veröffentlichte Studie schildert im Detail das Contact-Tracing von Passagieren eines Kreuzfahrtschiffs im Januar. Gäste der «Diamond Princess» besichtigten den Norden des Landes. Als sich nach Abfahrt des Schiffes herausstellte, dass einige Passagiere mit dem Virus infiziert waren, reagierten die taiwanischen Behörden rasch: Sie filterten alle ausländischen Mobiltelefonnummern aus den besuchten Gebieten im entsprechenden Zeitraum heraus, dann schauten sie per Geo-Lokalisierung, welche taiwanischen Mobiltelefone sich zeitgleich in der Nähe aufgehalten hatten. Diese mehr als 600 000 Nummern erhielten Warnnachrichten, sich notfalls in Selbstquarantäne zu begeben.

Die taiwanischen Behörden konnten laut der Studie sogar herausfinden, wie viele dieser gut 600 000 Telefonbesitzer später wegen Atemproblemen – einem verräterischen Symptom von Covid-19 – einen Arzt aufsuchten. Wie das genau geschah, konnten oder wollten zwei Co-Autoren der Studie auf Anfrage der NZZ nicht erklären.

Die nationale Krankenversicherungskarte, die jeder Taiwaner hat, enthält zwar viele persönliche Daten, nicht aber Telefonnummern. Eine Erklärung liefert ein mit den Vorgängen vertrauter Medizin-Professor. Chen Hsiu-hsi von der National Taiwan University sagt am Telefon: «Die Behörden können die Mobilfunknummern mit dem Einwohnerregister verbinden, und auch mit der Datenbank der Grenzkontrolle.» Diese Daten wiederum könne man mit den Versicherungsdaten verbinden.
Welche Daten verwendet die Regierung?

Offenbar über diesen Umweg stellten die Behörden fest, dass die gut 600 000 Telefonbesitzer anteilig seltener Atemprobleme hatten als die taiwanische Gesamtbevölkerung. Daraus schlussfolgern die Autoren der Studie, dass auch Contact-Tracing gegen die Verbreitung von Covid-19 helfen könne.

Dabei ist das unlogisch: Dass die aufgespürten Leute seltener Atemprobleme hatten, ist ja nicht dem Contact-Tracing zu verdanken, sondern eher Vorsichtsmassnahmen wie Selbstquarantäne. Wer zu Hause bleibt, kann sich auch nicht anstecken. Im Fall der «Diamond Princess» hätte das Contact-Tracing also nur dann geholfen, wenn die aufgespürten Leute sich vorher tatsächlich bei Passagieren angesteckt hätten. Dann hätten sie öfter Atemprobleme gehabt – und hätten somit dank Selbstquarantäne die Gesamtbevölkerung nicht anstecken können.

Bemerkenswert ist auch, dass die Regierung es bisher nicht für nötig hielt, ihre Bevölkerung umfassend über die Überwachung zu informieren – und dies nun erstmals in einer englischsprachigen Studie für ein internationales Fachpublikum tut. «In der Vergangenheit wussten wir nur, dass die Regierung Daten der Telekomanbieter nutzt», sagt Ho Ming-syuan von der Taiwan Association for Human Rights. «Aber wir wussten nicht genau, welche Daten sie wie verwendet.»

Ho hat deshalb im April im Namen seiner NGO die Regierung aufgefordert, umfassend Auskunft über alle Überwachungsmassnahmen zu geben. Sein Antrag ist vergleichbar mit Gesuchen nach dem Öffentlichkeitsgesetz in der Schweiz, womit Bürger die Behörden zur Offenlegung interner Dokumente bringen können. Der Menschenrechtler will unter anderem wissen, auf welcher Rechtsgrundlage, mit welchen Technologien und in welchem Zeitraum konkret welche Daten erhoben wurden. Zu all diesen Fragen gibt es in Taiwan oft genug nur Mutmassungen.

Ähnliches bemängelten Mitte April an einem Symposium im Parlament mehrere Professoren für Jus oder Technologie, die Europas strengeren Datenschutz lobten. Eine von ihnen, Chiang Ya-chi von der National Taipei University of Technology, sagt im Gespräch, in einer Notfallsituation wie dem Kampf gegen Covid-19 könne es zwar legitim sein, die Pandemie-Prävention zulasten ordnungsgemässer Verfahren zu priorisieren. Aber wenigstens im Nachhinein müsse die Regierung transparent sein, damit die Öffentlichkeit die Big-Data-Methoden genauer untersuchen könne.
Erinnerungen an die Diktatur

Es sieht nicht danach aus, dass die Regierung nun plötzlich sehr viel transparenter wird. Das kritisieren selbst einzelne Parlamentarier der Regierungspartei DPP – zum Beispiel Chung Chia-pin, der zur Spitze seiner Fraktion gehört und das erwähnte Symposium initiierte. Chung erzählt in seinem Büro, dass er als Student vom autokratischen Regime des Generals Chiang Kai-shek bespitzelt worden sei. Erst kürzlich sah er seine Akte ein – und war geschockt ob all der Details darin über sein Leben. «Eine Regierung sollte dich nicht heimlich ausspionieren», sagt er.

Auch Chung wünscht sich für Taiwan einen Datenschutz analog zur europäischen Datenschutzgrundverordnung (DSGVO). Der Parlamentarier befürchtet nun, dass all die während der Pandemie gesammelten Daten gehackt werden könnten – oder dass sie eben doch anderweitig genutzt werden, etwa zu Werbezwecken verkauft werden. Er fragt sich: «Werden die Behörden die Daten nach der Pandemie löschen?»

Chung Chia-pin weiss, dass er mit seiner Skepsis in Taiwan recht allein steht. Zu erfolgreich ist die Regierung im Kampf gegen Covid-19, zu beliebt ist sie. Chung sieht als Grund für mangelnde Kritik auch traditionelle Erziehungsmethoden. «Taiwanische Eltern sagen ihren Kindern gern: Ich mache dieses und jenes zu deinem Wohl. Aber sie sagen nicht, was sie genau wann machen.» Das führe dazu, dass die Taiwaner glaubten, ihre Regierung kümmere sich um sie wie ihre Mütter. Deshalb hinterfragten sie nicht zu sehr, wie die Regierung handele.";https://www.nzz.ch/technologie/coronavirus-bekaempfung-privatsphaere-leidet-in-taiwan-ld.1556615;NZZ;Matthias Sander;;;
30.07.2019;Capital One erwischt es in der Cloud von Amazon;"«Was steckt in deinem Portemonnaie?», lautet der Werbeslogan des fünftgrössten Herausgebers von Kreditkarten in den USA, Capital One, mit dem das Institut seit Jahren um Kunden buhlt. Seit Montag dürfte deren Antwort zerknirscht ausfallen. Denn die gemessen an ihrer Bilanzsumme laut Aufsichtsbehörden siebtgrösste US-Bank hat eingeräumt, dass sich Hacker bereits im März Zugriff auf Daten von 106 Mio. Kunden und Antragstellern für eine Kreditkarte verschafft haben. «Wo stecken meine Daten?», lautet deshalb die Frage, die die Kunden von Capital One derzeit mehr interessiert.

Es ist bis heute wohl der grösste Cyber-Überfall auf eine US-Bank und das umfangreichste Datenleck in der Branche seit dem Angriff auf Equifax, bei dem 2017 Kreditrating-Daten von knapp 150 Mio. US-Konsumenten gestohlen wurden. Erst in der vergangenen Woche hat sich Equifax mit einer Reihe von Aufsehern und Staatsanwaltschaften in den USA auf einen Vergleich geeinigt, der die Firma insgesamt 700 Mio. $ kostet und sie auf weitere Investitionen für die Verbesserung ihrer Technologie verpflichtet, was nach Einschätzung von Analytikern in diesem Jahr zu einem Verlust von 200 Mio. $ führen dürfte. Der ehemalige CEO Richard Smith ist trotz Daten-GAU freilich längst ehrenvoll und mit intakten Vergütungsansprüchen in den Ruhestand verabschiedet worden.

Capital One rechnet in der Folge des Hackerangriffs mit hohen Kosten. An der Börse wurde der Zwischenfall mit einem Abschlag von bis zu 7% oder gut 3 Mrd. $ Marktkapitalisierung quittiert. Die Aktie des Internetkonzerns Amazon, dessen Cloud-Dienste Capital One nutzt, zeigte sich unbeeindruckt. Bis Ende des nächsten Jahres will Capital One seine Daten ganz in die Cloud von Amazon Web Services migrieren. Bei den «Hackern» handelt es sich nach bisherigen Erkenntnissen nur um eine Person, eine ehemalige Mitarbeiterin von Amazon.";https://www.nzz.ch/wirtschaft/capital-one-erwischt-es-in-der-cloud-von-amazon-ld.1499200;NZZ;Stefan Paravicini;;;
11.02.2017;Wir Zwerge unter Datenriesen;"Leben wir in Filterblasen? Lesen wir nur noch Nachrichten, die unsere Weltsicht bestätigen? Vor sechs Jahren machte der politische Aktivist Eli Pariser diese Ansicht populär. Algorithmen sozialer Netzwerke würden dazu beitragen, dass jeder seine eigene, individualisierte Wahrheit erhalte. Die Folge: Polarisierung. Die These war anschaulich und elegant formuliert, traf den Zeitgeist und wurde ein Bestseller. Sie hatte nur ein Problem: Es gab kaum Belege dafür.

Forscher der Universität Amsterdam fassten letztes Jahr diverse Untersuchungen zum Thema zusammen und kamen zum Schluss, «dass es derzeit nur wenige empirische Belege gibt, die Sorgen um Filterblasen rechtfertigen.» Das Thema schien erledigt. Doch dann wurde Donald Trump zum US-Präsidenten gewählt – und in den Google-Trends passierte etwas Interessantes: «Filter Bubble» wurde gesucht wie nie zuvor. Leben wir vielleicht doch in Filterblasen?
Die Alleswisser

Eine Reihe von Theorien und Konzepten, gemäss denen die Öffentlichkeit im Netz verzerrt informiert oder gar manipuliert wird, macht derzeit wieder mediale Karriere. «Filter Bubbles» sollen polarisieren, «Fake News» in die Irre führen, «Social Bots» falsche Stimmung erzeugen oder «Psychological Targeting» labile Wähler umstimmen. Diese Theorien verfangen auffälligerweise nicht nur beim Publikum der Internetskeptiker. Sie kommen bei Lesern an, die trendige TED-Talks schauen und Facebook oder Twitter als wichtige Nachrichtenquelle nutzen. Doch verleiten Bücher mit Titeln wie «Sie wissen alles» und Berichte über immer neue, verblüffende Beispiele, wozu Big Data, Algorithmen oder Deep Learning bereits in der Lage seien, zu wissenschaftlich klingenden Erzählungen, die leicht ins Esoterische kippen.

Die Mathematikerin Cathy O'Neil fordert in ihrem Buch «Weapons of Math Destruction» (2016), die undurchschaubaren Modelle und Algorithmen aus dem Silicon Valley zu hinterfragen und nicht mehr jedem Versprechen in Formelform Glauben zu schenken. Die Forderung ist richtig. Genauso kritisch muss man allerdings hinschauen, wenn mit mathematischen Theorien und statistischen Auswertungen einfache Feindbilder geschaffen und politisiert werden: Wenn etwa kurzerhand die Likes und Shares von Fake-News-Beiträgen in sozialen Netzwerken gezählt werden, um zu demonstrieren, dass man in einem «postfaktischen Zeitalter» angekommen sei. Oder wenn behauptet wird, der Herkunft bestimmter Meinungen im Netz sei nicht mehr zu trauen, weil offenbar Social Bots Trends und Umfragen zugunsten Trumps oder des Brexit manipuliert hätten. Oder dass mit psychologisch abgestimmter Online-Werbung Leute vom Wählen abgehalten würden. Diesen Theorien liegt meist das Bild eines isolierten Internetnutzers zugrunde, über den man dank Datenspuren alles weiss und den man entsprechend steuern kann. Blickt man allerdings auf die Big-Data-Versprechen (und -Drohungen) der letzten Jahre zurück, ergibt sich ein anderes Bild: wenn es doch nur so einfach wäre.
Dumme Werbung

Naiv gefragt: Wie gut sind Facebook und Google darin, aufgrund von Personalisierung Produkte zu verkaufen? Im Zusammenhang mit der Filterblasen-Frage wiesen die Amsterdamer Forscher um Frederik Borgesius auf die Klickraten von 0,1 bis 0,5 Prozent bei Online-Werbung hin und merkten an, «dass Algorithmen von Unternehmen die Interessen von Leuten nicht besonders akkurat vorhersagen».

Ein Reporter der «Zeit» versuchte neulich herauszufinden, warum er als Nichtautofahrer ständig Opel-Werbung zu Gesicht bekam und ihn auch sonst kaum ein beworbenes Produkt interessierte. Die Antwort war einfach: Google personalisiert kaum – und kann es vielleicht gar nicht so gut, wie stets gesagt wird. Das Unternehmen verdient ausreichend daran, dass massenhaft potenzielle Kunden Produkte googeln.

Aber könnte man die Preise individualisieren? Wenn ein Unternehmen weiss, wer wie viel zu zahlen bereit ist (oder welche Kosten verursachen wird), kann es Preise individuell anpassen. Es muss den Kunden nicht einmal kennen, sondern kann ihn aufgrund von Korrelationen mit anderen einschätzen. Im Sammelband «Big Data Is Not a Monolith» (2016) wird vor den Folgen von diesem «Wenn alles alles enthüllt» gewarnt – etwa wenn ein Versicherter nicht mehr nachvollziehen kann, warum seine Prämie steigt, da sie nicht mehr von seinem Verhalten abhängt.

Welche Gefühle individualisierte Preise auslösen, musste der Online-Spiele-Hersteller Zynga erfahren, als er mit maschinell vergebenen Preisen experimentierte. Die Idee war theoretisch gut. Wer weiss, wie jemand spielt, weiss vielleicht auch, was die Person zu zahlen bereit ist. Zudem schauen Spieler nur auf ihr Display, schien man zu glauben, da fallen ihnen manipulierte Preise nicht auf. Doch auch App-Spieler kommunizieren, online und offline. Eine Ehefrau, die in Farmville plötzlich mehr für die «West Meadow Expansion» zahlen sollte als ihr Mann, reagierte wenig amüsiert und trat in Foren eine Empörungswelle los. Man muss Preisunterschiede begründen können.

Zahlt der User vielleicht mehr, wenn man ihn kurz glücklich macht? (Oder er wählt Trump, wenn man ihm Angst einjagt?) War es nicht Facebook im Jahr 2014 in einem spektakulären Experiment gelungen, das Gefühlsleben seiner User zu beeinflussen? Die Studie wurde kritisiert, weil Facebook mit den Gefühlen von 155 000 Usern gespielt hatte. Das Unternehmen liess sich den Vorwurf wohl gerne gefallen, schien er doch den Einfluss des Netzwerks zu demonstrieren. Oder eher nicht?

Im Experiment wurde die Anzahl Statusmeldungen mit positiven Wörtern reduziert. Mit dem Effekt, dass auch der Prozentsatz an positiven Wörtern in den Status-Updates der Probanden zurückging; jedoch bloss um 0,1 Prozentpunkte im Vergleich zur Kontrollgruppe, von 5,2 auf 5,1 Prozent. Negative Wörter nahmen um 0,04 auf etwa 1,8 Prozent zu. Umgekehrt ging der Anteil negativer Wörter um 0,07 Punkte zurück, und die positiven nahmen um 0,06 Punkte zu, wenn negative Inhalte reduziert wurden. Das ist zweifellos interessant. Aber der Effekt ist gering.

Selbst da, wo sich Menschen von Algorithmen steuern lassen wollen, scheitern diese, sobald die Aufgabe komplexer – sprich: menschlicher – wird. Keine Dating-Plattform konnte je einen Algorithmus entwickeln, der nachprüfbar zuverlässig Paare bildet. An einem Mangel an Daten kann es nicht liegen, denn User beantworten Hunderte, zum Teil intimste Fragen über sich selbst. Die Plattformen mögen darüber klagen, dass ihre User nicht ehrlich seien. Zyniker mögen behaupten, die Plattformen hätten kein Interesse an funktionierenden Algorithmen. Doch im Vergleich zu dem, was zwei Menschen austauschen, wenn sie einander begegnen, ist Big Data schlicht small.

Die Plattformen machen also im Wesentlichen nicht mehr, als möglichst viele Leute ähnlichen Alters aus der gleichen Region chatten zu lassen. Der Rest ist (leicht optimierter) Zufall. So weit die Bilanz nach einem Jahrzehnt Big Data.
Chiffre für die Polarisierung

Die Journalismus-Stiftung Pro Publica versucht derweil die Facebook-Algorithmen nachzubauen. Zweifellos ist es wichtig, dass Algorithmen transparenter werden und es Kriterien für ihre Qualität gibt – vor allem, wenn sie heikle Entscheidungen betreffen. Nur wird die Kenntnis der Facebook- oder Google-Algorithmen an politischen Diskussionen und an der Macht dieser Unternehmen so wenig verändern wie die Veröffentlichung des Coca-Cola-Rezepts an der Vormacht des Getränkeherstellers.

Zurück zur «Filter Bubble»: Natürlich gibt es sie. Wer nicht mehr über soziale Schichten hinweg heiratet, schafft sie sich. Wer in ein Quartier zieht, in dem nur Gleichgesinnte leben, schafft sie sich. Wer in sozialen Netzwerken nur Seiten abonniert, die ihm zusagen, schafft sie sich. Sie zu überwinden, ist mühsam, und Verantwortliche zu benennen, schwierig. Deshalb wird auch bei der nächsten politischen Entzweiung wieder nach «Filter Bubble» gegoogelt werden. Die Filterblase ist keine Theorie mehr, sondern nur noch eine Chiffre für die politische Polarisierung.";https://www.nzz.ch/feuilleton/filterblasen-und-aufgeblasene-thesen-wir-zwerge-unter-datenriesen-ld.144971;NZZ;Tin Fischer;;;
27.03.2018;Online-Manipulation: Es geht nicht nur um Datenschutz;"Beim Cambridge-Analytica-Fall geht es um mehr als nur um die Frage einer «Datenschutzverletzung», wie Facebook-Führungskräfte das definiert haben. Es geht um die Behauptung, dass man mittels Online-Daten menschliches Verhalten algorithmisch vorhersagen und beeinflussen kann, ohne dass die Nutzer sich eines solchen Einflusses bewusst sind.

Mit einer intermediären App konnte Cambridge Analytica grosse Datenmengen von mehr als 50 Millionen Rohprofilen ernten. Mittels Big-Data-Analysen wurden offenbar psychografische Profile erstellt, um anschliessend Nutzer mit passgenauen digitalen Anzeigen und anderen manipulativen Informationen anzusprechen. So sollen Wahlkampagnen auf der ganzen Welt zielgerichtet beeinflusst worden sein. Die Idee ist nicht neu. 2014 führte Facebook zusammen mit Forschern der Cornell University ein psychosoziales Online-Experiment mit mehr als 600 000 ahnungslosen Nutzern durch und modifizierte algorithmisch ihre Nachrichten, um Veränderungen in ihren Emotionen zu beobachten.

Die Ergebnisse der Studie, die in den prestigeträchtigen «Proceedings of the National Academy of Sciences» («PNAS») veröffentlicht wurden, zeigten tatsächlich die Fähigkeit des sozialen Netzwerks, die Nutzer emotional zu beeinflussen (sie glücklicher oder trauriger zu machen), ohne dass diese davon wussten. Das Phänomen wurde als «emotionale Ansteckung» bezeichnet. Wie im Falle von Cambridge Analytica löste die Studie zur emotionalen Ansteckung von Facebook heftige Kritik aus. Experten forderten neue Standards zur Aufsicht und Rechenschaftspflicht in der Social-Computing-Forschung.

Die Folgerung aus beiden Fällen ist, dass Facebooks Datenschutzerklärung mangelhaft ist. Sie erlaubte es 2014 Wissenschaftern, Daten für Forschungszwecke zu verwenden, obwohl «Forschung» zu diesem Zeitpunkt gar nicht in der Datennutzungsrichtlinie des Unternehmens aufgeführt war. Im gegenwärtigen Fall erlaubte sie einer Drittanbieter-App, Daten nicht nur von den Nutzern zu sammeln, die die App heruntergeladen hatten, sondern auch von deren «Freunden».

    Neben dem Datenschutz sind Überwachungsmechanismen notwendig, die es erlauben, auf Ereignisse während des gesamten Lebenszyklus der Datennutzung reagieren zu können.

Der Fall von Cambridge Analytica weist darauf hin, dass Datenschutz allein nicht ausreicht, um Menschen im digitalen Zeitalter ausreichend zu schützen. Die Akzeptanz der Nutzungsbedingungen und Datenschutzrichtlinien ist Voraussetzung für die Nutzung der meisten Online-Dienste, einschliesslich Facebook. Die meisten Nutzer akzeptieren Datennutzungsrichtlinien aber, ohne sie gelesen zu haben. Dies wirft Zweifel auf, ob die Akzeptanz dieser Richtlinien online als Einverständniserklärung genügt.

Es wird zudem nicht jede Benutzervereinbarung legitim, nur weil Benutzer auf «Akzeptieren» klicken. So hat 2012 etwa ein US-Gericht die Verträge eines grossen Online-Händlers für ungültig erklärt, weil sich das Unternehmen in den Nutzungsbedingungen das Recht gab, die Vereinbarung jederzeit einseitig zu ändern. Die Cambridge-Philosophin Onora O'Neill hat argumentiert, dass die Verfahren der Einwilligungserklärung letztlich darauf hinauslaufen müssen, «Täuschung und Zwang zu begrenzen». Nutzer müssten zudem die Möglichkeit haben, ihre Zustimmung zu widerrufen. Online-Dienste, von den Mainstream-Providers wie Facebook und Twitter bis zu den unzuverlässigsten wie Cambridge Analytica, aber scheinen umgekehrt die Kontrolle der Nutzer über die Weiterverwendung der von ihnen erzeugten oder empfangenen Informationen eher zu reduzieren.

Neben dem Datenschutz sind somit auch Überwachungsmechanismen notwendig, die es erlauben, auf unerwartete Ereignisse während des gesamten Lebenszyklus der Datennutzung reagieren zu können. Dazu gehören gerade auch Massnahmen, die verhindern, dass Menschen allenfalls die Kontrolle über ihre Entscheidungen, Emotionen oder Verhaltensweisen verlieren. Der Grossteil des heutigen Online-Ökosystems ist ein Wettrüsten bezüglich des Zugangs zum Unterbewussten: Benachrichtigungen, zielgerichtete Anzeigen, Autoplay-Plugins sind Strategien, um Suchtverhalten zu induzieren.

    Es wird zudem nicht jede Benutzervereinbarung legitim, nur weil Benutzer auf «Akzeptieren» klicken.

Forscher haben bereits ein adaptives Regelwerk gefordert, das die Nutzung von leistungsstarken Neurotechnologien zur Informationsgewinnung und -manipulation einschränken kann. Social Computing zeigt, dass wir nicht unbedingt die Gehirne von Menschen lesen müssen, um ihre Entscheidungen beeinflussen zu können. Es reicht aus, dass die Daten, die sie regelmässig – und oft unwissentlich – online austauschen, gesammelt und analysiert werden. Einer dieser Schutzmechanismen bezieht sich auf die kognitive Freiheit, d. h. auf die Freiheit der Menschen, ihre eigene kognitive Dimension einschliesslich ihrer Vorlieben, Entscheidungen und Überzeugungen selber zu kontrollieren. Kognitive Freiheit unterstreicht das Recht der Menschen auf Selbstbestimmung über ihre Gedanken, Emotionen und Überzeugungen – genau das, was Cambridge Analytica offenbar versucht ausser Kraft zu setzen.

Im vergangenen Jahr haben sich internationale Experten mit der Frage auseinandergesetzt, ob die Demokratie Big Data und künstliche Intelligenz überleben wird. Die Antwort auf diese Frage wird teilweise davon abhängen, wie wir die Freiheit des individuellen Geistes schützen.";https://www.nzz.ch/meinung/online-manipulation-es-geht-nicht-nur-um-datenschutz-ld.1368837;NZZ;Effy Vayena und Marcello Ienca;;;
05.05.2018;Welches politische System verarbeitet die Daten der Bürger für die Bürger besser: Demokratie oder Diktatur?;"Wer die Zukunft kennen will, die uns die Digitalisierung verheisst, liest Yuval Noah Harari (oder empfiehlt ihn wenigstens zur Lektüre). Der israelische Universalhistoriker zählt mittlerweile zu den führenden Intellektuellen der schönen neuen Welt. War er bis vor ein paar Jahren nur einem kleineren Fachpublikum bekannt, hat er sich mit seinen Büchern «Eine kurze Geschichte der Menschheit» und «Homo Deus. Eine Geschichte von Morgen» längst auf die Weltbühne katapultiert. Harari ist ständig unterwegs, gibt Impulsreferate, TED-Talks, Konferenzen. Und wenn er gerade nicht unterwegs ist, lebt er wie ein Gelehrter zurückgezogen in einem Moschaw bei Jerusalem. Dass er nach eigenen Angaben kein Smartphone besitzt, verleiht seinen Thesen nicht weniger Autorität, vielmehr scheint es sie zu beglaubigen.

Hararis Ansatz ist ein globalgeschichtlicher, er zeichnet die Geschichte mit dem ganz grossen Pinsel. Seine Thesen sind kühn, seine Assoziationen verblüffend. Big Data lässt sich nach Harari nur mit Big History erklären. In den Strategieabteilungen von Konzernen wird gerne noch immer so getan, als wäre Digitalisierung ein neuer Sendestandard, dem man mit ein paar neuen Glasfaseranschlüssen und Internetzugang beikommen kann. Dabei sind die Veränderungen, welche die digitale Transformation mit sich bringt, systemisch – und betreffen nicht nur die Unternehmerwelt, sondern auch die institutionelle Demokratie.
Zentralisierung: ein Mehrwert?

Was also hat uns der Mann zu erzählen? Bei einem TED-Talk in Vancouver sagte Harari kürzlich warnend: «Die grösste Gefahr, der sich die liberale Demokratie derzeit gegenübersieht, besteht darin, dass die Revolution in der Informationstechnologie Diktaturen effizienter macht als Demokratien.» Das klingt erst einmal ziemlich reisserisch. Auf den zweiten Blick ist der Gedanke jedoch bedenkenswert: Mit dem Voranschreiten künstlicher Intelligenz arbeiten zentralisierte Datenverarbeitungssysteme effektiver als dezentrale, und mit der Zentralisierung verschaffen sich Diktaturen einen kritischen Vorteil gegenüber Demokratien.

Um das Zitat besser zu verstehen, muss man sich ein wenig in die Lektüre von «Homo Deus» vertiefen. In der Logik des Dataismus, wie ihn Harari modelliert, sind freie Marktwirtschaft und staatlich gelenkter Kommunismus keine konkurrierenden Ideologien oder Institutionen, sondern Datenverarbeitungssysteme wie Bienenvölker oder Bakterienkolonien, die mehr oder weniger schnell sind. Während im Kommunismus sämtliche Daten von einem zentralen Prozessor erfasst werden, der zugleich die Entscheidungen trifft, werden in der freien Marktwirtschaft Daten von einem dezentralen Netz aus Produzenten und Konsumenten verarbeitet. Die Börse ist «das schnellste und effizienteste Datenverarbeitungssystem, das die Menschheit bislang geschaffen hat», schreibt Harari.

Das Ende der Geschichte muss vor diesem Hintergrund neu erzählt werden. «Der Kapitalismus hat den Kommunismus nicht deshalb besiegt, weil der Kapitalismus moralischer war, weil individuelle Freiheiten heilig sind oder weil Gott auf die heidnischen Kommunisten wütend war. Der Kapitalismus hat den Kalten Krieg gewonnen, weil verteilte Datenverarbeitung besser funktioniert als zentralisierte, zumindest in Zeiten beschleunigten technologischen Wandels», resümiert Harari. Der Datenblock des Kommunismus ist vor allem deshalb zusammengebrochen, weil die Informationsverarbeitung ineffizient war.
Die Frage nach der Effizienz

Hariri argumentiert hier im Grunde wie viele liberale Sozialphilosophen in der ersten Hälfte des 20. Jahrhunderts. Ludwig von Mises prophezeite den Untergang des Kommunismus aufgrund des Fehlens eines Preissystems – also einer zeitnahen und umfassenden Datenverarbeitung. Und er sollte bekanntlich recht behalten. Die Planwirtschaft, die effizienter als die Marktwirtschaft zu sein beanspruchte, ging an ihrer eigenen Ineffizienz zugrunde. Aus Ressourcenreichtum wurde eine Mangelwirtschaft.

Wie die Antipoden von Kapitalismus und Kommunismus sind auch die politischen Systeme von Demokratie und Diktatur gemäss Harari Datenverarbeitungssysteme, «konkurrierende Mechanismen zur Sammlung und Analyse von Informationen». Doch hier scheint die zentralisierte Informationsverarbeitung, sprich die Diktatur, einen Wettbewerbsvorteil gegenüber der Demokratie zu haben. «Da sowohl Menge als auch Geschwindigkeit der Daten zunehmen, könnten altehrwürdige Institutionen wie Wahlen, Parteien und Parlamente obsolet werden – nicht weil sie unmoralisch wären, sondern weil sie die Daten nicht effizient genug verarbeiten.» Technologische Revolutionen, schreibt Harari, liefen heute schneller ab als politische Prozesse, und dies führe eben dazu, dass Parlamentarier und Wähler gleichermassen die Kontrolle verlören. Sie seien immer zu spät.
Zu kurz gedacht

Das klingt erst einmal überzeugend, doch halten wir kurz inne. Technologische Prozesse waren schon immer schneller als politische. Schnelligkeit ist kein Massstab für die Akzeptanz und Legitimität demokratischer Entscheide. Das politische System der Demokratie – gerade das halbdirekte in der Eidgenossenschaft – hat sich über die Jahre als leidlich robust erwiesen.

Gewiss, niemand spricht heute mehr über Facebook- oder Twitter-Revolutionen wie beim Arabischen Frühling, sondern vielmehr von Manipulationen. Doch ist es ja nicht so, dass die Demokratie inkompatibel mit der Digitalisierung wäre, im Gegenteil: Island hat 2011 in einem einzigartigen Demokratie-Experiment eine Crowdsourcing-Verfassung auf die Beine gestellt. Und in Estland können Bürger über das E-Residency-Programm auf der Blockchain Ehen schliessen.

Könnte man also nicht die Antithese vertreten, dass Demokratien die besseren, weil funktionaleren Datenverarbeitungssysteme sind? Demokratien haben in diesem Modell zwar einen langsameren Prozessor und verzichten auf Treiber, doch verarbeiten sie als Open-Source-System alle Daten weitgehend diskriminierungsfrei. Vor allem: Der Souverän kann an den prozeduralen Regeln, dem Code, mitwirken. Eine Diktatur dagegen deinstalliert missliebige Programme – und ist durch die Ausschaltung gesellschaftlicher Updates wie ein altes iPhone irgendwann nicht mehr funktionsfähig.

Das Problem besteht eher darin, dass das Betriebssystem der Demokratie von innen heraus durch automatisierte Systeme bedroht wird, die auf immer mehr Festplatten der computerisierten Gesellschaft installiert werden. Algorithmen entscheiden, welche Nachrichten wir sehen, ob der Sozialhilfeantrag bewilligt wird oder ob wir kreditwürdig sind. Diese opaken Entscheidungssysteme konterkarieren nicht nur das Transparenzgebot der bürgerlichen Öffentlichkeit, sie etablieren auch einen autoritären Politikmodus. Algorithmen sind Handlungsanweisungen für Maschinen, die nach einer deterministischen Input-Output-Logik Programmierbefehle ausführen. Muster: Wenn Bonitätswert erfüllt, dann Kreditzusage. Sonst Insolvenz. Oder: Wenn Wahrheits-Score = 50 Prozent + x, dann Meinung. Sonst Zensur.
Der blinde Fleck

Die Möglichkeit, mit Programmiervorschriften die ohnehin schon brüchig gewordenen Verfahrensregeln liberaler Demokratien zu überschreiben und eine grosstechnische Manipulation der Gesellschaft durchzuführen, erscheint nach Facebooks Newsfeed-Experiment durchaus real. Der Konzern hat 2014 in einem riesigen sozialen Experiment den Newsfeed von fast 700 000 Nutzern manipuliert und jüngst in Bolivien, Kambodscha, Guatemala, Serbien, der Slowakei und Sri Lanka den Newsfeed in zwei Nachrichtenströme unterteilt – der als orwellianisch kritisierte Laborversuch wurde nach dem Datenskandal um die Analysefirma Cambridge Analytica eingestellt.

Es scheint also eher einen Systemkonflikt innerhalb von Demokratien zu geben, wie weit man Black-Box-Systeme ermächtigt und ob man sie Transparenzpflichten unterwerfen muss, was für das Funktionieren einer bürgerlichen Öffentlichkeit konstitutiv ist. An dieser Stelle ist in Hararis Theorie ein blinder Fleck.

Sein intellektuelles Verdienst besteht darin, die grossen philosophischen Fragen – Wer bin ich? Was will ich? – in den Kontext der Digitaldebatte einzubetten. Das Ich ist im digitalen Zeitalter ja keine rein subjektive Kategorie mehr, sondern ein Hybrid von Mensch und Maschine, weil Willensbekundungen, etwa Suchanfragen oder Sprachkommandos, instantan mit der Cloud synchronisiert werden und Algorithmen aus der massenhaften Analyse von Daten Präferenzen antizipieren. Von jedem von uns gibt es einen digitalen Doppelgänger, der unsere Vorstellungen, Wünsche und Ideen womöglich präziser abbildet als unser Denkapparat, was den Repräsentationsgedanken der Demokratie auf eine harte Probe stellt.

Harari schreibt: «Der Liberalismus wird an dem Tag zusammenbrechen, an dem das System mich besser kennt als ich mich selbst.» Es wäre der Punkt, an dem wir liberale Praktiken wie Einkaufen oder Wählen künstlichen Agenten überantworten, die rationalere Entscheidungen treffen – und die Demokratie ins Simulatorische abgleitet. «Liberale Gewohnheiten wie demokratische Wahlen werden obsolet werden, denn Google wird in der Lage sein, sogar meine politischen Überzeugungen besser zu repräsentieren als ich selbst», prophezeit Harari. Warum noch wählen gehen, wenn der Google-Algorithmus unsere Parteienpräferenz ohnehin schon kennt?

Der Liberalismus könnte letztlich daran scheitern, dass der mündige Bürger auf seine Freiheit verzichtet und algorithmische Autoritäten einen smarten Paternalismus ins Werk setzen. Das käme einem System gleich, in dem alles planbar ist, weil jeder berechenbar ist. Eine Dystopie, gewiss. Doch wer, wenn nicht Harari, könnte diese schöner erklären?";https://www.nzz.ch/feuilleton/welches-politische-system-verarbeitet-die-daten-der-buerger-fuer-die-buerger-besser-demokratie-oder-diktatur-ld.1382462;NZZ;Adrian Lobe;;;
26.08.2017;Wenn Jungökonomen auf Nobelpreisträger treffen;"Eine solche Gelegenheit bietet sich nur einmal im Leben: 350 Nachwuchsökonomen aus 66 Ländern haben einen strengen Auswahlprozess durchlaufen, um an der diesjährigen Tagung der Nobelpreisträger in Lindau teilnehmen zu können. Über 90 von ihnen dürfen verschiedenen Laureaten ihre Arbeit präsentieren. In einem kleinen Saal, direkt am Bodensee gelegen, stehen nun zehn Ökonomen kurz vor ihrer Präsentation. Darunter sind auch vier junge Leute, die an Schweizer Universitäten forschen. In der ersten Reihe sitzen die Wirtschaftsnobelpreisträger Lars Peter Hansen, Finn Kydland, Christopher Pissarides, Edward Prescott und Christopher Sims.
Faszinierendes «Big Data»

Die Redezeit ist strikt auf sechs Minuten beschränkt. Es geht um die unterschiedlichsten Themen: von der Handelstheorie über die Gesundheitsökonomie bis zu den Auswirkungen digitaler Währungen wie Bitcoins. Die Preisträger geben Anregungen, bringen neue Argumente vor und fragen kritisch nach. Die Redner lassen sich davon aber meist nicht aus der Ruhe bringen und widersprechen gelegentlich sogar.

Eine der Vortragenden ist Veronika Stolbova von der Universität Zürich. Sie verweist auf US-Präsident Donald Trump und wirft die Frage auf, welche Folgen das Pariser Klimaabkommen für Firmen hat. Die bisherigen Untersuchungen hätten gezeigt, dass etwa für Europas Banken kaum Risiken bestünden, die Pensionskassen aber ihr Portfolio genau anschauen sollten. Für die Russin ist es sehr motivierend, dass ihre Forschung einen direkten Einfluss auf das Leben hat. Aus ihrer Sicht könnten sich Wissenschafter auch durchaus mehr zu Wort melden. «Wenn Ökonomen zu einem Thema genügend Evidenz haben, sollten sie sich in die öffentliche Debatte einmischen», sagt sie. Damit könnten sie entscheidenden Einfluss auf die Meinungsbildung nehmen.

Ihre Kollegin Chiara Perillo, ebenfalls von der Universität Zürich, nimmt in ihrem Vortag die Effekte der unkonventionellen Geldpolitik der Europäischen Zentralbank auf die Wirtschaft unter die Lupe. Ökonometrische Methoden mit «Big Data» zu kombinieren, fasziniert sie an ihrer Arbeit. Dass immer mehr Daten frei zugänglich sind und auch immer bessere technische Möglichkeiten bestehen, um diese auszuwerten, verleiht der empirischen Forschung Anziehungskraft.

Dies bedeutet aber nicht, dass sich die theoretische Forschung auf dem Rückzug befindet. Besonders seit der Finanzkrise versuchten Ökonomen, bessere Modelle zu finden, sagt Elisabeth Pröhl von der Universität Genf, die über Markov-Gleichgewichte spricht. Sie ist in einem kleinen Spezialgebiet, einer Mischung aus Ökonometrie, Mathematik und ökonomischer Theorie, tätig. Was machen die Jungen anders als die erfahrenen Ökonomen? Sie versuchten vor allem vermehrt, aus anderen Fachgebieten wie Psychologie oder Physik Methodiken zu übernehmen, sagt Pröhl.

Der Vierte im Bunde, Stefan Legge von der Universität St. Gallen, hat die menschliche Komponente der Handelsbeziehungen untersucht und dafür gemessen, wie die Bevölkerungen in verschiedenen Ländern miteinander verwandt sind. Nach seinen Ausführungen fällt der Handel wesentlich leichter, wenn die Präferenzen auf beiden Seiten ähnlich sind. Daher ist er auch zuversichtlich für die Zeit nach dem Brexit. «Die Nachfrage nach Gütern wird anhalten, und es wird relativ leicht sein, mit britischen Firmen Handel zu treiben, weil wir viele Charakteristiken mit der britischen Bevölkerung teilen», sagt er im Anschluss an die Veranstaltung. Eine Gruppe junger Wissenschafter steht noch zusammen und diskutiert über die Begegnungen an der Tagung, neue Entwicklungen in der Ökonomie und die Karriereaussichten. Bald wird dabei klar, dass die Konkurrenz gross ist und man so ziemlich alles auf die akademische Karte setzen muss, um sich zu profilieren.
Der Traum vom Preis

Und wie sieht es mit dem Traum vom Nobelpreis aus? Die jungen Forscher lachen. Mit Wahrscheinlichkeiten kennen sie sich bestens aus; entsprechend bodenständig und nüchtern fallen ihre Antworten aus. Sie reichen von «Es wäre schon nett» über «Das plane ich nicht ein» bis zu «Schlicht utopisch». Die Ökonomen wollen durchaus Anerkennung für ihre Arbeit finden, verfolgen ihren Weg aber auch aus den gleichen Motiven wie die versammelten Nobelpreisträger, die ihr Leben der Forschung verschrieben haben: weil sie etwas lernen, verstehen und beitragen wollen.

Der Nachwuchs steht bereits auf den Schultern der Laureaten und baut auf deren Erfahrungen und Erkenntnissen auf, um die neuen Chancen zu packen. Wer weiss, vielleicht verschlägt es die eine oder den anderen dereinst doch noch einmal an die Tagung nach Lindau. Und vielleicht wird dann die Farbe des Bandes, an dem das Namensschild befestigt ist, nicht mehr grau, sondern blau sein. An der diesjährigen Tagung tragen nämlich nur die 17 anwesenden Nobelpreisträger ein blaues Band um den Hals.";https://www.nzz.ch/international/lindauer-tagung-der-nobelpreistraeger-was-angehende-top-oekonomen-bewegt-ld.1312796;NZZ;Natalie Gratwohl;;;
20.09.2019;Der brennende Amazonas-Regenwald könnte einem brasilianischen Vorzeigeunternehmen einen Strich durch die Rechnung machen;"Der 38-jährige Leonardo Alencar arbeitet bei Minerva als kommerzieller Direktor einer der grössten Schlachthausketten der Welt. Doch sein Job gleicht dem eines Investmentbankers. Der Nutztierwirt muss Dutzende Märkte im Blick haben. Denn in jedem Markt gelten andere Regeln und Preise: In Brasilien ist Tafelspitz dreimal so teuer wie Filet, weil es eine beliebte Grillspezialität ist. Die Argentinier dagegen legen am liebsten Short Ribs auf den Grill. Alencar importiert deswegen argentinischen Tafelspitz nach Brasilien und brasilianische Rippenstücke nach Argentinien. Därme und Mägen sind in China begehrt, in Europa kaum.
Unter Bolsonaro wurden Schutzmechanismen für den Regenwald aufgelöst

Die Preise reagieren auf Feiertage, Jahreszeiten oder Seuchen. In den USA geht die Grillsaison zu Ende, in Israel beginnt bald das Neujahrsfest, und in China grassiert die Schweinepest. Nicht nur der Dollar schwankt, auch die Maisnotierungen wirken sich auf den Preis von Rindfleisch aus, weil in den USA damit Rinder gefüttert werden. Skandale oder der Druck von Lobbys können Märkte über Nacht verschliessen. «Es geht darum, weltweite Trends und Preisentwicklungen möglichst frühzeitig zu erkennen», sagt Alencar. Es gewinne das Unternehmen, welches am schnellsten die Informationen aus den Märkten verarbeiten und auf veränderte Situationen reagieren könne.

Diese Flexibilität ist jetzt wieder gefragt: Brasilien steht wegen seines brennenden Amazonas-Regenwaldes plötzlich weltweit am Pranger. Unter dem rechtspopulistischen Präsidenten Jair Bolsonaro haben die Brandrodungen wieder deutlich zugenommen. Bolsonaro hat seit seinem Amtsantritt im Januar die institutionellen und rechtlichen Kontroll- und Schutzmechanismen für den Regenwald, die Indigenenreservate und Naturschutzgebiete systematisch aufgelöst. Das ist ein Rückschritt.
Das Handelsabkommen zwischen der EU und dem Mercosur ist gefährdet

Seit 2004 war es Brasilien gelungen, die Brände im Amazonasgebiet jedes Jahr zu reduzieren. Deswegen konnte das Land auch das Klimaschutzabkommen von Paris unterschreiben. Darin verpflichtet es sich, seine Kohlendioxidemissionen bis 2030 um fast die Hälfte zu verringern. Doch die ehrgeizige Zielvorgabe wird Brasilien nun kaum einhalten können. «Eineinhalb Dekaden lang konnten wir die Regenwaldvernichtung Jahr für Jahr herunterschrauben», sagt Adalberto Veríssimo von der renommierten Umweltorganisation Imazon. Doch jetzt sei die Zerstörung der Wälder wieder ausser Kontrolle geraten. Brasilien ist erneut der weltweite Umwelt-Paria, wie zuletzt vor zweieinhalb Dekaden.

Vor allem in Europa steigt die Kritik an Bolsonaros Amazonas-Politik. Die Europäer drohen das erst vor zwei Monaten ausgehandelte EU-Mercosur-Abkommen nicht zu ratifizieren. Im Mercosur sind neben Brasilien Argentinien, Uruguay und Paraguay versammelt – alles grosse Rindfleischexporteure. Mit dem Abkommen dürfen die Südamerikaner künftig bis zu 99 000 Tonnen Rindfleisch jährlich mit nur 7,5% Zoll einführen. Mehr als 40% werden dann aus Brasilien kommen. Kein Zufall, dass allen voran Frankreich und Irland das Abkommen aufkündigen wollen. Die Lobbys der Rinderzüchter dort gelten als besonders stark.
In Europa lassen sich höhere Preise für Rindfleisch erzielen

Minerva kann den schon letztes Jahr gesunkenen Absatz brasilianischen Rindfleischs in der EU bis jetzt ausgleichen. Das Unternehmen exportiert weiterhin Steaks aus seinen Schlachthäusern in Argentinien und Uruguay nach Europa. 20 000 Rinder werden in den 20 Schlachthöfen der Gruppe in Südamerika täglich verarbeitet. Auch in Paraguay und Kolumbien besitzt der Konzern Schlachthäuser.

Bis jetzt gehen nur etwa 13% von Minervas Ausfuhren nach Europa, das für den Konzern ein Nischenmarkt ist. Europa versorgt sich vor allem selbst mit Rindfleisch. Gerade einmal 4% der weltweiten Produktion werden importiert. Dennoch ist Europa ein strategischer Markt für Minerva, denn dort lassen sich höhere Preise erzielen. Es müssen aber auch höhere Standards erfüllt werden. Aus Sicht der importierenden Länder indes ist der Preis nicht entscheidend. Wichtiger sind Kriterien wie Tierwohl, Rückverfolgbarkeit der Herkunft und vor allem: der Schutz des Regenwalds.
Um nach Europa exportieren zu können, müssen etliche Auflagen erfüllt sein

Minerva hat seit langem in den Marktzugang nach Europa investiert. Im agrarischen Westen des Gliedstaates São Paulo, 800 Kilometer von der Küste entfernt, unterhält der Konzern eine Mastfarm und ein Schlachthaus, die für den Export nach Europa zertifiziert sind. Dort werden die Kälber rund 100 Tage gemästet. Viermal am Tag fährt ein Lastwagen im Schritttempo die Fressrinne entlang, um sie mit der Futtermischung zu füllen. Alles, was bei den Farmen in der Umgebung übrig bleibt, wird verwendet: Soja- und Erdnussschrot, Baumwollsamen, Pellets aus Orangenschalen, fermentierte Silage aus Zuckerrohrstengeln. 25 000 Rinder werden hier jährlich nur für Europa gemästet.

In Minervas Schlachthaus José Bonifácio, zwei Stunden entfernt mit dem Rindertransporter, werden die Tiere dann verarbeitet: Die mit Scan-Codes beklebten Rinderhälften bekommen ein blaues C aufgedruckt, wenn sie das Schlachthaus in Richtung Kühlzelle verlassen. Als Steaks, Filets oder Burger verarbeitet, werden sie zum Verschiffungshafen Santos transportiert für die Reise nach Norden. Im Amazonas-Regenwald weiden 40 Prozent der brasilianischen Rinder

Bis Minerva das C auf die Rinderhälften stempeln konnte, musste das Unternehmen über die letzten Jahre ein Dutzend Qualitäts- und Gütesiegel für seine Verarbeitungskette einsammeln: Die nordamerikanische PAACO (Professional Animal Auditor Certification Organization) prüft das Unternehmen in Fragen des Tierwohls. Die Weltbank, die über die Finanztochter IFC am Fleischkonzern beteiligt ist, klopft Minerva ab auf nachhaltige Unternehmensführung und saubere Geschäftspraktiken. Mit Greenpeace hat Minerva 2009 erstmals ein freiwilliges Abkommen abgeschlossen. Darin verpflichtet sich das Unternehmen etwa, keine Rinder zu kaufen, die von abgebrannten Amazonasgebieten stammen oder aus indigenen Schutzgebieten.

Und da liegt das Problem: Im Amazonas-Regenwald und in seinen Randgebieten weiden 40% der brasilianischen Rinder. Dort gelten strenge Regeln für die Branche. Die Rinderzüchter müssen 80% ihrer Flächen als Regenwald stehen lassen und dürfen nur einen Fünftel bewirtschaften. Doch der Staat ist weit weg, niemand kontrolliert das – unter Bolsonaro sowieso nicht mehr.
Greenpeace-Kampagnen zeigen Wirkung

Als sich Minerva dazu verpflichtete, auf die Herkunft seiner Rinder zu achten, zogen auch die anderen grossen Fleischkonzerne Brasiliens Marfrig und JBS mit. Das geschah nicht freiwillig. In Greenpeace-Kampagnen wie «Slaughtering the Amazon» (2009) oder «Grilling away the Amazon» (2015) standen Brasiliens Fleischkonzerne, die ihre Verarbeitungskette zu nachlässig geprüft hatten, plötzlich im Zentrum der Kritik. Abnehmer wie Gucci, Nike oder Adidas verpflichteten sich, kein Leder mehr von Regenwaldflächen zu verarbeiten.

Das Abkommen hielt acht Jahre. Dann wurde der brasilianische Konzern JBS, der grösste Fleischproduzent der Welt, vor zwei Jahren von der Umweltbehörde Ibama erwischt. In zwei seiner Schlachthäuser wurden Rinder verarbeitet, die von Farmen kamen, denen die Umweltbehörde zuvor illegale Brandrodungen nachgewiesen hatte. Greenpeace kündigte entnervt das Abkommen mit den Fleischkonzernen auf wegen der «fehlenden Glaubwürdigkeit in der Verarbeitungskette».
Ein Big-Data-Unternehmen hilft Minerva bei der Herkunftskontrolle

Minerva führt jetzt die Verpflichtungen des Abkommens mit Greenpeace in Eigenregie weiter. Der Konzern will nicht riskieren, Rinder zu verarbeiten, die illegal auf Regenwaldflächen geweidet haben. Minerva verlässt sich deswegen nicht auf den staatlich erhobenen Herkunftsnachweis. Das Unternehmen nutzt Niceplanet, einen privaten Dienst, um das Risiko kalkulierbar zu machen. Das ist ein Big-Data-Verarbeiter, der eine Unmenge unterschiedlicher Informationen nutzt.

Die Angaben der Farmen in den Katasterämtern auf dem Land werden mit denen der Steuer- und Umweltbehörden verknüpft und mit jenen der Agrarbanken und Genossenschaften abgeglichen. Gleichzeitig verfolgt Niceplanet ständig die aktuellen Brandrodungen beim brasilianischen Institut für Weltraumforschung (Inpe): Es wird beobachtet, ob sich Farmen verändern, ob sie an Schutzgebiete angrenzen und ob in der Umgebung Vegetation abgefackelt wurde. Der Dienst stellt sogar Verbindungen zwischen Verwandten her: Wenn ein Rinderzüchter auf den ersten Blick «sauber» arbeitet, aber bei seinen beiden Brüdern in den letzten Jahren Rinderherden in Schutzgebieten registriert wurden, dann erhöht Niceplanet den Wert der Risikoeinschätzung.
Die Nachweisbarkeit ist in Brasilien lückenhaft

Bei Minerva in José Bonifácio kontrollieren die Einkäufer in den gekühlten Büros direkt neben dem Schlachthaus am Computer das Risiko eines Züchters und geben nur ihr Okay für einen Kaufauftrag für eine Herde, wenn die Farm mit einem niedrigen Risiko eingestuft wird. Etwa im Gliedstaat Pará am Rande des Amazonasgebietes, wo in den letzten Jahren am stärksten abgeholzt wurde, sind rund ein Drittel aller Farmen tabu für die Einkäufer.

Greenpeace kritisiert, dass die Nachweiskette von der Geburt des Kalbes bis zum Schlachthof in Brasilien nicht hundertprozentig kontrolliert werde. Die Kälber müssen – je nach Absatzregion – erst einige Monate nach der Geburt mit einem Clip im Ohr registriert werden. Sie können also auf gerodeten Regenwaldflächen aufgewachsen sein. Vom Auditor Grant Thornton hat sich Minerva gerade bestätigen lassen, dass es sich an die Abmachungen hält. Doch auch der private Controller kritisiert im Bericht vom Juli, dass die Kälber vom staatlichen Erfassungssystem erst im Alter von bis zu zehn Monaten registriert werden müssen, die Nachweisbarkeit also lückenhaft ist. «Der Staat müsste hier für eine vollständige Erfassung sorgen», fordert Grant Thornton.
In den USA und in Europa wird mehr grilliert

Bei Minerva ist der Direktor Alencar trotz dem gegenwärtig schlechten Ruf Brasiliens zuversichtlich, dass Südamerika seinen Anteil am weltweiten Rindfleischmarkt ausbauen wird. Denn die aufsteigenden Massen in den Schwellenländern wollen künftig mehr Rindfleisch essen. Doch nicht nur dort. Die OECD erwartet, dass der Rindfleischkonsum auch in den wohlhabenden Industrieländern in den nächsten zehn Jahren um 8% zulegen wird – trotz den «no beef»-Kampagnen und wachsender Kritik am Fleischkonsum.

Minerva rechnet sich als Lieferant aus Brasilien besondere Chancen aus: Denn im tropischen Brasilien dominieren die aus Indien stammenden Nelore-Rinder. Ihr Fleisch ist magerer als das der europäischen Rassen. Zudem sei man in Europa zunehmend bereit, mehr für ein qualitativ besseres T-Bone-Steak aus Südamerika auszugeben, beobachtet Alencar und verweist auf die hiesige Grill-Modewelle mit Fachzeitschriften, Grillkursen und teurem Zubehör. «In Europa und den USA gibt es einen Trend zu grilliertem Premium-Rindfleisch», sagt er. Die Zeiten des geschmorten Sonntagsbratens mit viel Sauce seien vorbei.";https://www.nzz.ch/wirtschaft/verkohlte-steaks-ld.1507685;NZZ;Alexander Busch;;;
19.06.2020;Wenn Forscher Abkürzungen nehmen –oder warum der jüngste Medizinskandal die Forschung stärken könnte;"Das neue Coronavirus hat vieles verändert. Auch den Alltag von Wissenschafterinnen und Wissenschaftern. Selten waren Epidemiologen, Virologen und Ärzte so gefragte Leute wie in den letzten Wochen und Monaten. Wir alle hingen an ihren Lippen, wenn sie zur Lage der Nation sprachen. Was sie sagten, beeinflusste Behörden und Politik, aber auch ganz direkt unser Leben.

In Rekordzeit identifizierten die Forscher den neuen Erreger und entschlüsselten sein Erbgut. Damit standen schon bald diagnostische Tests zur Verfügung. Anhand ihres veränderlichen genetischen Fingerprints konnten die Experten auch rekonstruieren, wie sich die Mikrobe von China aus über die ganze Welt ausbreitete. Fehlten nur noch wirksame Medikamente und eine Schutzimpfung, dann hätte die Wissenschaft alle an sie gestellten Wünsche erfüllt.

Dass eine solche, unter grösstem Druck erbrachte Spitzenforschung auch Gefahren birgt, zeigt jetzt der Skandal um zwei zurückgezogene Medizinstudien aus der Harvard Medical School in Boston, USA. In der einen Publikation war der Einfluss von gängigen Blutdrucksenkern auf den Verlauf bei Covid-19 untersucht worden. Die andere (mit Zürcher Beteiligung) ging der Frage nach, ob schwer erkrankte Covid-19-Patienten von alten Malariamitteln profitieren. Präsident Trump hatte die Idee schon vor Monaten als «game changer» in der Bewältigung der Pandemie empfohlen. Er tat dies in seiner Art als gewiefter Verkäufer, der sich mehr für grosse Emotionen denn für kleinliche Fakten interessiert.
Geschönte oder sogar fabrizierte Datenbasis

Die Schadenfreude dürfte daher bei vielen gross gewesen sein, als die Harvard-Studie den Präsidenten widerlegte. Doch schon bald kam der dringende Verdacht auf, dass beide Arbeiten auf geschönten, wenn nicht sogar fabrizierten Daten basieren. Diese stammen von einer kleinen, unbekannten Firma namens Surgisphere aus Chicago. Das Unternehmen will die Patientendaten von Hunderten von Spitälern auf der ganzen Welt bezogen und analysiert haben. Was unbeteiligten Forschern sofort auffiel: Die Datensätze waren zu perfekt, um wahr zu sein. Zudem gab es viele Ungereimtheiten. So wiesen die Daten für Australien mehr Covid-19-Todesoper aus, als es in dem Land tatsächlich gab.

Die ganze Welt fragt sich nun: Warum sind die gegen den Himmel stinkenden Probleme den Studienautoren und den Verantwortlichen der Fachzeitschriften nicht aufgefallen? Die Arbeiten sind schliesslich im «Lancet» und im «New England Journal of Medicine» erschienen. Das sind zwei der ältesten und weltweit angesehensten Medizinjournale. Hauptsache, die Probleme seien entdeckt worden, könnte man einwenden und die Angelegenheit wieder vergessen. Der rasche Rückzug der Studien zeigt ja, dass das Forschungssystem funktioniert – und das selbst in Zeiten, wo die Zahl der Publikationen in Fachjournalen und auf sogenannten Preprint-Servern (s. Grafiken) massiv zugenommen hat.
Seit Januar sind immer mehr Preprints zum Coronavirus erschienen Wer so argumentiert, verpasst allerdings die Chance, aus dem Debakel zu lernen und das System zu verbessern. Denn das jüngste Fehlverhalten, das jetzt im Detail aufgearbeitet werden muss, wirft seinen Schatten auf altbekannte Problemzonen in der Medizinforschung. An erster Stelle sind die Studiendaten zu nennen. Sie stehen im Zentrum vieler Betrugsfälle. Das ist nicht erstaunlich, beginnt doch mit den beim Patienten erhobenen Rohdaten jede Studie. Wer diese Daten besitzt, hat die Deutungshoheit über die Forschungsarbeit. Dies auch deshalb, weil sich mit genügend Daten und der «richtigen» statistischen Analyse die unterschiedlichsten Ergebnisse gewinnen lassen.

Daten können zudem manipuliert werden. Meist ist der Fall aber subtiler: Die Daten werden unter Verschluss gehalten statt mit anderen Forschern und der interessierten Öffentlichkeit geteilt. So geschehen beim Skandal um das Grippemittel Tamiflu von Roche. Die Firma liess nur diejenigen Daten als Studien veröffentlichen, die ihre kommerziellen Interessen unterstützten. So konnte sie während Jahren behaupten, Tamiflu reduziere bei Grippepatienten die Komplikations- und Hospitalisationsrate. Dass das falsch war, hat erst eine firmenunabhängige Untersuchung an einer breiteren Datenbasis nachgewiesen. Diese musste in einem jahrelangen Rechtshandel erstritten werden.
Patientendaten öffentlich zugänglich machen

Wegen solcher Skandale fordern namhafte Wissenschafter schon lange, dass klinische Patientendaten in anonymisierter Form öffentlich zugänglich gemacht werden sollten. So können sie bei Bedarf von unabhängigen Forschern überprüft werden. Auch wenn es bei diesem Thema in den letzten Jahren Fortschritte gab, ist die Forderung noch lange nicht umgesetzt. Wobei nicht nur die kommerziellen Firmen in der Pflicht stehen, ihre Patientendaten mit anderen zu teilen, sondern auch viele akademische Forscher. Möglicherweise liesse sich die Situation verbessern, wenn Sponsoren wie der Nationalfonds ihre finanzielle Unterstützung an eine vollständige Datentransparenz knüpfen würden. Auch die Fachzeitschriften könnten hier mehr Druck machen.
Bereits mehr als 14 300 Preprints zum Coronavirus und zu Covid-19 Das Versagen von «Lancet» und «New England Journal of Medicine» beim Publizieren der zurückgezogenen Studien hat auch den sogenannten Peer-Review-Prozess in Misskredit gebracht. So wird die Überprüfung der eingereichten Arbeit durch ausgewiesene Fachkollegen bezeichnet. Nur wenn diese Experten grünes Licht geben, wird die Studie veröffentlicht. Dieses Verfahren zur Qualitätssicherung hat sich seit Mitte des letzten Jahrhunderts in den ernstzunehmenden Journalen durchgesetzt. Es jetzt wegen einzelner Verfehlungen abzuschaffen, wie das einige Stimmen fordern, hiesse, das Kind mit dem Bad auszuschütten. Besser ist es, den Prozess an neue Entwicklungen in der Forschung anzupassen. So zeigen die jüngsten Probleme bei renommierten Fachzeitschriften, dass bei klinischen Studien vermehrt Experten für grosse Datenmengen (Big Data), Studiendesign und statistische Analysen zur Überprüfung beigezogen werden müssen. Auch könnte es bei besonders komplexen Studien sinnvoll sein, die Arbeit vor der Publikation auf einem Preprint-Server zur öffentlichen Begutachtung aufzuschalten.

Ein weiteres Problemfeld betrifft die Sanktionen, die Forschern bei Fehlverhalten drohen. Sie sind in der Schweiz zu wenig klar und uneinheitlich geregelt. So ist es Aufgabe der Universitäten und Institutionen, ihre fehlbaren Angestellten zu bestrafen. Das kann dazu führen, dass das gleiche «Delikt» an der einen Uni zu einer Verwarnung, an einer anderen zur Entlassung führt. Diese Ungerechtigkeit liesse sich verhindern, wenn die Bestrafung von einem überregionalen Gremium übernommen würde.

Für eine solche Struktur spricht, dass auch die Richtlinien zum Thema wissenschaftliche Integrität eine nationale Basis haben. Diese hat die Schweizerische Akademie der Medizinischen Wissenschaften (SAMW) 2002 gelegt. Seither sind die Spielregeln relativ klar – «relativ», weil es gerade bei geringeren Vergehen stets einen beträchtlichen Ermessensspielraum gibt. So etwa bei Fragen wie: Haben die Autoren einer fehlerhaften Studie ihre Verantwortung zur Überprüfung der Arbeit genügend wahrgenommen? Oder haben sie durch ihr passives oder fahrlässiges Verhalten das Fehlverhalten anderer erst ermöglicht? Nicht weniger heikel ist die Frage, ob jemand genügend eigene Arbeit in ein Forschungsprojekt gesteckt hat, um als Autor auf der Publikation zu erscheinen.
Wissenschaft und Sport haben viele Parallelen

Wie im Sport ist auch die Wissenschaft darauf angewiesen, dass ihre Mitglieder fair spielen. In beiden Disziplinen wird es aber immer Leute geben, die versuchen, eine Abkürzung zu nehmen, sei das mit Doping oder mit unlauteren Forschungsmethoden. Dazu sagte der Berner Anatom und ehemalige SAMW-Präsident Ewald Weibel einmal: «Forschungsbetrug ist so alt wie die Forschung selbst. Denn Forschen ist eine menschliche Tätigkeit und deshalb allen menschlichen Tugenden und Untugenden ausgesetzt.» Um den Anteil der unsauber arbeitenden Forscher möglichst klein zu halten, müsse man diese «zur Wahrhaftigkeit erziehen». Für eine solche Erziehung braucht es neben klaren Regeln und verbindlichen Konsequenzen bei Grenzüberschreitung auch glaubwürdige Vorbilder.

Wissenschaftliches Fehlverhalten wird meist von zwei Faktoren getrieben: Geld und dem Ego einzelner Forscher. Beides steht den Zielen der Wissenschaft diametral entgegen. Hier geht es um Erkenntnisgewinn zum Nutzen der Allgemeinheit. Welchen Schaden Forschungsbetrug anrichten kann, zeigt beispielhaft die Wakefield-Studie, die eine angebliche Verbindung zwischen Masernimpfung und Autismus nachwies. Obwohl die «Lancet»-Arbeit 2010 aus dem Verkehr gezogen wurde, ist der falsche Zusammenhang bis heute nicht totzukriegen. Wissenschaftliches Fehlverhalten schadet somit nicht nur dem Ansehen und der Glaubwürdigkeit der Wissenschaft. Es spielt auch antiwissenschaftlichen Kreisen und den Promotoren von Verschwörungstheorien in die Hände.

Beim Thema «Forschungsbetrug und wie er zu verhindern ist» spielen aber auch Medien und Gesellschaft eine wichtige Rolle. So kann das ewige Gerede vom einzelnen «Starchirurgen» oder «Topvirologen» – selbst den «Topterroristen» gibt’s – schwache und narzisstische Persönlichkeiten verderben und die Bodenhaftung verlieren lassen. Welches Doppelspiel einige Pressetitel bei ihrer Berichterstattung an den Tag legen, hat das verbale Gefecht der deutschen «Bild»-Zeitung gegen den «Starvirologen» Christian Drosten gezeigt. Noch lieber als Helden sind dem Boulevard offenbar gefallene Helden.

Statt einzelne Ausnahmekönner zu Stars zu küren und ihnen den roten Teppich auszurollen, sollte in der Medizin und Forschung stärker der Teamansatz betont werden. Denn nur so lassen sich diese Bereiche sicherer und vertrauenswürdiger machen. Dass das möglich ist, hat die Luftfahrt mit ihrer eindrücklichen Sicherheitskultur vorgemacht. Dazu passt auch, dass man mit der Swiss oder mit Lufthansa fliegt und nicht mit dem «Starpiloten» X. Y.";https://www.nzz.ch/meinung/medizinskandal-warum-er-die-forschung-staerken-koennte-ld.1561721;NZZ;Alan Niederer;;;
19.04.2020;Bewegung nur noch in einem 50-Meter-Radius erlaubt: wie Taiwans Handy-Überwachung funktioniert;"Als der Akku von Milo Hsiehs Handy an einem Sonntagmorgen im März den Geist aufgab, stand kurz darauf die Polizei vor der Tür. Hsieh ist Student. Er war gerade aus Europa nach Taiwan zurückgekehrt, weshalb er wie alle Einreisenden präventiv in Quarantäne musste. 14 Tage musste er zu Hause bleiben, rund um die Uhr. Damit er sich daran hielt, überwachten die Behörden den Standort seines Handys, und damit er nicht ohne Handy die Wohnung verliess, riefen sie ihn jeden Tag zwei Mal an, jeweils zu einer anderen Zeit.

An jenem Sonntagmorgen war der Akku um 7 Uhr 30 leer, wie Milo Hsieh am Telefon erzählt. Um 8 Uhr 15 schaltete Hsieh sein Handy wieder an. Er hatte vier verpasste Anrufe von den Behörden. Fünf Minuten später war die Polizei da. Seine Mutter ging zur Tür und sagte den Beamten, dass ihr Sohn daheim sei; die Polizisten zogen ab. Hsieh staunt noch immer: «Für mich ist es beunruhigend zu wissen, dass die Behörden dich so sehr überwachen.»
Überwachungssystem soll exportiert werden

Taiwan hat es auch dank dem Einsatz neuer Technologie geschafft, die Zahl der Coronavirus-Infektionen auf derzeit 422 zu beschränken. Das Land hat in kürzester Zeit eine Reihe von Datenbanken miteinander verknüpft und neue Systeme eingeführt, über die oft recht wenig bekannt ist. Eines davon ist das «Überwachungssystem elektronischer Zaun» für Leute in Quarantäne. Diese Plattform will Taiwan anderen Ländern zur Verfügung stellen, wie Präsidentin Tsai Anfang April ankündigte. Am selben Tag sah die NZZ den «elektronischen Zaun» live in Taipeh. Eingeladen hatte Taiwans grösster Telekom-Anbieter, Chunghwa Telecom, der das System im Auftrag der Regierung entwickelt hat und nun betreibt. Die Benutzeroberfläche sieht aus wie Google Maps – nur dass farbige Punkte nicht Hotels oder Restaurants anzeigen, sondern Personen in Quarantäne. Landesweit waren das bei der Präsentation 45 575 Leute. Dazu kamen 2825 Personen in ebenfalls 14-tägiger «Selbstisolierung», weil sie mit Infizierten in Kontakt gewesen waren. Wie auf einer Internet-Karte kann man jede dieser Personen heranzoomen, bis man genau sieht, wo sie sich gerade aufhält. Die allermeisten Icons waren bei der Präsentation blau. Das heisst, dass die Handys dieser Leute sich am Quarantäne-Ort befanden, meist in ihrer Wohnung, vereinzelt in «Quarantäne-Hotels» oder einem Regierungszentrum. Aber in Neu-Taipeh war ein Icon rot umrandet – diese Person hatte sich verbotenerweise rund fünf Kilometer entfernt.

Ein Mitarbeiter von Chunghwa Telecom klickte auf das Icon. Es öffnete sich ein Fenster mit Name, Handynummer und Wohnsitz jener Person. Solche Daten müssen alle Einreisenden an den taiwanischen Flughäfen angeben, viele tun das per QR-Code und App. Der Grenzschutz gibt die Daten weiter an die Zentren für Krankheitskontrolle (CDC), welche die Virusbekämpfung mit weitreichenden Befugnissen steuern. Die CDC verifizieren die Daten, verknüpfen sie unter anderem mit der Datenbank des Nationalen Gesundheitssystems und aktualisieren die Liste mit den Leuten in Quarantäne. Der elektronische Zaun schliesst sich. Im engeren Sinne meint dieser Zaun den Radius, innerhalb dessen ein Mobilfunkmast sendet. Diesen Zaun darf eine Person in Quarantäne nicht überschreiten. In dichten Grossstädten wie Taipeh betrage der Radius im Schnitt 50 Meter, sagt Michael Lee, der Chef der Abteilung für Big Data bei Chunghwa Telecom. «Wenn du nur schon dein Haus verlässt, wird dein Mobiltelefon sich bei einer anderen Basisstation registrieren.» Dann sendet das System eine Warn-SMS an die Person und die Behörden. Reagiert die Person nicht, rückt die Polizei aus.
GPS-App soll kommen

Allerdings ist die Ortung per Funksignal recht ungenau. In etwa einem Prozent der Fälle gebe es falschen Alarm, sagt Lee. Dann rückt die Polizei womöglich umsonst aus. Deshalb soll der elektronische Zaun demnächst um GPS erweitert werden, das auf fünf Meter genau ist. Die Installation einer entsprechenden App soll für Leute in Quarantäne freiwillig sein, sagt Lee. Damit möglichst viele Leute die App herunterladen, soll sie Informationen zu Spitälern und Essenslieferdiensten sowie ein Spiel enthalten.

Die staatliche Nutzung von GPS-Daten ist heikel. Im Westen lehnen viele Leute diesen Eingriff in die Privatsphäre ab, weshalb Behörden beteuern, GPS nicht nutzen zu wollen oder zumindest geplante Apps nur freiwillig zu verbreiten. In Polen und Israel nutzen die Regierungen jedoch GPS zur Bekämpfung des Coronavirus, und in Asien tun das etwa China und Südkorea.

Auch in Taiwan gibt es Bedenken. Die Gesundheitsbehörde CDC kündigte schon Mitte März die Einführung von GPS an, Chunghwa Telecom sprach von Anfang April, doch die App ist immer noch nicht da. Eine Vertreterin des Gesundheitsministeriums erklärte kürzlich gar an einem Symposium im Parlament, bisher sei bewusst auf GPS verzichtet worden. Der Regierungsbeauftragte für Cyber Security, Howard Jyan, sagt der NZZ über die geplante App: «Wir kümmern uns derzeit um die Datenschutz-Probleme. Solange diese Probleme nicht geklärt sind, werden wir den Dienst nicht nutzen.»

Der Datenschutz ist in Taiwan generell geringer als etwa in der EU. Derzeit wird er teilweise praktisch ausgesetzt, unter Verweis auf gesetzliche Ausnahmen zur «Förderung des öffentlichen Interesses» und zum Schutz von Leib und Leben. Trotzdem beteuert Howard Jyan, dass gewisse Vorkehrungen gelten: Nur der Gesundheitsminister habe vollen Zugang zum System elektronischer Zaun; lokale Behördenmitarbeiter sähen lediglich die Quarantäne-Fälle in ihrem Gebiet. Beim Betreiber Chunghwa Telecom haben nach eigenen Angaben drei Personen Zugriff auf das System.

Laut Howard Jyan werden die persönlichen Daten der Betroffenen 14 Tage nach Ende der Quarantäne gelöscht. Nach der Pandemie werde das gesamte System gelöscht. Durch Audits solle sichergestellt werden, dass keine Behörde eine Kopie behalte. Aber: «Technisch ist alles möglich», gesteht Jyan.
Deutsche Telekom bestreitet Kontakt

Das taiwanische Know-how hat international Interesse geweckt. Israels Premierminister Netanyahu lobte öffentlich das Quarantäne-System, und die USA und Tschechien haben mit Taiwan zur Virusbekämpfung auch eine technologische Kooperation vereinbart.

Laut Chunghwa Telecom erkundigte sich zudem die Deutsche Telekom bei dem Unternehmen, wie der elektronische Zaun funktioniere. Beide Anbieter sind Mitglied im internationalen Telekom-Forum «hi-H program». Laut Chunghwa hielten die Deutschen das System für nicht konform mit dem EU-Datenschutz – und wollten sich angeblich auf europäischer Ebene für eine Ausnahme während der Pandemie einsetzen. Die Deutsche Telekom bestreitet, Chunghwa überhaupt kontaktiert zu haben. Ein Sprecher teilte mit: «Die EU ist bestrebt, eine Lösung auf Grundlage der geltenden Gesetze zu finden. Und dem können wir nur zustimmen.»";https://www.nzz.ch/technologie/wie-taiwans-handy-ueberwachung-funktioniert-ld.1551839;NZZ;Matthias Sander, Anja Lemcke;;;
24.08.2020;Chinas Minderheiten wagen den Spagat zwischen Tradition und Moderne;"Als die Sprache auf die einst hitzig geführten Diskussionen in der Dorfgemeinschaft um das Für und Wider von Touristen kommt, lächelt Gun Shuige, ein Mann in den Zwanzigern, kurz und sagt: «Zu Beginn waren die Älteren dagegen. Inzwischen haben sie jedoch eingesehen, dass der Tourismus uns neue Einnahmequellen beschert und wir dennoch unsere Traditionen pflegen können.» Er gehört den Biasha an – einem zur Minderheit der Miao gehörenden Stamm. Die Miao leben hauptsächlich im Südwesten Chinas. Bis vor wenigen Jahren kannte kaum jemand das im Bezirk Congjiang der Provinz Guizhou gelegene Dorf mit seinen 594 Haushalten. Inzwischen entdecken jedoch immer mehr Han-Chinesen die reizvolle Region, die zu den ärmsten, aber auch den schönsten Chinas zählt. Die hügelige Landschaft wirkt wie eine Märchenstube. Die Vegetation spriesst üppig. Die Luftqualität ist hervorragend. Immer wieder ist Vogelgezwitscher zu hören. Der Trubel der lauten und überfüllten chinesischen Metropolen ist im Biasha-Miao-Dorf weit weg.
Investitionen in die Infrastruktur zahlen sich aus

Gun, der als Touristenführer im Dorf arbeitet und in der Gemeinschaft wegen seines Wissens hohes Ansehen geniesst, weiss die Vorteile seiner Heimat heute noch besser zu schätzen. Er hatte einst in der wirtschaftlich weiter entwickelten Provinz Guangdong als Wanderarbeiter sein Glück gesucht. Heimisch ist er in der Provinz östlich von Guizhou nie geworden.

«Ich habe meine Familie vermisst, litt unter der schlechten Luft, den vielen Personen und konnte auch mit dem auf Konsum orientierten Lebensstil nichts anfangen», sagt der schmächtige Mann, der das traditionelle schwarze Gewand der Biasha trägt. Seinen Kopf ziert ein weisses Tuch mit bunten Stickereien. Er zog es nach einem Jahr in Guangdong vor, wieder in seine Heimat zurückzukehren. Sein Entscheid wurde auch durch den Umstand beeinflusst, dass in den vergangenen Jahren grosse Investitionen in die regionale Infrastruktur neue Perspektiven eröffnet haben. Seit 2014 hält 30 Autominuten vom Biasha-Miao-Dorf entfernt ein Hochgeschwindigkeitszug. Der Bahnhof befindet sich fast auf halbem Weg zwischen Guiyang, der Hauptstadt der Provinz Guizhou, und Guangzhou, der Hauptstadt der Provinz Guangdong. Nach Guiyang braucht der Schnellzug keine zwei Stunden. Einst dauerte die Autofahrt zwischen Guiyang und dem Bezirk Congjiang wegen der hügeligen Landschaft mehr als zehn Stunden.

Die Mehrzahl der Touristen kommt aus den umliegenden Provinzen, aber mit steigendem Wohlstand kommen auch immer mehr Chinesen aus weiter entfernten Landesteilen. Sie tauchen während ihrer Besuche in eine für sie fremde Kultur ein. Laut Gun blickt die Minderheit der Miao auf eine rund tausend Jahre alte Geschichte zurück. Eine zentrale Rolle spielen in ihrem Leben Bäume. Wenn jemand geboren wird, pflanzt die Familie einen Baum. Wenn der Nachwuchs seinen 15. Geburtstag feiert, gilt er als erwachsenes Mitglied der Dorfgemeinschaft. Dann wird abermals ein Baum gepflanzt. Bei seinem Tod wird schliesslich jener Baum gefällt, der bei der Geburt gepflanzt wurde, und daraus der Sarg gezimmert.

«Das geschieht alles an einem Tag. In der Regel tragen wir den Verstorbenen noch abends zu Grabe. Der Kopf muss bei der Bestattung gen Osten ausgerichtet sein, weil nach unserem Glauben die Vorfahren aus dieser Himmelsrichtung kommen», sagt Gun. Am Grab wird schliesslich zur Erinnerung an den Verstorbenen ein weiterer, dritter Baum gepflanzt.

Die Biasha haben sich auf die Wünsche der Touristen eingestellt. «Unsere wichtigste Einnahmequelle sind die täglichen Shows, bei denen wir den Touristen unsere Traditionen näherbringen», sagt Gun. So werden den Männern des Dorfes die Haare mit einer Sichel geschnitten und rasiert. Als einzige Gemeinschaft in ganz China dürfen die Biasha noch ihre traditionellen Schusswaffen besitzen. Sie dienten früher der Jagd. Nun werden sie den staunenden Touristen präsentiert.
Die Jugend muss eine Perspektive haben

Allerdings blickt die Dorfgemeinschaft wegen des Coronavirus auf schwierige Monate zurück. Zunächst war der Besuch von Touristen aus anderen Provinzen untersagt. Auch die täglichen Shows waren verboten. Zumindest haben die strikten Massnahmen eine Ausbreitung des Virus verhindert. Im Bezirk Congjiang, wo 90 Prozent der 370 000 Bewohner einer von fünf Minderheiten angehören, soll es bisher keine einzige Infektion gegeben haben. Nun ist auch wirtschaftliche Besserung in Sicht. «Wir kehren mit zwei täglichen Shows langsam wieder zur Normalität zurück», sagt Gun. Durch das Dorf schlendern ein paar Han-Chinesen, die aus der Provinz Guangdong für einen Tagesausflug angereist sind. An diversen Ständen können sie Handgefertigtes wie Einkaufstaschen und Puppen kaufen.

Eine ältere Frau, die seit drei Jahren ihre eigenen Schals und Tücher herstellt, freut sich über den Aufschwung. «Das laufende Jahr ist wegen der Pandemie bisher zwar hart gewesen. Aber davor haben wir von den Touristen profitiert», sagt sie. Auch sie wagt den Spagat zwischen Tradition und Moderne. Ihre handgefertigten Produkte können die Kunden mit dem Smartphone bezahlen.

Gun freut sich über die Entwicklung in seinem Dorf. Manchmal rege er sich zwar über Touristen auf, die den Lebensstil der Biasha nicht achteten und sich despektierlich verhielten. «Solche Vorfälle sind jedoch die Ausnahme», sagt er. Gun ist sich bewusst, dass sich die Zeit nicht zurückdrehen lässt. Ohne Perspektiven für die Jugend laufen die Biasha Gefahr, dass die Jungen die Heimat verlassen, um in wohlhabenderen Regionen ihrem chinesischen Traum nachzujagen. Solche Abschiede will die Minderheit vermeiden. Sonst stirbt ihre Kultur eines Tages aus.";https://www.nzz.ch/international/minderheiten-in-china-tourismus-als-fluch-und-segen-ld.1569642;NZZ;Matthias Müller;;;
07.06.2019;Der gläserne Hof – wie die Agrarindustrie nach den Daten der Bauern greift;"Wenn Landwirt Ruedi Bigler mit dem Traktor über das Feld fährt, muss er seine Hände nicht ans Lenkrad legen. Der Traktor bringt die Gülle GPS-gesteuert aus, den Dünger und die Pflanzenschutzmittel auch, zentimetergenau. Bigler ist Milchviehhalter im bernischen Moosseedorf. Er hat seinen Betrieb weitgehend automatisiert. Roboter melken die Kühe und füttern die Schweine. Am Computer wird registriert, wie viel die Kühe fressen, wie viel der Traktor sät und düngt. Bigler sagt, er sitze mittlerweile genauso oft am Computer wie jeder Büroangestellte.

Was Bauer Bigler tut, nennt sich Smart Farming, digitalisiertes Bauern also. Es wird die Landwirtschaft mindestens so stark revolutionieren wie die Erfindung des Traktors vor 125 Jahren. Die Landwirtschaft der Zukunft wird eine überwachte sein. Im Stall könnte die permanente Vermessung der Tiere den Kontakt zwischen Bauer und Tier ersetzen. Auf den Feldern werden Roboter mit Sensoren das Wachstum von Pflanzen überwachen, Unkräuter und Schädlinge identifizieren und mit weniger Gift als heute bekämpfen. Smart Farming ist gut für die Umwelt – und für die Konsumenten. Die Versprechen von Labels werden für alle überprüfbar werden, für jedes Tier wird ersichtlich sein, was es gefressen hat, wie lange es gelebt hat, wie oft es auf der Weide war.

Die Frage ist dann nur: Wem gehören in dieser Landwirtschaft der Zukunft die Daten? Und wer darf auf sie zugreifen?
Ein grosser Player mischt mit

Die Agrarindustrie hat das Potenzial von Big Data längst erkannt. Der Chemiekonzern Bayer hält über Monsanto Anteile an einer Smart-Farming-Lösung. In den USA investieren Amazon und Google in die Daten von Landwirten. In der Schweiz mischt ebenfalls ein grosser Player im Geschäft mit dem gläsernen Stall mit: der Agrarkonzern Fenaco, zu dem unter anderem die Marken Landi, Volg, Agrola und Ramseier gehören. Die bäuerliche Genossenschaft hat schon mehrere Millionen Franken in eine digitale Plattform gesteckt.

Die Plattform heisst Barto und ist zugleich Datenbank und Farm-Management-System. Sie soll Bauern dazu bringen, sämtliche Daten zu zentralisieren. Für die Bauern kann das attraktiv sein. Die Bürokratie gehört zu den meistbeklagten Problemen der Branche. Die Behörden wollen für die Ausrichtung von Subventionen wissen, wie viele Hektaren Weizen ein Bauer kultiviert und wie er diesen behandelt. Abnehmer von Fleisch und Milch fragen nach, wie viel Kraftfutter die Tiere gefressen haben. Mit Barto kann dieser Austausch automatisiert werden, die Formularschlacht entfiele.
Perfektes Vermarktungstool für Firmen

Aber Barto ist mehr als eine Datenbank. Das System bietet auch landwirtschaftliche Beratung an und soll für alle Hersteller verfügbar sein. Firmen wie Syngenta oder John Deere sind bereits Partner. Auf Wunsch sollen Bauern von den Partnerfirmen künftig Empfehlungen erhalten, etwa zu Saatgut oder Pflanzenschutz. Voraussetzung ist, dass die Bauern bereit sind, ihre Daten zu teilen. Die Plattform kann so zum perfekten Vermarktungstool für die Industrie werden. Das gilt insbesondere für Fenaco.

Der Agrarkonzern kontrolliert heute schon fast die ganze landwirtschaftliche Wertschöpfungskette. Er liefert den Bauern Saatgut, Pflanzenschutzmittel und Dünger und ist zugleich Abnehmer der Produkte. Die Kontrolle über die Betriebsdaten der Bauern verliehe dem Agrarkonzern eine noch gewaltigere Marktmacht. «Wenn ich weiss, wann der Bauer was gesät hat, und seine Düngerbezüge kenne, kann ich eine relativ gute Prognose machen, wie viel der Bauer ernten wird. Ich kann individualisierte Angebote für alles Mögliche machen», sagt ein Branchenkenner.

Die Betreiber von Barto betonen zwar, sie gäben keine Daten ohne Zustimmung der Bauern weiter. Jeder Bauer entscheide selbst, was mit seinen Daten passiere. Barto erhebe auch eine Nutzungsgebühr –weil man die Daten ja nicht verkaufen dürfe. Viele Landwirte sind trotzdem skeptisch.

Landwirt Bigler aus Moosseedorf sagt: «Ich würde es nie zulassen, dass Daten meines Betriebs auf einer zentralen Datenbank eines grossen Konzerns landen.» Ihm sei wichtig, dass er die Hoheit behalte. Zu gross sei die Gefahr von Missbrauch.
«Enorme Ballung von Macht»

Bei Barto handle es sich um eine «enorme Ballung von Macht, Know-how und Geld», sagt Martin Brugger vom Schweizer Bauernverband. «Wir dürfen die Augen nicht davor verschliessen, dass Fenaco viel Geld in Barto investiert hat.» Der Verband wollte sich ursprünglich an Barto beteiligen, hat dann aber verzichtet, weil das Projekt so kontrovers diskutiert wurde.

Noch kritischer äussert sich Fritz Rothen von der Bauernorganisation IP Suisse. «Wer kommerziellen Anbietern Betriebsdaten zur Verfügung stellt, kann nie wissen, in welchem Masse diese weiterverwendet werden», sagt er. Wenn sich auf einer Plattform sämtliche Daten von sämtlichen Bauernhöfen der Schweiz befänden, dann verschaffe das dem Betreiber einen enormen Wettbewerbsvorteil.

Rothen will mit IP Suisse noch diesen Sommer ein Alternative zu Barto lancieren. Sie soll ebenfalls den Austausch von Daten mit Branchenpartnern und Behörden vereinfachen, aber frei sein von Bindungen zur Agrarindustrie. Anders als Fenaco, die dem Bauern Futter, Dünger und andere Hilfsmittel verkaufen wolle, habe man kein kommerzielles Interesse an der Plattform.

Heinz Mollet von der Fenaco-Geschäftsleitung entgegnet, bei Barto stehe «der Mitgliedernutzen» im Vordergrund, «weniger die kommerziellen Interessen». Auch Brugger vom Bauernverband meint, der Agrarkonzern werde sich hüten, Daten von Barto missbräuchlich zu verwenden – nur schon aus Angst vor Reputationsschäden. Ausserdem verhindere die Konstruktion von Barto, dass eine einzelne Firma die Plattform dominieren könne.
Heikle Konstellation

Tatsächlich stehen diverse Firmen und Verbände hinter Barto. Sogar der Bund ist beteiligt, zumindest indirekt. Er ist Mehrheitseigner der Firma Identitas, diese wiederum ist zweite Hauptaktionärin von Barto, neben Fenaco.

Das ist eine heikle Konstellation. Während der Bund sich am Gemeinwohl orientieren solle, stehe für die Unternehmen die Gewinnmaximierung im Vordergrund, hält die Eidgenössische Finanzkontrolle in einem Bericht fest. Sie sieht darin einen Zielkonflikt und warnt vor «Haftungsfragen und Reputationsrisiken» – zum Beispiel, wenn Daten missbraucht oder gestohlen werden. Die Finanzkontrolle wirft auch die Frage auf, «warum der Bund die administrative Entlastung für den Landwirt bzw. Bewirtschafter nicht selbst bereitstellt und damit auch die Kontrolle über den Zugriff und die Verfügbarkeit dieser Daten behält».

Beim Bundesamt für Landwirtschaft versteht man die Aufregung nicht. Man engagiere sich nicht direkt bei Barto und nehme nicht Einsitz im Verwaltungsrat oder in anderen Gremien, sagt Vizedirektor Andreas Aebi. Auch der Bund habe ein Interesse daran, dass mit dem Digitalisierungsprojekt administrative Hürden abgebaut würden. Verlasse man den eingeschlagenen Weg, schade das der Landwirtschaft.

Bauer Bigler aus Bern glaubt nicht, dass die Bürokratie dank Hightech abnehmen wird. Sowieso blickt er wenig optimistisch in die Zukunft. Die Digitalisierung der Betriebe werde mit Sicherheit kommen – die Probleme der Bauern aber würden bleiben, sagt er. «Wir sind zu klein strukturiert und zu teuer, um mit dem Ausland mithalten zu können.» Daran werde auch der Einsatz von Big Data und künstlicher Intelligenz nichts ändern.";https://www.nzz.ch/schweiz/der-glaeserne-hof-wenn-sich-bauern-der-datenkrake-ausliefern-ld.1487395;NZZ;Angelika Hardegger und David Vonplon;;;
12.07.2017;Apple plant Rechenzentrum in China;"Der US-Technologiekonzern Apple hat sich dazu durchgerungen, erstmals auf chinesischem Boden ein Rechenzentrum für die Verwaltung von Cloud-Diensten und die Speicherung von Daten chinesischer Kunden zu errichten. Das Projekt wird in Zusammenarbeit mit der chinesischen Partnerfirma Guizhou Cloud Big Data Industry erfolgen und ist Teil eines Apple-Investitionsprogramms über rund 1 Mrd. $ in der Provinz Guizhou.

Die chinesische Regierung will die südwestchinesische Provinz Guizhou im Rahmen eines strukturpolitischen Programms zu Chinas führendem Zentrum im Bereich Internet-Infrastruktur, Datenverwaltung und Cloud-Technologie entwickeln. Die Entscheidung von Apple für ein chinesisches Datenzentrum ist eine Konzession an neue gesetzliche Vorschriften zur Netzsicherheit, die per 1. Juni in Kraft getreten sind. Die Regeln sehen für ausländische Unternehmen im Internet-Sektor zwingend vor, dass in China generierte Datenströme vor Ort gespeichert werden. Zugleich erfordern die Vorschriften, dass Cloud-Dienste durch chinesische Unternehmen als Lizenzinhaber betrieben werden. Ausländische Firmen sind vor diesem Hintergrund auf Partner angewiesen. China ist für Apple der zweitgrösste Absatzmarkt nach den USA. Microsoft und Amazon haben bereits eigene chinesische Datenzentren eröffnet.";https://www.nzz.ch/wirtschaft/konzession-an-neue-vorschriften-apple-plant-rechenzentrum-in-china-ld.1305752;NZZ;Norbert Hellmann;;;
06.07.2019;Evidenzbasierter Rassismus – Algorithmen sind immer nur so schlau wie die Daten, auf denen sie beruhen;"Wer hat nicht Mühe mit seinem eigenen Rassismus? Uns unterläuft wiederkehrend das, was ich den Fehlschluss der rassistischen Induktion bezeichne: Wir schliessen von der «Evidenz» der Hautfarbe – oder allgemeiner: der äusseren Erscheinung – eines Menschen auf seine gesellschaftliche Stellung, seinen Beruf, sein Inneres. John Hope Franklin, Geschichtsprofessor an der Duke University in North Carolina, gab 1995 ein Abendessen in einem Privatklub in Washington. Seiner schwarzen Hautfarbe wegen hielt ihn eine Angestellte für ein Mitglied des Personals (Franklin wurde übrigens 1962 das erste schwarze Mitglied des Klubs).

Man schiebe solche Vorfälle nicht leichthändig ab auf eine «typisch» amerikanische Mentalität. Ich erinnere mich noch an die fünfziger Jahre, als man nicht selten hörte: Aha, Italiener, arbeitest du auf dem Bau? Die ETH-Professorin für Umweltwissenschaft, Nina Buchmann, erzählte jüngst in einem Interview ein aktuelles Müsterchen dieses Fehlschlusses. Sie habe oft darauf hinweisen müssen: «Nein, ich bin nicht die Sekretärin von Professor Buchmann. Ich bin Professor Buchmann.» Auch hier der Fehlschluss: Aha, eine Frau, also eher Sekretärin als Professorin.

Frau Buchmann nennt dies ein «Missverständnis». Aber der Fehlschluss ist nicht harmlos. Wer seine Logik allein auf die Basis der Evidenz abstellt, betrachtet den Menschen als «von aussen» beschreibbares Objekt. Man nimmt ihn nicht als Person wahr, sondern als «Evidenz» für bestimmtes Verhalten. Natürlich sagen uns die Logiker, dass die Evidenz stets unvollständig bleibt und keinen zwingenden Schluss zulässt; es können zur schwarzen Hautfarbe noch so viele «evidenzielle» Merkmale treten. Das Problem aber ist der Anspruch, ja die Anmassung, einen Menschen durch induktiven Indizienbeweis als den zu «überführen», der er ist.
Die Sherlock-Holmes-Methode

Wir kennen diese Anmassung aus der Literatur: die Sherlock-Holmes-Methode. Bei aller Brillanz seiner Gedankenführung erscheint uns der fiktive Meisterdetektiv irgendwie unheimlich: seine kalte Hybris, durch kalkulierende Beobachtung einen Menschen zu kennen. Gewiss, er hat Erfolg damit, wirkt auf seine Mitmenschen aber auch beleidigend durch die Art, wie er seine Urteile über andere bildet. Nicht einfach, weil sie meist negativ sind. Oft basieren sie ja auf ganz banalen Beobachtungen, zum Beispiel, welche Zigarettenmarke jemand raucht, was jemand im Speisewagen isst oder in welchen Schuh jemand am Morgen zuerst schlüpft. Holmes verblüfft uns, wenn er aus solch unscheinbarer Evidenz ein präzises Täterbild zeichnet. Das Beleidigende an seinem Vorgehen reicht allerdings tiefer. Wir finden es anstössig, dass Holmes seine «Opfer» nicht primär als Personen betrachtet, sondern als Übungsobjekte forensischer Analyse, Prognose und Manipulation.
Der erkannte Kunde

Die Praxis von Sherlock Holmes grassiert heute in den neuen Medien, über das Forensische hinaus: möglichst viel Evidenz, viele Daten über eine Person sammeln, damit man aufgrund des Datenprofils zu einem möglichst treuen prädiktiven Modell des Verhaltens gelangt. Nun ist es nicht mehr die Person des Meisterdetektivs, welche die Schlüsse zieht, das tun heute Algorithmen, die auf riesigen Datenwiesen grasen.

Die evidenzbasierte Profilierung beginnt zum Beispiel bei der Debitkarte, die im Supermarkt das Erfassen der Ware und das kontaktlose Bezahlen ermöglicht. Diese Technik der sogenannten Nahfeldkommunikation dient aber nicht bloss einem solch speziellen Zweck. Auf der Karte ist ein kleiner Sender eingestanzt, der dauernd Signale ins Netz aussendet. Der Kunde wird schon beim Eintreten in den Supermarkt «erkannt», seine Laufwege und Warenvorlieben werden aufgezeichnet, gesammelt und von einem Algorithmus verarbeitet. Er berechnet die Wahrscheinlichkeit des künftigen Kundenverhaltens. Und er gleicht die Daten mit anderen Daten im Netz ab, so dass ein immer präziseres «persönliches» Profil des Kunden entsteht. Anders gesagt: Der Computer «weiss» mehr über den Kunden als dieser über sich selbst.
Tückische Ironie

Und hier nun beobachten wir eine tückische Ironie: Wenn schon die menschliche Intelligenz dem Trugschluss der rassistischen Induktion erliegt, wie soll dann die künstliche Intelligenz dagegen gefeit sein? Tatsächlich können Deep-Learning-Systeme auch Rassismus «lernen» und zur «Überführung» von Menschen prädiktiv eingesetzt werden. Die Mathematikerin Cathy O’Neil zeigt in ihrem Buch «Angriff der Algorithmen», wie sich rassistische Modellannahmen unbemerkt in Programme einschleichen: «Und wenn aus dem Modell erst einmal eine Überzeugung geworden ist, erstarrt es vollends. Es generiert toxische Annahmen, die kaum jemals überprüft werden, und gibt sich stattdessen mit Daten zufrieden, die diese Annahmen bestätigen und zementieren (. . .) Folglich ist Rassismus das schlampigste aller prädiktiven Modelle. Es wird durch planloses Datensammeln und falsche Korrelationen angetrieben, verstärkt von institutionellen Ungerechtigkeiten und vergiftet durch den ‹confirmation bias›.»
Niemand ist erschöpfend beschreibbar

Selbstverständlich beurteilen wir andere Menschen ständig anhand ihrer beobachtbaren Eigenschaften und Verhaltensweisen. Und es ist nicht per se anstössig, festzustellen, dass jemand eine schwarze Hautfarbe hat, homosexuell ist, aus einer Migrantenfamilie stammt oder einen kriminellen Vater hat. Solche Eigenschaften legitimieren nur nicht die «logische» Konklusion «Also bist du ein . . .». Der Schluss ist erstens fehlerhaft. Denn meist schiebt er als Zwischenschritte versteckte Annahmen oder Vorurteile ein (die Logiker sprechen von «Enthymem»). Der Schluss ist zweitens moralisch verwerflich, denn die personale Identität ist immer «mehr» als eine beliebig lange endliche Aufzählung evidenzieller Merkmale: «Du bist das, das, das et cetera pp.» Das hat im Übrigen keine moderne Philosophie so prägnant zu formulieren gewusst wie der Existenzialismus von Sartre: Menschsein, ein Ich zu sein, bedeutet, nie erschöpfend beschreibbar zu sein. Das nennt Sartre «Transzendenz des Ego». Wer diese Beschreibbarkeit beansprucht, ja zu erzwingen sucht – nicht wenige Big-Data-Maulhelden tun das –, verletzt die Menschenwürde. Sartre prägte dafür die berühmte Formel: «Die Hölle, das sind die anderen.»

Ich interpretiere sie so: Die anderen, das sind die, die mich so beschreiben, wie sie mich haben möchten, und am Ende akzeptiere ich diese Beschreibungen als Wesenszuschreibung meiner selbst. Ich sehe mich selbst immer schon als von anderen gesehen. In unserer verdateten, durchscheinenden Identität, die im Netz zirkuliert, geben wir uns als Personen zugunsten personifizierter Daten auf. Und so gesehen leben wir im Netz in der Sartreschen Hölle.
Kennen und Mögen, Ahnen und Erwarten

Im zwischenmenschlichen Umgang ziehen wir unsere Schlüsse nie allein auf der Grundlage von Evidenz, sondern auf der Grundlage von Kennen und Mögen, von Ahnen und Erwarten: dem Schonraum des Unwissens. Zu viel Wissen über den anderen beschädigt das gute Verhältnis zu ihm, beschädigt ihn selbst. Im «Tagebuch 1946 bis 1949» von Max Frisch stehen die gewaltigen Sätze: «Unsere Meinung, dass wir den anderen kennen, ist das Ende der Liebe (. . .) ‹Du bist nicht›, sagt der Enttäuschte oder die Enttäuschte: ‹wofür ich Dich gehalten habe.› Und wofür hat man sich denn gehalten? Fu?r ein Geheimnis, das der Mensch ja immerhin ist, ein erregendes Rätsel, das auszuhalten wir müde geworden sind. Man macht sich ein Bildnis. Das ist das Lieblose, der Verrat.»

Es geht nicht bloss um Rassismus. Es geht um eine fundamentale – eine ontologische – Vereinnahmung von uns Individuen durch ständiges Kategorisieren. Dieses Kategorisieren ist heute eine monumentale Industrie: Data-Mining. Wir wissen, wo du bist, was du willst, was du denkst – wir wissen, wer du bist!, rufen uns die Herren der Algorithmen fortwährend zu: Wir haben die ganze Datenevidenz über dich! «Ich bin nicht der, der ich bin!», muss man ihnen als existenzialistischen Kampfruf entgegenschleudern. Fordern wir ein Menschenrecht auf Nichterkanntsein ein.";https://www.nzz.ch/meinung/dumme-daten-erzeugen-einen-evidenzbasierten-rassismus-ld.1488481;NZZ;Eduard Kaeser;;;
04.12.2017;Öko und Techno – im Städtebau kein Widerspruch;"In Feuilletons und Leitartikeln wird heftig über die Neuerfindung der Städte des 21. Jahrhunderts debattiert. Die diesbezüglichen Prozesse und Perspektiven sind zu einem der prominentesten gesellschaftlichen Themen geworden – und das aus gutem Grund.

Seit 2008 leben mehr Menschen in Städten als auf dem Land. Bis zum Ende dieses Jahrhunderts werden die Städte für beinahe 90 Prozent des Bevölkerungswachstums und für 60 Prozent des Energieverbrauchs verantwortlich sein. Einerseits funktionieren diese geschäftigen Knotenpunkte des menschlichen Lebens als Zentren der Innovation auf unserem Planeten; andererseits verursachen sie den Löwenanteil der Umweltverschmutzung.

Nach manchen Schätzungen gehen auf die heutigen Städte um die 75 Prozent des weltweiten CO2-Ausstosses zurück, zudem sind sie für zahllose andere Schadstoffemissionen verantwortlich. Die Städte verschlingen weite Flächen von Wald, Ackerland sowie anderer Landschaft, sie verpesten Flüsse, Meere und Böden. Kurz gesagt: Falls wir Städte nicht bald richtig bauen, ist eine gesunde Zukunft für die Menschheit kaum vorstellbar, ganz zu schweigen von einer gesunden Biosphäre.
Smart oder grün?

Wenn ich es richtig sehe, lassen sich die Standpunkte zur Neuerfindung der Städte grossenteils zwei Lagern zuordnen. Die eine Seite ruft nach «smarten», «digitalen» oder «hochtechnisierten» Lösungen für den Städtebau. Der Schwerpunkt liegt auf den Informations- und Kommunikationstechnologien mit dem Potenzial, die Funktionsweise urbaner Räume zu verbessern. Angeheizt von dem ständig wachsenden Datenmaterial über Städte (zum Klima, zu den Verkehrsströmen, zum Grad der Umweltverschmutzung, zum Ausmass des Energieverbrauchs usw.), haben sich mehrere Schlüsselbereiche herausgeschält, die als mögliche Anwendungsfelder für Hightech-Eingriffe herangezogen werden; dazu zählen etwa die Bewegungsmuster von Menschen, die Verteilung von Energie, Nahrung und Wasser, der Umgang mit Müll. Die Befürworter stellen sich sprechende Städte vor, die für ihre Bewohner Live-Updates über die Umweltverschmutzung, das Parkieren, den Verkehr sowie die Wasser-, Strom- und Energieversorgung bereitstellen. Dank Erfindungen wie Ultra-Low-Power-Sensoren und webbasierten drahtlosen Netzwerken werden Smart Cities bald schon Wirklichkeit sein.

Das andere Lager klärt uns über den Bedarf an «grünen», «biophilen» oder sogar «wilden» Städten auf, in denen die Natur erhalten, wiederhergestellt und wertgeschätzt wird. Selbstverständlich waren Städte immer schon Orte, an denen das Wilde gerade nicht wohnt und die dazu entworfen wurden, die Menschen durch Mauern von der Natur zu trennen.

Eine wachsende Zahl aktueller Forschungsergebnisse belegt jedoch die für die Gesundheit positive Wirkung des Kontakts mit der städtischen Natur. Zu den Vorteilen zählen die Abnahme von Stress, ein gestärktes Immunsystem und erhöhte Konzentrationsfähigkeit. Noch wichtiger sind vielleicht die physischen, psychischen und emotionalen Pluspunkte, die offenbar so wichtig für eine gesunde Kindheit sind. Die Befürworter grüner Städte argumentieren zudem, dass viele der dringendsten Fragen unserer Zeit, darunter der Klimawandel, das Artensterben und der Habitatsverlust, gar nicht angegangen werden können, solange der Mensch die Natur um ihn herum nicht versteht und schützt.
Kein Widerspruch

Da haben Sie es also: Big Data gegen Mutter Natur – zwei Ansichten über die Zukunft der Städte, die allem Anschein nach an entgegengesetzten Enden des Spektrums angesiedelt sind. Die eine Seite schätzt technische Innovation; die andere die Weisheit der Natur und die Bindung zu ebendieser. Schaut man jedoch genauer hin, fällt auf, dass diese beiden Sichtweisen sich keinesfalls ausschliessen. In Wirklichkeit ergänzen sie sich sogar.

Eine Stadt kann durchaus gleichzeitig hochtechnisiert und reich an Natur sein. Heutzutage behauptet kaum noch ein Anhänger grüner Städte, wir müssten «zurück zur Natur». Stattdessen setzen sich die Vertreter dieser Denkschule für eine Zukunft ein, die gleichermassen von Technologie wie von Biologie bestimmt ist. Man prägt neue Begriffe wie «technobiophile Städte» oder «Nature Smart Cities», um dieses Mischkonzept zu beschreiben: urbane Räume, in denen die biologische ebenso wie die digitale Welt willkommen sind.

Ja, in Nature Smart Cities wird es eine Menge begrünter Dächer, begrünter Mauern und vernetzter begrünter Räume geben. Die Aussaat heimischer Pflanzen zieht heimische Insekten an, die wiederum heimische Vögel sowie andere Tierarten anlocken und Hinter-, Innen- und Schulhöfe in Miniatur-Ökosysteme verwandeln. Diese Kleinode urbaner Natur verbessern nicht nur die Gesundheit der Menschen, sondern sind für Unmengen bedrohter Arten die letzte Hoffnung. Darüber hinaus vermögen reich begrünte Städte intelligente Technologien vollauf nutzbar zu machen, die den Bewohnern der Stadt beim Umstieg auf erneuerbare Energien wie Wind- und Wasserenergie oder bei der Nutzung von Erdwärme helfen. Das grüne Verkehrswesen reduziert den CO2-Ausstoss und verbessert den Umweltschutz. Grüne Gebäude können wie Bäume funktionieren, weil sie mit Solarenergie betrieben werden und Abfall wiederverwerten. Städte arbeiten dann wie Wälder.
Mitverantwortung

Interessanterweise betonen beide Sichtweisen bezüglich unserer gemeinsamen urbanen Zukunft die Bedeutung einer informierten und engagierten Bürgerschaft. Es könnte gut sein, dass die Digitaltechnologien und Big Data den Menschen tatsächlich die Kontrolle zurückgeben, beispielsweise in Form erhöhter Partizipation an der Kommunalpolitik («E-Governance»). Darüber hinaus können ganz normale Bürger als Laienwissenschafter oder Laiennaturkundler wichtige Aufgaben bei der Wiederherstellung der urbanen Tier- und Pflanzenwelt übernehmen, indem sie Arten beobachten und Anpassungen vornehmen, um Qualität und Quantität der Natur um sie herum zu verbessern. Wir haben hier also eine Möglichkeit, den Menschen zu helfen, auf der Basis fundierter wissenschaftlicher Daten zu handeln (und nebenbei noch das wissenschaftliche Verständnis zu fördern).

Das Nachdenken über die Zukunft unserer Städte ist sehr viel mehr als nur heisse Luft. Zumindest im urbanen Raum können Mutter Natur und Big Data hervorragende Bettgefährten abgeben. Tatsächlich hängt vielleicht nicht nur unser Überleben, sondern ein Grossteil der Biodiversität unseres Planeten davon ab, dass diese Ehe vollzogen wird. Sollte sie gelingen, werden wir die Geburt einer neuen Stadt erleben – einer Stadt, in der die Menschheit ebenso blüht und gedeiht wie die Natur.";https://www.nzz.ch/feuilleton/oeko-und-techno-im-staedtebau-kein-widerspruch-ld.1325279;NZZ;Scott Sampson;;;
06.03.2020;Wo die repräsentative Demokratie versagt, übernehmen bald Algorithmen die Staatsgewalt;"Die Bildung einer Regierung wird in unseren parlamentarischen Demokratien zunehmend zum Schauspiel, zur Tragödie, zuweilen zur Farce. Natürlich zählen Kompromisse und Koalitionen seit je zum Instrumentarium politischer Machterhaltung. Doch was wir nördlich oder südlich der Alpen beobachten können, hat eine neue Qualität. Da wird das politische System als das vorgeführt, als was es inzwischen viele sehen: einen Schatten seiner eigenen Bestimmung, die darin bestünde, den Volkswillen zu repräsentieren und Frieden auf Zeit zu schaffen.

Die Vorkommnisse in Thüringen, Italien oder Israel sind ein Beispiel für die Verkommnisse der politischen Ordnung, die den Staat ausmacht. Sie sind auch ein konkretes Signal, dass man sich auf gravierende Veränderungen des politischen Systems einstellen muss. Doch wo die analoge politische Gestaltung des Staatswesens versagt, da wird die Technologie mit ihrem effizienten und agnostischen Zugriff auf alle verwendbaren Daten Schritt für Schritt übernehmen.
Die Gamification der Politik

Das ist ein Treppenwitz der Staatsgeschichte. Wird häufig vor dem Einfluss der Technologie, vor allem bei Wahlen, als einem gefährlichen Spiel gewarnt, könnte sich die Logik bald genau umgekehrt entfalten: Die von Menschen gemachte Politik erweist sich als neue Form von Gamification. Macht wird zum Spiel, dessen Regeln den politischen Willen überlagern, der von den Bürgerinnen und Bürgern ausgeht.

Das ist, zugegeben, nicht ganz neu. Der Politikwissenschafter Ulrich Sarcinelli hat die Anfänge dieser Degeneration politischer Repräsentation bereits Mitte der achtziger Jahre als «symbolische Politik» beschrieben. Der US-Ökonom Paul Krugman sah uns 2017 im «Zeitalter der Fake-Politik» angekommen. Der libertäre Internetunternehmer Peter Thiel kommt gar zu dem Ergebnis, dass «Freiheit und Demokratie nicht mehr länger vereinbar» sind.

Ruhiger lässt sich konstatieren: Das parteipolitische System ist immer ineffizienter geworden. Es spiegelt an vielen Stellen nicht mehr den Willen der Bürger und korrumpiert den Grundgedanken, der das Wesen des demokratischen Staates beschreibt: Das Volk übt die Staatsgewalt aus. Bisher musste man das irgendwie hinnehmen. Doch Technologie könnte hier mal ganz anders wirken als erwartet: als Disruption der Degeneration.

In nahezu allen Lebensbereichen hat Technologie in den vergangenen Jahren dazu beigetragen, Entscheidungen datenbasiert besser zu machen, um schneller auf veränderte Rahmenbedingungen reagieren zu können. Nur in der praktischen Umsetzung von Demokratie durch Wahlen ist sie noch immer nicht angekommen. Naiv ist, wer glaubt, der technologische Fortschritt werde sich nicht auch die Entscheidungsprozesse der Staatslenkung und -verwaltung zu eigen machen. Vielleicht ist das, gemessen am heutigen Stand der Erkenntnis, nicht mehr Dystopie, sondern Utopie der demokratischen Rettung?
Asimov sah es kommen

Der amerikanische Professor und Science-Fiction-Autor Isaac Asimov hat in der Kurzgeschichte «Franchise» (1955) die frühe Version einer «elektronischen Demokratie» entworfen. In dieser entscheidet der zufällig ausgewählte Amerikaner Normal Muller über die politischen Geschicke des gesamten Landes.

Ihm werden Fragen gestellt, und die Antworten darauf werden mithilfe des Computers Multivac ausgewertet und auf die Wahlpräferenzen der gesamten Bevölkerung hochgerechnet. Muller ist stolz, dass durch ihn die amerikanische Bevölkerung in die Lage versetzt wird, «frei und ungehindert ihr Wahlrecht auszuüben». Algorithmische Prognostik ersetzt individuelle Stimmabgabe.

So würde das heute sicher nicht aussehen. Denn die technischen Möglichkeiten der Datenauswertung reichen inzwischen viel weiter, als Isaac Asimov sich das Mitte der fünfziger Jahre vorzustellen vermochte. Längst lassen sich über Analysen von Twitter-Daten, Google Trends und anderen grossen digitalen Datensätzen ziemlich genaue Prognosen darüber erstellen, wie Menschen einkaufen, investieren und sich sonst so verhalten. Auch Wahlausgänge lassen sich vorhersagen.

So hat Univac 111, der erste kommerzielle Grosscomputer in den USA, schon 1952 auf Basis einer Stichprobe von einem Prozent der Wahlbürger korrekt den Erdrutschsieg Eisenhowers vorhergesagt, während die meisten Umfragen Stevenson vorne sahen. Bei der US-Präsidentschaftswahl 2016 sahen fast alle Meinungsforschungsinstitute Hillary Clinton vorne, die südafrikanische Firma Brandseye sagte einen Wahlsieg Trumps voraus.

Die Datenfirma analysiert per Algorithmus weltweit Tweets auf Stimmungslagen hin und prognostizierte den Sieg Trumps wie auch zuvor schon die Brexit-Entscheidung der Briten. Obwohl längst nicht alle betroffenen Bürger auf Twitter unterwegs sind, erlauben die zugrunde liegenden Datensätze erstaunlich präzise Prognosen.

Eine algorithmische Wahl, gestützt auf die Rechen- und Prognosekapazitäten künstlich intelligenter Systeme, könnte hinreichend genau beschreiben, was die Bürger wollen. Die Berechnungen liessen sich permanent auf Basis wachsender Datenmengen und immer zeit- und passgenau durchführen. Damit trügen sie auch den Veränderungen der Meinungsbildung Rechnung, die jederzeit bei einer Entscheidung auch kurzfristig möglich sind. Nicht ein ausgewählter Prototyp käme zu Wort, sondern jede Präferenz hätte theoretisch die gleiche Chance, in einem automatisierten Entscheidungsprozess berücksichtigt zu werden.

So ungewöhnlich dieser Gedanke erst einmal sein mag: Ist es wirklich vorstellbar, dass alle Lebensbereiche, das Einkaufen, die Partnersuche, die Jobsuche, zunehmend durch Algorithmen gesteuert werden, während die Entscheidungsfindung in Staat und Politik in der vordigitalen Unzulänglichkeit des Staatswesens, im menschlichen Makel steckenbleibt?
Volkswille statt Parteiinteressen

In einer Umfrage des Center for the Governance of Change unter 2500 Erwachsenen in Grossbritannien, Spanien, Deutschland, Frankreich, Italien, Irland und den Niederlanden sagte im Frühjahr 2019 ein Viertel der Befragten, politische Entscheidungen sollten lieber durch eine künstliche Intelligenz als durch Politiker getroffen werden. Das spiegelt zum einen den Vertrauensverlust, der Institutionen und ihren Repräsentanten seit einiger Zeit entgegenschlägt. Es spiegelt aber auch die Vorstellung, dass technologisch gestützte Entscheidungen vielleicht genauer, treffender oder gar gerechter sein könnten.

Das liesse sich beispielsweise auch am Haushaltsrecht des Parlaments erproben. Die Zuteilung der Mittel hängt nicht selten auch davon ab, wie durchsetzungsfähig ein Minister oder eine Ministerin ist. Ebenfalls davon, welche parteipolitischen Interessen mehr oder weniger Gewicht haben. Die Interessen der Bevölkerung können sich davon durchaus unterscheiden. Würde man einen Haushaltsentwurf auf der Grundlage datenbasierter Bedarfsanalysen durch ein KI-System erstellen lassen, die Bedürfnisse des Volks rückten anstelle parteipolitischer Interessen wieder in den Vordergrund.

Der Aufschrei aller Nostalgiker und Technophobiker schallt schon aus der Zukunft heran: Wie kann man es wagen, dem Volk und dem Parlament sein jeweils höchstes Recht zu nehmen? Das ist einerseits ein legitimer Einspruch. Aber er greift anderseits längst ins Leere.

Denn die Gamification des Staates hat beide Rechte weitgehend ausgehöhlt. Mithilfe von algorithmischen Vorausberechnungen könnte man die Logik des demokratischen Entscheidungsrechts umdrehen: Eine KI erarbeitet die Entscheidungsvorlagen auf Basis von Big Data. Das Volk und das Parlament stimmen dann darüber ab. So liesse sich ein demokratischer Vorbehalt in einem System garantieren, das von der perfektionierten Prognostik profitiert.

Eine wesentliche Voraussetzung dafür ist bis jetzt nicht gegeben: Die Daten müssten um die Verzerrungen bereinigt werden, die eine künstliche Intelligenz oft genauso wenig gerecht machen wie eine menschliche. Das wird nicht gelingen, wenn auf Basis von Daten der Vergangenheit in die Zukunft extrapoliert wird.

Wenn man dem Staatswesen aber eine Stunde null der Datensammlung gönnte, um ab dann ein neues, inklusives Datenrepositorium aufzubauen, wären die Vorzeichen andere. Die Qualitätssicherung dieser Daten müsste Verfassungsrang erhalten, die Aufsicht über sie höchsten Ansprüchen genügen.

Digitalisierung macht das Leben an vielen Stellen direkter. Dem werden sich auch Staat und Politik nicht dauerhaft entziehen können. Eine algorithmische Repräsentation politischer Präferenzen würde politisches Entscheiden auf die Ebene einer digitalen Direktdemokratie heben. Mit einem Abstimmungsvorbehalt für die errechneten Massnahmen bliebe der Mensch immer im Loop – allerdings am Ende der Entscheidungskette.

Was geschieht, wenn er am Anfang steht, haben wir jetzt lange genug als Sinnentstellung des politischen Systems beobachten können.";https://www.nzz.ch/feuilleton/die-zukunft-des-staates-uebernehmen-bald-algorithmen-ld.1543920;NZZ;Miriam Meckel;;;
19.04.2018;Hype und Nöte bei der Digitalisierung der Fondsbranche;"Alles muss digitalisiert werden, so lautet die Losung der heutigen Zeit. So auch in der Fondsbranche. Allenthalben werden Teams junger IT-affiner Mitarbeiter aufgebaut. Ein Expertenpodium am Branchenanlass Friends of Funds in Zürich spürte vermeintlichem und tatsächlichem Bedarf nach. Es kristallisierten sich drei Hauptgebiete heraus, in denen digitalisiert werden könnte oder sollte: Zunächst fragen sich alle, wie in Zukunft mit den Kunden kommuniziert werden soll, wie die Anlageberatung aussehen wird. Grossbritannien und die Niederlande, wo Kleinkunden seit der Abschaffung der Retrozessionen kaum noch von herkömmlichen Finanzberatern bedient werden, gelten hier als Vorbild.

Weiter versuchen die meisten Fondsgesellschaften, Big Data zu nutzen, um Investmentideen zu generieren und besseres Risikomanagement zu betreiben – und tüfteln umgekehrt an Software herum, welche die Fondsmanager davor bewahren soll, in der Informationsflut zu ertrinken, und ihnen helfen soll, Informationen rascher zu analysieren. Das ist alles gut und schön.
Der Fluch der Excel-Tabellen

Doch das Hauptproblem der europäischen Fondsbranche sind ihre Daten, die für die Administration gebraucht werden. Diese werden nach wie vor in kaum standardisierter Form über Excel-Tabellen hin und her geschoben, ein System, das aus vielen Gründen an seine Grenzen stösst: So erschwert es die Entwicklung von weitergehenden IT-Applikationen zum Beispiel eben im Kundendienst, und der Regulator verlangt eine Flut stets detaillierterer Daten, zum Beispiel im Rahmen der neuen EU-Finanzregulierung Mifid II, was ohne eine verbesserte Automatisierung im Datenmanagement kaum zu stemmen ist.
Schweizer Standardisierungs-Initiative

In der Schweiz haben sich die Grossbanken und die grossen Privatbanken zusammengetan in der sogenannten Open-Funds-Initiative, welche die Fondsanbieter zwingt, Daten in vereinheitlichten Formaten zu verschicken. Diese Initiative zeigt Wirkung und strahlt bereits nach Asien aus. Die Schweizer Banken hoffen, dass sie auch im Rest Europas Schule machen wird, denn schliesslich möchten die meisten namhaften Fondsanbieter ihre Produkte über die grossen Schweizer Banken absetzen können – und auf deren Plattformen darf immer öfter nur noch, wer die Standards einhält. IT-Berater Igor Testen (Aietes) gab allerdings zu bedenken, dass der Staat in Österreich bereits ein Standardisierungsprojekt durchgepaukt habe, das in Österreich zwar funktioniere, aber nicht ins Ausland ausgestrahlt habe.
Europa bleibt fragmentiert

Ist zumindest in der Schweiz alles Nötige einmal standardisiert, können sich die Sourcing-Partner forciert digitalisieren. Daniel Häfele, CEO des Vertriebsdienstleisters Acolin, meinte, selbstverständlich könne danach ein Teil der Aufgaben von Mittlern automatisiert werden. Er gab sich aber zuversichtlich, dass etwa für eine Acolin bis auf weiteres noch genug Arbeit übrig bleiben werde. Europa bestehe aus 27 Märkten mit unterschiedlichen Sprachen und – trotz gemeinsamem Markt – unterschiedlichen Regulierungen, hier werde sich der Fondsvertrieb nie so effizient gestalten lassen wie in den USA.

Nicht nur Acolin hat die IT-Kompetenz massiv aufgestockt, sondern auch die Insourcer von Fondsadministrationsaufgaben wie State Street (vertreten durch Daniel Genoud) oder RBC (vertreten durch Heribert Krämer aus Luxemburg). Diese Firmen wollen auch vermehrt versuchen, ihre aggregierten Datenbestände ihren Kunden zur Verfügung zu stellen, so dass diese damit zum Beispiel bessere Vertriebsstrategien erarbeiten oder kundenkonformere Produkte massschneidern können.
Schnell, schneller, am schnellsten

Die Teilnehmer an der Diskussion unter der Leitung von Ulrich Jacobi von K&W Software gerieten zwischendurch ins Schwärmen über die jungen IT-Mannschaften, die nötig seien, um die verstaubte Bank-IT abzulösen. Deren veraltetes System bedeute, dass ein Projekt mindestens ein halbes Jahr dauere, während heute ein Kunde am Freitag ein Bedürfnis nach Software-Unterstützung anmelden wolle, und am Montag müsse die Lösung bereitstehen. Dass eine Lösung im Finanzbereich auch noch fehlerfrei laufen sollte und je nachdem verschärfte Sicherheitsvorkehrungen nötig sind, wurde erwähnt, aber blieb eine Randnotiz.
Unbegrenzte Möglichkeiten?

Nicht zur Sprache kam, dass der Versuch, mithilfe besser gefilterter Daten besser zu investieren, kurzlebig sein kann, weil Konkurrenten gute Algorithmen tendenziell rasch nachbauen. Und dass Analysen riesiger Datensätze für eine Verbesserung des Vertriebs nicht selten auch banale Resultate ausspucken. Ob Fondsmanager, die elektronisch nur noch das zu lesen bekommen, was sie ohnehin immer lesen, noch Anlageideen ausserhalb der üblichen Bahnen generieren, wurde nicht angezweifelt. Lobend wurde eine Startup-Firma erwähnt, die mithilfe von Social-Media-Daten Risikoprofile von Anlagekunden erstellt, noch bevor sie je einen Anlageberater aufgesucht haben. Unerwähnt blieb, dass das Bild, das Menschen auf Social Media von sich selber vermitteln, oft nicht ganz mit dem übereinstimmt, was sie ihrem Arzt, Psychologen oder eben ihrem Bankberater über sich selber verraten würden.

Simon Stalder von Blackrock, der für vermehrte elektronische Kommunikation mit den Kunden eintrat, wies immerhin darauf hin, dass sich Robo-Advisor nur für bestimmte Kundensegmente eigneten. In der Praxis ist nicht immer klar, an wen sie sich richten. Dass auch die Blockchain-Technologie grosse Veränderungen bringen könnte, indem zum Beispiel das Vertragswesen digitalisiert würde, blieb unbestritten. Doch das ist erst der übernächste Entwicklungsschritt. Igor Testen gab weise zu bedenken, bei den heutigen Blockchain-Projekten bestehe noch immer die Gefahr, dass sie nie Zusatznutzen stiften würden, aber in der Zwischenzeit überproportional viele Ressourcen beanspruchten.";https://www.nzz.ch/finanzen/fonds/hype-und-noete-bei-der-digitalisierung-der-fondsbranche-ld.1378511;NZZ;Claudia Gabriel;;;
28.10.2019;Thematische Investments: Wenn der Megatrend zum Megaflop zu werden droht;"Anlageprodukte an den Kunden zu bringen, ist kein einfaches Geschäft. Abgesehen von regulatorischen Hürden machen es die Produkte selbst den Anbietern nicht gerade einfach, handelt es sich doch meist um recht abstrakte Versprechen, die sich mit den fünf Sinnen nicht so leicht erschliessen lassen. Deutlich zugänglicher sind Geldanlagen, wenn hinter ihnen eine Geschichte steht, die an das Wissen der Investoren anknüpft oder zu der diese sogar einen emotionalen Bezug haben. Und genau hier kommen thematische Investments ins Spiel, die in den vergangenen Jahren an Bedeutung gewonnen haben.
Herausfordernde Umsetzung

Besonders beeindruckend war die Zunahme von Geldern, die in thematische börsenkotierte Fonds (ETF) investiert sind. In Europa haben sie sich laut dem Datenanbieter Morningstar seit 2016 versiebenfacht, auf umgerechnet rund 8 Mrd. Fr. Noch weit grösser dürften die Beträge in aktiv verwalteten Themenfonds und in entsprechenden Investmentzertifikaten sein, an zuverlässigen Statistiken mangelt es jedoch. Auch in der Schweiz spielen thematische Anlagen eine wichtige Rolle, was sich unter anderem darin zeigt, dass Pictet zu den weltweit führenden Anbietern in dem Bereich zählt, wie auch daran, dass Vontobel vor wenigen Tagen die Plattform Investerest lanciert hat, die Anlegern den einfachen Zugang zu den Strategien zahlreicher Vermögensverwalter ermöglichen soll – mit Fokus auf thematische Investments. So viel vorneweg: Solche Anlagen bieten interessante Chancen, sie haben aber auch ihre Tücken.

Urbanisierung, 3-D-Druck, E-Mobilität, erneuerbare Energien, Fettleibigkeit, Kryptowährungen und vieles anderes mehr: Die Liste der Themen ist nicht nur lang, sondern auch breit gefächert, und sie wächst unaufhörlich. Allen Themen gemeinsam ist, dass sie künftig mehr Raum in unserem Leben einnehmen werden und in der Regel von langfristiger Natur sind – die Anbieter von entsprechenden Fonds oder Zertifikaten sprechen gerne von Megatrends. Die zunehmende Bedeutung wiederum lässt steigende Umsätze und Gewinne erwarten für Firmen, die auf den jeweiligen Gebieten tätig sind, und sie eröffnet Anlegern beachtliche Chancen. Durch das dynamische Wachstum ergebe sich zudem eine grössere Unabhängigkeit von Konjunkturzyklen, lautet ein weiteres Argument für solche Investments.

Was völlig einleuchtend klingt, ist in der Umsetzung jedoch mit einigen Schwierigkeiten verbunden. Erstens erfüllen sich die optimistischen Prognosen nicht immer. Gerade technologische Entwicklungen sind oft mit grossen Unsicherheiten verbunden. Beispielsweise hinkt die Entwicklung im Bereich 3-D-Druck den hohen Erwartungen bis jetzt stark hinterher. Auch bei anderen verheissungsvollen Technologien wie selbstfahrenden Autos oder E-Mobilität ist noch lange nicht ausgemacht, wann es zu einem Durchbruch kommen und welchen Unternehmen es gelingen wird, sich ein grosses Stück vom Kuchen abzuschneiden.
Friedhof der Themenprodukte

In der Auswahl der Titel liegt denn auch die zweite zentrale Schwierigkeit. Hier gilt es nicht nur jene Unternehmen zu identifizieren, die zu den Gewinnern eines Trends gehören. Ebenso elementar ist, dass der jeweilige Geschäftsbereich innerhalb der Gesellschaft eine ausreichende Grösse oder Bedeutung hat, um den Aktienkurs auch im spürbaren Mass beeinflussen zu können.

Drittens schliesslich spielt das Timing eine wichtige Rolle. Wer ein thematisches Anlageprodukt auflegt, wird dies zu einem Zeitpunkt tun, zu dem das Thema bereits einen hohen Bekanntheitsgrad hat und die Firmen, in die man investiert, eine eindrucksvolle Kursentwicklung aufweisen. Es besteht dann die Gefahr, dass bereits zu hohe Erwartungen in den Aktienkursen stecken und sich die Strategie erst einmal enttäuschend entwickelt.
Auf grosse Trends zu setzen, kann sich langfristig auszahlen Überraschenderweise hat sich die Wissenschaft nur am Rande mit thematischen Anlagen beschäftigt. Hinsichtlich Studien, die den Erfolg solcher Strategien untersucht haben, herrscht Fehlanzeige. Andreas Homberger von Hinder Asset Management verfolgt etliche Dutzend Fonds, ETF und Zertifikate, die eine Reihe von Anlagethemen abdecken. Das Bild, das sich aus seinen Daten ergibt, ist reichlich durchwachsen.

Vergleicht man die Rendite der jeweiligen Instrumente seit ihrer Lancierung mit dem Weltindex MSCI World, haben etwa jene, die die Themen erneuerbare Energien, Infrastruktur, Big Data und Virtual Reality betreffen, mehrheitlich enttäuscht. Überwiegend positive relative Erträge wurden dagegen in den Bereichen Biotechnologie, Cybersecurity, Digitalisierung sowie Automatisierung und Robotik erwirtschaftet.

Bei einer solchen Betrachtung unberücksichtigt bleibt jedoch der Umstand, dass viele Produkte, die unter den Erwartungen geblieben sind, gar nicht mehr existieren. Morningstar spricht von einem gut gefüllten Friedhof von Themenprodukten. Von den Themen-ETF, die vor 2012 lanciert worden seien, seien 80% bereits wieder geschlossen worden. Analysiert werden können aber nur die Renditen der überlebenden Produkte, was das Gesamtbild schönt.
Gefahr eng definierter Themen

Allerdings demonstrieren einzelne Anbieter, dass man mit Themen langfristig Renditen erzielen kann, die den Weltindex schlagen (vgl. Grafik). Pictet beispielsweise gelingt dies bei der Mehrheit der elf verfolgten Themen, was sich auch in der Entwicklung des Fonds Global Megatrend Selection spiegelt, in den diese Themen einfliessen.

Den Erfolgsaussichten zuträglich ist sicherlich, wenn sich ein Anbieter der spezifischen Herausforderungen bei Themen-Investments bewusst ist und diese adressiert. «Wir achten darauf, dass ein Thema stets von mehreren Trends unterstützt wird», sagt Markus Signer von Pictet. Dadurch soll verhindert werden, relativ kurzfristige Modeerscheinungen zu verfolgen. Zudem habe es sich bewährt, ein Thema nicht zu eng zu definieren. Das erlaube, flexibel damit umzugehen, etwa wenn sich die Aussichten für eine Nische innerhalb eines Themas verschlechterten, weil es zu regulatorischen Änderungen komme.

Die Rothschild & Co. Bank legt viel Wert auf die Identifikation der Aktien, die tatsächlich von einem Trend profitieren. Neben dem relevanten Anteil von Umsatz und Gewinn eines Unternehmens werde analysiert, welcher Teil der Aktienrendite nicht durch traditionelle Faktoren wie Grösse oder Momentum erklärt werden könne und wie sich dieser Rest verhalte im Vergleich zu anderen Firmen, die eng mit einem Thema verbunden seien, erläutert William Haggard. Ist die Korrelation der Residualgrössen zwischen den Kandidaten hoch, ist die Wahrscheinlichkeit ebenfalls gross, die richtigen identifiziert zu haben.
Auf den Zyklus achten

Auch für Tjeert Keijzer von Lazard Asset Management ist die richtige Auswahl und Einordnung von Firmen essenziell, was er am Thema Automatisierung und Robotik illustriert. Autonome Staubsauger seien letztlich Haushaltsgeräte, die nur geringe Margen versprächen. Dagegen gebe es Firmen, die zwar nicht zur Roboterindustrie zählten, aber spezifische Teile für diese herstellten und nur wenige Konkurrenten hätten, so dass sie stark am Roboter-Boom partizipierten. Zudem sei es wichtig zu verstehen, in welcher Phase des Zyklus sich ein Trend befinde. Je nachdem müssten Anleger eher auf den Umsatz, den Gewinn oder die Ausschüttungen achten.

Wer auf thematische Investments setzen will, sollte diese als Beimischung im Portfolio betrachten und einen Anlagehorizont von mindestens fünf Jahren besitzen. Zudem gilt es darauf zu achten, dass sich die Themen nicht zu stark überschneiden, da es sonst bei einzelnen Aktien wie jenen von Alphabet (Google) oder Amazon zu Häufungen bzw. Klumpenrisiken kommen kann. ";https://www.nzz.ch/finanzen/geldanlagen-wenn-megatrends-zu-megaflops-werden-ld.1518170;NZZ;Michael Schäfer;;;
27.06.2019;«Die Banken haben keinen Informationsvorsprung mehr»;"Seine Augen leuchten, wenn es um technische Themen geht. Schwierigkeiten gibt er offen zu und relativiert sie mit einem jugendlichen Lachen. Der 58-jährige Westschweizer Marc Bürki, der in Tunesien und Marokko aufgewachsen ist, begann seine berufliche Laufbahn nach einem Elektroingenieur-Studium an der EPFL in Lausanne als Telekom-Spezialist für die Europäische Raumfahrtagentur.

Doch schon 1990 entschied er, sich das erst im Entstehen begriffene Internet als selbständiger Unternehmer zunutze machen zu wollen. Auch dreissig Jahre später ist er offensichtlich noch mit Leib und Seele Fintech-Startup-Unternehmer. Die Idee, den Börsenhandel zu «demokratisieren», führte 1996 zur Gründung der Finanzplattform swissquote.ch. Mit ihr machten Bürki und sein Gründungspartner Paolo Buzzi erstmals Finanzdaten in Echtzeit allgemein zugänglich. Im Jahr 2000, inmitten der Internetblase, gingen die beiden mit Swissquote an die Börse.

Ein Jahr später erhielten sie eine Banklizenz, dank der die Finanzplattform zur ersten reinen Online-Retailbank der Schweiz wurde. Sie bot günstigen Zugang zum Devisenhandel und verbreiterte seither rasch die Palette an Produkten, die bei ihr gehandelt werden können. Inzwischen sind es über drei Millionen. Seit 2017 gehören auch Kryptowährungen dazu. Doch vieles von dem, was früher Pioniertaten waren, ist inzwischen auch andernorts zum Standard geworden. Bürki und seine Swissquote versuchen deshalb, sich immer wieder neu zu erfinden. 2018 verwaltete Swissquote Kundenvermögen von 23,8 Mrd. Fr. und erzielte einen Vorsteuergewinn von 53,8 Mio. Fr. Herr Bürki, Sie sind ein Schweizer Fintech-Pionier, der die traditionellen Banken herausfordert. Allen Untergangsszenarien zum Trotz dominieren diese das Finanzgeschäft aber weiterhin.

Als wir vor gut zwanzig Jahren begannen, war das Internet ein neues Medium und eher etwas für Geeks. Damals hat noch niemand daran geglaubt, dass es die Wirtschaft revolutionieren würde. Das kam später. Zugegebenermassen haben sich die traditionellen Banken aber gut geschlagen. Sie haben ein viel breiteres Angebot als die Newcomer, die jeweils nur einen kleinen Bereich des Finanzgeschäfts abdecken. Das macht es schwierig, die traditionellen Finanzhäuser anzugreifen. Nur mit Zahlungsdienstleistungen werden die sogenannten Neobanken jedenfalls kaum je Geld verdienen. Deshalb müssen sie ihre Produktepalette verbreitern, das bedingt aber eine teure Bankinfrastruktur.

Soeben hat Facebook angekündigt, zusammen mit zahlreichen anderen grossen Unternehmen die eigene Kryptowährung und Blockchain Libra lancieren zu wollen. Wird das den weltweiten Zahlungsverkehr und das Finanzsystem revolutionieren?

Facebook und die 27 Partner im Libra-Projekt haben, was viele anderer Kryptowährungen nicht haben: eine zwei Milliarden schwere Community. Auch wird Libra eine neue auf Open Source basierende Blockchain benützen und somit die Transaktionskosten auf fast null herunterbringen. Es gilt noch regulatorische Fragen zu klären, aber im Retail-Zahlungsverkehr und im Bereich von Micropayment könnte das durchaus eine kleine Revolution werden. Ob es das ganze Finanzsystem revolutionieren wird . . .sehr wahrscheinlich nicht.

Was war vor zwanzig Jahren Ihre wegweisende Innovation?

Auf Technologie zu setzen. Sie gab uns die Möglichkeit, in ein Geschäftsfeld einzudringen, das von eher konservativen Anbietern dominiert wurde, aber auch hart umkämpft war.

Warum hat denn die immer wieder heraufbeschworene grosse Disruption im Bankgeschäft noch nicht stattgefunden?

Weil die Entwicklung evolutionär verläuft. Die grossen Banken schlafen nicht. So haben sie ihr Internetangebot laufend erweitert. Die grossen Reiche sind im Finanzgeschäft noch nicht untergegangen.

In der Regel ist aber die Zeit von Grossreichen irgendwann vorbei. Blüht dieses Schicksal eines Tages auch den Universalbanken?

Nur wenn die Bankmanager nicht einsehen, dass sich die Welt ändert, und sie es verpassen, ihre Infrastruktur zu erneuern. Die Schweizer Banken halten mit der Entwicklung Schritt. Sie sind gut positioniert.

Also unangreifbar? Für Swissquote wäre das ein unerfreulicher Befund.

Wir schneiden uns bereits ein grosses Stück vom Finanzkuchen ab, etwa im Retail-Aktienhandel. Und wir wollen dieses Stück noch vergrössern.

War und ist denn Swissquote ein Disruptor?

Von der Art, wie wir unsere Dienstleistungen erbringen und mit den Kunden kommunizieren, verstehen wir uns als Disruptor.

Gleichzeitig hat Swissquote gewisse Probleme, die auch die etablierten Banken plagen. Ihr Geschäft wächst nur noch wenig.

In den Anfangsjahren erzielten wir hohe Wachstumsraten. Wir wollten aber immer aus eigener Kraft expandieren, und das benötigt Zeit. Grundsätzlich ist es schwierig, im Finanzgeschäft schnell zu wachsen, weil ein Unternehmen dafür zuerst eine aufwendige Infrastruktur erstellen muss. Zudem ist es unerlässlich, früher oder später eine Auslandspräsenz aufzubauen.

Die Aktie von Swissquote hat sich in den vergangenen zwölf Monaten ähnlich schlecht entwickelt wie die etablierten Bankentitel. So gesehen, ist Swissquote keine Tech-Firma.

Bei solchen Betrachtungen kommt es immer auf den Zeitraum an. Allerdings muss ich zugeben, dass Swissquote kein Startup-Unternehmen mehr ist, dessen Aktienwert sich in kurzer Zeit vervielfacht hat. Dafür haben wir aber ein stabiles Geschäftsmodell, gleichzeitig bauen wir immer noch unsere Infrastruktur aus. Wir glauben auch, dass unsere Bewertung steigen wird, wenn wir als Firma breiter aufgestellt sein werden. Dafür erweitern wir auch unsere Auslandspräsenz.

Nach der Ankündigung Ihrer letzten Quartalszahlen ist der Aktienkurs regelrecht eingebrochen.

Ja, das stimmt, der Kurs ist zuerst von 35 auf 70 Franken geklettert und hat sich dann fast halbiert. Da war wohl einiges an Spekulation und Short-Selling im Spiel. Eingebrochen sind wir, nachdem wir Rekordzahlen publiziert und gleichzeitig gesagt hatten, dass wir dieses Jahr etwas weniger Gewinn erzielen würden, weil wir in Singapur und andernorts in unsere Expansion im Ausland investieren wollten.

Das Ausland hat doch nicht auf eine Schweizer Online-Bank gewartet.

Ja, aber wir haben im Ausland bereits Kunden. Unsere Dienstleistungen sind gut, und die Marke Schweiz hilft uns, in fremden Märkten Fuss zu fassen. Neu wollen wir auch das Geschäft mit institutionellen Kunden forcieren, demnächst in Singapur. In der Schweiz gibt es zwar im internationalen Vergleich relativ viele Aktionäre, es ist aber ein kleiner Markt. Rund eine Million Menschen besitzen Aktien. Und nur rund 40% von ihnen handeln regelmässig selber Wertschriften.

Früher sprach man viel vom Aktiensparen. Warum hat es sich nicht stärker verbreitet, zumal Obligationen bei den niedrigen Zinsen keine attraktive Anlage sind?

Prozentual gesehen, ist der Anteil der Aktienbesitzer hierzulande ähnlich hoch wie in Nordeuropa, aber um einiges grösser als im Süden des Kontinents. Es handelt sich um ein kulturelles Phänomen, gleichzeitig braucht man ein finanzielles Polster, um in Aktien zu investieren.

Swissquote begann als reine Handelsplattform. Wird sich das Unternehmen zunehmend zu einer Universalbank entwickeln, um wieder höhere Wachstumsraten zu erzielen?

Das ist unser Ziel. Wir wollen unsere Dienstleistungen schrittweise ausbauen, denn das wünschen unsere Kunden. Eine solche Expansion kostet allerdings viel Geld, und man tritt gegen eine sehr starke Konkurrenz an, in der Schweiz nicht nur gegen global tätige Finanzkonzerne, sondern auch gegen die Kantonalbanken.

Was will denn Swissquote besser machen als diese Anbieter?

Das ursprüngliche Rezept war einfach: Wir konnten dank dem Internet einen niedrigeren Preis offerieren als die Etablierten. Das war der Einstieg ins Geschäft. Zudem haben wir Schnittstellen zu allen technischen Plattformen geschaffen. Junge Firmen müssen immer offen sein für Partner.

Was bedeutet das technisch?

Wir haben Softwareinstrumente, mit denen Vermögensverwalter ihre Kunden zentral betreuen und sie auch an internationale Börsenplätze anschliessen können.

Die Plattformökonomie macht diese Vermögensverwalter und Bankberater aus Fleisch und Blut nicht überflüssig?

Die Idee, dass es eines Tages nur noch die Technologie und die Endkunden geben wird, ist absurd und hat sich überholt. Gewisse Kunden haben schlicht keine Zeit, sich um Geldfragen zu kümmern. Oder sie haben keine Lust, weil sie lieber andere Interessen verfolgen. Neu ist allerdings, dass die Endkunden die Aktivitäten ihres Vermögensverwalters stärker kontrollieren möchten. Und dafür gibt es mittlerweile Instrumente. Anders als früher haben die Banken keinen Informationsvorsprung mehr. Das ist für mich die grosse Internetrevolution.

Swissquote beschäftigt mittlerweile rund 700 Mitarbeiter. Sind Sie jetzt eine etablierte Gesellschaft oder immer noch ein Startup?

Bei uns herrscht immer noch der Geist einer Jungfirma – ich bin einer der Ältesten im Unternehmen. Und wir versuchen, diesen Spirit zu erhalten.

Wie machen Sie das?

Das geschieht fast zwangsläufig. Der Grossteil unserer Angestellten ist sehr jung, rund 30 Jahre alt. Für viele ist es die erste Stelle nach dem Studium. Weil wir nicht so hohe Löhne bezahlen können wie die Grossunternehmen, verlassen uns die meisten nach einer gewissen Zeit wieder. Pro Jahr verlieren wir rund 20% von ihnen.
Marc Bürki, ist überzeugt, dass ein Schweizer Fintech wie Swissquote sich international orientieren muss. Das ist viel, im Durchschnitt liegt die Fluktuationsrate bei den Schweizer Firmen eher bei 10%.

Alle fünf Jahre erneuert sich unser Unternehmen gleichsam von selbst. Wir bilden junge Ingenieure im Banking aus. Die Ausbildung ist so gut, dass unsere talentierten Mitarbeiter früher oder später eine Offerte von einer anderen Firma erhalten. Das ist keine einfache Situation, wir können sie aber nicht ändern. Wir können nicht ein Discounter sein und gleichzeitig überdurchschnittliche Löhne bezahlen.

In Gland in der Westschweiz baut Swissquote derzeit ein Bürogebäude für 1000 Mitarbeiter. Viele Banken haben ihre hiesige Belegschaft reduziert und dafür diejenige in Polen oder Indien vergrössert. Warum machen Sie das nicht auch so?

Wir beschäftigen 700 Personen, davon sind rund ein Drittel IT-Fachkräfte. Und in der Ukraine arbeiten noch einmal rund 150 Informatiker für uns, sie sind jedoch bei einer Drittfirma angestellt. Dann haben wir seit neuerem kleine Teams in Spanien und Portugal.

In der Ukraine? Machen Ihnen die politischen Spannungen dort keine Sorgen?

Wir haben unsere Beziehungen ins Land schon vor einigen Jahren geknüpft, und es funktioniert ziemlich gut. In der IT gibt es eine Art West-Ost-Graben. In der Sowjetunion waren die Fachkräfte hochspezialisiert, und die Entwicklung geschah gleichsam durch Aneinanderreihung dieser Fachkräfte. Diese Tradition wirkt in der Ukraine nach. Dort findet man Spezialisten für ganz enge Fachbereiche. Im Westen dagegen sind die IT-Generalisten zu Hause. Die von der ETH ausgebildeten Ingenieure zum Beispiel haben eine sehr breite Basis. Was die IT betrifft, befindet sich die Produktionsmaschine deshalb im Osten, während die Kreativität eher im Westen zu Hause ist.

Wird es weitere Verschiebungen geben bei der geografischen Verteilung ihrer Angestellten?

Grundsätzlich benötigen wir so viele ETH-Ingenieure wie möglich. Leider ist deren Zahl beschränkt, weshalb wir unsere Standorte in Portugal und Spanien ausbauen werden. Dort ist das Niveau der IT-Fachleute ähnlich hoch wie in der Schweiz. Wenn wir aber umfangreiche Projekte in kurzer Zeit durchpeitschen müssen, setzen wir auf die Ukraine. Natürlich könnten wir dafür auch nach Indien gehen; aber die Ukrainer sind uns vertrauter.

In den vergangenen Monaten hat Swissquote stark auf Kryptowährungen gesetzt. Diese gelten als Spekulationsobjekte, die erst noch viel Energie fressen.

Da muss ich Sie korrigieren: Wir haben nicht auf Kryptowährungen gesetzt, sondern sahen darin schlicht eine Innovation, bei der man dabei sein muss – so wie Swissquote sehr früh das Robo-Advisory in die Angebotspalette aufgenommen hat. Swissquote will nicht nur eine Kryptobank sein, zumal das Geschäft nur 2% des Umsatzes ausmacht. Allerdings haben wir ein wenig unter der Euphorie gelitten, welche die Kryptowährungen ausgelöst haben. Die Umsätze schwollen an, nur um dann wieder stark zu schrumpfen.

Gewisse Banken bieten den Handel mit Kryptowährungen aus Furcht vor Geldwäscherei gar nicht an. Haben Sie keine Angst vor einem Rufschaden?

Nein, in der ersten Phase konnten die Kunden ja noch keine Kryptowährungen transferieren, sondern sie nur mit herkömmlichem Geld erwerben. Mittlerweile haben Investoren die Möglichkeit, Kryptowährungen über Swissquote zu transferieren. Dafür müssen sie aber strenge Regeln befolgen, und wir haben in Absprache mit der Finma klare Überwachungsprozesse implementiert.

Haben Kryptowährungen denn eine grosse Zukunft?

Bestimmt nicht alle von ihnen. Es ist allerdings eine gute Idee, über das Internet Produkte und Dienstleistungen auszutauschen, ohne dass dabei traditionelle Zahlungsmittel zum Einsatz kommen. Was darüber hinaus geht, ist aus heutiger Sicht Spekulation. Der Wertzuwachs, den die Bitcoins 2018 erfahren haben, war ohnehin absurd.

Welche Innovationen zeichnen sich für Sie im Finanzbereich jenseits der Kryptowährungen ab?

Ich sehe zwei Trends: Zum einen Anbieter, die Zahlungen auslösen oder das Robo-Advisory betreiben. Zum anderen das Thema Big Data. Die Analyse von Daten sollte es ermöglichen, an den Börsen eine bessere Performance zu erzielen. Beide Trends befinden sich noch in der Anfangsphase, werden den Banksektor aber stark verändern.

Wenn immer mehr Banken Big Data betreiben, wird es dann nicht immer schwieriger, eine höhere Rendite als der Gesamtmarkt, also Alpha, zu erzielen?

Big Data kann man verschieden nutzen, und die Algorithmen sind auch von unterschiedlicher Qualität. Als wir mit Robo-Advisory begannen, war das einfach nur ein gut entwickeltes Ingenieurprodukt. In einem zweiten Schritt geben wir nun dem Algorithmus die Möglichkeit, sich mit künstlicher Intelligenz selber zu optimieren. Bisher haben wir bei diesen neuen Instrumenten erst an der Oberfläche gekratzt. Vielleicht gibt es in der Zukunft tatsächlich kein Alpha mehr, davon sind wir aber noch weit entfernt.

Sie sind 58 Jahre alt – Zeit, um an die Nachfolge zu denken. Wann verkaufen Sie die Firma?

So spät wie möglich. Ich bin ein glücklicher Aktionär und Unternehmer. Ich spüre keinen Druck.

Ein Handwechsel ist aber eine Variante?

Als Gründer eines börsenkotierten Unternehmens kann man eine solche Transaktion nie ausschliessen. Mein Mitgründer Paolo Buzzi und ich besitzen 25% an Swissquote. Das verschafft uns eine gewisse Kontrolle, aber keine uneingeschränkte. Wir haben keine Absicht, einen Käufer zu finden. Die Arbeit macht uns immer noch Spass.

Ein Grossunternehmen könnte Swissquote schnell mit mehr Kapital ausstatten. Sie könnten dann rascher expandieren.

Das würde helfen, wir können aber auch aus eigener Kraft expandieren. Das dauert zwar etwas länger, man verliert dabei aber auch nicht die Kontrolle.";https://www.nzz.ch/wirtschaft/fintech-banken-und-libra-swissquote-chef-marc-buerki-im-gespraech-ld.1489177;NZZ;Peter A. Fischer und Daniel Imwinkelried;;;
27.06.2019;Künstliche Intelligenz – Hoffnungen und Befürchtungen;"In Bezug auf die Verwendung von künstlicher Intelligenz (KI) im Gesundheitssektor hat die American Medical Association Richtlinien herausgegeben, die sich mit Entwicklung, Verwendung und Regulierung von KI auseinandersetzen. Der Verband bezeichnet KI dabei als «augmented intelligence», also «erweiterte Intelligenz», was die Überzeugung widerspiegelt, dass KI die Arbeit von Ärzten verbessern und nicht ersetzen wird.
Vielfältige Anwendung

Die Anwendungen im Gesundheitssektor sind dabei sehr vielfältig und reichen vom Monitoring von Gesundheitsdaten über die Optimierung administrativer Prozesse bis hin zur ärztlichen Diagnoseunterstützung. Gerade das letzte Anwendungsfeld, in welchem die Diagnose eines Arztes durch KI erweitert und verbessert wird, ist vielversprechend, wirft aber auch Fragen auf.

So stellt sich beispielsweise die Frage nach den Haftungsfolgen für eine fehlerhafte KI-basierte Diagnose. Eine Auslegeordnung zu Problemfeldern der KI-Anwendung zeigt mindestens sechs kritische Aspekte. Das Problemfeld der «Diskriminierung» wurde jüngst am Beispiel von Amazons Bewerbungsalgorithmus aufgezeigt, welcher nach einigen Monaten gestoppt wurde, da ausschliesslich Männer, nie aber Frauen selektioniert wurden. Das Problemfeld «Sicherheit» zeigt sich etwa bei selbstfahrenden Autos, letztmals bei der tödlichen Kollision eines mit KI gesteuerten Uber-Fahrzeugs in Arizona. Dies führt auch unmittelbar zu den Problemfeldern «Ethik» und «Recht», denn KI-Algorithmen müssen mitunter schwierige Entscheidungen treffen (zum Beispiel Kollision mit einem Kind contra Kollision mit einer Gruppe älterer Menschen), die dann auch Haftungsfragen aufwerfen. Ist im Fall einer Kollision der Fahrer, der Automobilhersteller oder der Hersteller der KI zu belangen?

Auch die mit der Nutzung der Technologie einhergehenden Problemfelder «Datenschutz» (Wer hat Zugriff auf die Daten? Wofür werden diese verwendet?) und «Zukunft der Arbeit» (Wie viele Arbeitsplätze werden demnächst durch KI ersetzt?) sind in ihrer Sprengkraft nicht zu unterschätzen.

All diese Problemfelder deuten einen gewissen Regulierungsbedarf an, bezüglich der Algorithmen sowie der Unternehmen, die sie entwickeln und einsetzen. Die Algorithmen reichen von selbstfahrenden Autos über Entscheidungsunterstützungssysteme zum Beispiel im Gesundheitssektor bis hin zu mit künstlicher Intelligenz betriebenen Waffensystemen.
Richtlinien sind erforderlich

Nationale und internationale Institutionen wie die OECD oder die Europäische Kommission haben die Relevanz dieser Problematik erkannt und in jüngerer Zeit Arbeitsgruppen eingerichtet. Diese sollen unter anderem Richtlinien für einen fairen und vertrauensvollen Umgang mit KI erarbeiten. Die in den Arbeitsgruppen entwickelten Forderungen nach Transparenz, Nichtdiskriminierung und Fairness zeigen aber auch sehr deutlich die Grenzen in der Anwendung von KI auf. Denn manche Dilemmata lassen sich nicht auflösen. So liegt es in der Natur des maschinellen Lernens, dass Entscheidungswege nicht transparent sind. Auch zeigen sich ethische Dilemmata, zum Beispiel in der Definition von Fairnessbegriffen. Denn was aus dem Blickwinkel eines Unternehmens oder einer Behörde als fair angesehen wird, stimmt nicht zwingend mit dem Fairnessbegriff des Konsumenten überein.

Bei allen Bedenken ist aber auch das enorme Potenzial der KI zu sehen. Ich bin überzeugt davon, dass KI viel mehr Gutes als Schlechtes machen wird – auch wenn Unfälle, Fehldiagnosen usw. auftreten werden. Schon heute zeigen Siri oder Alexa, dass Technologien im Bereich der KI unser Leben zukünftig organisieren und auch optimieren werden. Sie werden viele heikle Themenfelder wie Mobilität oder Gesundheitsvorsorge deutlich verändern. Welche Daten sollen für welche Zwecke verwendet werden und wo soll die Grenze zwischen sinnvoller Optimierung und unethischen Anwendungen gezogen werden?

Eine entsprechend der Relevanz des Themas angemessen breite Diskussion ist in der Schweiz noch nicht zu beobachten. Sie ist aber zeitkritisch. Denn die Technologie wird heute implementiert, und sie wird unseren Alltag sehr schnell verändern.";https://www.nzz.ch/meinung/kuenstliche-intelligenz-hoffnungen-und-befuerchtungen-ld.1490653;NZZ;Martin Eling;;;
14.11.2016;Chefs überwachen Arbeitnehmer flächendeckend;"Ein entlassener Banker spielt entwendete Kundendaten an die Medien weiter. Angestellte lästern in der Kaffeepause über das ausschweifende Privatleben ihres Chefs – in Hörweite eines konservativen Kunden. Es geht noch unappetitlicher: Ein Fabrikarbeiter uriniert in Memphis, USA, auf ein Fliessband, auf dem der Puffreis zur Weiterverarbeitung für das weltberühmte Frühstücksmüesli von Kelloggs hergestellt wird. Dann stellt er die Aufnahme auf Youtube. Die Beispiele zeigen: Bereits ein einzelner Mitarbeiter kann die Reputation einer Firma erheblich beschädigen.
Sensible und paranoide Chefs

Jeder Mitarbeiter kann zum Sicherheitsrisiko werden. Dieses Problem wird potenziert durch die Technologie. Angestellte können ihre Handlungen jederzeit weltweit einem grossen Publikum zugänglich machen. Es überrascht nicht, dass Firmenchefs immer sensibler oder, je nach Sichtweise, paranoider auf den «Risikofaktor Mitarbeiter» reagieren.

Immer ausgefeiltere Software ermöglicht es Unternehmen, die Nutzung elektronischer Arbeitsmittel der Angestellten zu überwachen. Wer wann von wo aus arbeitet, Handy, Telefon oder Internet nutzt sowie die Überwachung und Auswertung von E-Mails, Nutzung der sozialen Netzwerke und sogar von persönlichen Daten – das alles ist inzwischen möglich. «Die Mehrheit der grossen Unternehmen überwacht heute ihre Mitarbeiter elektronisch praktisch flächendeckend», sagt Rolf Schatzmann, selbständiger Berater und früherer Chef des Bundessicherheitsdienstes. Angestellte seien sich der Tiefe dieser Überwachung meist nicht bewusst. Ein Banker merkte es erst, als er abends seine Kontakte vom Computer am Arbeitsplatz an seinen Firmen-Laptop zu Hause schickte, um dort weiterzuarbeiten. Am nächsten Morgen wurde er von der Bank zur Rede gestellt.

Silvia Böhlen, Sprecherin des Eidgenössischen Datenschutz- und Öffentlichkeitsbeauftragten (EDÖB), beobachtet seit einigen Jahren einen erhöhten Einsatz von Überwachungssystemen im Arbeitsbereich. Im Detailhandel und in der Gastronomie werde vermehrt auf Videoüberwachung gesetzt. Firmen mit erhöhten Sicherheitsrisiken wie Banken verfügten über immer komplexere Risikomanagement-Systeme, sagt Böhlen.
Konflikt zwischen Sicherheit und Privatsphäre
Banken treiben einen grossen Aufwand, um Mitarbeiter auszusortieren, die ein Risiko für ihre Reputation darstellen. Das gelingt nicht immer.
Ermes Gallarotti 11.10.2016

Der Datenschutz hat für alle Kommunikationskanäle klare Leitlinien dazu formuliert, was Firmen dürfen und was nicht. So braucht ein Unternehmen einen Rechtfertigungsgrund für eine Überwachung. Der Verwendungszweck der Daten muss bei der Beschaffung ersichtlich sein. Das Sammeln und Bearbeiten von Daten ohne bestimmten Zweck ist demnach unzulässig. Zudem muss das Personal über die Kontrollen informiert werden. In der Praxis gibt es aber einen grossen Graubereich zwischen verbotener und erlaubter Überwachung. Kann nämlich eine Firma ein überwiegendes Interesse wie etwa Sicherheitsrisiken geltend machen, darf sie beispielsweise auch einzelne Mitarbeiter überwachen. Für Firmen dürfte es heute einfacher sein, zum Beispiel Angst vor virtuellen Hackerattacken geltend zu machen, da diese oft auf die Unachtsamkeit auch von eigenen Mitarbeitern zurückgehen.

Weiter verschlechtert wird die Position der Mitarbeiter, weil sie selbst aktiv gegen ihre Firma vorgehen müssen. Mit dem Verdacht auf missbräuchliche Überwachung kann sich der Mitarbeiter an das kantonale Arbeitsinspektorat oder den Eidgenössischen Datenschützer wenden. Hier muss der Arbeitnehmer beweisen, dass seine E-Mails oder die Internetnutzung verfolgt werden. Das ist aber technisch oft gar nicht möglich. Firmen legen Mitarbeitern manchmal eine Einwilligung für eine Überwachung zur Unterzeichnung vor. Ein solcher Vertrag sei aber gar nicht gültig, sagt Böhlen. Ein Vertrag muss freiwillig unterzeichnet werden. Da sich Arbeitnehmer aber im Abhängigkeitsverhältnis befinden, dürften sie sich in der Praxis nicht gegen solche Verträge wehren.
Gefahr ist «erheblich»

Grössere Konzerne haben in der Regel Datenschutzbeauftragte. Diese hätten aber nicht immer einen detaillierten Einblick darüber, was überwacht werde, sagt Schatzmann. Häufig sei der Chefjurist auch noch Datenschutzbeauftragter. Der habe grössere Sorgen als die Einhaltung der Datenschutzvorschriften. Verknüpft eine Firma vorliegende Daten wie Alter, Zivilstand, Geschlecht, Position und Krankheitsmeldungen, kann sie wichtige Schlüsse ziehen. Verbindet man diese mit zusätzlich erhobenen Daten, ergibt sich ein ziemlich klares Bild des einzelnen Angestellten. Eine solche systematische Verhaltens-Überwachung ist sehr selten erlaubt. Böhlen weist warnend darauf hin, dass Firmen immer mehr Möglichkeiten hätten, Daten zu erfassen, zu verarbeiten und zu verknüpfen. «Das erhöht die Gefahr von Datenschutzverletzungen erheblich», sagt Böhlen. Sie empfiehlt das Gespräch mit dem Arbeitgeber. Ein Angestellter habe das Recht zu wissen, ob und in welcher Form er überwacht werde.";https://www.nzz.ch/wirtschaft/risikofaktor-mitarbeiter-firmen-agieren-in-der-grauzone-ld.128156;NZZ;Zoe Baches;;;
22.03.2018;Die smarten Datendiebe;"Der mutmassliche Datenklau, der diese Woche den Ruf von Facebook erschüttert hat und vom Aktienwert des Netzwerkbetreibers fast 50 Milliarden Dollar ausradierte, begann vor vier Jahren einigermassen unschuldig in einer Art psychologischem Labor in der britischen Universitätsstadt Cambridge. Michal Kosinski, ein damals 32-jähriger polnischer Psychologe am Psychometrics Centre der Universität, machte sich einen Spass daraus, herauszufinden, wie man von Facebook-Nutzern persönliche Profile würde erstellen können, indem man sie ihre Vorlieben, beispielsweise für kommerzielle Marken, Charaktereigenschaften wie Offenheit oder auch ihre Neurosen auflisten liesse. Aleksandr Kogan, ein Kollege Kosinskis an der Universität Cambridge, entwickelte auf Basis dieser Arbeiten ein Quiz, das er auf Facebook laufen liess. Er war nicht der Einzige, viele andere Erfinder von Anwendungen hatten ähnliche Ideen, aber Kogans Quiz war besonders intelligent aufgebaut und erfolgreich; über eine Viertelmillion User beteiligten sich daran. Wenig achtsam gaben sie auch Angaben ihrer Facebook-«Freunde» preis. Dadurch stieg die kumulierte Datenmenge sprunghaft auf rund 50 Millionen Personen-Datensätze an.
Tüftler und Verkäufer

Es war ein kleiner und logischer Schritt, Kogans Quiz auf den Kopf zu stellen. Anstatt Facebook zu nutzen, um aus Usern Persönlichkeitsprofile herzustellen, würde man Facebook-Mitglieder umgekehrt mit Kost bedienen können, die ihnen relevant erschien und auf die sie positiv reagieren würden. Den Gedanken hatte Alexander Nix, ein Londoner aus wohlhabendem Hause, der eine private Eliteschule in Eton absolviert und in Manchester studiert hatte. Nix arbeitete zunächst als Finanzexperte bei der Bank Baring und wechselte 2003 zu der 1993 von Nigel Oakes, einem anderen Eton-Schüler, gegründeten PR-Firma SCL Group, die sich auf die Nutzung der Verhaltensforschung in der Werbung spezialisierte. Nix erwarb den Datenberg, den Kogans Quiz bei Facebook angehäuft hatte. Deren Kauf war an sich nicht aufsehenerregend. In Grossbritannien ist der Kauf von Daten gang und gäbe, so kann man etwa gegen Gebühr in Erfahrung bringen, wie viel ein Nachbar für seine Liegenschaft bezahlt hat und ob er kreditwürdig ist – es sei denn, er habe die Angaben sperren lassen. Aber bei der genannten Transaktion und weiteren Handlungen geschahen zwei mutmassliche Regelverstösse, wie ein Whistleblower vor einer Woche der britischen Sonntagszeitung «Observer» verriet.

Erstens verwendete Facebook die Daten der «Freunde» ohne deren Wissen und Zutun. Die zweite Regelwidrigkeit lag darin, dass Kogan und Nix die Datensammlung weiterverkauften, was den Vertragsbestimmungen von Facebook zuwiderläuft. Kogan beteuerte allerdings am Mittwoch gegenüber der BBC, er habe die Transaktion damals für völlig normal und angemessen gehalten. Facebook relativierte diese Woche die Vorwürfe; die Verwendung der Drittdaten sei seither unterbunden worden, sagten Vertreter des Unternehmens zu ihrer Verteidigung. Facebook-Chef Mark Zuckerberg räumte am Mittwoch ein, in der Affäre seien Fehler gemacht worden. Nix seinerseits bestreitet alle Beschuldigungen. Der smarte Engländer hatte 2013 in New York eine Tochtergesellschaft der SCL Group gegründet, Cambridge Analytica (CA), um in Amerika auf Kundenfang zu gehen. Drei Jahre später nahmen Trumps Schwiegersohn Jared Kushner und der Leiter der digitalen Werbekampagne Trumps, Brad Parscale, die Firma unter Vertrag. Laut Nix spielten die fraglichen Facebook-Daten bei dem Auftrag keine Rolle und wurden nicht verwendet. Die britische Datenschutzbeauftragte Elizabeth Denham beantragte vor Gericht einen Durchsuchungsbefehl für die Londoner Büros von CA , um herauszufinden, ob Nix die Wahrheit sagt.

Soll man ihm glauben? Die Beziehung zwischen CA und der Trump-Kampagne war eng. Parscale sollte mit der grössten digitalen Polit-Kampagne aller Zeiten Hillary Clintons Wahlmaschine neutralisieren, er hatte freie Hand, abtrünnige Spezialisten von Google und Facebook einzustellen, auch 13 Mitarbeiter von CA wechselten das Büro und zogen im Trump Tower ein. Das Geschäftsmodell von CA glich verdächtig den Vorgaben Kosinskis und Kogans. Aus verständlichen Gründen protestierte die Universität Cambridge diese Woche gegen den Missbrauch ihres Namens. Dazu kommen fragwürdige Äusserungen von Nix in einem Gespräch, das Reporter des britischen Fernsehkanals Channel 4 unter falschen Angaben geführt und heimlich aufgenommen hatten. Beim vermeintlichen Kundenkontakt mit einem «sri-lankischen Geschäftsmann», der vorgibt, Wahlen in seinem Land manipulieren zu wollen, prahlt Nix mit seiner Rolle im Trump-Wahlkampf und den Tricks und Betrügereien, die seine Mitarbeiter auf die Waagschale legen könnten, bis hin zur Verwendung von Prostituierten, um politische Gegner zu erpressen. Nix behauptet seither, seine Äusserungen seien verfälscht worden, er habe bloss hypothetische Annahmen zum Besten gegeben. Gleichwohl wurde er daraufhin als CEO von Cambridge Analytica suspendiert.
Facebook-Daten werden längst genutzt

Die Verwendung von Facebook-Daten in politischen Kampagnen ist weder neu, noch war sie bisher stark umstritten. In britischen Wahlkämpfen seien kommerziell verfügbare Messgrössen benutzt worden, lange bevor es das Internet gab, sagt Tim Bale von der Queen Mary University in London. Bedenklich sei allerdings, wie viele Daten heute ohne Einwilligung von Kunden weitergegeben würden. Laut dem Politologen ist vor allem die Datenmenge stark angeschwollen, die Kampagnen zur Verfügung steht. Auch der Autor Jamie Bartlett, der demnächst ein Buch mit dem Titel «The People Versus Tech» veröffentlichen wird, nennt den kritischen Punkt, an dem Quantität in Qualität umschlägt. In einem Artikel in der Wochenzeitung «Spectator» stellt er fest, moderne politische Feldzüge drohten die demokratische Debatte zu untergraben. Cambridge Analytica verfügt laut eigenen Angaben über bis zu 5000 sogenannte Data Points, also Bruchstücke von Informationen, von fast 200 Millionen Amerikanern. Dies erlaubt es, Wähler gezielt zu bewerben, also mit bestimmten Botschaften etwa Eltern, Studenten, Rentner, die asiatischstämmigen Einwohner eines Stadtviertels anzusprechen.

Die wechselseitige Kommunikation im Internet erlaubt es, Botschaften in Echtzeit den Bedürfnissen und Ergebnissen anzupassen. Laut Bartlett fanden Parscales Mitarbeiter im amerikanischen Wahlkampf etwa heraus, dass die Vorliebe für einheimische Automarken mit hohen Sympathiewerten für Trump korrelierte. Die Analyse der von CA zur Verfügung gestellten Daten erlaubte auch Rückkoppelungen an konventionelle Veranstaltungen. Parscale empfahl Trump, Gelder auf gewisse Gliedstaaten umzuverteilen oder in Pennsylvania aufzutreten. «Kommentatoren hielten das damals für dumm, aber es war alles datengesteuert», schreibt Bartlett. Er hält den Skandal um CA für eine Ablenkung. Viel beunruhigender sei es, dass Methoden, die von Cambridge Analytica und anderen Firmen angewandt würden, völlig legal und weit verbreitet seien. Facebook ist nicht der einzige Anbieter relevanter Daten, weltweit betätigen sich Dutzende von Firmen mit Umsätzen im dreistelligen Millionenbereich als Makler von Messwerten.
Von der Wahl Trumps zu Brexit

Digital generierte und verfügbare Grössen wurden erstmals in grösserem Umfang vor zehn Jahren in der Präsidentschaftskampagne von Barack Obama eingesetzt, seither nahm der Trend deutlich zu. Laut Dominic Cummings, dem Chef der Brexit-Kampagne «Vote Leave» zum britischen EU-Referendum von 2016, setzte die Organisation 98 Prozent der eingesetzten Gelder (knapp 8 Millionen Pfund) für digitale Werbung ein. Cummings selber verwendete keine Expertisen von CA, jedoch nahm die kleinere, rechtsnationalistisch ausgerichtete Brexit-Truppe «Leave.EU» die Dienste der PR-Firma in Anspruch. Möglicherweise handelte es sich dabei weniger um eine Kundenbeziehung als um eine politische Symbiose. Die britische Wahlkommission untersucht derzeit, ob «Leave.EU» über den amerikanischen Arm von Cambridge Analytica nicht deklarierte Gelder zugespielt wurden. Aaron Banks, der Geldgeber hinter der fremdenfeindlichen Gruppe und ein langjähriger Unterstützer von Nigel Farages Ukip-Partei, bestritt diese Woche die Verbindungen zu CA.

Ein demokratisch relevanter Nachteil massgeschneiderter, digital verbreiteter politischer Werbung liegt darin, dass sie öffentlich nur schwer oder gar nicht nachvollziehbar ist. Ihr Anteil wächst unaufhörlich. Im amerikanischen Wahlkampf von 2016 gaben Republikaner und Demokraten zusammengerechnet 300 Millionen Dollar für derartige Botschaften aus. In Grossbritannien legten im Jahr darauf die Konservativen vor den Unterhauswahlen 2 Millionen Pfund für Facebook-Propaganda aus. Bei der Labour-Partei belief sich die Summe laut der Wahlkommission auf 500'000 Pfund, also ein Viertel. Das heisst nicht, dass Labour Facebook und seine 31 Millionen britischen Nutzer weniger im Visier hatte, im Gegenteil. Bale sagt, die Oppositionspartei habe einfach viel besser auf die freiwillige Mitarbeit von Anhängern abstellen können. «Die Labour-Mitglieder sind jung, die Fertigkeiten für den Einsatz von sozialen Netzwerken fielen Labour als Graswurzel-Bewegung in die Hände.» Geld ist nicht alles: Die digitale Kampagne von Labour erwies sich, gemessen am überraschend guten Abschneiden der Partei, als wesentlich erfolgreicher als diejenige der Tories.

Mittlerweile unternimmt Facebook selber Anstrengungen, um die Wirksamkeit seiner politischen Dienstleistungen zu verbessern. Vor den britischen Wahlen letztes Jahr stellte das Unternehmen frühere Berater David Camerons, des ehemaligen Premierministers, sowie der Labour-Partei ein. Der Einsatz massgeschneiderter Botschaften und anderer Hexereien erfordert Profis, das sollte nicht darüber hinwegtäuschen, dass Wahlkampagnen in angelsächsischen Ländern mit einem stark polarisierenden Majorzsystem bereits vor Jahrzehnten professionalisiert und ausgelagert wurden. In Grossbritannien begann die Entwicklung in den späten 1950er Jahren, als erstmals Werbefirmen unter Vertrag genommen wurden. Immerhin heckten die Parteien ihre Wahlstrategien damals noch hausintern aus. Das änderte sich in den 1990er Jahren, als – wie in Amerika bei der Wahl Bill Clintons – erstmals nicht nur die Werbung, sondern auch vorausgehende repräsentative Wählerumfragen und Analysen von Fokusgruppen an Spezialisten vergeben wurden. Nochmals zehn Jahre später fing man damit an, auch die Interpretation der Umfragen externen Experten zu überlassen, die damit gutes Geld verdienten. New Labour unter Tony Blair kaufte die Dienste des amerikanischen Politikberaters Bob Shrum ein, des ersten ausländischen Meinungsforschers an leitender Stelle eines Wahlkampfs im Vereinigten Königreich. Die Tories zogen nach und wurden ab 2005 zu einem der wichtigsten Kunden des konservativen australischen Konsulenten Lynton Crosby.

Crosby veränderte die Wahlkämpfe auf der Insel nachhaltig, Kritiker bezeichnen den 61-Jährigen als Meister der dunklen Künste. Er ist bekannt dafür, Botschaften radikal zu bündeln. Vor den Unterhauswahlen von 2015 soll er sich die Argumente angehört haben, mit denen die Tories wirtschaftspolitisch glaubwürdiger sein wollten als Labour. Interessant, habe er laut einem Augenzeugen geantwortet, aber untauglich; eine Botschaft genüge: Labour hat die Wirtschaft in den Grund gefahren, mit den Tories geht es aufwärts. Plakate brachten danach die Botschaft auf den Nenner, indem sie den damaligen Labour-Chef Ed Miliband zeigten, wie er mit der Abrissbirne hantierte.

Negative Kampagnen bürgerten sich ein. Auf Geheiss Crosbys wiederholte der damalige Verteidigungsminister Michael Fallon gebetsmühlenhaft, Miliband werde die britische Nuklearbewaffnung demontieren. Laut Boris Johnson, der Crosby zu Hilfe nahm, um 2012 Bürgermeister in London zu werden, pflegt der Polit-Guru jeweils eine sprichwörtliche tote Katze in die Runde zu werfen. «Dass alle entsetzt sind, ist egal», erzählte Johnson einem Journalisten, «wichtig ist, dass alle von der toten Katze reden.» Vor den vorgezogenen Unterhauswahlen letztes Jahr zahlten Theresa Mays Konservative Crosbys Firma mehr als je zuvor, vier Millionen Pfund – aber der Schuss ging hinten los. Crosbys Slogan – das Land benötige Mays «starke und stabile» Führerschaft – verfing nicht. Die Premierministerin wiederholte ihn so oft, bis ihn niemand mehr hören konnte, und handelte sich dafür den einprägsamen, spöttischen Übernamen «Maybot» ein, die Kombination ihres Namens mit «Robot» (Roboter).
Druck für neue Datenschutzregeln

Crosby arbeitete ebenfalls nicht mit Cambridge Analytica zusammen, dafür ist er sich zu gut. Aber er stellte seinerseits schon 2015 einen Spezialisten für digitale Netzwerke ein, Jim Messina, einen Amerikaner, der zuvor mit Obama zusammengearbeitet hatte. Unter ihm knöpften sich die Tories gezielt bestimmte Wählergruppen vor, beispielsweise Frauen, die zuvor die Ukip gewählt hatten, aber nun zauderten. Den Prestigeverlust nach der für die Tories enttäuschenden Unterhauswahl 2017 nahm das Zweierteam gelassen hin. «Wissen Sie, politische Feldzüge sind 10 Prozent meines Gewinns und 90 Prozent meiner Sorgen», sagte Crosby nach geschlagener Schlacht. Mit anderen Worten, er kann darauf verzichten. Im September des gleichen Jahres gründeten er und Jim Messina gemeinsam das Unternehmen Outra. Der Firmenzweck lautet: Beratung kommerzieller Kunden beim Einsatz massgeschneiderter Werbebotschaften, bei denen die Datensätze des Auftraggebers mit den Daten digitaler Netzwerke zusammenführt werden.

Digitale Politik-Werbung bringt gewisse Vorteile mit sich; wer sich nicht für die verbreiteten Losungen interessiert, wird effizienter als mit früheren Methoden ausgesiebt. Die Labour-Partei erinnerte vor einem Jahr Erstwähler kurz vor Ablauf einer entsprechenden Frist per Facebook-Botschaft daran, sich in die Wählerlisten einzutragen – man kann behaupten, dies sei ein Dienst an der Demokratie. Aber die Willkür und die mangelnde Übersicht, die in der Affäre um Cambridge Analytica zum Ausdruck kommen, irritieren. Eine neue Datenschutzverordnung der EU, die im Mai in Kraft tritt, regelt immerhin die strittige Frage, wem Daten gehören, bevor sie gehandelt werden, nämlich jedem Einzelnen. Ihre kommerzielle und politische Nutzung wird dennoch weiter zunehmen, wenn das Internet der Dinge die Zahl der Messwerte von Konsumenten und Bürgern nochmals exponentiell anwachsen lässt. Sie nehmen zwei Mal pro Tag die Milch aus dem Kühlschrank? Data Point! Sie löschen regelmässig abends um 23 Uhr 00 das Licht? Data Point! Ihr Sohn mag Spielzeugsoldaten aus Plastic? Data Point! ";https://www.nzz.ch/international/die-smarten-datendiebe-ld.1368409;NZZ;Markus M. Haefliger;;;
13.05.2019;Künstliche Intelligenz: Aus dem Strudel der Algorithmen taucht das Orakel wieder auf;"Es geschah in Krasnojarsk, Sibirien. Meine Kreditkarte gab kein Geld aus. Gesperrt. Nicht, dass ich zu wenig Einlagen gehabt hätte oder das Monatslimit erreicht worden wäre. Ich hatte nur vergessen, der Bank meine Reisepläne bekanntzugeben. So vermutete ihr Algorithmus einen Angriff krimineller Russen und sperrte meine Karte. Ich hätte ihm dankbar sein sollen, dem Algorithmus. Aber ohne Geld im kalten Sibirien war mir anders zumute.

Das Problem ist bekannt mit anderem Inhalt, aber gleichem Vorzeichen. Man erhält keinen Kredit, weil die Algorithmen dagegen sind. Und niemand weiss so recht, wie sie zu ihrer Meinung gekommen sind. Zu viele Faktoren fliessen in die Analyse ein: von Einkommen, Wohnort und Kreditgeschichte bis zu einem Eintrag neulich auf Facebook oder Twitter, der Spannungen mit dem Chef bezeugt und eine baldige Entlassung – und damit Kreditabzahlungsrückstände – als wahrscheinlich erscheinen lässt. In diesem Fall wird das Geld nicht deshalb verweigert, weil das System zu wenig, sondern weil es zu viel von mir weiss.

Und während ich vor meiner Reise nach Sibirien das System benachrichtigen könnte oder später jemanden anrufen kann, um die Karte zu entsperren, gibt es, wenn mir ein Kredit verweigert wird, keine Adresse für eine Richtigstellung. Das will die EU mit ihrem kürzlich vorgelegten Ethik-Katalog für die Entwicklung künstlicher Intelligenz nun ändern. Man soll das Recht haben, mit einer Bankmitarbeiterin den abschlägigen Bescheid zu diskutieren. Aber kann die dann wirklich die Gründe benennen? Ist es nicht so, dass oft nicht einmal mehr die Programmierer ihre überkomplexen Programme verstehen?
Wir wollen an Alexa glauben

Die Sorge ist keineswegs neu. Schon vor zwanzig Jahren wurde vor der Undurchschaubarkeit künstlicher Intelligenz gewarnt, die einen blinden Glauben an die Richtigkeit ihrer Entscheidungen verlange. Heute heisst das Angstwort (und der Titel eines Buches von Frank Pasqualle) Black-Box-Gesellschaft: Wir kennen die Inputs und Outputs des Systems, wissen aber nicht, wie das eine zum anderen wird. Diese Black Box ist keine Motorhaube, die man öffnen könnte, um nach dem Fehler zu suchen. Ihr Operationsmodus ist – je komplexer, umso mehr – mysteriös.

Die Black-Box-Gesellschaft ist die unvermeidliche Nebenfolge der Digitalisierung, die uns nicht nur heimsucht, wenn es ums Geld geht. Bei vielen steht sie schon zu Hause im Wohnzimmer und hört auf den Namen Alexa. Wissen wir, wie ihre Antwort auf unsere Frage zustande kommt? Bei Alexa fällt der Apfel immer so weit vom Stamm, dass er direkt aus dem Himmel zu kommen scheint. Wissen, so scheint es, ist plötzlich herkunftslos.

Das stimmt nicht ganz, denn man könnte herausfinden, aus welchen Quellen Alexa ihr Wissen bezieht. Aber wer tut das schon? Haben wir Alexa erfunden oder Siri oder Cortana, um ihnen zu misstrauen? Wollen wir nicht ebenso blindlings an sie glauben wie an unser Navi?
Der gute Diktator

Das neue Paradigma ist die Folge des Medienwechsels. Der Wechsel von schriftlicher zu mündlicher Anfrage ersetzt Googles KI-Nanny, die uns noch die Alternativen anzeigt, mit Amazons KI-Diktator, der diese verschweigt. Denn in der Ergebnisliste einer Google-Suchanfrage nach einem guten Restaurant in der Nähe kann das Lokal, das erst auf Platz 7 steht, noch Sieger werden, wenn es durch einen interessanten Namen, ein cooles Bild oder ein raffiniertes Detail meine Aufmerksamkeit auf sich zieht. Bei Alexa bleibt Platz 7 unsichtbar, wenn ich, schon etwas genervt vom Nachfragen, das erstbeste Ergebnis nehme, egal, wie gut es ist.

Der KI-Diktator, den man sich natürlich als einen guten vorstellen muss, ist schon lange auch Googles Ziel. Denn es sei ein Fehler, dass Google mehr als eine Antwort auf deine Frage gebe, sagte Google-CEO Eric Schmidt im Jahr 2005: Wir sollten in der Lage sein, dir die eine richtige Antwort zu geben, wir sollten wissen, was du meinst. Wir sollten wissen, was du meinst, heisst so viel wie: Wir sollten dich besser kennen. Besser auch als du dich selbst. Genau das ist der springende Punkt.

In seinem Buch «Homo Deus: Eine Geschichte von Morgen» imaginiert Yuval Noah Harari die Zukunft des Datings als Konversation von Bots: «Die Cortana eines potenziellen Liebespartners tritt an meine Cortana heran, und die beiden vergleichen Notizen, um zu entscheiden, ob wir zusammenpassen – ohne dass wir menschlichen Besitzer irgendetwas davon wissen.» So wie der Mensch nach der Auslagerung des Gedächtnisses in die Schrift sich eher auf das archivierte Material verliess als auf die eigene Erinnerung, so wird er sich in der Zukunft darauf verlassen, dass die KI die für einen Sachverhalt relevanten Daten besser verarbeitet hat als er.
So gut kann nur ein Algorithmus sein

Die Algorithmen werden uns nicht versklaven, vermutet Harari, sondern Entscheidungen für uns so gut treffen, dass wir verrückt wären, ihrem Rat nicht zu folgen. Und er fügt hinzu: «Sobald Google, Facebook und andere Algorithmen zu allwissenden Orakeln geworden sind, können sie sich durchaus zu Akteuren und schliesslich zu Souveränen weiterentwickeln.»

Wir wissen nicht genau, warum Cortana uns mit diesem Menschen und nicht mit jenem verbindet, aber wer sein Leben der KI am Steuer anvertraut, sollte ihr auch zutrauen, uns gut durchs Leben zu steuern. Immerhin würden wir nicht bezweifeln, dass Algorithmen viel mehr Daten viel verlässlicher verarbeiten können als wir. Und um nichts anderes als die perfekte Datenverarbeitung geht es schliesslich bei der Vermeidung von Kollisionen und der Beschaffung des passenden Lebenspartners.

Zur perfekten Datenverarbeitung gehört ein klarer Output. So verzichtet das Orakel 2.0 auf die Rätselhaftigkeit, die beim klassischen Orakel nicht nur die Quellen, sondern auch die Bedeutung seiner Aussage betrifft. Wenn das Orakel zu den Athenern angesichts der andrängenden Perser sagt: «Sucht Schutz hinter hölzernen Mauern», muss man erst einmal darauf kommen, dass damit nicht die Mauern der Stadt gemeint sind, sondern die Schiffe einer zu bauenden Flotte. Wenn die Hexen Macbeth verkünden, kein Mensch, der von einer Frau geboren worden sei, könne ihm gefährlich werden, darf man nicht übersehen, das Macduff per Kaiserschnitt auf die Welt kam.

Beim klassischen Orakel weiss man nie, woran man ist. Selbst wenn man wollte, man kann ihm nicht blind folgen. Es beansprucht das hermeneutische Engagement seiner Adressaten, veranlasst sie, dem seltsamen Spruch Sinn innerhalb ihrer konkreten Situation zu geben. Das Orakel der KI, das Harari beschwört, verzichtet auf diesen Rest an Verstrickung in die eigenen Angelegenheiten. Es mahnt nicht: «Erkenne dich selbst!», wie es am Eingang des Orakels von Delphi heisst, es sagt nicht: Versuche mich zu entschlüsseln. Es analysiert die Daten und sagt: Folge mir und frag nicht!
Digitaler Pantheismus

Zu diesem Unbedingtheitsanspruch des KI-Orakels passt seine neue Verortung. Denn es spricht nicht länger durch Hexen im Wald oder aus dem Mund einer besessenen Priesterin an einem Hang von Delphi. Es spricht fortwährend aus all den Computern, die uns begleiten und umgeben und wie Alexa mitten im Wohnzimmer stehen. Das KI-Orakel schafft nicht vom Rande der Gesellschaft aus Ordnung in dieser, es ist die permanent konsultierte und befolgte Ordnung.

Sofern wir die Algorithmen ermächtigen, die Dinge des Alltags untereinander abzuklären, spricht das KI-Orakel nicht nur, es handelt auch gleich für uns. Das ist bereits der Fall bei der Playlist, die aufgrund unserer bisherigen Seh- und Hörgewohnheiten auf Spotify oder Youtube erstellt wird. Und es ist eines der Versprechen, die sich um das Internet der Dinge und die Smart City ranken. Man wird irgendwann dem Kühlschrank vertrauen, dass er die richtigen Bestellungen vornimmt, nachdem er unsere Verbrauchsgewohnheiten mit unseren Gesundheitsdaten abgeglichen und dazu gegebenenfalls unseren Arzt konsultiert hat. Denn irgendwann ist auch so ein Kühlschrank nichts anderes als ein Orakel.

Das KI-Orakel kommt als Doppelparadox. Es ist das Ende der Willensfreiheit im Zeichen der Selbstoptimierung, und es ist die coole Wiederverzauberung der Welt auf der Grundlage ihrer absoluten Analysierbarkeit. Denn verzaubert ist die Welt, wenn aus der Black Box die zwar unmissverständlichen, aber kaum nachprüfbaren Anweisungen kommen, wie man leben soll.

Das ist, als spräche Gott zu uns, durch seine neuen Priester, die Algorithmen. Auch diesen Gedanken findet man bei Harari, der von einem «kosmischen Datenverarbeitungssystem» spricht, dem wir alle mit unseren Daten zuarbeiten. Es wird überall sein und alles kontrollieren, und die Menschen, so endet Hararis Geschichte von morgen, sind dazu bestimmt, darin aufzugehen. Ein digitaler Pantheismus, könnte man sagen, wenn dies keine Weisswaschung, oder kein «mathwashing», all der Vorurteile wäre, die den Input dieser Black Box bestimmen.";https://www.nzz.ch/feuilleton/kuenstliche-intelligenz-aus-den-algorithmen-winkt-das-orakel-ld.1479988;NZZ;Roberto Simanowski;;;
26.01.2016;Keine Algorithmen für ethische Fragen;"Wenn dies ein Krimi wäre (und warum auch nicht, denn es wird Tote geben und um Verantwortung gehen), begänne er vielleicht so: 1967, Oxford, die Philosophin Philippa Foot entwickelt ein Gedankenexperiment, welches die akademische Debatte über den richtigen Massstab moralischen Handelns für Jahrzehnte befeuern wird, das sogenannte Trolley-Problem: Darf eine ausser Kontrolle geratene Strassenbahn, die fünf unbeteiligte Menschen zu überrollen droht, absichtlich so umgeleitet werden, dass nur ein einzelner unschuldiger Mensch zu Tode kommt? Dieses Gedankenexperiment spielte man in unzähligen Varianten durch, um unsere moralischen Intuitionen besser zu verstehen und nach ethisch angemessenen Lösungen zu suchen.

Dann der scharfe Schnitt in die Gegenwart: Unterschiedliche Unternehmen experimentieren mit selbstfahrenden Autos, und diese Technologie wird als «next big thing» im Individualverkehr angepriesen. Fieberhaft wird an den zahllosen Problemen gearbeitet, die gelöst werden müssen, damit autonomes Fahren möglich wird. Dabei besteht allerdings ein grosses Problem, welches zum erwähnten Trolley-Problem führt.

Ein Beispiel: Ein mit fünf Personen besetztes Auto A hat auf einer Küstenstrasse einen Ausfall des Bremssystems. Ihm kommt ein mit einer Person besetztes Auto B entgegen. Ein Zusammenstoss ist unvermeidlich, wenn nicht eines der beiden Fahrzeuge über die Klippe fährt. Bleiben beide Fahrzeuge auf der Strasse, sterben sechs Menschen, fährt A über die Klippe, sterben fünf, fährt B über die Klippe, stirbt eine Person.

Bisher musste man sich über solche Fragen keine grossen Gedanken machen, weil die Entscheidung spontan vor Ort getroffen wird und die Fahrer weder die Informationen haben noch die Zeit, komplexe ethische Probleme zu lösen, wenn sie in eine solche Situation geraten. Dies ändert sich mit dem autonomen Fahren grundlegend. Wie die Autos in der geschilderten Situation reagieren, wird durch Algorithmen tief in der Software entschieden, so dass sich bei der Programmierung der Software die moralische Frage stellt, wie der Konflikt gelöst werden soll: Die Ethik muss vorab in das technische System eingespeist werden.

Aller Voraussicht nach wird autonomes Fahren deutlich sicherer werden als das heutige, nichtautonome Fahren, aber ethisch relevante Konflikte und Dilemmata (wie zum Beispiel durch technisches Versagen) werden nicht verschwinden, und daher stellt sich die Frage, wie eine Gesellschaft mit dieser Situation umgehen will. Intuitiv scheint es klar zu sein, dass wir dabei Algorithmen gestalten sollten, die sich im Konfliktfall für das kleinere Übel entscheiden. Aber wird dieser Massstab der Problemsituation wirklich immer gerecht? Und worin besteht überhaupt das kleinere Übel? Diese Fragen sind genuin ethischer Natur, so dass ihre Beantwortung nicht den Softwareingenieuren überlassen werden sollte.

Wahrscheinlich werden es viele für richtig halten, im oben geschilderten Fall das Auto mit einem Fahrgast zugunsten der fünf zu opfern – das wäre die utilitaristische Lösung des Problems: Ohne weitere Informationen über die Insassen der Fahrzeuge hiesse dies, dass man möglichst wenige Tote wollen sollte. Aber halt: Wir hätten ja prinzipiell viele Informationen, die uns Rückschlüsse über das «Glück» der Insassen erlauben.

Soziodemografische Daten wie Alter, Geschlecht, Einkommen usw. sind prinzipiell verfügbar und könnten daher in das Kalkül eingebunden werden, so zum Beispiel das Lebensalter. Zurück zum Beispiel: In Fahrzeug B befindet sich eine 18-jährige Frau mit einer Restlebenserwartung von über 60 Jahren, während im Fahrzeug A fünf über 90-jährige Menschen sitzen, deren gesamte Restlebenserwartung 10 Jahre beträgt. Opfern wir Fahrzeug A, so töten wir mehr Menschen, retten aber mehr «quality adjusted life years».

Wenn Sie nun empört «Altersdiskriminierung» rufen, zeigen Sie nur, dass Sie doch keine Utilitaristin bzw. kein Utilitarist sind. Aber wie kann es sonst gehen? Welche anderen überzeugenden ethischen Massstäbe haben wir, um eine Entscheidung zu stützen (denn entschieden werden muss ja). Die Aufrechnung von Menschenleben verstösst nicht nur gegen die moralische Intuition vieler Menschen, sondern im Grundsatz auch gegen das Prinzip der Menschenwürde, das gerade auf der Nicht-Kommensurabilität menschlichen Lebens aufbaut.

Diese Position, die auf den Philosophen Immanuel Kant zurückgeht, ist zugleich fester Bestandteil unserer Rechtssysteme und unserer Rechtsprechung: So etwa im Urteil des deutschen Bundesverfassungsgerichtes zum sogenannten Luftsicherungsgesetz 2005. Dabei ging es ebenfalls um eine dem Trolley-Problem analoge ethische Frage: Darf man ein als Terrorwaffe entführtes Passagierflugzeug abschiessen, um zu verhindern, dass dieses etwa in ein vollbesetztes Fussballstadion gelenkt wird? Darf man also die wenigen unschuldigen Passagiere im Flugzeug gezielt töten, um die grössere Anzahl unschuldiger Menschen im Stadion zu retten? Das Verfassungsgericht hatte in seinem Urteil 2005 eine entsprechende Gesetzesvorlage für verfassungswidrig erklärt, da bei einer derartigen Aufrechnung die betroffenen Passagiere zu blossen Objekten instrumentalisiert und daher ihrer Menschenwürde beraubt würden.

Aber auch diese Position ist nicht frei von Problemen: Zum einen besteht hier ein Dilemma, weil es in jedem Fall um die Verschonung oder Tötung menschlichen Lebens, also absolut schützenswerter Güter geht. Zum anderen haben Experimente zum Trolley-Problem gezeigt, dass eine solche Position für viele Menschen irgendwann an die Grenzen ihrer moralischen Plausibilität kommt. Dies sieht man, wenn man das Beispiel so verändert, dass man nicht die Wahl zwischen einem und fünf, sondern zwischen einem und hundert, tausend, einer Million oder einer Milliarde Leben hat. Irgendwann würden viele Menschen sagen, es sei richtig, den einen Menschen für die vielen zu opfern. Aber wann kippt die Waage? Darf hier wirklich bei keiner denkbaren Opferrelation verrechnet werden?

Kommen wir zurück zum Problem des autonomen Fahrens und der Notwendigkeit, ein ethisches Entscheidungsprinzip als Algorithmus in das technische System zu implementieren.

Bisher wurde von uns stillschweigend vorausgesetzt, dass die Technologie selbstfahrender Autos vernetzt ist, das heisst, dass während der Fahrt persönliche Daten (zum Beispiel über Alter, Gesundheit, Beruf) aus den Fahrzeugen an einen Zentralrechner gegeben werden, der dann entscheidet, was zu tun ist. Daraus ergeben sich nicht nur gravierende Fragen im Blick auf Persönlichkeitsrechte und Datenschutz, sondern man mag auch einwenden, dass bei einer derart vernetzten Technologie ein utilitaristisches Kalkül die Freiheitsrechte des Einzelnen auf inakzeptable Art einschränkt.

Und auch vom Standpunkt der Akzeptanz der neuen Technologie stellen sich Fragen. Bin ich bereit, in ein Fahrzeug zu steigen, von dem ich weiss, dass es in einer bestimmten Situation meinen Tod oder meine Verletzung in Kauf nimmt, um damit andere zu retten?

Auch hierfür bieten sich prinzipiell technologische Lösungen an. Heute schon kann man in einigen Autos zwischen einem sportlichen und einem komfortablen Modus wählen. Warum nicht in Zukunft auch die Wahl zwischen einem egoistischen (e-drive) und einem altruistischen (a-drive) Modus? Wählt der Fahrer den E-Modus, so wird das Fahrzeug niemals auf Kosten der Passagiere einen Unfall vermeiden, im A-Modus schon.

Auch diese Lösung bereitet Ihnen Unbehagen? Uns auch. Wichtig ist allein zu sehen, dass technologische Innovationen es erfordern, ethische Konflikte explizit zu lösen, die bisher implizit, situativ gelöst wurden. Dies bereitet uns einerseits Unbehagen, denn wir werden gezwungen, Verantwortung in Bereichen zu übernehmen, um die wir uns bisher nicht zu kümmern brauchten.

Darin liegt für uns andererseits aber auch eine grosse Chance. Wir können durch solche explizit gewählten Lösungen Todesfälle vermeiden. Und die Tatsache, dass wir eine Entscheidung fällen und systematisch umsetzen müssen, bevor wir selbst betroffen sind, macht es vielleicht einfacher, aus einer unparteilichen Position, unter dem «Schleier des Nichtwissens», wie dies der Philosoph John Rawls ausgedrückt hat, zu argumentieren. Darüber hinaus werden wir dazu gedrängt, die ethischen Grundlagen, auf denen unsere Gesellschaft fusst, zu reflektieren und zu bestimmen.

Wie wir gesehen haben, kann dies zu ungemütlichen Entscheidungssituationen führen, aber ein rationaler Diskurs – der durchaus auch verschiedene Antworten zulässt – ist allemal besser als ein Wegschauen. Dabei zeigt sich: Das autonome Fahren ist nur ein Beispiel für die Notwendigkeit, in übergreifenden gesellschaftlichen Fragen ethische Entscheidungen zu treffen und dafür Kompetenz aufzubauen.

Ein weiteres Beispiel, das sich in der Frage selbstfahrender Autos bereits andeutet, in seiner Dimension aber weit darüber hinausgeht, ist die zunehmende Digitalisierung der Gesellschaft. Datensammlung im Internet und die damit einhergehende algorithmenbasierte Entwicklung von Persönlichkeitsprofilen, die zum Zwecke eines personalisierten Pricing, personalisierter Werbung und der Auswahl der zur Verfügung gestellten Informationen erhoben werden, sind ein weiteres Grossprojekt, welches wir auch hinsichtlich seiner ethischen Dimension lieber heute als morgen anschauen sollten. Aber das wäre ein anderer Krimi.";https://www.nzz.ch/meinung/kommentare/keine-algorithmen-fuer-ethische-fragen-ld.4483;NZZ;Martin Kolmar;;;
24.06.2016;Selbstfahrende Autos in der Zwickmühle;"Plötzlich springen zwei Kinder auf die Strasse, und der heransausende Autofahrer kann nicht rechtzeitig bremsen. Soll er die Kinder umfahren? Oder seinen Pkw lieber gegen einen Baum lenken? Für jeden Autofahrer ist das eine Horrorvision. Für Ethiker und Philosophen illustriert das Gedankenspiel hingegen eine Herausforderung, die auf Gesetzgeber und Programmierer zukommt, wenn selbstfahrende Autos die Strassen erobern sollten.

Schliesslich muss irgendwer festlegen, nach welchen moralischen Prinzipien der Steuercomputer bei einem drohenden Unfall entscheidet. Soll die Software auf jeden Fall die Kinder schützen? Auch dann, wenn dies bedeutet, die Insassen des Autos umzubringen? Oder geht das Leben der Personen im Fahrzeug vor?
Das Auto opfert seinen Fahrer

Menschen haben zu dieser Frage mitunter eine widersprüchliche Haltung, wie nun eine im Wissenschaftsmagazin «Science» veröffentlichte Studie nahelegt. In einer Reihe von Online-Umfragen ermittelte ein Autorenteam um Iyad Rahwan vom Massachusetts Institute of Technology die Einstellung von insgesamt 1928 amerikanischen Internetnutzern zu autonomen Fahrzeugen.

Ein Grossteil der Befragten sprach sich für selbstfahrende Autos aus, deren Software bei unvermeidbaren Unfällen die Zahl der Opfer minimiert – auch wenn das bedeute, die Insassen des Autos zu opfern. Allerdings gaben deutlich weniger Teilnehmer an, solch ein «utilitaristisches» Auto auch kaufen zu wollen. Die Vorstellung, das eigene Fahrzeug könne einen selbst oder Angehörige opfern, sorge offenbar für Unbehagen, folgern die Forscher. Entsprechende Vorgaben könnten daher die Akzeptanz der Fahrzeuge erschweren. Ob selbstfahrende Autos letztlich eher im Sinne der Allgemeinheit handeln oder die Eigentümer um jeden Preis beschützen werden, ist noch völlig offen. Heutige Prototypen haben noch kein Moral-Modul. Und bis anhin wird vor allem darüber diskutiert, wer entsprechende Richtlinien festlegen soll: die Hersteller oder der Staat. «Wir werden das umsetzen müssen, was der gesetzliche Rahmen vorgibt und was gesellschaftlich akzeptiert wird», sagte Jungo Brüngger, das bei Daimler für Rechtsfragen zuständige Vorstandsmitglied, dazu vor kurzem im NZZ-Interview. Damit schob sie die Verantwortung elegant dem Gesetzgeber zu.
Konflikt mit der Verfassung?

Für diesen dürfte es nicht leicht werden, moralische Standards für selbstfahrende Auto zu definieren. Zumindest in Deutschland verstösst es gegen die Verfassung, in Gesetzen Menschenleben abzuwägen. In der Schweiz könnte es ähnlich heikel sein. Und auch die Alternative – ein in die Software eingebauter Zufallsgenerator, der entscheidet, wen das Auto in einer Extremsituation umbringt – dürfte auf Widerstand stossen.

Aus Sicht von Ethikern ist es umso dringlicher, diese Fragen zu diskutieren, bevor selbstfahrende Pkw durch die Städte kurven. Schliesslich bietet das auch Gelegenheit, die ethischen Grundlagen unserer Gesellschaft zu reflektieren und neu zu bestimmen, wie der St. Galler Volkswirt Martin Kolmar und der Bonner Philosoph Martin Booms im Januar in der NZZ schrieben.

Wissenschafter, die selbst autonome Maschinen entwickeln, warnen hingegen davor, der Moraldiskussion zu viel Gewicht zu geben. «Autonome und teilautonome Autos können sehr viele Menschenleben retten», sagt Roland Siegwart, Robotik-Professor an der ETH Zürich. Er gehe davon aus, dass dieses Argument letztlich die Diskussion um die Ethik selbstfahrender Autos überstrahlen werde. Die in der Software festgeschriebene Moral werde nur in wenigen Fällen zum Tragen kommen. «Und dem werden sehr viel mehr Fälle gegenüberstehen, in denen das selbstfahrende Auto verhindert, dass überhaupt eine brenzlige Situation entsteht.»";https://www.nzz.ch/wissenschaft/technik/moralische-dilemmata-selbstfahrende-autos-in-der-zwickmuehle-ld.91033;NZZ;Robert Gast;;;
09.08.2019;«Künstliche Intelligenz generiert unendlich viel neue Arbeit»;"Frau Gray, in Ihrem Buch «Ghost Work: How to stop Silicon Valley from Building a New Global Underclass» beschreiben Sie Menschen, die im Hintergrund für die künstliche Intelligenz arbeiten. Es sind aber nicht die Entwickler oder Programmierer, wie viele Nutzer vielleicht vermuten. Wer dann?

Es sind Menschen wie Sie und ich, aus allen gesellschaftlichen Schichten, nur sind sie unsichtbar. Die wenigsten wissen, dass künstliche Intelligenz den Menschen braucht, um zu lernen. Sei es, dass ein System Hassrede aufspüren soll wie bei Facebook, oder um zu erkennen, ob ein Bild eine Katze zeigt. Dafür müssen zunächst unzählige Bilder kategorisiert beziehungsweise Hasskommentare identifiziert werden. Das wird meistens von sogenannten Crowdworkern gemacht, auf Plattformen wie Amazon Mechanical Turk. Entwickler stellen ihre Aufgaben auf die Plattform, und die Arbeiter suchen sie sich aus. Dort entstehen diese unendlichen Mengen an Daten, die künstliche Intelligenz benötigt. Sie nennen diese Crowdworker «Geisterarbeiter», weil sie für die meisten Menschen unsichtbar sind. Wie kam es dazu? Das ist mir klar geworden, als ich bei Microsoft Research anfing. Ich habe die Informatiker dort gefragt: Wer sind denn diese Menschen? Es ist so typisch für Entwickler, mit gesenktem Kopf über einer Aufgabe zu sitzen. Viele denken nicht darüber nach, woher die etikettierten Daten kommen. Wenn sie ihre Jobs auf Mechanical Turk stellen und zurückbekommen, sehen sie ja nicht, wie ein Mensch daran arbeitet. Sie beschreiben die Aufgabe und bekommen die Daten sauber und bearbeitet wieder zurück.

Nicht einmal den Entwicklern selbst ist bewusst, dass menschliche Arbeit in ihre Systeme einfliesst?

Zumindest nicht genauer: Niemand konnte mir meine Frage beantworten, was das für Menschen sind. Manchen war es gar nicht so recht bewusst – andere hatten sogar Angst, genauer nachzuforschen, denn sie ahnten schon, dass sie möglicherweise auf ungute Arbeitsbedingungen stossen würden. Eine typische Antwort war: «Ich weiss es nicht, und ich muss es nicht wissen.» Deshalb habe ich angefangen selbst nachzuforschen.

Auf was für Arbeitsbedingungen sind Sie denn gestossen?

Es sind unsichere Arbeitsbedingungen, abhängig von Projekten, die Entwickler auf den Plattformen in Auftrag geben. Diese Jobs erledigen oft Menschen, die keinen regulären Vollzeitjob finden. Bei vielen verhindern das verschiedene Beschränkungen, zum Beispiel, wenn sie zeitlich eingeschränkt sind, weil sie ein Familienmitglied pflegen oder Kinder erziehen und flexibel arbeiten wollen. Wir konnten mit Menschen sprechen, die diese Jobs wieder aufgegeben haben: Die Hauptgründe waren, dass sie zu wenig Unterstützung bekamen und dass die Aufgaben zu kognitiver Erschöpfung führten.

Gerade wurde bekannt, dass bei Google Menschen Gespräche anhören, die Nutzer mit ihrem Smartphone geführt hatten – selbst dann, wenn sie nicht zuvor «Ok Google» sagten, das Startsignal für die Anwendung, um zuzuhören. Und vor einigen Wochen wurde bekannt, dass bei Amazon ebenfalls Menschen Alexa-Gespräche anhören. Es gab einen grossen Aufschrei in der Bevölkerung, weil Sorgen darüber aufkamen, dass private Informationen von Menschen anstatt von Maschinen gehört werden. Aber darum geht es nicht, oder?

Auch hier war der Hintergrund, dass die künstliche Intelligenz trainiert werden muss – es ist ein ganz üblicher Vorgang. Doch in der Öffentlichkeit ist das ebenfalls kaum bekannt. Viele hängen dem Mythos an, dass künstliche Intelligenz bedeutet, dass alles automatisch abläuft. Aber es läuft viel weniger automatisch, als die meisten denken. Natürlich müssen Mitarbeiter bei Google und Amazon immer wieder mal zuhören und prüfen, ob der Algorithmus alles richtig versteht, und ihn dann weiter trainieren.

Wird es einen Punkt geben, an dem die künstliche Intelligenz (KI) «alles» weiss? An dem die Crowdworker einfach genug Daten klassifiziert haben, so dass wirklich alles automatisch abläuft und keine Menschen mehr benötigt werden?

Nein, diesen Punkt wird es aus meiner Sicht nie geben, auch wenn das viele glauben, sogar viele Entwickler. Aber die Annahme ist doch absurd: Sprache verändert sich, Kultur verändert sich. Wie soll die künstliche Intelligenz aus unseren Daten schlau werden, wenn sich deren Bedeutung verändert? Es wird immer Menschen brauchen, die Daten etikettieren, klassifizieren und säubern, damit KI Sinn aus ihnen schöpfen kann. Sprache ist ein gutes Beispiel: Systeme wie Alexa verstehen schon sehr viel, und meistens funktionieren sie gut. Für die meisten Fälle gibt es genug Daten, so dass die Maschinen keine Regeln brauchen, sondern allein aus der Statistik lernen können: aus den Beispielen also, die Menschen klassifiziert haben. Aber oft geht es bei menschlicher Kommunikation um Nuancen. Genug Beispiele für alle Feinheiten wird es nie geben.

Müssen wir also gar keine Angst davor haben, dass die KI uns die Arbeit wegnimmt?

Das ist das grösste Paradox der künstlichen Intelligenz: Sie hat den Ruf, uns Arbeit abzunehmen. Dabei generiert sie unendlich viel neue Arbeit. Allerdings eben auch repetitive Arbeit, die nicht besonders abwechslungsreich ist. Und die Crowdworker sind vereinzelt, niemand investiert in ihre Fortbildung. Das ist wenig nachhaltig. Dieser Fehler ist seit der Industrialisierung immer wieder in der Geschichte geschehen: Menschen, von denen wir denken, dass wir sie bald nicht mehr brauchen, behandeln wir nicht gut. Wir werden sie aber immer brauchen. Und wir müssen auf die Bedürfnisse dieser Arbeiter jetzt eingehen. Sie müssen endlich wahrgenommen werden: In der Textilindustrie hat es für die Arbeiter in Bangladesh auch viel bewegt, als die Unternehmen gezwungen wurden, uns zu sagen, wessen Arbeit in unseren T-Shirts steckt. Die Nutzer von KI-Systemen – und das sind wir alle – müssen endlich zurückverfolgen können, wer alles Arbeit in die Dienstleistungen gesteckt hat, die sie nutzen.

Im vergangenen Jahr ging durch die Presse, wie Menschen bei Facebook Hassrede eliminieren und dabei den ganzen Tag die schrecklichsten Posts lesen mussten – eine harte Arbeit, die bei vielen zu psychischen Problemen geführt hat. Zuckerberg sagte damals: Deshalb muss das automatisiert werden. Wenn ich Sie richtig verstehe, wird es aber immer Menschen brauchen, die Hassrede auf diese Weise identifizieren?

Ja, um die künstliche Intelligenz zu trainieren. Dieser Prozess wird nie abgeschlossen sein. Hassrede verändert sich ja ebenfalls. Da KI aus Beispielen lernt, erkennt sie nur die Hassrede, die schon oft vorgekommen ist. Ich weiss, dass viele denken, das sei komplett automatisierbar. Aber das ist eine Illusion. KI wird Menschen nicht nur vorübergehend brauchen, sondern immer.

Sollten wir vielleicht einen Schritt zurücktreten und manche Entscheidungen eben nicht mittels künstlicher Intelligenz automatisieren? Sondern mit einem Computerprogramm, dessen Entscheidungen fest einprogrammiert sind?

Genau. Das ist ein Ziel des Buchs: den Menschen bewusst zu machen, dass es Fälle gibt, in denen womöglich besser ein regelbasiertes System für einen Menschen eintritt. Soll eine KI meinen Kalender organisieren, muss sie exakt den Grad meiner sozialen Beziehungen verstehen. Das ist hoch komplex. Entwickler übersehen oft, wie schwierig es ist, menschliche Bedürfnisse zu verstehen. Es fängt schon bei scheinbar einfachen Problemen an. Immer wieder üben Ärzte Kritik an Systemen, die per KI zum Beispiel medizinische Bilder interpretieren: Die Ärzte sind enttäuscht, weil es nicht so perfekt ist, wie es ihnen angepriesen wurde. Perfekt wird diese Automatisierung nie sein. Das merken ja auch die ersten Unternehmen: Immer wieder übernimmt eine Person eine Aufgabe, die angeblich eine KI macht.

So wie bei Google Duplex, wo künstliche Intelligenz angeblich Restaurantreservierungen für den Nutzer übernimmt – doch als die «New York Times» das kürzlich testete, riefen meist Menschen an und eben keine Maschine. Versuchen grosse Unternehmen den Mythos der magischen KI aufrechtzuerhalten?

Ich glaube nicht, dass diese Unternehmen bewusst verheimlichen, dass Menschen involviert sind. Es dürfte eher so sein, dass die Marketingabteilungen selbst nicht wissen, wie viele Menschen hinter so einem Prozess stecken, und dass diese Menschen Daten klassifizieren. Wir müssen das als eine Wertschöpfungskette betrachten: Je grösser ein Unternehmen ist, desto weniger wahrscheinlich ist es, dass jeder weiss, was alles in das Endprodukt einfliesst.";https://www.nzz.ch/wissenschaft/kuenstliche-intelligenz-generiert-arbeit-ld.1498206;NZZ;Eva Wolfangel;;;
14.11.2017;Die Schweiz als Supercomputer-Grossmacht;"Die zwei schnellsten Computer der Welt stehen in China, der drittschnellste heisst Piz Daint und steht in Lugano. Es ist ein vom amerikanischen Hersteller Cray geliefertes XC50-System, das im Schweizerischen National Supercomputing Centre (CSCS) in Lugano installiert ist. Piz Daint belegte vor einem Jahr noch Platz acht, stiess dann aber im Sommer 2017 auf den dritten Platz vor und konnte sich hier halten. Seine Leistung wird mit 19,6 Petaflops angegeben.
Chinesische Dominanz

Sunway Taihu Light, ein am chinesischen National Supercomputing Center in Wuxi installiertes System, führt die Rangliste an mit 93 Petaflops, Tianhe-2 auf Platz zwei bringt es auf 33,86 Petaflops. Tianhe-2 wurde von Chinas National University of Defense Technology entwickelt und wird im National Supercomputer Center in Guangzho eingesetzt.

Insgesamt ist die Schweiz mit drei Systemen auf der Top-500-Liste präsent. Diese Liste wird seit 25 Jahren zwei Mal pro Jahr zusammengestellt. Die Amerikaner dominierten diese Liste, doch in jüngster Vergangenheit wurden sich von den Chinesen verdrängt: 202 der 500 schnellsten Rechner stehen in China, sie erbringen zusammen 35,4 Prozent der Supercomputer-Rechenleistung. Der schnellste Computer der USA schafft es auf Platz fünf der Top 500, die 143 amerikanischen Supercomputer erbringen zusammen 29,6 Prozent der Top-500-Rechenleistung. Hinter China und den USA folgen Japan mit 35 Supercomputern, Deutschland (20), Frankreich (18) und Grossbritannien (15).
Hoffen auf die Koprozessoren

Die Gesamtperformance aller 500 Systeme ist auf 845 Petaflops angewachsen, verglichen mit 749 Petaflops vor einem halben Jahr und 672 Petaflops vor einem Jahr. Obwohl die Gesamtperformance um fast 100 Petaflops zunahm, liegt der relative Anstieg deutlich unter dem langfristigen historischen Trend. 94,2 Prozent der Prozessoren stammen von Intel. 102 Systeme arbeiten mit Beschleuniger oder Koprozessoren, 86 davon verwenden Grafikprozessoren von Nvidia.";https://www.nzz.ch/digital/die-schweiz-als-supercomputer-grossmacht-ld.1328482;NZZ;Stefan Betschon;;;
29.04.2016;Hilfe von Dr. Computer;"Es gibt derzeit wenige Begriffe, die mehr en vogue sind als Big Data. Auch in der Medizin wird ständig davon gesprochen. Keine Konferenz, an der nicht die Chancen und Risiken von Big Data erörtert würden. Und das, obwohl hinter dem Namen nichts fundamental Neues steckt. Die Medizin war schon immer auf Daten – oder wie man bisher sagte: Informationen – angewiesen.

Das beginnt mit der Befragung des Patienten nach seinen Symptomen: Wo tut es weh? Wie lange schon? Sind die Beschwerden abhängig vom Essen? Solche Informationen – oder klinischen Daten – geben meist schon deutliche Hinweise auf das zugrunde liegende Problem. Die anschliessende körperliche Untersuchung sowie allfällige Röntgenbilder, EKG und Laboranalysen führen nicht nur zu weiteren Daten, sondern meist auch zur Diagnose.

Doch nicht immer ist der Fall so einfach und übersichtlich. Dann kommt Big Data ins Spiel. Wobei das zentrale Wort «big» ist. Darum geht es. Um Informationen, die in grosser Menge, grosser Geschwindigkeit und grosser Vielfalt anfallen. Richtig analysiert, können diese Daten zu neuen wissenschaftlichen Erkenntnissen führen, bei schwierigen Entscheiden helfen und die Prozesse im Spital und in der Praxis verbessern. Das ist keine Zukunftsvision, die Entwicklung hat längst begonnen.
Jederzeit und überall

Denn in unserer modernen, globalisierten Welt werden Daten überall und jederzeit gesammelt, mit dem Smartphone ebenso wie mit Sensoren, die unsere Körperfunktionen sowie Veränderungen in der Umwelt (z. B. allergene Substanzen) überwachen können – und das in real-time. Computer mit gigantischen Rechen- und Speicherleistungen sowie neue mathematisch-statistische Verfahren und Algorithmen helfen, aus dem unendlichen Datenmeer sinnvolle Informationen herauszufiltern. So lassen sich heute schon erstaunlich genaue Wetterprognosen erstellen und der Verkehr regeln. Warum sollte Big Data nicht auch die Medizin revolutionieren?

Im Gegensatz zu anderen Branchen wie den Banken befindet sich das Gesundheitswesen erst am Anfang des digitalen Transformationsprozesses. So arbeitet in der Schweiz immer noch ein Grossteil der niedergelassenen Ärzte mit Papier. Und auch die landesweite Einführung eines elektronischen Patientendossiers (EPD), ohne das eine E-Health-Strategie illusorisch ist, steht noch bevor. Kommt dazu, dass das Gesetz zur Nutzung des EPD in der Schweiz eine doppelte Freiwilligkeit vorsieht. Das heisst, Ärzte wie Patienten können auf die digitale Krankenakte verzichten.

Dass das allerdings viele tun werden, darf bezweifelt werden. Denn zu gross sind die Versprechen rund um Big Data. An erster Stelle steht der Nutzen für den Einzelnen. Die Medizin werde viel präziser, sagen Fachleute. Und sie betonen, dass eine personalisierte Medizin, bei der jeder Patient eine auf seine Krankheit zugeschnittene Therapie erhält, ohne Big Data schlicht undenkbar sei.

Besonders gut lassen sich die neuen Möglichkeiten bei Krebserkrankungen veranschaulichen. Denn seit der Entzifferung des Erbguts wird immer klarer, dass es den typischen Krebs nicht gibt. Es gibt nicht einmal den typischen Brust- oder Darmkrebs. Die Heterogenität ist vielmehr so gross, dass jeder Patient praktisch seinen eigenen Krebs hat – wobei die Unterschiede auf der Ebene der Gene, von Signal-Proteinen oder des Stoffwechsels liegen können. Solche für die Behandlung relevanten Unterschiede festzustellen, setzt aufwendige diagnostische Tests voraus, die Millionen von Daten generieren, die nur mit Computerhilfe gelesen werden können.

Mit Big-Data-Ansätzen sollen aber nicht nur biologische Daten, sondern auch Informationen zu Gesundheit, Aktivität und Lebensstil erfasst werden. Diese beim Einzelnen gewonnenen Informationen können dann mit den Daten von Millionen von Menschen verglichen werden. So lassen sich ähnliche Patientenschicksale finden. Indem ihre Daten zu Therapie und Krankheitsverlauf abgeglichen werden, wird es möglich, die für den einzelnen Patienten beste Therapie zu bestimmen. Es spielt dann keine Rolle mehr, ob ich in New York oder in Aarau behandelt werde. Nicht der einzelne Arzt, sondern das weltweite Anzapfen von Wissen macht den Unterschied.
Aufbau der IT-Infrastruktur

Ganz so einfach ist es natürlich nicht. Um Big Data für die Medizin fruchtbar zu machen, müssen relevante Daten nicht nur aufgespürt, sondern auch «gelesen» werden können. In dieser Beziehung besonders anspruchsvoll sind unstrukturierte Daten wie handschriftliche Notizen, Bilder oder Audio-Files. Zudem braucht es für den weltweiten Datenaustausch eine kompatible IT-Infrastruktur, die in der Schweiz unter dem Namen Swiss Personalized Health Network erst aufgebaut wird.

Wie weit Big Data schon fortgeschritten ist, zeigen unzählige Einzelprojekte sowie erste Erfahrungen mit dem Star der Stunde: IBM Watson. Dabei handelt es sich um ein kognitives Computersystem, das die jüngsten Fortschritte in den Bereichen künstliche Intelligenz, maschinelles Lernen und Spracherkennung integriert. Dadurch kann der Computer, der auch ausserhalb der Medizin zum Einsatz kommt, die verfügbaren Datenquellen sehr schnell auf eine bestimmte Frage hin untersuchen – und mit jeder Aufgabe lernen und besser werden. In den USA hilft das System in einigen Kliniken den Ärzten bereits beim Erstellen von Krebsdiagnosen.

Dem IT-Tausendsassa wird aber noch mehr zugetraut. So soll IBM Watson auch das Team von Jürgen Schäfer vom Universitätsklinikum in Marburg unterstützen. Der als deutscher Dr. House bekannte Arzt – nach der gleichnamigen Fernsehserie um den genialen, aber verschrobenen Star-Mediziner – hat in Marburg ein Zentrum für unerkannte und seltene Erkrankungen aufgebaut. Dort treffen jährlich etwa 2500 Diagnose-Anfragen ein. Meist sind die Patienten schon von Dutzenden Ärzten gesehen worden. Schäfer erzählt von einer Frau, deren Depression auf eine hormonfreisetzende Spirale zurückzuführen war. Oder einem Mann mit Bauchschmerzen, die von Würmern aus dem Aquarium herrührten.

Laut Schäfer ist IBM Watson nur die nächste Generation von IT-Hilfe in der Medizin. Denn sein Team setzt bereits internetbasierte Diagnose-Unterstützungs-Systeme ein. In solche Systeme tippt der Arzt die wichtigsten Symptome des Patienten ein, und der Computer spuckt dann eine Liste mit möglichen Diagnosen aus. Damit aber nicht genug: Selbst eine einfache Google-Suche mit den Wörtern «blind, taub und herzschwach» führt heute in Minutenschnelle zu einer seltenen Vergiftung durch Kobalt – eine Diagnose, die vor wenigen Jahren noch praktisch unbekannt war. Das zeigt nicht nur, dass Big Data längst Realität ist, sondern auch, wie schnell IT-Systeme lernen können.

«Computer sind sehr gut darin, die Symptome des Patienten und andere Veränderungen in einen zeitlichen Zusammenhang zu bringen», sagt Schäfer. Etwa bei dem Patienten, der seit sechs Jahren Aquarien hat und seit fünfeinhalb Jahren über Bauchschmerzen klagt. Dass zwischen den beiden Informationen ein Zusammenhang bestehen könnte, werde in einem unstrukturierten Gespräch mit dem Patienten leicht übersehen», erklärt Schäfer. Denn kein Arzt denke bei Aquarien an Bauchschmerzen. Der Computer dagegen suche für alle zeitlichen Koinzidenzen nach möglichen Erklärungen – und komme so auch auf die Wurmkrankheit Bilharziose, die beim Patienten tatsächlich diagnostiziert werden konnte.

Schäfer ist ein grosser Befürworter von IT-Unterstützung in der Medizin. Damit könnten viele Menschenleben gerettet werden, ist er überzeugt. So etwa, wenn beim Patienten mit Computerhilfe die verschriebenen Medikamente auf Nebenwirkungen und gefährliche Wechselwirkungen getestet würden. Von einer Journalistin einmal gefragt, ob der Computer nicht das Vertrauensverhältnis zwischen Arzt und Patient belaste, antwortete er mit einer Gegenfrage: «Was würden Sie sagen, wenn auf einem Flug nach New York der Captain mitteilen würde: Zur Verbesserung des Vertrauensverhältnisses stelle ich jetzt den Bordcomputer aus?»

So wie Big Data bei der Patientenbetreuung hilft, lässt sich der Ansatz auch für die öffentliche Gesundheit nutzen. Dabei kann auf ganz neue Datenquellen und Datenströme zugegriffen werden, die sich im Internet oder in sozialen Netzwerken finden lassen. Eines der ersten Beispiele für dieses auch digitale Epidemiologie genannte Verfahren war das Instrument «Google Flu Trends», mit dem der Technologiekonzern anhand von Suchanfragen die zeitliche und örtliche Ausbreitung von Epidemien voraussagen konnte – und zwar schneller, als das mit den herkömmlichen Überwachungssystemen, die auf Meldungen von Ärzten und Spitälern basieren, möglich war.
«Noisy», aber wertvoll

Der Biologe Marcel Salathé von der ETH Lausanne (EPFL) benützt für seine digitalen Epidemiologie-Projekte Twitter-Daten. Diese seien öffentlich und daher einfach verfügbar, sagt der Forscher. Als Nachteil sieht er das, was im Englischen als «noisy» bezeichnet wird – die Tatsache, dass im Datenstrom viele unklare Signale «mitschwimmen», welche die Analyse erschweren.

Trotz dieser Herausforderung konnte Salathé mit Twitter-Daten dokumentieren, wie sich in den USA während der Grippepandemie von 2009 die Einstellung der Bevölkerung gegenüber der Grippeimpfung in den einzelnen Gliedstaaten unterschied. Was den Forscher besonders freute: Seine Daten deckten sich mit den später von den Gesundheitsbehörden publizierten Impfraten. «Das zeigt, dass wir mit den neuen Datenquellen sinnvolle Dinge messen können», sagt Salathé.

Weil wir mit Big Data immer besser in die Zukunft schauen können, wird auch von prädiktiver Medizin gesprochen. Beim Einzelnen können die individuellen Gesundheitsdaten immer genauere Hinweise auf künftige Krankheiten liefern. Dass dies auch auf der Ebene der Gesellschaft möglich ist, zeigen Wissenschafter wie Ben Reis von der Harvard University in Boston, USA. Für seine Big-Data-Analysen verwendet der Forscher die denkbar einfachsten Gesundheitsdaten: Diagnose-Codes und Angaben zur Behandlung, wie sie auf den Dokumenten stehen, die zwischen Ärzten, Spitälern und Krankenkassen ausgetauscht werden. Alleine mit diesen Angaben konnte Reis Patienten mit hohem Risiko für häusliche Gewalt oder Suizid erkennen – noch bevor die behandelnden Ärzte Verdacht schöpften .

Sollten uns solche Entwicklungen nicht Angst machen? Droht uns – in Anlehnung an Aldous Huxleys pessimistischen Zukunftsroman «Schöne neue Welt» – nicht eine Medizinwelt, wie wir sie uns in unseren schlimmsten Albträumen nicht vorgestellt haben? Mit dem gläsernen Patienten, der auf dem Altar der personalisierten Medizin seine Privatsphäre opfert? Die Sorge ist berechtigt. Andererseits gilt es zu bedenken, dass heute schon Daten in grossen Mengen gesammelt und gespeichert werden. Weil das nicht zum Spass geschieht, dürfte es das Klügste sein, die Nutzung des «wichtigsten Rohstoffs des 21. Jahrhunderts» so zu gestalten, dass nicht nur Krankenkassen und kommerzielle Firmen, sondern vor allem auch der Patient und Bürger davon profitieren.

Dass es auf diesem Weg noch viele Hindernisse zu beseitigen gilt, ist unbestritten. An erster Stelle stehen Fragen zum Datenschutz und zur Datensicherheit (vgl. Infobox unten). Dass diese Themen nicht trivial sind, zeigen die Erfahrungen in Dänemark. Das Land, das europaweit eine Vorreiterrolle in Sachen E-Health einnimmt, begann vor rund zwanzig Jahren mit dem Aufbau eines Netzwerks für digitale Gesundheitsdaten. Wie Hans Erik Henriksen, CEO von HealthCare Denmark, vor kurzem an einer Tagung in Bern einräumte, sind in Dänemark Fälle vorgekommen, wo Leute die Gesundheitsdaten von Prominenten und Nachbarn geknackt haben.
Technik alleine genügt nicht

Neben Fragen zur Privatsphäre werden wir uns auch mit ganz neuen Problemen beschäftigen müssen: Wer zum Beispiel ist verantwortlich, wenn ein Computer-Algorithmus eine Therapie empfiehlt, die nicht den erwünschten Effekt hat? Zudem sollten wir uns als Gesellschaft davor hüten, die Zukunft der Medizin alleine im technisch-wissenschaftlichen Fortschritt zu sehen. Denn für ein langes und gesundes Leben sind andere Bereiche wie eine gute Krankenpflege oder tragfähige soziale Netze ebenso wichtig. Auch wenn abzusehen ist, dass Big Data die Möglichkeiten der Medizin stark erweitern wird, bleibt eine zentrale Frage offen: Wird der digitale Fortschritt uns auch glücklicher machen?";https://www.nzz.ch/wissenschaft/medizin/hilfe-von-dr-computer-1.18732945;NZZ;Alan Niederer;;;
13.03.2017;Smart – und völlig gewissenlos;"Ob auf der Strasse, in der Wohnung oder im Operationssaal: Schon in naher Zukunft werden Roboter mehr und mehr unseren Alltag bevölkern. Die Frage, die die Forschung umtreibt, ist, ob diese Wesen dereinst ein Bewusstsein entwickeln – und wir es vielleicht gar nicht merken. Das würde den Einsatz von Robotern gewissermassen zur Sklaverei machen, zur Ausbeutung von potenziell selbstverantwortlichen Wesen. Und damit müsste man sich fragen, ob Robotern nicht Rechte und Pflichten zugestanden werden müssten, worüber der Rechtsausschuss des EU-Parlaments zurzeit diskutiert.
Blankocheck für Gewalt

Doch einmal abgesehen davon, dass man damit etwas Rechte verliehe, was lediglich ein Werkzeug ist, und dass man die Grenzen zwischen Mensch und Maschine einzureissen droht: Das Problem ist nicht, dass künstliche Intelligenzen irgendwann ein Bewusstsein erlangen und der Mensch es nicht merkt. Das Problem liegt vielmehr darin, dass wir gewissenlose Automaten schaffen.

Der australische Philosoph und Kognitionsforscher David Chalmers warnte kürzlich auf der Konferenz «Superintelligence: Science or Fiction?» des Future of Life Institute davor, dass wir eine Welt ohne Bewusstsein schaffen. «Für mich erhöht das die Wahrscheinlichkeit einer massiven Betriebsstörung in der Zukunft: die Wahrscheinlichkeit, dass wir menschliche oder übermenschliche KI-Systeme entwickeln, von denen keines bewusst ist. Das wäre eine Welt, die vielleicht von grossartiger Intelligenz wäre, aber es wird eine Welt ohne Bewusstsein und ohne subjektive Erfahrung sein.» Chalmers ist sehr vorsichtig, was die Annahme betrifft, Roboter könnten ein Bewusstsein entwickeln. Er definiert Bewusstsein restriktiv als «Gefühl dafür, wie es ist, ein intelligentes Wesen zu sein», wobei das Verständnis seiner selbst eigentlich natürlich weit über die Selbstreflexion hinausgeht und auch einen politischen Anspruch im Sinn von Freiheit begründet. Um dem Leben Bedeutung zu geben und Wert zu schaffen, seien subjektive Erfahrung und Bewusstsein nötig, hält er fest. Vor allem aber wäre eine Welt ohne Bewusstsein eine Welt der Regellosigkeit, ein Blankoscheck für Gewalt und Willkür. Wo es kein Bewusstsein gebe, betont er, gebe es auch keine Moral, keine Werte. Chalmers spricht von einem O-Outcome, einem Ergebnis, bei dem Moral schon gar nicht in den binären Zahlencodes vorkommt. Ist das der Zweck der künstlichen Intelligenz – dass moralische Abwägungsprozesse automatisiert werden mit dem Ergebnis, dass am Ende der Rechenoperation null herauskommt?
Seelenlose Automaten

Die Entstehung des Bewusstseins ist eines der grössten Rätsel der Menschheit. Natur- und Geisteswissenschafter streiten, ob das menschliche Bewusstsein nur ein Epiphänomen biochemischer Prozesse ist, eine Art unnützes Abfallprodukt der Evolution (so die These der Naturalisten), oder ob es gerade ein evolutiver Vorteil der Spezies Mensch war, um neu erkannte Aufgaben zu meistern.

Die These von bewusstseinsfähigen Maschinen wird damit begründet, dass Computer wie Hirne rechnen können. Bereits 1955 wurde an der Konferenz «The Design of Machines to Simulate the Behavior of the Human Brain» über die Frage diskutiert, ob man die Funktion des Gehirns durch einen mechanischen Denkapparat simulieren könne. Die Konferenzteilnehmer stellten schon zu Beginn klar, dass man einen Unterschied machen müsse zwischen einer Simulation der Struktur und einer Simulation der Funktion. Auch der Umstand, dass moderne Digitalcomputer und auch das Nervensystem elektrischen Impulsen folgten, dürfe nicht zu der Fehlannahme einer strukturellen Analogie verleiten. Das betonte der Mathematiker Alan Turing 1956 in seinem Aufsatz «Can machines think?». Wenn man Chalmers' Zurückhaltung in Bezug auf die Annahme von Bewusstsein als Prämisse akzeptiert und annimmt, dass künstliche Intelligenzen wohl nie ein Bewusstsein erlangen werden, muss man sich dem Problem auf zwei Ebenen nähern. Da ist zum einen die zunehmende Automatisierung, die Zunahme von KI-Systemen in allen gesellschaftlichen Bereichen – in urbanen Systemen, in der Justiz, im Gesundheitswesen, an Schulen, an den Börsen. Zum anderen aber sind wir mit der Tatsache konfrontiert, dass der Mensch immer maschinenähnlicher wird – und damit am Ende sogar sein Bewusstsein infrage steht.
Die Maschine Mensch

Das mechanistische Paradigma, das die Welt als grosse Maschine begreift, ist ein altes Konzept der Geistesgeschichte. Im 18. Jahrhundert veröffentlichte der französische Arzt Julien Offray de La Mettrie zum Entsetzen der Aufklärer sein Werk «Der Mensch als Maschine», in dem er die These aufstellte, der menschliche Körper sei eine Maschine, «welche selbst ihr Triebwerk aufzieht». Für La Mettrie war der Mensch im Vergleich zum Tier lediglich eine komplexere Art von Maschine.

Frank Schirrmacher schreibt in seinem Buch «Ego: Das Spiel des Lebens», dass man La Mettries «Der Mensch als Maschine» auch als Bauanleitung für das preussische Heer, «gewiss aber auch für das Weltbild von Friedrichs Untertanen lesen kann» – eine Reihe steuerbarer Befehlsempfänger, die in einem Obrigkeitsstaat auf Knopfdruck «funktionieren» und gehorchen. Auch Michel Foucault wies darauf hin, dass Friedrich II. seine Armee in einen «Automaten» mit mechanisch gedrillten Bewegungsabläufen verwandelt habe. Und wo heute der Bau von Robotern technisch schwer umsetzbar ist, muss der Mensch eben selbst zum Roboter werden. Die Entwicklung ist gegenläufig: Menschen delegieren immer mehr kognitive Kompetenzen an Maschinen und lassen sich in immer mehr Bereichen von Algorithmen leiten – bei der Beschaffung von Information, bei der Navigation im Auto oder sogar bei der Partnerwahl über Internetplattformen. Dabei werden sie selber immer maschinenähnlicher. Anderseits haben wir es mit Maschinen und KI-Systemen zu tun, die immer intelligenter und zumindest in gewisser Weise vielleicht sogar menschenähnlicher werden.

Dass der Mensch mit den intelligenten Computern Millionen von gewissenlosen Entitäten kreiert, die ihr eigenes Tun nicht reflektieren, sollte uns mehr beunruhigen als die Vorstellung, dass Maschinen ein Bewusstsein erlangen. Fragt sich nur, was das «Ich» konstituiert in Zeiten, wo unsere digitalen Doppelgänger in Serverfarmen lagern und das Smartphone als externe Festplatte unseres Gehirns mehr über uns weiss, als wir selbst wissen – ein Erinnerungsspeicher, der die Vergangenheit lückenlos speichert.
Lässt sich Geist berechnen?

Chalmers' Antwort auf die Vorstellung einer von künstlicher Intelligenz kontrollierten Welt ohne Bewusstsein wäre eine Welt, in der den künstlichen Agenten ein Bewusstsein einprogrammiert wird. «Vielleicht gibt es deshalb einen Imperativ, menschenähnliche künstliche Intelligenzen zu kreieren, so dass wir maximal sichergehen können, dass Bewusstsein entsteht.» Ist die maschinelle Bewusstseinserweiterung also der kategorische Imperativ der digitalen Moderne? Indem wir uns selber in allen menschlichen Charakteristika reproduzieren, können wir eine seelenlose Welt von Maschinen verhindern, so ist Chalmers überzeugt. Er schliesst nicht aus, dass Computer ein Bewusstsein entwickeln können. Wenn es gelänge, die Struktur des menschlichen Gehirns auf einem Computer zu reproduzieren und Neuronen durch Chips zu ersetzen –würde dann, fragt er, irgendetwas fehlen, was im Gehirn vorhanden ist? Der Informatiker David Gelernter würde dem entschieden widersprechen. In seinem Buch «Gezeiten des Geistes. Die Vermessung unseres Bewusstseins» argumentiert er, das menschliche Gehirn sei mehr als ein organischer Computer. Der Geist lasse sich nicht von Maschinen berechnen.

Das ist sicher richtig. Doch was ist die Einzigartigkeit des Geistes wert, wenn sich der Mensch selbst zum Automaten zerlegt und zum willfährigen Befehlsempfänger von geistlosen Robotern macht, deren Determinismus so gar nichts gemein hat mit menschlicher Intuition und Differenzierung? Worauf können Menschenrechte noch gründen, wenn sich der Mensch selber zum Cyborg aufrüstet? Technische Dispositive wie Algorithmen sind nicht unmoralisch, sondern amoralisch, und das ist noch viel schlimmer.
Ohne Kompass im «Maschinozän»

Zu glauben, durch die Duplizierung von neuronalen Strukturen entwickle sich mit der Zeit über eine gewissermassen genetische Naturgesetzlichkeit ein wie auch immer geartetes Bewusstsein von Maschinen, ist ein fataler Irrglaube. Gehirne operieren nicht algorithmisch. Der Mediziner Miguel Nicolelis argumentiert, das Gehirn sei nicht computerisierbar, weil menschliches Bewusstsein das Ergebnis unvorhersagbarer, nichtlinearer Interaktionen zwischen Milliarden von Zellen sei. Bewusstsein ist die Conditio sine qua non von Menschlichkeit. Wenn die Welt, die der Philosoph Huw Price in Anlehnung an die vom Menschen geschaffene Weltfabrik des Anthropozäns als «Maschinozän»bezeichnet, von immer mehr gewissenlosen Automaten ohne moralischen Kompass bevölkert wird, wird sie unmenschlicher werden. ";https://www.nzz.ch/feuilleton/wie-smart-werden-die-roboter-denn-sie-wissen-nicht-was-sie-tun-ld.150447;NZZ;Adrian Lobe;;;
11.01.2017;Mein bester Freund ist ein Algorithmus;"Mein bester Mitarbeiter hat keinen Namen. Er verrichtet unermüdlich seine Arbeit, pausenlos und ohne zu murren. 24 Stunden täglich und sieben Tage die Woche. Ferien kennt er nicht, Müdigkeit auch nicht. Gewissenhaft wie zuverlässig schuftet er, ohne zu schwitzen, ohne schlechte Gewohnheiten, ohne dumme Bemerkungen zu machen. Er geht niemandem auf die Nerven. Wie ein Maschinchen gehorcht er seinem genetischen Code. Er ist ein Maschinchen. Lautlos, ohne Zahnräder oder Antrieb, in seinem Erbgut stehen lediglich Programmierzeilen. Das ist mein bester namenloser Mitarbeiter. Ich habe ihn noch nie gesehen. Und trotzdem kennt er mich besser als ich mich selbst.

Seine einzige Aufgabe besteht darin, Mails von mir fernzuhalten, die ich nicht lesen will. Ich habe noch nie wirklich bemerkt, wie viel er zu tun hat und dass er ständig im Einsatz ist (selbst wenn er gerade nichts zu tun hat: Auch Achtsamkeit strengt an). Vermutlich kann ich ihm kein grösseres Kompliment machen als dies: Mir hat noch nie etwas gefehlt, obwohl er pausenlos herausfiltert, was mich mutmasslich nicht interessiert.

Woher er meine Vorlieben und Abneigungen kennt? Er schaut mir zu und beobachtet mich. Einmal eine Mail ungelesen gelöscht – und zack, er registriert es. Von dem betreffenden Absender kommt nichts mehr unter meine Augen. Mein bester Mitarbeiter könnte mir Angst machen.
Meine Begegnung mit Siri

Aber vielleicht müsste ich sowieso sagen: meine beste Mitarbeiterin. Denn die dienstbaren Geister im digitalen Zeitalter sind, dem Himmel sei's geklagt, allesamt Frauen, nein: Damen. Seit einiger Zeit versucht eine solche Dame auf meinem iPad mit mir Kontakt aufzunehmen, hartnäckig und etwas zudringlich.

Ich kann mich nicht mehr erinnern, wann es begann, plötzlich war sie da: Als ich einmal selbstvergessen zu lange auf dem Home-Button verweilte, meldete sie sich wie von Zauberhand. Als hätte sie mein Zaudern registriert, eilte sie mir zu Hilfe. Sie redete mich fürsorglich mit meinem Vornamen an, auch wenn sie ihn auf der zweiten Silbe betonte und von mir wie von einem Buch-Roman sprach.

Das irritierte mich, zugegeben. Ich wischte sie sogleich weg. Ich empfand sie als zu aufdringlich. Einmal gab ich nach und antwortete. Sie solle mir ein Restaurant empfehlen. Siri, so heisst die gute Fee, schürzte zu meinen Worten ihre etwas grell geschminkten ondulierenden Lippen. Dann hiess sie mich, die Ortungsdienste zu aktivieren, damit sie meinen Standort feststellen könne. Ich erschrak und wischte sie wieder weg. Ein anderes Mal bat ich Siri, ein Stück von Chopin aus meinem Album zu spielen. Sie verstand mich gut, nur suchte sie vergeblich nach «Shop» und fand nichts Passendes. Ich gab auf. Immerhin fand ich, dass ich Siri hätte mögen können. Jedoch ihr Name, so erwies sich, war lediglich ein apartes Akronym für «Speech Interpretation and Recognition Interface» – und doch von der Art, dass er Herzen höher schlagen lässt.

Chatbots heissen diese zuvorkommenden unsichtbaren Kobolde, die stets zu Diensten sind, auch wenn man sie nicht gerufen oder erwartet hat. Manchmal sind es bloss im Hintergrund unermüdlich ihr Tag- und Nachtwerk vollbringende Algorithmen oder Bots, nur die höher entwickelten Chatbots sprechen zu uns. Und manche können gar nicht genug von einem kriegen: Wie Hündchen sind sie immer zur Stelle und wollen mit Befehlen traktiert werden, unermüdlich apportieren sie Bälle, die man ihnen zuwirft. Manche zuversichtlichen Zeitgenossen drohen damit, unser Alltag werde in naher Zukunft von solchen digitalen Zauberlehrlingen in einem Mass erleichtert werden, dass uns die Hände wieder frei werden für – ja, wofür eigentlich? Fürs Eigentliche, fürs Wesentliche eben! Aber wie stets bei solchen Prophetien: Die Zukunft wird uns in einer Form ereilen, dass wir unsere heutigen Phantasien darin nicht wiedererkennen werden.
Chatbots für Einsame

Ähnlich erging es schon dem Amerikaner Robert Hoffer, der um 2001 herum zusammen mit Partnern den vielleicht ersten erfolgreichen Chatbot programmiert hatte. Smarter Child hiess das himmlische Kind, dem man Fragen stellen oder kleine Suchaufträge erteilen konnte. Es sollte seinen Benutzern das Leben erleichtern: Restaurants oder einen Pizzakurier finden, Hotels suchen oder das Kinoprogramm abfragen und damit den Zugriff auf verschiedene Internet-Anbieter ersparen.

Doch was taten die Benutzer (es waren Millionen in kurzer Zeit)? Sie fanden in dem kleinen Troll, was sie eigentlich suchten: einen Gesprächspartner. Mehr brauchten oder wollten viele nicht. Keinen Pizzakurier und auch nicht das Kinoprogramm oder dann höchstens als Beiwerk. Sie wollten jemanden zum Reden. Robert Hoffer berichtete jüngst an einer Konferenz, dass sie damals die höchsten Frequenzen jeweils um 15 Uhr erzielten, wenn die Ostküsten-Kinder von der Schule nach Hause kamen, dann wieder um 18 Uhr mit den Westküsten-Kindern und schliesslich um Mitternacht, wenn die Leute sich einsam fühlten.

Was taten also Smarter Childs Ingenieure? Sie optimierten ihren Bot für Konversation. Und schrieben ihm alle möglichen Skripte ins Repertoire, damit er eine Persönlichkeit entwickelte und Authentizität vermitteln konnte. Mit künstlicher Intelligenz hatte das wenig oder nur am Rande zu tun. Die Ingenieure mussten lediglich erahnen, welches die Fragen sein könnten, die man einem Algorithmus mit dem Aussehen eines Hampelmanns stellt, und welches passende und witzige Antworten darauf sein könnten.

    Nichts bringt so sehr das Menschliche und noch mehr das Allzumenschliche hervor wie ein Algorithmus.

Was die Ingenieure damals lernten, fasste der kalifornische Ghostwriter Joe Shepter unlängst nüchtern zusammen: Wer einen digitalen Assistenten programmiere, müsse damit rechnen, dass die Benutzer ihn früher oder später bitten werden, mit ihnen Sex zu haben. Darauf und auf vieles durchaus ebenso Naheliegende sollten, so Shepter, die Algorithmen vorbereitet werden.

Für alle digitalen Muffel hat Shepter eine beruhigende Nachricht: Mit Informationstechnologie und künstlicher Intelligenz erweitern sich nicht, wie viele glauben, vor allem unsere Kompetenzen, eher mehren sie die Kenntnisse von uns selbst. Nichts bringt so sehr das Menschliche und noch mehr das Allzumenschliche hervor wie ein dienstbarer Algorithmus. Intelligente digitale Technologie muss sich darum zuallererst an unseren niedrigen Instinkten bewähren.
Dürfen Chatbots fluchen?

Haben Sie schon einmal versucht, mit Siri auf Ihrem iPad oder mit dem Haushalt-Roboter Alexa von Amazon zu reden? Es kostet zunächst – naturgemäss – etwas Überwindung, mit einem Gerät zu reden. Man kommt sich dabei recht bescheuert vor (und tut es darum im Verborgenen). Aber wenn die Hemmungen einmal abgebaut sind, wenn man einmal die ernsthafteren und zugleich banalen Sachen ausprobiert hat (Terminplanung, Wetter abfragen), geht's erst richtig zur Sache. Und dann stellt der Mensch – naturgemäss! – immer die gleichen lustigen Fragen im Stil von «Liebst du mich?».

    Der Santa Claus Bot von Microsoft mutierte 2007 innerhalb von Tagen zu einem Sexmonster.

Jüngst mussten sich darum die Ingenieure und vor allem die Manager bei Amazon Gedanken darüber machen, welche Flüche Alexa von ihren Benutzern lernen dürfe und welche Schimpfwörter sie aus Gründen der Schicklichkeit besser nicht in ihren Wortschatz übernehmen sollte. Es galt ein Debakel zu verhindern, das Microsoft 2007 ereilte, als dessen Santa Claus Bot innerhalb von Tagen zu einem Sexmonster mutierte und aus dem Verkehr gezogen werden musste. Das alles sind weniger beruhigende als vielmehr ernüchternde Aussichten. Ob die dienstbaren Geister nun Alexa, Siri, Cortana oder Smarter Child (gewiss der beste und treffendste Name) heissen: Allesamt werden sie uns zu mehr Selbstähnlichkeit verführen – oder sanft zwingen. Denn im ungünstigen Fall werden sie unsere verborgenen Wünsche hervorbringen (die wir vielleicht sublimiert haben, die aber darob nicht sublimer geworden sind). Oder sie werden uns – im günstigeren, aber nicht viel besseren Fall – genauer kennen als wir uns selbst. Abweichungen von unseren Gewohnheiten werden sie vielleicht tolerieren, aber nicht goutieren und schon gar nicht befördern. Ihnen zuliebe werden wir am Ende unsere Gewohnheiten, die wir als solche nicht bemerkt haben, beibehalten.

Auch in unseren Applikationen in der Redaktion sind Chatbots im Einsatz. Einmal setzte ich eine kleine Meldung ab in unserem internen Kommunikationskanal: Und zack, meldete sich ein solcher Gnom in Bruchteilen von Sekunden und winkte mir zu, hiess mich willkommen, ergänzte meine Meldung mit zusätzlichen (genaueren!) Angaben oder tat einfach nur wichtig. Dann versuchte ich, ahnungslos natürlich, dem Kerlchen zu antworten. Es schwieg indigniert und schweigt bis auf den heutigen Tag.";https://www.nzz.ch/feuilleton/roboter-fuer-den-alltag-mein-bester-freund-ist-ein-algorithmus-ld.138128;NZZ;Roman Bucheli;;;
01.11.2017;Zwei Schweizer Konzerne haben mit künstlicher Intelligenz Grosses vor;"Die Angst, dass Roboter und andere Maschinen immer intelligenter und vielfältiger einsetzbar werden, treibt zahlreiche Menschen um. «Habe ich künftig noch Arbeit?», fragen sich nicht nur Industrie-, Bau- oder Landarbeiter, sondern zunehmend auch Leute, die auf einer Bank, bei einer Versicherung, in einer Arzt- oder in einer Anwaltspraxis arbeiten. Die meisten Roboter befinden sich jedoch noch immer hinter Gittern in einem Käfig, um beispielsweise an einer Produktionsstrasse einer Autofabrik hochrepetitive Tätigkeiten auszuführen. Sie nieten dort Teile zusammen oder lackieren die Karosserie.
Starkes Wachstum mit Robotern

Mit Intelligenz hat dies wenig zu tun. Noch müssen denn auch die meisten Roboter von Menschenhand programmiert werden, ehe sie ihre Arbeit aufnehmen können. Lernfähig sind sie kaum. «Ein menschlicher Arbeiter muss sich in eine neue Tätigkeit zuerst einarbeiten, lernt dann aber meist schnell und kontinuierlich hinzu», sagt Bazmi Husain, Chief Technology Officer (CTO) beim Zürcher Elektrotechnikkonzern und Roboterhersteller ABB, im Gespräch mit der NZZ. Ein Roboter verfüge im Gegensatz dazu zwar unmittelbar nach seiner Inbetriebsetzung über eine hohe Produktivität, doch danach verlaufe die Lernkurve flach. Um sich zu steigern, müsse er aus der Produktion abgezogen und für zusätzliche Anforderungen neu programmiert werden, gibt Husain zu bedenken.

ABB gilt als der weltweit zweitgrösste Hersteller von Industrierobotern nach dem aus Japan stammenden Marktführer Fanuc. Wie viel Umsatz das in unterschiedlichen Sektoren tätige Unternehmen mit Robotern erwirtschaftet, gibt das Management nicht bekannt. Es betont jedoch seit längerem regelmässig, in diesem Geschäft stark zu wachsen. Seit 2015 sind von ABB nicht nur konventionelle – aus Sicherheitsgründen meist nach wie vor in Käfigen untergebrachte – Industrieroboter, sondern (unter dem Markennamen Yumi) auch sogenannte Cobots erhältlich. Diese kollaborierenden Roboter sind dafür konzipiert, in unmittelbarer Nähe von Menschen zu arbeiten. Sie wurden zunächst hauptsächlich in der Fertigung von Kleinteilen bei Elektronikunternehmen eingesetzt, finden aber zunehmend auch Anwendung in anderen Industriesektoren wie der Bekleidungs- oder der Autoindustrie sowie in Dienstleistungsbereichen.

    «Ein fertig ausgebildeter Roboter sollte in der Lage sein, auch andere Roboter zu instruieren.»

Der Yumi, der in der Basisversion 40 000 $ kostet und damit teurer ist als gewisse bereits ab 30 000 $ erhältliche Konkurrenzprodukte, wurde in der Schweiz auch schon für Tätigkeiten wie das Einpacken von Weihnachtsgeschenken eingesetzt – zum Gaudi grosser und kleiner Kunden, wie Husain andeutet. Das Problem eines kollaborativ eingesetzten Roboters ist indes, dass er automatisch abschaltet, sobald ihn ein Mensch oder ein bewegliches Objekt wie ein Gabelstapler berührt. Damit ist seiner Produktivität Grenzen gesetzt. Um für menschliche Arbeitskräfte ein echter Partner zu werden, muss er befähigt werden, seine Umgebung differenzierter wahrzunehmen. Die Forscher und Entwickler von ABB arbeiten laut Husain mit Hochdruck an dieser Aufgabe. Die nächste Generation des Yumi werde über zusätzliche visuelle Fertigkeiten und deutlich mehr Sensoren verfügen, unterstreicht der CTO.
Zeitintensives Training

Eine weitere Herausforderung betrifft die Lernfähigkeit der Roboter. Heute beruht das Training eines Yumi auf einer sogenannten Lead-through-Programmierung. Dabei hält ein Instruktor dessen Arme und zeigt ihm, was er jeweils zu tun hat. Dieses Prozedere eignet sich im Wesentlichen nur für die Schulung vergleichsweise einfacher Tätigkeiten wie das Weiterreichen von Gegenständen von der einen zur nächsten Maschine in einem Produktionsvorgang. Es ist zudem zeitintensiv, weil der Roboter erst mit jeder einzelnen Art eines Objekts vertraut gemacht werden muss. Anders als eine menschliche Arbeitskraft weiss er nicht intuitiv, wie er unterschiedlich beschaffene Gegenstände greifen soll.

Die Forscher und Entwickler von ABB setzen darauf, dank dem Einsatz von künstlicher Intelligenz den Trainingsprozess bei künftigen Robotern deutlich beschleunigen zu können. Bazmi Husain, der seit 1981 für ABB tätig ist und vor seiner Ernennung als CTO im Jahr 2015 die Geschäfte des Konzerns in Indien leitete, schwebt gar ein Training von Robotern durch Roboter vor. «Ein fertig ausgebildeter Roboter sollte in der Lage sein, auch andere Roboter zu instruieren», sagt er.
In Zukunft autonom

Um ihren Nutzwert in einer Auto-, Elektronik- oder in einer Textilfabrik zu steigern, müssen die automatischen Helfer deutlich intelligenter werden. Die grosse Hoffnung der Industrie liegt laut Husain auf autonom operierenden Robotern.

Roboter, die beispielsweise ein identisches Teil nach dem anderen mit einer Etikette versehen, sollen auch dann noch funktionieren, wenn das Objekt verkehrt herum auf dem Produktionsband eintrifft. Sie werden den Gegenstand vor dem Etikettieren einfach umdrehen, damit er sich wieder in der gewohnten Lage befindet. Was sich banal anhört, ist zurzeit alles andere selbstverständlich. Heutigen Robotern geht die Fähigkeit, unterschiedliche Situationen zu verstehen und sich darauf einzustellen, weitgehend ab.

Ab welchem Zeitpunkt autonome Roboter zum Alltag gehören werden, lässt sich nach Auffassung von Husain schwer abschätzen. Der aus Indien stammende Topmanager weist darauf hin, dass die vollständige Elektrifizierung mechanischer Systeme in der Industrie rund fünfzig Jahre beansprucht habe. Weitere vierzig Jahre habe es gedauert, ehe Maschinen verbreitet über Softwareprogramme gesteuert worden seien. Die vollständige Digitalisierung der Industrie könnte laut seiner Einschätzung rund dreissig Jahre in Anspruch nehmen - getreu der Regel, dass jede industrielle Revolution schneller als die vorherige vonstattengeht. Sicher ist sich Husain diesbezüglich, dass autonome Systeme in Zukunft eine deutlich bedeutendere Rolle spielen werden als heute.

In seinen Augen hat die Industrie gar keine andere Wahl, als verstärkt auf intelligente Roboter zu setzen. Der CTO, dem sämtliche 8000 Mitarbeiter in der weltweiten Forschung und Entwicklung von ABB unterstellt sind, weist darauf hin, dass die Produktivität des globalen Industriesektors, gemessen an der Leistung der Werktätigen und dem Kapitaleinsatz, bis 2006 lange Zeit um ungefähr 1% pro Jahr gewachsen sei. Danach habe sie zu stagnieren begonnen und sei 2015 gar in den negativen Bereich abgerutscht. «Wir müssen neue Wege finden, um eine Leistungssteigerung herbeizuführen», sagt Husain.
Kostbare Zeit optimal nutzen

Einem anderen Wirtschaftszweig auf die Sprünge helfen will der Pharma- und Diagnostikkonzern Roche. Er hat Anfang Oktober in den USA, Grossbritannien, Deutschland, Spanien, Schweden und der Schweiz eine Softwarelösung auf den Markt gebracht, die Spitäler in der Diagnose und der Behandlung von Krebserkrankungen unterstützen soll. In vielen Krebskliniken kommen sogenannte Tumor-Boards zum Einsatz. Dabei handelt es sich um Zusammenkünfte, bei denen sich Ärzte aus verschiedenen Fachrichtungen wie der Onkologie, der Chirurgie, der Radiologie oder der Pathologie versammeln, um gemeinsam Fälle zu beurteilen.

Wie Tim M. Jaeger, Leiter des Geschäftsbereichs Diagnostic Information Solutions von Roche und ehemaliger Arzt, vorrechnet, stehen derartigen Expertengremien indes selbst in renommierten Kliniken im Durchschnitt nur drei bis vier Minuten pro Patient zur Verfügung. «Diese Zeit ist extrem kostbar, auch weil die einzelnen Patienten nicht selten mehrere Monate warten müssen, ehe sie an die Reihe kommen. Wir möchten einen Beitrag dazu leisten, dass die Ärzte möglichst gut dokumentiert sowohl zu diesen Treffen gehen als auch über das weitere Vorgehen entscheiden können.»

Das Gesundheitswesen gilt trotz modernen bildgebenden Verfahren wie der Computer- und der Magnetresonanztomografie oder der allmählichen Einführung elektronischer Patientendossiers in Sachen Digitalisierung oft noch als rückständig. Mitarbeiter der Diagnostiksparte von Roche fanden bei ihrer Marktanalyse heraus, dass in vielen Tumor-Board-Sitzungen zwar auch Power-Point-Präsentationen gemacht werden, Röntgen- und andere Aufnahmen teilweise aber noch als Papierabzüge aufgehängt werden. Dabei geht wertvolle Zeit verloren. Die neue Softwarelösung ermöglicht Ärzten, ihre Beiträge für die Treffen in eine zentrale Datenbank einzuspeisen. Damit könne es auch nicht mehr vorkommen, dass bestimmte Aufnahmen nicht angeschaut werden könnten, weil ein Arzt vergessen habe, sie mitzunehmen, sagt Jaeger.
Herausforderung Datenflut

Wie in fast allen Lebensbereichen nehmen auch in der Medizin die Datenmengen unaufhörlich und in hohem Tempo zu. In der Krebsbehandlung ist die Informationsflut besonders ausgeprägt. Jaeger weist darauf hin, dass man heutzutage beispielsweise nicht nur zwischen über dreissig Arten von Brustkrebs und rund fünfzig Typen von Lungenkrebs unterscheide. Allein für die Behandlung von Lungenkrebs seien in den USA rund fünfzig verschiedene Medikamente zugelassen, deren Eigenschaften ein Arzt kennen müsse. Damit nicht genug: Laut Jaeger laufen in Amerika allein über 700 klinische Studien zur Erforschung neuer Präparate gegen das Lungenkarzinom.

Roche verspricht sich viel vom Geschäft mit Softwareprodukten, die Ärzten und anderen medizinischen Fachpersonen beim Navigieren in einer zunehmend komplexen Welt beistehen sollen. Die Konkurrenz setzt sich dabei nicht nur aus traditionellen Pharma- und Diagnostikanbietern, sondern zunehmend auch aus IT-Konzernen zusammen, wie Jaeger bereitwillig einräumt. Das Basler Unternehmen, das konzernweit mehrere hundert Spezialisten für Fragen der künstlichen Intelligenz, des maschinellen Lernens oder der Analyse grosser Datenbestände (Big Data) beschäftigt, setzt darauf, dank seiner grossen Erfahrung in der Biologie bei Kunden jedoch einen Vertrauensbonus zu besitzen. Weitere Applikationen der jetzigen Softwarelösung für Tumor-Boards könnten laut Jaeger ein Instrument für die Literaturrecherche oder das schnelle Abrufen der Richtlinien von Spitälern für einzelne Behandlungsarten sein. Vorgesehen sind auch Lösungen für Ärzte, die sich mit anderen Krankheiten als Krebs beschäftigen.
Kein Ersatz für den Menschen

Trotz allen algorithmischen Fähigkeiten des neuen Produkts gehe es jedoch nicht darum, die Kompetenz der Mediziner infrage zu stellen, betont Jaeger. Die Entscheidungsgewalt liege nach wie vor beim Arzt. Auch der Elektrotechnikkonzern ABB beteuert, mit seinen Robotern niemandem die Arbeit wegnehmen zu wollen. Für CTO Husain gehört die Zukunft jenen Robotern, die Arbeiten zu erledigen imstande seien, deren Ausführung den Menschen überfordere. «Die menschliche Hand hat nun einmal nur sechs Bewegungsfreiheitsgrade.»";https://www.nzz.ch/wirtschaft/mit-kuenstlicher-intelligenz-neue-wachstumsfelder-erschliessen-ld.1325432;NZZ;Dominik Feldges;;;
10.03.2016;Durchbruch für künstliche Intelligenz?;"is Dienstag sollen insgesamt fünf Partien gespielt werden. Der Gewinner erhält ein Preisgeld von einer Million Dollar (912 000 Euro). Falls Alpha Go gewinnt, soll das Geld an das Kinderhilfswerk Unicef, Go-Verbände und Wohltätigkeitsorganisationen gespendet werden.

Alpha Go hatte bereits am Mittwoch gegen den 33-jährigen Südkoreaner Lee Sedol gesiegt, der 18 Weltmeisterschaften gewann, seit er mit zwölf Jahren professioneller Go-Spieler geworden war. Der Sieg des Programms wurde als Durchbruch für künstliche Intelligenz angesehen und galt als Überraschung, da Experten bis vor kurzem noch prognostiziert hatten, es würde ein weiteres Jahrzehnt dauern, bis Computer professionelle Go-Spieler schlagen könnten.
Eines der komplexesten Spiele überhaupt

Hunderttausende Südkoreaner verfolgten die Spiele live im Fernsehen oder auf YouTube. Alle großen Zeitungen berichteten über Lees Niederlage auf ihren Titelseiten. «Zwei Jahre alte künstliche Intelligenz beherrscht 5000 Jahre altes menschliches Spiel Go», hieß es in der Zeitung «Chosun Ilbo». Lee kündigte an, auch wenn er das dritte Spiel verlieren sollte, wolle er alle fünf Partien spielen.

Das traditionelle, aus China stammende Go-Spiel ist, die jemals erfunden wurden. Experten hielten es für schwierig, dass Computer diese Komplexität meistern könnten. Bei Go gibt es eine fast unbegrenzte Zahl von Strategien, um auf dem Spielfeld erfolgreich zu sein. Das erfordert von den Spielern auch, sich bei Zügen auf ihre Intuition zu verlassen.";https://www.nzz.ch/panorama/alltagsgeschichten/duell-zwischen-computer-und-mensch-durchbruch-fuer-kuenstliche-intelligenz-ld.6751;NZZ;AP;;;
31.03.2016;Wenn Algorithmen Dichter werden;"Der Ingenieur Adolph Knipe hat eines Tages die Idee, einen Computer zu bauen, der Kurzgeschichten und Romane schreiben kann. John Bohlen, sein Chef und Inhaber einer Computerfirma, willigt ein. Ein halbes Jahr später ist der Rechner fertig und spuckt nach Belieben Romane und Erzählungen aus, die für ihr jeweiliges Zielpublikum massgefertigt und entsprechend erfolgreich sind. Nach und nach sehen sich selbst namhafte Schriftsteller gezwungen, die Feder niederzulegen und ihre Namen an Bohlens Firma zu verkaufen – denn die Maschine repliziert ihren Stil besser, als sie es selbst je könnten.
Stochastische Gedichte

In seiner 1948 entstandenen Short Story «The Great Automatic Grammatizator» antizipiert der britische Schriftsteller Roald Dahl auf die für ihn übliche humoristisch-satirische Weise, wie eines Tages auch kreative Tätigkeiten von Computern schnell und zu niedrigen Produktionskosten übernommen werden. Dahls amerikanischer Kollege Fritz Leiber griff das Motiv des schreibenden Computers 1962 in seinem Roman «The Silver Eggheads» auf, in dem Schriftsteller – genauer gesagt, Ex-Schriftsteller – nur noch benötigt werden, um die «robot writers» zu beaufsichtigen. Ihre eigene Kreativität haben sie längst eingebüsst.

Das sind nur zwei Beispiele, die zeigen, wie früh sich Autoren mit der technologischen Zukunft ihres Berufs auseinandergesetzt haben, wie früh sie die Möglichkeiten des Computers erahnten. Unbekannt ist indes, ob Dahl oder Leiber wussten, wie dicht ihnen die Realität auf den Fersen war. Denn bereits 1959 generierte der Esslinger Informatiker Theo Lutz, ein Schüler des Philosophen Max Bense, mithilfe eines Zuse-Z22-Computers die ersten stochastischen Texte. «Darunter versteht man Texte, deren grammatikalische Struktur vorgegeben ist, deren Worte jedoch zufallsmässig bestimmt sind», erklärte Lutz in einem Essay. Später ergänzte er sein literarisches Programm um eine «Alternativmatrix», die den Inhalt sinnhafter erscheinen liess.

Andere Informatiker folgten Lutz, etwa Gerhard Stickel, der 1964 mit einem IBM-Rechner «Autopoeme» generierte. Manfred Krause und Götz Schaudt erweiterten 1967 die bestehenden Programme um Reimregeln sowie den Wortschatz von Goethe, Schiller, Grass, Rühmkorf und anderen Autoren. Die Computerlyrik oder digitale Poesie etablierte sich und liess Dahls und Leibers Visionen nun nicht mehr ganz so phantastisch anmuten.
Die vierte Revolution

Die Entwicklung von künstlichen Intelligenzen hat sich zwar, gemessen an den Prognosen der IT-Optimisten, deutlich verzögert, schreitet aber dennoch kontinuierlich voran. Längst steht zudem der Quantencomputer in den Startlöchern, der die Leistung und die Fähigkeiten der derzeit gebräuchlichen digitalen Chips um ein heute noch Unvorstellbares übertreffen wird. Keine Frage, die vierte industrielle Revolution kommt; zum vierten Mal seit der Erfindung der Dampfmaschine und des mechanischen Webstuhls werden sich unser Leben und unsere Arbeit gravierend verändern.

Schon zeichnen Prognosen, in diesem Fall von IT-Pessimisten, ein düsteres Bild der zukünftigen Lage. Laut einer neuen Studie der London School of Economics (LSE) sind die Revolutionäre 4.0, allen voran Roboter und Algorithmen modernster Provenienz, bereits dabei, allein in Deutschland 51,1 Prozent der Jobs zu übernehmen. Diesmal sind auch Tätigkeiten und Berufe betroffen, die bis anhin von den Revolutionen im IT-Bereich weitgehend verschont geblieben waren: Rechtsanwälte, Ingenieure, Ärzte, Designer, Journalisten.

Passend zur Diskussion geistert seit ein paar Jahren der Begriff des Roboterjournalismus durch die Presse, fast immer verbunden mit dem beschwichtigenden Hinweis, dass die recherchierenden und schreibenden Algorithmen über Börsen-, Wetter- und Sportberichte nicht hinauskämen. «Es ist unwahrscheinlich, dass Roboter den Journalismus jemals übernehmen werden», erklärte unlängst der namhafte amerikanische Computerlinguist Michael White in einem Interview mit der «FAZ».
Falscher Pessimismus

Unabhängig von der Frage, ob das Kompositum «Roboterjournalismus» eine geglückte Wortprägung darstellt, handelt es sich bei Whites Aussage um eine Unmöglichkeitsprognose, bei der jeder Zukunftsforscher die Augen rollt. Hätten die unzähligen Unmöglichkeitsprognostiker auch nur in der Hälfte der Fälle richtig gelegen, so hätte es wohl kaum ein technisches Zeitalter gegeben. Kein Flugzeug hätte sich je in die Luft erhoben, kein Handy ein Netz gesucht, in keinem Haushalt würde ein Computer stehen. Die technologischen Revolutionen wären allesamt ausgeblieben. Die Geschichte belegt indes, dass Unmöglichkeitsprognosen so gut wie nie zutreffen. Die Frage, die sich Zukunftsforscher von Karlheinz Steinmüller (Berlin) bis James Dator (Hawaii) stellen, ist daher auch nicht die nach der generellen Möglichkeit einer Innovation, sondern vielmehr die nach dem wahrscheinlichen Zeitpunkt. Es geht nicht um das Ob, es geht um das Wann.

Unmöglichkeitsprognosen werden somit auch die Entwicklung kreativer Algorithmen nicht aufhalten. Mit durchaus berechtigtem Optimismus versprach jüngst Kristian Hammond, Informatikprofessor an der Northwestern University in Illinois und Chefwissenschafter des Unternehmens Narrative Science, den baldigen Gewinn des Pulitzer-Preises durch einen schreibenden Algorithmus. Es spricht viel dafür, dass Hammond recht behalten könnte. Er weiss, wie viele Institute und Firmen an dem Projekt arbeiten, denn es verspricht immense ökonomische Erfolge.

Das beweist auch die Marktposition von Unternehmen wie Narrative Science (USA) oder Aexea (Deutschland): Ihre Algorithmen wandeln Zahlenmaterial und Statistiken in sprachliche Darstellungen um und generieren so täglich mehrere Millionen Texte für Veröffentlichungen aller Art. Dies ist jedoch nicht – wie Michael White es postulierte – das vorläufige Ende der Entwicklung, sondern vielmehr der Einstieg in eine umfassende computerbasierte Literaturproduktion.

Kreative Algorithmen sind bereits dabei, zu Konkurrenten von Journalisten und Schriftstellern zu werden. Dabei müssen die Algorithmen ihre eigenen Texte keineswegs im hermeneutischen Sinne verstehen. Sie müssen lediglich die benötigten Texte aus verschiedenen Bausteinen, aus Templates, Wortschätzen und syntaktischen Mustern zusammensetzen. Also aus jenen sprachlich-literarischen Elementen, die sich in irgendeiner Form messen und nach Regeln rekonstruieren lassen. Genau dies war schon Dahl und Leiber, den eingangs genannten Pionieren der 1950er und 1960er Jahre, bewusst.

In einer nicht mehr fernen Zukunft werden die global agierenden Medienkonzerne nicht nur von Algorithmen geschriebene Online-Zeitungen anbieten, sondern auch Ratgeber, Krimis oder Fantasy-Romane. Zeitungen wie auch E-Books sind dannzumal selbstverständlich individualisiert verfügbar. Die E-Book-Reader der Zukunft erfassen mit Sensoren die Reaktionen des Lesers und ändern den Text kontinuierlich. Bald wissen die lernfähigen Geräte, welche inhaltlichen oder ästhetischen Präferenzen der Leser hat, und erhöhen entsprechend die Zahl der Mordopfer oder der erotischen Abenteuer.
Der Leser entscheidet

Der Leser wird so zum Mitschöpfer, zum Prosumenten, der nur noch liest, was ihm vom ersten bis zum letzten Buchstaben gefällt. Die Anbieter werden dafür sorgen, dass die Fortsetzungen beliebter Reihen niemals enden. Ausserdem werden die Übergänge zwischen Roman und Werbung verschwimmen; aktuelle Produkte werden von den Algorithmen in die Handlung eingebaut. Der Leser kann wiederum Wünsche angeben und sogar Figuren aus anderen Werken in die Handlung integrieren. Harry Potter vs. Darth Vader? Alles eine Frage der Rechte. Sie lieben den Stil von Agatha Christie? Ihr E-Book-Reader kann Ihnen jeden verfügbaren Krimi im Stil der englischen Autorin bieten. Der Text wird dank dieser Entwicklung ebenso zum beliebig verfügbaren Segment der digitalen Welt wie das Bild oder der Ton.

Für den Leser zählt am Ende nur das Leseerlebnis, nicht die menschliche Autorschaft. Der Autor wiederum könnte, wie von Dahl oder Leiber beschrieben, zum literarischen Art-Director werden, der Handlungen und Figuren skizziert; ein kreativer Algorithmus generiert dann den Roman aus diesen Elementen. Zumindest die Autoren der Unterhaltungsliteratur dürften dieser Revolution zum Opfer fallen, ebenso deren Verlage und die Buchhandlungen. Globale Medienkonzerne werden die Themen wie den Markt dominieren.

Es sei denn, die Leser entscheiden sich doch gegen dieses Szenario und somit gegen journalistische und literarische Texte, die nichts Authentisches, Empfundenes, Erlebtes, Erduldetes, Erdachtes, Reflektiertes zu bieten haben, sondern nur die Simulation menschlicher Gedankengänge und Gedankenspiele. Setzte sich eine derartige Textgenese durch, dann würde der menschliche «Weltinnenraum», wie ihn Rainer Maria Rilke genannt hat, zu einer residualen Kategorie schrumpfen und einer Hochliteratur vorbehalten bleiben.";https://www.nzz.ch/feuilleton/schauplatz/kommt-die-zeit-der-computergenerierten-literatur-wenn-algorithmen-dichter-werden-ld.10562;NZZ;Bernd Flessner;;;
23.08.2018;Kühlung durch KI;"Wo die Menschen den Computer ansprechen wie ihresgleichen, wo sie sich der Maschine mit gesprochenen Befehlen mitteilen, wird es in den Rechenzentren rasch wärmer. Die automatische Spracherkennung – ebenso wie auch andere Aufgaben der künstlichen Intelligenz – benötigt sehr viel Rechenleistung. Oft sind die Endgeräte, die Smartphones, diesen Aufgaben nicht gewachsen, die Eingabedaten werden deshalb an Server in grossen Rechenzentren weitergeleitet und dort verarbeitet.
«Intelligenzbeschleuniger» als Stromfresser

Vor rund fünf Jahren sahen sich die Ingenieure von Google wegen der Popularität der Google Assistant genannten Spracherkennung gezwungen, spezielle Halbleiterkomponenten zu entwickeln, um die Rechenleistung der Servercomputer zu erhöhen. Wenn jeder Besitzer eines Android-Geräts die sprachgesteuerte Internetsuche nur einmal pro Tag nutzen würde, so berechneten Google-Ingenieure damals, müsste die Firma ein Dutzend neuer Rechenzentren bauen.

    Rund drei Prozent der weltweiten Elektrizitätsproduktion werden für den Betrieb der Servercomputer verwendet.

Anstatt mehr Computer einzusetzen, begann man bei Google, die vorhandenen Computer für die Aufgaben der künstlichen Intelligenz aufzurüsten. Seit 2016 hat Google Tensor Processing Units (TPU) genannte Einsteckkarten im Einsatz. Die dritte Generation dieser «Intelligenzbeschleuniger», vor wenigen Monaten angekündigt, ermöglicht im Vergleich zum Vorgänger aus dem Jahr 2017 eine Verdoppelung der Rechenleistung. Doch auch die Hitzeentwicklung hat sich erhöht. Eine wichtige Neuerung der TPU 3.0 ist deshalb Wasserkühlung: Das Wasser wird in dünnen Röhrchen direkt an die Chips herangeführt.
Einsparungen von 40 Prozent

Anwendungen im Bereich der künstlichen Intelligenz (KI) erfordern eine Erhöhung der Rechenleistung, und mit der Rechenleistung steigt der Aufwand für die Kühlung. Rund ein Drittel der Energie, die ein Rechenzentrum verbraucht, wird für Kühlung aufgewendet. Hinter diesem Durchschnittswert verbergen sich grosse Schwankungen, manchmal werden auch zwei Drittel der Energie in die Kühlung gesteckt. Und Rechenzentren benötigen viel Energie: Rund drei Prozent der weltweiten Elektrizitätsproduktion werden für den Betrieb der Servercomputer verwendet.

Da ist es gut zu wissen, dass die KI bald auch dabei helfen kann, die Kühlung zu optimieren und die dafür benötigte Energie zu senken. Entsprechende Ideen wurden bei Google bereits ab 2014 verfolgt. Anfänglich gab das von einem Google-Ingenieur namens Jim Gao entwickelte KI-System einfach nur Empfehlungen ab. Es blieb den Mitarbeitern in den Rechenzentren überlassen, was sie mit diesen Empfehlungen anstellten. Doch die Empfehlungen erwiesen sich als nützlich, sie ermöglichten es – so berichtete Google 2016 –, den Aufwand für die Kühlung um bis zu vierzig Prozent zu senken.

Google-Ingenieure sind nicht die einzigen, die sich damit beschäftigen, die Möglichkeiten der KI – genauer: des Deep Reinforcement Learning – für die Rechenzentren fruchtbar zu machen. Doch wo andere Forscher mit Modellen und Simulationen arbeiten, konnten die Google-Ingenieure die Verfahren im Alltagseinsatz testen.
Hohe Komplexität

Gao arbeitet heute für die Google-Tochter Deepmind, die sich in Sachen KI immer wieder mit innovativen Ansätzen hervorgetan hat. Es waren Mitarbeiter von Deepmind, die ein KI-System programmierten, das den weltbesten Go-Spieler schlagen konnte. Mitte August hat Gao zusammen mit Kollegen von Deepmind und Google ein verbessertes System für die Optimierung der Kühlung in Rechenzentren vorgestellt. Der Mensch hat in diesem System nur noch Überwachungsaufgaben, die Steuerung muss er der Software überlassen.

Es gibt viele Faktoren, die auf die Temperatur in einem Rechenzentrum einen Einfluss haben. Das Wetter spielt etwa eine Rolle, die Aussentemperatur, die Luftfeuchtigkeit. Neben diesen Faktoren, die sich nicht verändern lassen, gibt es Bedingungen, die veränderbar sind: die Arbeitsweise der Kältemaschinen, die Auslastung der einzelnen Computer. Viele der Variablen beeinflussen sich gegenseitig.

    Künstliche Intelligenz ermöglicht Energieeinsparungen von durchschnittlich dreissig Prozent ermöglicht. Weitere Verbesserungen seien zu erwarten.

Bei der Temperaturregelung müssten die Daten von «Tausenden» von Sensoren alle fünf Minuten ausgewertet werden, es gebe «Milliarden» von möglichen Eingriffen, berichtet Google. Die neue Software für die Temperatursteuerung im Rechenzentrum habe Eingriffsmöglichkeiten entdeckt, die den menschlichen Operatoren unbekannt waren, die sich aber bewährt hätten.
Sicherheit kommt vor Effizienz

Der Bericht, der das neue System beschreibt, legt grossen Wert auf Sicherheitsaspekte. Es handle sich um eine Safety-First-Software, es gebe eine redundante Liste von Sicherheitsauflagen, denen sich die Empfehlungen des Systems unterordnen müssten, die zudem einer «doppelten» Verifikation unterworfen seien. Die Operatoren hätten jederzeit die Möglichkeit, die KI-Software auszuhebeln und die Steuerung wieder selber zu übernehmen.

Die Google-Ingenieure um Gao scheinen sich ihrer Sache sicher zu sein – das neue System kommt seit einigen Monaten bereits in mehreren Rechenzentren zum Einsatz. Und obwohl das System hohen Sicherheitsanforderungen genügen muss, habe es bereits Energieeinsparungen von durchschnittlich dreissig Prozent ermöglicht. Weitere Verbesserungen seien zu erwarten.";https://www.nzz.ch/digital/kuehlung-durch-ki-ld.1413985;NZZ;Stefan Betschon;;;
13.11.2015;Von Ameisen und Übermenschen;"In einem spärlich erleuchteten Verlies ein Mann. Manchmal wird ihm durch eine Klappe eine Schale mit Essen gereicht. Meistens aber, wenn ein Schnarren und Kratzen zu hören ist, das die Öffnung der Klappe ankündigt, bringt man ihm Arbeit, Körbe voller Papier. Der Mann hat keine Ahnung, was es mit den Zetteln auf sich hat. Es sind chinesische Schriftzeichen zu sehen, Schnörkel und Krakel, die er nicht zu lesen versteht. Man hat ihm aber Regeln eingebläut, wie die Papiere zu ordnen sind. Er nimmt Körbe entgegen, sortiert Zeichen, Schnörkel hierhin, Krakel dorthin, dann gibt er die Körbe zurück.
An der Intuitionspumpe

Die Geschichte vom chinesischen Zimmer ist eine alte Geschichte. Es ist ein Gedankenexperiment, das sich der amerikanische Philosoph John Searle in den 1970er Jahren ausgedacht hat. Er wollte beweisen, dass der Versuch, künstlich intelligente Maschinen zu bauen, zum Scheitern verurteilt sei. Darf einer, der ausserhalb des Verlieses vor der Klappe steht und Buchstaben hineingibt, annehmen, wenn er Buchstabenfolgen zurückerhält, die Sinn ergeben, dass er es mit Intelligenz tun hat? Alan Turing, der Intelligenz definiert als die Fähigkeit, so zu tun, als ob, hätte Ja gesagt. Searle sagt – Nein: Das Wesen, das Schnörkel und Krakel sortiert, dürfe nicht als intelligent gelten. Es fehle ihm das Bewusstsein, die Intentionalität, ein Verständnis für das, was es tut.

Das «chinesische Zimmer» hat Widerspruch herausgefordert. Der Philosoph Daniel Dennett sah in der Geschichte nichts weiter als eine «faulty intuition pump», eine fehlerhafte Intuitionspumpe. Die Geschichte beweise nichts, aber sie machen einen glauben, verstanden zu haben, worum es geht.

Gegen Searle stellte sich auch das Philosophen-Ehepaar Patricia und Paul Churchland. Das «chinesische Zimmer» nehme Bezug auf herkömmliche Computer-Architekturen. Diese könnten, an die Vorgaben eines Programms gebunden, keine Intelligenz entwickeln. Anders sei es bei künstlichen neuronalen Netzwerken. Hier ist nicht ein Lösungsweg vorgegeben, sondern die Fähigkeit, durch Versuch und Irrtum einen Lösungsweg zu finden. Die Idee, künstliche Neuronen als Logik-Elemente einzusetzen, ist fast so alt wie die Computerwissenschaft. Sie wurde von Warren McCulloch und Walter Pitts 1943 vorgestellt. Schon mehrmals wurde der Forschungsansatz als Königsweg auf dem Weg zu einer künstlichen Intelligenz bejubelt, mehrmals als Sackgasse verworfen. Jetzt vermelden Wissenschafter, die auf diesem Weg vorangegangen sind, Durchbrüche.

Ein neuronales Netzwerk ist eine Ansammlung von Software-Moduln, die Eingangssignale in ein Ausgangssignal umwandeln. Im einfachsten Fall steht zwischen Input und Output ein Schalter, eine If-then-else-Verzeigung: Ja, der kleine Bildausschnitt, der verarbeitet werden muss, ist schwarz, nein, der Bildausschnitt ist weiss. Eine Gruppe von solchen Schaltern, mit unterschiedlichen Bildausschnitten beschäftigt, könnte lernen, wie sich auf einer Schwarz-Weiss-Foto eine Eins von einer Drei unterscheidet. Ein komplizierteres Modul besitzt nicht einen Schalter, sondern Drehregler, die ein Eingangssignal manchmal auch nichtlinear verändern. Ein kompliziertes neuronales Netz mit Millionen von Moduln kann im Rahmen eines Lernprozesses dazu gebracht werden, selber die Regler so zu verstellen, dass komplizierte Beziehungen zwischen Input und Output hergestellt werden können. Eine Software, die chinesische Schriftzeichen fast so gut wie ein Mensch sortieren kann, gibt es seit kurzem. Sie wurde in Lugano am Istituto Dalle Molle di Studi sull'Intelligenza Artificiale unter der Leitung von Jürgen Schmidhuber entwickelt. «Niemand in unserem Team», so sagt Schmidhuber im persönlichen Gespräch, «beherrscht die chinesische Sprache. Wir haben eine Software entwickelt, die etwas kann, was wir nicht können.»

Der Begriff «Artificial Intelligence» wurde 1956 anlässlich einer Konferenz geprägt, die von John McCarthy (Dartmouth College), Marvin Minsky (Harvard University) und Claude Shannon (Bell Laboratories) organisiert worden war. Diese Computerwissenschafter ahnten, dass Computer mehr sein könnten als nur Rechenmaschinen, sie wollten ihnen neue Anwendungsgebiete erschliessen. Einige Teilnehmer dieser berühmten Konferenz wollten das neue Teilgebiet der Informatik «complex information processing» nennen. Doch schliesslich einigte man sich auf «künstliche Intelligenz». Es gehe darum, so formulierte Minsky das Forschungsziel, den Computer Dinge machen zu lassen, die, wenn ein Mensch sie erledigt, Intelligenz voraussetzen.

So wurde die Intuitionspumpe in Gang gesetzt. Durch die Fixierung auf Besonderheiten menschlichen Verhaltens, die sich mit dem vielseitig schillernden Begriff «Intelligenz» umschreiben lassen, wurde Verwirrung gestiftet. Die KI-Forschung erlebte immer wieder hoffnungsfrohe Aufbrüche, das Ziel schien zum Greifen nah, doch dann zeigte sich hinter dem Gipfel ein weiterer Gipfel, ein neues Hindernis. Alle paar Jahre wurde ein Richtungswechsel vorgeschlagen, eine «neue KI» angekündigt: Formale Logik, genetisches Programmieren, Knowledge Engineering, Fuzzy Logic – immer wieder neue Paradigmen versprachen den raschen Erfolg, immer wieder aufs Neue wurden die Hoffnungen enttäuscht.

«Deep learning in neural networks» gilt als neueste Variante einer «neuen» KI. Das Paradigma ist aber schon alt, es hat bereits mehrere Boom-Bust-Zyklen hinter sich. Wissenschafter wie Geoffrey Hinton (Universität Toronto, Google), Yann LeCun (New York University, Facebook) oder Jürgen Schmidhuber, die heute als Vordenker der «neuesten» KI gefeiert werden, wurden auch schon als Aussenseiter belächelt, obwohl sie während Jahrzehnten demselben Forschungsansatz treu geblieben sind.

Was ist neu an der «neuesten» KI? Neu ist, dass es greifbare Erfolge gibt. Bei der Sprachverarbeitung oder bei der Bilderkennung wurden Durchbrüche erzielt. Nicht neu ist, dass die Begeisterung der Journalisten keine Grenzen kennt. Da hilft es wenig, dass die Marketingabteilungen der grossen Computerfirmen geradezu vernarrt sind in Adjektive, die verwendet werden, um intellektuelle Fähigkeiten von Menschen zu loben. Alles ist smart, alles intelligent. Das Fliessen und Strömen der Intuitionspumpe behindert eine öffentliche Debatte.

Bei IBM sind Computer jetzt «cognitive systems». Die Firma hat – ebenso wie Qualcomm – einen «Brain-Chip» angekündigt. Wissenschafter wie LeCun oder Schmidhuber glauben aber nicht, dass diese Chips für ihre Arbeit einen unmittelbaren Nutzen haben.

«Wir versuchen nicht, das Gehirn nachzuahmen», sagt Schmidhuber. Für die Gehirnforschung scheint er wenig übrig zu haben. «Es ist Jahrzehnte her, dass ich für meine Arbeit von der Gehirnforschung etwas lernen konnte. Heute versetzt uns die Beschäftigung mit neuronalen Netzen in die Lage, vorauszusagen, was die Neurobiologen dereinst entdecken werden.»

Es gehe nicht darum, das Gehirn nachzuahmen, sagt Schmidhuber – doch im Verlauf des Gesprächs nimmt er immer wieder auf den Menschen Bezug. Seine Systeme würden durch «Neugier» angetrieben, aber auch vom Bemühen, «Schmerz- und Hungersignale» zu vermeiden. Ist es nicht gefährlich, Begriffe aus der Psychologie zu benutzen? Sollte man anstatt von lernenden nicht besser von adaptiven Systemen reden? «Nein», sagt Schmidhuber. «Nichts beschreibt das, was unsere Systeme tun, besser als ‹lernen›.» Man wird es den Computerwissenschaftern nicht übelnehmen, dass sie keine Anstrengungen unternehmen, um die Leistungsgrenzen ihrer Systeme aufzuzeigen. Das könnte man von den Journalisten erwarten. Diese haben sich aber, wie es scheint, bereits damit abgefunden, dass sie bald durch Roboter ersetzt werden. Ein psychologischer Mechanismus, vergleichbar mit dem Stockholm-Syndrom, macht sie glauben, dass sie sich als Propagandisten der künstlichen Intelligenz hervortun müssen.

Der KI-Forschung sind dank schnelleren Computern im Bereich Machine Learning bedeutende Durchbrüche gelungen. Doch man wird nicht die Luft anhalten wollen, bis diese Systeme, die auf die Rechenleistung von Hunderten von Computern angewiesen sind, auf einem Smartphone installiert werden können. Diese Systeme sind in der Lage, in grossen Datenmengen Muster – Regelmässigkeiten – zu erkennen. Nicht jedes Problem lässt sich aber mit den statistischen Methoden der Mustererkennung lösen. Wo es zu einer Fragestellung in maschinenlesbarer Form keine Daten gibt («Ist Gott gut?»), wo die Daten widersprüchlich sind («Hat Beckenbauer gelogen?»), können diese Systeme keine Antworten liefern.
Goldgräberstimmung

Jungfirmen, deren Namen Bestandteile wie «Deep», «Mind» oder «Neuro» enthalten, sind gefragt. 2014 bezahlte Google für Deepmind mehr als 500 Millionen Dollar. Wichtige Mitarbeiter der britischen Firma hatten in Lugano bei Schmidhuber studiert. IBM kaufte AlchemyAPI, Apple übernahm VocalIQ, Yahoo IQ Engines und SkyPhrase. MetaMind, Skymind und Deepomatic sind noch zu haben. Firmennamen wie Deep Genomics oder StocksNeural deuten darauf hin, dass neuronale Netzwerke auch ausserhalb der Sprachverarbeitung und der Bilderkennung Bedeutung erlangen könnten.

Bei der Präsentation von Quartalszahlen Ende Oktober verkündete Google-CEO Sundar Pichai, man werde alle geschäftlichen Aktivitäten im Zeichen von Machine Learning neu überdenken. Diese Technologie, so schreibt Predo Domingos in dem kürzlich erschienenen Buch «The Master Algorithm», werde die Wirtschaft verändern wie einst, in den 1970er Jahren, die ersten Computer und später, in den 1990ern, das World Wide Web.

Auch Schmidhuber hat den Lockruf der Wirtschaft vernommen. Es sei eine Ehre, dass eine in Lugano von seiner Gruppe entwickelte Technik wie Long Short-Term Memory inzwischen als Teil von Google-Software von Milliarden von Menschen benutzt werde. In Zukunft wolle er aber die Früchte seiner Arbeit besser verwerten.

Schmidhuber hat in Lugano zusammen mit Wissenschaftern seines Instituts die Nnaisense AG gegründet. Laut Handelsregister möchte die Firma beitragen zur «Schaffung einer künstlichen Intelligenz». Es sollen «Softwarelösungen auf Basis neuronaler Netze» entwickelt werden, die sich bewähren bei medizinischer Diagnostik, Fahrerassistenz, in der Robotik, bei der Überprüfung von Dokumenten, bei Finanzprognosen oder als Teil von Bild-Suchmaschinen.
Private Forschung

Im Wettbewerb mit privaten Firmen, in der Konkurrenz gegen Apple, Facebook, Google oder IBM, hätten Universitätsinstitute einen schweren Stand, sagt Schmidhuber. Geld allein sei zwar nicht matchentscheidend. Aber die Möglichkeit, auf riesige Datenmengen und gewaltige Rechenleistung zugreifen zu können, gewähre den privaten Forschergruppen grosse Vorteile.

Wäre es besser, anstatt in der Südschweiz an der Westküste der USA zu forschen? «Das habe ich mich auch schon gefragt. Ich war eben gerade in Kalifornien, unser Projekt stiess auf grosses Interesse. Um aber unsere Ziele zu erreichen, müssen wir unabhängig bleiben.» Was will Nnaisense? «Wir möchten noch einmal eins draufsetzen auf das, was heute schon möglich ist. Wir möchten die erste kleine KI-Maschine bauen, ein universelles kleines Lernmaschinchen, das sich selber alles beibringen kann, das herausfinden kann, was es noch nicht weiss. Das ist die Zukunft der KI, nicht nur Mustererkennung, sondern der ganze Kreislauf: Umgebung beobachten, Muster verarbeiten, Handlungsmöglichkeiten erkennen, handeln, neue Umgebungen, neue Muster, neue Handlungsmöglichkeiten.»

Schmidhuber kommt in Fahrt: «In ein paar Jahren, vielleicht sind es ein paar Jahrzehnte, werden wir die Prinzipien der Intelligenz vollständig verstehen und Intelligenz künstlich erzeugen können. Es wird im Rückblick so einfach sein, so simpel, dass jeder Gymnasiast das wird nachbauen können.»

«Bedenken Sie: Alle zehn Jahre erhöht sich die Rechenleistung um den Faktor 100. In 30 Jahren ist das eine Million. In naher Zukunft, in weniger als zehn Jahren, werden wir erstmals einen Computer haben, der über so viel Rechenleistung verfügt wie ein menschliches Gehirn. Dann wird es, wenn der Trend anhält, noch ein paar Jahrzehnte dauern, und wir werden einen kleinen Computer haben, der so viel Rechenleistung besitzt wie alle Menschen zusammen. Und es wird nicht nur einen einzigen solchen Computer geben, sondern viele. Die Menschheitsgeschichte wird eine völlig neue Richtung nehmen, alles wird anders werden.»

Haben Sie keine Angst? «Nein, es gibt keinen Grund, sich vor den Maschinen zu fürchten. Angst machen mir die Menschen, Politiker und Generäle und ihre Atomwaffen.»

Am Ende eines gut zweistündigen Gesprächs, das Aufnahmegerät ist bereits abgeschaltet, bemüht sich Schmidhuber um einen versöhnlichen Ausklang. «Es ist ja doch so, dass Gleiches gern sich zu Gleichem gesellt. Filmschauspieler reden gerne mit Filmschauspielern, Computerwissenschafter mit Computerwissenschaftern. So werden auch die künstlich intelligenten Systeme der Zukunft sich zuerst auf künstlich intelligente Systeme stürzen. Die wollen mit den Menschen gar nichts zu tun haben. Die werden das Weltall erobern und uns so wenig beachten wie wir die Ameisen.»";https://www.nzz.ch/lebensart/gesellschaft/von-ameisen-und-uebermenschen-ld.3016;NZZ;Stefan Betschon;;;
08.06.2017;Wie Algorithmen die Arbeitswelt aufmischen – Drei Szenarien für das Jahr 2050;"Bei der Geburt hätten die Eltern nie gedacht, dass ihre heute 33-jährige Tochter irgendwann als Social-Media-Managerin arbeiten würde. Ebenso schwierig ist es, sich vorzustellen, welche neuartigen Berufe es in 33 Jahren geben wird. Wird ihr Enkel vielleicht Innenausstatter für virtuelle Räume, Algorithmen-Versicherer oder Kreativitäts-Coach?
Roboter in der Chefetage

Der Roboter tritt in der Arbeitswelt zunehmend als Konkurrent, Kollege und Chef auf. Industrieroboter verdrängen Fabrikarbeiter, Finanzberater werden von Algorithmen unterstützt, und die künstliche Intelligenz findet sich auch schon einmal in Chefetagen.

Maschinen ersetzen längst nicht mehr nur Muskelkraft, sondern übernehmen immer mehr Funktionen des menschlichen Gehirns. Vom 3-D-Drucker bis zur Blockchain-Technologie – der technische Fortschritt ist rasant. Unklar ist jedoch, in welchem Tempo sich dadurch die Arbeitswelt verändern wird. Die einen Experten sprechen von einer Revolution, die anderen von einer Evolution am Arbeitsmarkt. Immer anspruchsvollere Tätigkeiten können automatisiert werden. Die Unternehmen müssen aber zunächst in der Lage sein, die neuen technischen Möglichkeiten zu nutzen. Zudem heisst es noch nicht, dass der Job wegrationalisiert wird, wenn gewisse Aufgaben von einer Maschine erledigt werden. Häufig verändert sich durch deren Einsatz einfach der Aufgabenbereich der Mitarbeiter. Wenn zum Beispiel der Labor-Roboter im Spital einfache Tätigkeiten ausführt, haben Angestellte mehr Zeit, aufwendigere Tests durchzuführen. Zudem muss die Maschine überwacht werden. Von der Automatisierung besonders betroffen sind berechenbare Routinetätigkeiten, wie sie etwa Telefonisten oder Buchhalter ausüben. Weniger gefährdet sind dagegen Aufgabengebiete von Psychologen oder Kinderbetreuern. Tätigkeiten, die Sozialkompetenz, Kreativität, Intuition oder Unternehmergeist erfordern, lassen sich nicht so einfach in Algorithmen fassen.

    Flexibilität und eigenständiges Denken werden wichtiger. 

Im Zuge des technischen Fortschritts wandeln sich auch die Anforderungen an die Qualifikation der Arbeitskräfte. Entsprechend muss das Bildungssystem angepasst werden: Wichtiger werden vor allem Kompetenzen wie Flexibilität, kritisches Denken und Selbständigkeit.

Der Strukturwandel hat bisher immer Gewinner und Verlierer hervorgebracht und teilweise zu schmerzhaften Übergängen geführt. Der gesamtwirtschaftliche Wohlstand ist aber jeweils gestiegen, und es sind neue Modelle der sozialen Absicherung entstanden. Seit Beginn der Industrialisierung sind in den Industrieländern unter dem Strich mehr und wertschöpfungsintensivere Stellen geschaffen worden. Experten sind sich jedoch uneinig, ob auch in Zukunft die Beschäftigung zunimmt. Wird der Wohlstand wiederum wachsen, oder schnellt die Arbeitslosigkeit hoch, so dass der Konsum der Mittelschicht wegbricht? Knapp 300 Experten, die im Rahmen einer Studie des globalen Think-Tanks Millennium Project (Non-Profit-Organisation mit Mitgliedern wie z. B. die Bertelsmann-Stiftung) befragt wurden, erwarten im Durchschnitt, dass weltweit bis 2050 jeder vierte arbeitslos sein wird – sofern die Arbeitsformen und Sozialsysteme nicht angepasst werden. Viele der Experten betonen, dass neue Einkommensquellen geschaffen werden sollten, die nicht auf Erwerbsarbeit im Anstellungsverhältnis beruhen. Dafür gibt es verschiedene Möglichkeiten: Diskutiert wird etwa über das bedingungslose Grundeinkommen, das 60% der Befragten als «notwendig» oder «sehr wichtig» einschätzen.
Per App dirigierte Freelancer

Ob dereinst grössere Anpassungen der Sozialsysteme notwendig sein werden, hängt jedoch nicht nur von den schwer abschätzbaren Auswirkungen der digitalen Revolution auf die Arbeitsmärkte in unterschiedlichen Ländern ab, sondern auch von anderen langfristigen Trends wie Migration, Demografie oder Globalisierung. Je nach deren Entwicklungen können Roboter helfen, den sich akzentuierenden Fachkräftemangel zu lindern, oder sie tragen dazu bei, dass die Arbeitslosigkeit weiter steigt.

    Die Grenzen zwischen befristeter Anstellung und Selbständigkeit verschwimmen. 

Der technologische Umbruch hat nicht nur Einfluss auf die Beschäftigung, sondern führt auch zu neuen Arbeitsformen. In den USA ist rund ein Drittel haupt- oder nebenberuflich freischaffend tätig. Freelancer-Plattformen gewinnen an Bedeutung. Anbieter wie der Fahrdienst Uber verstehen sich nicht als Arbeitgeber, sondern als Vermittler von Dienstleistungen. Während sich rare Spezialisten so ihren Traum von Freiheit erfüllen, spüren weniger gesuchte Arbeitskräfte vor allem den wachsenden Wettbewerbsdruck und den Verlust von Sicherheit. Sie verdienen trotz vielen kleinen Jobs nicht genügend, um ausreichend für das Alter vorzusorgen. Bei einigen Internetplattformen sind die Beschäftigten jedoch sozialversichert. Die Grenzen zwischen befristeten Angestelltenverhältnissen und Selbständigkeit verschwimmen zusehends. In den Unternehmen wird die Arbeit ebenfalls autonomer und flexibler organisiert, Hierarchien werden flacher, und Home-Office verbreitet sich. Doch es gibt gegenläufige Tendenzen: US-Konzerne wie Yahoo oder IBM haben Mitarbeiter zurück ins Büro geholt, weil räumliche Nähe unter anderem die Kreativität fördere. Werden in Zukunft ein paar wenige Manager eine breite Masse von Freischaffenden via App steuern, oder schlägt das Pendel bald wieder zurück? Wird sich die Bedeutung und die Auffassung von Arbeit in Zukunft verändern? Entstehen vielleicht ganz neue Systeme? Im Folgenden seien drei Szenarien für die Arbeitswelt im Jahr 2050 skizziert.

? Die Arbeit ist flexibel Die erwähnte Social-Media-Managerin ist bereits in Rente. Da diese aber nicht zum Leben reicht, arbeitet sie noch als Ernährungsberaterin. Die Wochenarbeitszeit beträgt vier Tage. Angestellte passen ihr Pensum vermehrt der jeweiligen Lebensphase an. Der Sohn der Pensionierten hat sich für eine Auszeit entschieden. Ein Grossteil der Arbeit wird weltweit online über zwei globale Personaldienstleister vermittelt. Viele Erwerbstätige sind fest angestellt, gehen aber nebenberuflich noch einer selbständigen Tätigkeit nach. Für beide Arbeitsformen existiert eine rudimentäre soziale Absicherung.

? Es existiert ein Grundeinkommen Die ehemalige Social-Media-Managerin lebt von einem bedingungslosen Grundeinkommen und hilft in der Nachbarschaft. Ihr Sohn hat seinen Job als Teamleiter verloren, weil nun Algorithmen die Projekte auf die Mitarbeiter verteilen. Heute kümmert er sich um Gesundheitsfragen, wodurch sein sozialer Status gestiegen ist. Die Erwerbsarbeit zahlt sich für ihn finanziell aus, auch wenn Steuern und Abgaben die Schmerzgrenze erreicht haben. Sowohl bezahlte als auch unbezahlte Arbeit ist grösstenteils in weltumspannenden Netzwerken organisiert.

? Wenige Talente dominieren Die frühere Social-Media-Managerin hat mit ihrem Sohn eine Firma gegründet, die eine umfassende Beratung für «Top-Performer» anbietet. Diese zählen zusammen mit Cyborg-Arbeitskräften (halb Mensch, halb Maschine) zum kleinen, hochproduktiven und weltweit vernetzten Talent-Pool. Breite Bevölkerungsschichten erledigen weniger produktive Tätigkeiten, liefern den Talenten zu oder haben keine Arbeit. Ein Grossteil des Wissens, der Wertschöpfung, des Kapitals und der Macht ist bei einer kleinen Elite vereint, deren Stiftungen vor allem globale Probleme lösen sollen. Die Einkommensunterschiede haben deutlich zugenommen, und die sozialen Konflikte verschärfen sich. Eine Offline-Bewegung von Selbstversorgern erhält wachsenden Zulauf.
Beruflich neu erfinden

Wer diese Zeilen in 33 Jahren liest, wird wohl den Kopf schütteln, kommt doch die Zukunft erstens anders, und zweitens als man denkt. Menschen und nicht Maschinen haben es in der Hand, das Bildungssystem zu reformieren und neue Wege zu finden, um die Arbeitswelt von morgen zu gestalten. Auch die Einstellung zum Lernen und Arbeiten dürfte sich über die nächsten Jahrzehnte verändern. So reicht etwa eine einmalige Ausbildung immer weniger für das ganze Berufsleben aus.

Für die heutige Social-Media-Managerin ist daher nicht entscheidend, ob es ihren Beruf in ferner Zukunft noch geben wird, sondern wie sie es schafft, lebenslang neue Kenntnisse zu erwerben und sich von Zeit zu Zeit beruflich wieder neu zu erfinden.";https://www.nzz.ch/wirtschaft/zukunftsszenarien-2050-wie-algorithmen-die-arbeitswelt-umwaelzen-ld.1299726;NZZ;Natalie Gratwohl;;;
18.09.2019;Smarte Gadgets bestimmen unser Leben. Wir sind Gefangene der Daten, die wir selber schaffen – und merken es nicht einmal;"Wenn man in Googles mobile Suchmaske einen Begriff eingibt, kann es mitunter passieren, dass auf dem Bildschirm folgender Hinweis aufscheint: «Danach hast du schon einmal gesucht. Wenn du mireille hildebrandt algorithmic regulation aus dem Verlauf löschst, wird diese Suchanfrage dauerhaft von deinem Gerät entfernt.» Es klingt nach einer Warnung: Suche bloss nie wieder danach! Fragt man Apples Sprachsoftware Siri nach einem «Jailbreak» – einem halblegalen Hack des Betriebssystems, mit dem das iPhone für anderenfalls gesperrte Fremdanwendungen geöffnet werden kann –, mahnt der virtuelle Assistent: «Ich halte das für keine gute Idee.» Oder: «Gefahr, Will Robinson». Die programmierte Botschaft lautet: Die Daten bleiben unter Verschluss, du kommst aus deiner digitalen Einzelhaft nicht heraus!

Google hat eine Smart-Home-Technologie patentiert, die die Bewohner auf der Grundlage ihrer Stimme oder ihres Gesichts erkennt. Wenn das System merkt, dass eine Person allein zu Hause ist, riegelt es automatisch die Türen ab. Die smarte Fernbedienung würde den Nutzer anhand von Fingerabdrücken, Gesichtserkennung oder elektromagnetischer Wellen (RFID) identifizieren und etwa bei einem Kind die Zahl der TV-Sender einschränken. Das System weiss zu jeder Zeit, wo sich die Bewohner innerhalb des Hauses aufhalten – und was sie gerade tun.

Wenn ein Haushaltsmitglied eine Textnachricht mit dem Inhalt «Ich bin um 5 Uhr zu Hause» schreibt, würde das System seinen Standort bestimmen. Wenn die Verabredung nicht «eingehalten» wird beziehungsweise die Person sich verspätet, übernimmt der Household Policy Manager das Kommando: Die Türen werden verschlossen, wenn das System feststellt, dass die Kinder allein zu Hause sind, der Fernseher blockiert oder die rote Beleuchtung aktiviert, um eine Warnung auszusenden. Es sind Zustände wie beim elektronischen Hausarrest.
Wer flucht, macht sich verdächtig

In Michel Foucaults Klassiker «Überwachen und Strafen» aus dem Jahre 1975 müssen die Mitglieder der bürgerlichen Gesellschaft noch durch die kleinlichen Disziplinierungsmassnahmen «im Rahmen der Schule, der Kaserne, des Spitals oder der Werkstätte» abgerichtet werden. Durch die präventive Kontrolle individueller Verhaltensweisen im Smart Home werden solche Disziplinierungsinstitutionen, die ja selbst immer stärker auf Formen der Selbstkontrolle abzielen, obsolet.

Ein «Audio»-, Infrarot- und optisches «Monitoring» soll bald selbst die Kontrolle des Sprachgebrauchs ermöglichen: Das System könne anhand von «Audio-Signaturen» rohe Sprache («foul language») und «Schlüsselschimpfwörter» («bully keywords») erkennen, heisst es in dem Patent. Die Abweichung von der bürgerlichen Norm wird hier auf der Grundlage von Programmiervorschriften identifiziert und automatisch sanktioniert; Gehorsam wird algorithmisch hergestellt.

In einigen Umgebungen würde das System «die Identitäten der Individuen detektieren», sobald «ungewünschte Aktivitäten auftreten». Dank Kontextdaten und einer statistischen Inferenz könne auf den Tatbestand sowie die Identität des Sprechers geschlossen werden. Diese «Erkenntnisse», heisst es in polizeilichem Ton, würden «für eine weitere nachfolgende Verwendung gemeldet und gespeichert». Das Patent liest sich an manchen Stellen wie eine Drohung: Pass bloss auf, was du sagst!
Wir bauen am eigenen Kerker

Weiter heisst es, dass ein spezieller Sensor «ungewünschte Aktivitäten» wie giftigen Kompost, Alkohol oder Tabak entdecken und melden würde. Wer also Zigarette rauchend in der Küche ein Glas Whisky trinkt, bekommt vom Household Policy Manager womöglich eine Rüge – oder einen Hinweis, dass man die Qualmerei doch besser einstellen solle. Im Smart Home ist man längst nicht mehr Herr im eigenen Haus. Dass der Strafkatalog nicht expliziert wird – der Bewohner weiss nicht, was «unerwünscht» oder «schikanös» ist –, liegt in der opaken Disziplinartechnik begründet.

Foucault zeigt in «Überwachen und Strafen», wie das Prinzip des Gefängnisses allmählich auf den gesamten «Gesellschaftskörper» übergreift: auf Disziplinarinstitutionen wie Waisenhäuser, Kliniken, Kloster, Fabriken: «Die Kreise des Kerkersystems erweitern sich und entfernen sich immer mehr von der eigentlichen Strafjustiz, bis von der Gefängnisform nichts mehr übrigbleibt.»

Die von der Gegenkultur der 68er Bewegung betriebene Dekonstruktion von Disziplinarapparaten führt dazu, dass sich der zunehmend als Selbstdisziplin verstandene Gehorsam ein neues Gehäuse schafft: das Datengefängnis. Dieser Vollzug braucht kein Gebäude, keine Mauern, keine Aufseher; er vollzieht sich selbst. Das digitale Subjekt führt sich selbst dem virtuellen Haftrichter vor: durch strafbegründende (keine strafbefreienden!) Selbstanzeigen. Wir alle bauen diesen Computer-Kerker mit, und wir alle begeben uns in freiwillige Speicherhaft.
Die Polizei hört alles. Wirklich alles

Jeder Häftling bekommt wie im Gefängnis eine Matrikelnummer, einen Score als Erkennungszeichen, mit dem er maschinenlesbar und kontrollierbar wird. Die Nutzer sind Freigänger im offenen Vollzug: Sie bewegen sich unter elektronischer Aufsicht im öffentlichen und privaten Raum und kommen ungefragt Meldepflichten nach, indem das Gerät die Position des Trägers an einen Server übermittelt. Lockerung im Vollzug gibt es nur, wenn man das Gerät ablegt.

Das Gefängnis ist räumlich nicht mehr auf die Gefängnismauern begrenzt; die Internierungspraktiken weiten sich auf den öffentlichen Raum aus. Smart Citys etwa erinnern mit ihren Überwachungskameras, Bewegungsmeldern und Sensoren an Haftanstalten, die dem Operation Room regelabweichende Vorkommnisse melden. In immer mehr US-Städten werden akustische Überwachungssysteme installiert, die Schüsse lokalisieren und automatisch die Polizei alarmieren. Die akustischen Sensoren, die in Laternenmasten in rund sieben Metern Höhe befestigt sind, zeichnen mit ihren integrierten Mikrofonen unablässig Geräusche in der Umgebung auf.

Erkennt die Software ein schussähnliches Geräusch, wird ein Bericht an einen Zentralserver gesendet. Dort werden die Audio-Snippets ausgewertet (zur Sicherheit werden sie nochmals von menschlichen Kontrolleuren überprüft). Mittels Triangulation, eines Verfahrens, bei dem die Laufzeit und die Position der Schallquelle berechnet werden, erfolgt die Ortung. Laut einem Bericht der «New York Times» sollen die Schussdetektoren in der Stadt New Bedford im US-Bundesstaat Massachusetts auch einen lauten Streit auf der Strasse aufgezeichnet haben, der einer Schiesserei vorausging.
Facebook braucht keine Mauern

Das Internet ist womöglich der grösste historische Triumph des Gefängnisses, weil das Kerkerprinzip in jeder Funkzelle implantiert ist. Das Wesen des Gefängnisses besteht ja in der Gefangenschaft: ob diese sich nun räumlich in einem Gebäude oder strukturell in einer Logik verwirklicht. Das Gefängnis hatte noch Mauern, aus denen man ausbrechen konnte. Aus dem entgrenzten Datengefängnis gibt es aber kein Entkommen. Der Facebook-, Google- oder Amazon-Nutzer wird kaserniert – er befindet sich in einem abgeschlossenen Ökosystem, das er nicht mehr verlassen soll. An- und Abwesenheiten sowie Aufenthaltszeiten werden wie in einer Kaserne oder Schule kontrolliert.

Der Log-in/Log-out-Mechanismus suggeriert zwar, dass man die Schlüsselgewalt über seine Daten habe, doch letztlich werden diese Daten in Datensilos weggesperrt und dem Zugriff des Emittenten entzogen. Der Smartphone-Nutzer oder Träger eines Fitnessarmbands wird aber nicht interniert, sondern externiert. Interniert wird der Datenkörper, der in hochgesicherten Serverfarmen sein Dasein fristet. Dort werden die Daten «gesichert» – sie sollen nicht «ausbrechen».

Die fensterlosen Rechenzentren, die wie Strafkolonien in der Ödnis von Georgia oder Iowa gebaut werden, erinnern an Hochsicherheitstrakte: Es sind Befestigungsanlagen, die von hohen Betonmauern und Zäunen umgeben sind. Rechenzentren sind Zentren der Macht, Zentren der Kalkulation, wo neben der Produktion von Wissen auch eine Vorausschau und Simulation zukünftiger Ereignisse betrieben wird.
Fast im Belagerungszustand

Diese Machtzentren, die bewusst als «Anti-Monumente» (Andrew Blum) in der Peripherie errichtet werden, gilt es zu sichern. Die Konzerne verwenden dabei Techniken der Festungsarchitektur: Die Eingänge sind klein und versteckt, die bunkerhaften Blöcke teilweise von Wäldern und Wassergräben umschlossen. Die Rechenzentren werden zu veritablen Trutzburgen hochgerüstet – die Datenzitadellen sind so konstruiert, als müsste man für einen Belagerungszustand gewappnet sein. Google etwa schützt seine Server mit Metalldetektoren, laserbasierten Einbruchmeldeanlagen sowie Fahrzeugbarrieren.

Die Fortanlagen dienen einer Herrschaftssicherung nach innen und aussen: Nach innen sollen die Datensubjekte durch algorithmische Rechenoperationen regiert werden; von aussen soll niemand unbefugt in das Herrschaftsgebiet der Konzerne eindringen. Die Tech-Konzerne exekutieren mit der «Sicherungsverwahrung» ihr Gewaltmonopol in der Datensphäre.

Nicht Menschen sind hier hinter Gittern, sondern Server und die darauf gespeicherten Daten: Suchanfragen, Sprachkommandos, Chat-Protokolle, Gesichtsscans. In den Katakomben lagern leblose Datenkörper, die wie Geister in der digitalen Unterwelt herumspuken und plötzlich zum Leben erweckt werden können. Es verwundert nicht, dass eines der ersten Rechenzentren von Google den Spitznamen «The Cage», der Käfig, trug.

Der Name ist Programm: Die Server grosser Konzerne sind in zugangskontrollierten Stahlkäfigen untergebracht. Es ist auch ein Statement: Hier kommt nichts und niemand heraus! Da passt es ins Bild, dass der US-Breitbandanbieter United Fiber & Data (UFD) ein verlassenes Gefängnis in York im US-Bundesstaat Pennsylvania in ein Rechenzentrum umfunktionieren will. Wo einst Verbrecher ihre Strafe absassen, sollen heute Daten gesichert werden.";https://www.nzz.ch/feuilleton/daten-sind-unser-gefaengnis-mit-facebook-amazon-google-co-ld.1507516;NZZ;Adrian Lobe;;;
25.10.2017;Wie sich künstliche Intelligenz übertölpeln lässt;"Dies ist einer der besten Computerwitze, er ist insbesondere gut geeignet, den gegenwärtigen Stand bei der Erforschung der künstlichen Intelligenz kurz und knapp zu beschreiben – mehr noch: Es ist laut der britischen Tageszeitung «The Guardian» «the world's greatest universal joke», der Witz schlechthin, es ist – so die Wikipedia – ein Witz, der eine fast schon kultische Verehrung geniesst und in Literatur und Kunst und auch in der Populärkultur in unzähligen Anspielungen sich zeigt, es ist ein überaus lustiger Witz, der aber auch nachdenklich stimmt, denn er enthält eine abgrundtiefe Wahrheit, es ist – gleichzeitig – auch einer der kürzesten Witze. Er lautet: 42.
Wer zuletzt lacht

Überaus kluge Computerwissenschafter auf einem entfernten Planeten hätten einst, so berichtet Douglas Adams in seinem Science-Fiction-Bestseller «The Hitchhiker's Guide to the Galaxy» (1979), einen Computer daraufhin programmiert, die allerletzte, «ultimative» Frage zu beantworten und das Leben, das Universum und überhaupt alles zu erklären. Die Abarbeitung dieses Programms habe aber sehr viel Zeit in Anspruch genommen, erst nach 7,5 Millionen Jahren kamen die Berechnungen zu einem Ende.

Und die Antwort lautet – 42. 42? Was bedeutet das? Die Nachkommen der Programmierer wundern sich. Es stellt sich heraus, dass niemand weiss, was genau die Frage war, die der Computer bearbeiten sollte, sie war während der langen Wartezeit vergessen gegangen. Wenn die Antwort 42 ist, was ist dann die dazu passende «ultimative» Frage? Nachkommen der Programmierer beschliessen, ein Computerprogramm zu schreiben, das diese Frage beantwortet. Leider dauern die Berechnungen wiederum Äonen. Lange war die Erforschung der künstlichen Intelligenz (KI) erfolglos, doch dann, nach der Jahrtausendwende, häuften sich die Hinweise, dass neuronale Netzwerke sich unter praxisnahen Bedingungen bei Aufgaben im Bereich der Mustererkennung bewähren könnten. Seit ein paar Jahren überschlagen sich in diesem Teilbereich der KI, beim Machine-Learning, die Erfolgsmeldungen. Seit kurzem gibt es Software, die bei der Transkription von gesprochenen Äusserungen, beim Lesen von handschriftlichen Notizen oder bei der Erkennung von Bildinhalten den Menschen überlegen ist.

Bei neuronalen Netzwerken bilden Millionen von Neuronen und Abermillionen von Verknüpfungen eine überaus komplexe Apparatur, bei der im Verlauf eines langwierigen Trainingsprozesses Milliarden von Drehreglern und Stellschrauben so justiert werden, dass sich die gewünschte Zuordnung von Input und Output ergibt. Wie kann man beweisen, dass diese Apparatur zuverlässig funktioniert? Man kann es nicht. Bei der Bestimmung der Verlässlichkeit ist man auf Tests angewiesen. Es ist deshalb nachvollziehbar, wenn ein altgedienter Computerwissenschafter wie Ronny Ronen, der sich bei der Entwicklung von Mikroprozessoren hervorgetan hat, angesichts von neuronalen Netzen ein ungutes Gefühl hat. Laut Ronen ist die vorherrschende Einschätzung die, dass diese Technik «im Grossen und Ganzen ‹schwarze Magie›» ist. «Wir wissen, dass es funktioniert, aber wir verstehen nicht, warum.» Als Leiter des Intel Collaborative Research Institute for Computational Intelligence hat Ronen Forschungsarbeiten initiiert, die das Innenleben von neuronalen Netzwerken verständlich machen sollen.

«Verständliche KI» lautet das neue Leitbild, «explainable AI» oder auch nur XAI. Hinter diesem Banner versammeln sich immer mehr Wissenschafter. Die Forschungsabteilung der amerikanischen Armee, die Defense Advanced Research Projects Agency (Darpa), hat zu Beginn des Jahres ein Programm angekündigt, das universitäre Forschungsteams während fünf Jahren bei der Entwicklung von XAI unterstützen will.

Warum ist es überhaupt wichtig, in die Blackbox hineinzuschauen? Kann man nicht einfach zufrieden sein, dass diese künstlich intelligente Software meistens recht gut funktioniert und uns den Alltag erleichtert? Apps wie Alexa (Amazon), Assistant (Google), Cortana (Microsoft) oder Siri von Apple können gesprochene Befehle verstehen; Tag für Tag übersetzt Google Translate mehr als 200 Millionen Texte; Facebook ist seit Februar in der Lage, Gegenstände auf Fotos zu identifizieren. Diese Systeme sind nicht perfekt, aber gut genug für den Alltagsgebrauch. Warum sich sorgen, warum einen Blick in die Blackbox wagen?

Zunehmend wird KI in Bereichen eingesetzt, in denen Fehler unverzeihlich sind. KI entscheidet über Milliardeninvestitionen, über Militäroperationen; KI ersetzt den Fahrer oder den Arzt. Bei all diesen Aufgaben reicht es nicht, wenn die Software oft recht gute Ergebnisse erzielt. Wenn es darum geht, einen Gehirntumor, eine Retinopathie, eine Herzrhythmusstörung oder eine Krebserkrankung zu diagnostizieren, erwartet man von der Software nicht nur eine hohe Verlässlichkeit, sondern auch die Fähigkeit, eine Diagnose zu begründen.

Es hat sich nun aber gezeigt, dass sich neuronale Netzwerke leicht übertölpeln lassen. Christian Szegedy von Google entdeckte 2014 einige – so der Titel seines vielzitierten Aufsatzes – «verblüffende Eigenschaften von neuronalen Netzwerken»: Es kann vorkommen, dass Fotos nicht mehr erkannt werden, nachdem sie nur sehr geringfügig und für das menschliche Auge nicht wahrnehmbar verändert worden sind. Andere Forscher täuschten neuronale Netze mit Mustern: Der Computer entdeckt nun in abstrakten Bildern konkrete Dinge, sieht einen Pandabären oder eine E-Gitarre, wo das menschliche Auge nur Farben und Formen erblickt. Diese Arbeiten zeigen, dass die Computer bei der Bildanalyse anders vorgehen als Menschen und sie die Bilder nur sehr oberflächlich analysieren. Die künstlich intelligente Software ist wie ein Idiot savant manchmal zu Spitzenleistungen fähig, manchmal abgrundtief dumm.

Bilder von handgeschriebenen Zahlen wurden geringfügig und für Menschen unbemerkbar so verändert, dass sie für die Maschinen einen anderen Wert darstellten. Nicht nur Bilder lassen sich manipulieren, sondern auch akustische Eingabesignale: Es ist gelungen, in ein Video gesprochene Befehle einzubetten, die das menschliche Ohr nicht wahrnehmen kann, die sich aber der automatischen Spracherkennung mitteilen konnten. Viele dieser Angriffe erzeugen bei unterschiedlichen neuronalen Netzwerken ähnliche Ergebnisse, viele lassen sich erfolgreich auch dann nutzen, wenn man die Innereien eines neuronalen Netzwerks nicht kennt.

Es wurden Verfahren vorgestellt, um automatisch Bilder zu generieren, an denen sich die neuronalen Netzwerke verschlucken; es ist aber auch möglich, die Bildanalyse durch kleine Eingriffe in die Realität in die Irre zu führen: Durch eine Veränderung des Lernprozesses konnte eine Software, die Verkehrszeichen zuverlässig erkennen konnte, dahin gebracht werden, dass sie diese Schilder nicht mehr wahrnimmt, sobald sie mit einem gelben Klebezettel versehen worden sind.

All diese neuen Forschungsarbeiten, die versuchen, über Hintertürchen und mit manipulierten Daten die Leistung von neuronalen Netzwerken zu schwächen, sind interessant, denn sie erlauben Rückschlüsse auf die innere Funktionsweise dieser Netzwerke. Zuerst aber zeigen sie vor allem dies: Wir wissen viel zu wenig über diese Software.
Überraschende Volte

Es werden jetzt innerhalb der Wissenschaft der künstlichen Intelligenz Methoden gesucht, die helfen können, die Innovationen der künstlichen Intelligenz den Erforschern der künstlichen Intelligenz verständlich zu machen. Für den neuen Forschungsansatz wurde schon die Bezeichnung «AI Neuroscience» vorgeschlagen; Demis Hassabis spricht lieber von «Virtual Brain Analytics». Hassabis ist einer der Jungstars der KI, der 41-jährige Wissenschafter leitet die erfolgreiche Firma Deepmind. Er fordert, dass sich die KI wieder der Neurowissenschaft annähert. «Wegen ihrer Komplexität bleiben die Produkte der KI-Forschung oft Blackboxes; wir verstehen nur schlecht die Art der Berechnungen, die sich ereignen, oder die Repräsentationen, die sich beim Erlernen von komplexen Aufgaben herausbilden.» Hassabis ruft die KI-Forscher auf, sich das Handwerkszeug der Neurowissenschafter zunutze zu machen.

Man fühlt sich an Douglas Adams und an «42» erinnert: In den 1950er Jahren sahen sich die Begründer der KI berufen, mit den Mitteln der Mathematik und der Informatik die Geheimnisse des menschlichen Gehirns aufzudecken. Im dreibändigen «Handbuch der künstlichen Intelligenz» (1981) wird gar der Anspruch formuliert, die «Natur des menschlichen Geistes» («mind») softwaretechnisch erklären zu können. Jetzt wünschen sich die Computerwissenschafter, dass ihnen die Gehirnforscher helfen, die Software zu verstehen.";https://www.nzz.ch/meinung/wie-sich-kuenstliche-intelligenz-uebertoelpeln-laesst-ld.1323885;NZZ;Stefan Betschon;;;
12.02.2019;Liebe Maschinen – lasst uns doch Freunde sein;"Kurz vor der Landung, zehn Kilometer über der Mondoberfläche, begann neben dem Computerbildschirm ein Lämpchen zu blinken. «Wir haben ein Computerproblem», funkte der Pilot der Apollo-Landefähre, Neil Armstrong, zur Erde. «Fehlercode 1202.» Im Kontrollzentrum in Houston versuchten Computerfachleute unter grösstem Zeitdruck herauszufinden, was es mit diesem Fehlercode auf sich hatte. Schliesslich sagte einer: «Wir gehen weiter mit diesem Alarm.» Für diesen Funkspruch, für den Entscheid, den Computer zu ignorieren und den Landeanflug fortzusetzen, wurde der Ingenieur im Kontrollzentrum später vom amerikanischen Präsidenten mit einem Preis ausgezeichnet. Wegen Gesteinsbrocken auf dem geplanten Landeplatz zu einem Umweg gezwungen, Fehlermeldungen bezüglich Treibstoffmangels ignorierend, brachte Armstrong die Fähre schliesslich auf den Boden und konnte die historischen Worte sprechen: «The Eagle has landed.»
Pilot gegen Autopilot

Tausende von Ingenieuren haben in den USA während der 1960er Jahre für die Raumfahrt gearbeitet, sie haben in vielen Bereichen wichtige Innovationen vorangetrieben. Auch die Kooperation von Pilot und Autopilot musste grundlegend überdacht werden. Es gab Techniker, die die Astronauten als eine Art Frachtgut betrachteten; sie glaubten, dass der Erfolg der Apollo-Missionen am besten garantiert werden könne, wenn die «humans out of the loop» blieben, wenn die Menschen an Bord während des Fluges von allen wichtigen Entscheidungen ausgeschlossen würden. Andere vertraten den Standpunkt, dass eine bemannte Raumfahrt ohne Männer, dass eine Entdeckungsreise im All ohne Entdecker nicht sinnvoll sei. Vor allem die Piloten wollten sich nicht zu Passagieren degradieren lassen: Sie waren alle hochdekorierte Militärpiloten, die Kampfeinsätze geflogen hatten. Die Ereignisse bei der ersten Mondlandung gaben ihnen recht: Es braucht die «humans in the loop», Mensch und Maschine sind aufeinander angewiesen.

Das Apollo-Programm hat der Informatik – von der Halbleitertechnik bis zum Software-Engineering – viele wichtige Impulse gegeben. Doch eine zentrale Lektion scheint vergessen gegangen zu sein: Eine künstliche Intelligenz, die nicht in der Lage ist, sich mit natürlicher Intelligenz zu verbünden, ist zum Scheitern verurteilt.

Viele spektakuläre Flugunfälle der jüngeren Vergangenheit – beispielsweise 2009 der Absturz eines Airbus A330 der Air France über dem Atlantik mit 216 Toten – sind die Folge von Unstimmigkeiten zwischen Pilot und Autopilot. Wo die Menschen vom Autopiloten zur Untätigkeit verurteilt werden, ist es unvermeidlich, dass ihre Fähigkeiten verkümmern. Wenn dann der Autopilot versagt, kann es geschehen, dass die Piloten von einer Situation, die jeder Anfänger meistern könnte, überrascht und überfordert werden.
Spektakuläre Unfälle

Autonom fahrende Autos waren in jüngster Vergangenheit in einige spektakuläre Unfälle verwickelt, bei denen Menschen starben. 2016 wurde in Florida ein Tesla-Fahrer enthauptet, weil der Autopilot einen weissen Sattelschlepper, der die Fahrbahn versperrte, nicht als Hindernis erkannte. Nach einer Untersuchung nannte die amerikanische National Traffic Safety Administration menschliches Versagen als Unfallursache. Der Autopilot setze die ständige Aufmerksamkeit des Fahrers voraus, doch dieser habe sich während der Fahrt einen Harry-Potter-Film angeschaut. Das Problem mit den Autopiloten ist nicht nur, dass sie manchmal Fehler machen, das Problem ist auch, dass sie die Menschen dazu verleiten, Fehler zu machen.

Elon Musk hatte 2016 versprochen, dass ein Tesla 2017 ohne Chauffeur von Los Angeles nach New York fahren werde. 2017 bringe für das autonome Fahren den Durchbruch, erklärte auch John Krafcik, der innerhalb des Alphabet-Konzerns die Firma Waymo leitet. Der Durchbruch lässt allerdings auf sich warten. Die Technik ist noch nicht bereit, die Software muss noch dazulernen. Sie muss nicht nur die Strassen genauer studieren, sie braucht Nachhilfe auch im Fach «Menschliches und Allzumenschliches». Denn selbst wenn es in einem einzelnen Auto gelänge, den Menschen aus dem «Loop» zu nehmen und ruhigzustellen, müsste sich die Software doch auf die Begegnung mit Menschen einstellen: Kindern, die auf die Strasse rennen, Velofahrern, die sich zwischen den Autos hindurchschlängeln, Autofahrern, die vergessen, dass sie Vortritt haben.

Oft heisst es, die Computer seien «nachweislich» die besseren Autofahrer. Doch wo wurde dieser Nachweis geführt? Wie könnte man diesen Nachweis überhaupt erbringen? Wie viele Kilometer muss eine Software unfallfrei bewältigt haben, bis sie als sicher gelten darf? Software, die im Rahmen von Machine-Learning-Prozessen hergestellt wurde, zeigt sich dem Menschen als Blackbox. Diese Software besteht nicht aus Codezeilen, die man lesen, nicht aus Algorithmen, deren Zuverlässigkeit man mathematisch beweisen könnte. Manchmal überrascht einen diese Software durch ihre Intelligenz, manchmal durch ihre Dummheit. Amerikanische Forscher haben 2017 gezeigt, dass einige wenige Zentimeter weisses Klebband genügen, um die Software eines autonom fahrenden Autos zu verwirren. Auf ein Stoppschild geklebt, können ein paar wenige weisse Punkte und Striche, die ein Mensch leicht übersieht, die Software dazu bringen, Gas zu geben, anstatt abzubremsen.
Neue Arbeitswelten

Fortschritte bei der Erforschung der künstlichen Intelligenz (KI) werden oft als Schaukampf zwischen Mensch und Maschine inszeniert. 2016 stellte die Google-Tochter Deepmind eine Alphago genannte Software ins Rampenlicht der Weltöffentlichkeit: In einem mehrtägigen Event mit mehreren Begegnungen konnte das künstliche neuronale Netzwerk den besten Go-Spieler der Welt schlagen. Viel Beachtung fand 1997 der Kampf zwischen dem Computer Deep Blue von IBM und Garri Kasparow. Der Schachweltmeister verlor, die Maschine triumphierte.

Die Berichterstattung über KI folgt dem Muster der Schaukämpfe, es geht um ein «wir gegen die». Alle grossen Beratungsunternehmen haben sich schon zu Wort gemeldet, Studie um Studie versucht darzulegen, welche Berufsleute bald vom Spielbrett weggespickt werden. Laut einer vielzitierten Studie der University of Oxford ist in den USA knapp die Hälfte aller Jobs bedroht. Andere Studien geben sich mit weniger zufrieden. Der Prozentsatz der gefährdeten Arbeitsstellen mag variieren, die Resultate unterscheiden sich, doch bei der Herangehensweise ähneln sich all diese Studien: Es geht um ein «wir gegen die».
Paradigmenwechsel

Software-Ergonomie und Interface-Design sind Forschungsthemen, die innerhalb der Computerwissenschaft eine lange Tradition haben. Als Ahnherr dieser Forschungstradition gilt der amerikanische Elektroingenieur Doug Engelbart. Er hat unter anderem die Computermaus erfunden. Er arbeitete in den 1960er Jahren am Stanford Research Institute im Silicon Valley nur wenige hundert Meter entfernt vom Artificial Intelligence Laboratory der Stanford University. Hier hatte John McCarthy das Sagen. Er gilt als Gründungsvater der KI. Es heisst, das Engelbart und McCarthy sich nicht ausstehen konnten. Als bei einer Begegnung McCarthy von seiner Mission berichtet habe, die Maschinen intelligent zu machen, habe Engelbart geantwortet: «Was? Das alles machen Sie für die Maschinen? Und was machen Sie für die Menschen?»

Engelbart wollte die Computertechnik dazu nutzen, die Fähigkeiten des Menschen zu erweitern. Sein Labor nannte er Research Center for Augmenting Human Intellect. Die Intelligenzerweiterung (Intelligence Augmentation, IA) gilt als Gegenentwurf zur Artificial Intelligence (AI). Er habe die Wissenschaft der KI begründet im Bemühen, von der Psychologie wegzukommen und die Kybernetik zu überwinden, schrieb McCarthy einmal. Während Jahrzehnten dominierte der von ihm propagierte symbolische Ansatz weltweit die Erforschung der KI. Doch auch als dieses Paradigma in den 1990er Jahren verdrängt werden konnte, blieb der KI die Menschenferne erhalten.

Es ist absehbar, dass die KI die Welt und auch die Arbeitswelt stark verändern wird. Doch dieser Umbruch wird sich nicht anhand von Arbeitslosenstatistiken erfassen lassen. Die künstlich intelligenten Maschinen werden sich im Berufsleben vieler Menschen bemerkbar machen, aber nicht als Konkurrenten, sondern als Kollegen. Um sich bei der «kollaborativen Intelligenz» zu bewähren, müssen die Maschinen anders werden. Sie müssen lernen, das menschliche Gegenüber bei der Lösungsfindung einzubeziehen, sie müssen lernen, sich selber zu hinterfragen, mit Unsicherheiten umzugehen, zu improvisieren. Sie werden nicht nur Lösungen liefern müssen, sondern auch Lösungswege, nicht nur Antworten, sondern auch Fragen.";https://www.nzz.ch/meinung/kopie-von-liebe-maschinen-lasst-uns-doch-freunde-sein-ld.1459048;NZZ;Stefan Betschon;;;
29.08.2019;Der grosse Sprung in die Cloud;"Ein bisschen Wald, ein bisschen Autobahn, viele Äcker, Wiesen, Felder und ein kleiner Flugplatz. Es gibt auch einen Bahnhof und eine Bushaltestelle. Hier leben Menschen, irgendwo müssen Wohnhäuser sein, doch wer im Auto auf der Hauptstrasse den Ort erreicht oder wer am Bahnhof aus dem Zug steigt, wird vor allem Bürohochhäuer erblicken, Lagerhallen, Industriebetriebe. Auf einem Schild heisst es: «Chauffeur gesucht». Es ist ein Hinweis darauf, dass dieser Ort im Schweizer Mittelland ein Umschlagplatz ist für Güter aller Art. Ein Chemielogistiker hat hier Dutzende wenn nicht Hunderte von Tankwagen und Tank-Containern abgestellt, eine weltweit tätige Lebensmittelfirma organisiert von hier aus die Verteilung von Frischbackbrötchen, man sieht Lastwagen mit Kies, auf Geleisen warten lange Züge mit Neuwagen aus Deutschland darauf, abgeladen zu werden.
Hauptort im Cyberspace

Das ist Lupfig. Nicht Dorf, nicht Stadt, nicht Vorstadt, nicht Agglomeration. Im Web lobt der Gemeinderat den Ort einerseits als Industriestandort, der aber andererseits durch «seine natürliche Umgebung ein idyllischer und ländlicher Wohnort» geblieben sei. Das ist Lupfig. Einerseits, andererseits. Irgendwie dazwischen. Zwischen Berg und Tal, zwischen Stadt und Land, in der Mitte zwischen Basel, Bern, Luzern, Zürich. Lupfig liegt abseits der grossen Schweizer Städte. Doch im Cyberspace ist Lupfig ein Hauptort. Viele wichtige Datenleitungen führen hier vorbei. Das hat schon auch etwas mit den Lastwagen und der Logistik zu tun: Denn viele wichtige Datenleitungen folgen den Autobahnen und Eisenbahnlinien oder auch den Gas-Pipelines oder den Stromleitungen. Lupfig ist sehr eng mit dem Schweizer Internet Exchange Point Swiss-IX verbunden. Über diesen Knotenpunkt, der Schweizer Computer mit dem europäischen Internet (Euro-IX) verbindet, können bis zu 120 GBit pro Sekunde transportiert werden. Die Datenkabel im Boden lassen in Lupfig die Datenzentren in die Höhe schiessen.

Das Wort «schiessen» ist mit Bedacht gewählt. In nur zwölf Monaten hat die Green Datacenter AG in Lupfig ihr jüngstes, drittes Rechenzentrum gebaut. Das vierstöckige Gebäude mit einer Bruttogeschossfläche von 12 520 Quadratmetern wird «Zürich West 3» genannt. Vor einem Jahr wurde der erste Spatenstich ausgeführt, jetzt ist das Gebäude bereit für den Bezug.

An jedem Tag waren in den vergangenen zwölf Monaten rund hundert Handwerker auf der Baustelle beschäftigt. Man kann sich die Arbeitsorganisation vorstellen wie ein kompliziertes Ballett: Jeder einzelne Arbeitseinsatz ist darauf angewiesen, dass Vorarbeiten präzis ausgeführt und bestellte Baumaterialien pünktlich angeliefert worden sind. Hat es auch brenzlige Situationen gegeben, Verzögerungen, Tage, an denen es schien, als könnte der eng getaktete Fahrplan nicht eingehalten werden? «Brenzlige Situationen gab es täglich», sagt Reto Meier, der als «data center architect» die Bauarbeiten geplant und beaufsichtigt hat. «An jedem Tag gibt es viele zehntausend Probleme, man muss immer wieder eingreifen, man muss präsent sein, rasch reagieren.» Eine kleine Verzögerung an einem Ort könnte schnell grosse Auswirkungen haben, «das führt zu einem Dominoeffekt».
Hohe Effizienz

Beim Bau von Datencenter setzt Green mit «Zürich West 3» einen neuen Standard. Das Datencenter wurde auch für Hyperscaler gebaut. Das sind grosse, international tätige Computerfirmen wie Facebook oder Oracle, die selber Computing-Dienstleistungen anbieten. Diese Kunden setzen nicht nur höchste Anforderungen bezüglich Sicherheit und Verfügbarkeit, sie fordern auch Flexibilität und Raumreserven für schnelles Wachstum. «Zürich West 3» wird von Green angepriesen als das schweizweit erste «Hochleistungsrechenzentrum» oder «High-Density-Rechenzentrum». Das bedeutet, dass hier für jeden Computerschrank (Rack) 25 Kilowatt Strom zur Verfügung gestellt werden können. Das sei gut drei Mal mehr als in anderen Datenzentren. Die elektrische Leistung kommt den Computern zugute, sie verlangt aber auch einen Mehraufwand bei der Kühlung. Auf dem Dach von «Zürich West 3» gibt es acht Kältemaschinen. Sie sorgen dafür, dass die Temperatur in den «Datenhallen» 27 Grad nie übersteigt. Das Kühlsystem sei so effizient, dass an vier von fünf Tagen die Aussenluft genüge, um das Wasser zu kühlen, das die Hitze aus dem Inneren des Gebäudes herausbringt.

Die Energieeffizienz eines Rechenzentrums lässt sich mit einer Power Usage Effectiveness (PUE) genannten Kennzahl bewerten. Der PUE-Wert setzt die insgesamt in einem Rechenzentrum verbrauchte Energie ins Verhältnis mit der Energieaufnahme der Computer. Ein PUE-Wert von 2 würde bedeuten, dass die Lüftungsanlagen und andere Infrastrukturkomponenten gleich viel Strom verbrauchen wie die IT-Geräte. Je näher der Wert bei 1 liegt, desto kleiner ist der Stromverbrauch der Infrastruktur, desto effizienter ist ein Rechenzentrum. Ein Bericht des Basler Instituts für Wirtschaftsstudien schätzte 2014 den durchschnittlichen PUE-Wert von Schweizer Rechenzentren auf 1,73. Green erreicht bei «Zürich West 3» einen PUE-Wert von 1,19: Das stellt in Sachen Effizienz eine markante Verbesserung dar. Viele Wolken

Der Fortschritt in der Informatik lässt sich beschreiben als eine Zunahme der Virtualisierung. Virtualisierung bedeutet das Einfügen einer vermittelnden Instanz zwischen einem Anbieter und einem Nutzer einer Hardware- oder Software-Ressource. Auf diese Weise wird ein komplexes System in Subsysteme zerlegt, die über definierte Schnittstellen miteinander kommunizieren. Anstatt von «vermittelnder Instanz» reden Informatiker lieber von Abstraktionsschichten. Den Computer stellen sie sich vor als eine Art Crèmeschnitte, der Fortschritt zeigt sich unter anderem darin, dass die Zahl der Schichten zunimmt.
Anfänglich waren die verschiedenen Subsysteme in einem Gehäuse vereint, dann ermöglichten es Computernetzwerke, entfernte Ressourcen einzubinden. Auch das Internet ist eine Crèmeschnitte, ein zusammengesetztes Etwas. Es ist nicht ein Netzwerk, sondern ein Netz der Netze: ein Netz, das die Nutzung von entfernten Netzwerken ermöglicht, ohne dass man sich um die innere Organisation dieser anderen Netzwerke kümmern müsste. Bereits in den 1980er Jahren hat es sich eingebürgert, auf Diagrammen diese entfernten Netze, deren Beschaffenheit man nicht kannte, als Wolke einzuzeichnen.

Cloud-Computing-Angebote stellen für beliebige Benutzer («public cloud») oder für eine geschlossene Benutzergruppe («private cloud») Rechenleistung und Speicherplatz («infrastructure as a service», IAAS), eine Betriebssystemumgebung («platform as a service», PAAS) oder Anwendungsprogramme («software as a service», SAAS) zur Verfügung. Eine Hybrid-Cloud kombiniert private und öffentliche Formen der Nutzung. Cloud-Computing-Dienstleistungen sind für einen Anwender interessant, weil er nur für die Ressourcen bezahlen muss, die er tatsächlich braucht. Bei Belastungsspitzen lassen sich zusätzliche Kapazitäten vorübergehend dazukaufen. Amazon, ein Pionier des Cloud-Computing, hat diese Fähigkeit im Namen eines IAAS-Produkts zum Ausdruck gebracht: Die Dienstleistung wurde 2006 als Elastic Compute Cloud eingeführt. Microsoft, ein anderer global wichtiger Anbieter von Cloud-Computing-Dienstleistungen, hat sich zunächst in Sachen PAAS hervorgetan.

Das Symbol der Wolke steht für Rechenressourcen, die irgendwo im grossen, weiten Internet zur Verfügung stehen. Es ist eine Erleichterung, dass man sich nicht mehr um die Details der Datenspeicherung kümmern muss. Doch manchmal möchte man genau wissen, wo die Daten sich aufhalten und welcher Rechtsprechung sie unterliegen. Unter Umständen gibt es gesetzliche Regelungen, die den Export von Daten einschränken oder verbieten. Deshalb haben nun auch die globalen Anbieter von Cloud-Diensten angefangen, sich an den Landesgrenzen zu orientieren.

Microsoft hat am Mittwoch eine Schweizer Cloud angekündigt. Auf der Medienmitteilung ist das Matterhorn abgebildet, um Swissness zu verkünden. Die Firma hat keine eigenen Datencenter aufgebaut, die nun eingeweiht werden könnten. Vielmehr hat sie sich bei einem Betreiber eines Datencenters eingemietet. Wer das ist, wurde nicht bekanntgegeben. Dem Vernehmen nach ist der Standort, den Microsoft «Switzerland North» nennt, nicht identisch mit «Zürich West».

Neben «Switzerland North» in der Umgebung von Zürich hat Microsoft auch in der Nähe von Genf Platz in einem Rechenzentrum gemietet: «Switzerland West». Die beiden Zentren ergänzen sich, Ausfälle an einem Ort können am anderen kompensiert werden. Bereits habe man dreissig Schweizer Firmen für die Schweizer Cloud gewinnen können, schreibt Microsoft. Dazu gehören bekannte Namen wie UBS, Swiss Re, Mobiliar, Skyguide und Swisscom. Swisscom beispielsweise wird die Rechenleistung, die Microsoft in der Schweiz gemietet hat, um Cloud-Dienstleistungen anzubieten, nutzen, um den eigenen Kunden Cloud-Dienstleistungen anbieten zu können.
Der grosse Boom

Es gibt in der Schweiz einen Boom beim Bau von Rechenzentren. Zwischen 200 bis 400 Millionen Franken würden hierzulande jährlich in den Bau von neuen Rechenzentren gesteckt, heisst es in einer Studie von Deloitte. Es handle sich um einen «dynamischen Markt», man beobachte ein «rasantes Wachstum», es sei nun aber mit einer «Konsolidierung» zu rechnen. Von dieser Konsolidierung ist nun schon seit einigen Jahren die Rede, bisher ist sie nicht eingetreten.

Roger Süess hält ein Umsatzwachstum von gegen 50 Prozent im Schweizer Cloud-Geschäft für möglich. Der Ingenieur hat einen grossen Teil seiner Berufskarriere bei Grossbanken verbracht, er hat sich während Jahrzehnten mit Bankeninformatik beschäftigt. Nach Aufenthalten in Zürich und in amerikanischen und kanadischen Grossstädten ist er jetzt nach Lupfig umgezogen; seit Juli ist er CEO von Green. Das Landleben gefalle ihm, sagt er im persönlichen Gespräch. Er geniesse die Ruhe, gleichzeitig schätze er die Agilität von Green. Der Stellenwechsel komme ihm vor, als sei er von einem Öltanker auf ein Schnellboot umgestiegen. In der Schweiz sei das Wachstum beim Cloud-Computing noch ausgeprägter als in anderen Ländern; einerseits gebe es bei vielen hiesigen Firmen noch Nachholbedarf, andererseits drängten auch internationale Player ins Land, weil hier die Voraussetzungen für den Aufbau von Datacenter-Kapazitäten überaus günstig seien.

Treiber hinter diesem Boom ist das rasche Wachstum der Datenmengen, die überall verarbeitet werden müssen. Laut Schätzungen der Marktforschungsfirma IDC nimmt das Datenvolumen in Europa zwischen 2018 und 2025 um 25 Prozent pro Jahr zu. Die Schweiz profitiert aber im internationalen Vergleich überdurchschnittlich von dieser Nachfrage nach Datencenter-Dienstleistungen, denn das Land im Herzen Europas gilt als sicherer Hafen für Daten. Auf dem Data-Centre-Risk-Index der amerikanischen Immobilienfirma Cushman & Wakefield rangiert die Schweiz auf Platz drei nach Island und Norwegen. Wichtige Kriterien bei der Bewertung der Standorte sind Schutz vor Naturkatastrophen, Stabilität der Stromversorgung und politische Stabilität. Die Kosten für Energie fallen stärker ins Gewicht als die Unternehmenssteuern.
Es geht um Vertrauen

Die «Datenhallen» in Lupfig stehen noch leer. Bald werden Mitarbeiter der Hyperscaler mit 40-Tonnen-Lastwagen vorfahren und Cloud-Computing-Technik containerweise über die grosszügig konzipierten Rampen und Warenlifte an ihren Platz hieven. Diese Grosskunden kaufen von Green Stellfläche, Strom, Kühlung, Sicherheit.

Was ist im Kern das Produkt, das Green anzubieten hat? «Im Kern geht es um Vertrauen», sagt Süess. «Zu allererst ist es Vertrauen, das wir verkaufen. Wir sind eine Bank, eine Bank für Daten – eine Datenbank. Unsere Speichersysteme sind Tresore für Daten. Und Daten sind das Gold des 21. Jahrhunderts.»

Zu den namentlich bekannten Kunden von Green gehören ABB, HP und SIX. Es wird gemunkelt, dass auch Google sich im neuen Rechenzentrum von Green eingemietet habe. Eine Bestätigung dafür ist nicht zu erhalten. Green bedient auch mittlere und kleinere Firmen, die zunächst einfach nur Speicherplatz und Rechenleistung mieten. Oftmals wissen diese Kunden anfänglich noch nicht, was sie vom Cloud-Computing erwarten dürfen. Sie möchten von Green beraten werden. «Wir müssen das Business unserer Kunden verstehen, das ist die Voraussetzung für unseren Erfolg», sagt Süess. «Wir müssen den Kunden nicht nur eine Lösung aufzeigen für ein gegenwärtiges Problem, sondern auch einen Weg in die Zukunft.»

Werden bald alle firmeninternen Computerressourcen in die Cloud abwandern? «Der Trend Richtung Cloud ist unaufhaltsam. Jeder Informatikverantwortliche muss sich damit beschäftigen. Es ergibt keinen Sinn mehr, ein eigenes Datencenter zu betreiben. Das ist, als wollte man sich auch selber um die Stromerzeugung kümmern und Kraftwerke betreiben.» Es führten aber viele Wege in die Cloud, jede Firma müsse ihr eigenes Tempo finden. Weitere Ausbauschritte

In den vergangenen zehn Jahren hat sich in Lupfig die für Computertechnik bereitstehende Fläche von knapp 5000 auf über 40 000 Quadratmeter erhöht. Am 17. September soll «Zürich West 3» eingeweiht werden, und schon liegen die Pläne bereit für «Zürich West 4» mit einer Fläche von 11 860 Quadratmetern. Dieses Rechenzentrum könnte 2020 fertiggestellt sein. Auf dem Campus der Green Datacenter AG hätte es auch noch Platz für «Zürich West 5» und «Zürich West 6».

Welche Kriterien erlauben es, Fortschritte beim Bau von Rechenzentren zu erkennen? «Es dreht sich alles um Energieeffizienz. Hier wurden in jüngster Vergangenheit grosse Fortschritte erzielt.» Wie geht es weiter? «Es ist komplex und wird immer komplexer. Alles wird immer virtueller.»";https://www.nzz.ch/digital/der-grosse-sprung-in-die-cloud-ld.1505065;NZZ;Stefan Betschon;;;
15.12.2016;Die Demokratisierung der künstlichen Intelligenz;"Barcelona war vergangene Woche die Welthauptstadt der künstlichen Intelligenz. Mehrere tausend Wissenschafter aus aller Welt trafen sich hier anlässlich der Conference on Neural Information Processing Systems. Alles, was Rang und Namen hat, war anwesend: Yoshua Bengio (Université de Montréal) und Christopher Bishop (Microsoft) waren da, ebenso wie Demis Hassabis (Google), Geoffrey Hinton (Google), Yann LeCun (Facebook) und Jürgen Schmidhuber (Universität Lugano). Die Konferenz, die seit 1987 stattfindet, ähnelte noch vor kurzem einem Geheimtreffen, das – so erinnert sich Bishop in seinem Blog – ein paar wenige Hundert Forscher zusammenbringen konnte.
Rasches Wachstum

In jüngster Vergangenheit ist nun aber das Interesse an der künstlichen Intelligenz – genauer: am Machine Learning – explodiert. Weil einige der Pioniere auf diesem Gebiet ihre Werkzeuge auch anderen zur Verfügung stellen, konnten für Neueinsteiger die Hürden gesenkt werden. Microsoft wollte in Barcelona, so schreibt «The Official Microsoft Blog», Wege aufzeigen, wie sich Machine Learning demokratisieren lasse, wie der Kreis der Anwender, der aus diesen Techniken Nutzen zu ziehen vermag, ausgeweitet werden könne.

Es lässt sich für Aussenstehende nicht beurteilen, ob an diesem Treffen weltbewegende neue Ideen publiziert worden sind. Der Konferenzbericht mit mehreren hundert Beiträgen kommt aber gewichtig daher.
«Coming-out Party»

In den Medien erregte diese Konferenz zunächst nur deshalb Aufsehen, weil sich auch Ruslan Salakhutdinov zu Wort meldete. Salakhutdinov lehrt und forscht an der Carnegie Mellon University. Seit Ende Oktober leitet er zudem die KI-Abteilung von Apple.

Dass ein Computerwissenschafter Konferenzen besucht, sich mit seinesgleichen trifft, Gespräche führt, Vorträge hält und wissenschaftliche Aufsätze publiziert, ist nichts Besonderes. Ist es aber schon, wenn der Wissenschafter für Apple arbeitet. Apple liebt die Geheimniskrämerei, die einzige Möglichkeit, sich über die Forschungsbemühungen dieser kalifornischen Firma zu informieren, war bisher das Studium von Patentanträgen.

In Barcelona nun zelebrierte Salakhutdinov – wie es das Technik-Magazin «Wired» formuliert hat – seine «Coming-out Party». Er gab seine Absicht kund, die Öffentlichkeit künftig über die Resultate der von Apple finanzierten Forschung informieren zu wollen.
Überraschende Erkenntnisse

Apple hat Mühe, im Forschungsfeld künstliche Intelligenz mit der Konkurrenz mitzuhalten. Amazon, Facebook, Google, IBM und Microsoft legen ein forsches Tempo vor, konnten viele hochdekorierte Wissenschafter für sich gewinnen, haben zahlreiche Jungfirmen übernommen, publizieren laufend wissenschaftliche Aufsätze mit immer wieder überraschenden Erkenntnissen.

Es herrscht in den KI-Forschungslabors Fachkräftemangel. Gute Arbeitsbedingungen und hohe Saläre allein reichen nicht aus, um fähige Leute an sich zu binden, man muss – wie Apple offenbar erst jetzt festgestellt hat – den Wissenschaftern auch erlauben, zu tun, was Wissenschafter tun, nämlich mit anderen Wissenschaftern sich auszutauschen und die eigenen Ideen der öffentlichen Kritik auszusetzen.

Die Konkurrenz ist schon sehr viel weiter. Google und auch Microsoft haben nicht nur Forschungsresultate öffentlich gemacht, sondern auch Werkzeuge und Datensammlungen, die es für die Entwicklung von KI-Software braucht. Google publizierte vor einem Jahr ein Tensor Flow genanntes Framework für maschinelles Lernen. Die Software, die von Google für die eigenen Entwicklungsprojekte im Bereich Spracherkennung und Bildanalyse verwendet wird, wurde im Quelltext publiziert. Das Pendant von Microsoft – Cognitive Toolkit – ist seit diesem Sommer ebenfalls öffentlich zugänglich. Google und Microsoft offerieren darüber hinaus seit kurzem auch Cloud-Computing-Angebote, die Software-Werkzeuge, Datensammlungen und Rechenleistung bündeln. Die Google Cloud Machine Learning Platform ebenso wie das Microsoft Azure Machine Learning erleichtern externen Forschern die Durchführung von anspruchsvollen KI-Projekten.

Google Deepmind und die von Elon Musk gegründete Nonprofitorganisation Open-AI haben Anfang Dezember gleichzeitig Plattformen für Machine Learning öffentlich zugänglich gemacht.
«Brain Drain»

Die grosse Nachfrage nach KI-Forschern seitens der Privatindustrie sei «aufregend», aber auch «beunruhigend», schrieb kürzlich die Wissenschaftszeitschrift «Nature». In den vergangenen zwei Jahren hat allein Deepmind 144 Wissenschafter eingestellt. «Nature» lässt Wissenschafter zu Wort kommen, die befürchten, dass der Braindrain die universitäre Forschung schwäche, die Ausbildung von Nachwuchs behindere.";https://www.nzz.ch/digital/aktuelle-themen/machine-learning-die-demokratisierung-der-kuenstlichen-intelligenz-ld.135034;NZZ;Stefan Betschon;;;
26.07.2019;Die Diagnose kommt vom Computer;"Als die 45-Jährige das Universitätsspital an der Rämistrasse in Zürich wieder verlässt, hat sie kaum Notiz davon genommen, dass bei ihrer Krebsvorsorge künstliche Intelligenz (KI) im Spiel war. Die Frau ist zur Mammografie in die Klinik gekommen. Eine Routineuntersuchung beim Screening von Brustkrebs.

Allerdings weist ein Computer-Algorithmus, der die Röntgenbilder automatisch analysiert, sofort auf das dichte Brustgewebe der Patientin hin. Das kann Krebsherde bei der Mammografie maskieren – weshalb dann meist zusätzlich eine Ultraschalluntersuchung erfolgt. Im Falle der Mittvierzigerin schürt dieser Scan keinen weiteren Verdacht. Erleichtert taucht die Frau in ihr Alltagsleben ein.

«Künstliche Intelligenz lässt sich hervorragend in solchen Konstellationen einsetzen», kommentiert Andreas Boss, Radiologe am Zürcher Universitätsspital und Miterfinder der selbstlernenden Software für die Dichtebestimmung. Für Boss liegt der Nutzen heutiger KI gerade dort, wo sie klinische Prozesse standardisiert und Qualitätskontrollen erleichtert – und dabei Fehler vermeiden hilft, die Ärzte sonst machen.
Mittel gegen variierende Befunde

Natürlich könnte der Radiologe die mammografischen Bilder selber in Augenschein nehmen, um die Brustdichte zu beurteilen und zu entscheiden, ob eine Sonografie zum Krankheitsausschluss notwendig ist. Genau dies ist derzeit gängige Praxis. Allerdings kann der visuelle Befund von Untersucher zu Untersucher – und selbst bei ein und demselben Arzt – beträchtlich variieren.

«Wenn ein Radiologe eine Röntgenaufnahme heute und noch einmal in einer Woche bewertet, kommt er manchmal zu anderen Schlüssen, und eine Ultraschalluntersuchung würde dementsprechend gemacht oder eben nicht», verdeutlicht Boss das Problem. Die KI liefere dagegen bei wiederholter Analyse identischer Bilder stets denselben Wert. Mittlerweile hat sich ihr Einsatz am Zürcher Universitätsspital als Standard etabliert.

Und das Team von Boss denkt schon weiter. Ein logischer nächster Schritt sei, auch die Interpretation der Ultraschallbilder – die ebenfalls stark vom jeweiligen Radiologen abhängt – durch KI konsistenter zu machen, sagt Boss. Derzeit erprobt eine ganze Reihe internationaler Forscherteams und Medizintechnikfirmen genau solche und weitere Algorithmen für die Erkennung von Brustkrebs. Künstliche Intelligenz fügt sich so in kleinen Schritten, oft ganz unscheinbar, in diagnostische Abläufe ein.
Kommerziell erhältliche Algorithmen

Längst sind unterschiedliche KI-Algorithmen auch kommerziell erhältlich. Die Bremer Firma MeVis bietet beispielsweise eine Software für das Lungenkrebs-Screening an, die unter anderem auffällige Lungenknoten auf CT-Scans in 3-D vermisst und den Befundbericht automatisch erstellt. Ein Algorithmus des dänischen Unternehmens Visiana, der das Knochenalter bei Kindern bestimmt, ist schon seit einigen Jahren im Routine-Einsatz: etwa um Wachstumsstörungen abzuklären.

Normalerweise ermitteln Radiologen die Skelettreife, indem sie eine Röntgenaufnahme der Hand mit Referenzbildern vergleichen. Das erledigt das Computerprogramm schneller und im Schnitt auch zuverlässiger. Der Algorithmus war ursprünglich anhand von zwei Langzeituntersuchungen, den sogenannten Zürcher Longitudinalstudien, mit einigen hundert Schweizer Mädchen und Jungen entwickelt und validiert worden. Mittlerweile nutzen ihn nach Firmenangaben 150 Kliniken weltweit.

Viel ist darüber geredet worden, ob intelligente Algorithmen die besseren Ärzte seien, gleichsam Geister aus der Maschine, die den Menschen aus der Patientenversorgung vertreiben. Aber das ist die falsche Frage. Eine Daumenregel unter KI-Experten besagt, dass sich alles, was ein Arzt mit einem Blick vernünftig beurteilen kann, heute ebenso mit künstlicher Intelligenz diagnostizieren lässt. Auf absehbare Zeit dürfte KI in der Medizin daher vor allem für einfache, repetitive und fehleranfällige Aufgaben zum Einsatz kommen. «Wenn es richtig schwierig wird, hat KI momentan noch keine Chance», ist Boss überzeugt. KI ist daher ein Hilfs-, kein Allheilmittel.

Die Faszination für künstliche Intelligenz kommt nicht von ungefähr. Generell werden Computerprogramme mit diesem Begriff umschrieben, wenn sie bestimmte kognitive Fähigkeiten eines Menschen imitieren. Für die Medizin ist dabei ein Teilbereich der KI, das maschinelle Lernen, von besonderer Bedeutung – insbesondere das erst vor einigen Jahren eingeführte «Deep Learning», das auf komplexen («tiefen») künstlichen neuronalen Netzen beruht.
Neurobiologisch inspirierte Netze

Diese Computer-Algorithmen bilden das Gehirn zwar nicht wirklichkeitsgetreu ab, sind aber doch von der neurobiologischen Funktionsweise und Vernetzung echter Nervenzellen inspiriert. Beispielsweise können die Netz-Algorithmen mit bekannten Bilddaten «trainiert» werden und dabei ihre internen Parameter selbständig immer weiter anpassen, bis sie Muster und Strukturen auch in neuen Bildern treffsicher erkennen. Neuronale Netze lernen also aus Erfahrung.

Manche der frappierendsten KI-Anwendungen in der Medizin fussen dabei auf einem speziellen Typ von Netz-Algorithmen, bekannt als «Convolutional Neural Networks» (CNN). Diese können zum Beispiel auf Alltagsfotografien die abgebildeten Objekte fehlerfreier zuordnen als ein Mensch. Eine wachsende Zahl von Pilotstudien legt nahe, dass solche Algorithmen auch für die bildbasierte Diagnostik in der Medizin taugen.

In einem interdisziplinären Projekt von Google-Entwicklern und Medizinern beispielsweise erkannte ein CNN-Algorithmus krebsverdächtige Lungenknoten auf CT-Scans präziser als sechs Radiologen, sofern keine Voraufnahmen zur Verfügung standen. In einem anderen Fall konnte ein neuronales Netz schwarzen Hautkrebs auf digitalen Bilder zuverlässiger detektieren als 136 von insgesamt 157 getesteten Dermatologen an deutschen Universitätskliniken.

Ein CNN-Algorithmus zur Abklärung von diabetischen Netzhautveränderungen wertete Augenhintergrund-Fotografien ähnlich gut aus wie mehrere amerikanische Augenärzte. Und wieder ein anderer CNN-Algorithmus übertrumpfte ein Gremium von elf Pathologen dabei, mikroskopische Gewebeschnitte unter alltagsüblichem Zeitdruck auf Lymphknotenmetastasen hin zu beurteilen. Auch das Computerprogramm, das Andreas Boss am Universitätsspital Zürich für die Brustdichtemessung verwendet, basiert auf einer CNN-Architektur.

Darüber hinaus sind zahlreiche weitere klinische KI-Anwendungen denkbar, etwa um hochkomplexe oder dynamisch veränderliche Daten zu verarbeiten. So soll in einem schweizweiten Kooperationsprojekt untersucht werden, ob sich anhand von quantitativen Bilddatenanalysen der individuelle Krankheitsverlauf bei Hirntumorpatienten besser abschätzen lässt.
Strenge Praxistests erforderlich

«Auf lange Sicht könnte dies in therapeutische Entscheidungsunterstützungs-Systeme einfliessen», sagt Christoph Stippich, Neuroradiologe am Zürcher Universitätsspital, der an dem Projekt beteiligt ist. Unterdessen erforscht seine Kollegin Emanuela Keller von der neurochirurgischen Intensivstation mit akademischen und Industriepartnern, wie sich mithilfe von KI aus den Echtzeitsignalen medizinischer Monitoringsysteme spezifische Muster herausfiltern lassen, um bei Schwerkranken kritische Komplikationen vorherzusehen und zugleich Fehlalarme zu unterdrücken.

KI ist aber nicht gleich KI. Jeder Algorithmus muss sich erst in der Praxis bewähren. Eine generelle Frage dabei: Wie vertrauenswürdig sind Algorithmen bei der Patientenbehandlung, bei der zuletzt Ärzte für das Ergebnis geradestehen? Bekannt ist beispielsweise, dass sich spezifische KI-Anwendungen nicht einfach unbesehen auf andere Orte und Zusammenhänge übertragen lassen: Ein in Zürich entwickelter Bilderkennungs-Algorithmus muss in Zagreb längst nicht so gut funktionieren.

Denn bereits kleine Unterschiede in den Patientenkollektiven, Scanning-Protokollen und Bilddatenmustern können die neuronalen Netze irritieren – selbst dann, wenn solche Unterschiede mit blossem Auge auf den Bildern nicht wahrnehmbar sind. Tatsächlich lässt sich bei tiefen neuronalen Netzen in der Regel nicht sagen, wie sie ihre Entscheidungen im Einzelnen treffen, weshalb sie oft als «Black Boxes» charakterisiert werden.

Um solche Probleme in den Griff zu bekommen, fordern Experten, KI-Anwendungen multizentrisch an vielen verschiedenen Datensätzen zu validieren. Auch entwickeln Forscher zunehmend Algorithmen, die beispielsweise nicht nur eine bestimmte Diagnose liefern, sondern gleichzeitig nachvollziehbar machen, welche Bildstrukturen für das Ergebnis massgeblich waren – ein als «Explainable Artificial Intelligence» bezeichneter Ansatz.
Praktikabilität ist entscheidend

Fast noch ausschlaggebender für den Alltagseinsatz von KI ist ihre Praktikabilität. «Ein technisches Hilfsmittel wird genutzt, wenn es Zeit spart und nicht Zeit kostet», sagt Boss. Der Algorithmus, den sein Team für die Brustdichtemessung verwende, erleichtere den Arbeitsablauf. So habe die Röntgenassistentin, die die Mammografie durchführe, früher zunächst beim Radiologen nachfragen müssen, ob ein ergänzender Ultraschall erforderlich sei – für beide eine störende Unterbrechung, gerade bei vielen Untersuchungen hintereinander. Dank der KI-Analyse kann die Assistentin die Sonografie nun mit einem automatischen Brustultraschallgerät unmittelbar vornehmen, ohne die Patientin warten zu lassen, während der Arzt die Aufnahmen am Schluss gesammelt interpretiert.

Klar ist, dass sich gerade in der Radiologie neue Algorithmen nahtlos in bestehende IT-Systeme einpassen und sozusagen wie virtuelle Labortests an den Bilddaten ablaufen müssen. Ein vielversprechendes Konzept ist beispielsweise, diagnostische Scans automatisch nach Dringlichkeit zu sortieren. Mediziner sprechen auch von Triage. So setzt das amerikanische Spitalnetzwerk Geisinger in seinen Kliniken in Pennsylvania und New Jersey einen Algorithmus ein, der Schädel-CT in wenigen Sekunden auf Hirnblutungen überprüft und die Bilder bei auffälligem Befund für die prompte Beurteilung durch den Radiologen priorisiert.

Auch das umgekehrte Szenario verfolgen KI-Spezialisten. Ein europäisches Medizinerteam weist in einer Machbarkeitsstudie darauf hin, dass sich bei der Brustkrebsvorsorge viele normale Mammogramme mit einer bereits verfügbaren Software herausfiltern lassen und womöglich gar nicht mehr durch einen Radiologen bewertet werden müssten. Gerade bei flächendeckenden Screening-Programmen könnte dies die Arbeitsbelastung deutlich verringern.
Neue Möglichkeiten in armen Ländern

Besonders bemerkenswert mutet der Einsatz von KI in Ländern an, wo es nicht darum geht, Gesundheitssysteme zu entlasten – sondern die Versorgung überhaupt erst sicherzustellen. Laut einer Studie mit Patientinnen in Costa Rica wäre es in Gegenden ohne adäquate medizinische Vorsorge möglich, mithilfe eines einfachen Spekulums zur Inspektion der Vagina, einer Smartphone-Kamera und eines Bildanalyse-Algorithmus die Vorstufen von Gebärmutterhalskrebs zu entdecken. Längst praktisch bewährt hat sich eine Software des niederländischen Startups Thirona für die Früherkennung von Tuberkulose. Der Algorithmus, der in zahlreichen Gesundheitsprojekten etwa in Sierra Leone, Nigeria, Malawi, Pakistan und Bangladesh zum Einsatz kommt, erkennt anhand eines einfachen Röntgenbilds der Lunge, ob eine Person infiziert ist und einer weiteren Diagnostik bedarf oder nicht. Das spart oft Ressourcen und erlaubt eine rasche Therapie.

In eine ähnliche Richtung weist auch eine Studie mit rund einer halben Million pädiatrischer Patienten im chinesischen Guangzhou. Danach kann ein KI-basiertes Computerprogramm aus elektronisch gespeicherten Notizen und Symptombeschreibungen zahlreiche Diagnosen wie Asthma, Windpocken oder Meningitis zuverlässiger ableiten als jüngere Assistenzärzte.

Gerade in ressourcenarmen Regionen mit nur wenigen Medizinern sei eine solche Software nützlich, um behandlungsbedürftige Kinder schneller als bisher zu identifizieren, argumentieren die Studienautoren. Dabei ahme das selbstlernende System die ärztliche Logik nach, indem es etwa, von einem Hauptsymptom ausgehend, schrittweise die wichtigen klinischen Informationen aus den Beschreibungen und Befunden extrahiere.

Genau diese Strategie, mit künstlicher Intelligenz klinisches Denken zu imitieren, hält auch Andreas Boss für einen Schlüssel zum Erfolg. «Am besten ist es, wenn KI unsere Entscheidungswege nachbaut», sagt er. Dass sich dabei auch Konkurrenzsituationen zwischen Ärzten und Algorithmen ergeben, liegt in der Natur der Sache.

So komme es in seiner Abteilung gelegentlich vor, erklärt Boss, dass die KI-basierte Brustdichteanalyse eine Ultraschalluntersuchung unnötig erscheinen lasse, der Arzt beim Betrachten der Mammografie-Bilder aber einen anderen Eindruck gewinne und bei der Assistentin den fehlenden Scan nachbestelle. «Dann schauen wir uns die Bilder im Kollegenkreis noch einmal an, um den Befund zu überprüfen», erklärt der Radiologe. Bisher habe der Algorithmus immer richtig gelegen.";https://www.nzz.ch/wissenschaft/ki-in-der-medizin-hilfe-bei-einfachen-und-repetitiven-aufgaben-ld.1497525;NZZ;Martin Lindner;;;
10.02.2017;Fokus auf Medizin und Datenwissenschaft;"Zur Halbzeit ihrer ersten vierjährigen Amtszeit hat die Schulleitung der ETH Zürich am Donnerstag einen Ausblick auf die Strategieziele der kommenden Jahre gegeben. Dass die Medienkonferenz auf den 9. Februar gelegt wurde, war keineswegs Zufall. Auf den Tag genau drei Jahre nach dem Ja des Volks zur Masseneinwanderungsinitiative erklärte ETH-Präsident Lino Guzzella, dass nach einer langen Zeit der Verunsicherung die Gefahr des Ausschlusses der Schweizer Wissenschaftsgemeinschaft aus dem Europäischen Forschungsrahmenprogramm Horizon 2020 nun endlich abgewendet worden sei.
Bereits heute Kernkompetenz

Guzzella dankte dem Parlament, dem Bundesrat und insbesondere Bildungsminister Johann Schneider-Ammann dafür, dass sie die Rückkehr der Schweiz in den europäischen Forschungsraum möglich gemacht hätten. Denn um den im Begriff «Eidgenössisch» im Namen der Hochschule definierten Bildungsauftrag bestmöglich zu erfüllen, sei es wichtig, dass die ETH Zürich ihre Position unter den zehn besten Hochschulen der Welt behaupten könne. Dass viele «Bildungsausländer» nach einem höheren akademischen Abschluss in Zürich weiterhin in der Schweiz arbeiteten, sei gleichfalls ein Nutzen für unser Land.

Die ETH Zürich legt ihre strategischen Stossrichtungen der nächsten Jahre auf die Datenwissenschaft und die Medizin. Erstere ist laut Guzzella ein Schlüssel zum wissenschaftlichen Fortschritt. Insbesondere bei der Informationssicherheit sei die ETH führend. Sie werde mit ihrer Fachkompetenz wesentlich zur Umsetzung der bundesrätlichen Strategie «Digitale Schweiz» beitragen.

Die Medizin sei schon heute eine Kernkompetenz seiner Hochschule, betonte Guzzella. Ingenieurs- und Computerwissenschaften revolutionierten den medizinischen Alltag, und rund ein Drittel der Professorinnen und Professoren befasse sich mittlerweile direkt oder indirekt mit medizinischer Forschung.

Das bisher spektakulärste Projekt im Bereich Medizinaltechnologie war der Cybathlon 2016, eine Weltpremiere mit Ausstrahlung weit über die Schweiz hinaus. Der Wettkampf habe gezeigt, wie Menschen mit Behinderungen dank technischen Assistenzsystemen den Alltag besser meistern könnten, betont Guzzella und nennt den Cybathlon einen Treiber neuartiger und alltagstauglicher Assistenz-Technologien. 2020 soll er wieder durchgeführt werden.
Numerus clausus an der ETH

Um die Forschung besser mit jener der Universitäten zu verzahnen, bietet die ETH ab Herbst 2017 als Pilotprojekt einen Bachelorstudiengang der Humanmedizin mit 100 Plätzen an. Er soll laut Rektorin Sarah Springman als komplementäres Angebot medizinische Inhalte mit Naturwissenschaften und technischen Wissenschaften verbinden.

Nach dem Bachelor werden die Absolventen an die medizinischen Fakultäten der Universitäten Zürich, Basel und Lugano wechseln. Deshalb müssen sie, wie alle anderen Kandidaten für ein Medizinstudium auch, die Numerus-clausus-Prüfung bestehen. Dies ist ein Novum an der ETH, für welches das ETH-Gesetz eigens angepasst wurde. Springman betonte, dass es bei dieser einen Ausnahme bleiben werde. Das Schweizer Bildungssystem sei so gut, dass für die ETH-Zulassung keine zusätzlichen Prüfungen notwendig seien.";https://www.nzz.ch/zuerich/eth-zuerich-fokus-auf-medizin-und-datenwissenschaft-ld.144746;NZZ;Alois Feusi;;;
06.02.2017;Aus Datensumpf soll Datensee werden ;"Die beiden Eidgenössischen Hochschulen sind so etwas wie rivalisierende Schwestern. Sowohl in Zürich wie auch in Lausanne leisten Forschende und Studierende Grossartiges, nicht zuletzt immer wieder herausgefordert durch die Konkurrenz aus dem eigenen Haus. Umso bemerkenswerter ist es, dass die ETH Zürich und die École Polytechnique Fédérale de Lausanne bei einer für die Zukunft zentralen Herausforderung zusammenarbeiten: den Datenwissenschaften. Am Montag eröffneten sie das Swiss Data Science Center (SDSC) gewissermassen auf neutralem Terrain, in den altehrwürdigen Gemäuern des Schlosses Hünigen in Konolfingen (Bern).  Erfahrung bei IBM gesammelt

Das nationale Zentrum für Datenwissenschaft soll mithelfen, die Kluft zwischen den Datenanbietern, Informatikern und Wissenschaftern aus den verschiedenen Disziplinen zu überbrücken. Oder wie es Olivier Verscheure, der Leiter des neugegründeten Zentrums, auf den Punkt bringt: «Wir wollen einen Datensee, nicht einen Datensumpf.» Der gebürtige Belgier kehrt gewissermassen zu seinen Wurzeln zurück, hat Verscheure doch an der EPFL doktoriert. Anschliessend war er während einiger Jahre in der Forschungsabteilung von IBM tätig.

Angesichts der wachsenden Flut von Informationen in sämtlichen wissenschaftlichen Disziplinen werde die Herausforderung immer grösser, aus den gesammelten Daten relevante Erkenntnisse zu gewinnen, ist Verscheure überzeugt. Um von der rasanten Entwicklung nicht abgehängt zu werden, braucht es das spezifische Know-how von Datenspezialisten. «Das nationale Zentrum führt diese zusammen und bietet eine interdisziplinäre Plattform, die auch der Ausbildung und dem Wissenstransfer zugutekommt», betont Lino Guzzella, Präsident der ETH Zürich. Ein besonderes Augenmerk richten die Forschenden des SDSC mit Standorten in Zürich und Lausanne auf die Gebiete personalisierte Medizin, Umweltwissenschaften und Fertigungstechnologien. «Erkenntnisfabrik» für die Wissenschaft

Im Swiss Data Science Center steht kein superschneller Computer mit hoher Rechenleistung, vielmehr wollen die Forscher eine neuartige in der Cloud gehostete Plattform für Analysen unterschiedlichster Art entwickeln. In dieser sogenannten Erkenntnisfabrik werden geordnete, kalibrierte und anonymisierte Daten aufbewahrt, erforscht und analysiert. So können beispielsweise Wissenschafter des Wasserforschungsinstituts Eawag auf Messdaten zugreifen, die von anderen Schweizer Universitäten erhoben wurden.

Der ETH-Rat hat die Datenwissenschaft zu einem strategischen Forschungsbereich für die Jahre 2017 bis 2020 erklärt. Er lässt sich die Big-Data-Offensive denn auch einiges kosten. Gemäss Sprecherin Floriane Jacquemet sind in den kommenden vier Jahren Investitionen von insgesamt 30 Millionen Franken ins Swiss Data Science Center vorgesehen. In Lausanne und Zürich wird ein multidisziplinäres Team bestehend aus 30 bis 40 Informatikern und Datenwissenschaftern sowie weiteren Experten aus ausgewählten Wissenschaftsgebieten tätig sein. Bereits ab September dieses Jahres werden Masterstudiengänge in Datenwissenschaften an der EPFL und der ETH Zürich angeboten. ";https://www.nzz.ch/schweiz/nationales-zentrum-fuer-datenwissenschaft-aus-datensumpf-soll-datensee-werden-ld.143922;NZZ;Eric Aschwanden;;;
