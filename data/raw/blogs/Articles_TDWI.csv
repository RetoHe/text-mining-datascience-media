Datum;Titel;Text;Link;Quelle;Autor
26. Aug 20;Was ist dein Analytics Trend im Sommer 2020?;Melina Hofstetter:, Als Analytics Trend sehe ich zurzeit insbesondere dieDemokratisierung von Daten. Ich bin überzeugt, dass Daten und die Möglichkeit zu deren Analyse nicht nur für technisch versierte Experten möglich sein soll, sondern für jedermann in einer Organisation., Raphael Branger:, Die DWH Automation gewinnt weiter an Stellenwert. Während bisher die Code-Generation, z.B. in Form von SQL oder PowerShell, im Vordergrund stand, werden vermehrt ganze ETL Patterns stärker standardisiert und automatisiert werden., Julia Mehrtens:, Datengetriebene Kultur (data-driven culture): Damitdie Mitarbeitenden Daten für zielorientierte strategische und operativeEntscheidungen nutzen, brauchen Unternehmen einen datengetriebenen Handlungsrahmen. Ziel ist es, eine datengetriebene Kultur zu fördern, welche die Nutzer befähigt, mit Daten sicher umzugehen, um daraus Mehrwert zu generieren und somit die Unternehmensziele zu erreichen., Matthew Brandt:, Rückbesinnung auf die Basics: Unternehmen versuchen eher Ihre Prozesse strammzuziehen, anstatt sich mit neuen Technologien in unbekannte Gefilde zu begeben. Daher werden Skills von klassischen Data Engineers und Analysten wichtiger denn je., Julian Ereth:, „Daten für Alle“ – Unter dem Begriff „Daten Demokratisierung“ sehe ich die nächste Stufe der Self-Service-Bewegung kommen. Konkret geht es darum Organisationen zu befähigen, Daten an allen Stellen effizient einzusetzen. Dazu gehören kulturelle Elemente, wie bspw. das Thema „Data Literacy“, also die Fähigkeit kompetent mit Daten zu arbeiten, als auch technologische Komponenten, wie Daten Kataloge, die die Zugänglichkeit von Daten erhöhen., Allgemeine Fragen zum Blog, zum Verein und zum Thema AutorIn werden:Sandra SteingrubeCommunity Managersandra.steingrube(at)tdwi.eu, Fragen zum Inhalt:Leif HitzschkeTDWI e.V. Vorstandsmitgliedleif.hitzschke(at)tdwi.eu, TDWI e.V.Lindlaustraße 2cD-53842 TroisdorfTel: +49 (0)2241/2341-212, ImpressumDatenschutz;https://blog.tdwi.eu/was-ist-dein-analytics-trend-im-sommer-2020/;TDWI;Young Guns
19. Aug 20;TDWI Young Guns Mailing August 2020 | Interaktive Datenanalyse mit wenig Ressourcenverbrauch;Finnist seit Anfang des Jahres in Kontakt mit dem TDWI und richtet, sobald wieder möglich, ein BarCamp in seinem Studienort Siegen aus. Parallel zu seinem BWL-Studium ist er als studentischer Berater tätig und begeistert sich für intelligente Datenanalyse, gerne in Bezug auf Marketing- und Strategieentscheidungen., Saskiahat im vergangenen Jahr ihr Wirtschaftsinformatikstudium mit Data Science Schwerpunkt abgeschlossen und im Anschluss daran das Data Science Traineeprogramm bei der INFOMOTION GmbH absolviert. In ihrem aktuellen Kundenprojekt beschäftigt sie sich mit der intelligenten Analyse von Kundendaten und entwickelt dafür automatisierte Workflows in KNIME., Während es Großkonzernen aufgrund ihrer Marktposition und Größe leichter fällt, neue Abteilungen und Zuständigkeiten für Big Data und KI zu etablieren oder entsprechende Budgets freizugeben, fehlt es im Mittelstand oft an den dafür erforderlichen Ressourcen. So sind dreistellige Millionenbeträge für Data-Projekte bei DAX-Konzernen keine Seltenheit. Bei Mittelständlern hingegen ist bereits der Schritt zur externen Beratung eine Hürde – entsprechende Fachkräfte für den eigenen Betrieb zu finden sowieso schwierig. Der nächste Schritt bleibt somit aus und wird mit an ihre Grenzen stoßenden Excel-Tabellen kompensiert. Um nicht abgehängt zu werden, braucht es daher Lösungen, die keine zu großen Einstiegshürden bedeuten., Als derartige Lösungen eignen sich insbesondere interaktive Softwareprodukte, welche intuitiv und ohne Programmierkenntnisse nutzbar sind. Zu nennen ist hier vor allem das spannende Tool KNIME, vergleichbar mit dem IBM SPSS Modeler. KNIME wurde entwickelt von der Uni Konstanz und bietet neben der einfachen Bedienung auch eine große, hilfsbereite Community., Prozesse werden in KNIME als Workflow grafisch dargestellt und daher gleich dokumentiert. Ein Workflow besteht aus mehreren Bausteinen, hinter denen Befehle in verschiedenen Programmiersprachen liegen, was unerfahrenen Nutzern zu Gute kommt. Sie sind in einer Liste sortiert und über ihre Funktion leicht auffindbar, auch ohne sie vorher zu kennen. Die Bausteine sind außerdem nach Kategorien farblich markiert, so dass leicht ersichtlich ist, in welchem Bereich des Workflows Daten aus verschiedenen Datenbanken oder Quellen eingelesen, aufbereitet oder zu einer Graphik verarbeitet werden. Erfahrene Nutzer, die bereits eigene, erprobte Skripte geschrieben haben, können diese jedoch genauso gut hinter einem eigenen Baustein ablegen und verwenden., Diese kostenfreien Software-Lösungen zur interaktiven Datenanalyse, die sich auch ohne Vorkenntnisse verwenden, oder zumindest nach einmaliger Einführung entsprechend bedienen lassen, können außerdem auch ohne Anfangsinvestitionen vollumfänglich genutzt werden. Grafische Benutzeroberflächen erlauben eine Bedienung ohne Beherrschen von Programmiersprachen. Anwendungsbeispiele sind aus eigener Erfahrung das einmalige Automatisieren von wiederkehrenden ETL-Prozessen, aufwendige Analysen und Reporting, die Auswertung von Kundenverhalten oder Vorhersagemodellen. Auch von repetitiven Aufgaben, wie dem Versenden von Messdaten an Kunden aus verschiedenen Datenquellen, ließen sich vereinfachen, sodass mehr Zeit für andere Aufgaben bleibt. Ein gutes Beispiel in dem Kontext ist ein Kunde, der Informationen ans Bundesstatistikamt übermitteln musste und jeden Monat Daten aus etlichen Excel-Dateien kopiert und in eine große eingefügt hat. Dank eines Workflows kann er sich die Zeit nun sparen ihn bei Bedarf intuitiv anpassen., Mit der Zeit und steigender Erfahrung des Benutzers ist in KNIME neben den Möglichkeiten des Data-Mining vor allem auch der Einsatz von Machine Learning möglich und bietet so eine gute Basis für weitere Entwicklungen in Unternehmen., Was meint ihr? Beteiligt euch jetzt an der Diskussion beiLinkedIn,Slack oder direkt hier im Blog., Mein Highlight bei den Young Guns lässt sich gar nicht auf ein bestimmtes Event beziehen, sondern es ist vielmehr der Umgang miteinander. Ganz gleich welchen beruflichen Hintergrund oder akademischen Grad man besitzt, es wird über alle Ebenen hinweg auf Augenhöhe kommuniziert. Ich habe mich von Anfang an super wohl gefühlt und glaube, dass die Young Guns-Kultur jedem einen leichten Einstieg, Teamarbeit und vor allem gemeinsame Produktivität fördert., Allgemeine Fragen zum Blog, zum Verein und zum Thema AutorIn werden:Sandra SteingrubeCommunity Managersandra.steingrube(at)tdwi.eu, Fragen zum Inhalt:Leif HitzschkeTDWI e.V. Vorstandsmitgliedleif.hitzschke(at)tdwi.eu, TDWI e.V.Lindlaustraße 2cD-53842 TroisdorfTel: +49 (0)2241/2341-212, ImpressumDatenschutz;https://blog.tdwi.eu/tdwi-young-guns-mailing-august-2020-interaktive-datenanalyse-mit-wenig-ressourcenverbrauch/;TDWI;Finn Klophaus
12. Aug 20;Data Lake – eine Bestandsaufnahme in drei Akten: Organisation und Governance (3);"Die Diskussion um das Thema Data Lake ist oft sehr technologiegetrieben. Dies zeigt schon alleine die schiere Anzahl an Implementierungsansätzen und die endlose Produktlandschaft. Sobald ein Data Lake das Proof-Of-Concept-Stadium verlässt und erwachsen wird, kommt allerdings oft die Einsicht, dass zu einer Integration in eine Enterprise-Landschaft mehr als nur ein gutes technisches Fundament gehört. Insbesondere organisatorische, rechtliche und regulatorische Faktoren werden oft sträflich vernachlässigt, was sich bei einer Eingliederung in existierende Strukturen sowie im produktiven Alltagsbetrieb schnell rächt., Dieser Artikel stellt den dritten und damit letzten Teil einer Serie zur Bestandsaufnahme von Data Lakes in der Praxis dar und befasst sich mit den Bereichen Organisation und Governance. Ziel ist es hierbei Punkte aufzuzeigen, die es bei einer Umsetzung eines organisatorischen Rahmenkonzepts für einen Data Lake zu beachten gilt und Denkanstöße für praxistaugliche Lösungen zu geben. Die Diskussion teilt sich hierbei, wie Abbildung 1 zeigt, in externe Faktoren, wie bspw. Datenschutz und andere rechtliche Vorgaben, auf die Unternehmen selbst meist keinen direkten Einfluss haben und interne Faktoren, wie die Integration in eine Aufbau- und Ablauforganisation und die Etablierung einer passenden Data Governance., Die zunehmend wichtige Rolle von Daten im Alltag geht auch mit einer stärkeren Regulierung zur Sicherstellung einer rechtskonformen Speicherung und Auswertung einher. Sei es die Europäische Datenschutz-Grundverordnung (DSGVO) oder das vor Kurzem in Kraft getretene brasilianische Äquivalent LGPD. Die Einhaltung der Vorgaben stellt unter Anderem spezielle Anforderungen and die Organisation und die Prozesse innerhalb eines Data Lakes. Im Folgenden werden einige dieser Aspekte erläutert., Die Idee der Datensparsamkeit ist es nur so viele Daten zu sammeln, wie unbedingt notwendig. Dieses Konzept scheint auf den ersten Blick konträr zu der Idee eins Data Lakes, da hier meist eher das Mantra „erstmal alles speichern“ gilt. Dieser Widerspruch relativiert sich allerdings, da ein Großteil der Vorgaben sich nur auf personenbezogene Daten beziehen., Zur Einhaltung der Vorgaben bei personenbezogenen Daten, können verschiedene Methoden zum Einsatz kommen, wie bspw. eine Datenarchitektur, die eine Trennung von den eigentlichen und den zugehörigen Identitäten ermöglicht oder die personenbezogenen Daten ausreichend anonymisiert. Am effektivsten ist hierbei aber natürlich eine direkte Vermeidung der Aufzeichnung von personenbeziehbarer Daten. Damit wird auch eine unabsichtliche Speicherung und Verarbeitung direkt ausgeschlossen., Wenn es an die Auswertung der Daten geht kann man zwei Vorgehensmodelle unterscheiden. Bei der Ersten werden die Daten zuerst anonymisiert und dann ausgewertet (Anonymize-then-Mine). Damit stehen zum Zeitpunkt der Auswertung schon keine personenbezogenen Daten zur Verfügung und somitkönnen auch Drittparteien die Auswertungen problemlos durchführen. Allerdings geht dieser Ansatz auch mit einem Informationsverlust einher, da sich viele interessante Erkenntnisse oft erst durch den Personenbezug ergeben. Eine Auswertung der Originaldaten und eine anschließende Anonymisierung der Ergebnisse (Mine-then-Anonymize) bietet daher oft mehr Möglichkeiten zur Erstellung genauerer Modelle und Auswertungen. Allerdings muss dabei sichergestellt werden, dass die auswertende Partei die entsprechenden Berechtigungen besitzt bzw. Dateneigentümer ist., Ein Data Lake beinhaltet oft Daten aus zahlreichen verschiedenen Unternehmensbereichen und hat damit eine größere Bandbreite als ein traditionelles Data Warehouse, welches oft eher im Controlling und Finance-Bereich angesiedelt ist. Entsprechend vielfältig sind natürlich die Stakeholder, die mit einem Data Lake interagieren. Diese Gemengelage spiegelt sich auch in der Aufbau- und Ablauforganisation eine Data Lakes wieder., Um passende Organisationsstrukturen um einen Data Lake herum aufzubauen, sollte zuerst die Frage nach den Stakeholdern geklärt werden. Wie Abbildung 2 zeigt, kann hierbei sowohl eine fachliche Perspektive (Welche Geschäftsbereiche/Abteilungen sind betroffen?) als auch die funktionale Zuordnung (Wer verantwortet welche Aufgaben?) zum Einsatz kommen. Durch die Bandbreite sind diese Betrachtungen nicht immer trennscharf. Traditionell trennen sich die Aufgaben in die Entwicklung und den Betrieb eines Data Lakes, die eigentliche Auswertung von Daten (Analytics &amp; Data Science) sowie regulatorische Aufgaben (Governance &amp; Qualität). Fachliche Sta-keholder sind hierbei meist die IT-Abteilung sowie verschiedenste Geschäftsbereiche. Zudem haben natürlich auch andere datennahe Abteilungen, wie bspw. ein BICC oder die Rechtsabteilung ein gewisses Interesse an einem Data Lake., Entsprechend der Anzahl an Stakeholdern und deren breite organisatorischen Verteilung, bieten sich föderierte Organisationswerkzeuge, wie etwa ein interdisziplinäres Governance Gremium an. Die Aufgabe eines solchen Gremiums ist es technologische Standards, Vorgaben für die Datenqualität sowie einen Rahmen für einen systematischen Betrieb und eine sinnvolle Weiterentwicklung zu definieren. Ein solches Gremium besteht dabei meist aus einer virtuellen Organisationsstruktur mit Vertretern aus den verschiedenen Interessengruppen., Das fachliche Äquivalent des Governance Gremiums ist ein Center of Excellence, welches aus einem interdisziplinären Team von Expertenn besteht, welches die verschiedenen Stakeholder bei spezifischen Aufgaben, wie etwa dem On-Boarding neuer Datenquellen, dem Durchführen anspruchsvollerer Analyse-Vorhaben oder sonstigen Fragestellung, unterstützt. Je nach Anforderungen und Umfang der Aufgaben, kann es sich hierbei um eine feste Organisationseinheit oder eine virtuelle Struktur handeln., Neben der Betrachtung organisatorischer Strukturen zur Eingliederung eines Data Lakes in ein Unternehmen, spielen auch die Prozesse für einen systematischen Umgang mit Daten innerhalb eines Data Lakes eine wichtige Rolle. Die unten aufgeführten Punkte werden hierbei meist unter dem Begriff der Data Governance diskutiert., Eines der größten Risiken eines Data Lakes ist, dass dieser sich zu einem unübersichtlichen Data Swamp (Datensumpf) entwickelt. Dies geschieht insbesondere durch nicht-vorhandende oder nicht-standardisierte Prozesse zum Umgang mit den Daten und der naiven Idee des „store now, think later“-Ansatzes. Abhilfe schafft ein klar definierter Datenlebenszyklus, wie ihn Abbildung 3 in vereinfachter Form zeigt. Insbesondere die Prozesse der Beschaffung und Qualitätssicherung sollten dabei definiert werden. Hilfreich sind zudem eine klare Verteilung von Verantwortungen (bspw. mittels Data Stewards) sowie die Vorgabe konkreter Qualitäts- und Dokumentationsstandards. Um einen maximalen Wertbeitrag zu erreichen, sollte der Datenlebenszyklus zudem auch Vorgaben für Analyseprozesse sowie für das Rückspielen von Erkenntnissen und Feedback nach einem Einsatz im Feld umfassen., Ein häufiges Problem ist es, dass sich für bestimmte Datensätze niemand verantwortlich fühlt oder der eigentlich fachlich Verantwortliche kein eigenes Interesse an der Pflege der Daten hat. Ein Lösungsansatz ist es explizite Datenverantwortliche (Data Stewards) zu benennen, die fachliche Fragen zu den Daten beantworten können und auch Aussagen zu Qualität und Richtigkeit machen können. Von Vorteil ist es zudem, wenn eine gewisse Affinität zur Datenanalyse besteht, um auch nachgeschaltete Prozesse bestmöglich zu unterstützen. Bei der Idee von Data Stewards ist es zudem entscheidend, die Pflichten und Aufgaben konkret zu benennen und auch entsprechende Anreize oder Sanktionen zur Erfüllung dieser zu etablieren., Das Schmiermittel einer funktionierenden Data Governance und eines Data Lakes im Generellen sind Metadaten –  also „Daten über Daten“. Dies sind bspw. Informationen zu deren Ursprung, den Qualitätsgrad oder eine fachliche und zeitliche Einordnung. Viele dieser Daten können automatisch erfasst und dokumentiert werden. Ein großer Mehrwert liegt allerdings meist in den impliziten Metadaten, die oft nur in den Köpfen der Mitarbeiter vorhanden sind, wie bspw. die Bedeutung einzelner Kennzahlen für Geschäftsentscheidungen. Hier sollte ein entsprechendes Metadatenkon-zept erarbeitet werden, um diese Daten zu explizieren und bei weiteren Analysen zu berücksichtigen. Vielversprechende Ansätze gibt es hier in Kombination mit sogenannten Datenkatalogen, die schon im letzten Artikel kurz thematisiert wurden., Es zeigt sich, dass ein Data Lakes zur vollen Entfaltung seines Potentials mehr als nur eine gute technische Lösung sein muss. Ein sinnvolles organisatorische Rahmenwerk umfasst Maßnahmen zur Einhaltung und Dokumentation von regulatorischen Vorgaben, ein durchdachtes Konzept zur organisatorischen und prozessualen Eingliederung in ein Unternehmen sowie die Etablierung und Durchsetzung einer passenden Data Governance., Die Berücksichtigung dieser Faktoren stellt eine konsequente Ausrichtung am Business sicher, ermöglicht einen effizienten Langzeitbetrieb und ist damit das beste Mittel zu verhindern, dass aus einem klaren Data Lake ein trüber Data Swamp wird., Mit diesem Artikel zu „Organisation &amp; Governance“ schließt die Serie „Data Lake – eine Be-standsaufnahme in drei Akten“. Lesen Sie auch die anderen Beiträge zum Thema „Begriff und Motivation“ sowie „Architektonische Fragestellungen“., 1 TDWI Seminar “Data-Lake-Ansätze und Best Practices”, https://www.tdwi.eu/akademie/seminarsuche/seminardetails/seminar-titel/data-lake-ansaetze-und-best-practices.html, 2 TDWI E-Book: “Der Data Lake als zentrales Element in Analytics-Architekturen“, https://www.tdwi.eu/wissen/studien-buecher/e-books/wissen-titel/tdwi-e-book-der-data-lake-als-zentrales-element-in-analytics-architekturen.html, 3 TDWI E-Book: “Data Governance – Betriebliche Daten als Wirtschaftsgüter verstehen und behandeln“, https://www.tdwi.eu/wissen/studien-buecher/e-books/wissen-titel/tdwi-e-book-data-governance.html, * Der Beitrag spiegelt die Meinung des Autors wider und ist keine allgemeingültige Meinung des TDWI. Als TDWI bieten wir die Plattform, alle Themen und Sichtweisen zu diskutieren. *, Allgemeine Fragen zum Blog, zum Verein und zum Thema AutorIn werden:Sandra SteingrubeCommunity Managersandra.steingrube(at)tdwi.eu, Fragen zum Inhalt:Leif HitzschkeTDWI e.V. Vorstandsmitgliedleif.hitzschke(at)tdwi.eu, TDWI e.V.Lindlaustraße 2cD-53842 TroisdorfTel: +49 (0)2241/2341-212, ImpressumDatenschutz";https://blog.tdwi.eu/data-lake-eine-bestandsaufnahme-in-drei-akten-organisation-und-governance/;TDWI;Julian Ereth
29. Jul 20;Inside TDWI – Blog, Podcast, Webinare;Sehr geehrte Damen und Herren, Ladies and Gentlemen,, es ist soweit, hiermit begrüßen wir den 40. Beitrag in diesem Blog. Vielen Dank an die vielen Autoren, die Ihre Zeit investiert haben, um uns Ihre Erfahrungen und Ihr Wissen auf diesem Wege zu vermitteln. Das ist keine Selbstverständlichkeit, denn für einen Blogbeitrag muss man eine ganze Menge vorbereiten. Aber es lohnt, denn als Autor lernt man selbst nochmal eine ganze Menge dazu, wenn man einen Artikel schreibt und diesen veröffentlicht. Insofern handelt es sich um eine WIN-WIN-Situation und ich kann nur jeden ermutigen, sich mal an einem Blogbeitrag zu versuchen. Wir stehen Ihnen auch hilfreich zu Seite und werden gemeinsam einen spannenden und lesenswerten Artikel zusammen erstellen. , Wo Licht ist, da ist auch Schatten. Ja, auch uns hat Corona das Leben nicht einfacher gemacht und so warten Sie sicher schon auf den nächsten Podcast. Aktuell sind wir dabei neue, spannende Podcasts aufzunehmen und für Ihre Ohren aufzubereiten. Seien Sie gespannt oder auch sehr gerne als Interviewgast dabei. , Und hier gilt, wie immer:  Alle Themen rund um Analytics interessieren uns. Es muss nicht immer um den nächsten hippen Algorithmus gehen. Alles ist erlaubt. (Also fast alles. Sie wissen schon.) Wir können auch mal über Agilität in der Business Intelligence sprechen. Warum klappt dieses ganze Kanban/ Scrum-Ding in der BI irgendwie nicht richtig? Wie werde ich eigentlich ein Business Analyst? Kann man Tableau wirklich so schnell lernen? Der Phantasie sind keine Grenzen gesetzt. Schreiben Sie mir gerne, wenn es ein Thema gibt, über das Sie gerne mehr erfahren möchten., Genau das ist das Besondere an diesem Verein. Wir wollen den Wissensaustausch innerhalb der Analytics-Community vorantreiben. Wir wollen nichts verkaufen. Wir machen das, damit unsere Mitglieder sich stetig weiterentwickeln., Also zögern Sie nicht, wenn Ihnen beim Lesen dieses Artikels irgendeine Idee gekommen ist. Meine E-Mailadresse finden Sie rechts unter meinem Foto. , Genießen Sie die Zeit und bleiben Sie gesund., Ihr Leif Hitzschke, Allgemeine Fragen zum Blog, zum Verein und zum Thema AutorIn werden:Sandra SteingrubeCommunity Managersandra.steingrube(at)tdwi.eu, Fragen zum Inhalt:Leif HitzschkeTDWI e.V. Vorstandsmitgliedleif.hitzschke(at)tdwi.eu, TDWI e.V.Lindlaustraße 2cD-53842 TroisdorfTel: +49 (0)2241/2341-212, ImpressumDatenschutz;https://blog.tdwi.eu/inside-tdwi-blog-podcast-webinare/;TDWI;Leif Hitzschke
22. Jul 20;Rückblick auf die TDWI Virtual am 23. und 24. Juni;Diese Woche haben wir einen Blogbeitrag in etwas anderer Form, den wir mit Ihnen teilen möchten. Entdecken Sie Eindrücke der TDWI Virtual – die kostenfreie Online-Konferenz für Data, BI und Analytics, die erstmals am 23. und 24. Juni 2020 statt fand., Wir freuen uns über eine gelungene und erfolgreiche Veranstaltung und hoffen, dass wir Sie auch bei der nächsten TDWI Konferenz wieder begrüßen dürfen., Bleiben Sie über die TDWI Virtual und den TDWI informiert, indem Sie unseren Newsletter abonnieren., , Allgemeine Fragen zum Blog, zum Verein und zum Thema AutorIn werden:Sandra SteingrubeCommunity Managersandra.steingrube(at)tdwi.eu, Fragen zum Inhalt:Leif HitzschkeTDWI e.V. Vorstandsmitgliedleif.hitzschke(at)tdwi.eu, TDWI e.V.Lindlaustraße 2cD-53842 TroisdorfTel: +49 (0)2241/2341-212, ImpressumDatenschutz;https://blog.tdwi.eu/rueckblick-auf-die-tdwi-virtual/;TDWI;Sandra Steingrube
15. Jul 20;Auswertung der Umfrage des TDWI e.V. zum Thema: „Digitalisierung in Zeiten von Corona“;Das Homeoffice an sich und die aktuelle Arbeitssituation werden überwiegend sehr positiv wahrgenommen und das ist nach Angaben der TeilnehmerInnen sogar besser als erwartet. Kritisch werden die Möglichkeiten im Bereich der Akquise, die weiterhin bestehenden Fixkosten trotz sinkender oder gar nicht existenter Einnahmen, der steigende Aufwand mit der Kommunikation und die fehlende informelle Information sowie zum Teil nur mangelhafte Qualität der Netzwerke gesehen., 95% der Teilnehmer rechnen nicht mit einem abnehmenden Kundenvertrauen, 94% rechnen nicht mit einer Einschränkung der angebotenen Produkte oder Services. 67% der TeilnehmerInnen rechnen mit kurzfristig oder langfristig negativen Auswirkungen der Corona-Krise auf die Geschäftstätigkeit. Lediglich 22% rechnen mit positiven Konsequenzen. Die Unternehmen haben neben dem Entschluss zum Home-Office folgende Maßnahmen einzeln oder in Kombination ergriffen: Kommunikationsstrategie (60%), Erweiterung der digitalen Servicekanäle zum Kunden (39%), Erweiterung der digitalen Sales Kanäle (29%), Erstellung eines Cashflow-Notfallplans (22%), größerer Fokus auf Automation und KI (15%), Verbesserung der Wertschöpfungskette (8%). Bei dem Blick in die Zukunft ist die sich zum Teil schwierige wirtschaftliche Lage spürbar und Maßnahmen wie der Abbau der festen Büroräume unterstreichen dies., Das Bundesamts für Sicherheit für Informationstechnik warnt, dass Cyberkriminelle die Corona-Pandemie ausnutzen. Auch die Unternehmen nehmen das Thema Cyberrisk ernst und führen zu 59% Datenschutzbelehrungen durch. Für den Zugriff nutzen hierbei 86% einen VPN Client. Bezüglich der Verteilung der Arbeitsergebnisse, also Kennzahlen und Daten, geben 37% der TeilnehmerInnen an auch die Publish-Funktion der Software zu nutzen, was für eine bruchfreie Digitalisierung der betreffenden Prozesse spricht. Weitere Wege sind E-Mail, zentrale Speicherung, aber auch noch postalischer Versand., Digitalisierungsschub bestätigt!Viele TeilnehmerInnen geben an, dass es zu mehr Digitalisierung und Automatisierung kommen wird. Als Konkretisierung wurden die auch nach der Krise fortwährende Nutzung von: Homeoffice (52%), Virtuelle Meetings als Kommunikationsmittel, virtuelle Weiterbildungen, veränderte Prozesse und die virtuelle Zusammenarbeit genannt. Nur ein geringer Anteil der TeilnehmerInnen erwartet, dass es eine Rückkehr zur Geschäftspraxis vor Corona geben wird., Für Spontan-Entschlossene: Melden Sie sich noch kostenfrei für unser 1. TDWI-Vereins-Webinar heute, 15. Juli ab 17:30 Uhr an! Wir diskutieren gemeinsam mit Ihnen die Ergebnisse der Umfrage., Möchten Sie die komplette Auswertung lesen? Dann loggen Sie sich als Mitglied im Mitgliederbereich unter tdwi.eu ein. Dort können Sie den vollständigen Report lesen und downloaden. Haben Sie Fragen zur Mitgliedschaft oder zum Login? Dann melden Sie sich bei Tanja Kenda unter tanja.kenda(at)tdwi.eu., Allgemeine Fragen zum Blog, zum Verein und zum Thema AutorIn werden:Sandra SteingrubeCommunity Managersandra.steingrube(at)tdwi.eu, Fragen zum Inhalt:Leif HitzschkeTDWI e.V. Vorstandsmitgliedleif.hitzschke(at)tdwi.eu, TDWI e.V.Lindlaustraße 2cD-53842 TroisdorfTel: +49 (0)2241/2341-212, ImpressumDatenschutz;https://blog.tdwi.eu/auswertung-der-umfrage-des-tdwi-ev-digitalisierung-in-zeiten-von-corona/;TDWI;Claudia Koschtial
08. Jul 20;Can Machine Learning help to forecast COVID-19 infections – Part 1;Corona Virus – it has reached already 215 countries. Even remote locations such as islands and ships are affected (Worldometers.info 2020).Over 7.9 million people have been infected, 433,212 have died and 4.1 million recovered (14th of June). Every day more and more people are infected with COVID-19. These new infections have a very high impact and governments are binding their lockdown or loosening of these restrictions to this and other metrics., This article will describe how a data-driven prediction was created and how it is being updated regularly. The result is visualized in a dashboard – I encourage you to read the article first before studying the dashboard. The link will be in the article. , Please note: This article tries time-series algorithms on already present data, there will be no forward prediction. In my opinion, professional epidemiologists should make forward predictions., Another note: This project is far from being finished. Over time I plan to add more features, adjust algorithms, enhance data, etc. These changes will be reflected in the articles., This topic has been a lot within the news. I was inspired to do this by a Kaggle challenge: https://www.kaggle.com/c/covid19-global-forecasting-week-1. In this competition, the goal was not to create accurate forecasts but rather to identify the variables which contribute to the outbreak (Kaggle 2020)., In order to gain more knowledge on the field of time-series, I decided to create my own prediction project. It uses the following algorithms:, Many previous prediction models utilized the SIR-Model. This model divides people into (S)usceptible to infection, (I)nfected, or (R)emoved (recovery, immunity, quarantine or death) (Brockmann 2020)., The SIR model framework looks as follows (Sasaki 2020):,  is controlling how much the disease will be transmitted and  how much can be removed by death or recovery. Once people are healed, they are considered immune (in the model environment)., There are challenges with the data., I decided to try out the time-series algorithms on the data. Since it is based only on the confirmed infections, we should keep in mind that it faces the same challenges – highly affected by the amount of testing by country., It was for me very important to transform this project into an end-to-end machine-learning project. This means that the model and the dashboard are being updated regularly. For the visualization of the project I used Miro – the next gen free collaborative whiteboard platform., The board is available here. , The source data is being provided by Johns Hopkins University on Github. The repository is being updated every day. For the project, the time series confirmed infection data – global has been used (CSV). It comes in the following form: , There are countless discussions on Kaggle and other sites about which metric is the most suitable. It is important to use several metrics in order to evaluate the models., This framework is known as Supervised Learning. In Supervised Learning we know what we want to predict. In this case, the algorithm tries to predict the new infections from the data he has learned (Train). The algo uses some input values to predict the future value, in this example the future date features, days since outbreak and infections from 7 days ago. The predicted value will be measured against the Test set. The whole framework will be updated once per day. The notebook runs in Google Colab, automated via Selenium. The output is an Excel file which will be saved in Google Drive. The Dashboard pulls the Excel file once per day automatically., Click here., As a framework, I used Tableau Public. A very good choice if you would like to share dashboards with others via a web link., This brings me to the end of my article. Please comment or send me a message for any questions. In the upcoming articles, I want to provide more detail how (and why) the time-series algorithms are behaving during the prediction., You can find the code on Github or NBviewer., *Der Beitrag spiegelt die Meinung des Autors wider und ist keine allgemeingültige Meinung des TDWI. Als TDWI bieten wir die Plattform, alle Themen und Sichtweisen zu diskutieren.*, Brockmann, Dirk. 2020. The model. Accessed May 22, 2020. http://rocs.hu-berlin.de/corona/docs/forecast/model/., Cbap, Akhilendra Singh. 2002. Evaluation Metrics for Regression models- MAE Vs MSE Vs RMSE vs RMSLE. Accessed June 12, 2020. https://akhilendra.com/evaluation-metrics-regression-mae-mse-rmse-rmsle/., Gallagher, James. 2020. Coronavirus immunity: Can you catch it twice? Edited by BBC News. 28 April. Accessed May 22, 2020. https://www.bbc.com/news/health-52446965., Kaggle. 2020. COVID19 Global Forecasting (Week 1). March. Accessed May 22, 2020. https://www.kaggle.com/c/covid19-global-forecasting-week-1/overview., MDR THÜRINGEN/sar. 2020. So zählen wir die Corona-Fälle und Genesenen in Thüringen. 15 May. Accessed May 22, 2020. https://www.mdr.de/thueringen/coronavirus-covid-genesene-zaehlung-100.html., Sasaki, Kai. 2020. COVID-19 dynamics with SIR model. 11 March. Accessed May 22, 2020. https://www.lewuathe.com/covid-19-dynamics-with-sir-model.html., Worldometers.info. 2020. COVID-19 Coronavirus Pandemic. 14 June. Accessed June 27, 2020. https://www.worldometers.info/coronavirus/., Allgemeine Fragen zum Blog, zum Verein und zum Thema AutorIn werden:Sandra SteingrubeCommunity Managersandra.steingrube(at)tdwi.eu, Fragen zum Inhalt:Leif HitzschkeTDWI e.V. Vorstandsmitgliedleif.hitzschke(at)tdwi.eu, TDWI e.V.Lindlaustraße 2cD-53842 TroisdorfTel: +49 (0)2241/2341-212, ImpressumDatenschutz;https://blog.tdwi.eu/can-data-science-help-to-forecast-new-covid-19-infections/;TDWI;Armin Geisler
01. Jul 20;Ist das “Science” in Data Science noch zu retten?;Kritiker bezeichnen Data Scientists gerne mal als“Statistiker mit einem Mac”. Die Anforderungen in Jobangeboten für“Data Scientists” sind jedoch oft sehr umfangreich, von Technologien für die parallele Verarbeitung großer Datenmengen bis hin zu der Erstellung von Dashboards. Die Liste der gewünschten Skills scheint dabei beliebig lang werden zu können und geht über bloße Statistik weit hinaus. In vielen Jobangeboten wird Erfahrung mit wissenschaftlichen Methoden verlangt, idealerweise sogar verbunden mit einem Doktortitel. Dabei könnte man meinen, dass dies aktuell nicht mehr nötig ist., Data Scientists nutzen mittlerweile häufig Cloud-Dienste wie AWS und Azure. Sie werben damit, dass sie skalierbare Lösungen für typische Data Science Szenarien anbieten können. Ein klassischer Use Case ist die Analyse von Kundenanfragen bei einem für Endkunden produzierenden Unternehmen. Kundenanfragen kommen über E-Mails oder einen Chatbot und werden entweder mit Standardantworten für die häufigsten Fragen beantwortet oder an den Support weiterreicht. Hierfür kommt ein Klassifikationsmodell zum Einsatz, um die verschiedenen Fragen zu erkennen. Bislang entwickelten viele Data Scientists solche Modelle in Python Notebooks. Hierfür ist Erfahrung wichtig, wie aus Texten Features generiert werden und welche Modelle für die Klassifikation sinnvoll sind. Dieses Wissen muss in Programmcode übertragen werden. Als Ergebnis stehen oft mehrere hundert Zeilen Python-Code und viele Experimente, um alle Schritte zu optimieren. Der wissenschaftliche Teil des Data Science ist gesichert., Das Angebot der Cloud-Dienste geht mittlerweile jedoch über die Unterstützung einzelner Algorithmen hinaus und es ist möglich, mit einem Service ein Modell automatisiert zu entwickeln. Solche Systeme nennen sich AutoML. Sie automatisieren alle Prozessschritte bei der Entwicklung eines Modells auf Basis eigener Daten. Die Notwendigkeit von manuellem Feature Engineering und der Evaluation verschiedener Modelle entfällt. Der Cloud-Service evaluiert mehrere Algorithmen und optimiert jeweils die Parameter-Kombinationen. Die Laufzeit ist hierbei keine Hürde. Die Data Science-Aufgaben, die sonst Tage dauerten, geschehen nun, dank der Nutzung eines Clusters, innerhalb von Stunden. So sind auch Data Scientists vor Automatisierungsprozessen nicht sicher., Wenn ursprüngliche Aufgaben durch die Automatisierung wegfallen, welche Aufgaben werden Data Scientists übernehmen? Das Mapping der Kundenwünsche auf die Cloud Services, in Anbetracht der vorhandenen Kundendaten, wird zur zentralen Aufgabe. Das Verständnis der Algorithmen bleibt weiterhin von Bedeutung, aber lediglich, um Ergebnisse einordnen und Ressourcen besser einplanen zu können. Die wissenschaftliche Formulierung und Verifikation von Hypothesen werden durch die automatisierte Auswahl des besten Modells in den Hintergrund gedrängt., Ist also das“Science” in Data Science noch zu retten? Die Automatisierung schafft Raum, um wissenschaftliche Methoden auf eine grundsätzliche Fragestellung anzuwenden: Wie kann man mit Daten einen Mehrwert für das Unternehmen erzielen? Mit automatisch erstellten Modellen kann der Mehrwert schneller bewertet werden. Statt den Modellen werden Data Scientists Use Cases optimieren. Sie werden sich in naher Zukunft wohl eherSolution DevelopersoderAI Enabler nennen., In bestimmten Fällen bleibt die bisherige Kernaufgabe der Data Scientists dennoch erhalten. Sie entwickeln spezialisierte Algorithmen und Modelle in den Unternehmen, für die standardisierte Prozesse nicht ausreichend sind., Viele andere Unternehmen nutzen ihre vorhandenen Daten bisher allerdings nicht. Sie wünschen sich den Data Scientist alsSolution Developer,der ihnen die Möglichkeiten der Cloud Services aufzeigt und einen Mehrwert mit den Daten schafft. Zukünftige Jobangebote für Data Scientists werden also nicht nur wissenschaftliche Expertise fordern, sondern zunehmend auch Kommunikationsfähigkeit, um Unternehmen von den ausgewählten Data Science-Lösungen zu überzeugen., *Der Beitrag spiegelt die Meinung des Autors wider und ist keine allgemeingültige Meinung des TDWI. Als TDWI bieten wir die Plattform, alle Themen und Sichtweisen zu diskutieren.*, Allgemeine Fragen zum Blog, zum Verein und zum Thema AutorIn werden:Sandra SteingrubeCommunity Managersandra.steingrube(at)tdwi.eu, Fragen zum Inhalt:Leif HitzschkeTDWI e.V. Vorstandsmitgliedleif.hitzschke(at)tdwi.eu, TDWI e.V.Lindlaustraße 2cD-53842 TroisdorfTel: +49 (0)2241/2341-212, ImpressumDatenschutz;https://blog.tdwi.eu/ist-das-science-in-data-science-noch-zu-retten/;TDWI;Nicolai Erbs
24. Jun 20;TDWI Young Guns Mailing Juni 2020 | Transparency in Machine Learning;"Vorstellung, Christian ist seit einem halben Jahr bei den Young Guns in Wien. Er hat soeben sein Statistikstudium abgeschlossen und arbeitet bei mayato, einem BI-Consultingunternehmen. Ein großes Interesse liegt in der Tranzparenz von Machine Learning Modellen und damit befasst sich auch seine Masterarbeit., Raphael studierte Wirtschaftsinformatik und hält einen MA in Information Management. Heute arbeitet er als Principal Consultant Data &amp; Analytics bei der IT-Logix AG mit über 18 Jahren Praxiserfahrung im Business Intelligence und Datawarehousing Umfeld. Seit 2019 engagiert er sich für den Aufbau der TDWI Young Guns Community in der Schweiz., Persönliche Worte über Beweggründe, Motivation und Themeneinleitung, Transparenz von Machine Learning Modellen wird ein immer wichtigeres Thema, denn gerade „Black-Box-Modelle“ erfreuen sich großer Beliebtheit ob ihrer guten Vorhersagegüte. Black-Box-Modelle sind, wie der Name es schon sagt, Modelle, die einem Algorithmus folgen, dessen Entscheidungen allerdings nicht direkt nachvollziehbar sind. Damit die Vorhersagen aber möglichst transparent sind, also z.B. einzelne Gruppen nicht ungleich behandelt werden, oder der Einfluss einzelner Variablen auf das Ergebnis gemessen werden soll, ist es wichtig, diese Modelle und vor allem die Entscheidungen der Modelle auch als Mensch nachvollziehen zu können., Betrachten wir beispielsweise folgende Fragestellung: „Wie viele Fahrräder werden an einem Tag ausgeliehen?“ Hier kann nun von Interesse sein, warum die Vorhersage für die Tage schwankt, also „welche Variable hat einen wie starken Einfluss auf die vorhergesagten Werte?“ In diesem Fall können Methoden wie Partial-Dependence-Plots hilfreich sein, die modellagnostisch sind und somit nicht auf einen bestimmten Algorithmus basieren. Eine weitere Möglichkeit ist es, das komplexe Modell – nehmen wir ein Neuronales Netz – mit einem Einfacheren, z.B. einer logistischen Regression, zu nähern. In Letzterer können die Ergebnisse einfach interpretiert werden und eventuell sieht man auch einige Zusammenhänge, die das Neuronale Netz modelliert., Ein weiterer Ansatz liegt in der Modellierung von Vorhersageintervallen, denn oftmals ergeben Machine Learning Modelle nur Punktschätzer. Vorhersageintervalle können hier hilfreich sein, um eine Güte dieser Vorhersagen ableiten zu können. Diese Problematik kann unter anderem bei Random Forests beobachtet werden – ein Modell, bei dem als Vorhersage lediglich ein einzelner Wert geschätzt wird. Ein Lösungsansatz sind die Quantilen Random Forests. Das sind Modelle, die neben des Schätzers auch noch weitere Werte für Intervallgrenzen liefern. Man kann also schon folgern, in welchem Bereich der Vorhersagewert mit 95%-iger Wahrscheinlichkeit liegt. Nachdem die Schätzer der Intervallgrenzen recht unstabil sind, haben wir in meiner Masterarbeit ein anderes Verfahren entwickelt: Man schätzt die IQR (interquartile range, also die Quantile, in denen jeweils 25% und 75% der Ergebnisse liegen) mittels eines Quantilen Regression Forests und streckt diese durch multiplikative Dehnungsfaktoren zu validen Vorhersageintervallen eines selbstgewählten Levels (conformal inference). Somit kann also die Fragestellung „Wie viele Fahrräder werden an einem Tag ausgeliehen“ nicht nur mit einer einzelnen Zahl beantwortet werden, sondern man kann sagen „mit 95%-iger Wahrscheinlichkeit zwischen 50 und 100, im Mittel 80“. Hier sieht man auch gleich, dass die Intervalle eben nicht symmetrisch sein müssen, was gerade bei echten Daten ohnehin kaum der Fall ist., Aber bilde dir deine eigene Meinung – Hier ein paar spannende Links und Empfehlungen:, Meine Fragen, Was meint ihr? Beteiligt euch jetzt an der Diskussion bei LinkedIn oder Slack., Mein Highlight, Mein bisheriges Highlight bei den Young Guns war das virtuelle BarCamp im Mai 2020. Es gab die bisher einzigartige Möglichkeit ortsungebunden Erfahrungen miteinander auszutauschen. So konnten sich Teilnehmer*innen aus dem gesamten DACH-Raum vernetzen. Die Themen waren entsprechend breit aufgestellt, so dass für jeden etwas dabei war. Diskutiert wurde z.B. über Data Lakes und Data Governance – weit gefächerte Begriffe also, mit denen alle Anwesenden zuvor in verschiedenstem Ausmaß in Berührung kamen. Das beflügelte einen lebhaften Austausch und die zeitliche Begrenzung der Sessions wurde bis aufs Äußerste ausgereizt. Insgesamt empfinde ich – und so war auch das Feedbeck der Teilnehmer*innen – dieses Format als eine ideale Ergänzung zu den BarCamps vor Ort., Tipps, Vienna Deep Learning MeetupImport AI NewsletterThe Banana Data Podcast von Dataiku, Allgemeine Fragen zum Blog, zum Verein und zum Thema AutorIn werden:Sandra SteingrubeCommunity Managersandra.steingrube(at)tdwi.eu, Fragen zum Inhalt:Leif HitzschkeTDWI e.V. Vorstandsmitgliedleif.hitzschke(at)tdwi.eu, TDWI e.V.Lindlaustraße 2cD-53842 TroisdorfTel: +49 (0)2241/2341-212, ImpressumDatenschutz";https://blog.tdwi.eu/tdwi-young-guns-mailing-juni-2020-transparency-in-machine-learning/;TDWI;Christian Url
20. Jun 20;DIGITAL BARCAMP powered by TDWI Young Guns – Session zum Thema Product Analytics;In dieser kurzen Serie schreiben dieTDWI Young Gunsüber ihre Sessions im Digital BarCamp am 8. Mai. Dies ist Teil 4. Mehr zu dieser Veranstaltung erfahrt ihrhier.Wenn ihr Fragen habt, wendet euch gerne direkt an die Autoren., Die Diskussion meiner Session zum Thema „Product Analytics“ – Was ist es genau? Können wir gemeinsam eine Definition erarbeiten?“ begann mit einer kurzen Präsentation darüber, wie meiner Ansicht nach Produktanalytik definiert werden kann. Es stellte sich heraus, dass 50% der Teilnehmer an der Sitzung schon einmal von dem Begriff gehört hatten, wobei nur 1 Person den Begriff auch nur in einem geschäftlichen Kontext verwendet hatte., Nach der kurzen Einführung begann die Diskussion über den Unterschied zwischen „klassischen“ Analyse-Tools und Produktanalyse-Tools, wobei nicht für alle klar war, ob es einen klaren Weg gibt, die beiden zu trennen. Danach wurden Kommentare über digitale Produkte und den Unterschied zu einer klassischen Website, wie z.B. ein e-Commerce Shop, abgegeben. Außerdem haben wir darüber diskutiert, wie sich der Verkauf vielleicht unterscheidet. Es wurde angemerkt, dass Produktanalysen vielleicht eine Möglichkeit für Unternehmen sind, Analysen zu implementieren, ohne den Prozess der Datensammlung und -analyse vorher vollständig durchdenken zu müssen., Daraufhin begann die Diskussion über Logfiles und die Frage, ob dies auch als eine Form von Produktanalyse-Tooling betrachtet werden kann, ähnlich wie die Verwendung eines Tools wie Heap Analytics, Mixpanel, Amplitude usw. Es wurde angemerkt, dass Datenbank-Rohdaten sicherlich wie Logfiles in Form von Aggregationen verwendet werden könnten. Auch wurde darauf hingewiesen, dass typischerweise Informationen, die UX-Teams analysieren möchten, wahrscheinlich gar nicht erst in einer Datenbank zu finden wären, da diese Aktionen im Frontend stattfinden., Das Gespräch drehte sich um IOT und darum, wie einige Industriezweige, wie z.B. Lieferanten von Schwermaschinen, die Produktanalytik mit dem Ziel anwenden, zu verstehen, wie ihre physischen Produkte in einer realen Umgebung funktionieren. Die kleine Gruppe stimmte allgemein darin überein, dass die Definition von Produktanalytik auch diese Art von Daten umfassen könnte und/ oder sollte., Insgesamt wurde eine Definition der Produktanalytik selbst nicht notwendigerweise vereinbart und auch nicht explizit niedergeschrieben, aber ein gemeinsames Verständnis der Ziele der Produktanalytik im Vergleich zu einem eher „klassischen“ Ansatz war innerhalb der Gruppe vorhanden., Ich fand die Session unglaublich gelungen und bedanke mich nochmal bei allen Teilnehmern, denn diese haben mir auch direkt Feedback gegeben, dass die Diskussion super interessant und eine großartige Lernerfahrung gewesen war. Viele der Punkte, die erwähnt wurden, hatte ich vorher noch nicht berücksichtigt. So haben nicht nur die Teilnehmer was gelernt und das sollte ja auch das Ziel dieser Session sein., Allgemeine Fragen zum Blog, zum Verein und zum Thema AutorIn werden:Sandra SteingrubeCommunity Managersandra.steingrube(at)tdwi.eu, Fragen zum Inhalt:Leif HitzschkeTDWI e.V. Vorstandsmitgliedleif.hitzschke(at)tdwi.eu, TDWI e.V.Lindlaustraße 2cD-53842 TroisdorfTel: +49 (0)2241/2341-212, ImpressumDatenschutz;https://blog.tdwi.eu/digital-barcamp-powered-by-tdwi-young-guns-session-zum-thema-product-analytics/;TDWI;Matthew Brandt
19. Jun 20;DIGITAL BARCAMP powered by TDWI Young Guns – Session über BI / Daten-spezifische User Stories;In dieser kurzen Serie schreiben die TDWI Young Guns über ihre Sessions im Digital BarCamp am 8. Mai. Dies ist Teil 3. Mehr zu dieser Veranstaltung erfahrt ihr hier. Wenn ihr Fragen habt, wendet euch gerne direkt an die Autoren., Im Rahmen des ersten DIGITAL BARCAMP powered by TDWI Young Guns durfte ich die Session zu BI / Daten-spezifischen User Stories gestalten. Im Rahmen meiner Tätigkeit als BI-Berater hatte ich zu diesem Thema bereits früher Blogbeiträge verfasst und durfte nun mein Wissen mit der TDWI Young Guns Community teilen. Der zentrale Aspekt meiner Ausführungen war, dass wir Konzepte aus der (agilen) Softwareentwicklung nicht kopieren, sondern für BI / Daten-spezifische Anwendungen adaptieren müssen. Dabei gilt es darauf zu achten, dass die ursprünglichen Werte und Prinzipien erhalten bleiben. , Im Anschluss an die Präsentation führten wir eine Diskussion in der Gruppe. Vielen war die Problematik vertraut, dass Anforderungen in datenorientierten Vorhaben mindestens zu Beginn schwierig greifbar sind. Auch wurde davon berichtet, dass man nicht selten in wasserfallartiges Denken zurückfällt und statt End-2-End Stories eine Story z.B. pro Layer im DWH macht. Als Berater konnte ich wertvolle Rückmeldungen mitnehmen, u.a. dazu wie verständlich ich das Konzept in wenigen Minuten erklären konnte. Viele der Teilnehmer wiederum sind mit einem neuen Implus zum Thema User Stories in BI-Projekten aus dem virtuellen Barcamp zurückgekehrt., Allgemeine Fragen zum Blog, zum Verein und zum Thema AutorIn werden:Sandra SteingrubeCommunity Managersandra.steingrube(at)tdwi.eu, Fragen zum Inhalt:Leif HitzschkeTDWI e.V. Vorstandsmitgliedleif.hitzschke(at)tdwi.eu, TDWI e.V.Lindlaustraße 2cD-53842 TroisdorfTel: +49 (0)2241/2341-212, ImpressumDatenschutz;https://blog.tdwi.eu/digital-barcamp-powered-by-tdwi-young-guns-session-ueber-bi-daten-spezifische-user-stories/;TDWI;Raphael Branger
18. Jun 20;DIGITAL BARCAMP powered by TDWI Young Guns – Session zum Thema Agile Teams/Störungen vermeiden;In dieser kurzen Serie schreiben die TDWI Young Guns über ihre Sessions im Digital BarCamp am 8. Mai. Dies ist Teil 2. Mehr zu dieser Veranstaltung erfahrt ihr hier. Wenn ihr Fragen habt, wendet euch gerne direkt an die Autoren., In meiner Session machte ich eine Frage auf, die wohl alle was angeht. Denn nicht nur die Datennerds sind in agilen Teams zuhause. Agilität spielt in immer mehr Branchen und Teams eine große Rolle, auch weit weg von Softwareenteicklung. Da kommt die Frage: „Agile Teams: Wie vermeidet ihr Störungen der Sprints durch dringende betriebliche Anforderungen (Plötzliche Wünsche von Chefs, Nicht-Lieferungen von Daten,…)?“ doch gerade recht., Mit den anderen Teilnehmenden diskutierte ich eine halbe Stunde über die Puzzleteile zum Umgang mit „dringenden Themen“., Folgende Learnings konnten nach der intensiven Zeit festgehalten werden:, Die rege Beteiligung zeigt nochmal die Relevanz und das Interesse an dem Thema. Im Nachgang kam sogar die Idee auf eine Arbeitsgruppe zum Thema zu bilden, denn die 30 Minuten waren ein Tropfen auf dem heißen Stein., Hast Du Lust dich zum Thema einzubringen und Auszutauschen? Melde dich gerne bei uns, z.B. per E-Mail!, Allgemeine Fragen zum Blog, zum Verein und zum Thema AutorIn werden:Sandra SteingrubeCommunity Managersandra.steingrube(at)tdwi.eu, Fragen zum Inhalt:Leif HitzschkeTDWI e.V. Vorstandsmitgliedleif.hitzschke(at)tdwi.eu, TDWI e.V.Lindlaustraße 2cD-53842 TroisdorfTel: +49 (0)2241/2341-212, ImpressumDatenschutz;https://blog.tdwi.eu/digital-barcamp-powered-by-tdwi-young-guns-agile-teams-stoerungen-vermeiden/;TDWI;Andreas von Ballmoos
17. Jun 20;DIGITAL BARCAMP powered by TDWI Young Guns – Session über Externe Datenquellen;"In dieser kurzen Serie schreiben die TDWI Young Guns über ihre Sessions im Digital BarCamp am 8. Mai. Dies ist Teil 1. Mehr zu dieser Veranstaltung erfahrt ihr hier. Wenn ihr Fragen habt, wendet euch gerne direkt an die Autoren., Durch die Corona Pandemie waren wir am 8. Mai nach wie vor alle gezwungen Zuhause zu bleiben. Meine Young Guns Kollegen und Kolleginnen hatten die grandiose Idee ein Online BarCamp zu organisieren. Im BarCamp Nachbericht findet ihr mehr Informationen über den Ablauf, Durchführung und die anderen Sessions. Bevor ich mit meiner Session starten durfte, war ich natürlich normaler Teilnehmer des BarCamps und habe folgende Sessions besucht, die ich ebenfalls super spannend fand:, Meine Session hatte „Externe Datenquellen“ als Thema. In der heutigen Zeit ist es wichtiger denn je mit externen Daten zu arbeiten. Auch fachübergreifend ist dieses Thema von großer Relevanz – egal ob Analyst, BI Spezialist oder Data Scientist., Die VorbereitungZur Vorbereitung habe ich mir natürlich überlegt, welche Themenbereiche ich besprechen will:• Welche Datenquellen nutzt ihr?• Wofür nutzt ihr die Datenquellen?• Wo sind die Datenquellen integriert?, Das Ganze wollte ich natürlich aber im Voraus visualisieren. Zum Glück hat mir mein geschätzter Schweizer Young Guns Kollege –Raphael Branger– ein neues Tool vorgestellt – Miro (formerly known as RealtimeBoard)., Das Gute an Miro ist, man kann es über einen Link teilen und jeder kann darauf zugreifen. Hier findet ihr den Link zu dem Board., Die Session:, In der Session befanden sich 12 Teilnehmende aus allen Fachrichtungen und Industrien. Nach einer kurzen Einleitung in das Thema ging es auch direkt mit einer Diskussion rund um das Thema „Geodaten“ los. Dabei ging es darum Geodaten mithilfe eines OSM Servers zu „Geocoden“ – also die Adressen in Koordinaten umzuwandeln., Danach ging es direkt weiter mit den Wetterdaten. Hiervon haben wir gelernt, dass es eine Wetterdaten Integration in BigQuery von Google gibt., Auch die üblichen Datenquellen wie Twitter API, Github oder Statista kamen zur Sprache. Dabei wurden auch Use-cases angesprochen um zum Beispiel die Nutzung von Flughäfen vorherzusagen., Als nächstes haben wir uns die offiziellen Datenprovider (Hoppenstedt, Reuters, Bloomberg) angesehen. Gerade die Kosten von diesen Providern sind ja häufig sehr hoch – man kann also mit externen Daten auch Geld verdienen ;). Für Basisinformationen kann man jedoch im deutschsprachigen Raum auch auf den Bundesanzeiger setzen., Danach kamen wir zur Vorstellung der Web-Scraping Projekte: unter anderem Tatort-Blog, Stromblog, Stadt Hamburg Datahub und aus Datenproviderperspektive, Google Tag Manager., Natürlich kommt man beim Thema Scraping nicht um David Krieselherum. Seine Spiegel Mining und Bahn-Artikel stützen sich intensiv auf Scraping. Während der Diskussion hat Nils (Anwalt und Experte im Datenschutz) festgestellt, dass es in Ordnung sein könnte für statistische Zwecke öffentliche Daten im allgemeinen Interesse zu Scrapen und zu Analysieren. Wichtig ist laut Julian (Data Entrepreneur) aber auch die weitere Nutzung: „Ein kleineres Hobbyprojekt geht in Ordnung, für kommerzielle Zwecke nicht.“, Leider hat sich damit die Session dem Ende zugeneigt und wir sind zur Hauptsession zurückgekehrt. Es hat großen Spaß gemacht und es gab viele Insights., Allgemeine Fragen zum Blog, zum Verein und zum Thema AutorIn werden:Sandra SteingrubeCommunity Managersandra.steingrube(at)tdwi.eu, Fragen zum Inhalt:Leif HitzschkeTDWI e.V. Vorstandsmitgliedleif.hitzschke(at)tdwi.eu, TDWI e.V.Lindlaustraße 2cD-53842 TroisdorfTel: +49 (0)2241/2341-212, ImpressumDatenschutz";https://blog.tdwi.eu/digital-barcamp-powered-by-tdwi-young-guns-session-ueber-externe-datenquellen/;TDWI;Armin Geisler
10. Jun 20;Rückblick auf den ersten TDWI Online-Roundtable;"Jan Altin &amp; Gianleandro Sarro, Dashboard Tools on huge data sets:, Anlass &amp; Form:Die Planung und der Durchführungstermin des 23. TDWI Roundtable in Basel fiel just in der Zeit, in der aufgrund der Social-Distancing-Regelungen physische Veranstaltungen mit mehreren Menschen untersagt wurden. Viele waren ins Home-Office gezwungen: Eine Chance für TDWI, ein Online-Format anzubieten?, Weltweite Prämiere:Gemeinsam mit der Unterstützung des TDWI-Vorstandes, Christoph Kreutz, und Tanja Kenda und der freundlichen Einwilligung des Gastredners, Jörg Koch (iRIX Software Engineering AG) haben Jan Altin (biantik.com) und Gianleandro Sarro (trivadis.com) im Vorfeld verschiedene Tests durchgeführt und entschieden, den 21. April 2020 als Online-Veranstaltung anzubieten. Die Chance wurde genutzt, Ja. Und der Zuspruch war beeindruckend: Mit über 120 Anmeldungen aus Deutschland, Schweiz, Österreich und sogar aus Italien lag er über unseren Erwartungen. Ein herzliches Dankeschön an alle Teilnehmer., Start und Begrüssung:Bereits um 17:45h öffneten wir die virtuellen Tore, wo bereits einige Teilnehmer warteten. Kurz nach 18 Uhr begrüssen die Roundtable-Basel-Vorsitzenden Jan Altin und Gianleandro Sarro die Gäste zur weltweiten Prämie., Nach Vorstellung unserer Motivation, der Agenda und den virtuellen Spielregeln…, … hat Christoph Kreutz die TDWI vorgestellt., Jörg Koch von IRIX führte uns mit dem Thema „Dashboard Tools on huge data sets“ zuerst durch seine Motivation sowie das grundlegende Setting bevor er die einzelnen Tools vorstellte. IBM Cognos, Microsoft Power BI und Tableau wurden in Bezug Performance (auf der Grundlage einer Exasol DataViz) miteinander verglichen. Die Theorie wurde mit drei Demos und dem anschliessenden Fazit untermauert., Während des Vortrags haben die Teilnehmer – anregt durch die verschiedenen Einblicke – interessante Fragen und lebhaft diskutiert, teilweise über die Chat-Funktion wie auch über die Möglichkeit, selbst zu sprechen. Wie es sich für einen Roundtable gehört, haben wir schliesslich zum Apéro eingeladen, dieses Mal jedoch nur virtuell., Diesmal konnten wir mit unseren Gästen nicht bei einem Apéro in weitere Diskussionen und das Networking eingehen, hoffen aber sehr, auf die baldige Erholung der Pandemie und der Lockerung der Covid-19 Regulierungen, damit wir wieder vor Ort anstossen können., Wir bedanken uns herzlich beim Referenten für den Mut, die erste Online-Veranstaltung mit uns durchzuführen, bei TDWI e.V., die uns das Vertrauen ausgesprochen hat und last but not least an unsere Gäste, die aktiv mitgemacht haben; alle haben für das Gelingen des tollen Online-Events beigetragen., PS Der nächste Event, der 24. TDWI-Roundtable ist bereits angesetzt: 18. Juni 2020., PS2 Und am 25. August 2020 wollen wir unser Jubiläum, der 25. TDWI Roundtable in Basel wieder in gewohnten Rahmen begehen., Über Jan AltinSeit 1998 arbeitet Jan Altin als (selbständiger) Berater in den Rollen Project Manager, Business Change/Transformation/Requirements Manager, Scrum Master und Agile Coach. Schwerpunkt seiner Themen liegen in Data Warehousing, Business Intelligence, BI-Architekturen, Digitial/Agile Transformation und Agile Product Design. Er hat bei international tätigen Kunden (Banken/Finanzdienstleistungen, Telekommunikation, Automotive, Media, Öffentliche Verwaltung) Strategien, Projekte und Roll-outs aufgesetzt, begleitet und Kundenwerte erfolgreich gestaltet.Gleichzeitig ist Jan Altin als externer Dozent an diversen Hochschulen für das Fach Business Intelligence tätig., Allgemeine Fragen zum Blog, zum Verein und zum Thema AutorIn werden:Sandra SteingrubeCommunity Managersandra.steingrube(at)tdwi.eu, Fragen zum Inhalt:Leif HitzschkeTDWI e.V. Vorstandsmitgliedleif.hitzschke(at)tdwi.eu, TDWI e.V.Lindlaustraße 2cD-53842 TroisdorfTel: +49 (0)2241/2341-212, ImpressumDatenschutz";https://blog.tdwi.eu/rueckblick-auf-den-ersten-tdwi-online-roundtable/;TDWI;Gianleandro Sarro
03. Jun 20;DWH-Entwicklung im Umbruch;Immer mal wieder werde ich gefragt: Was muss ein guter Data Warehouse (DWH) Entwickler oder eine gute Entwicklerin können? Als ich vor über zehn Jahren in die DWH-Entwicklung einstieg, stellte ich die gleiche Frage. Die Antwort damals lautete: Lies mal die Bücher von Kimball (Stichwort „Dimensionale Modellierung“) und lerne SQL (SQL impliziert dabei auch Grundkenntnisse der relationalen Datenbanktheorie). Das sind beides ziemlich universelle Grundlagen, die auch heute noch zum Grundwortschatz eines jeden DWH-Entwicklers gehören sollten. In diesem Beitrag gehe ich der Frage nach, was im Jahr 2020 so alles von DWH-Entwicklern verlangt wird., Nach wie vor ist das dimensionale Modell dasjenige, welches Fachanwender mit Abstand am einfachsten verstehen, wenn sie selber Auswertungen erstellen müssen. Selbst wenn im Core-DWH häufig auch eine 3NF oder Data Vault Modellierung zum Zuge kommt, spätestens auf Data Mart Ebene und damit dem üblichen Zugriffslayer für die Endbenutzer kommt das dimensionale Modell zum Tragen. Interessanterweise ist das dimensionale Modell nicht nur für uns Menschen gut verständlich, sondern auch optimale Grundlage für In-Memory-Datenbanken und darauf aufbauende BI-Werkzeuge (Weiterführende Infos mit Beispielen aus der Microsoft-Welt hier und hier)., SQL wiederum ist die universelle Basissprache, um DWH-Strukturen anzulegen, mit Daten zu beladen und diese wiederum abzufragen. Selbst wenn einem klassische ETL-Werkzeuge und DWH-Automationswerkzeuge viel Arbeit bei der händischen Entwicklung von SQL-Code abnehmen, so wird man den generierten Code mindestens lesen können und bei Bedarf optimieren müssen. Natürlich gibt es viele Variationen und Erweiterungen zum eigentlichen Standard, z.B. für Graph-Abfragen, aber gerade auch neue Datenbankanbieter wie Snowflake setzen voll auf SQL als Grundlage., Insbesondere bei der Verwendung von webbasierten Quellsystemen und webbasierten Ziel-Datenbanken gibt es aber noch einen anderen „Sprachbedarf“ neben SQL: Shell-Programmierung, z.B. mittels PowerShell. Der Grund dafür liegt u.a. in der Nutzung von Dateischnittstellen. Um Daten zwischen webbasierten Systemen auszutauschen, werden diese oft in CSV, XML oder JSON Dateien exportiert und über FTP-Server, Data Lakes usw. verteilt. Zudem lassen sich viele Datenbanken schneller über einen Datei-basierten Bulk-Load befüllen, als einzelne Datensätze via SQL-Insert hinzuzufügen. Das Erstellen, zippen und kopieren der Dateien wird dann eben über eine Shell programmiert. Oder aber die Daten werden via REST und OData APIs ausgetauscht – oft ebenfalls unter Verwendung von Shell-Skripten, um Daten mindestens temporär in Dateien zwischenzuspeichern und dann ins DWH zu laden. Eine hervorragende Seite zum Erlernen und Nachschlagen von PowerShell in deutscher Sprache findet sich hier., Shell-Skripte werden aber auch noch in einem weiteren Kontext benutzt: Continuous Integration und Deployment. Wie ich bereits andernorts ausgeführt habe, bedeutet mehr Agilität in BI-Projekten immer auch mehr Automation. Das gilt nicht nur für die DWH-Entwicklung als solches (vgl. dazu auch den letzten Abschnitt unten), sondern auch für den Transport von Artefakten z.B. von einer Entwicklungs- auf eine Test- und Produktionsumgebung. Auch wenn‘s für die Orchestrierung Werkzeuge mit graphischer Oberfläche gibt (Jenkins, Azure DevOps,…), so werden die einzelnen Schritte meist als Skripte programmiert., Soweit so gut. Doch was brauchen DWH-Entwicklerinnen im Jahr 2020 neben den bisher erwähnten sonst noch so an Fähigkeiten? Ich beoachte eine Tendenz, dass wir uns einerseits stärker mit Prinzipien guter Softwarentwicklung auseinanderseiten müssen und andererseits ein hohes Mass an Abstraktionsvermögen benötigen. Dafür muss ich etwas ausholen und zwar zum Unterschied zwischen imperativer und deklarativer Programmierung. Ein umfassender Artikel dazu findet sich hier. Folgend versuche ich die zwei Ansätze anhand eines Beispiels aus der DWH-Welt zu beschreiben. Nehmen wir an, wir wollen eine Dimension mit SCD2-Historisierung erstellen. Wenn wir imperativ vorgehen, würden wir hingehen und den SQL-Code schreiben, welcher eine Serie von Arbeitsschritten ausführt: Zuerst legen wir die Dimensionstabelle mit ihren Feldern an. Dann erstellen wir eine Stored Procedure, um die Tabelle zu laden. Die Stored Procedure enthält Schritte, um zuerst neue Datensätze zu laden, danach überprüfen wir für bestehende Datensätze, ob sich etwas geändert hat. Falls ja, erstellen wir einen neuen Datensatz mit einem neuen „Gültig-Ab“ Datum. Imperativ bedeutet in diesem Kontext: Wir sagen der Maschine explizit, was sie tun soll. Im Gegensatz dazu abstrahiert ein deklarativer Ansatz die konkrete Anweisung: Anstelle von konkreter SQL-Programmierung definieren wir Metadaten und sagen der Maschine: Erstelle mir eine Dimension, welche gemäss dem Muster SCD2 historisiert wird. Danach übernimmt der Compiler bzw. ein Generator das Ausprogrammieren der benötigten Anweisungen auf SQL-Ebene. Es ist genau dieser deklarative Ansatz, welcher der Kategorie von DWH-Automationswerkzeugen zu Grunde liegt. Marktbekannte Vertreter sind z.B. WhereScape oder Timextender (eine gute Übersicht findet sich zudem hier). Und diese Werkzeuge verändern zunehmend die Arbeit des DWH-Entwicklers. Auf der einen Seite müssen er oder sie sich weniger um die Details der SQL-Programmierung kümmern sondern können sich vermehrt der fachlichen Problemlösung widmen – dort wo wirklicher Mehrwert geschaffen wird. Auf der anderen Seite braucht es Leute, welche die Generatoren programmieren können. Je grösser das Projekt, desto grösser auch die Chance, dass man die von einem Softwarehersteller vorgegebenen Muster erweitern oder anpassen will. Dabei muss man nicht zwangsläufig zur Softwareentwicklerin werden, jedoch helfen grundlegende Fähigkeiten in der Softwareentwicklung. Ich selber arbeite aktuell häufig mit WhereScape. Dieser Anbieter erlaubt es zum Beispiel, die bestehenden Generatoren zu erweitern oder gänzlich neue Muster abzubilden (ein Beispiel dazu haben mein Kollege Elgar Brunott und ich kürzlich hier beschrieben). Dazu sind im Wesentlichen drei Fähigkeiten gefragt: Erstens muss ich eine Vorstellung haben, wie das Resultat am Ende aussehen soll (da sind wir wieder bei den Modellierungs-, SQL- und Powershell-Fähigkeiten). Zweitens muss ich nun induktiv abstrahieren können, wie aus einem konkreten Beispiel eine allgemeine Regel formuliert werden kann. Drittens muss ich diese allgemeine Regel in der Sprache des verwendeten Werkzeugs ausdrücken können, im Falle von WhereScape ist das Pebble und sowie einer Regel-Engine., Die DWH-Entwicklung ist im Umbruch – weg von der Fleissarbeit hin zu mehr „Engineering“. Datenmodellierung und SQL gehören nach wie vor zum Pflichtrepertoire der DWH-Entwickler und DWH-Entwicklerinnen. Aber wir müssen unseren Horizont auch erweitern und ergänzende Sprachen wie PowerShell mindestens in ihren Grundzügen kennen und anwenden können. Dazu kommt der steigende Konkurrenzdruck. Wer Lösungen deklarativ entwickelt und die eigentliche Code-Erstellung automatisiert, hat die Nase vorne!, *Der Beitrag spiegelt die Meinung des Autors wider und ist keine allgemeingültige Meinung des TDWI. Als TDWI bieten wir die Plattform, alle Themen und Sichtweisen zu diskutieren.*, Ein sehr interessanter Artikel, vielen Dank dafür!, Persönlich würde ich noch wesentlich größeren Wert auf die Erstellung von logischen Datenmodellen und die Erfassung und Verwaltung der Metadaten zur verbesserten Automatisierung legen., Zudem würde ich gerne noch ergänzen, dass es für DWHI/BI-Entwickler mindestens mittelfristig wichtig ist, ein grundlegendes Verständnis für nicht-relationale Datenverwaltungsarchitekturen zu haben, um zukunftssicherere Entscheidungen treffen zu können., Guter Artikel! Punktgenau! Danke!, Allgemeine Fragen zum Blog, zum Verein und zum Thema AutorIn werden:Sandra SteingrubeCommunity Managersandra.steingrube(at)tdwi.eu, Fragen zum Inhalt:Leif HitzschkeTDWI e.V. Vorstandsmitgliedleif.hitzschke(at)tdwi.eu, TDWI e.V.Lindlaustraße 2cD-53842 TroisdorfTel: +49 (0)2241/2341-212, ImpressumDatenschutz;https://blog.tdwi.eu/dwh-entwicklung-im-umbruch/;TDWI;Raphael Branger
27. Mai 20;Data Science, Business Intelligence und Analytics – Beschreibung und Analyse mithilfe von Python;Deutsche Übersetzung dieses Artikels. In den letzten Jahren wurde die Datenwelt mit einer neuen Realität konfrontiert. Die bessere Verfügbarkeit von Daten und Rechenkapazität führte zu einem Anstieg der Jobrollen. Eine der wichtigsten neuartigen Jobrollen ist der Data Scientist. Die Definition lautet folgendermaßen:, „Als Data Scientist oder Datenwissenschaftler bist Du dafür zuständig, aus unstrukturierten Rohdaten eine strukturierte Datenbasis zu schaffen, zu analysieren und am Ende mit Deinem betriebswirtschaftlichen Knowhow eine Entscheidungsgrundlage für ein Unternehmen zu schaffen.“ (Schmole 2020)., Wie passt das zu den anderen Rollen: Business Intelligence (Experte) und Analyst?, Dieser Artikel hat das Ziel diese neuen Bereiche zu erforschen, beschreiben und analysieren. Dabei werden im Laufe des Artikels zuersteinmal die drei Rollen beschrieben gefolgt von einer Analyse des Deutschen Jobmarktes durch Python, Data Science, Business Intelligence und Analytics – alles das gleiche?, Meine kurze Antwort dazu ist: Es ist nicht das gleiche, aber es gibt Gemeinsamkeiten., Data Science nutzt strukturierte (Tabellen) und unstrukturierte Daten (Texte, Dokumente, Bilder, Videos) um Erkenntnisse und Muster zu erkennen. Data Science ist dabei auf zukünftige Informationen gerichtet während Business Intelligence auf vergangene Daten gerichtet ist. Beide Bereiche teilen Datenvisualisierung –und Management. Das Bild beschreibt es sehr treffend: , Analytics auf der anderen Seite ist ein Prozess um mithilfe mathematischer Methoden nützliche Muster zu erkennen (Cambridge University Press 2014)., Diese Einleitung hat uns einen kurzen Einblick in die drei Felder gegeben anhand der folgenden Analyse des Job-Marktes lässt sich erkennen worin die Gemeinsamkeiten und Unterschiede liegen., Job Markt für Data Science, Business Intelligence und Analytics in Deutschland:, In einer Analyse vom September 2018 wurden 900 Jobeinträge unter dem Suchbegriff     „Data Science“ in Deutschland gefunden (Piatetsky 2018). Wie sieht es heute aus?, Für diese Analyse wurden Daten von LinkedIn heruntergeladen. Es wurden folgende Suchbegriffe genutzt: „Data Scientist“, „Analyst“ und „Business Intelligence“ (BI). Das Resultat sind ca. 975 extrahierte Jobeinträge für jeden Begriff, insgesamt 2.923 Jobs., Unter diesen Suchbegriffen gibt es natürlich mehr Jobeinträge als in unserem Fall analysiert wurden. Die meisten Jobs gibt es im BI Bereich. Analyst ist an zweiter und Data Science an dritter Stelle. Immerhin haben die Data Science Jobs um 2,784 zugenommen. Das ist ein Anstieg von 400% zu 2018., Alle Jobs dieser Suche machen jedoch nur 5% der gesamten IT Stellen aus., Jobtitel:, Unter den Top 10 Nennungen wurden Data Scientist, Data Analyst und Business Intelligence Analyst am meisten genannt., Zwischen den Suchbegriffen und Jobtitel gibt es jedoch Überschneidungen. Am meisten taucht das Wort „Analyst“ auf. Dabei wird das Wort interdisziplinär in allen Bereichen genutzt. Die viertgrößte Gruppe zeigt Jobs in den drei Kategorien, welche keine der Suchbegriffe aufzeigen – welche sind es?, Die meisten dieser Jobs kommen aus dem BI Bereich. Der Data Engineer steht mit Abstand weit oben. Diese Rolle ist auf jeden Fall genauso wichtig wie die drei anderen Rollen., Firmen: , Die meisten Jobs kommen von der Webseite Campusjäger – einer Recruitment Agentur für Young Professionals und Studenten., Stadt:, Man ahnt es bereits……, ……die meisten Jobs kommen aus Berlin gefolgt von Hamburg und München. Die Karte hilft dabei diese Observation zu bestätigen. Eine hohe Anzahl von Stellen gibt es jedoch auch im Ruhrgebiet sowie in und um Frankfurt., Bei der genaueren Analyse der Ausbreitung hilft uns das Seaborn Heatmap Chart. Das Diagramm nutzt eine Normalisierungsfunktion der Daten um die Anzahl der Jobs zwischen den Kategorien pro Stadt zu vergleichen (dunklere Farbe). Es bedeutet zum Beispiel das es mehr Analystenjobs, als BI und Data Science Positionen, in Berlin gibt. Bei der Betrachtung von Frankfurt fällt auf, dass es hier auch mehr Analystenstellen gibt. Das könnte durch die Finanzindustrie getrieben sein, welche in Frankfurt ihren Hauptsitz hat und sich stark auf Analysten stützt. , Hannover auf der anderen Seite bietet vermehrt Data Science Positionen. Der Grund liegt hier sehr wahrscheinlich in den Versicherungsunternehmen welche in Hannover beheimatet sind. Die Digitalisierung ist ein großes Thema bei den Versicherern und es wird versucht durch Data Science eine Expertise in diesem Bereich aufzubauen., Alter der Anzeigen: , Die meisten Anzeigen wurden sind nicht älter als vier Monate. Am meisten gab es hierbei Jobs nicht älter als zwei Wochen., Karrierestufe: , Gute Neuigkeiten für Young Professionals und Absolventen, die meisten Jobs kommen aus dem Entry Level Bereich. Gerade im Data Science Bereich gibt es die meisten dieser Entry Level Positionen. Es gibt dort auch kaum Executive und Director Positionen im Vergleich zum etablierten BI-Bereich., Abteilung – erste Nennung:, Die meisten Jobs sind im Bereich Business Development, IT and Engineering zu finden., Analysten und BI-Experten dominieren den Business Development Bereich. In der IT sind vermehrt Data Scientists zu finden. Im Engineering Bereich sind die meisten Data Science Positionen angesiedelt., Der Finanzbereich stützt sich am meisten auf Analysten., Industrie – erste Nennung:, Fast 50% der Positionen sind im IT Bereich angesidelt., Der IT Bereich wird durch BI Experten und Data Scientisten geprägt. Marketing, Internet und die Chemieindustrie setzt hierbei mehr auf Analysten., Word Cloud: , Mithilfe einer Word Cloud ist es möglich unstrukturierte Daten zu analysieren. Hierbei geht es um die Jobangebot-Texte. Die Word Cloud zeigt die am meisten genannten Wörter. Vorher werden natürlich noch alle StopWords (a, and, or, etc.) entfernt., Die Word Cloud für Analyst:, Data Science:, Business Intelligence:, Alle drei nutzen sehr häufig die Wörter „team“ und „data“. In der Analyst Word Cloud das Wort wird das Wort „Customer“ sehr häufig genutzt. In der Data Science Word Cloud wurden die Wörter „Machine Learning“ sehr stark hervorgehoben. Data Science und BI nutzen beide den Begriff „Big Data“. BI nutzt natürlich den Begriff „Data Warehouse“., Am Ende dieses Artikels kann man sagen, dass Analytics, Business Intelligence und Data Science trotz der Unterschiede, gewisse Gemeinsamkeiten teilen. Diese Unterschiede und Differenzen wurden durch die Analyse des deutschen Jobmarktes hervorgehoben., Am wichtigsten ist in meinen Augen, dass alle drei Bereiche die Leidenschaft für Daten teilen., Der vollständige Code befindet sich als Notebook auf nbviewer oder auf Github., Cambridge University Press. 2014. Analytics. Zugriff am 17. Mai 2020. https://dictionary.cambridge.org/de/worterbuch/englisch/analytics., Piatetsky, Gregory. 2018. How many data scientists are there and is there a shortage? September. Zugriff am 17. Mai 2020. https://www.kdnuggets.com/2018/09/how-many-data-scientists-are-there.html., Pugsley, Stan. 2017. Understanding the Differences Between Data Science and BI. 5. Dezember. Zugriff am 17. Mai 2020. https://tdwi.org/articles/2017/12/05/bi-all-understanding-differences-data-science-and-bi.aspx., Schmole, Melanie. 2020. Was macht ein Data Scientist? Zugriff am 15. Mai 2020. https://www.get-in-it.de/magazin/arbeitswelt/it-berufe/was-macht-ein-data-scientist., *Der Beitrag spiegelt die Meinung des Autors wider und ist keine allgemeingültige Meinung des TDWI. Als TDWI bieten wir die Plattform, alle Themen und Sichtweisen zu diskutieren.*, Allgemeine Fragen zum Blog, zum Verein und zum Thema AutorIn werden:Sandra SteingrubeCommunity Managersandra.steingrube(at)tdwi.eu, Fragen zum Inhalt:Leif HitzschkeTDWI e.V. Vorstandsmitgliedleif.hitzschke(at)tdwi.eu, TDWI e.V.Lindlaustraße 2cD-53842 TroisdorfTel: +49 (0)2241/2341-212, ImpressumDatenschutz;https://blog.tdwi.eu/data-science-business-intelligence-und-analytics-beschreibung-der-bereiche-mithilfe-von-python/;TDWI;Armin Geisler
20. Mai 20;DIGITAL BARCAMP powered by TDWI Young Guns – Vol. 1;Zweifel, wie ein lockerer Austausch im digitalen Format funktionieren kann? Fragen, ob und wie das beliebte analoge Format in die digitale Welt mit der Grenze Bildschirm angenommen wird? Wir sagen – SUPER – ausprobieren – machen! Als Young Guns genau unser Gebiet und wir sind sehr stolz, dass das erste Digital BarCamp so viel Mehrwert für unsere Community gebracht hat. Wir behalten das neue, digitale BarCamp Format auf jeden Fall bei. So ist eine ganz neue Art von Austausch entstanden. Besonders durch die Vernetzung unserer DACH-weiten Community, haben sich nochmal ganz neue Sichtweisen und Erfahrungswerte aufgetan. Und außerdem ein Wir-Gefühl, das auch durch den Bildschirm entstanden ist. Besser hätten wir es uns nicht ausmalen können. Vielen Dank an die 35 Teilnehmenden, die das Digital BarCamp zu etwas ganz Besonderem gemacht haben., Ihr wollt wissen wie genau das alles abgelaufen ist?BarCamp PlanungErst einmal haben wir die Sessionplanung abgekoppelt. Uns war wichtig (und das ist ja eher BarCamp-untypisch), dass die Teilnehmer VOR dem BarCamp Termin eine Agenda an der Hand hatten. Diese ist aber genau wie sonst auch über eine Abstimmung entstanden. Wir haben hierfür sli.do genutzt. Ein Voting-Tool, wo jeder seine Session eintragen konnte und die Community dann eine Woche Zeit hatte die Agenda zu bestimmen., BarCamp DurchführungDann war der Tag gekommen. Wir hatten für Freitag, den 8. Mai von 17:30 – 20:00 Uhr eingeladen. Auch hier wieder ein bisschen geänderte Rahmenbedingungen, denn die Erfahrung der letzten Meeting-Zeit hat gezeigt, dass 3 Stunden vor dem Rechner zu hocken unglaublich anstrengend ist und sich hier ganz klar der Unterschied zum analogen Format zeigt.Beibehalten haben wir die Vorstellungsrunde und auch das 30 Minuten-Session-Format. Dank Zoom und seinen Breakout-Sessions konnten wir alle das ganze BarCamp lang im gleichen Tool bleiben., InhalteMit je zwei Sessions parallel konnten wir kleine Gruppen für einen intensiven Austausch garantieren., Hier unsere Sessions:• Agile Teams: Wie vermeidet ihr Störungen der Sprints durch dringende betriebliche Anforderungen (Plötzliche Wünsche von Chefs, Nicht-Lieferungen von Daten etc.)?• Data Lake: Was sind eure Erfahrungen? Technologie, Verantwortlichkeiten, Motivation, Success- And Fail-Stories• Data Governance: Umsetzung, Nutzen, Lessons learned. Was könnt Ihr berichten?• Product Analytics: Was ist es genau? Können wir gemeinsam eine Definition erarbeiten?• Externe Datenquellen: nutzt ihr sie um euren Daten anzureichern? Wie bindet ihr sie an (Direkte Integration im DWH, API, Web-Scraping, etc..)?• BI / Daten-spezifische User Stories, Allgemeine Fragen zum Blog, zum Verein und zum Thema AutorIn werden:Sandra SteingrubeCommunity Managersandra.steingrube(at)tdwi.eu, Fragen zum Inhalt:Leif HitzschkeTDWI e.V. Vorstandsmitgliedleif.hitzschke(at)tdwi.eu, TDWI e.V.Lindlaustraße 2cD-53842 TroisdorfTel: +49 (0)2241/2341-212, ImpressumDatenschutz;https://blog.tdwi.eu/digital-barcamp-powered-by-tdwi-young-guns-vol-1/;TDWI;Alex Krolla
13. Mai 20;Data Governance – Grundlagen, Konzepte und Anwendungen;In Zeiten zunehmender Digitalisierung findet heute fast jedes physische Objekt der realen Welt seine digitale Entsprechung in Form von Daten. Vor diesem Hintergrund erweist es sich als kaum verwunderlich, dass den Daten eine zunehmend steigende Bedeutung zukommt. So werden Daten kaum noch als notwendiges Übel verstanden, dem es mit den Mitteln der „Elektronische Datenverarbeitung (EDV)“ zu begegnen gilt, sondern vielmehr als Rohöl des 21. Jahrhunderts, das als unerlässlicher Schmierstoff die Rotation der Räder einer globalen Wirtschaft ermöglicht.Nicht verwunderlich, dass die durchdachte und professionelle Verarbeitung der verfügbaren Daten weit oben auf der Agenda aller großen Organisationen steht. Zahlreiche Projekte zum Aufbau eines effizienten und effektiven Datenmanagements wurden ins Leben gerufen, um die Verarbeitung von Daten gezielt zu planen und umzusetzen. Damit derartige Initiativen koordiniert erfolgen, bedarf es klarer Leitlinien und Vorgaben für die jeweiligen Aktivitäten im Datenmanagement. Unterstützung erfährt diese Forderung durch den zunehmenden Umfang regulatorischer Vorgaben sowie durch die wachsende Komplexität der eingesetzten IT-Landschaften., In diesem Kontext erlangt das Themengebiet Data Governance (DG) immer größere Bedeutung, verbunden mit dem Ziel, verbindliche Handlungsanweisungen und Organisationsstrukturen für den Umgang mit Daten zu etablieren. Somit umfasst Data Governance alle Regelungen, Mechanismen und Werkzeuge, die sich für einen verantwortungsvollen Umgang mit Daten als relevant erweisen und sich dabei auf fachliche und technische sowie vor allem auf organisatorische Betrachtungsperspektiven beziehen können., Der Herausgeberband „Data Governance – Grundlagen, Konzepte und Anwendungen“ greift nach einer Einordnung und Abgrenzung des Themas die unterschiedlichen Kernaspekte der Data Governance umfassend auf. Anschließend werden spezielle DG-Facetten und -Toolkategorien mit hoher praktischer Relevanz präsentiert, bevor die Darstellung spezifischer Anwendungslösungen erfolgt., Hier können Sie das Buch „Data Governance“ vorbestellen., Inhalte:, Zielgruppe, Allgemeine Fragen zum Blog, zum Verein und zum Thema AutorIn werden:Sandra SteingrubeCommunity Managersandra.steingrube(at)tdwi.eu, Fragen zum Inhalt:Leif HitzschkeTDWI e.V. Vorstandsmitgliedleif.hitzschke(at)tdwi.eu, TDWI e.V.Lindlaustraße 2cD-53842 TroisdorfTel: +49 (0)2241/2341-212, ImpressumDatenschutz;https://blog.tdwi.eu/data-governance-grundlagen-konzepte-und-anwendungen/;TDWI;Peter Gluchowski
06. Mai 20;Umsetzung eines Data Lakes | TDWI Young Guns Beitrag;"Vorstellung, Sebastianist seit 2016 bei der MYTOYS Group (member of the otto group) im Bereich Analytics und Data Science tätig. Seine Aufgabenschwerpunkte bestehen in der Entwicklung eines adaptiven und personalisierten Produktlistenscores, eines dynamischen Marketingattributionsmodels und der Konzeption und Umsetzung von Data Science Anforderungen an eine IT –Architektur. Zuvor forschte Sebastian 5 Jahre an Humboldt-Universität zu Berlin zu neurowissenschaftlichen Grundlagen von Sprache und des Einflusses von „fake news“ auf die Wahrnehmung der betroffenen Personen., Geroist Mitgründer und Geschäftsführer bei der QuinScape GmbH, einem Dortmunder IT-Dienstleistungsunternehmen mit 170 Mitarbeitern und Fokus auf Data &amp; Analytics. Er organisiert die Meetup-Gruppe „Business Intelligence &amp; Analytics Dortmund“ mit über 1.000 Mitgliedern und ist Vorsitzender des TDWI Roundtable Ruhrgebiet. Zuvor hat er an der TU Dortmund Informatik mit dem Nebenfach Betriebswirtschaftslehre studiert, 1999 mit Auszeichnung abgeschlossen und im Anschluss über die ressourcenschonende Entscheidungsfindung autonomer Agenten promoviert. Er ist leidenschaftlicher Skifahrer und glücklicher Familienvater mit zwei Söhnen und einem Hund., Persönliche Worte über Beweggründe, Motivation und Themeneinleitung, Durch die Einführung eines Data Lakes öffnet sich für Unternehmen die Möglichkeit, nahezu unbegrenzte Mengen an strukturierten und unstrukturierten Daten aus unzähligen Quellen zu speichern ohne Rücksicht darauf, ob und wie diese Daten in Zukunft genutzt werden können. Durch seine Beschaffenheit fördert ein Data Lake das Experimentieren und die Datenexploration durch einen breiteren Kreis an (Nicht-)Analysten und (Citizen) Data Scientists. Doch wie geht man mit den organisatorischen und personellen Herausforderungen bei der Einführung eines Data Lakes um, nachdem die strategische Entscheidung gefallen und ein Business Sponsor gefunden wurde? Zunächst müssen sich die an der Implementierung beteiligten Parteien auf Anwendungsfälle verständigen, die klar und deutlich den Bedarf und die Vorteilhaftigkeit eines Data Lakes aufzeigen. Anhand dieser Anwendungsfälle kann dann über die hierfür relevante Architektur gesprochen werden (z.B. key value vs. spaltenorientierte Datenbanken). Diese Punkte, Identifikation von spezifischen Anwendungsfälle und die daraus resultierende Anforderung an die Architektur, sind nicht nur Voraussetzung für die nächsten konkreten Schritte, sondern dienen auch dazu, für die beteiligten Mitarbeiter ein gemeinsames Verständnis aus- und aufzubauen. Wenn dies gelungen ist, kann im nächsten Schritt darüber diskutiert werden inwieweit eine Data Lake Technologie hausintern, unter Einsatz von vereinzelten Tools oder durch Erwerb einer Komplettlösung umgesetzt werden kann oder soll. Für Letzteres ist es im nächsten Schritt sinnvoll, für die Evaluierung externer Anbieter ein Proof of Concept zu erstellen, welches abgeleitet von den Anwendungsfällen Eigenschaften definiert, die es nicht oder unbedingt haben muss. Neben diesen organisatorischen Fragestellungen stellt die Einführung einer Data Lake Lösung neue Kompetenzanforderungen an Mitarbeiter, die in Abhängigkeit der Rolle identifiziert und entsprechend erweitert werden sollten., Aber bilde dir deine eigene Meinung – Hier ein paar spannende Links und Empfehlungen:, Eine allgemeine Einführung zu Data Lakes (english)Eine Bestandsaufnahme auf dem TDWI BlogA smarter way to jump into data lakes, Meine Fragen an die Community, Was meint ihr? Beteiligt euch jetzt an der Diskussion beiLinkedInoderSlack., Meine Highlights, Sebastian: „Mich begeistert an meiner Arbeit die schiere Interdisziplinarität in Data Science und Analytics und die Möglichkeit von anderen daten-begeisterten Menschen zu lernen! Von den Young Guns habe ich über einen Artikel im BI Spektrum erfahren, welches bei uns in der Firma manchmal herumliegt. Als technologie-affiner IT-Quereinsteiger freue ich mich auf den Austausch innerhalb des TDWI Vereins und natürlich innerhalb der Young Guns. Let’s do it :)!“, Allgemeine Fragen zum Blog, zum Verein und zum Thema AutorIn werden:Sandra SteingrubeCommunity Managersandra.steingrube(at)tdwi.eu, Fragen zum Inhalt:Leif HitzschkeTDWI e.V. Vorstandsmitgliedleif.hitzschke(at)tdwi.eu, TDWI e.V.Lindlaustraße 2cD-53842 TroisdorfTel: +49 (0)2241/2341-212, ImpressumDatenschutz";https://blog.tdwi.eu/umsetzung-eines-data-lakes/;TDWI;Sebastian Rose
29. Apr 20;Data Lake – eine Bestandsaufnahme in drei Akten: Architektonische Fragestellungen (2);Nachdem der letzte Beitrag dieser Serie das grundsätzliche Begriffsverständnis sowie die Motivation für den Aufbau eines Data Lakes thematisierte, betrachtet dieser Artikel nun Fragestellungen hinsichtlich der Architektur und des Aufbaus eines Data Lakes. Entsprechend den unterschiedlichen Beweggründen für die Etablierung eines Data Lakes, gibt es auch verschiedenste Ansätze für die Strukturierung einer entsprechenden Systemlandschaft, welche die folgenden Abschnitte anhand ausgewählter architektonischer Fragestellungen kurz verdeutlichen., Für eine systematische Annäherung an diese Frage, steckt Abbildung 1 den technologischen Rahmen eines Data Lakes ab. Hierbei gibt es, wie so oft bei diesem Thema, keine einheitliche Abgrenzung. Meist beinhaltet ein Data Lake insbesondere neuartige Ansätze der Datenhaltung, die im Zuge der Big-Data-Bewegung an Relevanz gewonnen haben (Enges Verständnis). In einem weiten Verständnis bezeichnen viele Organisationen aber auch ihre komplette Analytics-Landschaft, inklusive traditionellen Data-Warehouse-Strukturen und nachgelagerten informationsgenerierenden Systemen, als Data Lake., Beide Sichtweisen haben aber je nach Gegebenheiten der Umwelt und nach Grad der Integration traditioneller und neuartiger Ansätze ihre Berechtigung und die Wahrheit liegt in der Praxis meist irgendwo dazwischen., Ein zentraler Treiber des Data-Lake-Themas ist die Anforderung der Speicherung von großen Mengen an strukturierten- und unstrukturierten Rohdaten, wie bspw. Sensor-Daten oder Bild- und Video-Dateien. Traditionelle Data-Warehouse-Konzepte stoßen hierbei oft an Grenzen und wurden daher in den letzten Jahren oft durch Massenspeicher-fokussierte Data-Lake-Ansätze erweitert. Technologien wie Hadoop oder cloudbasierte Objektspeicher erlauben hierbei durch eine horizontale Skalierung große Datenmengen kostengünstig zu speichern und effizient zu verarbeiten. Vor diesem Hintergrund hat sich das Verständnis eines Data Lakes als „große Festplatte“ entwickelt, da alles was kein Platz im Data Warehouse (DWH) hat in einem Data Lake abgelegt werden kann., Umso mehr Daten sich nun aber in einem Data Lake befinden, desto näher liegt der Gedanke, dass dieser die traditionelle Rolle des DWH als führende Datenquelle (Single Point of Truth) übernimmt. Dieser Konflikt macht es daher essentiell sich Gedanken über das Zusammenspiel des DWH und des Data Lake zu machen., Abbildung 2 skizziert drei mögliche Lösungsansätze zu dieser Problemstellung. Bei einem parallelen Ansatz existieren DWH und Data Lake unabhängig voneinander und die vor- und nachgeschalteten Systeme füttern bzw. konsumieren beide System separat. Dieser Ansatz ist einfach und geradlinig, bringt aber auch zahlreiche Redundanzen und eine Integration wird leicht unübersichtlich., Da der Data Lake eher für Rohdaten gedacht ist und das DWH üblicherweise mit aufbereiteten und aggregierten Daten arbeitet, bietet sich als Alternative die Möglichkeit den Data Lake als Vorsystem zu einem DWH zu nutzen. Hier werden zuerst alle Daten in den Data Lake persistiert und anschließend ausgewählte Daten per ETL-Prozess transformiert und in einem DWH bereitgestellt. Dieses Konzept ähnelt dem klassischen Operational Data Store (ODS), mit dem Unterschied, dass der Data Lake Einträge langfristig speichert und Daten auch als eigenständige Komponente (bspw. für explorative Analysen) bereitstellt., Wird dieser Ansatz konsequent zu Ende gedacht, verschwimmen die Grenzen immer stärker und das DWH wird ein Teil des Data Lakes (im weiten Verständnis). Der Data Lake wird hiermit zum führenden System in der Datenschicht mit einem DWH als Teilkomponente für die Bereitstellung strukturierter Daten (bspw. für dispositive Auswertungen im Controlling). Diese führende Rolle der Rohdaten, wird auch stark von Techniken wie der DWH-Automatisierung propagiert, die es ermöglichen nachgelagerte Strukturen zu jeder Zeit auf Basis der zugrundeliegenden Rohdaten neu zu generieren., Eine weitere wichtige Säule eines Data Lakes ist die Integration zahlreicher heterogener Systeme und Datenquellen., Abbildung 3 veranschaulicht drei Stufen der Integration innerhalb eines Data Lakes. In einem wenig integrierten Szenario existieren verschiedene Systeme als losgelöste Daten-Silos. Diese Entwicklung ist oft bei historisch-gewachsenen Systemlandschaften zu beobachten, in denen etwa die traditionelle DWH-Kompetenzen im Finanzwesen aufgehängt sind und sich gleichzeitig zahlreiche operative Analyse-Systeme verteilt über die Fachbereiche finden. Entsprechend gibt es viele Redundanzen und ein Mehrwert durch eine Kombination von Daten ist schwer realisierbar. Eine Integration findet hier, wenn überhaupt, manuell oder mittels komplexer ETL-Strecken statt., Als Abhilfe gibt es verschiedene partiell integrierte Konzepte. Ein gutes Beispiel hierfür ist die vor einigen Jahren stark diskutierte Lambda-Architektur, die traditionelle (eher auf Stapelverarbeitung ausgelegt) und Echtzeit-orientierte Systeme (Streaming) durch eine voraggregierte Zwischenschicht (Serving-Layer) integriert. Hierdurch ergeben sich Synergie-Effekte und neue Anwendungsfälle. Gleichzeitig ist das eigentliche Problem der Systemredundanz nicht behoben und die Komplexität in der Verwaltung der Systemlandschaft steigt sogar eher., Eine eher neuartige Art für den Umgang mit komplexen Systemlandschaften sind ereignisgetriebene Architekturen. Motiviert aus den technologischen Fortschritten aus dem Bereich der Stream-Verarbeitung, setzt dieser Ansatz auf eine asynchrone Kommunikation zwischen den Systemen über einen zentralen Vermittler (Event-Hub). Der Vorteil hierbei ist die hohe Flexibilität, da die Systeme schnell und minimalinvasiv, also ohne größere Anpassungen des Systems selbst, integriert werden können. Gerade wenn ein Data Lake nicht nur als simpler Massenspeicher genutzt wird, sondern eine ganze Reihe an verschiedenen Datenhaltungs- und Verarbeitungskomponenten umfasst, bietet sich ein solcher Ansatz an., Steht man nun vor der konkreten Herausforderung des Aufbaus eines Data Lakes, stellt sich natürlich die Frage, welches Vorgehen das passendste ist. Wie so oft gibt es auf diese Frage keine ultimative Antwort und jede Organisation hat andere Anforderungen und Umweltbedingungen. Abbildung 4 versucht trotzdem verschiedene Vorgehen einzuordnen und eine Unterstützung bei der Architekturplanung eines Data Lakes zu geben., Ein grundsätzlicher Unterschied ist es, ob eine existierende Systemlandschaft transformiert werden soll (Brownfield) oder ob eine neue Landschaft ohne Vorbelastung aufgebaut werden kann (Greenfield). Gleichzeitig spielt die Komplexität des Szenarios (ausgedrückt durch die Anzahl und die Heterogenität der Systeme, den Umfang der Datensätze, die Anzahl der Abhängigkeiten zwischen den Komponenten etc.) eine entscheidende Rolle., Bei einer existierenden hoch-komplexen Landschaft ist es ggf. ratsam nicht zu sehr in die bestandenen Strukturen einzugreifen und einen Data Lake eher mit eigenständigen parallel Strukturen aufzuziehen. Ist die Komplexität aber überschaubar, sollte man eine stärkere Integration anstreben und im besten Fall auf eine Ablösung der Altsysteme durch den Data Lake hinarbeiten, so dass der Data Lake keine weitere Insel in einem Flickenteppich von Datensystemen wird., Anders sieht es aus, wenn man auf einer grünen Wiese startet. Hier hat man weniger Einschränkungen durch Altsysteme und eher die Qual der Wahl. Befindet man sich hier in einer komplexen Umgebung, die perspektivisch noch eher umfangreicher wird, liegt es nahe direkt auf einen flexiblen Ansatz, wie bspw. eine ereignisgetriebene Architektur zu setzen. Ist die Komplexität aber überschaubar, ist es ratsam dem KISS-Prinzip zu folgen und die am operativ passendste Architektur zu wählen und bspw. einfach das Data Warehouse mit einem Massenspeicher zu ergänzen oder eine parallele Streaming-Pipeline aufzubauen., Dieser Artikel hat nur einige ausgewählte Fragestellungen herausgepickt, die sich bei einem Aufbau eines Data Lakes stellen und viele Details, wie bspw. die Frage nach der Cloud, ausgelassen. Jedem der Punkte könnte man zudem ein eigenes Kapitel in einem Buch widmen und bekäme trotzdem keine Pauschalantworten. Es zeigt sich daher wieder, dass auch bei der Architektur ein Verständnis der grundsätzlichen Anforderungen und der Motivation notwendig ist, um einen zielführenden und zukunftsfähigen Rahmen aufzubauen., Die Diskussion der Punkte zeigt zudem, dass ein moderner Data Lake, bei einem richtigen Aufbau, weit mehr als nur eine Erweiterung der Analytics-Landschaft um eine große Festplatte sein kann. Als strukturiertes Konglomerat verschiedener Datenhaltungs- und Analyse-Systeme kann er eine zentrale Rolle bei dem Umgang mit zunehmender System- und Datenheterogenität übernehmen und sich damit zum technologischen Herzstück einer zukunftsorientierten Analytics-Landschaft emanzipieren., Der nächste Teil dieser Serie beschäftigt sich mit prozessualen und organisatorischen Herausforderungen bei der Implementierung eines Data Lakes in der Praxis., 1 Baar, H. und Kemper, H. G. „Integration von Big-Data-Komponenten in die Business Intelligence“, in Controlling (27, 4-5), S. 222–228, 2015., 1 TDWI Seminar “Data-Lake-Ansätze und Best Practices”, https://www.tdwi.eu/akademie/seminarsuche/seminardetails/seminar-titel/data-lake-ansaetze-und-best-practices.html, 2 TDWI E-Book: “Der Data Lake als zentrales Element in Analytics-Architekturen“, https://www.tdwi.eu/wissen/studien-buecher/e-books/wissen-titel/tdwi-e-book-der-data-lake-als-zentrales-element-in-analytics-architekturen.html, 3 Whitepaper „Streaming-Frist Architectures”, https://www.pragmatic-apps.de/de/streaming-first-architekturen/, *Der Beitrag spiegelt die Meinung des Autors wider und ist keine allgemeingültige Meinung des TDWI. Als TDWI bieten wir die Plattform, alle Themen und Sichtweisen zu diskutieren.*, Allgemeine Fragen zum Blog, zum Verein und zum Thema AutorIn werden:Sandra SteingrubeCommunity Managersandra.steingrube(at)tdwi.eu, Fragen zum Inhalt:Leif HitzschkeTDWI e.V. Vorstandsmitgliedleif.hitzschke(at)tdwi.eu, TDWI e.V.Lindlaustraße 2cD-53842 TroisdorfTel: +49 (0)2241/2341-212, ImpressumDatenschutz;https://blog.tdwi.eu/data-lake-eine-bestandsaufnahme-in-drei-akten-architektonische-fragestellungen/;TDWI;Julian Ereth
22. Apr 20;Digitalisierung in Zeiten von Corona: BI- und Analytics-Projekte stehen auch in der Krise oben auf der Agenda;Die Umfrage des TDWI e.V. zum Thema „Digitalisierung in Zeiten von Corona – wie gehen Sie in Ihrem Unternehmen mit den aktuellen Veränderungen um?“ zeigt deutlich auf, dass BI- und Analytics-Projekte weiterhin oben auf der Agenda stehen und damit wichtige Themen im Projektgeschehen der Unternehmen sind., Darüber hinaus zeigt sich, dass die Corona-Krise für starke Veränderung in den organisatorischen Abläufen der Unternehmen sorgt., An unserer Erhebung haben 115 Mitarbeiter aus Unternehmen verschiedener Branchen aus Deutschland, Österreich und der Schweiz teilgenommen. Antworten gaben Mitglieder der obersten Unternehmensleitung ebenso wie Mitglieder der mittleren Führungsebene. Rund 50% der Teilnehmer gaben an, aus der operativen Ebene zu stammen., Das Stimmungsbild im Markt ist verhalten positiv. Über 60% der Befragten schätzen die Gesamtsituation so ein, dass innerhalb der nächsten zwei Monate zur normalen Geschäftstätigkeit zurückgekehrt werden kann., Im Prinzip stehen die Optimisten, die eine Rückkehr zur Normalität in zwei oder vier Wochen erwarten und zusammen etwa 34% ausmachen, den Pessimisten gegenüber, die eine normale Geschäftstätigkeit in mehr als 12 Wochen (13+ Wochen) erwarten und dabei 27% der Antworten sind., Zwei Drittel der Befragten, und damit die deutliche Mehrheit, sehen negative Auswirkungen auf die Geschäftstätigkeit ihres Unternehmens, wobei die Hälfte aller Antworten einen kurzfristigen negativen Effekt bedingt durch diese Krise erwarten. Nur 17% sehen langfristige negative Auswirkungen. Im Gegensatz geht jeder fünfte Proband von positiven Auswirkungen aus., Um in der Krise handlungsfähig zu bleiben, setzen, bis auf wenige Ausnahmen, alle Umfrageteilnehmer mindestens ein Web-Conferencing-Werkzeug ein., Fast die Hälfte nutzt dabei konkret Microsoft Teams, in der Häufigkeit mit Abstand gefolgt von Skype und weiterem Abstand Zoom, Webex und weiteren Tools., Über zwei Drittel der Befragten benennen gleich mehrere Werkzeuge für Online-Meetings. Telefon bzw. Telefonkonferenzen wurden nur insgesamt lediglich elfmal benannt. Ob die Ära des Telefons damit fast beendet ist oder das Kommunikationsmittel nur einfach wegen der Selbstverständlichkeit keine Erwähnung fand, bleibt in der ersten Analyse unklar., Die Befragung zeigt, dass BI- und Analytics-Projekte als nach wie vor wichtig angesehen werden und auf der Agenda bleiben. Über 50% der Befragten erwarten, dass die BI- und Analytics-Projekte wie geplant fortgesetzt werden: allerdings werden bei weiteren 40% die Projekte vorerst verschoben., Einen Projektabbruch befürchten nur 2% der Befragten (+7% Sonstiges)., Im qualitativen Teil der Befragung zeichnen die Teilnehmer der Umfrage ein sehr positives Bild vom Home-Office und der Remote-Arbeit als solches. In den Unternehmen wurde das Home-Office schnell umgesetzt und funktioniert bei den meisten Befragten besser als erwartet., Neben strukturellen Herausforderungen wie der Kaffeeverfügbarkeit, Kinderbetreuung und fehlender Bandbreite wird vor allem der fehlende persönliche Kontakt zu den Kollegen und Kunden als größte Herausforderung im Home-Office genannt. Konkret werden dabei Herausforderungen im Informationsfluss, der informellen Abstimmungen mit Kollegen und Kunden, aber auch Top-Down-Entscheidungen aufgeführt., Beratungsunternehmen sehen insbesondere große Hemmnisse im Kontext der Akquise, die längerfristige Auswirkungen haben können., Losgelöst von der detaillierteren Auswertung, die wir in den kommenden Tagen im Mitgliederbereich veröffentlichen, werden wir in einer Nachfolgeumfrage insbesondere den Aspekt der alternativen Abbildung von Kontakten und Netzwerken betrachten. Als Verein, der sich das Ziel gesetzt hat, Interessensgruppen zusammenzubringen und Austausch zu fördern, sind insbesondere die Erfolgsfaktoren bei der digitalen Umsetzung solcher Prozesse ein wichtiges Ergebnis, welches wir gerne mit Ihnen erarbeiten und teilen möchten., Die Befragung birgt gerade in den qualitativen Antworten viel Potenzial für interessante Analysen. So schätzen einige Befragte das Thema Projektfortführung sowie Projektinitiierung schlechter als erwartet ein. Mit Blick auf die Antworten zur direkten Nachfrage zu den BI- und Analytics-Projekten ergibt sich so ein Widerspruch, den es in den nächsten Wochen genauer zu untersuchen gilt., Die gesamte Analyse der Umfrage finden Sie in den kommenden Wochen imMitgliederbereich., Allgemeine Fragen zum Blog, zum Verein und zum Thema AutorIn werden:Sandra SteingrubeCommunity Managersandra.steingrube(at)tdwi.eu, Fragen zum Inhalt:Leif HitzschkeTDWI e.V. Vorstandsmitgliedleif.hitzschke(at)tdwi.eu, TDWI e.V.Lindlaustraße 2cD-53842 TroisdorfTel: +49 (0)2241/2341-212, ImpressumDatenschutz;https://blog.tdwi.eu/digitalisierung-in-zeiten-von-corona/;TDWI;Carsten Felden
15. Apr 20;Krisenzahlen;Vielleicht sind es Ihnen im Augenblick zu viele Themen, die um das Coronavirus kreisen. Aber die Eindämmung der Ausbreitung des Virus prägt unseren Alltag ja auch enorm – und das zu Recht. Der TDWI ist ein Verein, der datengetriebenes Handeln in den Vordergrund stellt und Entwicklungen in diesem Zusammenhang möchte er den Mitgliedern vermitteln. Dabei sehen wir aber immer wieder, dass die Herausforderung oftmals darin liegt, passende und stimmige Quellen zusammenzustellen, so dass wir einen Mehrwert für unsere tägliche Arbeit aus den Zahlen generieren zu können. So geht es mir in diesem Beitrag nun darum, ein bisschen jenseits aller Aufregung, zur Betrachtung von Zahlen einzuladen und eigene Schlüsse zu ziehen., In einem Meme bekam ich unlängst die Information, dass die Italiener in der Krise vor allem Zigaretten und Grappa kaufen. Das wirkte zunächst sympathisch auf mich, aber in Verbindung mit Social Distancing auch nicht wirklich stimmungshebend – es scheint dann doch eher Individualerlebnis zu sein. Glaubt man nun Statista, so hat ergänzend im Vergleich zum März 2019 in Italien die Nachfrage nach Toilettenpapier um 140% zugenommen – in Deutschland übrigens nur um 35% (so wirken die Regale gar nicht auf mich). Aber Statista selbst gibt auch noch die Information, dass die Nachfrage sich hierzulande mehr als verdreifacht hätte (211%) … Da geht es dahin, mein Vertrauen in die Zahl. Also auch offizielle Seiten hinterlassen zahlenseitig Fragen bei mir., Dazu möchte ich auf zwei interessante Links verweisen (es gibt noch viele mehr und ich würde mich ausdrücklich freuen, als Kommentar zum Blogbeitrag noch ein paar weitere von Ihnen zu erhalten). So informiere ich mich beispielsweise über Infektionsraten für Deutschland vor allem über das Robert-Koch-Institut (RKI). Einen weltweiten Überblick erhält man über die John-Hopkins Universität, die allerdings in ihren Zahlen auch Differenzen zu nationalen Zahlenwerken wie dem des Robert-Koch-Institutes hat. Am Tag des Verfassens meines Beitrages, dem 08.04.20 sind es beim RKI für Deutschland 103.228 COVID-19 Fälle, bei John-Hopkins 107.663. Nimmt man nun das RKI als Basis, so handelt es sich um eine Abweichung in Höhe von 4.435 Fällen, also etwa 4,3%. Wenn ich nun eine Quelle für mein weiteres Arbeiten suche, wer hat denn dann wie Recht?, Diese Fragestellung kennen alle, die Datenauswertung betreiben. So sind es auch hier die alten Bekannten, die uns hier wieder begegnen. An erster Stelle unterschiedliche Datenquellen, die zum Teil auch Doppelzählungen beinhalten können. Im Weiteren gibt es unterschiedliche Erhebungszeitpunkte und nicht zuletzt unterscheiden sich auch noch Definitionen. Ein informativer Artikel zu unterschiedlichen Zahlen in Bezug auf COVID-19 wurde auf der Seite von Quarks veröffentlicht. Dieser schlussfolgert: „Zahlen sind immer nur so gut und aussagekräftig, wie die Methode, mit der sie erhoben werden.“ Das gilt für die Wissenschaft und auch für die Methoden der Datenauswertung in Organisationen und Unternehmen., Und denken wir das Thema weiter: Wir wollen diese Zahlen nun nicht nur für ein Berichtswesen nutzen, sondern basierend darauf Vorhersagen treffen. Eine vom Herrn Prof. Dr. Drosten, einem dieser Tage vielzitierten Virologen der Charité, als interessant bewertete Studie kommt aus seiner Sicht zu dem Ergebnis: „Nur das Maßnahmenbündel aus Isolation erkrankter Patienten, sozialer Distanzierung (Abstand halten) in der gesamten Bevölkerung plus der Schließung von Universitäten und Schulen ist ausreichend wirksam…  … um die Kapazitäten der Intensivstationen nicht zu überlasten … … Allerdings muss man das Ganze dann für fünf Monate durchhalten“., Fünf Monate – das ist eine zunächst kleine Zahl stellvertretend für große Folgen. Zwischenmenschlich ist das vielleicht noch durch umfangreiche Alternativen wie Videotelefonie denkbar, aber wirtschaftlich ist es sicher schwerer darstellbar. Für diese fünf Monate ist es ja nun die Herausforderung, eine Vorhersage zu treffen, so dass ich mein Unternehmen bzw. meine Geschäftstätigkeit darauf ausrichten kann. Welche Zahlen brauche ich nun und woher bekomme ich diese? Ich muss unterschiedliche Szenarios durchdenken (und damit vorhersagen), so dass ich mein Risiko einschätzen und steuern kann. Mit düsterer Perspektive hat das ifo-Institut beispielsweise solche Szenariorechnungen betrieben und dabei eine unterschiedliche Beeinflussung der Produktivität unterstellt. Es gibt die Gruppen:, Zudem werden die Dauer des Shutdown, die Dauer der Post-Shutdown-Phase zur Erholung und auch die Stärke der Auswirkung selbst variiert. Die Ergebnisse der resultierenden 18 Szenarios reichen bei einem einmonatigen Shutdown von einer Reduktion des BIP um 4,3% in 2020 bis zum dreimonatigen Shutdown und einer Reduktion des BIP um 20,6% für 2020. Der Shutdown von 5 Monaten wurde als Szenario nicht bewertet. , Welche Zahlen zählen in einer solchen Situation? Und da sehen wir die Einseitigkeit, die trotz aller Standardisierungsansätze und Prozessdefinitionen immer vorhanden ist. Es entscheidet derjenige, der die Analyse aufsetzt. Für viele geschäftlich individuell zusammengestellte Daten muss die jeweilige Organisation dann für sich bewerten, welche Subjektivität ein Problem darstellt. In unserer aktuellen Situation und in dieser Krise stehen – zumindest für mich – klar die Opfer und die potentiellen Opfer der Pandemie im Vordergrund. Hier betreffen Entscheidungen Länder mit ihren Menschen und dann ist es umso wichtiger, dass man sich auf die Datenanalyse verlassen kann und sich dann die richtigen Maßnahmen ableiten lassen., Ich wünsche mir einen aussagekräftigen Informationsstand unter Abstimmung der Definitionen und Werte. Also die Informationen, welche die Szenarios zusammen darstellen und auch die gleichen Annahmen treffen. Ich würde mir zudem wünschen, dass wissenschaftliche Erkenntnisse an einer gemeinsamen Stelle kommentiert werden. Im Kopf habe ich dazu kollaborative BI-Dashboards. So möchte ich Sie heute auf einen kostenfreien TDWI-Akademie-Webcast hinweisen, in dem wir das im Detail vorstellen. Herstellerneutral, aber anhand von Beispielen. Vielleicht eine Anregung, die Sie mitnehmen können in den Alltag – vor und nach dem Shutdown., Bleiben Sie gesund und arbeiten Sie mit daran, dass Zahlen etwas wert sind – weil das Ergebnis zählt!, Das RKI gibt selber zu, dass seine Zahlen vielfach verspätet sind. Eine „Wochenend-Delle“ bei Neuinfektionen ist sachlich auch eher unwahrscheinlich. Wir alle als TDWI/Datenenthusiasten hätten gerne bessere Zahlen. Nach Neufassung des Infektionsschutzgesetzes 2014 hat das RKI das Projekt DEMIS initiiert. Hier geht es darum die unterschiedlichen Meldewege und -verfahren zu vereinheitlichen und zu beschleunigen. Leider gibt es 5 Jahre später noch keine Ergebnisse und die Zeit, JHU etc. rufen lieber selber in den Landkreisen an – eine Posse.Was mit den vorhandene (JHU) Zahlen gemacht werden kann, ist zum Beispiel aus „Neuinfektionen“, „genesenen“ und „verstorbenen“ eine (ungefähre) Zahl der „aktiven“ Infektionen zu errechnen. Und diese ist in den letzten Tagen (Stand 15.4.) rückläufig., Danke für den Kommentar! Ja, auch die Prozesskette der Datenerhebung ist hier ganz klar ein Thema und auch das kennen wir als Datenenthusiasten – danke für den schönen Begriff in dem ich mich auch subsummiert fühle – aus dem täglichen Umgang mit Daten!, Interessant sind auch diese Plots (http://www.dkriesel.com/corona) von David Kriesel, da sie das prozentuale Wachstum der Fälle visualisieren. Ebenfalls schön ist diese Einsicht dort auch in die anderen Länder wie ich finde., Allgemeine Fragen zum Blog, zum Verein und zum Thema AutorIn werden:Sandra SteingrubeCommunity Managersandra.steingrube(at)tdwi.eu, Fragen zum Inhalt:Leif HitzschkeTDWI e.V. Vorstandsmitgliedleif.hitzschke(at)tdwi.eu, TDWI e.V.Lindlaustraße 2cD-53842 TroisdorfTel: +49 (0)2241/2341-212, ImpressumDatenschutz;https://blog.tdwi.eu/krisenzahlen/;TDWI;Claudia Koschtial
08. Apr 20;"Rückblick zum TDWI Afterwork Event „KI unter der Haube“ powered by Young Guns &amp; INFOMOTION";Am 26. Februar luden die TDWI Young Guns, das AI Meetup Stuttgart und INFOMOTION zum ersten TDWI Afterwork Event in Stuttgart ein, um gemeinsam mehr über das Thema Künstliche Intelligenz zu lernen. Über 50 Teilnehmer lauschten den drei Experten-Vorträgen und diskutierten in einer lockeren Atmosphäre die Funktionsweise von KI und Anwendungen in der Praxis., Nach einer kurzen Einführung zeigte Saskia wie KI schon heute in der Praxis genutzt wird. Durch zahlreichen Beispielen aus dem Alltag zeigte sich, dass ganz oft KI in Produkten steckt, in denen man es nicht erwartet. Gleichzeitig bremste Saskia aber auch die Erwartungen etwas und setzte steile Thesen zu einer Machtübernahme in einen realistischen Kontext., Direkt im Anschluss nahm Philipp den Faden auf und ergänzte den theoretischen Teil des Abends. Konkret war das Ziel seines Beitrags die Funktionsweise eines neuronalen Netzwerks live an einem Beispiel zu erläutern. In Excel baute Philipp hierfür schrittweise ein neuronales Netzwerk auf und erklärte dabei die relevanten Begriffe der verschiedenen Schichten und die wichtige Rolle des Parameter-Tunings. Das gelang auch wunderbar – obwohl die Zahlenkonstrukte auf den ersten Blick abschreckend aussahen., Nach einer kurzen Erfrischungspause mit Snacks und Getränken vertiefte Tobias mit seinem Beitrag zu „Evolutionary AI“ die vorherigen Themen. Er zeigte hierfür verschiedene Möglichkeiten des Model-Trainings und ging auf die Vorteile eines evolutionären Ansatzes ein (der tatsächlich auf die Evolutionstheorie zurückgeht). Um das Ganze anschaulicher darzustellen, nutze er dann eine einfache AI, die sich eine Version des beliebten Handyspiels „Flappy Bird“ selbst beibrachte und dabei immer besser wurde., Bei einem abschließenden Feierabendbier gab es dann einen angeregten Austausch zum aktuellen Stand und zukünftigen Entwicklungen im KI-Sektor und Interessierte kamen in den Genuss einer Führung durch das Innovation Lab der Firma INFOMOTION. Bis gut 22 Uhr saßen die Teilnehmer anschließend noch zusammen und schmiedeten schon Pläne für das nächste TDWI Afterwork Event., Allgemeine Fragen zum Blog, zum Verein und zum Thema AutorIn werden:Sandra SteingrubeCommunity Managersandra.steingrube(at)tdwi.eu, Fragen zum Inhalt:Leif HitzschkeTDWI e.V. Vorstandsmitgliedleif.hitzschke(at)tdwi.eu, TDWI e.V.Lindlaustraße 2cD-53842 TroisdorfTel: +49 (0)2241/2341-212, ImpressumDatenschutz;https://blog.tdwi.eu/tdwi-afterwork-event-ki-unter-der-haube/;TDWI;Sandra Steingrube
01. Apr 20;Business Intelligence meets Data Science: Einschätzungen aus einem Expertengespräch (2/2);"Am 30.1.2020 haben wir im Rahmen der Meetup-Gruppe „Business Intelligence &amp; Analytics Dortmund“ 1 eine Podiumsdiskussion zum Thema „Business Intelligence meets Data Science“ durchgeführt, an der zwei Experten als Vertreter der jeweiligen Fachdisziplin teilgenommen haben und die der Autor dieses Beitrags moderieren durfte. 2 Dieser Blogbeitrag beschreibt den abschließenden zweiten Teil des Gesprächs. Hier geht’s zum ersten Teil., Prof. Dr. PeterGluchowski leitet den Lehrstuhl für Wirtschaftsinformatik, insb.Systementwicklung und Anwendungssysteme, an der Technischen Universität inChemnitz und konzentriert sich dort mit seinen Forschungsaktivitäten auf dasThemengebiet Business Intelligence. Er beschäftigt sich seit rund 20 Jahren mitFragestellungen aus dem Bereich Business Intelligence und war lange Jahre imVorsitz des TDWI Germany e.V., den er mitbegründet hat. 3, Dr. Martin Schmitzhat in Dortmund Physik studiert, hierzu promoviert und ist seit 2014 bei derFirma RapidMiner Inc. Beschäftigt, nimmt dort mittlerweile die Rolle des „Headof Data Science Services“ ein und steht damit in direktem Austausch mitAnwenderunternehmen. RapidMiner wurde von Gartner 2019 zum sechsten Mal inFolge in den Magischen Quadranten für „Data Science and Machine LearningPlatforms“ aufgenommen. 4, 5, Im Folgenden möchten wir einige Aspekte der Diskussionherausgreifen und die Meinung der Experten in komprimierter Form vorstellen. DerFokus liegt dabei auf dem Ergebnis der Diskussion, nicht darauf, wer was genaugesagt hat. Klarstellend sei betont, dass es sich dabei um persönlicheEinschätzungen und Meinungen als Momentaufnahme der Diskussion handelt; vieleFragestellungen des Zusammenwachsens von Business Intelligence und Data Sciencebefinden sich – wie überhaupt die gesamte Landschaft rund um Data &amp;Analytics – weiterhin im Fluss und sind entsprechend kontinuierlichenVeränderungen unterworfen., Wem gehören denn dieDaten, wer ist für diese zuständig?, Die Bedeutung der Daten – und damit einhergehend auch dieFrage, wem diese gehören – kann man gar nicht überbetonen. Tatsächlich stelltsich hier häufig die Frage, wer überhaupt genug Daten besitzt bzw. gewinnen kann,um diese vorteilhaft auszuwerten. Gute Beispiele für Unternehmen, die sichdarauf verstehen, Daten zu sammeln und zu kapitalisieren sind Facebook undGoogle., Hingegen wird es häufig nicht ausreichen, wenn einmittelständisches Unternehmen zwei Maschinen eines Typs betreibt, um hierausbesondere Erkenntnisse zu gewinnen. Dennoch kann dies für den Hersteller derMaschinen ganz anders aussehen, schließlich verfügt dieser ja über einedeutlich größere installierte Basis. An dieser Stelle stellt sich insofernschnell die Frage, wem die Daten überhaupt gehören und es ist daher auch nichtüberraschend, dass aktuell viele Verträge in dieser Hinsicht angepasst werden, umklar zu regeln, wer die Daten besitzt bzw. benutzen kann., Ein anschauliches, anderes Beispiel sind Flugzeuge, derenDaten sowohl Airlines als auch die Hersteller der Flugzeuge auszuwertenversuchen um beispielsweise möglichst klug Wartungsarbeiten einzuplanen. Oderdie von Autos generierten Daten beim autonomen Fahren., Und wer innerhalb einer Organisation ist für dieDaten verantwortlich?, Hier gibt es aus dem Bereich Business Intelligence kommendbereits lange das Prinzip des Data Owners: Der Date Owner ist für die ihmzugeordneten Daten entscheidungsbefugt, zum Beispiel in Hinblick auf dasLöschen von Daten., Dies ist ein wichtiger Aspekt, der durch die DSGVO erstmaligverbindlich an Bedeutung gewonnen hat. Vor der DSGVO war es üblich, Daten immerweiter anzusammeln. Mit der DSGVO ist erstmalig eine Verpflichtung in Kraftgetreten, gewisse Daten zu löschen., Data Ownership regelt, wer hierfür die Verantwortung trägt., Kommen wir nochmal zuunserem Ausgangsthema zurück, dem Spannungsverhältnis zwischen Data Science undBusiness Intelligence. Lassen sich beide Disziplinen vereinigen?, Es gibt durchaus Verfechter des Ansatzes, dass man BusinessIntelligence breiter verstehen sollte und Data Mining ein Bestandteil vonBusiness Intelligence ist, letztlich also mit heutigen Worten auch DataScience. Im Extremfall hört man, Data Science sei einfach „besonders komplexe“Business Intelligence., Diese Sichtweise birgt aber auch ein großes Risiko, da dieMentalitäten komplett verschieden sind. Vereinfacht gesprochen, geht es beiBusiness Intelligence darum, dass Menschen aus Daten und ihrer Visualisierunglernen, bei Data Science hingegen um das Maschinenlernen. , Das menschliche Lernenund das Maschinenlernen sind hier grundlegend verschieden., Für Menschen ist wichtig, die richtigen Daten in geeigneterForm präsentiert zu bekommen, die eigentliche Entscheidung bzw. der Lernprozesswir dann aber angereichert um menschliches Wissen und Erfahrung. Häufig wird esgenutzt um einzigartige Entscheidungen datenunterstützt zu treffen., Beim Maschinenlernen wird ein Algorithmus auf ein Muster inden Daten trainiert, das dazu existieren muss und eine gewisse Stabilität imZeitablauf aufweisen muss. Auf neuen Daten lässt sich diese Musterkennunganwenden, z. B. um Betrugsfälle zu erkennen. Meist wird das Verfahren dann füreine große Anzahl Daten angewandt um eine Vielzahl strukturell gleicherEntscheidungen automatisiert oder semi-automatisiert zu treffen., Für Data Scientists ist es wichtig, Algorithmen zuvalidieren 6. Daten werden in Trainings- und Testmengen zerlegt, ein Modellauf Basis von Trainingsdaten trainiert und anschließend die Güte mit Hilfe vonTestdaten validiert. Im Bereich Business Intelligence gibt es keineEntsprechung hierfür, da die Entscheidung durch Menschen getroffen wird, nichtmaschinell; in diesem Sinn lässt sich Business Intelligence nicht validieren., Auf der anderen Seite ist Business Intelligence konzeptionellviel einfacher zu verstehen und braucht keine besondere Erklärung: Letztlichwerden Daten ausgelesen, geeignet verarbeitet und anschließend in einem Berichtoder Dashboard visualisiert. Dieses Prinzip erschließt sich Menschenunmittelbar, im Grunde bleibt nur die Frage der Datenquellen und genauenVorverarbeitungsschritte., Im Bereich Data Science lernen Algorithmen typischerweise multivariateZusammenhänge in Daten. Diese sind per se nicht einfach zu beschreiben: fürMenschen erschließt sich vielfach nicht konkret, was genau der Algorithmusgelernt hat, da sich dies aufgrund der hohen Dimensionalität (z. B. 500) nicht anschaulichbeschreiben lässt., Sehr plakativ istBusiness Intelligence also näher am menschlichen Lernen, Data Science istMaschinenlernen. Schließen sich das menschliche Lernen und maschinelles Lernendenn gegenseitig aus?, Zunächst mal ist datenbasierendes, menschliches Lernensicherlich ein Erfolgsrezept, insbesondere seit der Renaissance. BusinessIntelligence steht in dieser Tradition. Hingegen ist Data Science und dasmaschinelle Lernen ziemlich jung, Data Science ist der Neuling, der etwasanders macht., Gelegentlich wird gesagt, Data Science ersetze Kausalitätdurch Korrelation; es wird in Daten ein Zusammenhang zwischen Eingangsparameternund einer Ausgangsgröße als Muster erkannt, ohne dass ein wirklicherUrsache-Wirkungs-Zusammenhang geschlossen wird. Dies erschließt sich einemMenschen nicht mehr ohne Weiteres: es ist schwierig, aus einem solchenerkannten Muster menschlich zu lernen., Noch extremer kann man die Position einnehmen, dassmaschinelles Lernen und automatisierte Entscheidungen in letzter Konsequenzdazu führen könnten, dass der Mensch an Wissen verliert. Werden Entscheidungenvollautomatisiert durch Maschinen getroffen, könnte das Wissen über dieZusammenhänge für Menschen verloren gehen, der Mensch würde dem Computer danneinfach blind vertrauen., Dem steht der unter anderem der Trend „Explainable AI“entgegen, in den derzeit große Hoffnungen gesetzt werden 7. Die Idee isthierbei, das maschinell Gelernte erklärbar zu machen, so dass es sich vonMenschen nachvollziehen lässt und entsprechend auch zu menschlichem Lernenführt., Man kann auch entgegen, dass sich menschliches undmaschinelles Lernen häufig sehr gut ergänzen. Zum Beispiel geschieht dies beimSchach oder GO: Wirkliche Schachprofis machen ihre Vorbereitung längstcomputergestützt und sind hierdurch auch deutlich besser geworden. Im BereichGO haben Computeralgorithmen Lösungsstrategien entwickelt, die von Menschenbislang nicht eingesetzt wurden – dann aber schnell auch von Menschenaufgegriffen worden. , Im Grunde sollte man die beiden Disziplinen nichtgegeneinander ausspielen. Es gibt Situationen, in der eine sehr große Masse anEntscheidungen getroffen werden muss, zum Beispiel beim Aufdecken vonpotenziellem Kreditkartenbetrug. Hier ist klar, dass maschinelle Verfahreneingesetzt werden müssen. Genauso gibt es Einzelfallentscheidungen, die inhohem Maße auf menschlichem Wissen basieren und wo Daten allenfalls zurUnterstützung herangezogen werden können. Wiederum gibt es Situationen, indenen Maschinen Vorschläge für Entscheidungen generieren könne, dieschlussendlich von einem Menschen verifiziert werden; diese Situation findetman bspw. häufig in medizinischen Anwendungen. , Und es gibt sehr viele Situationen, in denen die perfekteKombination aus Mensch und Maschine besteht, wo man also die jeweiligen Stärkenmiteinander kombiniert., Was gibt es fürSchnittstellen zwischen Data Science und Business Intelligence?, Tatsächlich gibt es sehr häufig ganz einfacheSchnittstellen. Zum Beispiel generiert maschinelles Lernen vielfach Scores,beispielsweise eine Abschätzung der Kündigungswahrscheinlichkeit von Kunden.Auch wenn gelegentlich automatisiert auf solche Scores reagiert werden kann,ist eine häufige Anwendung einfach darin, den Score als Attribut zum Kunden imData Warehouse zu speichern. Von hier ausgehend wir der Score dann geeignet anden Fachanwender transportiert, sei es in diesem Beispiel über das CRM-Systemoder eine Anzeige in speziellen Berichten oder Dashboards. Viele Deployments imBereich Data Science sind unspektakulär und liefern einfach nur Daten, diewieder in das Data Warehouse zurückfließen., Und wie können sichData Science und Business Intelligence sonst noch ergänzen?, Selbstverständlich können beide Disziplinen sehr viel durchdie Zusammenarbeit voneinander profitieren. Wie bereits angesprochen, findensich im Bereich Data Science häufig Einsteiger ohne lange Berufserfahrung.Natürlich können diese von den Erfahrungen „alter Hasen“ aus dem BereichBusiness Intelligence profitieren, insbesondere deren Erfahrungen im Netzwerkenin der Organisation. Besonders wichtig ist auch die Nähe zu den Fachabteilungen,die für beide Disziplinen sehr wichtig ist., Eine Idealvorstellung könnte sein, dass beide Disziplinen inihren Teams die jeweilige Besonderheiten wahren und sich spezialisieren können,aber in einer übergreifenden Einheit auch die Austausch gefördert wird, so dassman voneinander profitieren und lernen kann, zumal sich inhaltlich auchÜberlappungen in gewissen Tätigkeiten – zum Beispiel dem Data Engineering –ergeben können., Aber vermutlich wird es noch eine gewisse Zeit dauern, bissich hier eine Art „best practice“ für die Organisation etabliert., 1 https://www.meetup.com/de-DE/Business-Intelligence-und-Analytics-Dortmund, 2 https://www.meetup.com/de-DE/Business-Intelligence-und-Analytics-Dortmund/events/266751314/, 3 https://www.tu-chemnitz.de/wirtschaft/wi2/wp/de/team/prof-dr-peter-gluchowski/, 4 https://www.linkedin.com/in/martin-schmitz-03886a94/, 5 https://rapidminer.com/resource/gartner-magic-quadrant-data-science-platforms/, 6 https://rapidminer.com/resource/correct-model-validation/, 7 https://en.wikipedia.org/wiki/Explainable_artificial_intelligence, Allgemeine Fragen zum Blog, zum Verein und zum Thema AutorIn werden:Sandra SteingrubeCommunity Managersandra.steingrube(at)tdwi.eu, Fragen zum Inhalt:Leif HitzschkeTDWI e.V. Vorstandsmitgliedleif.hitzschke(at)tdwi.eu, TDWI e.V.Lindlaustraße 2cD-53842 TroisdorfTel: +49 (0)2241/2341-212, ImpressumDatenschutz";https://blog.tdwi.eu/business-intelligence-meets-data-science-einschaetzungen-aus-einem-expertengespraech-2-2/;TDWI;Gero Presser
25. Mrz 20;Rückblick auf den TDWI Roundtable Rhein-Neckar;Am 26. Februar 2020 fand das 6. Treffen des TDWI Roundtable Rhein Neckar statt. Wie schon des Öfteren durften wir die Räumlichkeiten des paritätischen Forums am Park in Heidelberg nutzen. Die Organisation im Vorfeld verlief reibungslos, weswegen auch 24 Teilnehmer den Weg zum Roundtable fanden. Vielen Dank an Tanja Kenda vom TDWI für die Organisation., In einem sehr gut gefüllten Raum begrüßte Roland Mannshardt alle Teilnehmer und begann mit einer kurzen Vorstellung des TDWI, dessen Aktivitäten und den Möglichkeiten, die eine Mitgliedschaft bietet., Danach übergab er das Wort an Dennis Maier, Data Scientist bei der CAIRO AG. Mit großem Elan stieg dieser in das Thema „Bildanalyse via Neuronaler Netze“ ein. Zuerst schilderte er den Schmerz des Kunden, für den die Lösung entwickelt wurde. Im Großkraftwerk Mannheim ruhen 1.000.000 technische Zeichnungen in Form von Tif-Dateien, welche keinerlei Zusatzinformationen bieten. Daher sind diese Dokumente durch eine Suche nicht im Dokumentenmanagement durchsuchbar und somit quasi unnütz. Nach der Vorstellung des für diese Problemstellung entwickelten Services mit einer Klassifizierung, einer Texterkennung und Objekterkennung ging es ans Eingemachte., Als erstes wurden die Begrifflichkeiten „AI“ und „machine learning“ definiert und gegenüber zur normalen Softwareentwicklung abgegrenzt. Danach wurde es technisch. Dennis erklärte sehr anschaulich was ein „classifier“ ist und wie dieser basierend auf einem neuronalen Netz funktioniert.  Durch verschieden Verfahren, welche im Detail erklärt wurden, gelang es Dennis ein sehr gutes Bild der Arbeitsweise zu zeichnen. Hierbei plauderte er auch aus dem Nähkästchen und gab bereitwillig Auskunft über den Aufbau des eigens hierfür entwickelten neuronalen Netzes und die Bildbearbeitungsverfahren.  Auch bei der Texterkennung werden eigene Verfahren genutzt, welche Dennis den Teilnehmenden erklärte. Einen weiteren Mehrwert bot am Ende noch der Bericht über die Erfahrungen aus der Entwicklung und dem Projektmanagement. Hierbei unterstütze Andreas Hoinisch, Projektmanager bei der CAIRO AG, mit einigen zusätzlichen Informationen., Die Fragen und Antworten am Ende und während des Vortrags waren zahlreich und tiefgehend. Die Anwesenden zeigten sowohl großes Interesse an den technischen Details als auch an den wirtschaftlichen Aspekten des Projekts. Die Chance zum Networking wurde im Anschluss rege genutzt., Die nächsten Roundtables finden voraussichtlich am Dienstag, dem 09.06.2020 an der Hochschule Karlsruhe und am Mittwoch, dem 11.11.2020 in Heidelberg statt., Andreas Hoinisch und Roland Mannshardt, Allgemeine Fragen zum Blog, zum Verein und zum Thema AutorIn werden:Sandra SteingrubeCommunity Managersandra.steingrube(at)tdwi.eu, Fragen zum Inhalt:Leif HitzschkeTDWI e.V. Vorstandsmitgliedleif.hitzschke(at)tdwi.eu, TDWI e.V.Lindlaustraße 2cD-53842 TroisdorfTel: +49 (0)2241/2341-212, ImpressumDatenschutz;https://blog.tdwi.eu/rueckblick-auf-den-tdwi-roundtable-rhein-neckar/;TDWI;Roland Mannshardt
18. Mrz 20;Update zu Entwicklungen rund um SARS-CoV-2;Liebe TDWI-Community,liebe Leserinnen und Leser des TDWI Blogs,, da die Gesundheit, Sicherheit, und das Wohlbefinden unserer Mitglieder, Teilnehmer, Sprecher, Partner und Mitarbeiter für uns oberste Priorität hat und sich die Entwicklung und Empfehlungen in Bezug auf das Corona-Virus stetig verändern, möchten wir Sie heute über den aktuellen Stand unserer Vereinsaktivitäten informieren., Wir als TDWI e.V. folgen den behördlichen Empfehlungen und Anweisungen und sagen alle geplanten Präsenzveranstaltungen, wie TDWI Roundtable, Arbeitskreise und BarCamps vorerst bis Ende April ab. , Um Ihnen nach Möglichkeit trotzdem hochwertige Inhalte bereitzustellen und eine Möglichkeit zur Diskussion zu geben, sind wir aktuell mit den Roundtable-Vorsitzenden im Austausch, ob und zu welchem Zeitpunkt wir Online-Angebote im Sinne der Roundtable anbieten können. Natürlich wird das Netzwerken anders sein, als vor Ort. Wir sehen hier aber eine große Chance für uns als TDWI, unserer Mission des Wissensaustauschs und Vernetzung gerecht zu werden., Diesen Blog möchten wir natürlich weiterhin pflegen und werden so auch in den nächsten Wochen daran arbeiten, spannende Themen mit Ihnen zu teilen. An der ein oder anderen Stelle werden wir auch auf vergangene Events zurückblicken., Melden Sie sich bei uns, wenn Sie für den TDWI Blog schreiben möchten., Darüberhinaus möchten wir Ihnen folgende Links zur Verfügung stellen: , Bei weiteren Fragen rund um die aktuelle Situation sowie den TDWI, freuen wir uns, wenn Sie uns schreiben., Mit besten GrüßenIhr TDWI e.V., Allgemeine Fragen zum Blog, zum Verein und zum Thema AutorIn werden:Sandra SteingrubeCommunity Managersandra.steingrube(at)tdwi.eu, Fragen zum Inhalt:Leif HitzschkeTDWI e.V. Vorstandsmitgliedleif.hitzschke(at)tdwi.eu, TDWI e.V.Lindlaustraße 2cD-53842 TroisdorfTel: +49 (0)2241/2341-212, ImpressumDatenschutz;https://blog.tdwi.eu/update-zu-entwicklungen-rund-um-sars-cov-2/;TDWI;Sandra Steingrube
11. Mrz 20;Business Intelligence meets Data Science: Einschätzungen aus einem Expertengespräch (1/2);"Am 30.01.2020 haben wir im Rahmen der Meetup-Gruppe „Business Intelligence &amp; Analytics Dortmund“ 1 eine Podiumsdiskussion zum Thema „Business Intelligence meets Data Science“ durchgeführt, an der zwei Experten als Vertreter der jeweiligen Fachdisziplin teilgenommen haben und die der Autor dieses Beitrags moderieren durfte. 2 Dieser Blogbeitrag beschreibt den ersten Teil des Gesprächs., Prof. Dr. Peter Gluchowski leitet den Lehrstuhl für Wirtschaftsinformatik, insb. Systementwicklung und Anwendungssysteme, an der Technischen Universität in Chemnitz und konzentriert sich dort mit seinen Forschungsaktivitäten auf das Themengebiet Business Intelligence. Er beschäftigt sich seit rund 20 Jahren mit Fragestellungen aus dem Bereich Business Intelligence und war lange Jahre im Vorsitz des TDWI e.V., den er mitbegründet hat. 3, Dr. Martin Schmitzhat in Dortmund Physik studiert, hierzu promoviert und ist seit 2014 bei derFirma RapidMiner Inc. Beschäftigt, nimmt dort mittlerweile die Rolle des „Headof Data Science Services“ ein und steht damit in direktem Austausch mitAnwenderunternehmen. RapidMiner wurde von Gartner 2019 zum sechsten Mal inFolge in den Magischen Quadranten für „Data Science and Machine LearningPlatforms“ aufgenommen. 4, 5, Im Folgenden möchten wir einige Aspekte der Diskussionherausgreifen und die Meinung der Experten in komprimierter Form vorstellen. DerFokus liegt dabei auf dem Ergebnis der Diskussion, nicht darauf, wer was genaugesagt hat. Klarstellend sei betont, dass es sich dabei um persönlicheEinschätzungen und Meinungen als Momentaufnahme der Diskussion handelt; vieleFragestellungen des Zusammenwachsens von Business Intelligence und Data Sciencebefinden sich – wie überhaupt die gesamte Landschaft rund um Data &amp;Analytics – weiterhin im Fluss und sind entsprechend kontinuierlichenVeränderungen unterworfen., Eingangs möchten wirdie Frage betrachten, wie man überhaupt zu einem Experten bzw. Vertreter derjeweiligen Disziplin wird., Was ist ein Data Scientistbzw. wie wird man dies? , Man muss hier in der Realität zunächst stark unterscheidenzwischen dem, was bei Vorreitern wie Facebook oder Netflix gemacht wird unddem, was typischerweise Data Scientsts bei deutschen Firmen vorfinden. Firmenwie Netflix sind unglaublich datengetrieben, dort ist jedem klar, wieentscheidend Daten und Algorithmen für den Geschäftserfolg sind, zum Beispielkonkret die Bedeutung des Empfehlungssystems (Recommender Engine) für denwirtschaftlichen Erfolg. Dies ist tief in der Denkweise und Kultur verankert,diese Unternehmen denken datengetrieben, teilweise „AI First“. Hingegen findetman bei typischen deutschen Firmen eine ganz andere Kultur vor., Die meisten Data Scientists haben eineingenieurwissenschaftliche Ausbildung, zum Beispiel landen viele Physiker indiesem Bereich. Tatsächlich ist aber der konkrete Einstieg in den Beruf eine echteHerausforderung. Das sieht man zum Beispiel gut daran, dass unterReddit/DataScience 6 rund ein Drittel der Beiträge die Frage behandeln, wieman den Einstieg schafft, konkret den ersten Job als Data Scientist findet.Nach dem Einstieg sammelt man dann Erfahrungen und entwickelt sich weiter., Vielfach wird ein etwas gefährliches Bild des DataScientists vermittelt. Der idealtypische Data Scientist kann sich mit tiefgehendemInformatikwissen, methodischen Wissen und zudem fachlichem Expertenwissen inProblemstellungen eindenken und mit guten Moderationsfähigkeiten seine Ansätzeerfolgreich verkaufen. Das scheint aber eher eine Idealisierung zu sein; manschiebt hier quasi alles, das man sucht, in die Rolle des Data Scientist und wundertsich hinterher, dass real existierende Menschen diese Rolle nicht ausfüllenkönnen., Tatsächlich muss auch nicht jeder Data Scientist beliebigtief in der Theorie stecken, beispielweise Backpropagation runterdeklinierenmüssen. Gerade mit Self-Service Data Science Werkzeugen wie RapidMiner wird esmöglich, talentierte Mitarbeiter aus Fachabteilungen in Richtung „Citizen DataScientist“ zu entwickeln., Und wie „landet“ manim Bereich Business Intelligence? , Das Thema Business Intelligence ist an Hochschulen nicht sonderlich hoch aufgehangen, egal in welchem Studiengang. Allerdings werden Grundzüge durchaus im Bereich Informatik und Wirtschaftsinformatik behandelt. An der TU Chemnitz ist dies anders, dort gibt es tatsächlich einen eigenen Studiengang zu dem Thema. Hierauf gibt es sehr positive Resonanz von Anwendungsunternehmen und Beratungshäusern, die händeringend Mitarbeiter im Bereich Business Intelligence suchen. , Im Gegensatz zu dem relativ neuen Thema Data Science gibt esum Bereich Business Intelligence Fachexperten, die seit dutzenden Jahrenerfolgreich in dem Thema arbeiten. Theoretisch lassen sich solche Expertenfinden, allerdings ist das Gehaltsniveau auch entsprechend hoch., Mitbringen sollte man sicherlich gesundesDatenbankverständnis, wobei sich auch der Bereich Business Intelligence immermehr ausdifferenziert hat. So gibt es mittlerweile auch viele im Frontendtätige Entwickler, die vor allem Wissen um Usability bzw. User Experience undDesign mitbringen sollten. Im Backend sollte man sich nicht zu schade sein,Code zu schreiben und grundsätzlich komplexe Strukturen durchdringen zu können.Eine Affinität zu Daten und Datenstrukturen ist in jedem Fall wcihtig., Es schließt sich dieFrage an, wie das jeweilige Thema organisatorisch üblicherweise inOrganisationen verankert ist., Business Intelligenceist üblicherweise ein Kompetenzzentrum, konkret das „Center of CompetenceBusiness Intelligence“ bzw. in Kurzform „CoC BI“. , Das CoC BI ist dann entweder einem Fachbereich zugeordnet,häufig dem Controlling. Oder es ist in der IT angesiedelt. Je nach derAufhängung unterscheiden sich dann auch die Aufgaben und Schwerpunkte derTätigkeit., Insgesamt kann man aber sagen, dass die Organisationüblicherweise klar ist und man hier auf langjährige Erfahrungswertezurückblicken kann. Man weiß, welche Skills benötigt werden, welche Profilegesucht werden und wie die Aufgaben zu organisieren sind, insgesamt gibt es zurOrganisation vergleichsweise wenig Diskussionsbedarf., Data Science hingegenist als Thema neuer, entsprechend weniger etabliert ist die Organisation.Auch hier findet man immer wieder Kompetenzzentren oder Center-of-ExcellenceAnsätze, ebenso eine Zuordnung zum CIO oder CFO., Ein durchaus signifikantes Problem tritt auf, wenn das Team„disconnected“ von den eigentlichen Fachanwendern mit dem zu lösenden Problemsind.So kann es in produzierenden Unternehmen zu Situationen kommen, wodas Data Science-Team fernab des Werks tätig ist und es dann entsprechendschwer hat, mit den Ingenieuren im Werk effektiv zusammenzuarbeiten. Die Nähezur Fachabteilung ist wesentlich für den Projekterfolg., Bemerkenswert ist, dass viele Data Scientists Anfänger sind.Die Ursache ist einfach, selbst wenn die Studiengänge zu dem Thema in denletzten Jahren aus dem Boden spießen, gibt es sie doch noch nicht all zu lange.Im Gegensatz zu Business Intelligence findet man keine Mitarbeiter mit 20Jahren Erfahrung. Tatsächlich wirkt sich dies teilweise auf die Organisationaus, da viele Data Scientists neu in Unternehmensstrukturen sind und noch nichtüber die Erfahrung „alter Hasen“ verfügen. Insgesamt konnten organisatorischeFragen hier noch nicht so lange reifen wie im Bereich Business Intelligence., Business Intelligencehat sich seit längerer Zeit in Unternehmen etabliert. Wie kommt es, dass sichData Science daneben als eigene Disziplin etablieren konnte?, In der Tat wurden natürlich schon immer Daten genutzt umProzess zu verbessern und viele Algorithmen, die im Bereich Data Scienceeingesetzt werden, sind nicht neu., Was sich allerdings dramatisch verändert hat, ist die Mengeder verfügbaren Daten. Dies sieht man zum Beispiel am Beispiel eines Autos, dasmittlerweile in Echtzeit sehr große Datenmengen in die Cloud schickt, dieanschließend für Auswertungen genutzt werden können., Insgesamt sind mit den verfügbaren Datenmengen auch unsereMöglichkeiten drastisch gewachsen, diese zu speichern und zu verarbeiten. Nehmenwir das Beispiel Fabriken: selbst mittelständische und kleine Unternehmenverfügen häufig über Devices, mit Hilfe derer Maschinendaten in Echtzeit in dieCloud übertragen werden., Mit der Verfügbarkeit dieser Daten und dem sichtbaren Erfolgvon Vorreitern wird es übersichtlich, dass Data Science zum Unternehmenserfolgbeitragen kann, hier aber anders verfahren werden muss als im traditionellenThema Business Intelligence., Es geht also in hohemMaße um die Verfügbarkeit großer Datenmengen und den damit einhergehenden neuenAnalysemöglichkeiten. Früher hatte ja typicherweise das Data Warehouse dieHoheit über die Daten. Wie sieht das heute aus?, Das ist sicherlich richtig, wobei sich Business Intelligenceund das Data Warehouse traditionell auf strukturierte Daten beschränkt haben.Heute sind aber in hohem Umfang auch semistrukturierte und unstrukturierteDaten verfügbar, zudem Massendaten (wie z. B. Sensordaten), die zwarstrukturiert sind, aber dennoch nicht zu den klassisch im DWH gespeichertenDaten zählen., Eine wichtige Veränderung, dass Speicher dramatischgünstiger geworden ist. In diesem Zusammenhang war Big Data sicher ein großerund wichtiger Hype, der auch die C-Level Ebene von großen Unternehmen erreichthat. Big Data verspricht, große Datenmengen – typischerweise Rohdaten – günstigzu speichern und zu verarbeiten. Hierdurch wurden Budgets freigeräumt, Projekteund Piloten gestartet aber letzten Endes hat vieles dann doch nicht den Weg indie Produktion geschafft., Data Lakes sind entstanden als Sammelstelle fürunstrukturierte Daten, im Grunde als „Spielwiese“ für Data Science. In den DataLake fließen Daten ohne Vorverarbeitung aus unterschiedlichen Quellsystemen. EinStück weit hat man dabei allerdings viele Fehler wiederholt, die man im BereichBusiness Intelligence bereits vor 20 Jahren gemacht hatte: viele Data Lakessind zum Datensumpf geworden, in dem sich Daten nicht sinnvoll wiederfindenlassen. Diese fehlende Organisation ist sicherlich auch ein Treiber für dasThema Data Catalog, das derzeit an Bedeutung gewinnt – also die Ordnung derDatenlandschaft., Ein Stück weit spiegeln sich in den Konzepten des DataWarehouse und Data Lakes aber auch die unterschiedlichen Kulturen derTeilbereich wider., Im Bereich Business Intelligence liegt der Fokus aufKonsistenz und Korrektheit sowie Verfügbarkeit der Daten, also Dinge, die manaus der klassischen IT kennt. Die Daten müssen doppelt und dreifachkontrolliert sein, fließen dann zum Beispiel in Geschäftsberichte ein. , Hingegen brauchen viele Data Science Anwendungen Rohdaten.Alles Weitere – zum Beispiel eine Verdichtung oder Vorverarbeitung – ist dannbereits Teil der Data Science Anwendung. Ähnliche Anforderungen findet mandurchaus auch in den Fachabteilungen. Es geht um Agilität, um Flexibilität undTime to Market, letztlich Tendenzen, die auch zu dem Erfolg des Self-Service BIMarkt geführt haben., Auch wenn es sicher Überlappung in der Datenbereitstellungfür Data Science und Business Intelligence gibt, ist der Bedarf dochstrukturell anders. Data Scientists brauchen Rohdaten wie sie typischerweisenicht im Data Warehouse gespeichert sind. Die Vorverarbeitungssschritte, die imBereich Business Intelligence mit dem Ziel der Datenqualitätsverbesserungdurchgeführt werden, sind für Data Science teilweise sogar problematisch bzw.schädlich – hier ist wichtig auf die unveränderten Rohdaten zugreifen zukönnen., Gute Data Pipelines sind aber sicher für beide Disziplinenhilfreich, Data Engineering insofern eine Serviceleistung, die für beideBereiche wichtig ist., Der abschließendezweite Teil des Expertengesprächs ist Gegenstand des nächsten Blogbeitragsunserer Reihe Business Intelligence meets Data Science., 1 https://www.meetup.com/de-DE/Business-Intelligence-und-Analytics-Dortmund, 2 https://www.meetup.com/de-DE/Business-Intelligence-und-Analytics-Dortmund/events/266751314/, 3 https://www.tu-chemnitz.de/wirtschaft/wi2/wp/de/team/prof-dr-peter-gluchowski/, 4 https://www.linkedin.com/in/martin-schmitz-03886a94/, 5 https://rapidminer.com/resource/gartner-magic-quadrant-data-science-platforms/, 6 https://www.reddit.com/r/datascience/, Allgemeine Fragen zum Blog, zum Verein und zum Thema AutorIn werden:Sandra SteingrubeCommunity Managersandra.steingrube(at)tdwi.eu, Fragen zum Inhalt:Leif HitzschkeTDWI e.V. Vorstandsmitgliedleif.hitzschke(at)tdwi.eu, TDWI e.V.Lindlaustraße 2cD-53842 TroisdorfTel: +49 (0)2241/2341-212, ImpressumDatenschutz";https://blog.tdwi.eu/business-intelligence-meets-data-science-einschaetzungen-aus-einem-expertengespraech-1-2/;TDWI;Gero Presser
04. Mrz 20;Low code … wow App?!;Bei einem Konzert in der Kulturmetropole Leipzig würde man sagen: vor ausverkauften Rängen – bei einem Anwenderforum des TDWI vielleicht eher – mit ausgeschöpfter Teilnehmerzahl – fand am 28.2.2020 an der HTWK in Leipzig das erste Anwenderforum des Jahres 2020 statt. Als Dozent agierte Elias Steine, ein TDWI Young Gun, der mit seiner großen Begabung für alle Themen rund um Microsoft sicher schon einigen von Ihnen aufgefallen ist. Ziel des Workshops war die Entwicklung zweier eigener Apps in knapp drei Stunden durch die Teilnehmer. Damit sollte auch eine eigene Bewertung möglich sein, ob die versprochene Einfachheit von Low Code tatsächlich im eigenen Umgang spürbar ist., Eine Low-Code-Application-Platform (LCAP) ermöglicht die Erstellung von Anwendungen ohne größere Programmierkenntnisse und ist dadurch auch für den Fachbereich nutzbar (Self Service ist ja ein bekannter Dauerbrenner …). Anstelle eigener Programmbausteine in Form von Code werden vorgefertigte grafische Elemente zusammengefügt. Mit diesem Ansatz stehen die Low-Code-Plattformen in der Tradition oder sogar als Nachfolger der Rapid-Application-Development Werkzeuge, die bereits in den 1980er Jahren Softwareentwicklung flexibler und schneller werden ließen. Die Hauptursache der aktuell zunehmenden Verbreitung der Low-Code-Application-Platforms (LCAP) wird im Mangel an entsprechend qualifizierten Programmierern gesehen. Im Kontext der Business Intelligence und Analytics bieten LCAPs den Fachbereichen die Möglichkeit, einen Schritt weiter zu gehen. So können aus Erkenntnissen Handlungsempfehlungen abgeleitet und zum Teil sogar prozessual umgesetzt werden. Traditionell sind hierfür oft zeit- und kostenintensive Erweiterungen in ERP oder CRM Systemen nötig. Mit Low-Code Apps ist dies mit einem verhältnismäßig geringen Aufwand möglich zumal der Fachbereich selbst die Weiterentwicklung übernehmen kann. Generell ist allerdings das Thema Schatten IT, also der Aufbau von Systemen, die nicht durch die IT-Abteilung aufgebaut und betrieben werden, schon lange vital und beginnt nicht erst mit der Verfügbarkeit der LCAP. Die Vor- und Nachteile, insbesondere aus einer Risikoperspektive, diskutierte dann auch Elias Steinle im Rahmen seines Workshops. Der Vorteil der einfachen Entwicklung birgt natürlich das Risiko, dass keine sorgfältige Planung erfolgt und damit beispielsweise existierende Standards ignoriert, Schnittstellen nicht genutzt und notwendiges Know-How (immer wieder) neu aufzubauen ist. Auch entstehen Risiken, die vielleicht für den einzelnen kaum bewertbar sind, die im Einzelnen die Daten- oder im Allgemeinen die IT-Sicherheit betreffen. Herausforderungen entstehen oft spätestens dann, wenn die Nutzerzahl zunimmt und gleichzeitig Transparenz über einzelnen Handlungen abnimmt. Dem gegenüber stehen allerdings positive Effekte wie die Nutzung der Innovationskraft des Fachbereichs für Schritte der Digitalisierung ebenso wie auch das schnelle und unkomplizierte Kennenlernen und Verändern fachlicher Prozesse. Letztendlich lassen sich Lerneffekte im Fachbereich in Bezug auf Prozesse erzielen, ohne eine IT-Abteilung damit zu belasten., Sicherlich sind diese Aspekte zu durchdenken und auch für jede Organisation abzuwägen. Im Workshop ging es aber vor allem erst einmal darum, die Möglichkeiten kennenzulernen und auch für sich selbst festzustellen, ob die Entwicklung tatsächlich so einfach wie versprochen ist. Die erste Entwicklung war eine kleine TDWI Event App, die auf einfache Weise Übersicht zu aktuellen Veranstaltungen ebenso wie die Möglichkeit zur direkten Anmeldung gab. Dabei wurde neben Formatierungsmöglichkeiten auch die Anbindung von Datenquellen und das Zurückschreiben auf selbige dargestellt. Die zweite App beschäftigte sich mit dem Thema Sprachassistenz. Ein aktuell viel diskutiertes Thema, das auch im aktuellen Gartner Hype Cycle zum Thema Artificial Intelligence äußerst prominent bewertet wird. In der Praxis könnten beispielsweise Call-Center Mitarbeiter selbst eigene Sprachassistenten bauen, überwachen und ständig verbessern., So lernten die Teilnehmer auf einfache Weise Variablen zu initialisieren, sogenannte Flows zu erstellen, Oberflächen zu gestalten, um auf diese Weise wirklich selbst am eigenen Laptop die Entwicklung durchzugehen. Der eigene Erfolg stimmte die TeilnehmerInnen sehr zufrieden und so konnte der Tag noch bei einem netten Miteinander ausklingen., Tanja Kenda, die das Anwenderforum seitens des TDWI e.V. mit organisiert hat, hat auch aktiv teilgenommen: „Durch die Art und Weise, wie Elias Steinle alles Schritt für Schritt erklärt hat, konnte ich sehr gut folgen und hatte am Ende tatsächlich eine eigene App erstellt! Es ging primär um die Erstellung und den Aufbau und nicht so sehr um das Design der App. Am Aussehen konnte ich im Anschluss selbst noch etwas ‚feilen‘. Im zweiten Teil der Veranstaltung, der Erstellung des Chatbots, fand ich besonders den logischen Weg interessant und wie einfach dieser in Low Code aufzubauen ist. Alles in Allem hat sich der Nachmittag mehr als gelohnt!“, Merken Sie sich jetzt schon den Termin für das nächste Anwenderforum am 16. Oktober in Stuttgart vor., Allgemeine Fragen zum Blog, zum Verein und zum Thema AutorIn werden:Sandra SteingrubeCommunity Managersandra.steingrube(at)tdwi.eu, Fragen zum Inhalt:Leif HitzschkeTDWI e.V. Vorstandsmitgliedleif.hitzschke(at)tdwi.eu, TDWI e.V.Lindlaustraße 2cD-53842 TroisdorfTel: +49 (0)2241/2341-212, ImpressumDatenschutz;https://blog.tdwi.eu/low-code-wow-app/;TDWI;Claudia Koschtial
26. Feb 20;Data Lake – eine Bestandsaufnahme in drei Akten: Begriff und Motivation (1);Der Begriff Data Lake wird seit einigen Jahren heiß diskutiert. Bis heute gibt es kein einheitliches Verständnis und die Erwartungen der Anwender liegen verteilt zwischen universellem Heilsbringer und großer Festplatte. Diese Serie betrachtet das Konzept Data Lake auf Basis aktueller Entwicklungen sowie Erfahrungen in der Praxis und versucht dabei das breite Spektrum an Begrifflichkeiten und Ansätzen einzuordnen. Dieser erste Artikel der Serie betrachtet dabei initial den Begriff des Data Lakes und die zugrundeliegende Motivation.  , Im Zuge der digitalen Transformation spielt der Rohstoff Daten eine immer wichtigere Rolle. Die Erhebung und Auswertung von Daten ist dabei traditionell eine Kernkompetenz des Bereichs Business Intelligence und Analytics (BIA). Aktuelle Trends wie Big Data, Industrie 4.0 und das Internet der Dinge verändern die Anforderungen an BIA allerdings enorm. Immer mehr Daten müssen immer schneller erhoben, gespeichert und ausgewertet werden. Traditionelle Ansätze aus der dispositiven Datenhaltung, wie die Idee eines zentralen Data Warehouse als konsistenter Single-Point-of-Truth, stoßen hierbei immer öfter an ihre Grenzen., Als Lösung wurde vor einigen Jahren die Idee des Data Lakes geboren. Dieser sehr griffige und bildhafte Begriff wurde mit Freuden von der Marketing-Maschinerie der Industrie aufgenommen und dementsprechend gibt es bis heute zahlreiche unterschiedliche Verständnisse und Erwartungen im Zusammenhang mit dem Thema Data Lake. Manche Definitionen heben hierbei die Speicherung von großen Mengen unstrukturierter Rohdaten hervor („Ein #DataLake ist wie eine überdimensionierte Festplatte, auf der alles an einem Ort gespeichert wird“ 1), andere legen den Schwerpunkt eher auf den Einsatz zahlreicher heterogener Systeme („a concept that includes a collection of storage instances of various data assets“ 2). Abhängung vom Anwendungsfall hat jede dieser Beschreibungen seine Berechtigung. Ein etwas weiter gespannte Arbeitsdefinition für diese Serie könnte daher wie folgt lauten., Ein Data Lake ist eine Sammlung von Komponenten und Konzepten zur Speicherung, Verwaltung und Bereitstellung von Daten jeglicher Art (strukturiert, unstrukturierter sowie Rohdaten) einer Organisation., Bei Diskussionen mit Anwendern und Architekten ergibt sich manchmal der Eindruck bei einem Data Lake handelt es sich sprichwörtlich um eine eierlegende Wollmilchsau und durch die richtige Implementierung würden sich alle Probleme nahezu von selbst lösen. Dem ist aber natürlich meist nicht so. Für eine nüchterne Nutzenanalyse ergibt es daher Sinn zuerst die unterschiedlichen Motive zur Etablierung eines Data Lakes zu verstehen., In der Praxis lässt sich durch die kausalen Zusammenhänge der Motive oft eine Kombination der verschiedenen Punkte beobachten. Des Weiteren betiteln nicht alle Organisationen ihre Vorhaben als Data Lake. Gerne hört man auch Begriffe wie Data Hub, zentrale Datenplattform oder Data-Science-Sandboxes. Die grundsätzlichen Intentionen dieser Ansätze gleichen allerdings meist dem, was man unter dem Thema „Data Lake“ subsumiert., Auch wenn es verschiedene Verständnisse über den Umfang und die Aufgabe eines Data Lakes gibt, zeigt sich ein Konsens darin, dass ein Data Lake eine Sammlung von Komponenten und Konzepten zur Speicherung, Verwaltung und Bereitstellung von Daten jeglicher Art ist und damit als Reaktion auf die neuen Anforderungen an BIA-Landschaften gesehen werden kann.  Entsprechend dieser breiten Definition gibt es auch unterschiedliche Motive zum Aufbau eines Data Lakes in einer Organisation. Die prominentesten sind hierbei die Massendatenspeicherung, die Verarbeitung von Daten in Echtzeit, die steigende Wichtigkeit von Data Science und anderen analytischen Ansätzen sowie die Bändigung heterogener Systemlandschaften. Auch wenn in der Praxis nicht immer die Rede von einem Data Lake ist, verfolgen viele Unternehmen Ansätze, die sich sehr gut in diese Thematiken einordnen lassen. , Der nächste Teil dieser Serie beschäftigt sich mit verschiedenen Architekturansätzen zur Implementierung eines Data Lakes in der Praxis., 1 https://www.alexanderthamm.com/de/artikel/grundlagen-anwendungsfaelle-data-lake/ 2 https://www.gartner.com/en/information-technology/glossary/data-lake, 1 TDWI Seminar “Data-Lake-Ansätze und Best Practices”https://www.tdwi.eu/akademie/seminarsuche/seminardetails/seminar-titel/data-lake-ansaetze-und-best-practices.html 2 TDWI E-Book: “Der Data Lake als zentrales Element in Analytics-Architekturen“https://www.tdwi.eu/wissen/studien-buecher/e-books/wissen-titel/tdwi-e-book-der-data-lake-als-zentrales-element-in-analytics-architekturen.html, Spannender Beitrag. Ich habe gelesen, dass das ERP System sehr hilfreich für Datenmanagement ist., Danke für den Kommentar. ERP Systeme sind traditionel eher operative Vorsysteme, die meist Daten für analytische System bereitstellen, also bspw. Bestellungen werden als aggregierte Verkaufszahlen in einem Data Warehouse bereitgestellt. Durch den technologischen Fortschritt verschmelzen hier die Grenzen aber immer mehr und viele operative Datenbanken bieten die Möglichkeit direkt analytische Auswertungen zu fahren (bspw. per In-Memory-Technologie bei Hana). ERP-Systeme sehe ich daher eher als eine Komponente für die auch Datenmanagement-Ansätze angewendet werden sollten bzw. die in ein übergreifendes Datenmanagement integriert werden sollten., Allgemeine Fragen zum Blog, zum Verein und zum Thema AutorIn werden:Sandra SteingrubeCommunity Managersandra.steingrube(at)tdwi.eu, Fragen zum Inhalt:Leif HitzschkeTDWI e.V. Vorstandsmitgliedleif.hitzschke(at)tdwi.eu, TDWI e.V.Lindlaustraße 2cD-53842 TroisdorfTel: +49 (0)2241/2341-212, ImpressumDatenschutz;https://blog.tdwi.eu/data-lake-eine-bestandsaufnahme-in-drei-akten-begriff-und-motivation/;TDWI;Julian Ereth
19. Feb 20;Join us and share your passion with peers!;Diese Woche möchten wir euch ein neues Event vorstellen, bei dem der TDWI als Partner dabei ist: c/o data science. Dazu gibt Nils Bruckhuisen einen Einblick in das letzte Event in Bonn:, Im November 2019 wurde ein neues Veranstaltungsformatbegründet – c/o data science. , Einige Eindrücke davon teile ich hier mit euch. , Zwischen Campingwagen aller Stile und Epochen wurdenlehrreiche Vorträge gleich neben inspirierenden Barcamp-Sessions abgehalten.Allerdings stehen diese Caravans nicht unter freiem Himmel, sondern – was imNovember durchaus angenehmer ist – in einer großen Halle unweit des Bonner UNCampus. , Viele Firmen und Experten der datengetriebenen Wirtschaft hattenjeweils eine Parzelle des Camps belegt, so schlenderte man zwischen denCampingwagen und konnte hier einmal fachsimpeln und dort einmal an einerTombola teilnehmen – letztere resultierte im Gartenzwerg auf dem Iron Throne…, Neben Food Trucks auf dem Hof gab es abseits derCampingwagen auch noch eine Galerie zu entdecken – und das taten bei den dortstattfindenden Vorträgen auch viele:, So war etwa von einem Sprecher zu erfahren, dass das Unternehmen das gewaltige Aufkommen an Emails mit Spark behandelt. Dabei seien nur 70 % der eingehenden Emails von Menschen geschrieben. Zudem werde alles, was länger als 250 Wörter ist schlicht abgeschnitten., Als Bonner Lokalmatador steurte die DHL eine Keynote bei,für die sich alle vor der Bühne versammelt. Im Anschluss wurde beicampingtypischen Getränken noch genetzwerkt (etwa mit den TDWI Young Guns) –oder mittels VR – Brille dem Flug einer Drohne folgend so manchesSchwindelgefühl erlitten., Insgesamt ist es die richtige Schlussfolgerung aus der erstenc/o Data Science, in 2020 gleich die zweite und dritte folgen zu lassen!, Mehr zu c/o data science:, Gemacht für Data Scientisten, die sich weiterbilden undnetzwerken möchten, gibt es bei c/o data science spannende Use Cases undehrliche Antworten der Vortragenden., Die Talks werden von Experten gehalten, die sich zu ihremThema ausfragen lassen. Es gibt Data Science live und in action ohne Schnörkeloder leere Worthülsen., Es bleibt viel Zeit zum Networking und für den Austausch mitGleichgesinnten in Hacksessions und im Barcamp., Insbesondere beim Barcamp ist Initiative gefragt: Bringteigene Fragen und Projekte mit und präsentiert sie vor Ort. Es ergeben sichdann kurze Sessions mit Personen, die am gleichen Thema Interesse haben. Ausdem Impulsvortrag entstehen angeregte Diskussionen., c/o data science findet in 2020 am 28.04. in Hamburg und am29.09. in Bonn statt. Die Locations sind einen zweiten Blick wert: In Hamburgsind wir im Braugasthaus Altes Mädchen an der Schanze und in Bonn im Basecamp –ein Hostel mit Retro-Wohnwagen., Werft einen Blick auf co-datascience.de oder folgt uns bei LinkedIn und Twitter, um auf dem Laufenden zu bleiben. Erzählt auch euren Kollegen von diesem Event., Allgemeine Fragen zum Blog, zum Verein und zum Thema AutorIn werden:Sandra SteingrubeCommunity Managersandra.steingrube(at)tdwi.eu, Fragen zum Inhalt:Leif HitzschkeTDWI e.V. Vorstandsmitgliedleif.hitzschke(at)tdwi.eu, TDWI e.V.Lindlaustraße 2cD-53842 TroisdorfTel: +49 (0)2241/2341-212, ImpressumDatenschutz;https://blog.tdwi.eu/join-us-and-share-your-passion-with-peers/;TDWI;Nils Bruckhuisen
12. Feb 20;Clustermap using hierarchical clustering in Python – A powerful chart to display many aspects of data;"A so-called “Clustermap” chart serves different purposes and needs. This article has the aim to describe how you can create one, what purposes it serves and we will have a detailed look into the chart. This chart includes a hierarchical clustering which we will investigate as well., During the last years, the programming language Python became popular for many people interested to provide insights from data. For some time now, Seaborn has been my package of choice for creating charts (Waskom, 2018)). ,  , How it all started:, The way I came across the Clustermap started with a data-related problem. From a current private project, I received a large amount of real estate data which looked like this: , The aim was to display the place and the construction phase of the objects to make some conclusions about the place and the number of houses during the construction phase. , Let us have a look at the steps how to create the chart in Python., Please note that for the Clustermap no NA values are allowed. , This is very easily being done by , This looks very interesting. It already shows us some information, for example that most of the house offers are located in Nordrhein-Westfalen., If we want to point out the differences ofconstruction phase by state, we have to apply an intermediate step., Let’s say we want to find out the state with the most houses in “projected” stage, regardless of the amount. We do that by applying the MinMaxScaler into the equation. The code looks as follows for our chart: , The phrase “standard_scale=0” applies the scaler either by row (0) or column (1).  The Scaler works with the following formula:  , It basically shrinks the range of the values to 0 and 1 (or -1 and 1 for negative values) (Keen, 2020). We have done that in our chart and here it is: , We can now say that in Saxony is the highestratio of projected houses. , On the margin you might have realized that there are several lines. These are part of a so called “Dendrogram” and display the hierarchical clustering (Bock, 2013)., , The interesting thing about the dendrogram is that it can show us the differences in the clusters. In the example we see that A and B for example is much closer to the other clusters C, D, E and F. , It becomes much clearer if we put a line between the clusters: , There are two hierarchical clustering methods. In our example we focus on the Agglomerative Hierarchical Clustering Technique which is showing each point as one cluster and in each iteration combines it until only one cluster is left, this picture sums it up (Dey, 2020):, , Transferring that into example, we see that some states are very close to others – for example Brandenburg and Sachsen. Why is that? In this case we need to understand the underlying clustering methods., From our last code we recall that the current clustermap was created by:, The method gives us the hierarchical clusteringmethod. We can choose from (The SciPy community, 2019):, And some more, but we will focus on these five methods., Single linkage method , Also called the min method and defined by:  . This means that this algorithm takes the closest two points in the clusters and therefore describes it as the similarity of two clusters. See the picture on the right side (Alvez, 2011). This method applies well if there are non-elliptical shapes without any outliers or noise. You see that this method separates the clusters in the first and second case but fails to do so in the case with noisy data (Al-Fuqaha, 2014).,  Complete – Farthest Point Algorithm, As the name outlines, this is the opposite method – also called max and defined by:   . It means that the algorithm takes the farthest point and describes it as the similarity of two clusters. It does very well in case of noisy data but could risk to break large clusters. You can see that in the third panel of the picture on the right. For the other clusters this method does not perform so well (Al-Fuqaha, 2014)., , Group Average, The next method is called Group average or UPGMA or Average Linkage and defined by:  . It is basically the distance between two clusters and then calculates the average of the similarities. This method is not so much influenced by outliers and performs well in case of noisy data. You can see that in the third tile of the picture on the right. The downside is that this method is skewed towards globular clusters (Al-Fuqaha, 2014). , , WARDS method:,  This method is defined by:  and uses the increase in squared error (SSE). It is similar to group average if the distance between points is squared. It is less biased by outliers and noise but to globular clusters (similar as group average). The method is the hierarchical twin to K-Means clustering (Al-Fuqaha, 2014). , , , , Let’s turn back to the example but to better understand the data we will use the standard values, not scaled. We will also use the total values per state to not get confused. The code is as follows:  , In the example we used the single linkage method which means that the closest points form a cluster. You can see by looking on the chart that this already happened. For example, Bayern and Niedersachsen form one cluster because they lie close to each other – data-wise of course ??. , The question is now: Is this the right method to cluster the data? To visualize the data, we need to put the chart into a Multidimensional scaling (“Multidimensional scaling is a visual representation of distances or dissimilarities between sets of objects” (Statistics How To, 2015)). To achieve this, we need to first put the data in a distance matrix which looks like this:  , Afterwards we are able to create the mentioned visual which looks as follows:, By looking on the picture, I would separate three large clusters as follows: , We recall that the first Clustermap with thesingle method does that quite well. What about the other ones?, Complete:, Average:, Ward:, Overall, the single and average method show agood performance here since they separate the clusters pretty well. It could behowever that you want to achieve a different goal, then you should investigatethe other methods as well. , In conclusion, this article looked at theClustermap and how to create it. Besides that we also investigated thehierarchical clustering methodsa which are part of this chart., Please also have a look on the Jupyter Notebookon nbviewer or on GitHub. , Thank you for reading,, Armin, Al-Fuqaha, A. (2014). Clustering Analysis – Lecture Slides. Kalamazoo: Western Michigan University., Alvez, P. B.   (2011). Inference of a human brain fiber bundle atlas from high angular resolution diffusion imaging. PhD THESIS, University of Paris-Sud 11, Graduate School of Sciences and Information Technologies,Telecommunications and Systems, Paris. Retrieved February 1, 2020, Bock, T. (2013, September 20). What is a Dendrogram? Retrieved January 19, 2020, from   https://www.displayr.com/what-is-dendrogram/, Dey, D. (2020). ML | Hierarchical clustering (Agglomerative and Divisive clustering). Retrieved February 1, 2020, from https://www.geeksforgeeks.org/ml-hierarchical-clustering-agglomerative-and-divisive-clustering/, Keen, B. A. (2020). Feature Scaling with scikit-learn. Retrieved January 18, 2020, from http://benalexkeen.com/feature-scaling-with-scikit-learn/, Michener, C., &amp; Sokal, R. (1957). A quantitative approach to a problem of classification. Evolution, pp. 11:490–499. Retrieved from https://www.sequentix.de/gelquest/help/upgma_method.htm, Statistics How To. (2015, June 17). Multidimensional Scaling: Definition, Overview, Examples. Retrieved February 6, 2020, from https://www.statisticshowto.datasciencecentral.com/multidimensional-scaling/, The SciPy community. (2019, December 19). scipy.cluster.hierarchy.linkage. Retrieved January 19, 2020, from https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html, Waskom, M. (2018). Seaborn – Example gallery. Retrieved January 08, 2020, from https://seaborn.pydata.org/examples/index.html, Allgemeine Fragen zum Blog, zum Verein und zum Thema AutorIn werden:Sandra SteingrubeCommunity Managersandra.steingrube(at)tdwi.eu, Fragen zum Inhalt:Leif HitzschkeTDWI e.V. Vorstandsmitgliedleif.hitzschke(at)tdwi.eu, TDWI e.V.Lindlaustraße 2cD-53842 TroisdorfTel: +49 (0)2241/2341-212, ImpressumDatenschutz";https://blog.tdwi.eu/hierarchical-clustering-in-python/;TDWI;Armin Geisler
05. Feb 20;Business Intelligence meets Data Science: Zwei Welten treffen aufeinander;"Auf den ersten Blick ist es verblüffend: Sowohl Business Intelligence als auch Data Science verfolgen den Erkenntnisgewinn aus Daten. Und dennoch trennen die beiden Themen häufig Welten: kulturell, organisatorisch und im Hinblick auf Werkzeuge. In dieser Serie aus Blog-Artikeln möchten wir näher in dieses Thema eintauchen, liegen doch in der Zusammenführung der beiden Themen erhebliche Chancen. Im ersten Teil betrachten wir die Ausgangslage näher., Der Begriff„Business Intelligence“ wurde 1989von Howard Dresner geprägt als Oberbegriff für “concepts and methods to improvebusiness decision making by using fact-based support systems” 1. Inden 1990’er Jahren hat sich der Begriff und die Einführung von BusinessIntelligence langsam durchgesetzt, es handelt sich insofern um eine Disziplinmit Tradition. Die dabei verfolgte Zielsetzung ist, Daten so aufzubereiten,dass hieraus Erkenntnisse gewonnen werden um im Management bessereEntscheidungen zu treffen., Gewissermaßen als Standardmodell hat sich vereinfacht folgende Architektur herausgeprägt:, Daten werden aus Quellen extrahiert, per ETL (Extract,Transform, Load) in ein gewünschtes Zielformat transformiert und anschließendin ein zentrales Datenwarenhaus geladen, das als zentrale Quelle fürAuswertungen dient. Im Datenwarenhaus werden die Daten historisiert aufbewahrt., Heutzutage erlauben es Self-Service Werkzeuge immer mehrAnwendern, selbständig Daten visuell zu analysieren und hierdurch Erkenntnisseals Basis für Entscheidungen zu gewinnen., Business Intelligence ist eine gewachsene Disziplin miteiner gewissen Tradition; in den meisten Organisationen gibt es einentsprechendes Kompetenzzentrum (CoC) und die bereitgestellten Berichte undAnalysen sind wesentlich für die Steuerung von Organisationen., Data Science istein Oberbegriff für wissenschaftliche Methoden, Prozesse, Algorithmen undSysteme um Wissen und Einsichten aus strukturierten und unstrukturierten Datenzu gewinnen, dieses Verständnis des Begriffs wurde 1992 geprägt 2. ErheblichenAuftrieb hat das Thema durch den Artikel „Data Scientist: The Sexiest Job ofthe 21st Century“, erschienen 2012 im Havard Business Review, gewonnen 3.Seitdem genießt das Thema erhebliche Aufmerksamkeit, es gibt vielfältige neueAusbildungsangebote und auch immer mehr traditionelle Organisationenbeschäftigen Data Scientist um eine datenbasierte Grundlage für Entscheidungen zugewinnen und um Prozesse zu automatisieren und zu optimieren., Üblicherweise ist Data Science ein projektorientiertes Vorgehen, das in vielen Fällen explizit oder implizit dem CRISP-DM folgt, dem „Cross-industry standard process for data mining“ 4:, Ausgehend vom fachlichen Anwendungsfall und der Datenlagewerden die Daten vorverarbeitet, um anschließend zu einem Modell zu gelangen,also einer meist vereinfachten, ausführbaren Funktion zur Beschreibung des untersuchtenSystems. Die Ergebnisse des Modells – dessen Güte – wird evaluiert undmöglicherweise das Modell schließlich in Produktion gebracht, also konkretgenutzt. In der Praxis sind viele Schleifen und Rücksprünge erforderlich, umschließlich zum Ergebnis zu gelangen. Bedeutsam ist hier, dass, ein Großteilder Zeit für die (Beschaffung und) Vorverarbeitung der Daten aufgewandt wird., Das Thema Data Science kann immer noch als „hip“ bezeichnetwerden; es zieht gerade Menschen mit akademischem Hintergrund undnaturwissenschaftlicher oder mathematischer Ausbildung an., Sowohl Business Intelligence als auch Data Science verfolgen letztlich das Ziel, Erkenntnisse aus Daten zu gewinnen um eine Organisation zu verbessern. Sie sind damit integraler Bestandteil der Digitalisierung, quasi das Herzstück dieses Megatrends. Im Zuge der Digitalisierung wird (fast) alles digital, also zu Daten. Entsprechend kann man die Bedeutung des Erkenntnisgewinns aus Daten nicht überschätzen, sie ist die zielgerichtete Nutzung der immer größer werdenden Datenmengen. Oder um es mit den Worten von Gartner zu sagen: „Data and analytics will become the centerpiece of enterprise strategy, focus and investment“ 5. , 1 http://dssresources.com/history/dsshistory.html, 2 https://en.wikipedia.org/wiki/Data_science, 3 https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century, 4 https://en.wikipedia.org/wiki/Cross-industry_standard_process_for_data_mining, 5 https://www.gartner.com/smarterwithgartner/why-data-and-analytics-are-key-to-digital-transformation/, Allgemeine Fragen zum Blog, zum Verein und zum Thema AutorIn werden:Sandra SteingrubeCommunity Managersandra.steingrube(at)tdwi.eu, Fragen zum Inhalt:Leif HitzschkeTDWI e.V. Vorstandsmitgliedleif.hitzschke(at)tdwi.eu, TDWI e.V.Lindlaustraße 2cD-53842 TroisdorfTel: +49 (0)2241/2341-212, ImpressumDatenschutz";https://blog.tdwi.eu/business-intelligence-meets-data-science-zwei-welten/;TDWI;Gero Presser
29. Jan 20;Analytics-Strategie: Welche Komponenten sind wichtig?;Die Jahre zwischen 2020 und 2030 sollen wieder goldene Jahre werden. Ich denke, auch ohne prophetische Fähigkeiten ist es möglich festzuhalten, dass es in vielen Projekten weiter um Schlagworte wie Digitalisierung und Artificial Intelligence gehen wird – es könnten also goldene Jahre für die Unterstützung aller organisationalen Aktivitäten im Sinne der Automatisierung mittels Algorithmen und IT werden., In meinem letzten Beitrag hatte ich Ihnen bereits Anlässe genannt, eine analytische Strategie in den Fokus zu nehmen. Dafür muss als Einstieg eine gemeinsame Definition zum Themenkomplex der Analytics gefunden werden, gerade um existierende und sinnvolle denkbare Projekte zu erfassen und diese in der Ausrichtung gemeinschaftlich zu betrachten. Diese Projekte bilden nun einen Ausgangspunkt der Strategie, denn diese zeigen den tatsächlichen Nutzen, den Analytics für das bestehende oder sogar die Erweiterung des Geschäftsmodells der Organisation haben kann. Eine Vision oder ein Leitbild der Analytics-Strategie muss zusammen mit der Definition entstehen., Weitere wichtige Aspekte sind –so existent –angrenzende Strategien wie beispielsweise Data-Strategy, Business-Intelligence-Strategie, Digitalisierungsstrategie und übergeordnet eine IT-Strategie und Geschäftsstrategie. Hier sind individuell Berührungspunkte zu prüfen. Beispielsweise lassen sich aus der Business-Intelligence-Strategie vielleicht Synergieeffekte bezüglich einer Data Governance finden. Natürlich kann eine Analytics-Strategie auch ohne Rückkopplung auf dem Reißbrett entstehen, aber dann treten die Hürden bei der operativen Umsetzung auf. Die Umsetzungsperspektive im Kopf zu haben und zu behalten ist ein Kernaspekt, um Nutzen realisieren zu können., Nun kann eine Analytics-Strategie mit den Dimensionen inhaltlich ausgestaltet werden, die in der nachfolgenden Abbildung gezeigt werden., In den Dimensionen „Organisation“ und „Prozesse“ werden die Organisationseinheiten und deren Zusammenspiel in Prozessen erarbeitet. Damit einher gehen Verantwortlichkeiten und die Definition notwendiger Kompetenzen.  Auch müssen in der Dimension „Personen“ entsprechende Rollenzuordnungen vorgenommen werden, um Verantwortlichkeiten konkreten Personen zuzuweisen, notwendige Personalkapazitäten zu bemessen und auch um Weiterbildungskonzepte mit konkreten Schulungen zu konzeptionieren. Eine häufige Fragestellung ist dabei, ob es eine zentral angesiedelte Organisationseinheit mit analytischer Kompetenz, oder eher dezentrale Analytics-Einheiten in den Fachbereichen geben soll (Lesen Sie dazu einen meiner nächsten Artikel. ?? )., Die Dimension „Architektur“, betrachtet die Architekturgestaltung in enger Verknüpfung mit der angrenzenden Dimension „Werkzeuge“.  Hier werden die grundsätzlich nutzbaren Werkzeuge als Standards definiert, damit beispielsweise ein späterer Support durch die IT gewährleistet ist., Die Dimension „Kultur“ adressiert das erforderliche Change-Management, um die definierten Strukturen effizient in der Organisation zu etablieren. Dazu gehören agile Methoden, aber auch eine Bewusstseinsschaffung, dass eine Kultur des positiven Scheiterns notwendig ist („fail fast“). Auch ein Scheitern ermöglicht Lerneffekte: Welche Daten führten nicht zum gewünschten Ergebnis? Welche Daten haben wir nicht, oder nicht in ausreichender Qualität?, Wenn klar ist, wo das Ziel ist, lässt sich der Weg definieren. Stehen also alle Dimensionen einer analytischen Strategie, lässt sich eine Roadmap aufstellen. Diese definiert dann Reihenfolgen und auch Messpunkte für eine Prüfung. Und nicht zuletzt möchte ich Ihnen noch mitgeben: manchmal ändert sich die Welt (was jetzt nicht wirklich neu ist) – es ist daher gut, die eigene Strategie regelmäßig anzupassen, ohne wilde Sprünge zu machen: es muss definierte Rhythmen zur Überprüfung der Strategie geben, um so sicher zu stellen, dass Ihre Organisation mit Ihrer Analytics-Strategie auch neue Herausforderungen meistern kann. Dies alles im Sinne der viel beschworenen Agilität …, Allgemeine Fragen zum Blog, zum Verein und zum Thema AutorIn werden:Sandra SteingrubeCommunity Managersandra.steingrube(at)tdwi.eu, Fragen zum Inhalt:Leif HitzschkeTDWI e.V. Vorstandsmitgliedleif.hitzschke(at)tdwi.eu, TDWI e.V.Lindlaustraße 2cD-53842 TroisdorfTel: +49 (0)2241/2341-212, ImpressumDatenschutz;https://blog.tdwi.eu/analytics-strategie-komponenten/;TDWI;Claudia Koschtial
22. Jan 20;Welches Mindset braucht eine datengetriebene Organisation?;"Das große Datensammeln hat begonnen, jeder will die Möglichkeiten von Big Data und Analytics für sich nutzen. Das Ziel: Effizientere Prozesse und kundenorientierte Produkte, um die Wettbewerbsfähigkeit zu stärken. Doch wer eine datengetriebene Organisation sein möchte, muss sich bewusst sein, dass jeder Mitarbeiter einen Einfluss auf diesen Erfolg hat. Es reicht am Ende des Tages nicht aus, eine neue Software einzukaufen, ein Analytics Team aufzustellen und Data Scientisten zu rekrutieren. Menschen wollen wissen, warum etwas passiert und welchen Einfluss dies auf den Arbeitsplatz nehmen wird. Unterstützen kann dabei die Förderung eines Mindset und das funktioniert nicht auf Knopfdruck., Nicht noch ein Mindset, Zugegeben, in den letzten Jahren haben sich die Titel überschlagen und zu jedem Trend gab es ein neues Mindset – wie agiles Mindset, innovatives Mindset oder aktuell data-driven Mindset. Der Titel ist dabei eher zweitranging, umso wichtiger ist es für die geplante datengetriebene Organisation zu verstehen:, Als Basis, um zu verstehen, was mit Mindset überhaupt gemeint ist, dient die Theorie von Carol Dweck, Professorin für Psychologie. Laut Dweck habenMenschen ein Growth und ein Fixed Mindset. Beide sind unterschiedlich ausgeprägt und bedingen einander. Die Ausprägungen variieren je nach erlebten Erfahrungen, Motivationen und Umwelteinflüssen., Die Basis, Menschen mit einem Growth Mindset haben ein dynamisches Selbstbild und sind davon überzeugt, dass sie sich selbst immer wieder weiterentwickeln können und Fehler dazugehören. Ganz nach dem Motto: Mit der richtigen Motivation, kann jeder alles machen oder werden.Im Gegenzug haben Menschen mit einem Fixed Mindset ein statisches Selbstbild und gehen davon aus, dass Können nur mit einem bestimmten Talent möglich ist. Auch werden Fehler eher vermieden und Situationen bewertet: Werde ich als Sieger oder Verlierer rausgehen? Hier wird nach dem Motto: Wer nichts wagt der nichts verliert, gelebt., Haben Menschen oder sogar ganze Teams überwiegend ein Fixed Mindset, wäre das für datenanalytische Vorhaben, in denen es viel um Experimentieren geht, tödlich. Dennoch ist es realistisch, in manchen Situationen, mal ausgeprägter im Growth oder im Fixed Mindset zu sein. Es kommt auf die Situation an, denn fortlaufendes Wachstum verbraucht Energie, die wieder aufgetankt werden muss. Vielleicht durch einen schönen Urlaub oder indem die Konzentration auf eine Aufgabe gelenkt wird, die man beherrscht., Das Mindset in einer datengetriebenen Organisation, In einer Organisation arbeiten viele Menschen in unterschiedlichen Abteilungen und Teams zusammen und besitzen verschiedene Fähigkeiten und Erfahrungen. Eines haben sie gemeinsam: Die Strategie und ihre übergeordneten Ziele. Der nächste Schritt ist, das gewünschte Mindset davon abzuleiten. Wenn wir dem Kind einen Namen geben wollen, dann entwickeln wir in einer datengetriebenen Organisation ein sogenanntes data-driven Mindset mit folgenden Prinzipien:, Diese Auflistung dient nur als Beispiel, um einen groben Überblick möglicher Prinzipien zu vermitteln. In der Realität bedarf es eines umfänglicheren Blickes in die Organisation und das zugehörige Umfeld., Fazit, Eine neue strategische Ausrichtung, Umstrukturierungen, innovationsfördernde Methoden oder Analyse-Software, all diese Dinge passieren derzeit und sie haben ihren Sinn – auch, wenn nicht jeder mit Applaus jubelt. Organisationen müssen genau diesen Sinn an die Mitarbeiter kommunizieren, sich kritischen Fragen stellen und erste Unsicherheiten nehmen. Der nächste Schritt ist die Ableitung von Leitprinzipien, welche ein data-driven Mindset fördern, damit am Ende die Ziele zufrieden und motiviert erreicht werden können. Die Prinzipien müssen immer wieder aktiv gelebt werden und motivieren, z.B durch Geschäftsführer, Führungskräfte, Projektleiter oder People Coaches und selbstverständlich durch alle Mitarbeiter. Ein Puzzleteil für eine erfolgreiche datengetriebene Organisation fehlt noch, die Kultur. Was ist an der Organisationskultur so wichtig? Das thematisiere ich in meinem nächsten Artikel. Zur Vorbereitung gebe ich Ihnen eine Frage mit: Wie stellen Sie sicher, dass Ihre Organisationskultur von allen gelebt wird bzw. wird sie das überhaupt?, Edgar Schein (1985) definiert Kultur als «ein Muster von Grundannahmen, das eine Gruppe bei der Bewältigung ihrer Probleme externer Anpassungund interner Integration erfunden, entdeckt oder entwickelt hat, das sich bewährt hat und als bindend betrachtet wird; und das daher an neue Mitglieder als rationaler und emotional korrekter Ansatz für den Umgang mit Problemen weitergegeben wird», Die Organisationskultur in einem Unternehmen entsteht allmählich, sie wächst gemeinsam mit dem Aufbau von Strukturen und Abläufen. Alle Organisationsmitglieder sind durch ihr Handeln und ihr Verhalten bei der Entwicklung der Unternehmenskultur beteiligt., Ist es da nicht umgekehrt, dass zuerst der Sinn und Zweck der Organisation verstanden werden muss? Daraus entsteht die Kultur und das „Mindset“ ist nur ein Teil der Kultur., Oder liege ich hier falsch?, Allgemeine Fragen zum Blog, zum Verein und zum Thema AutorIn werden:Sandra SteingrubeCommunity Managersandra.steingrube(at)tdwi.eu, Fragen zum Inhalt:Leif HitzschkeTDWI e.V. Vorstandsmitgliedleif.hitzschke(at)tdwi.eu, TDWI e.V.Lindlaustraße 2cD-53842 TroisdorfTel: +49 (0)2241/2341-212, ImpressumDatenschutz";https://blog.tdwi.eu/welches-mindset-braucht-eine-datengetriebene-organisation/;TDWI;Julia Mehrtens
15. Jan 20;"Business Intelligence &amp; Analytics – Mode oder Trend?";"Themen beim Anwenderforum 2019 in Stuttgart waren unter anderem die Zusammenführung von BI und KI über integrierte Governance-Strukturen, Process Mining in einer Versicherung sowie die Auswertung von Produktdaten in Unternehmensnetzwerken., Am 11. Oktober 2019 fand das traditionelle TDWI Anwenderforum in den Räumlichkeiten des Literaturhauses Stuttgart statt. An der Veranstaltung mit dem Titel „Business Intelligence &amp; Analytics – Mode oder  Trend?“ haben über 100 BI- und DWH-Interessierte teilgenommen und dabei  über innovative Themen, spannende Herausforderungen und praktische Lösungsansätze diskutiert. In der für TDWI-Mitglieder und Alumni des Betriebswirtschaftlichen Instituts kostenfreien Veranstaltung stellten acht Referenten, vornehmlich aus der Unternehmenspraxis, aktuelle  Projekte und Entwicklungen im BI-Bereich oder KI-Initiativen ihrer Unternehmen vor., Erkan Turna (Landesbank Baden-Württemberg) zeigte die nächste  Entwicklungsstufe für den Datenfluss im Datenmanagement der Landesbank auf. In der Folge diskutierte er die Relevanz verschiedener Herausforderungen, die aktuell in seinem Team thematisiert werden. Michael  Vogl und Dr. Xuanpu Sun (Mercedes-AMG) stellten Datenhaltung und Analysen für Testfahrzeugte vor. Herr Vogl stellte die wichtigsten Erkenntnisse aus seiner Tätigkeit vor, darunter zum Beispiel zur Zusammensetzung des Teams oder der Priorisierung der Aufgaben. Im  Vortrag von Marc Schanbacher (Allianz) wurden reale Ergebnisse aus dem Process Mining bei der Allianz im Kontext der kontinuierlichen Prozessverbesserung eingeordnet und vorge-stellt. Anschließend stellte Herr Schanbacher vor, wie mit Robotic Process Automation definierte  Prozessschritte automatisiert wurden und werden., Abgerundet wurde der Praxispart durch die Vorträge von Prof. Dr.  Hans-Georg Kemper und Dr. Henning Baars (beide Universität Stuttgart) sowie Prof. Dr. Heiner Lasi (Ferdinand-Steinbeis-Institut der Steinbeis-Stiftung), die aktuelle Forschungsergebnisse aus der BIA vorstellten. Im Fokus standen dabei der Aufbau einer unternehmensspezifischen KI-Governance und das Potenzial von Analyseszenarien, die auf einer unternehmensübergreifenden Datenhaltung  basieren. Im Anschluss an die Vorträge hatten die Teilnehmer Gelegenheit, mit den Referenten zu diskutieren. Themen in der Diskussion waren unter anderem die Relevanz von Werten beim KI-Einsatz, das Risiko bei der Automatisierung von Prozessen durch Prozess Roboter sowie die  Möglichkeiten der Steigerung der Wettbewerbsfähigkeit durch innovative  Datenmanagementansätze., Jetzt schon vormerken:Das nächste TDWI-Anwenderforum in Stuttgart ist für Oktober 2020 geplant., Vorher veranstaltet der TDWI e.V. das nächste Anwenderforum am Freitag, 28. Februar 2020, zusammen mit der HWTK Leipzig. , Die Digitalisierung manueller Prozesse verspricht hohe Mehrwerte für Unternehmen. In Zeiten knapper IT-Ressourcen ist es für Unternehmen jedoch schwierig, Softwareentwickler mit dem nötigen Fachwissen  vorzuhalten. Low-Code Plattformen versprechen dieses Dilemma zu lösen,  indem sie die Erstellung von Apps ohne tiefgreifende Programmierkenntnisse ermöglichen. Dadurch soll der Fachbereich selbst manuelle Prozesse automatisieren und nutzenstiftende Anwendungen bauen können., In dieser Veranstaltung werden zwei Anwendungen von den Teilnehmern  selbst unter Anleitung erstellt. Neben einer Veranstaltungsapp wird auch ein Sprachassistent im Kontext TDWI am Beispiel der Microsoft Power Platform gebaut. Im Anschluss folgt eine kritische Diskussion auch über die technologischen Aspekte hinaus., Weitere Informationen finden Sie auf unserer Webseite., Die Teilnehmerzahl ist begrenzt auf 30 Personen. Die Plätze werden chronologisch nach Anmeldung vergeben. Bei mehr Anmeldungen eröffnen  wir eine Warteliste., Allgemeine Fragen zum Blog, zum Verein und zum Thema AutorIn werden:Sandra SteingrubeCommunity Managersandra.steingrube(at)tdwi.eu, Fragen zum Inhalt:Leif HitzschkeTDWI e.V. Vorstandsmitgliedleif.hitzschke(at)tdwi.eu, TDWI e.V.Lindlaustraße 2cD-53842 TroisdorfTel: +49 (0)2241/2341-212, ImpressumDatenschutz";https://blog.tdwi.eu/business-intelligence-analytics-mode-oder-trend/;TDWI;Jens Lachenmaier
08. Jan 20;Augmented Analytics – Das Beste aus Mensch und Maschine?;"Das neue Jahr hat gerade begonnen und somit wagen wir einen Ausblick auf die Trends der Zukunft. Überall sind sie zu finden, die „Top 3“ (oder 5 oder 10) Trends für Data &amp; Analytics für 2020. Und obwohl sich die Einschätzungen natürlich nicht vollständig überdecken, gibt es doch einen erstaunlich breiten Konsens in einem Punkt: Das Thema „Augmented Analytics“ wird in nahezu jedem Trendbericht erwähnt (z. B. 1)., Was verbirgt sich hinter Augmented Analytics und warum wirddieses Thema als so wichtig eingeschätzt? „Augmented“ lässt sich in diesemKontext mit erweitern, vergrößern oder ergänzen übersetzen. „AugmentedAnalytics“ ist eine Form der Datenanalyse, bei der der Mensch stärker durch dieMaschine unterstützt wird indem mehr Vorgänge als bislang automatisiert werden,technisch mit Hilfe von Verfahren aus dem Bereich Künstlicher Intelligenz,speziell Verfahren des Maschinellen Lernens und aus dem Segment NaturalLanguage Processing (NLP, maschinelle Verarbeitung natürlicher Sprache). , Konkret ist die wohl anschaulichste Form von Augmented Analytics der Zugang zur Datenanalyse über eine natürlichsprachige Schnittstelle. Stellen Sie sich vor, Sie können Ihr System „Alex“ fragen: „Alex, wie haben sich die EMEA Umsätze im letzten Quartal im Vergleich zum US-Markt entwickelt?“. Die Kunst besteht darin, diese natürlichsprachige Anfrage in eine technische Analyse zu übersetzen, die richtigen Daten heranzuziehen, das Ergebnis geeignet aufzuarbeiten und dem Anwender schließlich geeignet zu präsentieren. , Was auf den ersten Blick einfach klingen mag, ist im Detail weitaus komplexer als man denken mag. Bei einer Frage wie „Alex, was ist diesen Monat bedeutsames passiert“ wird schnell klar, dass eine sinnvolle Antwort nur mit viel Kontextinformationen z. B. zum Geschäftsumfeld oder dem Fragenden gegeben werden kann. Die Herausforderungen liegen im Beispiel nicht (nur) in der natürlichsprachigen Schnittstelle, sondern in der „intelligenten“ Vorverarbeitung und Analyse von Daten und der Identifikation relevanter Auffälligkeiten, die es wert sind, dem Analysierenden berichtet oder geeignet vorgelegt zu werden. , Bislang waren viele dieser Aufgaben dem Menschenvorbehalten. Durch aufwändige Ersteinrichtung und routinemäßige Betrachtungeiner Vielzahl von Auswertungen wurden Auffälligkeiten identifiziert, die es zuhinterfragen lohnt. Mit Augmented Analytics verschieben sich Teile dieser Aufgabenin Richtung Maschine: wie auch an anderen Stellen lassen sich mehr Vorgängedurch den Einsatz von Methoden aus dem Bereich der Künstlichen Intelligenzautomatisieren. Der Mensch bleibt dabei Adressat der mutmaßlichen Erkenntnisse,wird aber effektiver in seinen Entscheidungsprozessen unterstützt. „Augmented analytics representsa new approach to problem-solving that supports humans in the decision-makingprocess, not replace them.“ 2, Augmented Analytics verbessert die Zugänglichkeit derDatenanalyse dramatisch und hat das Potenzial, wirklich jedem Mitarbeiter Datenund Erkenntnisse hieraus zugänglich zu machen. Datenanalyse wird so einfachzugänglich wie der Wetterbericht. Schon seit geraumer Zeit wird im BereichBusiness Intelligence diskutiert, dass zu wenige Fachanwender wirklichdatenbasiert arbeiten und Zugang zum Erkenntnisgewinn aus Daten haben. Zwar hatsich ausgehend vom klassischen Reporting die Datenanalyse u.a. durch EmbeddedAnalytics und durch Self-Service Werkzeuge erheblich vereinfacht und von der ITbzw. dem CoC Business Intelligence in Richtung der Fachanwender verschoben.Trotzdem sind Hürden verblieben, die Augmented Analytics versprichteinzureißen., Natürlich benötigt aber auch Augmented Analytics dieentsprechenden Daten im Hintergrund, möglichst geordnet (katalogisiert) und inhoher Qualität. Ebenso sind vielfältige Kontextinformationen erforderlich umsinnvoll auch auf abstraktere Fragen und gezielt den Bedarf des jeweiligenAnwenders Antworten zu geben. Um ein solche Infrastruktur zu schaffen istweiterhin viel menschliche Arbeit erforderlich, wenn auch die Unterstützungdurch „intelligentere“ Ansätze in allen Bereichen – von der Datenorganisationbis zum Erkenntnisgewinn – stetig besser wird., Tatsächlich hat Gartner bereits Mitte 2017 einen vielbeachteten Bericht mit dem Titel „Augmented Analytics Is the Future of Data and Analytics“ 3 herausgegeben und wiederholte Ende 2019 diese Prognose. Im „Hype Cycle for Analytics and Business Intelligence, 2019“ steht Augmented Analytics auf dem maximalen Punkt der Erwartungen, direkt am „Peak of Inflated Expectation“ 4. Vielleicht sind die Erwartungen also zu hoch, möglicherweise erwarten wir zu viel zu schnell. Die grobe Richtung scheint jedoch unstrittig, alleine der Weg braucht mehr Zeit. , Wie in anderen Bereichen auch, wird die Fähigkeit vonSoftware auch im Bereich Data &amp; Analytics um neue Werkzeuge aus dem BereichKünstliche Intelligenz erweitert, so dass insgesamt der Automatisierungsgradsteigt. Letztlich hilft dies dem Menschen, der in Zusammenarbeit mit derMaschine produktiver wird. , 1 https://www.gartner.com/smarterwithgartner/gartner-top-10-data-analytics-trends/, 2 Gartner,Augmented Analytics Is the Future of Analytics, Rita Sallam, Carlie Idoine, 30October 2019 , 3 https://www.gartner.com/en/documents/3773164/augmented-analytics-is-the-future-of-data-and-analytics, 4 https://www.gartner.com/en/newsroom/press-releases/2019-10-02-gartner-reveals-five-major-trends-shaping-the-evoluti, Allgemeine Fragen zum Blog, zum Verein und zum Thema AutorIn werden:Sandra SteingrubeCommunity Managersandra.steingrube(at)tdwi.eu, Fragen zum Inhalt:Leif HitzschkeTDWI e.V. Vorstandsmitgliedleif.hitzschke(at)tdwi.eu, TDWI e.V.Lindlaustraße 2cD-53842 TroisdorfTel: +49 (0)2241/2341-212, ImpressumDatenschutz";https://blog.tdwi.eu/augmented-analytics/;TDWI;Gero Presser
28. Dez 19;Analytics-Strategie – yet another strategy?!;"Mit dem folgenden Beitrag möchte ich aus meiner Erfahrung mal ein paar Anlässe und auch gute Gründe für das Thema Analytics-Strategie diskutieren, denn es ist wichtig zu wissen, warum man sich jenseits der Marketingstimmen damit als Organisation befassen sollte.  Business-Intelligence-Strategie, Datenstrategie, Digitalisierungsstrategie, IT-Strategie, Big-Data-Strategie und jetzt noch eine Analytics-Strategie –  wie viele Strategien braucht eine wettbewerbsfähige Organisation, damit sie wettbewerbsfähig bleibt? Pauschale Antworten auf diese Frage sind schwierig, aber es gibt beispielsweise folgende Indikatoren für die Sinnhaftigkeit:, Auch die Existenz erster Analytics-Projekte in Ihrer Organisation sind ein guter Anlass, über eine analytische Strategie nachzudenken. Denn nur so können Sie technische und prozessuale Umgebungen schaffen, die den Erfolg der Projekte langfristig sichern, um Insellösungen zu verhindern und Hürden bei der Betriebsübernahme frühzeitig zu adressieren und bestmöglich zu umschiffen., Wir brauchen eine Analytics-Strategie! Und nun?Eine Analytics-Strategie sollte einerseits stets mit der Findung eines strategischen Zielbildes beginnen, also eine Antwort auf die Fragen:  „Warum brauchen wir Analytics und was wollen wir damit erreichen?“. Mit der Erfahrung aus Projekten zeigt sich, dass dazu eine gemeinsame Definition für Analytics in der konkreten Organisation zu finden ist., Natürlich gibt es Lehrbuchdefinitionen, aber: ist die OCR-Erkennung in Dokumenten nun von der Analytics-Strategie erfasst oder ist sie ein ganz anderes Thema, weil es nur Bestandteil der Werkzeuge ist? Und was hat Artificial Intelligence mit Analytics zu tun? Sie bemerken sicherlich, dass es nicht so einfach ist, die Diskussion zu führen. Wichtig ist aber, dass diese Diskussion geführt wird und in einem Miteinander Festlegungen erfolgen, die als Rahmen den Erfolg Ihrer Analyticsbemühungen eher sichern., Andererseits ist es für eine Analytics-Strategie wichtig, konkrete Analytics- Themen zu sammeln und neue Projektideen zu entwickeln. Diese Themen sind es, die Ihren Wettbewerbsvorteil generieren und im Sinne eines „Business Case“ den zentralen Treiber für eine Analytics-Strategie  darstellen. Lassen Sie sich beispielsweise von Methoden wie dem System Thinking zur Kreativitätsförderung und auch dem Periodensystem der KI inspirieren! Wenn sich allerdings organisationsweit keinerlei Themen ergeben, die sich unter der gemeinsamen Definition von Analytics subsummieren lassen, ist auch die detaillierte Ausgestaltung einer Analytics-Strategie mindestens in Frage zu stellen. Diese sollte Ihnen ja immer die Frage beantworten können, warum Sie als Unternehmen diesen Aufwand überhaupt auf sich nehmen., Die Gestaltung einer Analytics-Strategie erfolgt dann in den Dimensionen: Personen (Ressourcen), Kultur, Organisation, Prozesse, Architektur, Werkzeuge und ganz zentral Daten – mehr dazu in einem meiner nächsten Blog-Beiträge (stay tuned ;))!, Wichtig ist allerdings: stimmen Sie die Analytics-Strategie mit den eingangs benannten Strategien ab! Überschneidungen zur Business-Intelligence- und zur Datenstrategie sind beispielsweise bei Themen wie Data Governance inklusive Datenschutzkonzept,  Datenqualitätsmanagement oder auch Datenversorgung und Datenspeicherorten zu erwarten. Auch das Thema Digitalisierung ist beispielsweise in der Realisierung bezüglich der zu betrachtenden Prozessziele vielleicht nur über den Einsatz von Analytics in den Prozessen darstellbar. Damit sind die Analytics-Ressourcen auch ein Asset für die Digitalisierung. Die Überschneidungen zur IT-Strategie sind bei Architektur und Werkzeugwahl zu finden. Die Abstimmung ist also wichtig für den Erfolg aller Strategien und zeigt, dass Analytics durchdacht in einer Analytics-Strategie in den Unternehmen Teil des Wandels sein sollte, um verstetigte Wettbewerbsfähigkeit zu erhalten!, Jetzt möchte ich es nicht verpassen allen Lesern zu danken und einen guten Start in ein von Glück und Gesundheit geprägtes 2020 zu wünschen., Allgemeine Fragen zum Blog, zum Verein und zum Thema AutorIn werden:Sandra SteingrubeCommunity Managersandra.steingrube(at)tdwi.eu, Fragen zum Inhalt:Leif HitzschkeTDWI e.V. Vorstandsmitgliedleif.hitzschke(at)tdwi.eu, TDWI e.V.Lindlaustraße 2cD-53842 TroisdorfTel: +49 (0)2241/2341-212, ImpressumDatenschutz";https://blog.tdwi.eu/analytics-strategie-yet-another-strategy/;TDWI;Claudia Koschtial
18. Dez 19;Schneller, professioneller, systematischer – Wie entwickelt man heute einen Datalake/ein DWH/eine Machine-Learning-Plattform?;"Die globale Konkurrenz zwingt Unternehmen heutzutage zu einer immer größeren Geschwindigkeit bei der Umsetzung von Projekten. Das gilt auch für Projekte, bei denen es um die Integration von Daten und den Aufbau von Datalakes, Data Warehouses oder Plattformen für Machine Learning geht. Thema des Abends war daher die Betrachtung von Möglichkeiten zur Steigerung der Produktivität von solchen Projekten durch einen generischen Ansatz: Man leistet gewisse Vorarbeiten, um anschließend wesentlich schneller und mit höherer Qualität konkrete Anwendungsfälle bzw. Projekte umsetzen zu können., Herr Henrik Behrens von der Data Reply GmbH führte in das Thema ein, indem er folgende Fragen beantwortete:, Zum Thema gab es anschließend zwei Fachvorträge jeweils der Unternehmen ProSiebenSat.1 und Allianz Clobnal Corporates &amp; Specialities (AGCS). Beide Unternehmen haben den gewählten Ansatz auch in Form einer Live-Demo präsentiert, so dass die Anwesenden einen sehr konkreten Eindruck von der jeweiligen Arbeitsweise bekamen:, Im Vortrag „Winning the productivity race“ haben Johannes Dieterich und Alexander Crusciel von ProSiebenSat.1 gezeigt, wie sie Model Driven Development (MDD) mit einem generischen Plattform-Ansatz kombiniert haben, um die Time-to-Market zu minimieren. Dabei kam ein Framework zum Einsatz, das eine selbstentwickelte Kommandosprache für die Definition der Ladestrecken verwendet und in einer sehr rudimentären Form von Herrn Behrens beim 15. TDWI Roundtable in München unter dem Titel „Implementierung eines DWH mit Spark in 60 Minuten“ vorgestellt worden war. Außerdem kommt mit dem „MID Innovator“ ein Modellierungswerkzeug zum Einsatz, mit dem eine Geschäftsobjektmodellierung gemacht wird, aus der vollautomatisch ein Raw Data Vault-Datenmodell und das Mapping zu den Quelltabellen generiert wird. Das Framework erlaubt es, das Mapping einzulesen und führt dann automatisch die notwendigen Ladeprozesse mit Spark aus, ohne dass eine Programmierung seitens der Anwender erforderlich wäre. Für den Business Vault und die Datamarts definieren die Anwender die fachliche Logik in Form von Views, und das Framework erledigt die Materialisierung dieser Views automatisch in der richtigen Reihenfolge und erlaubt diverse Delta-Modi zur inkrementellen Aktualisierung der transformierten Daten., Der zweite Vortrag „Data Warehouse Automation mit Domain Specific Languages und Data Vault“ von Wolfgang Tanzer (AGCS), Johannes Reitzner und Mathias Höreth (metafinanz) zeigte einen Ansatz, bei dem eine Sprache zur Definition von Data Vault-Strukturen mit fachlicher Logik entwickelt wurde, die eine frühe Validierung erlaubt. Damit ist gemeint, dass Fehler bereits automatisch während der Eingabe gefunden werden, bevor der Code tatsächlich deployt und ausgeführt wird. Weitere Vorteile dieses Ansatzes sind neben der automatischen Generierung des Codes auch das einfache Refactoring des Datenmodells und die Generierung von Data Lineage-Graphen., Das Interesse der Teilnehmer war groß: Der Roundtable war schon Tage vorher ausgebucht, und es gab während der Vorträge zahlreiche Fragen und Diskussionen, und auch nach Ende der Vorträge gab es noch einen intensiven Austausch zwischen den Teilnehmern beim Buffet bis zum Ende der Veranstaltung um 21.15 Uhr., Hello to every body, it’s my first pay a quick visit ofthis webpage; this blog includes amazing and actually good material for readers., Allgemeine Fragen zum Blog, zum Verein und zum Thema AutorIn werden:Sandra SteingrubeCommunity Managersandra.steingrube(at)tdwi.eu, Fragen zum Inhalt:Leif HitzschkeTDWI e.V. Vorstandsmitgliedleif.hitzschke(at)tdwi.eu, TDWI e.V.Lindlaustraße 2cD-53842 TroisdorfTel: +49 (0)2241/2341-212, ImpressumDatenschutz";https://blog.tdwi.eu/wie-entwickelt-man-heute-einen-datalake-ein-dwh-eine-machine-learning-plattform/;TDWI;Henrik Behrens
11. Dez 19;Vergleich der Tools APEX, ELK und Splunk für Monitoring und Analytics;Monitoring und Analytics spielen in zunehmend komplexen Systemlandschaften eine immer größere Rolle. Sie unterstützen nicht nur die Erkennung des derzeitigen Zustandes, sondern auch das Treffen der richtigen Entscheidungen und schnelle Reaktionen., Aus dem hohen Bedarf entstand eine Fülle an verschiedenstenPlattformen und Anwendungen, welche große Datenmengen anschaulich präsentierenund weiterverarbeiten. Nicht immer ist es daher trivial, die beste Technologieentscheidungzu treffen. Um dies zu erleichtern, sollen die Ergebnisse des Vergleichs dreierausgewählter Werkzeuge nachfolgend kurz vorgestellt werden, um deren Stärken,Schwächen und geeignete Anwendungsfälle aufzuzeigen. , Verglichen wurden die Frameworks Splunk Enterprise 7.3, ELKBasic 7.3 und Oracle APEX 5.1, zunächst mit dem Fokus auf Datenbankmonitoring.Später wurden auch weitere eigene Produkte und Standardanwendungen in dasMonitoring integriert und Analysen vorgenommen. Der Vergleich erfolgte jeweilsmit gleichartigen strukturierten und unstrukturierten Daten und Zielen. JeglicheSoftware wurde on-premises betrieben, wobei auch Cloud-Angebote existieren., Oracle APEX und ELK Basic sind frei verfügbar, ELK bietet auch eine Open Source Implementierung an. Splunk hingegen ist ein nach der Menge täglich indizierter Daten lizenziertes Produkt. Alle drei Lösungen nutzen eine Weboberfläche zur Visualisierung., APEX läuft in der relationalen Oracle Datenbank. Die Entwicklung von Monitoring-Anwendungen setzt damit auch eine entsprechende Datenmodellierung voraus. Maßgebliche Einschränkung ist für APEX allerdings die geringe Menge an vordefinierten Schnittstellen zum Einlesen von Daten, insbesondere von Echtzeitdaten. Es handelt sich eher um ein webbasiertes Anwendungsentwicklungs- und Auslieferungswerkzeug als um eine Monitoring und Analytics Plattform. Daher wurde hier nur eine Lösung für Datenbanken geschaffen, welche mittels PL/SQL-Jobs die entsprechenden Daten in eine Performance-Datenbank lädt, auf deren Tabellen wiederum APEX zur Visualisierung zugreift. Verschiedene Diagrammformen und formatierte Tabellen stehen zur Veranschaulichung bereit, APEX unterstützt dabei das Editieren der Daten am stärksten. Bezüglich der Darstellung von Dashboards im Browser sind alle drei Werkzeuge sehr ähnlich, daher soll ein Beispiel genügen., Weit mehr Schnittstellen und Streaming-Unterstützung bieten ELK und Splunk. ELK basiert auf drei eigenständigen Softwarekomponenten, deren Entwicklung und Release synchronisiert wird – Elasticsearch (Speicherung), Logstash (Vorverarbeitung) und Kibana (Visualisierung). Auch ELK verlangt die Definition eines festen Schemas und die Extraktion relevanter Felder beim Einlesen der Daten. Diese können über verschiedenste Eingaben verfügbar gemacht werden. Beispielsweise kann dies über Agenten auf den Quellen, sogenannte Beats, erfolgen. Für verschiedenste Einsatzzwecke werden Module mitgeliefert, welche sehr einfach ein komplettes Setup von der Ingestion bis zum Dashboard in wenigen Schritten ermöglichen., Benötigt man weitere Flexibilität, da sich beispielsweise Logformate öfter ändern oder tiefgreifende, wechselnde Maschinendatenanalysen möglich sein sollen, so ist Splunk das Werkzeug der Wahl. Während APEX und ELK die Struktur der Daten bereits bei der Ingestion vorgeben, bildet Splunk neben diesem Schema-On-Write auch ein Schema-On-Read. Somit können auch nach dem Einlesen der Daten, zur Suchzeit, noch weitere Felder extrahiert werden oder auch andere Manipulationen erfolgen. Dadurch kann eine leichte Anpassung ohne Erfordernis des erneuten Datenladens erfolgen. Dies kann über einen Assistenten oder reguläre Ausdrücke realisiert werden. Daneben erhöht die mächtige Splunk-Abfragesprache „Search Processing Language“, kurz SPL, die Auswertungsmöglichkeiten beträchtlich. Sie unterstützt neben statistischen Auswertungen auch Transaktionsverarbeitung, Feldwertvergleiche, maschinelles Lernen, Prognosen und viele weitere Funktionen zur Suchzeit. Auch sind in der Splunk Enterprise Lizenz nützliche Features wie Alerting integriert, welche auch in ELK eine kostenpflichtige Lizenz erfordern. Für große täglich indizierte Datenmengen kann die Splunk Enterprise Lizenz allerdings auch zu teuer werden, obwohl der Preis je Gigabyte mit wachsendem Lizenzvolumen geringer wird., Im erweiterten Anwendungsfall des Vergleichs ELK versus Splunk wurden E-Mails mit Datenbankstatusmeldungen im JSON-Format in ELK und Splunk eingelesen. Dabei zeigte sich, dass ELK für strukturierte Daten ähnlich schnell wie Splunk zu einer ähnlichen Monitoringlösung führen kann, insbesondere wenn die Datenstrukturen keine Änderungen erleben. Durch die bei Splunk funktionsreichere Abfragesprache sind die Möglichkeiten zur Analyse in Splunk allerdings erheblich mächtiger. Hier benötigt ELK einen höheren Entwicklungsaufwand und meist auch externe Werkzeuge und Programmierung. Zu ELK kann weiterhin festgehalten werden, dass es den höchsten Nutzen bietet, wenn die Daten und Analysen bekannt, nahe am Standard und nicht änderungsfreudig sind. Splunk hat seine Stärken hingegen in der Arbeit mit unstrukturierten, sich oft ändernden Daten und Anforderungen., Zusammenfassend ging aus dem Vergleich über unserem Anwendungsfall hervor, dass jedes der Werkzeuge in seinem Spezialgebiet am geeignetsten ist. Dies zeigt auch die Entwicklung hin zu Lösungen, welche jeweils „Best-of-Class“-Technologien nutzen und verbinden. Während APEX seine Stärken in Nutzerinteraktion, Bearbeitung und programmatischen Abläufen hat, sind die beiden anderen Werkzeuge in ihrer Funktion ähnlicher und stärker auf Auswertungen ausgerichtet. Daher soll abschließend ein übersichtlicher Vergleich beider Produkte anhand einiger Entscheidungskriterien gegeben werden., , Allgemeine Fragen zum Blog, zum Verein und zum Thema AutorIn werden:Sandra SteingrubeCommunity Managersandra.steingrube(at)tdwi.eu, Fragen zum Inhalt:Leif HitzschkeTDWI e.V. Vorstandsmitgliedleif.hitzschke(at)tdwi.eu, TDWI e.V.Lindlaustraße 2cD-53842 TroisdorfTel: +49 (0)2241/2341-212, ImpressumDatenschutz;https://blog.tdwi.eu/vergleich-der-tools-apex-elk-und-splunk-fuer-monitoring-und-analytics/;TDWI;Tom Fels
04. Dez 19;Data Science, Business Intelligence and Analytics – Analyzing differences and similarities with Python – Update: Map added;Within the last years, the data world was confronted with a new reality. The availability of data and computing power has increased dramatically and so have the job roles. The most prominent among this new role is the Data Scientist. The definition is as follows: , „Someone who studies or works in data science (= the use of scientific methods to obtain useful information from computer data)“ (Cambridge University Press 2014) , So how does it fit into the other roles related to data: Business Intelligence and Analyst?, This article has the aim to shed more light into these fields by first describing the roles and then analyze the job market in Germany with the help of Python., Data Science,Business Intelligence and Analytics – is it all the same?, Looking at this question – my answer would be: No it is not the same, but there are similarities., Data Science uses structured (tables) and unstructured (text documents, pictures, videos) data to extract insights and patterns. Data Science focuses on future information while Business Intelligence focuses on past data. Both share Data Virtualization and Data Management. This picture summarizes it very well:, Analytics on the other hand is: “A process in which a computer examines information using mathematical methods in order to find useful patterns” (Cambridge University Press 2014). , This short overview gave us an insight aboutthe three fields and their differences as well as similarities. Let us have alook on the job market in Germany. The analysis of this market can show uswhere and how these fields are emerging. , Job Market for DataScience, Business Intelligence and Analytics in Germany:, In an analysis from September 2018 it wasclaimed that there are 900 jobs under the keyword “data science” in Germany (Piatetsky 2018). Can we do betternowadays? , For this analysis, I got data from three jobsearches on LinkedIn: “Data Scientist”, “Analyst” and “Business Intelligence”(BI). There are roundabout 975 items for each search term, totaling 2,923 jobs., There are more jobs on LinkedIn for these areaswith most of the jobs under the BI term. Analyst is second and Data Sciencelast. Data Science job offers increased by already 2,784 jobs, which is anincrease of more than 400% to last year!, The full amount of jobs from this search isstill only 5% of the IT function job market in Germany. , Job Title:, A look on the Top 10 mentioned Job titlesreveals that Data Scientist, Data Analyst and Business Intelligence Analyst arementioned mostly. , There seems to be some overlap in the data so Icounted the occurrence of the titles in the data. Interestingly, mostly theword “Analyst” was mentioned. The analyst term is used in BI and Data Sciencejobs quite frequently. The fourth largest group shows jobs with none of thesearch terms. Which are they?, They are mostly coming from the BI search butthe most mentioned job was the Data Engineer which is as important as the threeother jobs. , Companies: , Most of the jobs have been posted byCampusjäger which is a recruitment agency from Germany focused on students andyoung professionals. , City:, You will probably have guessed it already and you are correct: , Most of the jobs are placed in Berlin, followed by Hamburg and Munich. The map confirms our observation – most of the jobs are located in these cities and but also in other areas such as the Ruhrgebiet. They have in common that they are urban areas., Lets look on the distribution with the seaborn heatmap. For this chart I  applied a standard scaler which means that we are comparing which jobs  are more frequent per city (darker color). It means for example that  there are more Analysts in Berlin than jobs from the BI and Data Science  term. Let us focus on Frankfurt for example: More Analysts than BI and  Data Science. This could be caused by the financial industry which is  relying on Analysts. Hannover on the other hand has more Data Scientists  – caused by the major insurance companies situated here and which are  starting to explore this domain.  , Time posted: , Most of the jobs have been posted within thelast 1 week up to 4 months. , Seniority level: , Good news for young applicants, most of thejobs can be found in the Entry Level area. Standing out here is the DataScience domain, which is relatively new and there are not so many Executive andDirector positions compared to the established BI area. , Job Function –first mention:, The top three job function areas are BusinessDevelopment, IT and Engineering. , While Analysts and BI experts are dominatingthe Business Development area, the IT function uses more Data Scientists. DataScientists dominate the Engineering area. , As being outlined before, Finance is heavilydominated by Analysts., Industry – firstmention:, Almost 50% of the jobs are situated in the ITindustry. , The IT industry is dominated by BI experts andData Scientists while Marketing, Internet and Chemicals rely on more Analystsin their industry. , Word Cloud: , A good way to analyse the unstructuredinformation from the job offer texts are word clouds or word maps. They showthe most frequent words and the word size describes the frequency. , A look on the Word Map for Analyst:, Data Science:, Business Intelligence:, All three rely on the word “team” and “data”heavily. The Analyst word cloud shows that the word customer is quiteimportant. On the Data Science map, the word “Machine Learning” is veryimportant. Business Intelligence and Data Science share the importance on “BigData”. Business Intelligence then focuses of course on “Data Warehouse”. , Coming to the end of this article, it can bestated that Analytics, Business Intelligence and Data Science share somesimilarities but do also have differences. These differences and similaritieswere underlined by the analysis of the German job market for these three areas., The three areas share the passion for data,which is in my opinion the most important similarity., You can have a look on the full code on nbviewerhere or on GitHub. , Cambridge University Press. 2014. Analytics. https://dictionary.cambridge.org/de/worterbuch/englisch/analytics., Cambridge University Press. 2014. „Data Scientist.“ https://dictionary.cambridge.org/dictionary/english/data-scientist., Piatetsky, Gregory. 2018. How many data scientists are there and is there a shortage? 09.   https://www.kdnuggets.com/2018/09/how-many-data-scientists-are-there.html., Pugsley, Stan. 2017. Understanding the Differences Between Data Science and BI. 5.12.   https://tdwi.org/articles/2017/12/05/bi-all-understanding-differences-data-science-and-bi.aspx., , Allgemeine Fragen zum Blog, zum Verein und zum Thema AutorIn werden:Sandra SteingrubeCommunity Managersandra.steingrube(at)tdwi.eu, Fragen zum Inhalt:Leif HitzschkeTDWI e.V. Vorstandsmitgliedleif.hitzschke(at)tdwi.eu, TDWI e.V.Lindlaustraße 2cD-53842 TroisdorfTel: +49 (0)2241/2341-212, ImpressumDatenschutz;https://blog.tdwi.eu/data-science-business-intelligence-and-analytics-analyzing-differences-and-similarities-with-python/;TDWI;Armin Geisler
27. Nov 19;Neulich in Zürich sollte das BICC abgeschafft werden …;Anfang November fand in Zürich die alljährliche TDWI Konferenz Schweiz statt. Sie ist etwas kleiner als die Schwester in München, aber man kommt dadurch besonders gut in den Austausch mit anderen Anwendern und verliert sich auch nicht aus den Augen. Eine Plattform des Austauschs bieten beispielsweise die interaktiven World Cafés. So hatte ich das Glück das spannende Thema “Hat das BI Competence Center ausgedient?” am Tisch zu moderieren. Ein Business Intelligence Competence Center (BICC) soll (grob formuliert) eigentlich dazu dienen, zentral Projekte und Aufgaben im Bereich Business Intelligence zu koordinieren. Ich erwartete eine breite Diskussion und wurde keineswegs enttäuscht. Der Tisch war sehr gut besucht und die Teilnehmer repräsentierten dann in der Diskussion ein breites Spektrum. , Eingangs diskutierten wir ganz provokativ, ob sich der Name und das BICC überholt habe. BI sei schließlich in der Wahrnehmung im Markt ein ausgedientes Thema. Dieser These kann ich mich wirklich nicht anschließen. Auch die Diskussion ergab dann einstimmig, dass die Aufgaben des BICC wie Data Governance (Data Catalogue, Data Quality, Master Data Management, …), Standardisierung, zentrale Datenbereitstellung, Entscheidung über Use Cases und der Bedarf an Bereitstellung eines zentralen Know-Hows absolut vital sind. Lediglich die Umsetzung von konkreten Fachbereichsanforderungen scheint sich beispielsweise durch Self Service zu verändern.  , Ein Teilnehmer berichtete, dass man das BICC gar nicht mehr braucht und man das BICC in seiner Organisation komplett aufgelöst habe. Dennoch war das Thema Standards und Leitplanken auch nach seiner Meinung noch wichtig, so dass man die zu koordinierenden zentralen Themen in sogenannten Chapters (Chapter Frontend, Chapter Datenqualität usw.) definiert, und die eigentliche Arbeit dann immer direkt im Fachbereich stattfindet, also keine Trennung mehr in der Umsetzung zu spüren ist. Auch existieren konkrete Verantwortlichkeiten für bestimmte Datenbereiche und damit auch eine fachliche Hoheit analog zu den Data Stewards. Die Organisation findet also in einer Art Matrix statt. Nach 1,5 Jahren Erfahrung zeigte sich aus seiner Sicht deutlich eine größere Nähe zum internen Kunden und mehr Zufriedenheit auf beiden Seiten, denn das Verständnis für die Herausforderungen des anderen sind durch mehr Nähe gegeben. Insgesamt ist das eine Bewegung, die man auch im Markt deutlich spürt: die Umsetzung (vormals IT) wandert immer näher an die Fachbereiche und sichert somit höhere Anforderungsnähe und Zufriedenheit., Ein anderer Teilnehmer berichtete, dass das BICC nach wie vor existiert, sich aber eigentlich nur noch um die zentrale Datenbasis – das Enterprise Data Warehouse kümmert – die Umsetzung in Frontends usw. findet bei den eigentlichen Fachanwendern statt, denn Werkzeuge wie Power BI (das wurde mehrfach konkret benannt) ermöglichen es den Fachbereichen, ihre Anforderungen selbst umzusetzen. Dabei ist es aus seiner Sicht wichtig agiler zu entwickeln und höhere Geschwindigkeiten zu haben, aber eben auch datenseitig einen Single-Point-of-Truth zu schaffen und gemeinsam Standards zu setzen. , Schlussendlich wurde auch sehr zufrieden von einem zentralen BICC mit den klassischen Rollen berichtet. So wurden noch zentral Anforderungen aufgenommen und aufbereitet sowie anschließend umsetzt. Somit stellt das BICC als starker zentraler Dienstleister mit zentral definierten Rollen nach wie vor Zufriedenheit im Fachbereich sicher, hier aber deutlich mit dem Thema Standardreporting im Vordergrund. Es waren also alle Positionen dabei. , Hier ein Bildbericht:, Abgerundet hat diesen Eindruck dann noch der Vortrag von Herrn Scheller und Herrn Dr. Brabec (Helsana), die über die erfolgreiche Veränderung des BICC auf dem Web zur Agilität berichteten. Hier wurden auch noch einmal Konzepte wie Scrum für größere Entwicklungen und Kanban für die Transformation beschrieben. Eine gelungene Darstellung mit Hürden und Erfolgen und sogar einer entsprechenden Umfrage, die Auswirkungen der Veränderungen belegte., Im Ergebnis muss man festhalten, dass es kein Konzept gibt welches nach Schema F den Erfolg in jeder Organisation sichert, sondern je nach Ausgangslage passende organisatorische Strukturen geschaffen werden müssen. Agilität war als Thema spürbar und auch der Wunsch, die Grenzen zwischen IT und Fachbereich so zu gestalten, dass ein echter und zielführender Dialog mit mehr Zufriedenheit auf allen Seiten möglich wird. , Allgemeine Fragen zum Blog, zum Verein und zum Thema AutorIn werden:Sandra SteingrubeCommunity Managersandra.steingrube(at)tdwi.eu, Fragen zum Inhalt:Leif HitzschkeTDWI e.V. Vorstandsmitgliedleif.hitzschke(at)tdwi.eu, TDWI e.V.Lindlaustraße 2cD-53842 TroisdorfTel: +49 (0)2241/2341-212, ImpressumDatenschutz;https://blog.tdwi.eu/neulich-in-zuerich-sollte-das-bicc-abgeschafft-werden/;TDWI;Claudia Koschtial
20. Nov 19;Und noch einen Schritt weiter …;Willkommen im Blog des TDWI – der größten Community für Dataund Insights in Europa. Nächste Woche starten wir mit dem ersten Beitrag vonClaudia Koschtial zum Thema „Hat das BI Competence Center ausgedient?”. Undjetzt haben auch Sie die Möglichkeit schriftlich dabei zu sein. Per Schrift?, Genauso ist es – der TDWI bietet Ihnen bereits eine Vielzahlan Möglichkeiten sich innerhalb der BI-Community auszutauschen: TDWIKonferenzen in München und Zürich, Roundtable, Barcamps, Apéros, Anwenderforen,Webinare, Podcast und jetzt auch einen Blog. Mehr Vielfalt geht nun wirklichnicht., Und Sie als Vereinsmitglied können sich aktiv am Blog beteiligen. Wir wünschen uns Ihre Beteiligung, denn genau das macht den Verein aus. Hier finden Sie eine Erläuterung, wie Sie Autor im TDWI-Blog werden. Sie brauchen nicht 2000 Wörter schreiben. Für uns zählt der Inhalt., Mit einem Beitrag helfen Sie nicht nur anderenBI-Interessierten, sondern erhöhen auch ganz nebenbei Ihre Reichweite imInternet. Geben Sie ruhig Ihre Autorenseite bei der nächsten Bewerbung an.Autorenseite? Jawohl – wir veröffentlichen Ihren Beitrag nicht nur auf unsererreichweitenstarken Webseite, sondern wir stellen auch ein kleines Profil IhrerPerson online. Also legen Sie gleich los und reichen Ihren ersten Beitrag ein., Außerdem können Sie sich über die Kommentarfunktion an derfachlichen Diskussion beteiligen. Wir sind gespannt auf Ihre Beiträge,Meinungen und Kommentare., Wenn Sie noch Fragen haben, was immer es auch sein mag, dannschreiben Sie uns gerne., Ihr Leif Hitzschke, Allgemeine Fragen zum Blog, zum Verein und zum Thema AutorIn werden:Sandra SteingrubeCommunity Managersandra.steingrube(at)tdwi.eu, Fragen zum Inhalt:Leif HitzschkeTDWI e.V. Vorstandsmitgliedleif.hitzschke(at)tdwi.eu, TDWI e.V.Lindlaustraße 2cD-53842 TroisdorfTel: +49 (0)2241/2341-212, ImpressumDatenschutz;https://blog.tdwi.eu/und-noch-einen-schritt-weiter/;TDWI;Leif Hitzschke
06. Jan 21;Can Machine Learning help to forecast COVID-19 infections – Part 2 – Linear Regression;The article works with data from 25th of November., My last article was written about the ability to forecast new Corona infections with the help of Machine Learning. The first article was “setting the scene” by introducing the topic and the framework for the project. This article shall be continuing by presenting the first model –                 Linear Regression., Unfortunately, the pandemic is still in full mode and new infections are at an all-time-high. We also must deal with new problems such as reinfected patients (Cruickshank, 2020) or mutations of the Corona virus (Callaway, 2020)., Unfortunately, also, the numbers do not look encouraging, despite several lockdowns, social distancing and wearing face-masks – the amount of new infections are still increasing. The Virus has reached today around 219 countries, more than 53 Million people have been infected and more 1.3 Million people have died (Worldometers.info, 2020)., Let’s turn our attention to the question for this article:, Can Linear Regression help to forecast COVID-19 new infections?, What is Linear Regression?, Linear Regression is (as the name suggests) a regression model which is widely used by all sorts of professional in various industries. The term regression originates from Francis Galton in the 19th century. He investigated that the heights of descendants of tall ancestors tend to regress down towards a normal average. This is known as regression to the mean (Galton, 1989). , Linear Regression is one of the simplest but also very effective Machine Learning algorithms. In theory it works like this: “Linear regression attempts to model the relationship between two variables by fitting a linear equation to observed data. One variable is an explanatory variable, and the other is to be a dependent variable. For example, a modeler might want to relate the weights of individuals to their heights using a linear regression model.” (Department of Statistics, Yale University, 1998), Regression is often referred to as problems where you must predict a continuous variable such as weight or height. This is only half the truth since there are also regression models which can predict categorical variables for example Logistic Regression by using a log function., There are two forms of Linear Regression. Simple Linear Regression where there is only one input variable (x) to predict the output (y) and Multiple Linear Regression where we have multiple variables (x1, x2…) to predict y., Can Linear Regression help to forecast COVID-19 infections?, For the first (simple) example we assume that we only have one Variable and we select                 “Days since outbreak global” as our independent variable (X) and Confirmed new infections as our dependent variable (Y), The plot looks like this – for worldwide:, That looks good, each dot represents one day. The data becomes more spread out – the variance increases over time., For each Machine Learning Algorithm, we start with the model evaluation. In order to achieve that, we need to split our dataset into a train and test part. If we assume one week as our train and test split, the data will look like this:, So, the algorithm needs to fit a line in order to predict the yellow points as accurate as possible. The line is denoted with y = intercept + coefficient * x. The intercept is the line on the Y-Axis through which the line passes. The coefficient describes the slope of the line. By applying Python, we found the following:, So the 200th day of the pandemic would be described with y = -71,924 + 1,704 * 200 = 268,876 new infections., After fitting the line, the chart looks like this:, But stop! What is happening in “linear_regressor.fit”? Let’s dive in to understand it better., Basically, the command is using the Ordinary Least Squares (OLS) to fit a line to the data which will show a minimum sum of squares. The sum of squares is the sum of differences between the predicted and current data (Carvalho, 2020). The differences are called “residuals” and examples have been marked in the chart above., What we are now doing is basically extend the fitted line of the training data into the future. We are utilizing the values (Days since the outbreak global – 301, 302, 303) from the Test set. By doing that we can compare the test set points to the predicted data points., If we compare Test and the predicted values, we see already that the line is not fitting well. The data looks more curvilinear shaped. The prediction (red line) is far off the test values (orange line)., In order to compare the models, the following measures have been using: Mean Average Error (MAE), Root Mean Squared Error (RMSE) and Root Mean Squared Logistics Error (RMSLE):, Cool, so I can apply Linear Regression all the time?, We need to take care that there are certain conditions (Hariharan, 2020) are satisfied. For our example we introduced a new variable “Weekday (1-7)” for x. We are now performing multiple Linear Regression., Linearity: The error terms (or residuals) of X and Y must be linear shaped. Let’s visualize this to see it., No perfect linear relationship, it tends to underestimate by the begin of the pandemic and underestimates towards the end. , Normality of the error terms: OLS assumes that the error terms are normally distributed. The easiest way to test is a visual:, Or Anderson-Darling test for normality (Macaluso, 2018). The AD test is above our threshold (p=0.05)., This means both have failed and we can see that the model is biased towards underestimation. A way to fix this is by applying nonlinear transformations such as a quadratic function., Multicollinearity: The independent variables should not be correlated. It can cause problems while interpreting the model. A high VIF (Variance inflation factor) can be resolved by centering the variables (subtracting the mean from the value) (Frost, 2017). You can find a more information here. In our case we are in acceptable limits for the two variables, but we need to check again with more variables. , Autocorrelation: There should be no autocorrelation of the error terms. It can happen if we miss some information which is being recognized by the model. We can test this by doing the “Durbin Watson Test”. It results in a score of 0.3. That means there is positive autocorrelation. We can fix this by adding a lagged variable (Macaluso, 2018). By adding a lagged variable (7-day lagged Confirmed new infections), we have increased the score to 1.3 which comes close to the area of no autocorrelation., Homoscedasticity: A rather complicated term to describe that there should be a consistent variance of our error terms. We already know that this will be a challenge with our data since it is time series (Carvalho, 2020). We know that this data is not linear shaped and underestimating most cases., In the end we receive a very handy Stats-Report which shows us a lot of information regarding our model. We have now three variables and a very high R2 score but strong signs of Multicollinearity. That makes it very challenging to deduct any conclusions from the model., Also, we find that only one of the variables is below the p value threshold of 0.05. We could get rid of the other variable since the high p value indicates that changes in the predictor variable are not related to the response variable (Minitab, LLC, 2013)., Conclusion:, This article showed if Linear Regression can forecast New COVID-19 infections. We investigated a simple Linear Regression as well as the conditions which need to be satisfied for Linear Regression. In the end it can be concluded that a prediction is possible but there are strong doubts regarding the interpretability of the model., It would be dangerous to draw conclusions from this model if it would be used to predict future data., Nevertheless, the model showed great performance while using multiple variables as input. An extension would be model fine tuning in the form of variable elimination, non-linear transformations, value centralization and robustness tests such as Cross-Validation., A test with more variables showed a good performance with reservation to all the points mentioned above:, At this point you should have understood the model and its implications. We will use the next article how Machine Learning can help to tweak the parameters of the model in order to find the best fit – called Gradient Descent., Until then: Stay healthy!, You find the full Github repo here and the Jupyter Notebook here. Also check out the Dashboard which is being updated regularly, Callaway, E. (2020, September 8). The coronavirus is mutating — does it matter? Retrieved November 14, 2020, from https://www.nature.com/articles/d41586-020-02544-6, Carvalho, T. (2020, May 25). OLS Linear Regression Basics with Python’s Scikit-learn. Retrieved November 15, 2020, from https://medium.com/python-in-plain-english/ols-linear-regression-basics-with-pythons-scikit-learn-4ecfe88145b, Cruickshank, S. (2020, October 16). Coronavirus reinfection cases: what we know so far – and the vital missing clues. Retrieved November 14, 2020, from https://theconversation.com/coronavirus-reinfection-cases-what-we-know-so-far-and-the-vital-missing-clues-147960, Department of Statistics, Yale University. (1998). Linear Regression. Retrieved November 14, 2020, from http://www.stat.yale.edu/Courses/1997-98/101/linreg.htm, Frost, J. (2017, April 2). Multicollinearity in Regression Analysis: Problems, Detection, and Solutions. Retrieved November 21, 2020, from https://statisticsbyjim.com/regression/multicollinearity-in-regression-analysis/, Galton, F. (1989). Regression analysis. Statistical Science., pp. 81-86. doi:10.1214/ss/1177012581, Hariharan, S. (2020, January 12). Linear Regression: A complete story. Retrieved November 20, 2020, from https://medium.com/analytics-vidhya/linear-regression-a-complete-story-c5edd37296c8, Macaluso, J. (2018, May 27). Testing Linear Regression Assumptions in Python . Retrieved November 24, 2020, from https://jeffmacaluso.github.io/post/LinearRegressionAssumptions/, Minitab, LLC. (2013, July 1). How to Interpret Regression Analysis Results: P-values and Coefficients. (T. M. Blog, Editor) Retrieved November 26, 2020, from https://blog.minitab.com/blog/adventures-in-statistics-2/how-to-interpret-regression-analysis-results-p-values-and-coefficients, Worldometers.info. (2020, November 14). COVID-19 Coronavirus Pandemic. Retrieved November 27, 2020, from https://www.worldometers.info/coronavirus/, *Der Beitrag spiegelt die Meinung des Autors wider und ist keine allgemeingültige Meinung des TDWI. Als TDWI bieten wir die Plattform, alle Themen und Sichtweisen zu diskutieren.*, Allgemeine Fragen zum Blog, zum Verein und zum Thema AutorIn werden:Sandra SteingrubeCommunity Managersandra.steingrube(at)tdwi.eu, Fragen zum Inhalt:Leif HitzschkeTDWI e.V. Vorstandsmitgliedleif.hitzschke(at)tdwi.eu, TDWI e.V.Lindlaustraße 2cD-53842 TroisdorfTel: +49 (0)2241/2341-212, ImpressumDatenschutz;https://blog.tdwi.eu/can-machine-learning-help-to-forecast-covid-19-infections-part-2/;TDWI;Armin Geisler
23. Dez 20;"Der TDWI wünscht Ihnen frohe Weihnachten &amp; einen guten Rutsch";"Der TDWI wünscht Ihnen und Ihren Familien ein frohes Weihnachtsfest und schöne Feiertage. Kommen Sie gesund und wohlbehalten in ein glückliches und erfolgreiches Jahr 2021., Obwohl in diesem Jahr auch für uns nicht alles so lief wie geplant, haben wir viele schöne Erfahrungen gemacht. Insbesondere die digitalen Roundtable und BarCamps haben gezeigt, wie pulsierend und inspirierend unsere TDWI Community ist. Im neuen Jahr werden wir Ihnen weiterhin Wissensdrehscheibe und Netzwerkplattform sein. Insbesondere freuen wir uns auf neue Formate wie die TDWI Themenzirkel oder den TDWI StackTalk., Erst einmal treffen wir uns weiterhin digital. Wir wünschen uns aber ganz besonders die Möglichkeit, auch wieder persönlich in Kontakt zu treten, Gespräche zu führen und gemeinsame Erlebnisse zu haben., Herzlichen Dank für Ihr Vertrauen und die gute Zusammenarbeit und Unterstützung im Jahr 2020. Wir freuen uns auf das kommende Jahr – bleiben Sie mit uns in Kontakt und gestalten Sie die Community!, Zum Schluss möchten wir noch einige Mitglieder der TDWI-Community zu Wort kommen lassen. Wir haben gefragt, wie 2020 für sie war und auf was sie sich in 2021 freuen:, Mein berufliches Highlight im 2020 war die Aufnahme zweier Podcasts zu Agile BI und IBCS Themen. Nicht zu vergessen die digitalen Barcamps mit den TDWI Young Guns, welche immer wieder eine willkommene Abwechslung waren im Home-Office-Alltag., Das Jahr 2020 hat uns allen viel abverlangt. Viele von haben innerhalb kürzester Zeit ihre heimischen Wohnzimmer in vollfunktionsfähige Arbeitsplätze verwandelt. Meetings und Abstimmungen finden fast nur noch virtuell statt. Und auch das Netzwerken auf Konferenzen und Tagungen hat sich in den digitalen Raum verschoben.So sehr man das alte Normal hierbei vermisst, so ist es doch beeindruckend, was mit der heutigen Technologie alles möglich ist. Durch die zunehmende Digitalisierung, gewinnt auch das Thema Data &amp; Analytics weiter an Wichtigkeit. Gerade deswegen freue ich mich, dass die TDWI Community trotz der neuen Situation so aktiv ist und mit Formaten wie virtuellen BarCamps und Konferenzen, Webinaren sowie Audio- und Video-Podcasts schnell auf die neuen Gegebenheiten reagieren konnten.Unterm Strich hat das Jahr 2020 viele innovative und kreative Ansätze hervorgebracht hat (wenn auch nicht immer freiwillig). Nichtsdestotrotz freue ich mich auf ein neues Jahr mit hoffentlich mehr Normalität., 2020 war ein herausforderndes aber auch sehr spannendes Jahr in beruflicher und privater Hinsicht. Eins meiner absoluten Highlights 2020 waren für mich die BarCamps mit den TDWI Young Guns. Die Durchführung der BarCamps auf digitaler Ebene hat uns nochmal einen guten Input interessante Einsichten gegeben. Ein weiteres Highlight war die hohe Nachfrage nach Analysen, BI und Data Science Lösungen. Ich finde 2020 hat den Themen nochmal einen großen Push gegeben und ich freue mich schon auf 2021 wegen meiner neuen beruflichen Position und den interessanten Themen mit den Young-Guns (Hackathon, Blog, Video-Podcast, usw.)., Mein Highlight 2020 lag im Bereich der Visualisierungen. Ich lernte die Standards von IBCS kennen, hörte mich während des Lockdowns durch die Podcasts von „Storytelling with Data“ und absolvierte einen Online-Kurs auf Coursera. Ich freue mich, im 2021 noch aussagekräftigere und besser kommunizierende Darstellungen rauszuhauen!, Allgemeine Fragen zum Blog, zum Verein und zum Thema AutorIn werden:Sandra SteingrubeCommunity Managersandra.steingrube(at)tdwi.eu, Fragen zum Inhalt:Leif HitzschkeTDWI e.V. Vorstandsmitgliedleif.hitzschke(at)tdwi.eu, TDWI e.V.Lindlaustraße 2cD-53842 TroisdorfTel: +49 (0)2241/2341-212, ImpressumDatenschutz";https://blog.tdwi.eu/der-tdwi-wuenscht-ihnen-frohe-weihnachten-und-einen-guten-rutsch/;TDWI;Sandra Steingrube
09. Dez 20;Ein Dankeschön an alle TDWI Roundtable Organisatoren;Ein aufregendes Jahr liegt hinter uns. Danke für Ihre und Eure Teilnahme und das Interesse an den TDWI Roundtable. Danke auch an alle lokalen Organisatoren, die im Jahr 2020 Roundtable organisiert und so erfolgreich durchgeführt haben., Nachdem Mitte März aufgrund der Covid19-Pandemie einige Termine abgesagt oder verschoben werden mussten, haben viele unserer Vorsitzenden ihre Roundtable erfolgreich ins Netz verlegt. Den virtuellen Start hat der Roundtable Basel schon am 24. April mit 137 Anmeldungen und 82 Teilnehmern gemacht.Bis Ende 2020 haben sechs Präsenzveranstaltungen und 15 Online-Roundtable erfolgreich stattgefunden.Während dieser Zeit haben wir den TDWI Roundtable Bern neu gegründet. Wir danken Flavio Curti und Ueli Gantenbein für ihr Engagement und heißen sie herzlich willkommen in der Runde der TDWI Roundtable.Ein weiteres tolles Ereignis hatten wir in Basel: am 25.08. ‚feierte‘ der TDWI Roundtable Basel sein 25. Jubiläum! Vielen Dank, Gianleandro Sarro und Jan Altin! Sobald wieder möglich, wird die Feier nachgeholt!Für den 21. Oktober war in Zürich sogar wieder eine Präsenzveranstaltung geplant. Unter gut ausgearbeiteten Hygienevorschriften der HWZ Zürich waren alle Plätze schnell vergeben. Doch leider machte Covid19 uns auch hier das Leben schwer: Kurz vor dem Termin stieg die Zahl der Krankheitsfälle in der Schweiz – eine Welle von Absagen von Teilnehmern war die Folge. Am Ende hat der Termin sicher mit 11 Gästen stattgefunden. Danke, Herbert Stauffer, für diesen Versuch und Dein Engagement!Wir gehen ‚auf Nummer sicher‘ und starten ins Jahr 2021 wieder mit Online-Veranstaltungen. Wir haben gelernt, dass die digitalen Roundtable den Vorteil haben, dass man sie von überall besuchen kann. Insofern möchten wir das Format gerne auch nach der Pandemie beibehalten. Wie genau das aussehen wird, steht noch nicht fest. Trotzdem freuen sich die Gastgeber der Roundtable und wir uns, Sie und Euch wieder persönlich zu treffen., Wann wir wieder auf Präsenztermine switchen können, ist derzeit noch nicht absehbar. Wir bleiben dran und informieren an dieser Stelle., Bereits feststehende Online-Termine:, 21.01. Ruhrgebiet: „Komplexe Datenbankprojekte aus Managementsicht“ – fehlendes gegenseitiges Verständnis als Projektrisiko28.01. Rhein-Neckar: Vom DW zur Data-Platform – Flexibilisierung durch hybride Infrastruktur11.02. Frankfurt: Qualitätssicherung und Datentransparenz, Geplante Online-Termine:, 14.01. Berlin26.01. Bern, Wir danken den Roundtable-Vorsitzenden für ihren unermüdlichen Einsatz und wünschen allen eine wunderbare Weihnachtszeit und ein gesundes neues Jahr 2021. Passt auf euch auf!Euer Team des TDWI e.V., Allgemeine Fragen zum Blog, zum Verein und zum Thema AutorIn werden:Sandra SteingrubeCommunity Managersandra.steingrube(at)tdwi.eu, Fragen zum Inhalt:Leif HitzschkeTDWI e.V. Vorstandsmitgliedleif.hitzschke(at)tdwi.eu, TDWI e.V.Lindlaustraße 2cD-53842 TroisdorfTel: +49 (0)2241/2341-212, ImpressumDatenschutz;https://blog.tdwi.eu/ein-dankeschoen-an-alle-tdwi-roundtable-organisatoren/;TDWI;Tanja Kenda
11. Nov 20;BICC – ist das Kunst oder kann das weg?;Gefühlt ist es erst ein Wimpernschlag her, da waren Business Intelligence Competence Center (BICC) der neuste Schrei und jede Organisation, die etwas auf sich hielt, musste diese Organisationsform einführen. Doch erweiterte Anforderungen sowie neue Technologien und Kompetenzen im BI- und Analytics-Umfeld, zeigen heute den Bedarf eines Re-Designs dieser Organisationsmodelle auf. Dabei spielt insbesondere der Einsatz von künstlicher Intelligence (KI) im Zusammenspiel mit Data Science eine gewichtige Rolle. Zudem unterstreichen agile Projektmethodiken die Notwendigkeit dieser Vorgehensweise auch hinsichtlich Unternehmensstrukturen Rechnung zu tragen. Vor dem Hintergrund dieser Entwicklung stellt sich die Frage, ob das BICC in seiner bisherigen Form noch zeitgemäß ist., Vor etwa zehn Jahren kamen erste Diskussionen und in der Folge Initiativen auf, das Thema BI auch in neue Organisationsformen zu überführen. Viele Anwendungsunternehmen hatten über vorangegangene Projekte eine umfassende Reporting-Architektur aufgebaut. So wurden auch erste Erfahrungen gesammelt, welche Maßnahmen sich erfolgreich umsetzen ließen und welche weniger. Um die Schnittstelle zwischen IT und Fachbereich dauerhaft zu schließen und die Synergien aus dem Konglomerat von Fach- und IT-Kompetenz zu nutzen, wurden neue Competence Center ins Leben gerufen. BI war fortan nicht mehr nur ein Oberbegriff für Reporting.Bei der Umsetzung dieser neuen Abteilungen gab es unterschiedliche Ausprägungsformen. Dabei wurde zwischen komplett virtuellen, hybriden bis zu physischen Competence Centern unterschieden, die BI- und Prozessexperten aus IT und Fachbereich umfassten. Zudem gab es auch in der Berichtslinie verschiedene Herangehensweisen, ob das BICC als eigenständige Abteilung im Finanzbereich, in der hauseigenen IT oder als Stabstelle aufgehängte wurde.Gerade in der Anfangszeit dieser neuen Organisationsmodelle und allen voran bei virtuellen Zusammenschlüssen zeichneten sich immer wieder Probleme im laufenden Betrieb der BI Competence Center ab. Waren insbesondere Delegierte der Fachfunktionen nur virtuell oder in Teilzeit dem BICC abgestellt, kam es nicht selten zu Konflikten im Zeitmanagement, da der Arbeit im BICC zumindest zu Beginn nur eine untergeordnete Priorität zugestanden wurde.Mit der Zeit konnten aber viele Unternehmen das für sie passende BICC Konstrukt finden und manifestieren. Und die zahlreichen Vorteile, die mit der Einführung eines BI Competence Center einhergehen, sind auch Vorbild für viele andere Geschäftsprozesse gewesen. So wurden in Anlehnung an diese Konglomerate auch Competence Center für Customer Relationship Management (CRM) oder beispielsweise Data Management bzw. Data Governance etabliert., Nicht nur BI-Projekte wurden lange Zeit nach klassischer Wasserfallmethodik geplant und durchgeführt. Die Vorteile für die Anwenderunternehmen waren dabei die vorhersehbaren Projektkosten sowie die Projektdauer. Mit der Zeit haben sich aber mehr und mehr agile Vorgehensweisen durchgesetzt.Teilweise setzen technische wie fachliche Bedingungen eine agile Projektierung voraus. Data Science Projekte skizzieren oftmals nur einen rudimentären betriebswirtschaftlichen Rahmen und beanspruchen Raum für Erkenntnisse, die sich erst im Laufe des Projektes herausschälen. Außerdem gibt es mittlerweile eine Fülle an neuen Werkzeugen im BI- und Analytics-Markt, deren Anwendung durch eine agile Herangehensweise erprobt werden müssen. Spezielle Anforderungen an Analytics und Visualisierungen bedingen zum Teil dedizierte Tools.Agile Projektteams sind ein stückweit vergleichbar mit den zuvor beschriebenen BI Competence Centern. Für einen zuvor abgesteckten Zeitraum verfolgen Vertreter aus IT und Fachbereich das gleiche Projektziel und arbeiten in einem temporären Team eng zusammen. Auch hier bringt jedes Projektmitglied seine jeweiligen Kompetenzen und Fertigkeiten aus seiner Spezialdisziplin ein und gewährleistet so ein hochgradig leistungsstarkes Team. Somit hat der Ansatz nach Scrum oder anderen agilen Projektmethodiken mehr und mehr Einfluss auf die Aufbauorganisation., Um insbesondere eine enge Verknüpfung zwischen den fachlichen Anforderungen und der Machbarkeit einer technischen Umsetzung herzustellen, können allen voran in frühen Projektphasen agile Projektvorgehensweisen hilfreich sein.Agile Vorgehensmodelle bieten sich in mehrfacher Hinsicht an, um beispielsweis technisches und konzeptionelles Neuland zu beschreiten. Das Arbeiten in kleinen exklusiven Teams, die in Sprints von nur wenigen Wochen ununterbrochen Output erzeugen, lässt schnell Rückmeldungen zu, ob die avisierten Ziele zu erreichen sind. Dieser Output lässt dauerhaft die Möglichkeit zu, Nachjustierungen in der technischen Ausrichtung vorzunehmen oder – wenn nötig – Diskussionen hinsichtlich fachlicher Kompromisse anzustrengen. Somit trägt dieser methodische Ansatz dem Wunsch nach einem höheren Tempo in der Umsetzung und Ergebniserzeugung Rechnung.Erste Erfahrungen aus der Praxis zeigen, dass Lösungen aus agilen Projekten sehr viel häufiger als gemeinsamer Projekterfolg aus IT und Fachbereich verstanden werden und somit eine ungleich höhere Akzeptanz bei allen Projektbeteiligten schafft., In der Vergangenheit verfolgten viele Anwenderunternehmen den Aufbau eines zentralen Data Warehouses, um dem „Single-Point-of-Truth“ Anspruch gerecht zu werden. Mittlerweile haben sich solche Initiativen überholt. Vielmehr ist ein vermehrter Trend hin zur Dezentralisierung festzustellen.Sinnvollerweise steht zu Beginn von Big Data Initiativen der konkrete Anwendungsfall im Mittelpunkt. Data Scientists werden in diesem Zusammenhang direkt dem entsprechenden Fachbereich zugeordnet. In der Umsetzung kommen dabei von Fall zu Fall unterschiedliche Modellierungs- und Analytics-Werkzeuge zum Einsatz. Immer seltener besteht der zwingende Bedarf temporär oder dauerhaft generierte Datenbestände zentral bereitzustellen. Dezentral ist das neue zentral. Es müssen lediglich Schnittstellen für den fachbereichsübergreifenden Datenaustausch im Bedarfsfall berücksichtigt werden.Der „Single-Point-of-Truth” wird sozusagen durch das „Single-Net-of-Truth“ ersetzt. Die Daten verbleiben dort, wo sie benötigt bzw. generiert werden. Eine Data Governance schafft den Rahmen für ein unternehmensweit durchlässiges Netz an Informationen., Und dieser dezentrale Ansatz einer neuer Datenarchitektur kann sich schlussendlich auch in der Aufbauorganisation der Zukunft widerspiegeln. Der Netzwerkgedanke löst den zentralistischen Ansatz mehr und mehr ab.Wie zuvor beschrieben werden Data Science Kompetenzen in der Regel direkt in der Prozessorganisation angesiedelt. Da diese Funktionen auch fachliche und technische Unterstützung von ordinären BI-Disziplinen benötigen, sind Ansiedlung weiterer BI-Rollen im Fachbereich die logische Folge. Somit finden sich in den Linienorganisationen komplette Teams wieder, die von der fachlichen bis zur technischen Expertise den kompletten BI- und Analytics-Stack abdecken – ähnlich, wie es in agilen Projektteams für einen temporären Zeitraum der Fall ist.Somit tritt an die Stelle des vormals zentralistischen BICC zukünftig ein unternehmensweites Netz an BI-Experten, die sich über entsprechende Governance-Strukturen vor allem virtuell organisieren und abstimmen. Dabei gibt es Funktionen, wie Infrastruktur und Cloud-Architektur, die zunehmend an Bedeutung gewinnen und auch weiterhin in zentralen Competence Centern bzw. der IT organisiert werden sollten.Bei einer vermehrten Anzahl von BI- und Analytics-Werkzeugen sowie unterschiedlichen Datentöpfen, ist eine funktionierende Data Governance wichtiger denn. Hier können zentrale Instanzen Richtlinien vor-geben, die für alle Funktionen und Beteiligten im unternehmensweiten Netz maßgeblich sind., Die Projekte werden agil und mit cross-funktionalen Teams besetzt. Fach- und IT-Experten arbeiten für eine abgesteckte Projektdauer eng zusammen. Die Praxis zeigt, dass diese Vorgehensweise immer häufiger von Erfolg gekrönt ist. Warum sollten also Anwenderunternehmen nicht folgerichtig den nächsten Schritt gehen und zentrale Strukturen aufbrechen und an deren Stelle ein Netzwerk aus BI- und Analytics-Experten aufsetzen?Business Intelligence wäre nach der Einführung der BI Competence Centern einmal mehr der Taktgeber hinsichtlich neuer Organisationsmodelle., *Der Beitrag spiegelt die Meinung des Autors wider und ist keine allgemeingültige Meinung des TDWI. Als TDWI bieten wir die Plattform, alle Themen und Sichtweisen zu diskutieren.*, Allgemeine Fragen zum Blog, zum Verein und zum Thema AutorIn werden:Sandra SteingrubeCommunity Managersandra.steingrube(at)tdwi.eu, Fragen zum Inhalt:Leif HitzschkeTDWI e.V. Vorstandsmitgliedleif.hitzschke(at)tdwi.eu, TDWI e.V.Lindlaustraße 2cD-53842 TroisdorfTel: +49 (0)2241/2341-212, ImpressumDatenschutz;https://blog.tdwi.eu/bicc-ist-das-kunst-oder-kann-das-weg/;TDWI;Daniel Eiduzzis
04. Nov 20;TDWI Young Guns Mailing Oktober 2020 | Ein Blick hinter die Kulissen;"Unser heimliches Lieblingsformat für den Austausch innerhalb der Community. Im September haben wir das zweite digitale Data &amp; Analytics BarCamp durchgeführt. Das Feedback: Durchweg positiv! Das freut uns so sehr, da wir so vor allen Dingen auch den überregionalen Austausch stärken können. Das ist ja auch unglaublich wichtig, wenn man sich sonst eher so in seiner eigenen kleinen Welt und Informationsblase bewegt. ImBlogartikelkönnt ihr alles ganz genau nachlesen. Und die Planung für Vol. 3 ist schon voll im Gange., Melde dich gerne unter young-guns(at)tdwi.eu! Wir freuen uns schon auf dich ??, Am 16.10. fand unser jährliches Strategiemeeting statt. Hier schaffen wir uns gemeinsam Zeit, um Pläne für unsere Aktivitäten im nächsten Jahr zu schmieden. 2021 steht unter dem Motto „noch mehr Eigenverantwortlichkeit schaffen“. Besonders freuen wir uns darüber, dass auch unser Strategiemeeting von Mal zu Mal wächst. Wo sich letztes Jahr 6 engagierte Menschen getroffen haben, sind wir diesmal sogar mit 11 Personen in die konkrete Planung gegangen: Wir möchten Chapter aufbauen und neue Formate etablieren. Wir möchten weiter Mehrwert für dich schaffen und das Tor zur einfachen Wissensvermittlung und TDWI Community bilden., Alle ToDos und Pläne und was wir uns weiter vorstellen präsentieren und kommunizieren wir auf unserer Hauptplattform SLACK –Komm gerne dazu, schau’s dir an und werde Teil der Community!, Was können wir Young Guns eigentlich für dich sein? Wir verstehen uns als Rahmen und Präsentationsplattform. Unsere unterschiedlichen Formate bieten dir die Möglichkeit dich und dein Thema vor unserer Community und auch im gesamten Verein zu präsentieren. Da wir herstellerunabhängig und inhaltsgetrieben arbeiten, finden dich so alle, die nach Inhalten und Mehrwert suchen., TDWI Virtual:, Sei bei der TDWI Virtual dabei! Folgende Themen werden an den drei Terminen vorgestellt:, 06. Oktober 2020 (die Vorträge können alle im Check-In auf der Webseite nachträglich gehört werden)Framing the data-driven company: Data Strategy // Data Governance, 27. Oktober 2020 (die Vorträge können alle im Check-In auf der Webseite nachträglich gehört werden)Building the data-driven company: Architecture // Data Management, 10. November 2020Working in the data-driven company: Advanced Analytics and AI // Data Science, Melde dich kostenfrei an!, Wir versuchen immer weiter zu wachsen. Das geht natürlich nicht ohne Strukturen zu schaffen, die sich selber tragen. Wir möchten keine starre Regierung vorgeben, der man einfach hinterherläuft. Wir möchten so viel Flexibilität bieten, wie eben nur möglich ist. Dafür haben wir maldie unterschiedlichen Stufen von Engagementzusammengefasst, damit jeder auch für sich entscheiden kann, wieviel kann und möchte ich mich einbringen., Habt ihr Fragen zu uns? Besucht uns beiLinkedInoderSlack., Allgemeine Fragen zum Blog, zum Verein und zum Thema AutorIn werden:Sandra SteingrubeCommunity Managersandra.steingrube(at)tdwi.eu, Fragen zum Inhalt:Leif HitzschkeTDWI e.V. Vorstandsmitgliedleif.hitzschke(at)tdwi.eu, TDWI e.V.Lindlaustraße 2cD-53842 TroisdorfTel: +49 (0)2241/2341-212, ImpressumDatenschutz";https://blog.tdwi.eu/tdwi-young-guns-mailing-oktober-2020-ein-blick-hinter-die-kulissen/;TDWI;Alex Krolla
23. Okt 20;Framing, building and working in the data-driven company;Heute möchten wir ein wenig Werbung in eigener Sache machen – denn: Der Tag 2 der TDWI Virtual steht vor der Tür! Wir beschäftigen uns mit Themen rund um Architektur und Datenmanagement., Dies sind die Termine im Überlick:, 06. Oktober 2020Framing the data-driven company:Data Strategy // Data Governance, 27. Oktober 2020Building the data-driven company:Architecture // Data Management, 10. November 2020Working in the data-driven company:Advanced Analytics and AI // Data Science, Haben Sie Tag 1 verpasst? Kein Problem. Sobald Sie sich registrieren, können Sie im Check-In auf der Webseite alle Vorträge noch einmal anschauen., Seien Sie dabei und nutzen Sie die TDWI Virtual als Ihren Navigator zur data-driven company!Erhalten Sie Inspiration und Lösungsansätze für Ihre Projekte und das bequem von Ihrem Zuhause aus., Wir bieten Ihnen als Praktiker in Data, Business Intelligence und Analytics einen Wissenstransfer sowie eine persönliche und berufliche Fort- und Weiterbildung. Des Weiteren geben wir Partnern und Sponsoren die Plattform, mit der Community in Kontakt zu treten, Jetzt kostenfrei zur TDWI Virtual II anmelden, Allgemeine Fragen zum Blog, zum Verein und zum Thema AutorIn werden:Sandra SteingrubeCommunity Managersandra.steingrube(at)tdwi.eu, Fragen zum Inhalt:Leif HitzschkeTDWI e.V. Vorstandsmitgliedleif.hitzschke(at)tdwi.eu, TDWI e.V.Lindlaustraße 2cD-53842 TroisdorfTel: +49 (0)2241/2341-212, ImpressumDatenschutz;https://blog.tdwi.eu/framing-building-and-working-in-the-data-driven-company/;TDWI;Sandra Steingrube
07. Okt 20;"Wie man Data &amp; Analytics Projekte komplett an die Wand fährt – und wie man es verhindert";"Kennen Sie das? Es kommt ein neues Projekt auf den Tisch – es geht um Reporting, Big Data Analytics, Machine Learning, eine Data Platform für das Unternehmen? Das Controlling möchte die Finanz-Daten des ERP-System analysieren, das Sales &amp; Marketing Team muss die CRM-Daten auswerten, in der Produktion gibt es IoT-Geräte als erste Prototypen. Es wird immer viel in „moderne“ Technologien investiert, doch wie erfolgreich sind diese Invests?, Exkurs: Data &amp; Analytics Projekte scheiternIn der Öffentlichkeit sind kaum Informationen zu gescheiterten Data &amp; Analytics Projekte bekannt – warum eigentlich? Es gibt zwar jede Menge Artikel wie und warum Business Intelligence oder Big Data Projekte scheitern – aber konkrete Anwendungsfälle und Lessons Learned sind kaum zu finden. Im Gegensatz dazu sind Berichte über gescheiterte Einführungen von ERP (Enterprise Resource Planning) in Konzernen vorhanden – wie beispielsweise bei LIDL, Haribo, Otto, Deutsche Bank, Deutsche Post (Link). Ohne konkrete öffentliche Berichte – nehmen wir einfach an: Data &amp; Analytics Projekte scheitern., Data &amp; Analytics Projekte unterscheiden sich von ERP-ProjektenBleiben wir am Beispiel und konkretisieren: Data &amp; Analytics Projekte unterscheiden sich von ERP-Projekten: eine ERP-Einführung dient dazu die primäre Prozesskette von rückwärts effektiv und effizient abzuwickeln (Kunde bestellt, Versand liefert, Produktion produziert, Einkauf beschafft) und sekundäre Prozesse in den Bereichen HR, Finanzen, Controlling oder auch IT zu unterstützen. Data &amp; Analytics Projekte basierend auf einer ERP-Einführung dienen meist der Analyse und dem Berichten dieser Daten. Ein weiterer, wichtiger Unterschied ist, dass ein Data &amp; Analytics Projekt meist eher ein „hartes“ Enddatum hat (ja, es gibt Ausnahmen!), wohin gegen alle Projektbeteiligten einer ERP-Einführung auf ein fixes Enddatum für die Einführung hinarbeiten. Wird dieser Termin nicht gehalten, ist ein Aufschrei vorprogrammiert., Welche Möglichkeiten gibt es, dass Data &amp; Analytics Projekte nicht scheitern?Diese Unterschiede sollten Verantwortliche von und Mitarbeiter in Data &amp; Analytics Projekten kennen. Denn sie ermöglichen unterschiedliche Planungs- und Implementierungs-Konzepte. Doch wie gelingt es, dass man relativ schnell, einfach und kostengünstig zu einem guten Ergebnis kommt? Also ein qualitativ hochwertiges Ergebnis mit absoluter Sicherheit abliefert?, „Hochwertige Ergebnisse“ sind immer sehr positive Nutzer-Erfahrungen mit IT-Systemen gestützten, datengetriebenen Prozessen (kurz: Nutzung analytischer Applikation). Um eine hochwertige Applikation schließlich auszuliefern bedarf es einer Design Phase – in der der Analyst versteht welchen Bedarf der Kunde hat, das konzeptionelle Design entwirft und im Anschluss das finale Produkt erstellt. Das ist das Vorgehen des „Design Thinking Ansatzes“ – und ich liebe ihn., Design Thinking bedeutet Entwicklung im Kopf des KundenWarum? Design Thinking bedeutet die Entwicklung von Software für einen Kunden – aus Kundensicht – das klingt einfach. Im Alltag dominieren dennoch weiterhin Interviews mit Lasten- und Pflichtenheft.  Für Data &amp; Analytics Projekte in der Design Phase ist dieses Vorgehen kein Best Practice (mehr) – diese Methodik ist alt und wurde durch neue Techniken überholt., Design Thinking gibt einen Rahmen vor, um herauszufinden, wer der Nutzer ist, was ihn antreibt und in der täglichen Arbeit frustriert. Diese Nutzer werden Personengruppen (Persona) mit gemeinsamen Eigenschaften zusammengeführt und festgehalten, wie deren Sicht auf die analytische Applikation ist und was sie wirklich benötigen um Ihre Performance zu verbessern. Ein praktisches Beispiel und eine meiner größten persönlichen Erlebnisse – ein Vertriebsleiter wollte die Kundenumsätze basierend auf den Geschäftsjahren der jeweiligen Kunden sehen und analysieren. Diese Information brachte ihn in Verhandlungen in eine bessere Position. Folglich mussten die Stammdaten erweitert werden. Ohne das Interview und das Verständnis für seine tägliche Arbeit wäre diese Funktion niemals entwickelt worden., Nachdem der Bedarf des Nutzer verstanden wurde, beginnt die Phase des konzeptuellen Entwurfs mit Storyboards, Wireframes, visuellem Design und die Darstellung des Anwendungsfalls. Direkte Rücksprachen mit offenen Fragen an den Nutzer sind erlaubt und explizit gewünscht – er soll verstehen was er bekommt. Meistens justieren die Nutzer ihren wahren Bedarf sogar noch einmal., Im Anschluss werden die Ergebnisse der konzeptionellen Phase genutzt um das finale produktive Design zu entwerfen. Dazu zählen die Interaktionen mit der analytischen Applikation über das Endgerät, visuelle Spezifikationen und die formelle Erfassung als User Story für die Entwickler., Und richtig gelesen – bisher wurde noch keine Zeile Code geschrieben, noch kein Data &amp; Analytics Tool in die Hand genommen oder der Data Lake, das Lake Haus oder das Data Warehouse umfassend realisiert. Denn erst wenn dem Kunden klar ist was er bekommt und der Analyst die Anforderungen so dokumentiert, dass es die Entwickler aus Kundensicht verstehen, erst dann fängt die Entwicklung an., Implementierung? Geben Sie die organisatorischen und technischen RahmenbedingungenvorLiegen User Stories zur Umsetzung vor können diese auch umgesetzt werden. Voraussetzung dafür ist die Schaffung organisatorischer und technischer Leitlinien. Am besten ist es bereits im Anhang der User Story die architektonische Lösung und technischen Komponenten zu beschreiben – denn nichts ist schlimmer als streitende Entwickler über die „beste“ Lösung. Wichtig ist in diesem Zusammenhang, dass die architektonische Lösung als Standard innerhalb der Organisation definiert wurde. Denn sonst wird die Lösung früher oder später entweder zum „Schatten-IT-Projekt“ oder als „Individual-Lösung“erklärt, die irgendwann eine hohe technische Schuld fordert., Parallel zur Standard-Architektur und den technischen Komponenten sollte auch definiert sein, wer die entsprechenden Arbeitspakte mit den jeweiligen Werkzeugen umsetzt. Wer im Team welche Rolle besetzt, diese auch ausführt und welche Kapazitäten einbringt sind die absoluten Basics in jedem Projekt – egal ob modern oder klassisch – und dennoch kommt es häufiger vor, dass dies nicht der Fall ist., Ist die User Story laut Team umgesetzt, erfolgt anschließend der User Acceptance Test, in dem der Kunde das Produkt/ den Service akzeptiert oder ablehnt. Dieser Test erfolgt gemeinsam zwischen dem Business Analysten des Teams sowie dem Kunden und evtl. Dem Key User des Bereichs. Erst wenn der Kunde bestätigt, dass das Produkt live genutzt werden kann, ist die User Story fertig. Lehnt er ab, geht die User Story zurück ins Team. Es zählt der Grundgedanke: „wer bestellt, der bezahlt und wer bezahlt, hat Recht“., Wie treiben Sie BI und Data Analytics-Entwicklungen voran?Wenn Sie Fragen haben oder Rückmeldung geben wollen – ich bin immer an einem Austausch interessiert. Bitte wählen Sie die Kommentarfunktion unten, um eine öffentliche Diskussion zu führen. Für private Anschreiben kontaktieren Sie mich bitte über mein Xing– oder LinkedIn-Profil., *Der Beitrag spiegelt die Meinung des Autors wider und ist keine allgemeingültige Meinung des TDWI. Als TDWI bieten wir die Plattform, alle Themen und Sichtweisen zu diskutieren.*, Allgemeine Fragen zum Blog, zum Verein und zum Thema AutorIn werden:Sandra SteingrubeCommunity Managersandra.steingrube(at)tdwi.eu, Fragen zum Inhalt:Leif HitzschkeTDWI e.V. Vorstandsmitgliedleif.hitzschke(at)tdwi.eu, TDWI e.V.Lindlaustraße 2cD-53842 TroisdorfTel: +49 (0)2241/2341-212, ImpressumDatenschutz";https://blog.tdwi.eu/wie-man-data-analytics-projekte-komplett-an-die-wand-fahrt-und-wie-man-es-verhindert/;TDWI;Tobias Riedner
23. Sep 20;DIGITAL BARCAMP powered by TDWI Young Guns – Vol. 2;"So schön war unser digitales Barcamp nämlich wieder., Besonders der internationale Austausch, der so zustande kommt, ist – unserer Meinung nach – Gold wert. Wann kann man sich mal in lockerer Runde, ohne Rückreise oder Terminstress mit den Schweizer Kollegen unterhalten? Dass dieser Aspekt so wertvoll ist, hätten wir uns vor dem ersten digitalen Barcamp nicht träumen lassen – aber manchmal kommt es ja einfach so, wie es kommen soll., Was sagt die Community?, „Ich habe wieder spannende Insights bekommen und freue mich jedes Mal über die guten Diskussionen. Außerdem ist es schön, alle mal wieder zu sehen“, sagt Armin, der mittlerweile ein fester Bestandteil der Community ist. Aber auch neue Teilnehmer konnten wir begeistern. Bianca beschreibt ihr erstes Barcamp Erlebnis folgendermaßen: „Ich war wirklich überrascht wie schnell die Zeit vergeht. Es war super interessant, sodass ich mir gut vorstellen könnte auch bei einem längeren Format dabei zu sein.“, Und Jakob, der das erste Mal als Sessionleiter teilgenommen hat sagt: „Mir hat der Austausch richtig gut gefallen. Es war super hilfreich die unterschiedlichen Meinungen zu meinem Thema zu hören.“ Alle sind sich in der Feedbackrunde einig: Es hat viel Spaß gemacht J An dieser Stelle ein großes DANKE an die Sessionleiter und alle Teilnehmer! Ohne euch – kein Barcamp!, Nach dem Barcamp ist vor dem Barcamp, Wir sind auf jeden Fall schon wieder in der Planung für Vol. 3 und freuen uns auf die vielen altbekannten und neuen Gesichter, die wir bei jeder Ausgabe des Data &amp; Analytcs Barcamps – egal ob analog oder digital – begrüßen dürfen., Wenn ihr wissen wollt wie unsere Planung oder die genaue Durchführung abläuft, schaut doch mal im Blogartikel zu Vol. 1 vorbei., Hier unsere Inhalte &amp; Sessions vom Barcamp Vol. 2:, Allgemeine Fragen zum Blog, zum Verein und zum Thema AutorIn werden:Sandra SteingrubeCommunity Managersandra.steingrube(at)tdwi.eu, Fragen zum Inhalt:Leif HitzschkeTDWI e.V. Vorstandsmitgliedleif.hitzschke(at)tdwi.eu, TDWI e.V.Lindlaustraße 2cD-53842 TroisdorfTel: +49 (0)2241/2341-212, ImpressumDatenschutz";https://blog.tdwi.eu/digital-barcamp-powered-by-tdwi-young-guns-vol-2/;TDWI;Young Guns
16. Sep 20;Rückblick auf den 15. TDWI Roundtable Ruhrgebiet | Online;"Am 27.08.2020 fand der 15. Roundtable Ruhrgebiet statt, zum zweiten Mal als Online Roundtable auf Basis von Microsoft Teams. Mit rund 80 Teilnehmern war die Veranstaltung zum Thema „Data Governance – Best Practices &amp; Handlungsfelder“ sehr gut besucht., Die Veranstaltung war gleichzeitig als TDWI Roundtable Ruhrgebiet und Meetup der Gruppe „Business Intelligence und Analytics Dortmund“ angelegt., Hauptvortrag: Data Governance – Best Practices &amp; Handlungsfelder, In seinem Vortrag hat Dr. Carsten Dittmar (Partner und Area Director West bei der Alexander Thamm GmbH) eine Einführung in das immer wichtiger werdende Thema Data Governance gegeben und konkrete Handlungsfelder und Bausteine für eine Umsetzung aufgezeigt., Motiviert hat Dr. Dittmar seinen Vortrag mit der Erkenntnis, dass die Organisation von Daten häufig die Archillesferse von Unternehmen ist, die Daten hier nicht wie ein strategisches Asset behandeln. Data Governance professionalisiert diese Herausforderung und lässt sich in unterschiedliche Handlungsfelder (wie bspw. Prozesse &amp; Verfahren) gruppieren. Für die konkrete Umsetzung wurde ein Vorgehensmodell präsentiert und näher erörtert, welche Ziele sich beispielhaft mit welchen Maßnahmen erreichen lassen. Eine integrierte Befragung der Teilnehmer hatte zum Ergebnis, dass die Ziele Datenqualität/-integrität und Datentransparenz im Fokus stehen, gefolgt von Datenverfügbarkeit und Datenschutz &amp; Daten-Compliance., Vertieft wurde das Thema mit der Einführung eines Data Catalogs als zentrale Maßnahme einer Data Governance. Im Anschluss an die Einführung, was unter einem Data Catalog verstanden wird, hat Dr. Dittmar die wichtigsten Vorteile aufgezeigt und eine Übersicht gängiger Werkzeuge (auf Basis des Magischen Quadranten von Gartner) gegeben. Eine erneute Befragung der Teilnehmer hatte das Ergebnis, dass mehrheitlich keine Lösung im Einsatz ist oder – mit deutlichem Abstand – eine selbst gebaute Lösung z. B. auf Basis von Excel oder Confluence genutzt wird., Abschließend wurden Einblicke in die Praxis gegeben, was häufige Gründe dafür sind, dass die Umsetzung einer Data Governance nicht immer erfolgreich ist. Die Gründe reichen von schlichter Ablehnung („Ich brauche keine Datenpolizei“) bis hin zu Fehlern in der Umsetzung., Zusammenfassung &amp; Ausblick, An den Umfragen und auch der anschließenden Diskussion war klar erkennbar, dass die Teilnehmer mit hohem Interesse dem Vortrag gefolgt sind und vielfach auch Handlungsbedarf zum Thema Data Governance in der eigenen Organisation sehen. Unter anderem wurde die Frage diskutiert, ob es überhaupt eine Option sein kann, das Thema Data Governance zu ignorieren bzw. welche Implikationen sich aus einer solchen Haltung ergeben., Der nächste Roundtable Ruhrgebiet ist für Anfang 2021 geplant., Zum Thema Data Governance ist außerdem folgender Band erschienen:, Data GovernancePeter Gluchowski (Hrsg.)Grundlagen, Konzepte und AnwendungenDieses Buch greift nach einer Einordnung und Abgrenzung des Themas die unterschiedlichen Kernaspekte der Data Governance umfassend auf. Anschließend werden spezielle Facetten und Toolkategorienmit hoher praktischer Relevanz präsentiert, bevor die Darstellung spezifischer Unternehmenslösungen erfolgt., Über Peter Gluchowski, Prof. Dr. Peter Gluchowski leitet den Lehrstuhl für Wirtschaftsinformatik, insb. Systementwicklung und Anwendungssysteme, an der Technischen Universität in Chemnitz und konzentriert sich dort mit seinen Forschungsaktivitäten auf die Themengebiete Business Intelligence und Analytics. Er beschäftigt sich seit rund 20 Jahren mit Fragestellungen, die den praktischen Aufbau dispositiver bzw. analytischer Systeme zur Entscheidungsunterstützung betreffen. Seine Erfahrungen aus unterschiedlichsten Praxisprojekten sind in zahlreichen Veröffentlichungen zu diesem Themenkreis dokumentiert. Er ist Mitbegründer des TDWI Vereins und Ehrenmitglied sowie TDWI Fellow., Allgemeine Fragen zum Blog, zum Verein und zum Thema AutorIn werden:Sandra SteingrubeCommunity Managersandra.steingrube(at)tdwi.eu, Fragen zum Inhalt:Leif HitzschkeTDWI e.V. Vorstandsmitgliedleif.hitzschke(at)tdwi.eu, TDWI e.V.Lindlaustraße 2cD-53842 TroisdorfTel: +49 (0)2241/2341-212, ImpressumDatenschutz";https://blog.tdwi.eu/rueckblick-auf-den-15-tdwi-roundtable-ruhrgebiet-online/;TDWI;Gero Presser
02. Sep 20;Der Weg in die Business Intelligence: Leif Hitzschke im Gespräch mit Bianca Schütz;"Erhalten Sie heute einen kurzen Einblick in unsere aktuelle Folge des Data &amp; Insights Podcasts!, Wir haben heute Bianca Schütz zu Gast. Sie war Head of Business Intelligence bei Casamundo, ist aktuell bei HAUSGOLD und hat über 10 Jahre Erfahrung im Online Business. Ihr Schwerpunkt in drei Worten: Tableau, Google Analytics und AWS. Und nebenbei oder vielmehr parallel berät sie auch noch KMU in allen Analytics-Fragestellungen., Leif: Herzlich willkommen, Bianca Schütz., Ich steig direkt ein in das Thema. Ich habe das Thema „Der Weg in die Business Intelligence“ genannt, weil ich gerne mit dir die Frage klären wollte: Business Intelligence – ist das eine Rolle? Ist das ein Business Analyst? Ist das ein Data Analyst? Gehört da noch mehr dazu? Was ist dazu deine Meinung?, Bianca: Ich finde, es ist nicht nur eine Rolle. Ich finde es schwierig, das abzugrenzen. Für mich sind zum Beispiel ein Data Analyst und ein Data Scientist etwas ganz anderes. An meinem Beispiel: Ich habe vorher mehr Analysten-Tätigkeiten gemacht. Das heißt, ich war wirklich in den Daten, habe sie analysiert und den Stakeholdern bereitgestellt. Ich bin jetzt in meiner aktuellen Rolle dann auch mit einem größeren Fokus im Bereich Data Engineering unterwegs. Also ich bin immer noch kein full-time Data Engineer, aber ich habe jetzt einen ganz anderen Fokus und aus dem Grund finde ich schon, dass es auch Unterschiede gibt zwischen den einzelnen Rollen. Wenn man das abgrenzt, dann würde ich sagen, dass der Data Engineer wirklich für die Daten zuständig ist. Das bedeutet, dass er die Daten bereitstellt und aufarbeitet, so dass sie wirklich für die Analyse verwendet werden können. Der Job des Analysten ist dann eher in Richtung Analyse der Daten, Auswertung, Bereitstellung und klare Kommunikation an die Stakeholder. Ein Data Scientist ist es für mich – da gibt es glaube ich keine offizielle Abgrenzung –, wenn Machine Learning dazukommt. Also dann, wenn es eher um Predictive Analytics geht und da wirklich mehr der Science Aspekt und Machine Learning Algorithmen hinzukommt., Leif: Für mich ist das auch so. Für mich gibt es auch diese drei Bereiche: Data Engineering auf der einen Seite, in der Mitte dann den Data/Business Analyst – darüber kann man sich streiten, wie man das bezeichnet – und dann gibt es noch den Data Scientisten. Business Scientist habe ich auch noch nicht gehört. Deshalb würde ich immer diese drei Säulen sehen., Siehst du das in einer Abteilung oder sollte man das getrennt betrachten? Wenn ich ein bisschen auf den Markt schaue, dann sieht das unterschiedlich aus. Da gab es auch in unserem TDWI Blog mal Beiträge zu dem Thema. Gehört Data Science zu BI? Ist das ein Teil davon oder etwas komplett anderes?, Bianca: Ich glaube, es kommt ein bisschen auf die Unternehmensgröße und Zusammensetzung an. Also ich habe auch schon viele Konstrukte gesehen, z.B. dezentrale BI, zentrale BI, BI und Data Science parallel zueinander … Ich glaube, was immer wichtig ist, ist die Zusammenarbeit. Das heißt es macht vielleicht in größeren Unternehmen schon Sinn, dedizierte Analysten zu haben, die zum Beispiel fürs Marketing oder Finance tätig sind oder wirklich Analysten im BI-Team zu haben, die diesen Fokus haben, die größere Expertise haben, weil natürlich die Fragestellungen sich verändern oder abweichen können zwischen den einzelnen Abteilungen. Wenn es parallele Teams gibt, dann ist es für mich sehr wichtig, dass die Teams zusammenarbeiten und kommunizieren. Denn gerade in Richtung Anforderungsmanagement und Kommunikation ist es sehr schwierig, wenn man mehrere Player hat, die im Unternehmen mitmischen und Anforderungen stellen oder auch Zahlen kommunizieren. Wenn zwei Teams nicht miteinander sprechen, dann kann es sein, dass sie parallel die gleichen Sachen machen oder mit anderen Betrachtungsweisen die Zahlen ansehen und dass dann mehr Chaos als wirkliche Klarheit entsteht., Leif: Ich denke, das ist ein guter Punkt: Kommunikation. Die Rollen müssen klar sein – wer ist für was verantwortlich. Wahrscheinlich wäre es gut, wenn diese Teams auch zumindest einem Department untergeordnet sind, so dass es eine Einheit oben drüber gibt, die das Ganze beobachten kann und ggf. gegensteuern kann, wenn es keine klaren Rollen gibt. Würdest du auch sagen, dass ein Data Analyst eigentlich auch mehr im Fachbereich angesiedelt sein sollte? Es gibt da ja unterschiedliche Konstrukte. Einmal gibt es den Data Analysten, der im Data-Team angesiedelt ist und der dann an den Fachbereich ausgeliehen wird. Ich habe aber auch Konstrukte gesehen, wo der Data Analyst direkt im Fachbereich sitzt., Bianca: Aus meiner eigenen Erfahrung finde ich es besser, wenn es möglich ist, dass man ein Analysten-Team hat, das dann einen gewissen Fokus haben kann, um bestimmte Teams besonders gut zu unterstützen. Aber dadurch, dass man sich ja auch im Urlaub vertreten muss, heißt es nicht, dass eine Person nur dediziert für ein Thema zuständig sein sollte. In Sachen Wissensaustausch finde ich es extrem wichtig, wenn man wirklich einen Teambezug hat. Es geht natürlich auch, wenn es mehrere Analysten gibt, die für bestimmte Teams zuständig sind – die kann man auch einmal im Monat oder alle paar Wochen zusammensetzen zum Austausch, aber es ist doch einfacher, wenn man in einem Team ist und dann wirklich weiß, was der andere macht und was die aktuellen Probleme sind oder welche Business-Fragen aktuell wichtig sind. Ich habe mich bei casamundo dafür eingesetzt, dass wir eine zentrale BI wurden und habe mich als Teamleitung eingesetzt, dass wir so aufgestellt waren, dass jede/r in den Urlaub gehen konnte, ohne dass die Business Intelligence komplett zusammenbricht, weil nur eine bestimmte Person eine Frage beantworten kann. Auch da gab es natürlich Fragen, die nicht jeder beantworten konnte. Aber aus meiner Sicht ist eine Mischung ideal – generalistisch aufgestellt zu sein und aber auch eine Spezialisierung zu haben, so dass es einen klaren Ansprechpartner für die Abteilung gibt. Gerade bei mittelgroßen Unternehmen ist es eine gute Wahl, das so zu machen., Hören Sie im Podcast bei 07:30 Minuten weiter …, Allgemeine Fragen zum Blog, zum Verein und zum Thema AutorIn werden:Sandra SteingrubeCommunity Managersandra.steingrube(at)tdwi.eu, Fragen zum Inhalt:Leif HitzschkeTDWI e.V. Vorstandsmitgliedleif.hitzschke(at)tdwi.eu, TDWI e.V.Lindlaustraße 2cD-53842 TroisdorfTel: +49 (0)2241/2341-212, ImpressumDatenschutz";https://blog.tdwi.eu/der-weg-in-die-business-intelligence-leif-hitzschke-im-gespraech-mit-bianca-schuetz/;TDWI;Leif Hitzschke
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
