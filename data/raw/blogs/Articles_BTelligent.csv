Datum;Titel;Text;Link;Quelle;Autor
28.10.2019;            Neural Averaging Ensembles für Tabellendaten mit TensorFlow 2.0        ;", , , Neuronale Netze werden erfolgreich auf so ziemlich jeden Datentyp angewandt: Bilder, Audio, Texte, Videos, Graphen usw. Nur wenn es um Tabellendaten geht, sind baumbasierte Ensembles wie Random Forests und Gradient Boosted Trees immer noch sehr viel verbreiteter. Wenn man diese erfolgreichen Klassiker durch neuronale Netze ersetzen will, dürfte Ensemble Learning immer noch eine Schlüsselidee sein. Dieser Blogbeitrag erklärt, warum das so ist. Dazu gibt’s ein Notebook mit den praktischen Details., Im Jahr 2017 erfanden Günter Klambauer und seine Koautoren die selbstnormalisierenden neuronalen Netze mit dem erklärten Ziel, neuronale Netze fit zu machen für Tabellendaten. Zu dem Zweck verbesserten sie Dense-Feed-forward-Netze, die Standardarchitektur für Tabellendaten, indem sie eine neue Aktivierungsfunktion benutzten. Mit dieser Modifikation können sehr viel tiefere Architekturen trainiert werden. Die vorherige Grenze einer Tiefe von ungefähr vier Schichten war damit überwunden. Obwohl selbstnormalisierende Netze gut funktionieren und Resultate erzielen, die mit Gradient Boosting und Random Forest vergleichbar sind, konnten sie doch die Dominanz klassischer Ensemblemethoden bis jetzt nicht brechen. Ein Grund dafür könnte sein, dass selbstnormalisierende neuronale Netze keine reiche, vielseitige Theorie dafür liefern, wie eine neuronale Architektur für eine bestimmte Anwendung aussehen sollte. Sie erlauben uns lediglich, deutlich tiefer zu gehen als jemals zuvor (was natürlich bereits ein großer Fortschritt ist)., Letztes Jahr habe ich einen Konferenzvortrag über selbstnormalisierende neuronale Netze gehalten. Ich versuchte, die Idee zu popularisieren, dass jetzt genau der richtige Zeitpunkt ist, um sich intensiv mit neuronalen Netzen zu beschäftigen, wenn man mit Tabellendaten arbeitet. Ich bin immer noch überzeugt, dass das richtig ist. Aber nur einen Tag nach meinem Vortrag hatte ich die Ehre und das Vergnügen, einen kleinen, intensiven Workshop mit Hans Georg Zimmermann zu besuchen. Während dieses Workshops wurde mir klar, dass ich neuronale Netze auf eine ziemlich oberflächliche Weise betrachtet hatte, ohne die mathematische Tiefe dessen, was ich im Workshop gelernt habe. Ich habe außerdem gelernt, dass es sehr viel ältere neuronale Methoden gibt, um mit Tabellendaten zu arbeiten, Methoden, deren Entwicklung schon während des Winters der künstlichen Intelligenz begann. Und in diesem Fall sind Oldies auch Goldies., Ich habe den Eindruck, dass diese Methoden noch nicht die Aufmerksamkeit gewonnen haben, die sie verdienen. Es ist zweifelhaft, ob ich in der Lage sein werde, das mit einem Blogbeitrag für eine voraussichtlich ziemlich beschränkte Leserschaft zu ändern, aber ich werde mein Bestes geben. Ich werde versuchen, ein paar erste Schritte zu beschreiben; insgesamt ist es deutlich mehr Stoff, als in einen Blogbeitrag passt. Alle guten Ideen, die hier vorgestellt werden, habe ich von Hans Georg Zimmermann; alle Fehler und Verdrehungen in diesem Text sind meine eigenen. Insbesondere der Gebrauch von Embeddings weiter unten ist meine eigene Variante dieser Methoden., Es ist immer noch nicht völlig verstanden, warum neuronale Netze so gut funktionieren. Es wird allerdings immer klarer, dass die Überparametrisierung, die sie im Regelfall auszeichnet, dazu beiträgt. Um zu verstehen, warum das so ist, darf man nicht durch die Brille der klassischen Statistik schauen, die uns erklären würde, dass typische neuronale Netze viel zu viele Parameter haben. Das ist tatsächlich ein Problem, aber darum kümmern wir uns später. Im Moment schauen wir uns den Trainingsprozess aus der Perspektive der Optimierungstheorie an. Aus diesem Blickwinkel sieht man, dass die scheinbar überflüssigen Parameter uns (oder vielmehr den Trainingsprozess…) davor bewahren, in einem lokalen Minimum stecken zu bleiben. Ansonsten hätten wir auch ein Problem: In den Optimierungsproblemen, die wir beim Training neuronaler Netze lösen, gilt keine der Konvergenzgarantien der klassischen Optimierungstheorie. Das Steckenbleiben in einem lokalen Minimum wäre also der Normalfall., Wie gehen wir jetzt mit den problematischen Aspekten der Überparametrisierung um? Sie führt dazu, dass es nicht eine einzige optimale Gewichtskonfiguration für unser neuronales Netz gibt, sondern sehr viele. Wir können nicht wissen, welche die „richtige“ ist. Ensemble Learning bietet einen sehr pragmatischen Ausweg aus diesem Dilemma an: Wir benutzen mehrere Kopien unseres neuronalen Netzes, jedes davon ausgestattet mit einer der optimalen Gewichtskonfigurationen. Anschließend mitteln wir einfach über die Ergebnisse, die sie liefern. Das reduziert die extreme Varianz, mit der wir es aufgrund der Überparametrisierung zu tun haben. Der ganze Vorgang, einschließlich der Mittelwertbildung, kann in einem einzigen neuronalen Netz abgebildet werden: Die identischen Kopien unseres neuronalen Netzes (des schwachen Lerners) werden jeweils mit der Eingangsschicht verbunden und mit einem einzelnen Mittelwertbildungsneuron als Output. Beim Training dieses Gesamtnetzes sorgen die unterschiedlichen Zufallsinitialisierungen der schwachen Lerner dafür, dass jeder von ihnen eine andere optimale Gewichtskonfiguration lernt., Wenn man diesen Argumenten folgt, ist eine Netzwerkarchitektur, die über identische Kopien kleinerer neuronaler Netze (die schwachen Lerner) mittelt, nicht so verrückt, wie sie auf den ersten Blick aussehen mag. Ich nenne diese Art von Architektur gern ein neuronales Averaging Ensemble. Wir haben bis jetzt noch nicht über die schwachen Lerner selbst gesprochen, die Bausteine dieser Architekturen. Was für ein Netzwerk sollten wir als schwachen Lerner benutzen? Egal wofür wir uns entscheiden, wir werden auf jeden Fall diverse Kopien davon haben. Möglicherweise ist es also eine gute Idee, ein kleines, simples Netz auszuwählen. Die einfachste Möglichkeit ist im Universal Approximation Theorem beschrieben: Ein einziger Hidden Layer reicht schon. Natürlich sind oft tiefere Netze in der Praxis überlegen. Aber für den Moment verfolgen wir mal diese einfachste Möglichkeit. Es bleibt dann nur eine einzige Frage zu klären, um unseren schwachen Lerner komplett zu spezifizieren: Wie viele Neuronen brauchen wir?, Die Antwort ist erstaunlich einfach: Wir trainieren einen einzelnen schwachen Lerner auf den Trainingsdaten (und um der Geschwindigkeit willen müssen wir ihn nicht perfekt trainieren, ein paar Epochen reichen aus). Wir fangen mit einer niedrigen Anzahl von Neuronen im Hidden Layer an, zumBeispiel 10. Nachdem wir das Netz trainiert haben, prüfen wir, ob irgendeines der Neuronen im Hidden Layer überflüssig ist, d.h., ob sein Output mit dem eines anderen Neurons hochkorreliert ist. Wenn nicht, probieren wir eine größere Anzahl Neuronen (20 zum Beispiel), bis wir zwei hochkorrelierte Neuronen haben. Auf diese Weise stellen wir sicher, dass der schwache Lerner überparametrisiert ist, aber nicht zu extrem. In der Praxis ist es eine gute Idee, dieses Vorgehen mit verschiedenen Startwerten für den Zufallszahlengenerator zu wiederholen, um eine robuste Lösung zu finden. Im Zweifelsfall wählen wir die Anzahl Neuronen eher zu hoch als zu niedrig., Jetzt fehlt nur noch eins zu einem kompletten Sizing: die Anzahl Kopien des schwachen Lerners. Menschen mit einem Hang zur Kontrolle können die Anzahl Kopien mit einer ähnlichen Herangehensweise bestimmen, wie wir sie schon für die Größe des schwachen Lerners verwendet haben: Ein Ensemble mit einer gewissen Anzahl Kopien trainieren, dann schrittweise mehr Kopien probieren, bis zwei davon hoch korrelierte Ergebnisse erzeugen. Ich würde mich freuen, wenn das jemand ausprobieren mag, und ich bin neugierig auf die Ergebnisse., Allerdings hat die Anzahl schwacher Lerner in einem Ensemble drei Eigenschaften, die einen entspannteren Ansatz sinnvoll erscheinen lassen:, Wir nehmen daher eine Abkürzung: Wir benutzen eine niedrige Anzahl Kopien (ungefähr 10) für erste Experimente und eine höhere (50 bis 100), wenn wir das Modell finalisieren. Das funktioniert im Regelfall gut. Wenn jemand auf Ausnahmen stößt, bin ich sehr interessiert, davon zu hören., Die oben beschriebene Modellierung funktioniert gut, solange wir uns auf Regressionsprobleme mit rein numerischen Eingangsdaten beschränken. Für kategoriale Eingangsvariablen und bei Klassifikationsproblemen (also bei kategorialem Output) müssen wir das Netz modifizieren., Für Klassifikationsprobleme brauchen wir lediglich die übliche Softmax-Schicht (als letzte Schicht in jedem der schwachen Lerner). Bei binärer Klassifikation können wir an Stelle dessen auch eine Sigmoid-Schicht mit einem einzelnen Neuron verwenden, um die Wahrscheinlichkeit lediglich einer Klasse abzubilden. Das ist der Ansatz, den ich im Notebook verfolgt habe., Kategoriale Eingangsdaten könnten wir mit Hilfe von Dummyvariablen abbilden. Dieser Ansatz hat bekannte Nachteile, insbesondere treibt er die Dimension der Eingangsdaten in die Höhe. Außerdem führt er oft zu gigantischen schwachen Lernern, wenn man ihn mit der oben beschriebenen Methode des Sizings verwendet. Glücklicherweise gibt es für die Probleme mit Dummyvariablen auch eine ebenso bekannte Lösung, nämlich Embeddings., Für jede kategoriale Eingangsvariable fügen wir unserem schwachen Lerner eine Embeddingschicht direkt nach dem Input Layer hinzu. Damit das funktioniert, müssen die kategorialen Daten so aufbereitet sein, dass jede Kategorie durch eine ganze Zahl von 0 bis Anzahl der Kategorien minus 1 dargestellt wird. Anstatt jeden schwachen Lerner mit seinen eigenen Embeddings auszustatten, können wir sie auch eine gemeinsame Embeddingschicht teilen lassen. Das Notebook ermöglicht Experimente mit beiden Ansätzen. Mein Eindruck ist, dass eine geteilte Embeddingschicht normalerweise gut funktioniert. Es ist also nicht nötig, das Modell dadurch aufzublasen, dass jeder schwache Lerner seine eigenen Embeddings bekommt., Embeddings sind ein faszinierender Kniff, aber sie haben einen Preis: zusätzliche Hyperparameter. Für jede kategoriale Variable müssen wir uns entscheiden, wie viele Dimensionen der Output haben soll. Es gibt dafür mehrere Ansätze: Man kann diese Hyperparameter (gemeinsam mit allen anderen) in einer randomisierten Suche bestimmen, man kann die Ausgabedimension von der Anzahl der Kategorien ableiten, oder man kann einfach auf Basis der eigenen Intuition eine Zahl raten. Ich habe mich für die letzte Möglichkeit entschieden. Die Bestimmung der Ausgabedimension ist ein interessantes Detail, und ich freue mich, wenn jemand Lust hat, ihre oder seine Erfahrungen zu teilen., Wenn wir die Genauigkeit der beschriebenen Modelle mit Gradient Boosted Trees vergleichen wollen und uns dafür nur einen Absatz lang Zeit nehmen, wird das Ergebnis naturgemäß angreifbar sein. Wir versuchen es trotzdem mal. Um zu zeigen, dass diese Methoden auch dort gut funktionieren, wo Gradient Boosting bekanntermaßen stark ist, benutzen wir einen sehr kleinen Datensatz, der außerdem viele kategoriale Variablen enthält. Letzteres ist wichtig, weil baumbasierte Methoden meistens glänzen, wenn sie auf kategoriale Daten angewandt werden. Glücklicherweise hat Szilard Pafka ziemlich viel Arbeit im Bereich Gradient Boosting Benchmarks geleistet. Diese Arbeit basiert außerdem auf einem Datensatz mit vielen kategorialen Variablen. Szilard hat verschiedene Implementationen von Gradient Boosting getestet, mit verschieden großen Versionen eines einheitlichen Datensatzes. Die kleinste Variante hat nur 10.000 Zeilen. Diese werden wir hier benutzen, nur um den Mythos zu bekämpfen, dass Deep Learning immer riesige Datenmengen braucht. Der beste AUC auf dem Testset in Szilards Benchmark für diesen Datensatz ist 70,3. Erreicht wird er von Xgboost. Szilard hat keine umfangreiche Hyperparametersuche durchgeführt, sondern eine kleinere, eher explorative. Da wir unsere Hyperparameter sogar noch weniger getunt haben, wird der Vergleich mit Szilards Ergebnissen unserem Modell keinen unfairen Vorteil verschaffen. Im Notebook kann man nachvollziehen, dass wir mit dem neuronalen Ansatz einen AUC von 69,5 erzielen, wir sind also grob auf einem vergleichbaren Performancelevel. Diese optimale Performance wird bereits nach nur 4 (!) Trainingsepochen erreicht, so dass das Training schneller ist als mit Gradient Boosting., Ich hoffe, dass dieses Beispiel als Appetitanreger taugt. Natürlich ist es nur ein einzelnes Beispiel, mit einem einzigen Datensatz. Wir alle wissen, dass das nicht viel beweist. Darum der Aufruf: Experimentiert mit euren eigenen Daten und lasst mich wissen, wie es läuft!, Wo diese Ideen herkommen, ist noch sehr viel mehr – wir haben nur an der Oberfläche einer tiefen Theorie gekratzt. Diese Techniken können in verschiedene Richtungen ausgebaut werden. Man kann sogar die Analogie zu klassischen Ensembles noch weiter treiben in Richtung von Boostingmodellen, die noch bessere Resultate erzielen. Es gibt auch spezialisierte Techniken für die Modelltransparenz. Ich werde möglicherweise einige dieser Techniken in zukünftigen Blogbeiträgen behandeln. In der Zwischenzeit bin ich gespannt darauf, von euren Erfahrungen zu hören!, , © 2021 b.telligent";https://www.btelligent.com/blog/neural-averaging-ensembles/;B-Telligent;Dr. Michael Allgöwer
25.10.2019;            Datenschutz im DWH         ;Mehr als ein Jahr nach der Einführung der Datenschutz-Grundverordnung (DSGVO) fällt es vielen Unternehmen noch immer schwer, die Themen Data Warehouse (DWH) und Datenschutz unter einen Hut zu bringen. Die in den Unternehmen vorherrschende kundenzentrierte Datenmodellierung ist hierbei eine besondere Herausforderung. Sie führt bei vielen Anforderungen der DSGVO zu großen Problemen in nahezu allen datengetriebenen Prozessen. Doch warum ist es so schwer, die beiden Themen miteinander zu vereinen?, Die größten Veränderungen, die die Datenschutz-Grundverordnung mit sich bringt, entstehen durch die sogenannten Betroffenenrechte. Diese stellen unterschiedliche Anforderungen an das Data Warehouse und werfen einige neue Handlungsfelder auf. Folgender Überblick zeigt die Key Facts der wichtigsten Betroffenenrechte nach DSGVO auf:, Da in fast allen Prozessen im DWH personenbezogene Daten eine tragende Rolle spielen, ist zu erkennen, wie viele datengetriebene Prozesse und Datenkategorien von den genannten Aspekten betroffen sind. Um den gesetzlichen Normen zu genügen und den hohen Bußgeldern zuvorzukommen, die gerade durch die starke Außenwirkung der Betroffenenrechte ein großes Risiko bergen, haben wir sechs DWH-Architekturbausteine identifiziert, die Ihnen den Weg zum datenschutzkonformen DWH erleichtern sollen. , Consent Management Platform: Bereits bei der Erhebung der Daten ist das gesetzeskonforme Consent Management unverzichtbar. Mithilfe einer Consent Management Platform ist es möglich, die Einwilligungen des Kunden zu dokumentieren, nachzuhalten und den unterschiedlichen Datenkategorien im DWH zuzuordnen. Die Nutzung von Einwilligungen ermöglicht somit die DSGVO-konforme Durchführung von sonst kritischen personenbezogenen Datenverarbeitungsprozessen. Mit der Integration des Consent Managements in den Datenhaushalt des DWHs schützen Sie sich außerdem vor der Verarbeitung von Daten, deren Einwilligungen entzogen wurden oder nicht mehr aktuell sind., Pseudonymisierungsengine: Bei der Verarbeitung personenbezogener Daten ist eine Pseudonymisierung ein probates Mittel, um kritische Verarbeitungen mit einem gewissen Schutzniveau durchzuführen. Wichtig ist hierbei jedoch, dass die Pseudonymisierung nicht vom Datenschutz entbindet, da eine Reidentifikation jederzeit möglich ist. Die Pseudonymisierung bringt trotzdem Vorteile mit sich und ist gerade im analytischen Bereich des DWH ein häufig genutztes Mittel, um gesamtheitliche Datenanalysen durchzuführen. Eine Pseudonymisierungsengine und deren Integration in die gesamten Datenbewirtschaftungsprozesse kann hier behilflich sein. Jedoch sind die starke Absicherung des Pseudonymisierungsschlüssels und eine konsequente Anwendung über alle Datenprozesse innerhalb des DWHs dabei ein absolutes Muss., Anonymisierungs-/Löschengine: Eine Löschengine ist aufgrund der Datenschutzanforderungen in der DWH-Architektur unumgänglich. Grundsätzlich lässt sich festhalten, dass im datenschutzrechtlichen Kontext eine Anonymisierung einer Löschung gleichzusetzen ist. Eine Anonymisierung sollte daher immer der physischen Löschung vorgezogen werden, da die durch die Löschung verursachten Folgen für Reporting- und Analyseabteilungen oft gravierend und irreversibel sind. Gerade in der Konzeption der gesetzeskonformen Löschumsetzung bei möglichst minimalem Informationsverlust liegt die größte Herausforderung. Mithilfe eines geeigneten und juristisch abgesicherten Konzepts können alle relevanten personenbezogenen Daten identifiziert, lokalisiert und automatisiert anonymisiert werden. Zeitaufwendige manuelle Prozesse werden so vermieden., Auskunftsengine/-reports: Die Auskunftsengine dient zur automatisierten, vollständigen und schnellen Lokalisierung der personenbezogenen Daten und generiert Reports für Auskunftsersuchen. Da die personenbezogenen Daten einzelner Personen oft über das gesamte DWH verteilt sind, sind auch hier die Vermeidung von manuellen Tätigkeiten und die Steigerung der Rechtssicherheit durch die Vollständigkeit der Auskunft große Pluspunkte. Gerade die strukturierte Kategorisierung der Daten durch das Metadaten-Management ist die unverzichtbare Basis für die Auskunftsengine., Verzeichnis der Verarbeitungstätigkeiten: Das Verzeichnis der Verarbeitungstätigkeiten dokumentiert alle Verarbeitungen mit personenbezogenen Daten. Gerade im DWH-Umfeld ist ein aktuelles und dynamisches Verfahrensverzeichnis unverzichtbar, um der Dokumentations- und Informationspflicht gemäß den gesetzlichen Vorgaben nachzukommen. Wichtig hierbei ist vor allem die Vollständigkeit der Verarbeitungsbeschreibungen nach Artikel 30 DSGVO. Die Integration des Verzeichnisses der Verarbeitungstätigkeiten in die Datenschutzprozesse und in das Corporate-Verfahrensverzeichnis sind Herausforderungen, die gemeistert werden müssen. Besonders nützlich kann hierbei die Verknüpfung des Verzeichnisses mit dem Freigabeprozess der Verarbeitungen sein. Hierbei können Verarbeitungen automatisiert freigegeben werden, wenn entsprechende genehmigte Verfahren im Verzeichnis vorhanden sind. Geeignete Tools (z.B. D-Quantum der Firma Synabi) und ein adäquates Metadaten-Management sind bei diesem Prozess sehr hilfreich. , , D-Quantum Verfahrensverzeichnis, , Metadaten-Management/Data Lineage: Die beschriebenen Anforderungen und Lösungen für ein DSGVO-konformes DWH sind ohne die Erfassung von Informationen über die Daten und datenverarbeitenden Prozesse nicht umzusetzen. Die Entwicklung eines Datenkatalogs zur Darstellung der Data Lineage und der datenverarbeitenden Prozesse erfordert die Etablierung eines Metadaten-Managements, für dessen nachhaltige Steuerung eine Data Governance Verantwortung tragen muss. Für mehr Informationen zum Thema Metadaten-Management und Data Governance empfehlen wir Ihnen unser Webinar „DSGVO und D-QUANTUM“ am 28.11.2019., , All diese Bausteine sind probate Mittel, um den vermeintlichen Gegensatz zwischen Datenschutz und Data Warehouse aufzubrechen. Bedient man sich der richtigen Architektur und setzt alle genannten Aspekte ordnungsgemäß um, so hilft es einem nicht nur, gesetzeskonform zu handeln, sondern gleichzeitig Kosten zu minimieren und einen maximalen Nutzen aus den vorhandenen Daten zu ziehen. Reporting- und Analyseabteilungen können dann auch weiterhin mit einem hohen Maß an Rechtssicherheit agieren und ein Maximum an Mehrwert generieren., Falls Sie weitere Fragen zu den Bausteinen haben oder Unterstützung bei deren Umsetzung benötigen, kontaktieren Sie unsere Experten gerne!, , © 2021 b.telligent;https://www.btelligent.com/blog/datenschutz-im-dwh/;B-Telligent;Thomas Perk
23.10.2019;            CDP-Anbieter im Fokus: Segment        ;Segment wurde 2011 als Start-up gegründet. Der Hauptstandort liegt in Kalifornien, die Firma besitzt aber auch Standorte in New York, Vancouver und Dublin. Zu den Kunden zählen Unternehmen in den unterschiedlichsten Branchen und Größen. Neben Start-ups wie Bonobos oder Instacart gehören Branchengrößen wie IBM, Levi?s und trivago zu den Anwendern von Segment., , Segment ist darauf spezialisiert, Daten aus den unterschiedlichsten Datenquellen zu einer konsistenten Datenbank zusammenzuführen. Dies geschieht zum einen über diverse Schnittstellen, die direkt in Segment integriert sind, und zum anderen über eine standardisierte API, die das Anbinden von Datenquellen vereinfacht. Segment wirbt damit, die Menge an Code, die für die Anbindung von Quellen notwendig ist, massiv zu reduzieren, was die Anbindung beschleunigt und vereinfacht. Darüber hinaus bietet Segment viele Konnektoren zu anderen Plattformen an, um unkompliziert Daten an Folgesysteme weiterzuleiten., Im Kern versucht Segment, die Daten innerhalb der CDP auf eine (Kunden-)Identität zu reduzieren und sie so für andere Systeme vorzubereiten. Dazu gehören auch Funktionalitäten, die das Handling in Bezug auf den Datenschutz vereinfachen. Über eine grafische Oberfläche lassen sich so Daten oder Datentöpfe für Exporte sperren, oder ihre Verarbeitung kann verschlüsselt werden. Unterstützt wird diese Funktionalität durch ein Open-Source-Programm, welches das Consent-Management auf Websites regeln kann, so dass die korrekte Umsetzung der DSGVO gewährleistet ist., Aus diesem Grund lässt sich Segment innerhalb der CDP-Kategorien folgendermaßen einordnen:, Die Customer Data Platform von Segmentbeinhaltet folgende Schwerpunkte:, Im Kern ist Segment für das Handling von Daten konzipiert. Das Tool ist fast vollständig darauf ausgelegt, Daten so einfach wie möglich in die CDP zu laden, die Qualität der Daten zu überwachen sowie den Datenexport zu steuern. In Policies lässt sich festlegen, welche Daten im CDP gespeichert werden und welche Daten in welcher Form weiterverarbeitet werden dürfen. Daneben können sie katalogisiert und damit einheitlich gehandhabt werden. Durch standardisierte Skripte reduziert Segment die Menge an Code, die geschrieben werden muss, wenn neue Datenquellen angebunden werden müssen., Im Analytics-Bereich fokussiert sich Segment auf die Erstellung von Segmenten. Hierfür werden keine Algorithmen oder AI verwendet, sondern die Kundengruppen werden direkt aus einer grafischen Oberfläche herausgebildet. Die so gebildeten Segmente können dadurch einfach den Businessanforderungen angepasst werden, ohne dass dafür BI- oder Data-Science-Personal benötigt wird. Wer tiefgreifendere Analytics betreiben möchte, muss hier auf externe Tools zugreifen, die sich aber einfach mit Segment verbinden lassen., , Mit Personas bietet Segment die Möglichkeit, eine einzelne Nutzeridentität über mehrere Systeme oder Kanäle hinweg zu managen. IDs lassen sich aus unterschiedlichen Quellen zusammenführen, und abhängig von der verwendeten Software können die Informationen aus diesem zentralen Profil in andere Systeme übertragen werden., Natürlich lassen sich in Segment auch Zielgruppen bilden. Über eine grafische Oberfläche können Personen auf Basis ihres Verhaltens gruppiert und jederzeit aktuell gehalten werden. Diese Gruppen lassen sich dann an unterschiedliche Tools wie Facebook Ads oder Marketo weitergeben, damit die Kundensegmente zielgerichtet kontaktiert werden können., Protocols ist eine grafische Übersicht, mit deren Hilfe die Datenimporte in die CDP beobachtet werden können. Mit Hilfe gemeinsamer Datenkataloge lassen sich Daten über das Unternehmen hinweg einfach beschreiben, und die Datentransparenz erhöht sich. Gleichzeitig kann innerhalb der Plattform die Qualität der Daten überprüft werden. Abschließend bietet Protocols die Möglichkeit, die Datenlieferungen zu steuern. So können unvorhergesehene Datensendungen geblockt werden, bevor sie in die CDP laufen., Das korrekte Handling von persönlichen Daten ist heute so wichtig wie nie zuvor. Segment klassifiziert direkt beim Eingang der Daten, ob es sich um personenbezogene Daten handelt, und stuft diese mit Hilfe eines Ampelsystems dahingehend ein, wie kritisch die Information ist. Damit ist es möglich, direkt zu bestimmen, welche Daten überhaupt gesammelt werden sollen und wie die Daten weiterverarbeitet werden. Eine Besonderheit ist der Open-Source-Consent-Manager, der direkt in die eigene Website eingebaut werden kann und über den Nutzer die unterschiedlichen Opt-ins erteilen oder widerrufen können., , Segment ist für jede Organisation eine Überlegung wert, die eine Daten-CDP oder ein Datahub sucht. Gleichzeitig unterstützt Segment bei der DSGVO-konformen Datenspeicherung und Datenverarbeitung. Gerade wenn es bereits mehrere Quellsysteme gibt, aus denen Daten zu einer Teilnehmeridentität zusammengeführt werden müssen, kann Segment zielführend eingesetzt werden., Sie haben Fragen zu der Customer Data Platform von Segment?, Kontaktieren Sie mich gerne!, , © 2021 b.telligent;https://www.btelligent.com/blog/cdp-anbieter-im-fokus-segment/;B-Telligent;Julian Dütsch
08.10.2019;            CDP-Anbieter im Fokus: Arm Treasure Data        ;"Die Firma Treasure Data wurde 2011 in Kalifornien gegründet und 2018 von Arm gekauft. Neben dem Hauptsitz in Mountain View bestehen Niederlassungen in Korea und Japan. Selbst bezeichnet sich Arm Treasure Data als eine Enterprise CDP; das bedeutet, dass die Plattform dafür optimiert ist, mit einer hohen Firmenkomplexität, in vielen unterschiedlichen Business Units und Regionen zu operieren. Arm Treasure Data hat sich auf die Bereiche Automotive, Retail und Konsumgüter, Gaming, Marketing und Werbung sowie IoT fokussiert. Zu den Kunden gehören unter anderem Subaru, Wish, Bandai Namco und Dentsu., , Im Kern möchte Arm Treasure Data Daten sammeln, zusammenführen und segmentieren. Durch die über 120 Konnektoren können fast alle gängigen Systeme sowohl als Inbound- als auch als Outbound-Kanal angebunden werden. Sollte die im Unternehmen verwendete Software dabei nicht unterstützt werden, gibt es eine offene API, die es ermöglicht, weitere Tools anzuhängen. Zusätzlich beinhaltet die CDP von Arm Treasure Data ein Cookie-System, durch welches das Nutzerverhalten auf der Website in Realtime verfolgt und analysiert werden kann. Durch Pipelines werden die Daten direkt nach Treasure Data übermittelt und dort weiterverarbeitet., Außer der Möglichkeit, Daten zu sammeln, zusammenzuführen und anzureichern, besitzt Arm Treasure Data eingebaute Machine-Learning- und Artificial-Intelligence-Algorithmen, um die Nutzer zu segmentieren und Nutzerprofile zu erstellen. Dabei achtet die CDP darauf, dass die länder- und regionsspezifischen Datenschutzbestimmungen eingehalten werden, so dass die Daten auch rechtlich korrekt verwendet werden. Die Ergebnisse der Analysen können zudem im eingebauten Dashboard dargestellt werden, wobei auch der Einsatz von Third-Party-Software wie Tableau oder Power BI möglich ist. Ergänzt wird die Analytics-Plattform durch sogenannte Workflows, in denen Datenverarbeitungsprozesse direkt in Arm Treasure Data erstellt werden können., Aus diesem Grund lässt sich Arm Treasure Data innerhalb der CDP-Kategorien folgendermaßen einordnen:, Die Customer Data Platform von Arm Treasure Databeinhaltet folgende Schwerpunkte:, Die Daten werden bei Arm Treasure Data innerhalb eines Data Lakes gespeichert. Damit handelt es sich um eine Software as a Service. Die Daten, die aus den unterschiedlichen Datentöpfen in die CDP fließen, werden identifiziert und befüllen eine einheitliche Datenplattform. Zu den Datenquellen gehören unter anderem PoS, IoT, Web, SaaS, Mobile und Offline-Ressourcen, die mit Second- oder Third-Party-Daten angereichert werden können. Durch Workflows werden diese Daten umgehend verarbeitet, so dass Reportings oder Analysen direkt mit den neuen Daten ausgegeben werden., Direkt implementiert sind in Arm Treasure Data auch Möglichkeiten, Teilnehmer zu segmentieren. Das Tool zeichnet sich durch ein integriertes Dashboarding-Modul aus, durch das sich die Ergebnisse dieser Segmentierung einfach betrachten lassen. Darüber hinaus besitzt die CDP AI- und ML-Algorithmen, um Scorings vorzunehmen oder Empfehlungen auszusprechen. Diese Ergebnisse können dann in Dashboards verfeinert und die Daten auch in Tools wie Python oder R transferiert werden., Arm Treasure Data hält sich bei den Kampagnenmanagementfunktionen zurück und setzt hier auf Drittanbieter. Hierfür stellt die Plattform fertige Konnektoren zur Verfügung, um die Segmente zu exportieren und abhängig vom Anbieter auch direkt Responses anzufordern, wodurch die Marketingkampagnen deutlich optimiert werden können., , Die eingebauten AI- und ML-Algorithmen bieten eine komfortable Möglichkeit, Segmentierungen zu erstellen und diese zu verwalten. Über Dashboards sind die Ergebnisse einfach zu überblicken, und auch Anpassungen können direkt über eine grafische Oberfläche durchgeführt werden., Durch die Workflows ist es möglich, ohne zusätzliche ETL-Tools Datentransformationen durchzuführen. Direkt beim Upload der Daten in die CDP können die Prozesse gestartet werden, was die Verarbeitung der Daten in Echtzeit ermöglicht. Analysten können dadurch die benötigten Daten einfach für andere Abteilungen bereitstellen, ohne eine IT-Abteilung zu involvieren., Arm Treasure Data richtet sich an große Unternehmen, die eine stabile und hoch skalierbare Lösung suchen. Durch den sehr offenen Ansatz lässt sich bereits verwendete Software einfach integrieren, oder spezifische Bausteine von Treasure Data können durch spezialisierte Tools ersetzt werden. Dies hat den Vorteil, dass Firmen, die am Anfang ihrer digitalen Transformation stehen, ein Tool an der Hand haben, das ein gutes Maß an Grundfunktionalitäten mit sich bringt, später aber problemlos erweitert werden kann, wenn man weiter in die Digitalisierung investieren möchte., Sie haben Fragen zu der Customer Data Platform von Arm Treasure Data?, Kontaktieren Sie mich gerne!, , © 2021 b.telligent";https://www.btelligent.com/blog/cdp-anbieter-im-fokus-arm-treasure-data/;B-Telligent;Julian Dütsch
11.09.2019;            SAP BW Query – performante Nutzung der Filter mit Massendaten, oder: „Wie bekomme ich die Filter flott?“        ,  ;SAP BW stellt als Business-Intelligence-Paket viele Möglichkeiten für ein performantes Reporting bereit – birgt jedoch auch zahlreiche Hemmnisse, die die Performance merklich verlangsamen. Am Beispiel von in der Anwendung verfügbaren Berichtsfiltern zeigt dieser Beitrag, wie kleinste Adaptionen die Leistung von SAP BW mindern und wie eine performante Einstellung der Filtermöglichkeiten zu besseren Ergebnissen führen kann., Durch Definition der fachlichen wie technischen Inhalte ermöglicht die Nutzung von SAP BW Query eine perfekte Vorbereitung der Interaktion von Anwendern und Daten. Was viele Anwender – vor allem fachlich orientierte Power User – jedoch meist nicht wissen: Jede noch so kleine Einstellung kann mitunter immense Auswirkungen auf die Report-Performance haben. Im Falle von Filtern kann dies beispielsweise von schlicht nicht nutzbaren bis hin zu hochperformanten und schnellen Reports variieren., Das nachfolgende Beispiel zeigt eine Filterauswahl aus dem Bereich EC-PCA mit dem InfoObject „Belegart“ in Analysis for Office:, , Bei der Betrachtung von Berichtsfiltern stellt sich der Anwender zuerst einmal die Frage: Wo kann das Filterverhalten definiert werden und welche Gründe gibt es, diese Einstellungen an die eigenen Reporting-Anforderungen anzupassen? Wie bei SAP BW üblich, gibt es zahlreiche Möglichkeiten, das Verhalten der Filterauswahl zu definieren, um so eine größtmögliche Wiederverwendung zu erreichen. Gerade für Query-Entwickler kann dies eine erhebliche Arbeitserleichterung darstellen, da diese im idealen Fall keine Einstellungen ändern müssen. Klar ist: Anwender sollten die zur Verfügung stehenden Filtermöglichkeiten kennen und nutzen, denn die von SAP vordefinierte Auswahl stellt die am wenigsten performante Lösung dar und lässt in Sachen Schnelligkeit häufig zu wünschen übrig., Die Einstellung des Filterverhaltens kann in folgenden Objekten vorgenommen werden:, 1. Im InfoObject und somit global für CompositeProvider und Query: , , 2. Im CompositeProvider, um auf von dem expliziten Datenmodell begründete Anpassungen zu reagieren:, , 3. In der Query, wenn Besonderheiten im Query-Aufbau eine Änderung notwendig machen:, Die oben getroffene Auswahl ändert das Abfrageverhalten von SAP BW auf die Datenbank und damit auch die Anzahl der sichtbaren Einträge. Die Ermittlung von Filtern aus den Stammdaten beispielsweise ist in sehr hoher Geschwindigkeit möglich, zeigt allerdings auch Merkmale ohne Daten, was den Anwender irritieren kann., Folgende Filteroptionen stehen zur Verfügung:, Auswahl, Beschreibung, Problematik, Nur verbuchte Werte für Navigation (Standard), Der Standard stellt bei der Filterung nur Merkmale zur Auswahl, die in der Query vorkommen. Diese Einstellung wertet somit sämtliche in der Query enthaltenen Kennzahlen anhand des Filteraufrisses aus., Bei einem Merkmal mit hoher Kardinalität, wie einer Belegnummer, werden für alle Belegnummern (Queryfilter werden berücksichtigt) die Kennzahlen der Query ausgewertet. Hat eine Query viele Ausnahmeaggregationen oder ist die Anzahl der Einträge in den Stammdaten sehr hoch, kann es bis zur Anzeige der Filterauswahl sehr lange dauern, oder der Vorgang wird sogar abgebrochen. Da die Werte durch die Analytic Engine ermittelt werden, wird zudem die Beschränkung der Anzahl von Einträgen bei der Ermittlung nicht berücksichtigt., Nur Werte im InfoProvider, Die Einstellung ignoriert die Kennzahlen einer Query (berechnete sowie eingeschränkte) und führt einen Join mit den zugrundeliegenden InfoProvidern durch. Dies kann direkt auf der HANA-Datenbank passieren und berücksichtigt auch die Begrenzung der Ergebniszeilen., Diese Einstellung muss gewählt werden, wenn das InfoObject eine hohe Kardinalität aufweist oder die Berechnungen in einer Query sehr komplex sind. Die Queryfilter werden berücksichtigt, jedoch nicht die Kennzahlen. Somit könnten im Filterfenster Einträge ohne Daten angezeigt werden, wenn zum Beispiel eingeschränkte Kennzahlen genutzt werden., Werte in Stammdatentabelle, Die InfoProvider werden ignoriert und es wird nur auf die Stammdatentabelle zugegriffen. Dies unterdrückt den Join auf die InfoProvider und stellt die performanteste Variante dar., Stammdatenabfragen sind sinnvoll, wenn bewusst alle Merkmale eines InfoObjects zur Verfügung stehen sollen. Zudem kann ein CompositeProvider aufgrund von Joins, Berechnungen oder anderen Eigenschaften verursachen, dass die Filterliste nicht performant geöffnet wird., Diese Einstellung umgeht diesen und sämtliche Filter einer Query und ist dabei der performanteste Weg des Zugriffs., Merkmalsbeziehungen, Kann nur in einer Query ausgewählt werden und ist relevant für die Planung. Diese Filteroption soll hier nicht weiter behandelt werden., Die folgenden Beispiele aus der Praxis zeigen Ursache und Lösung möglicher Probleme auf:, Problem, Ursache, Lösung, Analysis for Office stürzt häufig ab, wenn der Anwender die Auswahlliste öffnet., Dies ist häufig der Fall, wenn ein InfoObject mit hoher Kardinalität, wie der Belegnummer, angezeigt werden soll und Ausnahmeaggregationen in der Query vorhanden sind. Die Analytic Engine muss für alle Belege die Berechnungen durchführen, was häufig zum Abbruch aufgrund Speichermangel oder Laufzeit führt., Änderung auf die Einstellung „Nur Werte im InfoProvider“ im InfoObject. Dies umgeht die Berechnung sämtlicher Kennzahlen für jeden Eintrag (z.B. Belegnummer) und führt einen direkten Join auf HANA-Ebene durch. Dieser wird über TOP 1000 abhängig von der Begrenzung in BO eingeschränkt und ist somit performant., , Das Auswahlfenster von z.B. Kalendermonat (0CALMONTH) öffnet sich nur sehr langsam., Eine Query ist häufig auf einen Monat eingeschränkt, um das Datenvolumen zu begrenzen. Ändert der Anwender diesen Filter, führt die Analytic Engine für den gesamten Datenbestand (da der Zeitfilter fehlt) die Berechnung der Kennzahlen durch., Änderung auf die Einstellung „Nur Werte im InfoProvider“ in der Query. Dies umgeht die Berechnung der Kennzahlen ohne zeitliche Einschränkung und führt einen direkten Join auf HANA-Ebene durch., Das Auswahlfenster öffnet sich nur langsam, obwohl nur Werte aus dem InfoProvider gelesen werden., Wird ein virtueller InfoProvider mit Zugriff auf ein Vorsystem oder Hadoop genutzt, kann auch ein Join auf die Daten nicht performant sein., Die Einstellung „Werte in Stammdatentabelle“ in dem CompositeProvider umgeht den Zugriff auf die Bewegungsdaten und liest direkt aus der Stammdatentabelle., Da diese Einstellungen nur in SAP-BI-Mandanten wie Design Studio, Analysis for Office etc. berücksichtigt werden, müssen Tools von Drittanbietern das Verhalten selbst steuern. Wie dies zum Beispiel für Longview funktioniert, ist in einem unserer Blogbeiträge beschrieben., Kleine Änderung, große Wirkung: Um Aufwände bei der Entwicklung von Querys zu vermeiden, sollten Fachbereich und BI-Abteilung immer im Austausch bleiben, Probleme mit den Query-Einstellungen besprechen und die Lösung zentral bereitstellen. Dabei sollte sich aber auch der Power User über das Verhalten von Filtern und deren Auswirkungen bewusst sein, um ggf. eine Optimierungsmöglichkeit erkennen und kommunizieren zu können., Sie stoßen häufiger auf Performance-Probleme? Sprechen Sie uns gerne an. In einer ersten Analyse oder unserem BI-Quickcheck identifizieren wir Stolpersteine und bieten Ihnen schnelle und unkomplizierte Hilfe zur Selbsthilfe., , © 2021 b.telligent;https://www.btelligent.com/blog/kleine-ursache-grosse-wirkung-filter-in-sap-bw/;B-Telligent;Markus Sontheimer
06.09.2019;            CDP-Anbieter im Fokus: Tealium        ;Tealium bezeichnet sich selbst als Universal Data Hub und hat seine Wurzeln im Enterprise Tag Management. Tealium wurde 2008 in San Diego, California, gegründet und hat heute Niederlassungen in den USA, Singapur, Großbritannien, Deutschland, Japan, den Niederlanden, Frankreich und Australien. Zu den Lösungen von Tealium gehören Tealium iQ, das wohl weitverbreitetste unabhängige Enterprise Tag Management, sowie die Customer Data Platform Tealium AudienceStream., Unter den internationalen Kunden finden sich Namen aus allen Branchen wie beispielsweise Retail (u.a. Adidas, Ikea, Patagonia), Travel und Hospitality (u.a. American Airlines, Cathay Pacific, TUI), Technology (u.a. Sony, IBM, Nokia), Finance (u.a. HSBC, AXA) und Media (z.B. The Economist). In Deutschland nutzen unter anderem Booking.com, Lufthansa oder die Allianz die Lösungen von Tealium., , Die große Stärke der Lösungen von Tealium ist das Sammeln und Verknüpfen von digitalen Daten und somit das Aufbrechen von in sich geschlossenen Datensilos. Der Tag Manager Tealium iQ ermöglicht das clientseitige Sammeln von Daten, während EventStream es möglich macht, serverseitig Daten zu erfassen. Diese Datensammlung bildet die Basis, um mit Hilfe von AudienceStream über mehrere Quellen hinweg die Daten zu standardisieren und somit ein einheitliches Nutzerprofil zu erstellen. Die erzeugten Zielgruppen und gemessenen Ereignisse können dann sowohl server- als auch clientseitig zurückgespielt oder über SQL oder JSON Files an z.B. ein Data Warehouse oder eine Data-Science- bzw. BI-Abteilung sowie an Visualisierungstools weitergegeben werden. So werden Daten in Echtzeit über sämtliche Kundenkontaktpunkte technologie- und teamübergreifend verknüpft., Zur Aktivierung werden die Profile oder Segmente über Standard-Konnektoren oder APIs an die entsprechenden Kampagnentools weitergegeben., Die Customer Data Platform von Tealium lässt sich somit innerhalb der CDP-Kategorien folgendermaßen einordnen:, Die Customer Data Platform von Tealium beinhaltet folgende Schwerpunkte:, , Über 1.200 Out-of-the-box-Konnektoren sowohl auf der Datensammlungs- als auch auf der Aktivierungsseite ermöglichen schnelles und einfaches Anbinden vieler Datenquellen. Sollte dennoch ein Konnektor nicht vorhanden sein, kann die Quelle über eine API/Webhook angeschlossen werden. Mit dem Tag Manager und EventStream können nicht nur sämtliche Kanäle einfach angebunden, sondern auch Website- oder App-Elemente einfach vertaggt werden., Für die Analyse können die Daten an ein Data Warehouse, an Machine-Learning- oder BI- und Visualisierungstools übergeben werden. Dazu zählen Amazon Web Services, Microsoft Azure, Google Cloud Platform, Hadoop Apache Spark, Terradata oder Tableau., Tealium bietet auch auf der Engagement-Seite eine Vielzahl von Standard-Konnektoren zur Integration von externen Dienstleistern wie z.B. CRM-Systemen oder Marketing-Clouds. Einige Beispiele wären hier Adobe Campaign, Microsoft Dynamics, Salesforce, aber auch Google DV360 (ehemals DoubleClick) bzw. Facebook. Darüber hinaus ist Tealium grundsätzlich für die Zusammenarbeit mit jedem MarTech-Anbieter offen., , Besondere Highlights von Tealium sind das Visitor Stitching über AudienceStream sowie die leichte Einbindung des weitverbreiteten unabhängigen Enterprise Tag Managements Tealium iQ., Beim ersten Besuch der Seite ohne Nutzer-ID wird ein Tealium-Cookie mit Tealium-ID gesetzt. Diese ID wird anschließend von AudienceStream ausgelesen, um ein Nutzerprofil zu erstellen. Bei einem wiederholten Besuch der Seite mit anschließendem Login werden die Login-Daten gehasht zum bestehenden Nutzerprofil hinzugefügt, wodurch dieses nun einen eindeutigen Identifier hat. Somit bleiben auch die Nutzerdaten vor dem ersten Login erhalten. Loggt sich der Nutzer später über weitere Geräte ein, werden auch diese Informationen dem Nutzerprofil angefügt. Um die Erkennung der Nutzer so effizient wie möglich zu machen, arbeitet Tealium nach einem Key-Ring-Prinzip. Somit können zum Beispiel auch eine Facebook- oder Google-ID zum regulären Login hinzugefügt werden, damit der Nutzer später auch ohne Login wiedererkannt wird. All diese Daten kombiniert führen zu einem einheitlichen Nutzerprofil über mehrere Sitzungen und verschiedene Geräte hinweg., Der weitverbreitete Tag Manager von Tealium gilt als schnellste Tag-Management-Lösung, was sich positiv auf die Seiten-Performance auswirkt. Gleichzeitig gibt es kaum ein System mit so vielen Out-of-the-box-Konnektoren, die ohne großen Implementierungsaufwand genutzt werden können. Zudem bietet Tealium iQ neben QA-Testing-Tools zur Sicherung der Datenqualität auch einen Consumer Privacy Manager und somit eine einfache Integration des Website Consents. Der Tag Manager wird im Gesamtbild des Universal Data Hubs jedoch hauptsächlich als Datenlieferant für die CDP gesehen und lässt sich wie andere Tag Manager mit der CDP nutzen., , Unternehmen, die über den Einsatz einer leistungsstarken Customer Data Platform nachdenken und anbieterunabhängig und -neutral in der Wahl ihrer Engagement-Kanäle bleiben wollen, sollten sich den Universal Data Hub von Tealium genau ansehen., Sie haben Fragen zu der Customer Data Platform von Tealium?, Kontaktieren Sie mich gerne!, , © 2021 b.telligent;https://www.btelligent.com/blog/cdp-anbieter-im-fokus-tealium/;B-Telligent;Alexander Nitsche
02.09.2019;            Bestärkendes Lernen, Bayes-Statistik und TensorFlow Probability: ein Kinderspiel – Teil 2        ;"Im ersten Teil unserer Serie haben wir uns angeschaut, wie die Bayes-Statistik genutzt werden kann, um den Bedarf an Trainingsdaten beim bestärkenden Lernen zu verringern. In diesem Artikel wollen wir diese Idee mithilfe von TensorFlow Probability, einem brandneuen Tool von Google, in ein kleines Modell umsetzen., Schere, Stein, Papier ist sicherlich eines der einfachsten Spiele überhaupt. Dennoch findet man im Internet eine erstaunliche Menge an Literatur dazu. Wir wollen die Bayes-Statistik nutzen, um dieses Spiel zu spielen und dabei die Ungleichgewichte auszunutzen, denen menschliche Entscheidungen unterworfen sind. Dazu entwickeln wir ein Modell von der Denkweise unseres Gegners und wählen anschließend einen Spielzug, mit dem wir ihn schlagen können. Die wichtigsten Abschnitte unseres Codes werden in diesem Artikel dargestellt und erläutert. Der komplette Code zum Selbstausprobieren findet sich im vollständigen Notebook. Um unser Modell möglichst einfach zu halten, konzentrieren wir uns auf das einfachste Ungleichgewicht: Wir wollen Unterschiede in den Häufigkeiten modellieren, mit denen ein menschlicher Spieler sich für einen der drei Spielzüge entscheidet., Eines der schönsten Merkmale der Bayes-Methode besteht darin, dass man ein Modell erstellen kann, indem man einfach nur den Prozess der Datengenerierung Schritt für Schritt beschreibt., Probieren wir das einmal für unser Spiel:, Wenn wir das jetzt in ein Modell übersetzen, ist es am einfachsten, mit dem letzten Schritt zu beginnen, weil er sich auf die konkreten Daten bezieht, die wir nutzen wollen. In unserem Fall handelt es sich um sehr simple Daten: Es wird lediglich gezählt, wie häufig sich unser menschlicher Gegner bisher für Schere, Stein bzw. Papier entschieden hat., Zieht man aus einer Urne, die verschiedenfarbige Kugeln enthält, eine Kugel heraus, legt sie zurück und wiederholt mehrmals, so folgt die Anzahl der Kugeln jeder Farbe der sogenannten Multinomialverteilung. Wenn wir also behaupten, dass die Anzahlen der drei Spielzüge aus einer Multinomialverteilung gezogen werden, bedeutet das nur, dass sich die Spielzüge ebenso verhalten wie die Kugeln im Urnenexperiment. Das bedeutet auch, dass jede Spielrunde unabhängig von allen anderen Runden ist. Ein menschlicher Spieler wendet häufig Strategien an, die Abhängigkeiten zwischen den Runden schaffen, indem er sich beispielsweise scheut, einen bestimmten Spielzug häufiger als zwei- oder dreimal hintereinander zu wählen. Wir wissen also schon, dass die Unabhängigkeit nicht immer gegeben sein wird. Sie kann dennoch eine sinnvolle Annahme sein, wenn sie es uns ermöglicht, gut zu spielen., Bisher ist nichts an unserem Modell spezifisch bayesianisch. Die Verwendung einer Multinomialverteilung in diesem Kontext versteht sich für jeden Statistiker quasi von selbst – sogar für diejenigen, die sich vehement von Bayes-Ansätzen distanzieren., Genuin bayesianisch ist der nächste Schritt, bei dem wir festlegen müssen, welches Vorwissen in Bezug auf die Wahrscheinlichkeit von Schere, Stein und Papier wir in unser Modell integrieren möchten. Dazu legen wir eine Wahrscheinlichkeitsverteilung fest, die verschiedene Kombinationen von Wahrscheinlichkeiten für Schere, Stein und Papier liefert. Dafür eignet sich die sogenannte Dirichlet-Verteilung. Wenn Sie davon zum ersten Mal hören – keine Sorge. Für unsere Zwecke genügt es, zu wissen, dass wir die Version mit drei Parametern nutzen und dass jeder der drei Parameter einem möglichen Spielzug entspricht: Schere, Stein oder Papier. Ist einer der Parameter (z.B. für Papier) größer als ein anderer (z. B. für Stein), dann wird Papier mit höherer Wahrscheinlichkeit auftreten als Stein. Sind alle drei Parameter gleich eins, wird aus der Dirichlet-Verteilung eine Gleichverteilung. Diesen Fall werden wir für die A-priori-Verteilung nutzen. Das bedeutet, dass wir nicht wissen, welcher Spielzug mit einer höheren Wahrscheinlichkeit auftritt als ein anderer, weshalb wir konsequenterweise eine sogenannte nichtinformative A-priori-Verteilung verwenden, die keine der drei Möglichkeiten bevorzugt., Damit ist unser Modell fertig. Es besteht aus zwei Schritten: Im ersten Schritt werden die Wahrscheinlichkeiten für Schere, Stein und Papier aus einer Dirichlet-Verteilung gezogen, und im zweiten Schritt werden diese Wahrscheinlichkeiten in einer Multinomialverteilung eingesetzt, die die jeweilige Anzahl für jeden der drei Spielzüge generiert., Wenn wir die Ideen aus dem vorherigen Abschnitt in Code übersetzen, sieht das Ergebnis so aus., joint_log_probability.py, , Als Erstes fällt vielleicht auf, dass wir hier keine Wahrscheinlichkeit berechnen, sondern den Logarithmus einer Wahrscheinlichkeit. Das hat rein numerische Gründe: Die Wahrscheinlichkeiten, mit denen wir arbeiten, fallen in manchen Fällen so winzig aus, dass numerische Probleme auftreten können. Verwenden wir stattdessen Log-Wahrscheinlichkeiten, können wir den verfügbaren Zahlenbereich einer Gleitkommazahl besser ausnutzen, sodass numerische Probleme sehr viel unwahrscheinlicher werden. Log-Wahrscheinlichkeiten sind genauso einfach zu benutzen wie die Wahrscheinlichkeiten selbst. Man muss lediglich immer dann, wenn man Wahrscheinlichkeiten multiplizieren würde, die entsprechenden Log-Wahrscheinlichkeiten addieren., Unser wesentliches Ziel in diesem Schritt besteht darin, eine Funktion zu schreiben, die die gemeinsame Log-Wahrscheinlichkeit eines Datenpunktes (d.h. drei Zahlen, jeweils eine für Schere, Stein und Papier) und die entsprechenden Parameterwerte (hier reichen die Wahrscheinlichkeiten für Stein und Papier, aus denen sich die Wahrscheinlichkeit für Schere dann ergibt) ausgibt. Der Rest ist einfach: Wir verwenden die Dirichlet-Verteilung für die Berechnung der Log-Wahrscheinlichkeit unserer Wahrscheinlichkeitsparameterwerte und die Multinomialverteilung für die Berechnung der Log-Wahrscheinlichkeit der Anzahlen für Schere, Stein und Papier. Anschließend addieren wir die beiden Log-Wahrscheinlichkeiten und erhalten die gemeinsame Log-Wahrscheinlichkeit dafür, dass diese Anzahlen gemeinsam mit diesen Wahrscheinlichkeitsparametern (p_rock und p_paper) auftreten. Fertig! Wir haben unser Bayes-Modell in Code übersetzt., Ich habe gerade behauptet, dass unser Modell fertig sei. Wie aber schätzt man die Parameter des Modells (insbesondere p_rock und p_paper), wenn man lediglich ein paar Daten und eine Funktion für die gemeinsame Log-Wahrscheinlichkeit hat? Glücklicherweise ist in TensorFlow Probability für genau diese Aufgabe eine ziemlich raffinierte Maschinerie eingebaut. Sie nennt sich Markov-Chain-Monte-Carlo, und wir wenden die neueste und beste Variante dieser Methode an, die auf den Namen Hamilton-Monte-Carlo hört. Damit können wir Stichproben aus der A-posteriori-Verteilung unseres Modells ziehen – also von der Verteilung, die wir von der A-priori-Verteilung ableiten, indem wir aus den beobachteten Anzahlen von Stein, Schere und Papier lernen. Ist unsere Stichprobe groß genug, erfahren wir alles, was wir über die Verteilung wissen müssen: die Form, den Durchschnitt, den Median, die Modi usw. Hamilton-Monte-Carlo zählt zu den Verfahren, die ziemlich verrückt klingen, bis man feststellt, wie gut sie funktionieren. Die intuitive Idee besteht darin, dass man sich die Verteilung, aus der man Stichproben ziehen möchte, als eine hochdimensionale Landschaft aus Bergen (hohe Wahrscheinlichkeit) und Tälern (geringe Wahrscheinlichkeit) vorstellt. Nun lässt man eine Kugel über diese Landschaft rollen. Überall dort, wo sie entlangrollt, zieht sie in regelmäßigen Zeitabständen Stichproben aus der Verteilung. Da die Kugel bergab an Fahrt gewinnt, rollt sie in den Tälern jedoch deutlich schneller – und zieht dort weniger Stichproben – als auf den Bergen. Überraschenderweise lässt sich das so umsetzen, dass es funktioniert. Wer mehr zu diesem Thema wissen möchten, dem seienJeremy Kuns großartige Einführung in das Thema Markov-Chain-Monte-Carlo und der Blog-Artikel von Richard McElreath zu Hamilton-Monte-Carlo empfohlen. In unserem Fall ist es dieser Code-Schnipsel, der die Stichprobenziehung mithilfe von Hamilton-Monte-Carlo durchführt:hamilton_monte_carlo.py, , Zunächst legen wir beispielhaft die beobachteten Daten fest, an die wir das Modell anpassen wollen. In unserem Fall ist das fünfmal „Stein“, während „Schere“ und „Papier“ kein einziges Mal gewählt wurden. Anschließend bereiten wir die Markov-Kette technisch vor und ziehen dann Stichproben daraus. Wenn wir die Verteilung der Stichproben für p_rock graphisch darstellen (Abbildung 1, links), sehen wir, dass der Median bei 0,8 liegt, was bei fünfmal Stein und keinem einzigen Mal Schere und Papier sehr plausibel ist. Der Graph zeigt auch, dass es in Bezug auf den genauen Wert von p_rock noch einige Ungewissheit gibt. Die Graphen für p_paper und p_scissors (Abbildung 1, Mitte und rechts) zeigen, dass diese Werte wie erwartet sehr nahe an null liegen. Diese Werte könnten wir jetzt direkt für das bestärkende Lernen nutzen. Doch zunächst wollen wir überprüfen, ob unsere Simulation funktioniert hat., , Abbildung 1, Wer die Markov-Chain-Simulation ausprobiert hat, stellt fest, dass es rund 17 Minuten dauert, um die entsprechende Zelle im Notebook auszuführen. Das liegt daran, dass der Code im Modus Eager Execution ausgeführt wird. TensorFlow 2 bietet alternativ auch Ausführungsmechanismen, die so ähnlich funktionieren wie der Graph-Modus in TensorFlow 1. Ich konnte jedoch nicht herausfinden, wie man sie mit TensorFlow Probability kombinieren kann, um die Markov-Chain-Simulation zu beschleunigen. Eigentlich sollte es reichen, wie in unserem Notebook tf.function zu verwenden (einmal als Decorator und einmal als Funktion). Leider bringt das keine Verbesserung der Performance. Derselbe Code (mit kleinen Änderungen für den Graph-Modus) wird in TensorFlow 1 etwa zehnmal schneller ausgeführt. Ich gehe davon aus, dass das Problem demnächst gelöst wird. Über entsprechende Ideen von Lesern (und Leserinnen!) freue ich mich. Möglicherweise müssen wir uns auch einfach bis zur nächsten Version von TensorFlow Probability gedulden. Bis dahin nehme ich lieber die schlechte Performance in Kauf, als das Notebook in TensorFlow 1 zu präsentieren, das sehr bald überholt sein wird., Da wir diese große Blackbox namens Hamilton-Monte-Carlo zum ersten Mal angewendet haben, ist es besser, wenn wir prüfen, dass sie auch die richtigen Ergebnisse generiert. Zum Glück befinden wir uns in einer außergewöhnlichen Situation: Normalerweise braucht man für die Parameterschätzung in Bayes-Modellen numerische Verfahren wie Hamilton-Monte-Carlo. Da wir unser Beispiel jedoch sehr simpel gehalten haben, befinden wir uns in der äußerst seltenen Situation, für die Parameterschätzung eine einfache mathematische Formel zur Verfügung zu haben. Damit können wir die Monte-Carlo-Ergebnisse ganz einfach überprüfen: Wenn wir eine A-priori-Dirichlet-Verteilung mit den Parametern (1, 1, 1) und den Daten (s, r, p) haben (wobei s/r/p die beobachteten Anzahlen von Schere/Stein/Papier sind), ist die entsprechende A-posteriori-Wahrscheinlichkeit (die Verteilung, aus der wir mithilfe von Hamilton-Monte-Carlo Stichproben gezogen haben) eine Dirichlet(1 + r, 1 + p, 1 + s)-Verteilung. Wenn man die entsprechenden Graphen der A-posteriori-Verteilung (Abbildung 2) mit Abbildung 1 vergleicht, stellt man fest, dass sie einander sehr ähnlich sind., , Abbildung 2, Nun ist es an der Zeit, unser Modell mithilfe von bestärkendem Lernen in ein Spiel zu verwandeln. Da das Spiel mehr Spaß macht, wenn die Spielzüge schnell berechnet werden, verwenden wir die „mathemagische“ Abkürzung aus dem letzten Abschnitt für die Berechnung der A-posteriori-Verteilung. Dieser Schnipsel enthält den Hauptteil des Codes.main_game_code.py,  Er wird vom UI-Code aufgerufen, sobald wir im Spiel einen der Buttons betätigen. Das Argument „button_info“ ist entweder Schere, Stein oder Papier. Wer das Spiel beim Lesen ausprobieren möchte, kann dazu wieder das vollständige Notebook benutzen., Das Spiel benutzt unser Modell, um herauszufinden, wie hoch die Wahrscheinlichkeit jeweils ist, dass sich unser menschliche Gegner für einen der drei Spielzüge entscheidet. Die Spielzugdaten und die daraus berechneten Wahrscheinlichkeiten werden nach jeder Runde aktualisiert. Anschließend ziehen wir aus dieser Verteilung den Spielzug, den unser Gegner wahrscheinlich wählen wird, und berechnen daraufhin unseren eigenen Spielzug: Entscheidet sich unser Gegner für Stein, wählen wir Papier. Wählt er Papier, entscheiden wir uns für Schere usw. Auf diese Weise entwickeln wir eine randomisierte Strategie, die genau widerspiegelt, wie häufig unser Gegner jedes Symbol wählt. Diese Strategie verwendet nicht nur einen Punktschätzer, sondern eine vollständige Verteilung der Wahrscheinlichkeit jedes einzelnen Spielzugs. Man kann sehen, wie sich diese Verteilung im Spielverlauf verändert. Dargestellt wird sie in einem kleinen Graphen, der nach jedem Spielzug aktualisiert wird. Es ist interessant, diese Änderungen zu beobachten, aber gleichzeitig verleihen sie dem menschlichen Gegenspieler einen unfairen Vorteil. Deshalb lassen sie sich abschalten, indem man am Anfang der Zelle „show_plot = False“ setzt., Wer bis hierher gelesen hat, mag (im besten Fall) den Eindruck gewonnen haben, dass die Bayes-Statistik und die Markov-Chain-Monte-Carlo-Methode spannend und nützlich sind. Vermutlich ist da auch ein Gefühl, dass ich viele wichtige Details übersprungen habe (was definitiv zutrifft). Um die Lücken zu schließen, gibt es einige hervorragende Bücher, die ich nur empfehlen kann: Einen tollen Einstieg ins Thema für Programmierer bietet Cam Davidson-Pilons „Bayesian Methods for Hackers“. Das Buch ist kostenlos auf Github verfügbar, und alle Beispiele wurden kürzlich in TensorFlow Probability übertragen. Wer sich eine eher statistisch geprägte Perspektive wünscht, dem sei „Statistical Rethinking“ von Richard McElreath ans Herz gelegt. Die Beispiele wurden (noch?) nicht in TensorFlow Probability übersetzt; lesenswert ist das Buch dennoch. Richard McElreath erklärt Konzepte auf klare und einfache Weise, die esoterisch anmuten, wenn man anderswo darüber liest. An dieser Stelle müsste ich eigentlich auch Literatur zum Thema bestärkendes Lernen empfehlen, doch leider bin ich noch auf kein Buch gestoßen, das mir so gefällt wie die beiden anderen Titel. Deshalb bin ich gespannt, was Ihre Lieblingsbücher zum Thema sind und welche Abenteuer Sie bei eigenen Experimenten mit Bayes-Statistik und bestärkendem Lernen erleben. Für Fragen, Kommentare und Anregungen bin ich jederzeit dankbar., , © 2021 b.telligent";https://www.btelligent.com/blog/bestaerkendes-lernen-bayes-statistik-teil-2/;B-Telligent;Dr. Michael Allgöwer
15.08.2019;            Bestärkendes Lernen, Bayes-Statistik und TensorFlow Probability: ein Kinderspiel - Teil 1        ;Bestärkendes Lernen hat den schlechten Ruf, riesige Datenmengen zu benötigen, sodass Agenten nur mit simulationsgenerierten Daten realistisch trainiert werden können, also z. B. in einem Computerspiel. In diesem Artikel gehen wir anhand eines kleinen, leicht verständlichen Beispiels der Frage nach, wie die Bayes-Statistik hier Abhilfe schaffen kann. Im zweiten Teil dieser Blog-Serie schauen wir uns an, wie das in der Praxis mit TensorFlow Probability, einem brandneuen Tool von Google, möglich ist., In einem aktuellen Podcast-Interview beschreibt der führende Forscher und Vertreter der Bayes-Statistik, Andrew Gelman, die Bayes-Methode folgendermaßen: „Es gibt zwei statistische Ansätze: Beim einen trifft man möglichst wenige Annahmen, beim anderen möglichst viele.“ Letzteres sei bei der Bayes-Statistik der Fall, erklärt er. Klingt erst einmal positiv, oder? Jedoch genießen Annahmen unter Data Scientists einen eher zweifelhaften Ruf, da sie einerseits zwar für die Modellierung gebraucht werden, andererseits aber auch die Fehleranfälligkeit erhöhen., In der Bayes-Statistik werden Annahmen vollkommen anders bewertet: Anstatt sie zu vermeiden, nutzt man sie gezielt als Modellierungstools. Die Bayes-Modellierung kann Domänenwissen flexibel aufnehmen und so zu einem integralen Bestandteil der Modellberechnungen machen. Auf diese Weise schlägt man zwei Fliegen mit einer Klappe: Erstens kann Wissen über die Problemdomäne viel einfacher genutzt werden – und zwar auf genau dokumentierte und transparente Weise. Zweitens können Annahmen auch einfacher überprüft und bei Bedarf modifiziert werden, wenn sie Teil des Modells selbst sind. Ausführliche Prüfungen spielen in der Bayes-Methode eine entscheidende Rolle, um das Fehlerrisiko zu minimieren., Wenn die Bayes-Methode das schwarze Schaf der Statistik ist (und manche halten es tatsächlich dafür), könnte man bestärkendes Lernen vielleicht als den merkwürdigen Neulingin den Bereichen Data Science und Machine Learning bezeichnen. Auch wenn viele der bekannten Techniken des Machine Learning zum Einsatz kommen, ist das Setting ein vollkommen anderes. Anstatt das Modell mit einer großenDatenmenge zu trainieren, zu bewerten und zu optimieren, sind die Trainingsdaten beim bestärkenden Lernen nicht vom Modell getrennt. Vielmehr muss dieses aus einer Reihe von Aktionen eine auswählen und erhält daraufhin eine Belohnung. Anschließend wählt das Modell die nächste Aktion aus, erhält die nächste Belohnung und so weiter. Dabei ist es auf die Maximierung der Belohnungen aus. Die Lerndaten sind also nicht vorgegeben, sondern werden erst durch die Interaktion des Modells mit seiner Umgebung produziert., Die bekannteste Anwendung des bestärkenden Lernens findet sich im Gaming-Bereich. Die Niederlage der amtierenden E-Sport-Weltmeister im Computerspiel Dota gegen die Deep-Reinforcement-Agents von OpenAI hat Schlagzeilen gemacht. Auch das Computerprogramm AlphaZero von DeepMind basiert auf bestärkendem Lernen. Die Rechenkapazitäten, die dabei zum Einsatz kommen, sind immens: Insgesamt 45.000 Jahre Spielzeit haben die Agents von OpenAI im Schnelldurchlauf absolviert. Und die Bedeutung von Spielen und Simulationen für das bestärkende Lernen beschränkt sich nicht auf solche besonders öffentlichkeitswirksamen Fälle. Im OpenAI Gym, einer beliebten Umgebung für das Training von Agents, trifft man auf viele Kult-Computerspiele wie Pong, diverse Atari-Spiele, aber auch physikalische Simulationen, bei denen ein Agent beispielsweise erlernen kann, ein inverses Pendel auf einem Wagen zu balancieren. Hier zeigt sich eine interessante Parallele zur Bayes-Statistik: Beim bestärkenden Lernen nehmen wir häufig an, die Regeln der Umgebung und ihrer Interaktion so gut zu kennen, dass wir eine Simulation als Trainingsumgebung für die Agents entwickeln können. Mit anderen Worten: Bestärkendes Lernen arbeitet in der Regel mit so umfangreichenAnnahmen, dass häufig rein simulierte Spielsituationen zum Einsatz kommen, die mit der „Realität“ nichts zu tun haben. Wie wäre es, wenn man die andere Methode, die auf starkenAnnahmen basiert – nämlich die Bayes-Statistik – nutzen würde, um bestärkendes Lernen auch auf die reale Welt anzuwenden?, Nutzen wir diese abstrakten Ideen doch einmal, um ein konkretes Modell zu entwickeln. Dabei bleiben wir der Tradition des bestärkenden Lernens treu, indem auch wir ein Spiel verwenden. Allerdings brechen wir mit der Tradition, indem die Lernumgebung nicht simuliert wird, sondern in der Interaktion mit einem echten Menschen besteht. Da diese so einfach wie möglich sein sollte, entscheiden wir uns für das beliebte „Schere, Stein, Papier“. Laut Spieltheorie hat dieses Spiel ein einziges Gleichgewicht, bei dem beide Spieler ihre Aktionen gleichzeitig zufällig wählen. Oder einfacher ausgedrückt: Die beste Strategie besteht darin, zufällig zu entscheiden. Doch auch die Spieltheorie hat ihre Annahmen, und diese treffen selten zu, wenn menschliche Spieler beteiligt sind. Menschen sind nicht gut darin, vollkommen zufällig zu handeln. Deshalb ist es interessant, einen Agenten zu entwickeln, der lernt, die Vorlieben seines menschlichen Gegenübers auszunutzen., Im nächsten Teil dieser Serie werden wir …, ... und uns daran gewöhnen müssen, unserem Computer in „Schere, Stein, Papier“ unterlegen zu sein., , © 2021 b.telligent;https://www.btelligent.com/blog/bestaerkendes-lernen-bayes-statistik-teil-1/;B-Telligent;Dr. Michael Allgöwer
12.08.2019;            Daten- oder zufallsgetrieben – wie treffen Sie Ihre Entscheidungen?        ;Datenbasierte Entscheidungsfindung gehört zu den wichtigsten Merkmalen moderner Organisationen. Anders als die Managementtheorie besagt, enthalten Daten in der Praxis aber immer auch Messfehler. Mit anderen Worten: Daten sind nie zu 100 Prozent korrekt. So weit, so bekannt. Trotzdem: nur die wenigsten handeln nach diesem Wissen. Finden Sie in diesem Blogbeitrag heraus, was passiert, wenn das Problem von Messfehlern ernstgenommen wird. Lassen Sie uns die bittere Pille gemeinsam schlucken., Stellen Sie sich das folgende fiktive, aber durchaus realistische Szenario vor: Ein großes Unternehmen nutzt regelmäßig Daten zum Marktanteil auf den Märkten, auf denen es vertreten ist. Dazu kauft es Marktforschungsdaten zu den weltweiten Branchenumsätzen, die detailliert nach Produktkategorien aufgeschlüsselt sind. Die Daten werden im gesamten Unternehmen genutzt, um wichtige Entscheidungen zur Vertriebsmethode, zu Produktionsstandorten und weiteren wichtigen Themen zu treffen. Die am häufigsten herangezogene Kennzahl ist das prozentuale Umsatzwachstum im Vergleich zum Vorjahr. Diese Zahl kann für das Gesamtgeschäft oder für einzelne Produkte und Länder betrachtet werden – je nachdem, welche Entscheidung es zu treffen gilt. Alle Beteiligten sind froh, über diese harten Fakten zu verfügen, und stolz darauf, ihre Entscheidungen datenbasiert und nicht nach dem eigenen Bauchgefühl zu treffen., Unterhält man sich mit Mitarbeitern dieses Unternehmens, erzählen sie, wie lehrreich die genutzten Daten seien. Markus, der als Country Manager tätig ist, sagt: „Bevor wir diese Daten hatten, war ich davon überzeugt, mein Geschäft wie meine Westentasche zu kennen. Die Märkte, für die ich zuständig bin, sind alle gesättigt und unsere Branche ist sehr stabil. Die Wachstumsrate bleibt also Jahr für Jahr mehr oder weniger gleich – zumindest dachte ich das. Aber als ich erstmals die tatsächlichen Marktdaten vorliegen hatte, war ich mehr als überrascht! Ich stellte fest, dass die Märkte immer in Bewegung und alles andere als beständig sind. Zunächst hat mich das geschockt und ich erkannte, dass ich dringend meinen Ansatz ändern musste. Bis dahin hatte ich immer versucht, mit ruhiger Hand zu agieren. Aber wenn Märkte tatsächlich nie stillstehen, darf man das selbst auch nicht. Heute treffe ich meine Entscheidungen viel schneller und bin durchaus bereit, mich ebenso schnell umzuentscheiden. Jeder Einzelne im Team ist wesentlich agiler geworden. Zwar hadern manche noch mit dem Verlust von Sicherheit und Vorhersehbarkeit. Aber um zu überleben, muss man schnell und flexibel sein.“ Markus wirkt stolz, als er seinen kleinen Vortrag hält. Aber er sieht auch erschöpft aus. Die aktuellen Zahlen sind letzte Woche reingekommen und zeigen einen plötzlichen Geschäftsrückgang in Markus? Märkten. Massenentlassungen stehen im Raum und Markus ist nicht sicher, ob diese noch abzuwenden sind., Wennman Markus fragt, warum die Zahlen für einen Markt so schlecht ausfallen, zuckt er mit den Schultern. „Ehrlich gesagt konnte ich das noch nicht ermitteln. Mit der gesamtwirtschaftlichen Situation hängt es nicht zusammen: Die Menschen haben genug Geld, um es auszugeben. Bevor wir die Daten vorliegen hatten, wäre ich von einem Wachstum von fünf Prozent ausgegangen. Der Markt ist zwar gesättigt, aber die Menschen verfügen über eine hohe Kaufkraft und wollen häufig neue und bessere Produkte kaufen.“ Was macht man in einem solchen Fall? Man schluckt die bittere Pille. Nur dann ist man in der Lage, nicht nur die Daten zu sehen, sondern auch den darin enthaltenen Messfehler. Man fängt also an, genauer hinzusehen: Welche Daten liegen hier vor? Es sind Marktforschungsdaten, die im Rahmen einer Umfrage unter Unternehmen aus der Branche erhoben wurden. Welche Art von Fehler ist in diesem Fall wahrscheinlich? Auch ohne genaue Kenntnis der Datenerhebung lässt sich sicher sagen, dass zwei Prozent der Befragten zu den Optimisten zählen. Nehmen wir also an, Markus liegt richtig und der Markt wächst tatsächlich jährlich um fünf Prozent. Was würde herauskommen, wenn man dies aus der verzerrten Perspektive eines zweiprozentigen Messfehlers betrachten würde? Eine schnelle Simulation bringt Klarheit:, Die Simulationsergebnisse sind überraschend (siehe Diagramm unten). Obwohl wir wissen, dass die tatsächliche Wachstumsrate in unserer Simulation fünf Prozent beträgt und die Messabweichung bei lediglich zwei Prozent liegt, schwanken die gemessenen Wachstumsraten heftig zwischen knapp drei und über acht Prozent. Diese extremen Schwankungen sind nicht das Resultat unglücklich gewählter Startwerte des Zufallsgenerators, sondern tatsächlich repräsentativ für einen gewöhnlichen Simulationslauf – was sich ganz leicht feststellen lässt, wenn man ein wenig mit den Startwerten spielt. Was also ist hier passiert? Wie sich unten links im Diagramm ablesen lässt, wirkt sich der Messfehler auf die absoluten Daten nur geringfügig aus. Bezogen auf die Wachstumsrate wird der Fehler jedoch verstärkt und führt zu einer variablen Kurve. Stützt man sich in seinen Entscheidungen auf diese Daten, lässt man sich also in Wirklichkeit vom Rauschen leiten: Die getroffenen Entscheidungen fallen ebenso variabel aus wie die Kurve selbst. Die Bezeichnung „agil“ macht das Ganze nicht besser., Was kann man tun? Zunächst einmal gilt es zu verstehen, dass diese Art der Überinterpretation häufig vorkommt und sich keineswegs nur auf Wachstumsraten beschränkt. In seinem lesenswerten Buch „Narren des Zufalls“ (Original: „Fooled by Randomness“) nennt der sehr empfehlenswerte Autor Nassim Nicholas Taleb viele weitere Beispiele. Die Lösung dieses Problems ist jedoch deutlich älter. Sie stammt von Immanuel Kant, dem wichtigsten Denker der Aufklärung im 18. Jahrhundert. Sein Leitspruch war das alte lateinische Sprichwort „Sapere aude!“, das er folgendermaßen übersetzte: „Habe Mut, dich deines eigenen Verstandes zu bedienen!“., Genau darauf kommt es an: Daten erst zu nutzen, wenn man sie auch wirklich verstanden hat – einschließlich ihrer Mängel und Beschränkungen. Manchmal bedeutet das, dass man die vorliegenden Daten nicht so nutzen kann, wie man es gerne würde. Dann gilt es, entweder eine andere Methode zu finden – oder bessere Daten. Einer Versuchung sollte man jedoch niemals nachgeben: Fehler einfach zu ignorieren., , © 2021 b.telligent;https://www.btelligent.com/blog/are-you-data-driven/;B-Telligent;Dr. Michael Allgöwer
08.08.2019;            CDP-Anbieter im Fokus: Evergage        ;"Evergage wurde 2010 in Boston unter dem Namen Apptegic mit dem Ziel gegründet, Nutzerdaten auf Websites und Webanwendungen zu sammeln und diese zu analysieren, um eine individuelle Customer Experience über alle Kanäle in Echtzeit zu ermöglichen. Nachdem sie sich im Mai 2012 als Finalist beim TechCrunch Disrupt Startup Battlefield durchgesetzt hatten, wurde der cloudbasierte Service veröffentlicht. Ein Jahr später, im August 2013, fand das Rebranding zu Evergage statt. Seit dem Jahr 2015 verfeinert künstliche Intelligenz die A/B- und multivariaten Testing-Möglichkeiten sowie Produkt- und Content-Empfehlungen, die Evergage zur Personalisierung nutzt. Seit 2018 expandiert das Unternehmen in die DACH-Region. Aktuell ist Evergage im Gartner Magic Quadrant for Personalisation Engines als „Leader“ zu finden. Zudem wurde die Lösung von Forrester als Strong Performer für B2B Customer Data Platforms gelistet und hat gerade den MarTech Award im Bereich CDP gewonnen. Zu ihren Kunden zählen unter anderem Walmart, Neiman Marcus, Abercrombie &amp; Fitch, VW, Dell, Lenovo sowie weitere Marken aus den Branchen Retail, Finanzen, Tech, Travel und Entertainment., Der Schwerpunkt von Evergage liegt deutlich auf dem Sammeln und Analysieren von online gewonnenen Nutzerdaten, um diese in Echtzeit für Personalisierung auf der Webseite oder innerhalb einer mobilen App sowie im E-Mail-Marketing zu nutzen. Es ist möglich, die online gewonnenen Daten mit weiteren Daten von z.B. Callcentern, einem Data Warehouse oder In-Store-Käufen anzureichern. Diese Daten werden verwendet, um mit Hilfe von Identity Stitching (Verknüpfen) ein einheitliches Profil des einzelnen Kunden zu erstellen. Aufgrund dieses Profils sowie kontextbezogener Daten können Affinitätsmodelle sowie Predictive Scores (vorausberechnete Wahrscheinlichkeit für ein Nutzersegment) erstellt werden., Über Machine Learning werden nicht nur Produkt- und Content-Empfehlungen erzeugt, sondern es wird auch der richtige Kanal dafür vorgeschlagen. Dies ermöglicht ein individuelles Campaign Management mit A/B-Testing-Möglichkeit sowie die Durchführung von multivariaten Tests auf der Website. Zudem können die erzeugten Segmente in einem Eins-zu-eins-Kontext an die entsprechenden Kanäle übergeben werden., Die Customer Data Platform von Evergage lässt sich somit innerhalb der CDP-Kategorien folgendermaßen einordnen:, Die Customer Data Platform von Evergage beinhaltet folgende Schwerpunkte:, Eine Vielzahl von Standardkonnektoren zu CRM- und ERP-Systemen sowie Webanalyse-Tools und Ad Networks werden durch die Möglichkeit einer ETL-Integration sowie die Anbindung von SDKs oder APIs ergänzt. Ein komplexes Datenmodell ist allerdings noch nicht vorhanden., Insights-Segmentierung im Tool und Predictive Alerting (vorausberechnete Warnhinweise) sind möglich. Konnektoren zu BI und Data Warehouse sowie eine Data Science Workbench zu Apache Zeppelin, Spark, Python oder R sind für tiefergehende Analysen vorgesehen., Evergage ist eine Engagement CDP mit umfangreichen, durch Machine Learning unterstütztenSegmentierungsmöglichkeiten?und Kampagnenmanagement-Funktionalitäten, die auf Eins-zu-eins-Ansprache abgestimmt sind., Besondere Highlights von Evergage sind die Verarbeitung der Daten in Echtzeit, die Machine Learning Recommendations und die daraus resultierende Eins-zu-eins-Hyper-Personalisierung sowie eine Struktur, die auch B2B-Anwendungsfälle zulässt., Onlinedaten können von Evergage in Echtzeit erhoben und verarbeitet werden. Dies ermöglicht eine echte Eins-zu-eins-Personalisierung, die das Nutzerverhalten auf der Website mit einbezieht., Evergage hat seine Stärke bei der Onpage-Personalisierung, die in Echtzeit das Nutzerverhalten verarbeitet und die Nutzererfahrung auf der Seite dadurch individualisiert. Der Nutzer wird auf der Website identifiziert und bekommt damit persönlichen Content ausgespielt. Durch das Verhalten auf der Seite kann der Inhalt permanent den Bedürfnissen des Nutzers angepasst werden. So wird z.B. auf der Startseite ein bestimmtes Produktvideo mit Bezug auf die zuvor besuchten Seiten gezeigt. Wurde dieses Video bereits gesehen, wird ein dazu passendes Anschlussvideo gezeigt. Diese Personalisierung ist auch in E-Mails möglich. In diesem Fall stellt sich der Inhalt der Mail beim Öffnen passend zu den Interessen des Nutzers zusammen. Auf diese Weise können personenbezogene mit kontextbezogenen Daten verbunden werden., , Evergage benutzt künstliche Intelligenz, die auf der Grundlage des Nutzerverhaltens permanent ihr Scoring-Modell zur Segmentierung neu berechnet und dieses somit stetig optimiert. Diese Machine-Learning-Technologie kann nicht nur für die Echtzeitpersonalisierung verwendet werden, sondern schlägt auch den Engagement-Kanal mit der größten Erfolgswahrscheinlichkeit für den nächsten Kommunikationsschritt vor. Somit wird auch die Kanalwahl beim Kampagnenmanagement effizient genutzt., Unternehmen, die einen starken Fokus auf ihr Digitalgeschäft haben, bei dem die Website-Personalisierung eine zentrale Rolle spielt, sollten sich Evergage genauer ansehen. Immer dann, wenn eine Eins-zu-eins-Ansprache des Kunden auch über weitere digitale Kanäle gewünscht wird, sollte Evergage in Betracht gezogen werden., Sie haben Fragen zu der Customer Data Platform von Evergage?, Kontaktieren Sie mich gerne!, , © 2021 b.telligent";https://www.btelligent.com/blog/cdp-anbieter-im-fokus-evergage/;B-Telligent;Alexander Nitsche
06.08.2019;            SAP BusinessObjects 4.2 SP07 und ein Blick in die Zukunft        ;", In den vergangenen Jahren gab es immer wieder Nachrichten und Gerüchte über die Zukunft von SAP BusinessObjects., Mittlerweile gibt es interessante Neuigkeiten:, Am 04.03.2019 hat SAP das Support Package 07 für die „SAP BusinessObjects Business Intelligence Platform 4.2“ (SBOP BI 4.2) veröffentlicht. Dabei handelt es sich laut Aussage von SAP um das letzte Feature Pack für das Release 4.2. Ein Support Package 08 ist zwar geplant, wird aber ein reines Maintenance Release darstellen, das „nur“ Bugfixes und keine neuen Funktionserweiterungen beinhaltet., In Web Intelligence ist jetzt die dynamische Sortierung der Wertelisten von Eingabesteuerelementen möglich (ohne Tricks und Klimmzüge). Darüber hinaus können jetzt auch die Standardwerte von Eingabesteuerelementen dynamisiert werden. Diese Funktion haben viele Anwender schon sehnsüchtig erwartet, vor allem in Verbindung mit Datumsfeldern. Zusammen mit den hierarchischen Eingabesteuerelementen, die bereits in Support Package 06 dazugekommen sind, ergibt sich daraus eine Fülle an neuen Möglichkeiten., Das „fiorisierte Launchpad“ wurde weiter ausgebaut und unterstützt jetzt mehr Funktionen aus der „klassischen“ Oberfläche. Die volle Funktionsgleichheit wird für das kommende Release SBOP BI 4.3 angestrebt., Eine große Neuerung stellt auch die neue „Fiori Admin-Console“ dar, die es nur in der Fiori-Darstellung gibt., Natürlich bringt das Support Package nicht nur neue Funktionen mit, sondern auch eine große Anzahl an Bugfixes. Die Liste der „Fixed Issues“ ist 16 Seiten lang und enthält neben einigen Schönheitsfehlern in den diversen Viewern und Oberflächen auch Lösungen für MemoryLeak-Probleme (z.B. im Crystal-RAS-Server) und Fehlerbereinigungen in Treibern und Formeln., Ende 2019 wird die „SAP BusinessObjects Business Intelligence Platform 4.3“ (SBOP BI 4.3) an den Start gehen und etliche Neuerungen mitbringen. Auch wenn sich bis zum endgültigen Release noch einige Änderungen ergeben können, so stehen doch mittlerweile die wichtigsten Änderungen fest:, Das Fiori-Launchpad wird die Standard-Benutzeroberfläche werden und alle Funktionen des bisherigen Launchpads enthalten. Das „klassische“ BI-Launchpad wird es (zunächst) weiterhin geben und es wird auch nach wie vor gepflegt und auf dem aktuellen Funktionsstand gehalten., Web Intelligence präsentiert sich als komplett neu entwickelte Version auf Basis von HTML 5, sowohl in der Web- als auch in der Desktop-Variante. Die Oberfläche wurde dabei moderner (jünger) gestaltet und soll deutlich einfacher und aufgeräumter in der Bedienung sein. Es gibt dann auch keine Java-Version mehr., Generell werden alle Client-Anwendungen in 64-Bit-Versionen ausgeliefert werden., Die Installation und Konfiguration wurde stark überarbeitet und soll vor allem bei den Themen SSO und SSL deutlich einfacher und übersichtlicher werden., Außerdem wird es eine stärkere Verzahnung mit den SAP-BI-Cloudlösungen geben. Die Systeme können stärker miteinander verzahnt werden (hybride Ansätze); das betrifft sowohl Datenquellen als auch User-Verwaltung und Reports., User, die zum Beispiel in der SAP Analytics Cloud angelegt wurden, können auf Wunsch automatisch in SAP BO übernommen werden und umgekehrt. Zu diesem Zweck unterstützt SBOP 4.3 auch die Anmeldung mit der E-Mail-Adresse als Usernamen., WebI-Berichte können in der Cloud zur Verfügung gestellt werden. Auswertungen, bei denen SAP-Analytics-Cloud-Modelle auf (On-Premises-)Universen zugreifen oder (On-Premises-)WebI-Berichte auf Cloud-Daten, werden so ermöglicht., Crystal Reports wird weiterhin beibehalten und erhält wie bisher regelmäßige Updates der Datenbanktreiber und Bugfixes., Es werden auch einige alte Zöpfe abgeschnitten:, Desktop Intelligence wird (endgültig) gestrichen und auch die serverseitige Unterstützung dafür wird eingestellt., Es wird keine Flash-Unterstützung mehr geben. Das bedeutet das Ende von Dashboards/Xcelsius., Auch in SBOP 4.2 wird Flash nur noch bis maximal Ende 2020 unterstützt., Als Ersatz wird Lumira Designer empfohlen. Eine toolgestützte Migration ist allerdings nicht möglich., Des Weiteren werden auch noch die folgenden Produkte eingestellt:, Hier wird als Ersatz die SAP Analytics Cloud empfohlen. Auch hierfür ist bisher kein Migrationstool geplant., Nach einer Zeit der Unsicherheit offenbart sich nun langsam eine grobe Marschrichtung bei SAP zum Thema BI., SAP BusinessObjects wird (noch) nicht von SAP Analytics Cloud abgelöst, sondern Cloud-Technologie und On-Premises-Strukturen haben ihre individuellen Vorteile und können nun kombiniert und gemeinsam verwendet werden., Auf lange Sicht wird das Thema Cloud sicher einen Hauptschwerpunkt bei SAP bilden, aber trotzdem werden die On-Premises-Produkte wie SAP BO weiterhin gehegt und gepflegt – zumindest bis voraussichtlich 2025., , Wenn Sie Fragen zu BusinessObjects haben oder Unterstützung bei Projekten benötigen, melden Sie sich bei uns. Wir helfen Ihnen gerne weiter., , © 2021 b.telligent";https://www.btelligent.com/blog/sap-bo-42-sp07-und-ein-blick-in-die-zukunft/;B-Telligent;Karl Hörbst
01.08.2019;            Customer Data Platforms - Erfolgsfaktor Evaluierung        ;Endlich: Der Startschuss zur Digitalisierungsstrategie ist gefallen und das Marketing soll ein wichtiger Baustein werden. Im Zuge der Strategie wurde beschlossen, eine Customer Data Platform (CDP) zur Bildung der 360 Grad Kundensicht und Automatisierung von Marketing Journeys im Realtime Umfeld zu nutzen. Eine Entscheidung, die, neben dem nicht geringen Investment wesentlich die Marketing-Prozesse und Bereiche (Online, E-Mail, E-Commerce und CRM) eines Unternehmens beeinflusst. In diesem dritten und letzten Part meiner CDP-Serie möchte ich daher auf die korrekte Auswahl und Bewertung des passenden CDP-Tools eingehen., Das Herzstück einer Evaluierung sind die Demo-Workshops mit ausgewählten CDP-Anbietern. Diese Workshops wollen, wie ein gutes Projekt, gut strukturiert und vorbereitet sein. Hierzu teilen Sie den Prozess idealerweise in vier Teile:, Im Rahmen der Vorbereitungsphase sollte ich auf folgende Punkte achten und mit genügender Zeit vorbereiten:, Folgende Bewertungsdimensionen kann ich u.a. als Grundlage nutzen:, , Nachdem die Vorbereitungen zu den Anbieter-Workshops abgeschlossen wurden und die Anbieter entsprechend die Agenda erhalten haben, kann es nun mit dem interessantesten Teil der Evaluierung weitergehen: Das Durchführen der Demo-Workshops mit den CDP-Anbietern., Zur Nachbereitung der Pitch-Termine sollten dann alle Informationen auf Vollständigkeit geprüft werden:, Eine Entscheidung über die endgültige Auswahl einer CDP zu treffen ist nicht leicht, zumal jede CDP ihre Stärken und Schwächen hat und einige Funktionen sich im Gesamtbild der Lösung auszugleichen scheinen. Umso wichtiger ist es, sich nicht vom ersten Eindruck günstiger Preise oder einer modernen Oberfläche täuschen zu lassen., Entscheidend wird es sein, alle Informationen vergleichbar zu machen, damit die Entscheidung auf einer belastbaren Grundlage steht., Die Einführung einer CDP kann unter Umständen zu einem teuren Vergnügen werden, wenn sich im Nachhinein herausstellt, dass die falsche Lösung durch Fehler im Evaluierungsprozess ausgewählt worden ist. Diesen Fehler können Sie vermeiden, indem Sie den Evaluierungsprozess zur Auswahl einer CDP strukturiert und fundiert mit der richtigen Vorlaufzeit durchführen. Der wichtigste Teil entfällt sicherlich in der Vorbereitung der Demo-Workshops. Hier entscheidet sich durch die Qualität des Anforderungskatalogs und der Use Cases, wie gründlich Sie Ihre CDP auswählen., , © 2021 b.telligent;https://www.btelligent.com/blog/cdp-die-neue-wunderwaffe-im-marketing-teil-3/;B-Telligent;Laurentius Malter
18.07.2019;            CDP-Anbieter im Fokus: Commanders Act        ;2010 unter dem Namen TagCommander ursprünglich als Tag-Management-Lösung gegründet, wuchs die Plattform mit ihren Kunden so weit im Funktionsumfang, dass zum siebten Geburtstag im Jahr 2017 ein Rebranding zu Commanders Act vorgenommen wurde. Seitdem bilden neben dem TagCommander (Tag-Management) noch der FuseCommander (Cross Device/ID Matching), der DataCommander (Realtime Data Activation) sowie der MixCommander (Live-User-Journey-Analyse) und der TrustCommander (Consent-Management/Privacy) die Basis für ein ganzheitliches Consent-Management und eine Customer Data Platform. Commanders Act wird in 17 Ländern weltweit von führenden Marken im E-Commerce wie z.B. Decathlon, Clarins, MyTheresa oder Promod, aber auch von Kunden anderer Branchen wie z.B. Airfrance, Allianz, AXA, Europcar oder Disneyland eingesetzt. Neben dem Hauptsitz in Paris, Frankreich, hat Commanders Act Büros in München, Hamburg, Mailand und Madrid., Mit dem Tag-Management-Ansatz von Commanders Act liegt ein deutlicher Fokus auf dem Sammeln von datenschutzkonformen Echtzeitdaten auf der Website und innerhalb der digitalen Customer Journey. Das Fusionieren von Nutzeridentitäten (Online- und Offline-Daten) in Kombination mit Deduplikation hilft dabei, sich einen ganzheitlichen Blick auf den Kunden zu verschaffen und damit die User Experience über alle Touchpoints hinweg zu verbessern., Die Customer Data Platform lässt sich innerhalb der verschiedenen CDP-Kategorien folgendermaßen einordnen:, Die Customer Data Platform von Commanders Act beinhaltet folgende Schwerpunkte:, , Bei der Data Collection liegt mit dem MixCommander der Fokus auf Digital Campaigns. Mit dem TagCommander können Onsite- und App-Elemente einfach vertaggt werden, um die entsprechenden Daten zu erheben.Der DataCommander erhebt Onsite-Daten. Über den FuseCommander können Cross-Device- und Cross-Channel-Nutzer-IDs gematcht werden. Der TrustCommander bietet gleich bei der Datensammlung den nötigen Privacy-Aspekt durch Erhebung und Verwaltung von Nutzer-Consent., Regelbasierte Segmentierung sowie Integration von verschiedenen BI-Modellierungen und Prediction sind möglich. Visualisierung, Reporting und Advanced Segmentation sind nur über Drittanbieter verfügbar., Commanders Act bietet umfangreiche Möglichkeiten zur Onsite-Aktivierung und Integration von externen Online-Dienstleistern (Real Time Bidding, DMP Feed, Retargeting, Social, CRM, BI usw.)., , Besondere Highlights von Commanders Act sind der Tag Manager mit integrierter Consent-Lösung, die Verarbeitung der Daten in Echtzeit und die Effizienzsteigerung durch eigene Attributionsmodelle., Der TagCommander ist ein Tag Manager mit integrierter Consent-Lösung, der enorme Vorteile beim Verwalten und Steuern von Opt-ins und Opt-outs bietet. Tags werden einfach an den Consent geknüpft und können priorisiert werden. So können Datenpunkte auf der eigenen Website und entlang der digitalen User Journey einfach erfasst und in der CDP datenschutzkonform verarbeitet werden. Optional kann der Tag Manager auch serverseitig verbaut werden., Sollte bereits ein anderes Tag-Management-System im Einsatz sein, ist es dank des modularen Systems ohne weiteres möglich, die CMP- oder CDP-Bestandteile von Commanders Act unabhängig vom TagCommander zu nutzen., Durch die Integration von CDP und CMP können Kunden Kampagnen gezielt über die Kanäle aussteuern, bei denen ein Nutzer-Consent vorliegt, und Lücken bei der Consent-Einholung schließen., Ein weiteres Highlight ist die Verarbeitung der Daten in Echtzeit mit Hilfe des DataCommanders, der eine verzögerungsfreie Segmentierung der Kundendaten und Auslieferung an die Delivery Channels ermöglicht., Mit dem MixCommander kann die Customer Journey genau untersucht und mit Hilfe von eigenen Attributionsmodellen die Marketingeffizienz gesteigert werden. Über den eingebauten Live Report Builder bietet der MixCommander zudem die Möglichkeit, Analysen zu Attributionsmodellen und Kanalperformance nahezu in Echtzeit (1 Minute Verzögerung) und mit einer unbegrenzten Anzahl an Dimensionen und Analyseebenen auszuwerten., Bei Projekten mit Online-Fokus, bei denen das Consent-Management elegant an das Tag- und Daten-Management gekoppelt sein soll sowie die Notwendigkeit besteht, Daten in Echtzeit zu verarbeiten, sollte Commanders Act in jedem Fall in Betracht gezogen werden., Sie haben Fragen zu der Customer Data Platform von Commanders Act?, Kontaktieren Sie mich gerne!, , © 2021 b.telligent;https://www.btelligent.com/blog/cdp-anbieter-im-fokus-commanders-act/;B-Telligent;Alexander Nitsche
18.07.2019;            Erfolgreiches digitales Marketing durch die richtigen Daten        ;Der Anteil digitaler Kontaktpunkte, die essenziell für den Erfolg des eigenen Geschäftsmodells sind, wächst zunehmend. Dadurch ist die Online-Customer-Journey heute ein nicht wegzudenkender Bestandteil der Customer-Intelligence-Strategie. Neben den Online-Werbekontakten, die entlang der gesamten digitalen Customer Journey stattfinden, spielt hier die eigene Website eine zentrale Rolle. Egal ob im klassischen E-Commerce, bei der Lead-Generierung oder auch für Content Provider, der User steht immer im Mittelpunkt. Mit Hilfe von Digital Analytics können das Verhalten und die Bedürfnisse der User besser verstanden werden, so dass daraus fundierte Handlungsempfehlungen abgeleitet werden können, die zu einem verbesserten Service, einem besseren Produkt oder zu einer optimierten Customer Experience führen., Um erfolgreich Entscheidungen für sämtliche digitalen Kontaktpunkte fällen zu können, ist es wichtig, dass eine Digital-Analytics-Kultur im Unternehmen etabliert wird. Im engen Austausch mit allen Fachbereichen wie beispielsweise dem Produktteam, dem Onlinemarketing, den Website- und App-Verantwortlichen werden die entsprechenden Anforderungen erarbeitet und die passende Tracking-Strategie implementiert., Für einen umfassenden kundenzentrischen Blick müssen die Bedürfnisse und das Verhalten der Kunden kanal-, geräte- und plattformübergreifend analysiert werden. Ein wichtiger Bestandteil der kundenzentrierten Kommunikation ist damit die Analyse der digitalen Kanäle und die damit verbundene Ableitung von konkreten Handlungsempfehlungen. Für Digital Analytics haben sich dabei drei wesentliche Bestandteile herauskristallisiert:, , Alle drei Bereiche sollten idealerweise Hand in Hand arbeiten, um größtmögliche Synergien zu generieren., Ziel der Webanalyse ist eine kontinuierliche Erfolgskontrolle von Webseiten. Webanalyse bildet dabei die Basis, um das Verhalten von Usern auf einer Webseite zu verstehen, die User Experience zu optimieren und nach den Conversion-Zielen zu optimieren. Zum Userverhalten zählen zum Beispiel die Verweildauer, besuchte Seitenbereiche, Interaktion mit Elementen, wie Video-Views oder auch die Onsite-Suche. Das Conversion-Ziel hängt vom Geschäftsmodell ab und kann ebenso ein Verkauf im E-Commerce sein wie die Gewinnung eines Leads im B2B-Bereich oder auch die Interaktion mit einem Artikel eines Onlinemagazins., Im Rahmen der Webanalyse wird außerdem beurteilt, welche Nutzer auf der Seite sind (z.B. Anzahl Nutzer und Nutzerqualität), wo sie herkommen (Akquisekanäle) und ob sich das Nutzerverhalten je nach Akquisekanal unterscheidet. Hierfür ist die Untergliederung der Seite in Seitenbereiche und verschiedene Ziele sinnvoll. So kann herausgefunden werden, ob die Nutzer auch die gewünschte Handlung auf der Seite durchführen, wie z.B. ein Produktkauf im E-Commerce, das Ausfüllen eines Kontaktformulars oder das Lesen eines Artikels. Eine Analyse des Abschlussprozesses (Conversion Funnel) auf der Seite kann Aufschluss über mögliche Optimierungspotentiale liefern. Zum Beispiel können Bezahlmethoden identifiziert werden, die Nutzer eher zum Kauf verhelfen, oder Formularfelder, die zum Abbruch führen. Zum Bereich der Onpage-Analyse gehört auch das fortwährende Anpassen und Verbessern der Seite durch Tests und Conversion-Optimierung. Nicht nur klassische Webanalyse-Tools, die auf Seitenaufruf- und Hit-Basis Nutzerdaten sammeln, können hier hilfreich sein, sondern auch Tools, mit denen man Mauszeigerbewegungs- und Klick-Heatmaps sowie ganze Sitzungen einzelner Nutzer aufzeichnen kann, stellen eine gute Ergänzung dazu dar., Das Online Marketing Controlling umfasst die Auswertung der gesamten Onlinemarketing-Maßnahmen sowie deren Zusammenspiel, also die gesamte digitale Customer Journey. Ziel dabei ist eine möglichst effiziente Verteilung des Marketingbudgets. Die Auswertung und Optimierung des Kanals an sich übernimmt in der Regel eine Agentur oder das Inhouse-Team, das für die operative Umsetzung verantwortlich ist. Schwieriger zu beurteilen ist das Zusammenspiel der Kanäle, gerade dann, wenn unterschiedliche Parteien zusammenarbeiten. Der übergreifende Blick über alle Kanäle und die entsprechenden Customer Journeys ist wichtig, um den gesamten Marketingmix zu bewerten und Budgets richtig zu allokieren. Mit Hilfe von verschiedenen Attributionsmodellen neben der Last-Klick-Attribution ist es möglich, den ganzen Marketingmix zu optimieren, Ressourcen zu sparen und die Budgeteffizienz zu steigern. Bei der Last-Klick-Attribution wird lediglich dem Klick auf das letzte Werbemittel die Conversion zugeordnet. Andere Modelle berücksichtigen auch Kontaktpunkte (Werbemittel oder sogar Customer-Service-Kontakte) in der gesamten Journey bis zum Kauf. Das geeignete Modell ist nicht nur vom Kanal, sondern auch sehr stark vom Geschäftsmodell, dem Produkt und schlussendlich dem einzelnen Nutzer abhängig., In der Webanalyse werden erst einmal anonyme Userdaten gesammelt und zu Nutzerclustern gruppiert, obwohl das gesamte Onlinemarketing eigentlich einzelne Kunden anspricht. Die digitale und reale Welt verschmelzen zunehmend. Wenn früher eine gesamte User Journey nur online betrachtet wurde, ist heute der Besuch einer Website oder das online gesehene Produktvideo nur ein Berührungspunkt einer weitaus komplexeren Customer Journey, die sich gleichermaßen über verschiedene digitale und reale Kontaktpunkte erstreckt. Ebenso erschwert die Nutzung von mehreren Geräten die cookiebasierte Auswertung. Um dies zu umgehen, können die Cookiedaten mit einem anonymisierten userbezogenen Identifier verknüpft werden, um so eine ganzheitliche User Journey über alle digitalen Kontaktpunkte hinweg abzubilden.,  Mit diesen Daten können Nutzergruppen segmentiert werden, die deutlich relevantere Targeting-Optionen ermöglichen. Informiert sich ein Kunde im Webshop ausführlich über das Produkt, kauft aber nicht online, fällt er in ein Retargeting-Cluster für Websitebesucher. Führt er den Kauf später im stationären Laden durch, kann diesem Kunden, z.B. mit Hilfe einer Loyalty Card als Identifier, der Offlinekauf zugeordnet werden. Somit kann dieser individuelle Kunde wieder aus dem Retargeting-Segment für Websitebesucher ausgeschlossen werden, um ihn nicht unnötig mit Werbemitteln zu belästigen und gleichzeitig Marketingressourcen einzusparen. Auch das Ausspielen von echtem personalisiertem Content wird so möglich., Wichtig ist, dass die volle Kontrolle über die Auswertungen und das dadurch erlangte Wissen im Unternehmen intern und nicht extern verankert ist. Die Analyse muss neutral und unabhängig von Einflussfaktoren wie z.B. Marketingbudgets oder Präferenzen der Websiteentwicklung entstehen. Daher ist es immer empfehlenswert, das Webtracking und Marketing-Controlling neutral und parallel dazu zu etablieren.,  b.telligent deckt das gesamte Spektrum der Digital-Analytics-Welt ab: von der Anforderungsanalyse und dem Aufbrechen von Datensilos über den Auswahlprozess des richtigen Webtracking-Tools und die Implementierung der Lösung bis hin zur Durchführung der Webanalyse und der ganzheitlichen Customer-Journey-Betrachtung sämtlicher digitaler Touchpoints. Somit werden gemeinsam mit dem Kunden datengestützte Entscheidungen im digitalen Marketing und der Websiteoptimierung etabliert., , Sie möchten sich tiefergehend mit dem Thema Digital Marketing auseinandersetzen?, Kontaktieren Sie mich gerne!, , © 2021 b.telligent;https://www.btelligent.com/blog/erfolgreiches-digitales-marketing-durch-richtige-daten/;B-Telligent;Christian Endres
16.07.2019;            Kausale Inferenz: die ewige Frage nach dem „Warum“ in allen Begegnungen        ;"Das Thema Kausalität ist in der Machine-Learning- und Data-Science-Community derzeit stark im Gespräch. Erst letztes Jahr erörterte Judae Pearl, einer der bekanntesten Forscher auf dem Gebiet Bayesscher Netze, in seinem „Book of Why“, dass das Verständnis von Kausalität die Zukunft künstlicher Intelligenz prägen wird. Auch der Begriff „Prescriptive Analytics“, als Abgrenzung oder weiterer Schritt nach „Predictive analytics“, ist gerade in aller Munde. Das Verständnis kausaler Zusammenhänge ist auch hier unverzichtbar., , Kausalität lässt sich überall in unserer Welt finden, etwa in ganz alltäglichen Erfahrungen: Wenn es regnet, werden Straßen nass. Auch in der Business-Intelligence-Welt können wir von einem besseren Verständnis kausaler Zusammenhänge profitieren: Um zu entscheiden, in welchem Werbekanal ein begrenztes Marketingbudget am besten angelegt ist, sollten wir die Auswirkungen der Werbemaßnahmen möglichst gut kennen. Bei diesen Beispielen können wir logisch argumentieren, warum die eine Größe von der anderen abhängen sollte. Was ist nun aber, wenn solche Informationen nicht zur Verfügung stehen? Oder wenn wir einfach gar nichts über die beobachteten Variablen wissen? Dieses Problem wird als kausale Inferenz (bei zwei Variablen) bezeichnet:, , Um Kausalität mathematisch bzw. statistisch zu formulieren, wird meistens der do-Formalismus benutzt. Hierbei werden bedingte Wahrscheinlichkeiten betrachtet, wobei die Bedingung nicht wie üblich die Beobachtung einer Variablen P(y|X = x), sondern das Eingreifen in das System und das Festlegen der Variablen auf einen Wert ist: P(y|do (X = x)). Um auf den Regen und die nasse Straße zurückzukommen: Hier hätten wir einmal die Wahrscheinlichkeit für Regen, wenn wir eine nasse Straße beobachten (sehr hoch) im Gegensatz zu der Wahrscheinlichkeit für Regen, wenn wir die Straße mit einem Wasserschlauch nass machen (nicht mehr so hoch). Dass diese eben unterschiedlich sind, sagt uns etwas über die kausale Richtung – die eben nicht „Nasse Straße“ —&gt;„Regen“, sondern gerade umgekehrt ist., Kausale Zusammenhänge sind überall zu finden, die Tuebingen Cause Effect Pairs sind eine Sammlung von Datensätzen aus der echten Welt. Beispiele hieraus sind etwa:, , , , Die beschriebenen Methoden helfen uns dabei, den Bayes-Faktor zu bestimmen. Doch es gibt auch eine Methode, hier neuronale Netzwerke einzusetzen. 2017 wurden unter Beteiligung von Facebook AI Research Causal Generative Neural Networks vorgestellt. Dieses Konzept macht sich zunutze, dass neuronale Netzwerke in der Lage sind, Funktionen universell zu approximieren, also prinzipiell jede beliebige Funktion zu modellieren. Hier erhält das Netzwerk die Beobachtungen der einen Variablen, etwa x, und wird dann darauf trainiert, die andere Variable, y, damit zu modellieren. Auch hier tritt wieder die Asymmetrie zwischen kausaler und antikausaler Richtung in Erscheinung. Die wahre kausale Richtung ist in der Regel einfacher zu modellieren als die entgegengesetzte, antikausale Richtung. Die Richtung, in der das Netzwerk die Daten besser modellieren kann, wird dann als die tatsächliche Richtung angenommen., Für das anfangs angesprochene Beispiel, ein Marketingbudget optimal aufteilen zu wollen, ist die Arbeit mit Zeitreihen typisch. Hier ist nun die Fragestellung eher: „Was wäre passiert, wenn die Dinge anders gewesen wären?“ – also z.B. bei vergangenen Werbekampagnen andere Kanäle benutzt worden wären. Diese Fragestellung ist typisch, wenn in der Volkswirtschaftslehre von kausaler Inferenz gesprochen wird, manchmal spricht man auch von Rubins „Potential Outcome Framework“. Eine der modernsten Methoden für solche Analysen sind „Bayesian Structural Time Series”. Google hat hier die Library CausalImpact für die Programmiersprache R veröffentlicht, die auf diese Methodik zurückgreift. Mit gesammelten Zeitreihendaten vergangener Marketingkampagnen können wir nun genau dieser Frage auf den Grund gehen, also einschätzen, wie z.B. der Umsatz ohne die entsprechenden Werbekampagnen gewesen wäre., Das Thema hat zahlreiche Bezüge zur Welt des Machine Learnings. Mitunter ähneln sich die Methoden – so werden etwa Kernels auch bei der Klasse der SVM-Algorithmen genutzt. Die Causal-Generative-Neural-Network-Algorithmen sind ein Beispiel dafür, wie der ML-Werkzeugkasten genutzt werden kann, um zur kausalen Inferenz beizutragen. Denkbar ist aber auch, dass kausale Inferenz als Teil eines größeren Machine-Learning-Szenarios verstanden wird. Im Fall von mehreren Variablen könnte mit kausaler Inferenz im Rahmen eines Unsupervised-Learning-Kontexts in einem ersten Schritt die kausale Richtung der Variablen ergründet werden. Ist die kausale Richtung der Variablen klar, kann dann mit Supervised Learning an konkreten prädiktiven Modellen gearbeitet werden. , Referenzen:, , © 2021 b.telligent";https://www.btelligent.com/blog/kausale-inferenz-die-ewige-frage-nach-dem-warum/;B-Telligent;Maximilian Kurthen
04.07.2019;            Customer Data Platforms – eine Klassifizierung        ;"Anders als bei Marketing-Automation-Lösungen wird nicht nach Funktionsumfang wie Enterprise bzw. Best-of-Breed unterschieden, sondern nach dem funktionsseitigen Schwerpunkt der Customer Data Platform. Das heißt, je nachdem, wie die Anforderungen definiert sind, kann man bereits im Vorhinein eine Auswahl der richtigen CDP-Art durchführen., Insgesamt lassen sich drei verschiedene Kategorien von Customer Data Platforms unterscheiden:, Die Data CDP ist in diesem Kontext analog einer Kundendatenbank zu sehen, weil sie innerhalb der CDP mit Standard-Konnektoren für Datenzusammenführung, Datenaggregationen, das Identitäts-Matching und die Datenpersistenz sorgt. Einige Funktionen zur Diagnose bzw. Sicherung und zum Monitoring der Datenqualität sollte die Lösung mitbringen; auf diese Weise kann bereits bei der Integration der Daten eine hohe Datenqualität innerhalb der CDP sichergestellt werden., Eine Analytics CDP reichert zum einen die CDP-interne Kundendatenbank um Segmentierungsinformationen und Kundenprofile an und erstellt Scores. Zum anderen werden in einer Analytics CDP mit den Daten und Informationen Selektionen und Zielgruppenbestimmungen teilweise mit künstlicher Intelligenz durchgeführt, die dann in nachgelagerten Journeys genutzt werden. State-of-the-Art-Darstellungen der Analysen und Kennzahlen runden das Bild einer modernen Analytics CDP ab, Die Engagement CDP vereint die Disziplinen Kundendatenbank, Analyse/Selektion und Kampagne. Sie erstellt durch Standard-Konnektoren und Identitäts-Matching die für Kampagnen notwendige Kundensicht, produziert Segmente und selektiert Zielgruppen. Diese Zielgruppen erhalten dann in Multi-Step- und Multi-Channel-Kampagnen zielgenaue Angebote. Der Schwerpunkt einer Campaign CDP liegt damit in der Orchestrierung der Customer Journeys., , Wirft man einen zweiten Blick auf die einzelnen Klassifizierungen, so muss man vor allem hinsichtlich der Data CDP und der Engagement CDP folgende Fragen stellen:, Ein Blick auf meine anzuschließenden Quellsysteme hilft mir, die Frage zu beantworten. Habe ich viele Quellsysteme, die ich mit Standard-Konnektoren an meine Data CDP anbinden kann, und kann ich die Kundendaten mit einer identischen Kundennummer relativ problemlos zusammenführen?, Falls ja, kann eine Data CDP hier die erste Wahl sein. Dann kann es zu einer interessanten Konstellation kommen, ich der ich eine Engagement CDP an eine Data CDP anschließe. Über eine Datenversorgung zwischen den CDPs sollte man sich dann ebenfalls Gedanken machen. Alternativ kann ich bestimmte Dinge wie einen Golden Record, die Konsolidierung von Kundendaten aus verschiedenen Quellen oder auch eine persistente Datenhaltung auch in einem extra dafür vorgesehenen vorgeschalteten Data bzw. Customer Hub erstellen. Auf die Nutzung von Standard-Konnektoren muss dann zwar verzichtet werden, aber die Datenmodellierung und die Konsolidierung der Daten kann ich dann über moderne ETL-Werkzeuge bewerkstelligen., Einige Engagement-CDP-Anbieter bieten sehr ausgereifte Personalisierungsfunktionalitäten für die Touchpoints Web, Mobile App oder auch E-Mail an, die ein wirkungsvolles Engagement der Kunden an den entsprechenden Touchpoints sicherstellen. Daher haben wir eine weitere Unterteilung der Engagement CDP vorgenommen., Eine detaillierte Beschreibung der einzelnen Kategorien inklusive der Erweiterungen von b.telligent finden Sie hier., Use Cases (Anwendungsfälle) sollte man dann erstellen, wenn man Teile des Geschäftszwecks bzw. bestimmte Teile des Geschäftsmodells in einem System abgebildet haben möchte. In einem Use Case wird somit das Verhalten eines Systems oder einer bestimmten Funktion aus Anwendersicht beschrieben. Um die Funktionen einer bestimmten CDP anforderungsseitig zu definieren und auch zu bewerten, sollte man sich im Klaren sein, welche Use Cases durch welchen Teil einer CDP abgedeckt werden können. Die einzelnen CDP-Typen unterscheiden sich somit auch signifikant im Abdeckungsgrad in Abhängigkeit der einzelnen Use Cases., Bei Auswahl der richtigen CDP-Art sollte man unbedingt auf die verschiedenen Schwerpunkte achten, die die einzelnen Kategorien von Customer Data Platforms aufweisen. Legt man den Schwerpunkt der Anforderungen auf die Integration der Kundendaten, so ist eine Data CDP die erste Wahl, liegt der Schwerpunkt auf Analytics oder komplexe Journeys, so fällt die Wahl auf eine Analytics oder Engagement CDP. Vorher erstellte Use Cases, die das eigene Geschäftsmodell möglichst konkret abbilden, sollten bei der Auswahl der richtigen Customer-Data-Platform-Kategorie unerlässlich sein., Im dritten und letzten Beitrag der CDP-Blogreihe geht es um die Auswahl und Entscheidungsfindung für eine Customer Data Platform: Wie gehe ich bei einer Evaluierung vor? Welche einzelnen Schritte sind zu tun? Was gibt es dabei zu beachten? Und: Welche typischen Fehler kann man vermeiden?, , © 2021 b.telligent";https://www.btelligent.com/blog/cdp-die-neue-wunderwaffe-im-marketing-teil-2/;B-Telligent;Laurentius Malter
27.06.2019;            Erweitern von Datenlösungen On-Premises auf Snowflake in Azure        ;Diese komplette und schrittweise schnelle Demo zeigt Ihnen, wie man eine existierende Datenquelle intern mit einer modernen Cloud-Warehouse-Lösung wie Snowflake verbindet: , Außerdem zeigt die Demo, dass Snowflake in Azure eine großartige Kombination aus der Azure Datenfabrik, den Funktionen von Azure und Microsoft Power BI neben weiteren bekannten verbundenen Services im Azure-Cloud-Ökosystem ist, inklusive der Funktion des einmaligen Anmeldens im Azure Active-Adressbuch. Es wird außerdem aufgezeigt, dass es möglich ist, eine Hyperscale-Unternehmens-Datenanalytiklösung mit einer Zwei-Anbieter-Strategie zu bauen., Die Azure Datenfabrik kann sowohl ELT und ETL Aufgaben erledigen. Momentan ist kein systemeigener Snowflake Open-Source-Anschluss in ADF V2 verfügbar. Allerdings sind mit dem Snowflake.net Driver verschiedene Optionen für Arbeitsbelastungen mit Azure-Funktionen und benutzerdefinierten Aktivitäten, zusammen mit Azure Datenfabrik-Pipelines, möglich. , Die Szenarien hängen von vielen Faktoren und Umständen ab. In unserem Fall sind die Quellen meistens relationale Datenbank-Management-Systeme. Aus verschiedenen Gründen wollen wir Daten nicht aus unserer Quelle in eine CSV-Datei ziehen und die CSV dann zur Cloud-Blob-Lagerung übertragen sowie diese in Snowflake mit systemeigenen Anschlüssen einspielen. Allerdings wäre dies immer noch ein sinnvolles und geläufiges Szenario., Wir wollen ein Szenario verwenden, in dem wir beide Datenbanken direkt verbinden können, indem wir mithilfe der Azure Datenfabrik die Daten direkt aus den Quellentabellen in Snowflake-Tabellen einspielen. Nach dem Einspielen der Daten möchten wir Microsoft Power BI dazu verwenden, die Daten mithilfe einer Live-Verbindung zu dem Analytik-Modell zu analysieren. Diese direkte Verbindung zum Datenlager hat den Vorteil, dass wir die Daten nicht wieder aus Snowflake laden müssen., , Hintergrund: Architektur bei Azure – wie wir es gebaut haben, Installieren Sie den Snowflake ODBC Driver, , Öffnen Sie ODBC Datenquellen (64-Bit) und erstellen Sie eine neue Datenquelle, , Geben Sie die Details in das Snowflake-Verbindungs-Dialogfenster ein, , Installieren und konfigurieren Sie die Azure Runtime-Integration und erstellen bzw.konfigurieren Sie die Runtime-Integration in einer Azure Datenfabrik V2, , , Installieren Sie Azure Runtime-Integration, intern (On-Premises), , Registrieren Sie die Laufzeit (Runtime) und geben Sie den Authentifizierungsschlüssel der ADF V2 Runtime ein, den Sie vorher erstellt haben, , Erstellen Sie einen neuen ODBC-verknüpften Service in der Azure Datenfabrik, Konfigurieren Sie den ODBC-verknüpften Service, indem Sie den Namen der Datenquelle, den Benutzernamen und das vorher erstellte Passwort eingeben., , , Testen Sie die Verbindung in der Azure Datenfabrik, , Erstellen Sie eine Tabelle in Snowflake - Datentypen sind in dieser schnellen Demo irrelevant., , , Erteilen Sie im Bedarfsfall entsprechende Zugriffsrechte. Für diese Demo wählen wir die Schnellversion, was normalerweise nicht empfohlen wird:, , Kopieren Sie den Datenassistenten (Wizard) aus der Azure Datenfabrik V2, , , Erstellen Sie einen neuen verknüpften Service mit einem internen SQL-Server. Alle verknüfpten Services, die Azure Runtime-Integration unterstützen, sind hier verfügbar, inklusive Oracle, Sap, MySQL, Teradata, etc. Oder Sie können sich einfach mit einem „Direct Load“ durch die Azure Datenfabrik mit Snowflake verbinden., , , , Geben Sie den Servernamen ein, den Sie durch die vorher konfigurierte Azure Runtime-Integration erhalten haben, und lassen Sie sich authentifizieren. Testen Sie die Verbindung. , , Wählen Sie nun die kürzlich konfigurierte interne Datenquelle aus., , Wählen Sie Ihre Datentabellen aus Ihrer internen Datenquelle aus., , Wählen Sie den mit Snowflake verknüpften Service aus, der vorher als Bestimmungsort erstellt wurde, , Ordnen Sie die ausgewählten Tabellen zu, , Ordnen Sie die Spalten den Tabellen zu, , Einstellungen, , Zusammenfassung, , Lassen Sie die Pipeline laufen, , Verbinden Sie sich mit der Power BI Direct Query , , , , , , , © 2021 b.telligent;https://www.btelligent.com/blog/snowflake-erweiterung-von-datenloesungen-on-premises/;B-Telligent;Klaus Blaschek
19.06.2019;"            SAP BW - Distinct Count optimiert oder: ""Wie zähle ich meine Kunden?""        ";"Ausnahmeaggregationen waren vor HANA in SAP BW häufiger eine Herausforderung bei der Laufzeit; dazu gehörte auch die Distinct-Count-Operation. Diese wird zum Beispiel beim Zählen von Kunden aus den Aufträgen genutzt., Früher wurden Distinct-Count-Operationen häufig folgendermaßen umgesetzt: Es wurde eine berechnete Kennzahl mit dem Wert 1 angelegt und diese dann über eine Ausnahmeaggregation aufsummiert. Was in Umgebungen ohne HANA gut geklappt hat, führt aktuell jedoch dazu, dass der Pushdown nicht mehr funktioniert. Je nach Einstellung kann es also dazu kommen, dass die Berechnung nicht optimal durchgeführt wird., Eine Lösung für HANA-Umgebungen muss also her: Konzentrieren wir uns daher im Weiteren auf die optimale Implementierung von Distinct Count mit BW on HANA oder BW/4HANA., Die Funktion ist als Ausnahmeaggregation in SAP BW bereits integriert und kann für Kennzahlen oder in Queries in bereits berechneten Kennzahlen direkt aktiviert werden. Dadurch wird erreicht, dass der optimale Ausführungsplan ausgewählt wird. Da wir zu einem InfoObjekt die Vorkommnisse in den Daten zählen wollen (zum Beispiel unsere Kunden in den Aufträgen), muss das zu zählende InfoObjekt ausgewählt werden. Dies sieht in den BW-MT so aus:, , , Für eine optimale Ausführung des Distinct Count ist im nächsten Schritt das Datenmodell vorzubereiten, um unnötige Operationen auf der Datenbank zu verhindern. Gerade bei einer Ablage in InMemory-Datenbanken mit Column Store ist dies relevant, um eine optimale Ausnutzung der Datenbank-Features sicherzustellen. Dies gilt datenbankübergreifend auch für Produkte anderer Hersteller und stellt ein systemimmanentes Verhalten dar., Für die Ermittlung der Kunden benötigt das BW ohne Optimierung des Datenmodells folgende Objekte, die im Ausführungsplan berücksichtigt werden müssen:, Da die ADSOs keine SID enthalten, muss während der Ausführung ein Join auf die SID-Tabelle des InfoObjekts stattfinden. Dies kostet sehr viel Ressourcen, da der Column Store nicht voll genutzt werden kann (Column vs. Join-Operation), und führt bei einer hohen Anzahl von Stammdaten zu einer längeren Laufzeit. Sind nur wenige Ausprägungen vorhanden, ist dies zu vernachlässigen., Um diesen Join zu vermeiden, existiert im ADSO die Einstellung „Stammdatenprüfung beim Laden/Aktivieren und SID-Speicherung“, wenn man ein InfoObjekt in dem ADSO-Dialog auswählt (siehe Screenshot unten). Somit wird bei der Aktivierung des ADSOs zusätzlich zur Merkmalsausprägung auch der SID-Wert in einer eigenen Spalte gespeichert. Dies verhindert die Notwendigkeit eines Joins, und die Distinct-Count-Operation kann direkt auf der aktiven Tabelle des ADSOs ausgeführt werden., In neueren Support Packages kann diese Optimierung auch über eine Remodellierung durchgeführt werden und das ADSO muss vorher nicht geleert werden. In früherern Versionen kann dabei noch eine Lösung der Inhalte notwendig sein. Abschließend sollte diese Einstellung nicht pauschal für alle Objekte genutzt werden, da die SID-Spalten generell mehr Speicher belegen, sondern wohlüberlegt eingesetzt werden (siehe Webinar: Best Practice – Implementierung von SAP BW on HANA)., , Ähnlich dem SID-Problem kann es bei objektübergreifenden Datenmodellen/Abfragen auch zu Performanceproblemen kommen. Dies resultiert daraus, dass die Datenbank bei einem Union von Tabellen (wie zum Beispiel bei Nutzung eines Composite Providers mit mehreren Objekten) diese einzeln behandeln muss. Im PlanWiz-Ausführungsplan sieht man dies daran, dass zum Beispiel eine temporäre Tabelle angelegt wird. Dies erhöht die Anforderungen an den Hauptspeicher und auch die Laufzeit zur Ausführung., Die ""Probleme"" führen zu folgenden Ausführungsschritte, sich wie auch in anderen Datenbanken notwendig wären:, Lösung Schritt 1:, Diesen können wir einfach vermeiden, indem wir die SID in das ADSO aufnehmen., Lösung Schritt 2 und 3:, Das Datenmodell muss anhand der häufigen Abfragen der Anwender angepasst werden. Erkennt die Analytic Engine beispielsweise, dass ein ADSO für die Ergebnisse nicht notwendig ist, wird dieses auch nicht gelesen (Pruning). Somit erreichen wir die beste Laufzeit, indem sichergestellt ist, dass nur ein ADSO bei der Ermittlung eines Ergebnisses genutzt wird. Einfach lässt sich dies mit semantischen Gruppen in BW/4HANA erreichen. Die analytische Engine muss vorher wissen, was in einem ADSO enthalten ist, um dieses auszuschließen. Ohne diese Information muss es die Inhalte prüfen und nutzt eine temporäre Tabelle. In der Praxis konnte in dem Beispiel für eine ähnliche Problemstellung die Speichernutzung der Query von 20 GByte während der Ausführung mit einer Laufzeit von 30 s auf 200 MByte und 1 s reduziert werden, da wenig Daten von der Datenbank zwischengespeichert werden müssen., , Eine Optimierung des Datenmodells ist immer abhängig von den Abfragen der Anwender. Somit sollte regelmäßig eine Prüfung durchgeführt werden, ob es noch den Anforderungen an die aktuellen Analytics-Prozesse genügt. Berücksichtigt man dies, kann sich die volle Leistung der HANA-Datenbank zusammen mit einem BW entfalten und die Anwender zufriedenstellen., , © 2021 b.telligent";https://www.btelligent.com/blog/sap-bw-distinct-count-optimiert-wie-zaehle-ich-kunden/;B-Telligent;Markus Sontheimer
13.06.2019;            CDP-Anbieter im Fokus: CrossEngage        ;CrossEngage?wurde 2015 in Berlin von den beiden Gründern und Geschäftsführern Manuel Hinz and Dr. Markus?Wübben?mit dem Ziel gegründet, ein Tool zu schaffen,dasdie Marketingaktivitäten über alle Kanäle bündeln kann. Als Customer-Data- und Engagement-Plattform kombiniert?CrossEngage?eine echtzeitfähige Plattform für Kundendaten mit kanalübergreifendem Kampagnenmanagement. Durch eine datenbasierte und über alle Kommunikationskanäle individuell abgestimmte Interaktion können Unternehmen ihre Kundenbindung stärken, die Profitabilität steigern und effizienter neue Kunden gewinnen.?CrossEngage?überzeugt bereits Kunden wie DB Vertrieb,?HelloFresh,?Limango, Hertha BSC Berlin,?BodyChange,?Stylefile,?Friendsurance,?Mycs, Karl Lagerfeld, Depot und Finanzcheck., Als EngagementCDP ermöglicht?CrossEngage?eine über alle Kommunikationskanäle abgestimmte Kundenansprache. Während Endkunden so eine individuelle und sinnvolle Markenkommunikation erleben, profitieren Unternehmen von einer gestärkten Kundenbindung sowie der effizienteren Akquise neuer Kunden. Diese Faktoren entscheiden zunehmend über die langfristige Profitabilität und den Fortbestand von Unternehmen – besonders im Kontext der immer stärker werdenden Plattformökonomie., Im Mai 2018 wurde?CrossEngage?von Gartner Inc. in die Liste der vier „Cool Vendors in?Multichannel?Marketing“ aufgenommen,in der neue und innovative Anbieterausgezeichnet werden, die Unternehmen beim Erreichen ihrer Marketing- und Geschäftsziele unterstützen.?, Mit der?Real-CDP-Zertifizierung erfüllt?CrossEngage?die strengen?Mindestanforderungen?der Branche in Bezug auf eine offene Datenverarbeitung.?Für die technische Implementierung verfügt das Unternehmen über ein umfangreiches Partnernetzwerk. Dieses unterstützt sowohl bei der Anbindung bestehender Kanäle, um diese?weiterzunutzen, als auch bei der flexiblen Integration neuer Lösungen., ?, ?Die Customer Data Platform von Optimove beinhaltet folgende Schwerpunkte:, , , Integration non-relationaler und relationaler Daten durch das Integrationsteam im Projektsetup, Realtime-Datenverarbeitung ist möglich.?, CrossEngage?beschränkt sich hier?bewusst?auf eine umfangreiche Segmentierung. Zusätzliche analytische Funktionalitäten?können über Technologiepartner abgedeckt werden.?, CrossEngage?ist eine Engagement CDP mit umfangreichenSegmentierungsmöglichkeiten?und Kampagnenmanagementfunktionalitäten. Es steht einegroße Auswahl an deutschsprachigen und internationalen Partnernzur Verfügung, die sich auf dieAusspielung in den Kanälenfokussiert haben.?, ?, CrossEngage?lieferteineumfangreicheKundensegmentierungauf relationalen und nicht-relationalen Daten,so dass dieseAudiencesanschliessendüber ein übersichtliches Canvas zu mehrstufigen Cross-Channel-Kampagnen verbunden werdenkönnen. Besonders sticht hierbei z.?B. die?Referenzierungsfunktion?für Events heraus, die es dem Marketer erleichtert, mehrstufige Kampagnen auf Event-Basis zu bauen.?Auch werden Kontrollgruppen korrekt definiert., Das Unternehmen verfügt über ein umfangreiches Partnernetzwerk im deutschsprachigen Raum, was bei dem aktuell sehr UK-/US-lastigen CDP/MarTech-Ökosystem sowie vor dem Hintergrund der DSGVO eine spannende Alternative darstellt., CrossEngage?eignet sich grundsätzlich für alle Unternehmen, die mehrere CRM- und Marketingkanäle bespielen und aufeinander abstimmen müssen.?Unternehmen, die einen starken Fokus auf ihr Digitalgeschäft haben und immer neue Kanäle und Technologien zentral integrieren und nutzen wollen.?, Sie haben Fragen zu der Customer Data Platform von CrossEngage?, Kontaktieren Sie mich gerne!, , © 2021 b.telligent;https://www.btelligent.com/blog/cdp-anbieter-im-fokus-crossengage/;B-Telligent;Alexander Nitsche
04.06.2019;            CDP-Anbieter im Fokus: Optimove        ;Von der Huffington Post 2014 als „Marktführer“ und „eines der wenigen erfolgreichen Start-up-Unternehmen in Tel Aviv“ bezeichnet, wuchs Optimove, das vom ersten Tag an profitabel war und es auch weiterhin ist, auf über 100 Mitarbeiter an, ohne Fremdkapital zu erhalten. Im September 2016, sieben Jahre nach der Markteinführung von Optimove, nahm das Unternehmen seine erste Investition an. IGP, eine Private-Equity-Investmentgesellschaft, die Wachstumskapital für Technologie-Start-ups in der späten Phase bereitstellt, führte eine Investitionsrunde von 20 Millionen US-Dollar bei Optimove durch. Die Mittel werden verwendet, um das Wachstum des Unternehmens zu beschleunigen., Integration non-relationaler und relationaler Daten durch das Integrationsteam im Projekt-Setup, Daten für das statistische Modell werden täglich geladen, Realtime-Daten können ebenso integriert werden., Statistische Modellierung von Segmenten, umfangreiche Reportingfunktionalitäten, Optibot unterstützt mit KI (Künstliche Intelligenz) und ML (Machine Learning) bei der Kampagnenoptimierung, Optimove bietet umfangreiche Kampagnenfunktionalitäten, es können sowohl zusätzliche Optimove-Kanalmodule (Optimail, Optipush), als auch externe Dienstleister angebunden werden., , Eines der Highlights von Optimove sind die sogenannten selbstoptimierenden Kampagnen sowie der Optibot., Eine selbstoptimierende Kampagne ist eine wiederkehrende Kampagnenreihe, die im Laufe der Zeit automatisch optimiert wird. Der Algorithmus von Optimove verfolgt, wie sich konkurrierende Aktionen für jedes Mikrosegment innerhalb der Zielgruppe der Kampagne verhalten, und verwendet diese Daten, um den Cluster-Aktionsmix schrittweise anzupassen, um so eine maximale Steigerung zu erreichen. Dies führt dazu, dass jeder Kunde die Kampagne erhält, auf die er am ehesten jedes Mal reagiert., Optibot wandelt Kunden- und Kampagnenleistungsdaten autonom in wertvolle Erkenntnisse um, so dass Sie Ihre Relationship-Marketing-Aktivitäten kontinuierlich optimieren können. Der allererste Marketingoptimierungs-Bot verwendet KI und ML, um die Analyse und Interpretation der Leistung von Marketingkampagnen zu automatisieren und um mit einem Klick umsetzbare Maßnahmen zu empfehlen, die die Effektivität der Kampagne im Laufe der Zeit deutlich erhöhen., Marketingteams, die einen Einstieg in die analytische Optimierung von Kampagnen suchen, finden mit Optimove ein Tool, das sie über eine ansprechende Oberfläche benutzerfreundlich unterstützt., , Sie haben Fragen zu der Customer Data Platform von Optimove?, Kontaktieren Sie mich gerne!, , © 2021 b.telligent;https://www.btelligent.com/blog/cdp-anbieter-im-fokus-optimove/;B-Telligent;Alexander Nitsche
28.05.2019;            Best Practice für SQL-Statements in Python        ;"Dank eines verpflichtenden Interfaces für Datenbank-Connectors, der ""Python Database API Specification v2.0, PEP249"", wurden alle aktuellen Connectors so entwickelt, dass Datenbankverbindungen und die SQLs für Datenabrufe und Datentransaktionen über dieselben Befehle gestartet werden können. Mehr oder weniger erhält man auch überall im gleichen Format Ergebnisse zurück. In diesem Punkt gibt es gefühlt noch die größten Abweichungen von der geforderten Vereinheitlichung. Das sollte aber niemanden davon abschrecken, Python-Skripte als eine flexible Methode für Automatisierungen von Datenbankoperationen zu verwenden., Alle Connectors beinhalten eine execute-Funktion, die ein SQL-Statement als String-Parameter übernimmt und auf Seiten der Datenbank ausführen lässt. So richtig sinnvoll wird der Einsatz von Python hier aber erst, wenn man SQLs dynamisch und datengetrieben erzeugt., Genau an dieser Stelle möchte ich einhaken und - angefangen bei den einfachsten, aber unklügsten Wegen - verschiedene Alternativen und letztlich eine Best Practice aufzeigen, wie SQL-Strings übergeben werden sollten., Ich muss zu Beginn auch noch mal klarstellen, dass alle außer der letzten Alternative potentiell gefährliche Sicherheitslücken darstellen. Es ist möglich, wenn nicht andere Sicherheitsvorkehrungen getroffen werden, dass sensible Daten abgerufen oder gar gelöscht werden., Zunächst erstellen wir eine Testdatenbank in SQLite. Ich wähle SQLite deswegen zur Demonstration, weil SQLite in Python mitgeliefert wird, Datenbanken rein im Arbeitsspeicher für die Laufzeit des Skriptes erstellt werden können und die Beispiele daher bei jedem selbst replizierbar sind. Fehlerfreie Ausführung der Beispiele kann ich aber nur ab Python 3.x garantieren., Im Codebeispiel wurde nach dem Modulimport eine namenlose Datenbank im Speicher initialisiert, indem im connect-Befehl als Speicherort der Datenbank ':memory:' übergeben wurde. Anschließend wird eine Mitarbeitertabelle namens ""staff"" erzeugt und mit den ersten Daten befüllt., So weit, so gut. Wir wollten aber nicht für alle Mitarbeiter, die wir in die Tabelle noch einfügen werden, extra einen kompletten Insert-Befehl verfassen. Wenn die Mitarbeiternamen bereits als Aufzählung vorliegen, drängt es sich förmlich auf, eine Schleife zu verwenden., , , Das ging trotz aller guter Vorsätze daneben. Die Fehlermeldung ""TypeError: Can't convert 'int' object to str implicitly"" weist uns darauf hin, dass wir vergessen haben, den Datentyp der person_id von integer auf str zu casten. Auch wenn Python in fast allem sehr flexibel ist, ist es dennoch eine stark typisierte Sprache und inmutables-Strings lassen sich nicht mit Integern kombinieren., Da muss der Compiler früher aufstehen, nächster Anlauf:, , Na bitte, es läuft, aber schön sieht es nicht aus. Vor allem wenn man sehr viele dieser Verkettungen im Code verwendet, ist das Statement sehr zerstückelt. Außerdem muss ich mich immer selbst um die Typkonvertierung kümmern., Wenn man Python-Literatur durchblättert, auch so manche aktuelle, und man diverse Foreneinträge in Stack Overflow oder sonst wo liest, sieht man Techniken, die in unserem Beispiel so aussehen:, , Funktioniert prima. ""%d"" ist der Platzhalter für eine Zahl (digit) und ""%s"" der Platzhalter für einen String., Sollte man aber mit dieser Schreibweise einen Wert mehrfach gebrauchen müssen, wird es wieder etwas unübersichtlich. Stellen wir uns ein Beispiel vor, in dem wir in einer Abfrage diverse Bedingungen prüfen., , , Sobald wir hier weitere Platzhalter einfügen, wächst die Fehlergefahr und wir müssen jedes Mal die Positionen im Code abzählen. Besser ist es hier, benannte Platzhalter zu wählen., , , Wunderbar, der Code ist jetzt viel lesbarer, da wir sofort sehen, was wir wo einfügen., Diese Schreibweise hat nur ein kleines Problem, und zwar gilt sie als veraltet und wurde zumindest in Python 3 durch eine bessere abgelöst. Vor etwa 3-4 Jahren hab ich mehrfach in Foren gelesen, dass diese Schreibweise sogar als deprecated galt. Das heißt, sie sollte nicht mehr verwendet werden, da ihr Fortbestand in späteren Python-Versionen nicht garantiert ist. Davon ist man wohl heutzutage abgekommen, vermutlich wegen der noch sehr hohen Verbreitung in Modulen., Die neue offizielle Schreibweise verwendet geschweifte Klammern. Sie sieht nicht nur anders aus, sondern birgt auch viel mehr Potential in Bezug auf Formatierungsmöglichkeiten. Wenn man sich ohnehin die neue Schreibweise aneignet, da sie mehr kann, warum sie dann nicht durchgängig einsetzen?, Zunächst die einfache Variante:, , Man beachte, dass man hier, wenn die Darstellung in Bezug auf Kommastellen oder führende Nullen usw. keine Rolle spielt, keine Unterscheidung zwischen Strings oder numerischen Werten für die Platzhalter treffen muss. Ja man kann sogar z.B. Tupel direkt verwenden:, , Die Format-Funktion des Strings ruft die __str__-Methode jedes Objekts auf. Das entspricht dann jeweils str(object)., Auch hier gibt es die Schreibweise mit Labels, allerdings übergibt man kein dictionary, sondern schreibt die Zuordnung in Form von Funktionsparametern., , Alle, die etwas schreibfaul sind, können auch Tuple Packing und Tuple Unpacking für sich nutzen., , , row ist hier in jedem Durchlauf der Schleife ein Tupel, weil enumerate() zwei Werte als Tupel zurückgibt. Diese werden dann in die Variable row gezwungen. Mit der Schreibweise "".format(*row)"" entpackt man das Tupel wieder und ruft die Werte in der entsprechenden Reihenfolge ab., Dasselbe klappt auch mit Dictionaries., , , Das extremste Beispiel für Tippfaulheit wäre dieses theoretische Beispiel einer Insert-Funktion., , , Hier holt sich der Format-Befehl die Daten einfach aus den im Namespace des Funktionsaufrufs definierten Variablen, in diesem Fall die Funktionsparameter. Dieses Beispiel habe ich selbst in Python 2.7 verwendet. In Python 3.x geht das wohl nicht mehr - ich denke, das ist auch besser so., Jetzt stellen wir uns vor, wir bekommen einen neuen Mitarbeiter, den fünfzigsten, namens O?Reilly. Der wird geschwind eingefügt:, , , SQLite meckert hier ""sqlite3.OperationalError: near ""Reilly"": syntax error"". Woran liegt es? Ganz offensichtlich wird der Name zwar in das SQL-Statement eingefügt, aber das Hochkomma im Namen wird als Ende des Namensstrings interpretiert, also "" , 'O'Reilly') ""., Hier kann man nun im Codingrausch Abhilfe schaffen, indem man das Hochkomma escapet. Das variiert je nach Datenbank, in SQLite muss man die Hochkommata doppeln., , Puh, das funktioniert erst mal, aber ist letztlich nur ein billiger Workaround., So, jetzt bekommen wir einen neuen Mitarbeiter, den einundfünfzigsten, nämlich Herrn ""');DROP TABLE staff;"". Komischer Nachname, aber wenn das vom User so eingegeben wurde, dann wird das schon seine Richtigkeit haben., Wenn wir nicht aus dem Anlass mit Herrn O?Reilly diverse Character escapen würden und wenn wir nicht mit SQLite arbeiten würden, was hier keine 2 Statements in einem execute-String zulässt, würde der Abfragestring wie folgt aussehen., , , Hier wird durch ungesäuberten User-Input die ganze Tabelle gelöscht. Diese SQL-Injections sind reale Gefahren für die Sicherheit der Datenbank. Vor wenigen Jahren noch bin ich beim Rumprobieren selbst auf größere Online-Händler gestoßen, die in der Produktsuche nicht mal das Hochkomma säuberten., Alle drei aufgezeigten Alternativen funktionieren. Ich selbst verwende wegen der Schnelligkeit noch hier und da die eine oder andere Schreibweise davon., Ganz klare Best Practice ist jedoch die Verwendung von ""Parameterized Queries"". Unabhängig von der Programmiersprache sollte jeder Datenbank-Connector diese Art der Abfragenübermittlung unterstützen. In Python weiß ich zumindest aus der Praxis, dass es für Oracle, MySql, SQLite und PostgreSQL funktioniert., Die Idee dahinter ist, dass man den SQL-String nicht komplett selbst zusammenstellt und dann an den Connector übergibt, sondern dass man ein Template und die Parameter für das Template übergibt., Es gibt verschiedene Vorteile. Zum einen übernimmt der Datenbanktreiber jegliche Typenkonvertierungen und Spezialbehandlungen von Sonderzeichen wie Hochkommata. Man muss sich also nicht darum kümmern, wie die jeweiligen Konventionen der Datenbank sind., Zum anderen bringt es auf manchen Datenbankplattformen Performancevorteile, wenn man denselben Abfragerumpf sehr häufig mit unterschiedlichen Parametern ausführt. Der SQL-Parser muss dann die Abfrage nicht jedes Mal neu parsen, um die Ausführung zu planen, sondern greift auf frühere Ausführungen zurück und wechselt nur die Werte am Platzhalter aus., Leider ist die Art und Weise der Parametrisierung über verschiedene Datenbankplattformen hinweg alles andere als einheitlich. Ein Python-Datenbanktreiber besitzt zumindest ein Attribut names paramstyle, das angibt, welche Technik zu verwenden ist., , , Im SQLite-Beispiel prüfen wir erst, welcher paramstyle wichtig ist. Es ist 'qmark', also question marks/Fragezeichen. Das insert-Statement zeigt die Verwendung. Es werden für jede Position Fragezeichen eingefügt. In der richtigen Reihenfolge müssen dann als zweiter Parameter im Funktionsaufruf die Werte als Tupel übergeben werden. Typenkonvertierungen laufen automatisch, auch bei Datumsobjekten., In PostgreSQL z.B. gibt es ein anderes Format, das auch benannte Parameter zulässt., Eigentlich ist der Titel ""Best Practice"" irreführend. Es sollte eher ""Only Practice"" heißen. Parametrisierte Abfragen scheinen auf den ersten Blick umständlich zu sein. Vor allem, wenn man das SQL-Statement dynamisch über Stringverkettungen zusammenbaut, ist leicht der Überblick über Parameter zu verlieren, besonders wenn, wie in SQLite, die Platzhalter nur mit Fragezeichen gekennzeichnet sind. Dass man sich aber keine Sorgen über die Typenkonvertierung und Sonderzeichen machen muss, ist dafür schon mal ein großer Benefit., Normalerweise müsste ich jetzt sagen, dass der Sicherheitsaspekt, nämlich die Vermeidung von SQL-Injections, der wichtigste und allein ausreichende Grund sein sollte, nur die Best Practice zu verwenden., Im täglichen Data-Science-Umfeld, in dem viele Sicherheitsvorkehrungen getroffen wurden, bis man Zugang zu den Daten erhält, und wo die Anwendung nicht notwendigerweise durch potentiell bösartige User bedient werden kann, ist der Sicherheitsaspekt eher eine Kür. Nur sehr selten hatte ich über Jahre im Arbeitsumfeld die Situation, in der ein potentiell gefährlicher Python-Code anderen außerhalb des direkten Projektes zugänglich war. Und in diesen Fällen hätte eh jeder selbst direkten Zugriff auf die Datenbank gehabt., Wenn man Python zur SQL-Generierung voll ausreizen will und richtig mächtige dynamische SQL-Abfragen erstellt, muss man irgendwann zwangsläufig wieder der Best Practice untreu werden. Man kann nämlich nur Werte-Inputs parametrisieren. Tabellennamen und Spaltennamen können weiterhin nur durch eine der anderen oben beschriebenen Techniken dynamisch eingesetzt werden., Effektiv, muss ich gestehen, setze ich deswegen selbst immer einen Mix aus parametrisierten Abfragen und String-Templates mit {} ein. Man muss dann eben stets selbst dafür sorgen, dass nur autorisierte Nutzer Zugriff auf das Skript und die Daten erhalten. Ein erster Schritt ist es schon, dass jeder, der ein Python-Datenbank-Skript bedient, für die Verbindung einen eigenen und keinen generischen User benötigt., , © 2021 b.telligent";https://www.btelligent.com/blog/best-practice-fuer-sql-statements-in-python/;B-Telligent;Stefan Seltmann
09.05.2019;            Das Advanced Data Store Object (ADSO) und seine Tabellen        ;"Mit SAP BW on HANA kommt das ADSO mit neuen Tabellenstrukturen und Funktionen. Im Vergleich zu den InfoProvidern, die auf nicht auf HANA basierenden SAP BW-Systemen genutzt werden, besitzen ADSOs die Fähigkeit, ihre Funktion ohne Verlust der abgelegten Daten zu ändern. Dies schließt auch eine Änderung der Inhalte von Tabellen mit ein, wenn der Typ verändert wird., Ein ADSO besteht dabei immer aus drei Tabellen, die je nach ADSO-Typ gefüllt und verarbeitet werden. Nicht genutzte Tabellen werden vom System trotzdem angelegt. Somit ist die Nutzung in Routinen, HANA-Experten-Skripten etc. möglich, aber generell nicht immer richtig., Das folgende Diagramm zeigt die Tabellen eines ADSO mit dem technischen Namen ""SAPBLOG""., , , Um eine stabile Implementierung bei einer Änderung des Objekttyps zu garantieren, werden bei der Aktivierung des ADSO zwei Views mit den Namen /BIC/ASAPBLOG6 (Extraktion) und /BIC/ASAPBLOG7 (Reporting) angelegt. Diese zeigen auf die Tabelle mit den jeweils aktiven Daten und/oder auf die Extrakt-Tabelle. Die Views werden dabei automatisch mit einer Änderung des Typs eines ADSO aktualisiert und ermöglichen so auch bei Anpassungen des Modells einen stabilen Ladeprozess ohne Seiteneffekte., Die folgende Tabelle zeigt die unterschiedlichen Einstellungen eines ADSO, die den Aufbau der Views beeinflussen. Kommt ein View in beiden Spalten der folgenden Tabelle vor, enthält dieser auch beide SAP BW-Tabellen über eine UNION-Funktion., ,  Damit ergibt sichdie generelle Regel, dass, egal welcher ADSO-Typ genutzt wird, ein Lookup im ABAP oder SQL immer auf den 7er View erfolgen sollte, damit die Implementierung nach einer Änderung des ADSO-Typs reibungslos funktioniert. Da stets alle Tabellen angelegt werden, fällt ein Fehlverhalten oftmals nicht sofort auf, falls die Tabelle vom SAP BW nicht mehr genutzt wird., Wer sich die Views anschauen möchte, sollte dies am besten via SAP HANA Studio erledigen, da das ABAP DDIC nicht sofort aktualisiert wird. Deshalb sind etwaige Anpassungen dort nicht direkt sichtbar., Das Beispiel zeigt den Reporting View eines ADSO mit allen Info-Objekten als Schlüssel (vormals InfoCube genannt). , , © 2021 b.telligent";https://www.btelligent.com/blog/das-advanced-data-store-object-adso-und-seine-tabellen/;B-Telligent;Markus Sontheimer
18.04.2019;            Was ist eigentlich SAP BW/4 HANA und wie unterscheidet es sich von SAP BW on HANA 7.5        ;SAP BW/4 HANA ist nun seit genau einem Jahr für Kunden verfügbar. Grund, einmal zurückzublicken und einen Vergleich mit SAP BW on HANA 7.5 anzustreben. Zudem werde ich von vielen Kunden gefragt, was genau der Unterschied zwischen SAP BW/4 HANA und SAP BW on HANA ist. Deshalb möchte ich dies einmal darstellen., SAP BW 7.5 on HANA stellt eine Welt zwischen „altem BW“ und neuem „HANA-optimierten BW“ dar. Somit integriert sich SAP BW 7.5 on HANA in beide Welten (wie die Unterstützung von alternativen Datenbanken, InfoCubes etc. und auch neuen ADSO-Objekten und der Eclipse-Umgebung) und stellt die beim Code-Split verwendete Grundversion dar. Erkennbar ist die gemeinsame Basis sehr stark an der immer tieferen Nutzung der BW Modeling Tools mit steigender Support-Package-Nummer. So können z. B. auch mit SAP BW 7.5 on HANA SP4 nur noch InfoObjekte über Eclipse verwaltet werden. Ein Großteil des Codes wird also in beiden Major Releases verwendet. Dies spiegelt sich auch in Hinweisen wider, die sehr häufig für SAP BW 7.5 (und auch früher) und SAP BW/4 HANA relevant sind, z. B. beim Analytic Manager oder der MDX-Schnittstelle (siehe z. B. Hinweis 2477412). Aber auch komplett neue Funktionen wie die Erweiterung der externen HANA-Views für den nativen SQL-Zugriff auf BW-Objekte sind in SAP BW 7.5 on HANA vorhanden und werden stetig erweitert., Somit ist klar: SAP BW/4 HANA ist keine komplette Neuentwicklung und es besteht aktuell kein Zwang zu einem Releasewechsel von SAP BW 7.5 zu SAP BW/4 HANA. Jedoch spricht auch nichts dagegen, da SAP BW/4 HANA auf einer stabilen Softwarebasis steht. Grundvoraussetzung ist dabei die reine Nutzung von HANA-optimierten Objekten. Unterstützt werden Siedabei durch das „BW/4 HANA Starter Add-on“, das dabei hilft, Legacy-Objekte wie InfoCubes zu konvertieren und auch die BW/4-HANA-Kompatibilität bei Weiterentwicklungen sicherzustellen. So existieren für SAP BW 7.5 verschiedene Kompatibilitätsmodi:, BW Mode, Normales SAP BW Release wie auch schon SAP BW 7.3 on HANA, SAP BW 7.4 on HANA, jedoch mit stärkerer Optimierung auf SAP HANA als Datenbank-Backend., Compatibility Mode, SAP BW 7.5 (nur mit SAP HANA) mit SAP BW/4 HANA Add-on. In diesem Zustand können Legacy-Objekte wie InfoCubes weiterhin existieren, diese können jedoch nicht erstellt oder verändert werden (außer sie werden einer expliziten Whitelist hinzugefügt)., B4H Mode, Das System befindet sich im SAP-BW/4 HANA Modus. Es können nur HANA-optimierte Funktionen genutzt werden. Der Funktionsumfang entspricht einem SAP BW/4 HANA System und ermöglicht so ein einfaches Upgrade auf SAP BW/4 HANA., , Die Modi können im Report RS_B4HANA_CHECK_ENABLE gesetzt werden., , Für die Zukunft wird es sicher spannend, wann wichtige neue Features nur in SAP BW/4 HANA Einzug finden und somit einen echten Mehrwert für existierende Umgebungen bieten. Bis dahin besteht aber kein existenzieller Grund, von SAP BW 7.5 on HANA zu SAP BW/4 HANA zu wechseln. Wichtig dabei ist, dass bei neuen Installationen natürlich gleich SAP BW/4 HANA verwendet werden sollte. Wie oben erwähnt besteht kein Grund, ein älteres Release einzuführen aufgrund der ähnlichen Codebasis., Interessante weiterführende Informationen finden Sie hier:, SAP BW/4 HANA FAQ: https://assets.cdn.sap.com/sapcom/docs/2016/08/c4458a08-877c-0010-82c7-eda71af511fa.pdf, SAP BW/4 HANA Roadmap: https://www.sap.com/documents/2016/08/740d6709-877c-0010-82c7-eda71af511fa.html, , © 2021 b.telligent;https://www.btelligent.com/blog/sap-bw4-hana-und-der-unterschied-zu-sap-bw-on-hana-75/;B-Telligent;Markus Sontheimer
11.04.2019;            HOWTO: Einfaches Web Scraping mit Python        ;"Vor zwei Wochen wurde ich von einem oft genutzten Online-Versand, dessen Namen an ein Fluss in Südamerika erinnert, per freundlicher Info-Mail auf eine Aktion aufmerksam gemacht. Und zwar wurden mir drei Musik-CDs aus einer großen Auswahl für 15€ angeboten., Ich erwerbe immer noch gerne, wie früher, Musik auf physischen Tonträgern und wollte mir das Angebot genauer ansehen. Nun stellte sich heraus, dass etwa 9,000 CDs offeriert wurden, und das über etwa 400 Seiten im Online-Shop. Dieser Shop bietet mir die Möglichkeit, das Angebot nach Beliebtheit oder nach Kundenbewertung zu sortieren. Wenn ich jedoch die Beliebtheit absteigend betrachte, finde ich viele Titel, die nicht mehr ganz meiner Altersklasse entsprechen. Andererseits, wenn ich nach Kundenbewertung sortiere, stellt sich heraus, dass der Shop die Bewertungen ungewichtet verarbeitet. D.h. irgendeine CD mit volkstümlichen Schlagern wird mit nur einer 5-Sterne Bewertung vor einer anderen CD mit 4.9 Sternen auf 1000 Bewertungen aufgeführt., Ich hatte zunächst keine Lust mir alle 400 Seiten händisch durchzusehen, ob was von Interesse für mich dabei ist. Daher griff ich auf einen Trick zurück, den ich früher schon häufiger eingesetzt hab, und zwar den Content der Website automatisch abzuernten. Dieses Vorgehen ist alles andere als neu, jetzt aber hat das Kind im Rahmen der Data Science Community einen neuen Namen und zwar web scraping., Es ist nicht unwahrscheinlich, dass man als Data Scientist selbst einmal Daten aus dem Netz saugen muss. Darum will ich anhand meines einfachen Problem zeigen, wie gering die Einstiegshürde hierfür mit Python sein kann., Zum Glück verwendet der Online-Anbieter eine Request-Methode, welche die Parameter des Requests in der URL im Klartext darlegt. Hier ein Beispiel, das ich etwas anonymisiert habe:, Man erkennt, dass die Seite an zwei Stellen referenziert wird, einmal bei _pg_1und bei &amp;page=1&amp;. Wenn ich also diese beiden Stellen anpasse, kann ich direkt durch alle Unterseiten iterieren., Um die Website nun tatsächlich zu lesen, verwenden wir ein Modul, dass in Python fest integriert ist, nämlich urllib. Das Laden der Seite sieht dann so aus (Die URL ist von mir wieder verstümmelt.):, Man verwendet das Request-Objekt, um eine Website zu öffnen. Der HTML-Code wird ausgelesen, wenn der Code des Request 200 ist, der übliche Code für das erfolgreiche Aufrufen einer Website. Wir kennen alle den 404 wenn die Seite z.B. nicht gefunden wird. Die String-Funktion um den Leseschritt ist notwendig, um auch wirklich strings und nicht bytestrings zu erhalten. Das würde uns sonst später Probleme bereiten., Jetzt wo wir den HTML-Code der Seite haben, können wir ihn nach CD-Titel und den Interpreten durchsuchen. Ich habe zunächst vorher mir direkt den HTML-Code über den Browser angesehen, um verdächtige Muster zu entdecken, nach denen man suchen kann. Dabei ist mir aufgefallen, dass alle Titel mit &lt;h2 class=""a-size-medium a-color-null s-inline s-access-title a-text-normal""&gt; beginnen und schließlich mit Audio CD aufhören. Dazwischen befindet sich noch eine Menge für uns unwichter Code., Wir schneiden aus der Seite nun die jeweiligen Code-Stellen mir Regulären Ausdrücken aus und speichern uns die Angaben in eine Liste Weg., re.findall gehört zum Package für Reguläre Ausdrücke. Damit Suche ich alle Stellen im HTML-Code, welche dem obigen Muster entsprechen, wobei .*? der Platzhalter für eine beliebige Kette ist. Dieser Platzhalter ist hier ""not greedy"", das heißt er versucht den unbekannten Bereich auf so wenig wie möglich Zeichen zu matchen, sonst würde man mehrere Alben gleichzeitig erfassen., Für jedes der gemachten Code-Schnippsel wiederhole ich den Prozess mit dem Suchschema '&gt;.*?&lt;', das mir nun ""not greedy"" die einzelnen Inhalte der Tags zurückgibt. Durch ""kritisches Betrachten des Codes"" kann ich feststellen, dass der Albumtitel im ersten Tag (Index 0) und der Künstler im 14ten Tag (Index 13) gespeichert ist. Um nun die Klammern der Tags noch loszuwerden, indizieren wir die Ergebnisse auf das zweite bis vorletzte Zeichen (1:-1), Das Ergebnis wird als Tupel an meine Ergebnisliste page_content angehängt., Am Schluss kommt der einfache Part. Wir öffnen einen ""file handle"" und schreiben jedes Album-Artist-Tuple mit Tabulator getrennt in das File. Ich verwende hier das print-Literal aus Python3, welches für jeden Print-Befehl von selbst einen Zeilenumbruch einfügt und welches den file handle als Ziel-Parameter für die Ausgabe akzeptiert., Den Part mit dem Auslesen der Website muss man natürlich in einer Schleife abwickeln und dabei die Seitenzahl parametrisieren. Hier wäre das Gesamtscript:, Zack fertig!, So, meine Datei offers.txt kann ich nun in Excel z.B. öffnen und mir mit einer Pivot-Tabelle ansehen. Das erleichtert mir nun enorm, überhaupt erstmal die am Angebot beteiligten Interpreten zu überblicken., Ich denke, ich habe mit unter eine halben Stunde etwa solange für das Script gebraucht, wie ich für das Durchklicken der Seiten benötigt hätte, dafür hatte ich mehr Spass. Und da ich das Script in Wahrheit in Form einer Klasse mit den Teilschritten als Methoden geschrieben habe, werde ich es bei ähnlichen Szenarien wahrscheinlich wieder hernehmen können., , © 2021 b.telligent";https://www.btelligent.com/blog/howto-einfaches-web-scraping-mit-python/;B-Telligent;Stefan Seltmann
04.04.2019;            Der CompositeProvider und eine „versteckte“ Funktion        ;"Mit der Migration auf SAP BW/4 HANA oder der Aktivierung des BW/4-HANA-Modus, der hierbeschrieben ist, können Legacy-Objekte nicht mehr genutzt werden. Darunter fallen auch die beliebten MultiProvider. Diese werden genutzt, um verschiedene InfoProvider zu kombinieren und eine Abstraktionsschicht zu einer BW-Query herzustellen. Dabei können Objekte der InfoProvider verbunden werden, auch wenn diese in einem der Provider nur als Navigationsattribut vorliegen, also nicht direkt in der Faktentabelle des InfoProviders. In diesem Beispiel wird das Land einmal direkt und einmal als Navigationsattribut vom Kunden zugeordnet., , Wie realisiert man dies bei CompositeProvidern? Bei der ersten Nutzung von CompositeProvidern in Projekten fällt erstmal auf, dass die grundsätzliche Funktionalität der des MultiProviders entspricht und der Funktionsumfang sogar umfangreicher ist. Ein CompositeProvider kann mit der Möglichkeit, Objekte durch einen „Join“ zusammenzuführen, mehr als der MultiProvider, der nur einen „Union“ beherrscht. Daher wird der CompositeProvider auch als Nachfolger von InfoSets und MultiProvidern gehandelt. Aber zurück zur ersten Nutzung des CompositeProviders: Vorerst fällt nichts Besonderes auf. Die Funktionen sind alle vorhanden und statt des MultiProviders können nun CompositeProvider genutzt werden. Auch die Aktivierung von Navigationsattributen ist im Reiter „Output“ problemlos möglich., , Wenn es dann im Laufe der Implementierung dazu kommt, dass Objekte verknüpft werden sollen, die in nur einem InfoProvider als direkt verfügbares Objekt vorliegen, stoßen einem die Modeling Tools gehörig vor den Kopf. Es ist nicht möglich, im Reiter „Scenario“ im Target-Bereich des CompositeProviders Navigationsattribute für eine Zuordnung hinzuzufügen., , Hat die SAP also diese beliebte Funktion des MultiProviders vergessen oder absichtlich entfernt? Nein, ich kann Sie dort beruhigen. Sie ist nur „versteckt“, wie schon im Titel dieses Artikels angekündigt, und zudem hat sich die Logik der Zuordnung geändert. Ein CompositeProvider kann weiterhin keine Navigationsattribute für ein Mapping bereitstellen, jedoch kann man den Zielobjekten Navigationsattribute der InfoProvider zuordnen. Dafür müssen diese in der Ansicht mit einem Rechtsklick auf den InfoProvider angezeigt werden., , Sobald diese Darstellung aktiviert ist, erscheinen alle Navigationsattribute unter den jeweiligen Objekten beim InfoProvider und können mit den Zielobjekten des CompositeProviders verbunden werden., , Anfangs habe ich mir die Frage gestellt, warum die SAP diese tolle Funktion wohl versteckt hat. Ein Grund könnte sicher in der Performance liegen. Durch den zusätzlichen Join bei der Nutzung von Navigationsattributen (Hinweis: Man kann im Output auf das Objekt im CompositeProvider wieder ein Navigationsattribut aktivieren, hat somit mehrfache Joins der Objekte – InfoObjekt im InfoProvider -&gt; Navigationsobjekt im InfoProvider -&gt; Navigationsobjekt im CompositeProvider) kann es bei größeren Datenmengen schnell zu einer Verschlechterung der Performance kommen, wenn die InfoObjekte eine hohe Granularität besitzen, also viele verschiedene Einträge., , © 2021 b.telligent";https://www.btelligent.com/blog/der-compositeprovider-und-eine-versteckte-funktion/;B-Telligent;Markus Sontheimer
04.03.2019;            Customer Data Platforms – ein Überblick über die Kategorien        ;, © 2021 b.telligent;https://www.btelligent.com/blog/customer-data-platforms-ueberblick-ueber-die-kategorien/;B-Telligent;Alexander Nitsche
12.02.2019;            Customer Data Platforms - Abgrenzung zu Kampagnenmanagementsystemen        ;"Deutsche Unternehmen müssen einen großen Schritt in Richtung digitale Transformation und Verbesserung der Customer Experience wagen, diese Einsicht reift bei den Entscheidern zunehmend. Die Gründe hierfür sind vielfältig:, Da erscheinen CDP-Systeme, die es in den USA bereits seit 2013 gibt, wie gerufen. Während Kampagnenmanagementsysteme eher workflowbasiert oftmals in relationalen Datenmodellen CRM-basierte Kampagnenprozesse automatisieren, kümmern sich CDP-Systeme ganzheitlich um die Optimierung der Kundeninteraktion auf allen digitalen Kanälen wie Web, E-Mail, Mobile, Onlineshop oder Social – von der kundenzentrischen Datenhaltung in Big-Data-Technologien bis hin zum E-Mail-Versand in Realtime., Durch den Einsatz von Big-Data-Technologien ist eine Customer Data Platform datenmodellseitig sehr flexibel und äußerst schnell, und jeder Marketer ist in der Lage, seine CDP aufzusetzen, zu benutzen sowie weiterzuentwickeln – unabhängig von BI-/IT-Ressourcen., , Die drei Phasen Data, Decisions und Delivery bilden den Daten- und Prozessfluss innerhalb einer CDP ab:, , In der ersten Phase, der Datenaufnahme, werden die Daten aus den verschiedensten Datenquellen in die CDP geladen. Bevor die Kundenidentitäten allerdings erstellt bzw. verknüpft werden können, müssen diese standardisiert und transformiert werden. Durch die kundenzentrische Verdichtung der Daten können dann weitere Datenlogiken erstellt werden. Nach der Weitergabe in die Decision-Phase werden notwendige Analysen und Selektionen für die Zielgruppenbestimmung durchgeführt, um im Anschluss die workflowbasierten Schritte zur Kampagne definieren zu können. Die Anbindung der Kanäle erfolgt dann in der letzten Phase „Delivery“., , Die wesentlichen Unterschiede zu einem Kampagnenmanagementsystem liegen zum einen im Bereich „Data“, in dem man sich selbst die kundenzentrische Sicht aufbauen kann, und zum anderen in dem Realtime-Funktionsbereich, der einer DMP stark ähnelt. Des Weiteren haben Customer Data Platforms oftmals nicht die Funktionsvielfalt, wie User sie aus der Kampagnenmanagement-Welt kennen., Die im Nachfolgenden aufgeführte Auflistung gibt einen groben Überblick über die Funktionen einer CDP wieder:, , Mit den immer neuen Buzzwords wie CRM, KMS, CDP, DWH, Data Mart, DMP, CMP &amp; Co. wird die CRM-Welt immer unübersichtlicher. Es gilt, den Überblick zu bewahren: Zu welchem Einsatzzweck bzw. in welcher Customer-Lifecycle-Phase benötige ich welche Lösung?, Einen groben Überblick über die verschiedenen Lösungen im Marketing-Automation-Umfeld gibt folgendes Schaubild wieder (aus Customer-Lifecycle-Sicht):, , , Um den jeweiligen Abdeckungsgrad zu identifizieren, hilft ein perspektivisch anderer Blick auf die drei Phasen der CDP-Architektur (Data, Decisions, Delivery):, , Auf den ersten Blick erscheinen Customer Data Platforms wie eine moderne, dynamische, schlanke und frischere Variante eines „klassischen“ Kampagnenmanagementsystems. Doch als Unternehmen muss ich mir, bevor ich mich für eine Lösung entscheide, immer die Frage stellen: Werden alle meine Anforderungen und Wünsche, die von meiner aktuellen oder geplanten Kampagnenmanagementlösung abgedeckt werden, auch von einer CDP abgedeckt? Die folgenden Gegenüberstellungen können bei der Entscheidungsfindung helfen:, , Wann der Einsatz welcher Lösung sinnvoll ist und wann es sich lohnt, auf die jeweils andere Lösung umzusteigen, ergibt sich aus der Beantwortung der oben gestellten Fragen und hängt von den Wünschen, Anforderungen und Zielen jeder Organisation ab. Der Anforderungskatalog, die zukünftige Architektur und die dazugehörigen Use Cases sollten daher individuell mit der in Frage kommenden Kampagnenmanagement- bzw. Customer-Data-Platform-Lösung abgeglichen werden, um eine zukunftssichere Entscheidung treffen zu können., In den nächsten Blogbeiträgen erwarten Sie eine Anbieterübersicht über die verschiedenen CDP-Lösungen sowie Tipps &amp; Tricks zur Toolevaluierung!, , © 2021 b.telligent";https://www.btelligent.com/blog/cdp-die-neue-wunderwaffe-im-marketing-teil-1/;B-Telligent;Laurentius Malter
07.02.2019;            BCBS 239 - Chance für effiziente Business Intelligence im Bankenumfeld        ;"Die Europäische Zentralbank (EZB) sowie viele andere Aufsichtsbehörden erkannten in der Analyse der Ursachen der Finanzkrise 2008 unter anderem große Defizite in der IT und Datenarchitektur der Institute. Diese Defizite führten dazu, dass Risikobewertungen nicht schnell und präzise genug und nicht vollständig durchgeführt werden konnten. Kurz gesprochen: Risiken konnten weder im angemessenen Maße berichtet noch kontrolliert werden. Aus dieser Lehre heraus wurde von dem Baseler Ausschuss für Bankenaufsicht (engl. Basel Committee on Banking Supervision, kurz BCBS) der vielseits durch die Presse bekannte Standard 239 veröffentlicht, auch bekannt als „Principles for effective risk data aggregation and risk reporting“ bzw. als AT 4.3.4 der MaRisk., BCBS 239 ist eine regulatorische Vorgabe für Finanzinstitutionen. Die Umsetzung ist dabei gestaffelt nach der Systemrelevanz der Institutionen. Es werden hierbei generell drei verschiedene Abstufungen vorgesehen:, Die Umsetzung von BCBS 239 war für die G-SIBs und D-SIBs, wie auf nachfolgendem Zeitstrahl ersichtlich, bis spätestens April 2018 vorzusehen. Für alle sonstigen Institutionen ist die Umsetzung nicht verpflichtend, wird jedoch von den Aufsichtsbehörden als wichtig und dringend empfehlenswert angesehen., Auch wenn BCBS 239 nur für Finanzinstitutionen eine zwingende Vorgabe ist, so sind die Transparenz, die gesicherte Qualität der Daten und die Verfügbarkeit sowie die hierfür notwendige Infrastruktur und Organisation zur heutigen Zeit, in der „die Daten das Gold der Zukunft sind“, auch ein klarer Mehrwert für Institutionen quer durch alle Branchen. Es ist somit empfehlenswert, die Grundwerte von BCBS 239 zumindest in Teilen in den BI-Strategien anderer Unternehmen mit zu berücksichtigen., BCBS 239 beschreibt notwendige Schritte zum Wandel in der Handhabung von Daten im Bankenumfeld. Es wurden hierbei die folgenden vier grundsätzlichen und eng miteinander verbundenen Themenbereiche erkannt:, Die Etablierung einer BCBS-239-konformen Data-Governance-Organisation bzw. IT-Architektur und damit die Entwicklung einer konzern- sowie geschäftsübergreifenden Definition im Hinblick auf die Erfassung und Verarbeitung von risikorelevanten Daten ist ein wichtiger Baustein auf dem Weg zu einem BCBS-239-konformen Unternehmen. Darüber hinaus besteht auch Handlungsbedarf in den Bereichen Risk-Daten-Aggregation und Risk-Reporting, denn die Anforderungen an die Beschaffenheit und Konsistenz von Risikodaten und Risikoreports sind hoch., Wichtig vor allem in Bezug auf international agierende Institutionen ist, dass alle beschriebenen Handlungsfelder nicht je Land isoliert betrachtet werden. Vielmehr werden durch die Aufsichtsbehörden die lokalen Umsetzungen sowie das Zusammenspiel zwischen den Ländern bzw. der Zentrale geprüft. Unstimmigkeiten durch ein isoliertes Vorgehen sind dringend zu vermeiden!, Erhalten Sie nachstehend (einfach die Überschrift aufklappen) eine vollständige Liste bzw. Übersicht mit sämtlichen Handlungsfeldern, die innerhalb des Wandels zu einer BCBS-239-konformen Institution zu berücksichtigen sind, und erfahren Sie, wie die Handlungsempfehlungen konkret aussehen!, Das Themengebiet „regulatorisch“ betrifft Empfehlungen bzw. Vorgaben für die Aufsichtsbehörden und wird deswegen hier nicht weiter betrachtet., Governance &amp; Prozesse, IT-Infrastruktur &amp; Organisation, Datenmanagement, Risk-Reporting, BCBS 239 ist keine einmalige Anstrengung, die, wenn einmal implementiert, fertig und abgeschlossen ist. Vielmehr ist es ein Kreislauf, der bei der Erstrealisierung anfängt und in einer sich stetig verändernden Welt regelmäßig geprüft und angepasst werden muss., Aus der langjährigen Projekterfahrung von b.telligent hat sich das strukturierte und organisierte Planen von Maßnahmen der folgenden Themenbereiche als effiziente Vorgehensweise entwickelt:, ,  Spezialisten aus allen vier Bereichen sollten dabei stets die Konformität nach den BCBS-239-Prinzipien prüfen, um eine ausgearbeitete Handlungsstrategie sowie konkrete Maßnahmen hinsichtlich der umzusetzenden Prozesse sowie der technischen Realisierung ableiten zu können., Der Start eines jeden BCBS-239-Projektes sollte idealerweise durch sogenannte Quick Checks in den Bereichen Governance &amp; Infrastruktur, Risk-Daten-Aggregation sowie Risk-Reporting (s.o.) erfolgen. Diese decken sowohl die fachlichen als auch die technischen Aspekte ab und sollten im Rahmen einer stetigen Kontrolle in weiteren Zyklen wiederholt stattfinden., Data Governance umfasst die Definition der Hoheit/Verantwortung von Systemen, Daten, Kennzahlen und Business-Prozessen. Die Dokumentation und Einhaltung solcher Rollen/Aufgaben sind die theoretischen und fachlichen Grundlagen der regulatorischen Anforderungen. Notwendige Handlungsfelder in diesem Bereich werden aus den Analysen der Datenstrategie ermittelt., Aus technischer Sicht wird die Unterstützung im DWH und Berichtswesen für die folgenden BCBS-239-Themen benötigt:, Aus den Analysen der Datenstrategie sowie den Definitionen der Data Governance und den aus den gelebten Prozessen ermittelten technischen Handlungsfeldern ergibt sich ein Großteil der Aufgaben für diesen Punkt., Für die Bereiche Datenintegration, Datenbanken sowie Berichtswesen kann auf die Toolexpertise der b.telligent-Partner zurückgegriffen werden. Gerne unterstützt b.telligent als anbieterunabhängige Unternehmensberatung bei einer möglichen Toolevaluierung. Zum Thema Data Governance und Datenqualitätsmessung wird der Einsatz von am Markt üblichen Tools empfohlen., Zum Abschluss eines jeden Entwicklungszyklus sollte eine Prüfung durch eine neutrale Stelle wie z.B. durch die interne Revision erfolgen. Das Ergebnis einer solchen Prüfung kann als Input zum nächsten Entwicklungszyklus dienen., Sie haben weitere Fragen zum Thema BCBS 239 und dazu, wie auch Ihnen der Weg zu einer BCBS-239-konformen Organisation gelingt? Kontaktieren Sie mich gerne!, Principles for effective risk data aggregation and risk reporting, Januar 2013, https://www.bis.org/publ/bcbs239.pdf, Rundschreiben 09/2017 (BA) – Mindestanforderungen an das Risikomanagement – MaRisk, 27.10.2017 https://www.bafin.de/SharedDocs/Veroeffentlichungen/DE/Rundschreiben/2017/rs_1709_marisk_ba.html?nn=8249098, Report on the Thematic Review on effective risk data aggregation and risk reporting, Mai 2018 https://www.bankingsupervision.europa.eu/ecb/pub/pdf/ssm.BCBS_239_report_201805.pdf, , © 2021 b.telligent";https://www.btelligent.com/blog/bcbs-239-chance-fuer-effiziente-bi-im-bankenumfeld/;B-Telligent;Stefan Steinhaus
17.12.2018;            Serverless Data Science auf AWS        ;Serverless stellt als „Function as a Service“ das höchste Abstraktionslevel der XaaS-Familie dar. Im Kern bedeutet das, dass der Benutzer sämtliche Aufgaben, die mit dem Betrieb von Infrastruktur zusammenhängen, an den Cloud-Provider abgibt. Sämtliche Fragen der Bereitstellung, der Wartung oder der Skalierung von Servern treten damit in den Hintergrund., Der größte Vorteil dieses Ansatzes ist, dass der Benutzer ausschließlich für das Aufrufen der Funktionen bezahlt (s.u. für mehr Details zum Preismodell). Falls der entsprechende Code nur sporadisch genutzt wird, entstehen dadurch große Einsparpotenziale., Aus einer Data-Science-Perspektive heraus gibt es mehrere Use Cases, die von einem Serverless-Ansatz profitieren können. Allgemein formuliert, liegt die größte Stärke des Ansatzes im On-Demand-Bereich. Beispielhaft können hier von Dateien getriggerte Pipelines oder von APIs getriggerte Vorhersageanfragen genannt werden. Unten demonstriere ich diesen Ansatz mit einem On-Demand-Beispiel für Social-Media-Analysen zu einem bestimmten Suchbegriff., Sämtliche Cloud-Anbieter haben FaaS in ihr Ökosystem integriert. Um das Beispiel greifbarer zu machen, nutze ich AWS und seine Lambda-Funktion, aber die jeweils beste Wahl hängt stark vom Use Case ab., AWS führte die Lambda-Funktion bereits 2014 ein und erweitert seitdem konsequent den Funktionsumfang. Zu diesem Zeitpunkt unterstützt sie Node.js, Python, Ruby, Go und C#. Lambda-Funktionen bestehen aus zwei Hauptteilen:, Die Kosten für die Benutzung werden von drei Parametern getrieben:, Beispielsweise kostet es nur $2,51 pro Monat, um eine Funktion 1.000-mal täglich aufzurufen, die 512MB Speicher benötigt und ungefähr zehn Sekunden läuft. Die Ausführungszeit ist dabei der größte Kostentreiber. Das ist auch der Grund, warum Lambda-Funktionen besonders für kleine und schnelle Codeteile vorteilhaft sind. Eine höhere Anzahl von Aufrufen stellt kein großes Problem dar, aber eine längere Ausführungszeit kann schnell durchschlagen. Um Beispielrechnungen durchzuführen, hat AWS hier einen Rechner bereitgestellt., Die Liste an AWS-Services, die Lambda-Funktionen starten bzw. ihre Ergebnisse empfangen können, ist lang. Für Data-Science-Anwendungen sehe ich vor allem zwei Haupttrigger. Zum einen Trigger, die mit dem Dateisystem (S3) zusammenhängen. Diese Trigger können ausgelöst werden, wenn beispielsweise eine neue Datei abgespeichert wird. Zum anderen sind die Möglichkeiten in Kombination mit dem AWS API Gateway schier grenzenlos. Dieses Setup findet sich vor allem im Bereich von On-Demand Analytics., Ähnlich flexibel stellen sich die möglichen Empfänger dar. Lambda-Funktionen können beispielsweise ihre Ergebnisse in dafür vorgesehene S3-Buckets schreiben oder sie über das API-Gateway nach außen verfügbar machen. Genauso einfach können Benachrichtigungsdienste wie SNS oder SES angesteuert werden., Wichtig zu betonen ist, dass zudem jede Kombination an Triggern und Empfängern denkbar ist. So einfach die Konfigurationen auch sind, müssen dabei aber immer auch die zusätzlichen Abhängigkeiten und die steigende Komplexität berücksichtigt werden., Nun zu einem konkreten Beispiel dafür, wie Lambda-Funktionen für Data-Science-Zwecke eingesetzt werden können. Ich habe ein Beispiel aus dem Bereich On-Demand Analytics gewählt, aber es gibt weit mehr mögliche Anwendungsfälle für solch einen Ansatz., Mein Beispiel wird vier verschiedene Dinge tun:, , Um solch eine Lambda-Funktion zu implementieren, müssen wir drei Schritte durchführen:, Der dritte Schritt ermöglicht eine Kostenschätzung, die dann zur finalen Bewertung herangezogen werden kann., Der Python-Code für diese Lambda-Funktion sieht so aus:https://gist.github.com/timo-boehm/598e9f7ae5231f889a197af96be48787,  Um das Beispiel einfach zu halten, habe ich die notwendigen Schlüssel für die Twitter-API fest programmiert. Das sollte in einem tatsächlichen Use Case offensichtlich anders gelöst werden. Ich habe außerdem die Anzahl notwendiger Packages auf ein Minimum reduziert. Diese Entscheidung ist vom zweiten Baustein motiviert: Layers., Es gibt verschiedene Wege, Layers auf AWS zur Verfügung zu stellen. Der einfachste Weg ist der Upload einer Zip-Datei, die die notwendige Ordnerstruktur beinhaltet („\python\lib\pythonX.X\site-packages\“). Wichtig dabei: Je kleiner die Zip-Datei, desto performanter die Lambda-Funktionen. Da die Ausführungszeit der größte Kostentreiber ist, lohnt es sich hier also, so sparsam wie möglich zu sein., Nun, da die notwendigen Bausteine platziert sind, können wir das Beispiel evaluieren. Im Schnitt beträgt die Laufzeit ungefähr 2,5 Sekunden. Würde diese Funktion täglich 10.000 Anfragen ausführen, würden die Kosten pro Monat also nur $3 betragen! Die Kosten-Nutzen-Rechnung ist damit unschlagbar., Lambda-Funktionen (und ihre Entsprechungen bei anderen Cloud-Anbietern) taugen nicht für alle Data-Science-Anwendungen. Zudem kann die Leichtigkeit, mit der sie sich einrichten lassen, Fluch und Segen zugleich sein. Der Segen liegt in der Geschwindigkeit, mit der experimentiert werden kann. Der Fluch liegt in dem Potenzial für „Quick Fixes“, die schnell zu Unübersichtlichkeit und Qualitätsproblemen führen können. Jeder Anwender muss deshalb den Einsatz von Lambda-Funktionen bewusst planen. Das gilt besonders für die Operationen, die einer Lambda-Funktion innerhalb der Cloud-Umgebung zugänglich sind., In der Summe überwiegt aber das große Potenzial. Beispielsweise können trainierte Machine-Learning-Modelle als Objekte in S3 hinterlegt werden, und dann kann die kostengünstige Lambda-Funktion mit dem Berechnen der Vorhersagen betraut werden. Kombiniert mit anderen AWS-Services, z.B. Step-Functions, können sie zudem einfach in Datenverarbeitungsprozesse integriert werden, die kleinere Machine-Learning-Bausteine enthalten., Wie viele andere Begriffe und Konzepte ist auch „Serverless“ nicht die Lösung aller Probleme. Trotzdem ist es ein mächtiges Tool, das seinen Platz im Data-Science-Werkzeugkasten sicher finden wird., , © 2021 b.telligent;https://www.btelligent.com/blog/serverless-data-science-auf-aws/;B-Telligent;Dr. Michael Allgöwer
15.11.2018;            VERY Best Practice: Arbeiten in Python mit Pfaden - Teil 2        ;Im letzten Eintrag haben wir in einer Lösung von weniger als zehn Zeilen mit einer rekursiven Funktion die Möglichkeit geschaffen, Ordner zu scannen und die Dateien nach Änderungsdatum und Dateigröße auswertbar zu machen., Aufbauend auf diesem Beispiel möchte ich die Latte nochmal etwas höher legen und noch bessere Alternativen aufzeigen., Alter Wein in neuen Schläuchen?, Die finale Lösung für Pfadverkettung sah im früheren Beispiel so aus:, Das Positive daran ist, dass die Lösung unabhängig vom Betriebssystem funktioniert und Strings nicht direkt mit „+“-Zeichen oder String-Formatierungen kombinieren muss., Es besteht jedoch noch ein Fehlerpotential, nämlich wenn jemand den Verzeichnispfad versehentlich oder aus falscher Überzeugung mit einem abschließenden Pfadtrenner definiert., Dieses Beispiel zeigt zwar funktionierenden Code, der Aufruf des Pfades wird aufgrund des letzten fehlerhaften Trenners jedoch einen Fehler verursachen. Solche Fehler können ständig auftauchen, wenn User die Pfade in Config-Files, weit weg vom Code, pflegen und nicht auf die Konventionen achten., Seit Python 3.4 gibt es jedoch eine bessere Lösung in Form des pathlib-Moduls. Es deckt die datei- und ordnerbezogenen Funktionen des os-Moduls von Python über einen objektorientierten Ansatz ab., Hier zunächst die alte Variante:, , Und hier die neue Alternative:, Beides liefert hier genau dieselben Ergebnisse. Warum ist die zweite Variante so viel besser?, Objektorientiert und fehlertoleranter, Zunächst einmal sind die Aufrufe objektorientiert, was Geschmackssache sein kann, aber mir persönlich sehr viel besser gefällt. Es gibt hier ein Objekt wie die Pfaddefinition, und die hat Eigenschaften und Methoden., Spannender ist aber ein hier angewendetes Beispiel für das Überladen von Operatoren:, Die Division von zwei Pfaden sieht hier zunächst wie ungültiger Code aus. Tatsächlich wurde lediglich im Path-Objekt der Divisionsoperator so überladen, dass er wie eine Pfadverkettung funktioniert., Neben diesem Syntactic Sugar werden über Path-Objekte noch andere typische Fehler abgefangen:,  Diese Variante ist also nicht nur schöner, sondern auch robuster gegenüber Falscheingaben. Neben anderen Vorteilen ist der Code auch völlig unabhängig vom Betriebssystem. Man definiert zwar nur ein generisches Path-Objekt, auf einem Windows-System manifestiert sich dieses aber als „WindowsPath“ und auf einem Linux-System als „PosixPath“., Die meisten Funktionen, die sonst einen String als Pfad erwarten, kommen auch direkt mit einem „Path“ klar. In den seltenen Ausnahmen kann man einfach mit „str(Path)“ das Objekt wieder auflösen., In der Lösung des letzten Blogs verwendete ich os.listdir, os.path.isdir und eine rekursive Funktion, um durch den Pfadbaum zu iterieren und zwischen Ordnern und Dateien zu unterscheiden., Eine schönere Lösung bietet os.walk. Die Methode erzeugt keine Liste, sondern erstmal einen Iterator, den man Zeile für Zeile abrufen kann. Die Ergebnisse beinhalten dann jeweils den Ordnerpfad und in einer Liste alle Dateinamen unter diesem Pfad. Das Ganze passiert von sich aus rekursiv, so dass man mit einem Aufruf alle Daten erhält., Wenn man beide eben vorgestellten Techniken kombiniert, erhält man eine neue Lösung, die schlanker ist, völlig betriebssystemunabhängig, robuster gegenüber inkonsequenten Pfadformaten und frei von explizten Rekursionen:, , Wenn man hier in Bezug auf Best Practice noch eins nachlegen kann, dann schreibt mir! Ich freu mich auf Feedback., Lesen Sie hier den ersten Teil des Blogbeitrags., , © 2021 b.telligent;https://www.btelligent.com/blog/best-practice-arbeiten-in-python-mit-pfaden-teil-2/;B-Telligent;Stefan Seltmann
14.11.2018;            Best Practice: Arbeiten in Python mit Pfaden - Teil 1        ;Nehmen wir an, wir wollen einen speziellen Pfad genauer katalogisieren. Ich wähle als einigermaßen reproduzierbares Beispiel ein User-Verzeichnis auf einem Windows-10-System:, Die Variablenzuweisung wird bei Ausführung sofort mit einem Fehler quittiert:, Der Interpreter kommt nicht mit der Zeichenfolge \U klar, da Unicode-Zeichen mit ähnlicher Folge eingeleitet werden. Die Situation haben wir dem Problem zu verdanken, dass Windows-Systeme als Pfadtrenner „\“ und Linux-Systeme „/“ verwenden. Dummerweise ist der Windows-Trenner gleichzeitig die Einleitung für diverse Sonderzeichen oder Escapes in der Unicode-Kodierung, und schon haben wir das Durcheinander. Da sich die Systeme genauso wenig in absehbarer Zeit angleichen werden wie Dezimaltrennzeichen verschiedener Länder, müssen wir hier zu einer von drei Lösungen greifen., , Lösung 1, die hässliche Variante:, Man vermeidet Windows-Pfadtrenner komplett und schreibt den Pfad von Anfang an mit Linux-Trennern:, Der Interpreter evaluiert den Pfad dann korrekt, als wäre es von Anfang an ein Linux-System., , Lösung 2, die noch hässlichere Variante:, Man verwendet Escape-Sequenzen., Neben der Unleserlichkeit stört mich daran, dass man nicht bei jeder Buchstaben-Trenner-Kombination escapen muss. Hier halt nur vor dem „U“ und dem „b“., , Lösung 3, die elegante:, Man verwendet Raw-Strings und setzt „r“ als Prefix vor den String, um zu signalisieren, dass Sonderzeichen nicht evaluiert werden sollen., Zurück zur Aufgabe: Wir wollen zunächst alle Elemente eines Ordners auflisten. Den Pfad haben wir bereits., Mit dem einfachen Befehl os.listdir erhalten wir damit die Auflistung als Liste von Strings, und zwar nur die Dateinamen ohne Pfad. Ich verwende hier und in allen übrigen Beispielen Type Hinting als zusätzliche Dokumentation des Codes. Diese Schreibweisen sind erst ab Python 3.5 verfügbar., Die Dateiauflistung ist erstmal fein, mich interessieren aber hier noch die Statistiken der Dateien. Hierfür gibt es os.stat., Um den Dateipfad zu übergeben, müssen wir erst Dateinamen und Pfad kombinieren. Hierzu habe ich in freier Wildbahn schon oft folgende Konstrukte gesehen und selbst auch in meiner Anfängerzeit so eingesetzt. Zum Beispiel:, A und B sind hässlich, weil sie Strings mit „+“ verketten. Dazu gibt es in Python keinen Grund. B ist dabei besonders hässlich, weil man unter Windows ein doppeltes Trennzeichen braucht, sonst wird es als Escape-Sequenz für die schließenden Anführungszeichen gewertet., C und D sind etwas schöner, da sie String-Formatierungen verwenden. Sie lösen aber noch nicht das Problem der Systemabhängigkeit. Wenn ich unter Windows das Ergebnis ausgebe, erhalte ich nämlich einen funktionierenden, aber inkonsistenten Pfad mit meinem Mix aus Trennern:, , Hierfür gibt es eine Lösung seitens Python, nämlich os.sep bzw. os.path.sep. Beide geben die Pfadtrenner des jeweiligen Systems zurück. Sie sind in ihrer Funktion identisch, die zweite explizitere Schreibweise macht jedoch unmittelbar klar, um welchen Separator es sich handelt., Also könnte man schreiben:, Das erzeugt ein besseres Ergebnis, allerdings zu Kosten eines unübersichtlicheren Codes, wenn man mehrere Pfadabschnitte kombinieren würde., Es hat sich daher als Konvention eingebürgert, die Pfadelemente über die Stringverkettung zu kombinieren. Das ist noch kürzer und generischer:, , Wenden wir das auf unser Verzeichnis an:, Unter anderem erhalten wir als Ergebnis (nicht dargestellt) st_atime, die Zeit des letzten Zugriffes (access time), st_mtime für die letzte Veränderung (modification time), st_ctime für den Zeitpunkt der Erstellung (creation time). Zusätzlich enthält st_size die Größe des Files in Bytes. Mich interessiert im Moment nur die Größe und das letzte Veränderungsdatum. Ich wähle ein einfaches Listenformat für die Speicherung., , Das Ergebnis daraus ist auf den ersten Blick zufriedenstellend. Es ergeben sich jedoch zwei neue Probleme. Listdir unterscheidet nicht zwischen Dateien und Ordnern. Listdir geht auch nur von der Ebene eines Ordners aus und bearbeitet nicht die Unterordner. Wir benötigen also eine rekursive Funktion, die zwischen Ordner und Datei unterscheidet. os.path.isdir prüft für uns, ob sich hinter einem Pfad ein Ordner verbirgt., , Fertig! In einer Funktion von weniger als zehn Zeilen ist das Problem gelöst. Da ich das Ergebnis filesurvey als Liste von Tupeln geplant habe, kann ich das Ergebnis problemlos auch in einen Pandas-Dataframe überführen und dort für Analysen nutzen, wie z.B. Speichersummen über Ordner hinweg., , Ich weiß, der Blogeintrag versprach eigentlich, das Problem mit Best-Practice-Mitteln zu lösen. Vor einigen Jahren hätten meine Ausführungen tatsächlich den Titel auch verdient, aber Python entwickelt sich immer noch weiter und selbst bei solchen einfachen Use Cases werden noch Verbesserungen möglich. In einem zweiten Teil werde ich diesen Use Case nochmals aufgreifen und mit eleganteren Methoden lösen., , Lesen Sie hier den zweiten Teil des Blogbeitrags., , © 2021 b.telligent;https://www.btelligent.com/blog/best-practice-arbeiten-in-python-mit-pfaden-teil-1/;B-Telligent;Stefan Seltmann
26.10.2018;"            Snowflake-Cloud-DB und Python: ""zwei gute Freunde""        ";"Snowflake ist eine native Cloud-DB und läuft auf AWS und inzwischen auch auf Azure. Die Internetverbindung vom Client zur Cloud und die Daten innerhalb der DB sind verschlüsselt. Dabei kann es während der Ausführung beliebig und automatisch hochskalieren und am Ende wieder herunterschalten. Da für das (Speicher-)Volumen und die Ausführungszeit gezahlt wird, können so durch die geringen Zeiten Kosten gespart werden. Eine ausführliche Online-Dokumentation ist unter folgender URL verfügbar: https://docs.snowflake.net/manuals/index.html, Man muss übrigens kein AWS-Kunde sein, um Snowflake verwenden zu können. Snowflake selbst bietet als Cloud-DB-Service keine eigenen ETL-Tools an, sondern überlässt dies den Herstellern von ETL-, Reporting- oder Self-Service-BI-Tools. Diese bieten meist native Treiber und Connections an, um ihre Tools mit Snowflake verwenden zu können. Soll im Unternehmen kein separates ETL-Tool eingesetzt werden, gibt es verschiedene Möglichkeiten, die Daten zu laden und die ETL-Strecken zu realisieren. Das Umsetzen der Logik in SQL und die Orchestrierung über Python ist dabei ein möglicher Weg., Gemäß eigener Umsetzung bietet Snowflake Treiber für Python, Spark, ODBC und JDBC an. Das bedeutet, dass ein sogenannter Python-Connector für Snowflake existiert, um vom eigenen Client aus die DB-Verbindung herstellen zu können., In diesem Artikel wird genau dieser Python-Connector anhand von Beispiel-Snippets vorgestellt: Mit Hilfe des Python-Connectors (= DB-Verbindung) können ETL-Ladestrecken (manuell) realisiert werden. Scheduling kann beispielsweise per „Cronjob“ erfolgen, Quellen (csv, xml, Tabellen, xlsx etc.) können importiert und sogar weitere SQL-Skripte aufgerufen werden. , Vorgehensweise, In Python werden mit Hilfe von ""pip"" neue Packages installiert:, ""pip install snowflake-connector-python"", In Python-Code wird folgender Modulimport möglich:, ""import snowflake.connector"", Damit steht der Python-Connector zur Verfügung, und die Umsetzung kann starten. Im Folgenden ein erstes Python-Snippet: , (Test der Verbindung), try:, finally:, try:, finally:, Nach diesem ersten Python-Snippet wird im nächsten Schritt versucht, Stored Procedures innerhalb von Snowflake aufzurufen. Snowflake Stored Procedures werden in **JavaScript** (!) implementiert., , Da staunt der ETL-Architekt, Data Engineer, und der SQL-Entwickler ..., Man stellt sich spontan die Frage: JavaScript und DBMS? Wie soll das gehen? Innerhalb von Stored Procedures werden drei Snowflake-eigene Objekte zur Verfügung gestellt:, ""snowflake"", ""Statement"", ""ResultSet"", Sie befähigen JavaScript innerhalb von Snowflake zu DDL + DML. Stored Procedures werden, unabhängig davon, ob in Snowflake oder in Python, per ""Callprocname()"" aufgerufen., Hinweis am Rande:, Ein **Template** für JavaScript Stored Procedures mit SQL-Zugriff in Snowflake:, , , Es wird aufgezeigt, wie man in Python diese Stored Procedure ""my_proc1()"" von oben aufrufen kann. Hierbei werden weitere Snowflake-spezifische SQLs für die Umgebung gebraucht; diese sind ""mandatory"" und werden hier exemplarisch aufgezeigt und nicht näher behandelt, da es den Rahmen dieses Artikels sprengen würde:, Wie steht es nun um die Performance?, Welche Ausführungszeiten werden mit verschiedenen Varianten in Python erreicht?, Die Varianten in Python sind die folgenden (Werte aus einem „POC“-Projekt ermittelt):, (1) Aus Python heraus werden Stored Procedures in Snowflake aufgerufen., (2) Die Stored Procedures werden in Python re-implementiert bzw. nachgebaut und in Python ausgeführt., (3) Keine Stored Procedures, sondern:,  aus Python heraus werden SQL-Statements (= SELECT) direkt in Snowflake ausgeführt,,  ohne den Umweg über Stored Procedures. , Hierbei gab es mehrere Durchläufe bzw. Testaufrufe:, zu (1): python calling javascript stored proc.:, Ausführungszeiten (sec): , (1) 8.61, (2) 7.28, (3) 6.00, (4) 6.70, (5) 8.64,, (6) 14.49, (7) 7.27, (8) 7.77, (9) 8.85, (10) 8.55, , zu (2): python re-implementing stored proc.:, Ausführungszeiten (sec): , (1) 6.83, (2) 4.76, (3) 6.57, (4) 5.27, (5) 5.19,, (6) 4.81, (7) 5.58, (8) 5.38, (9) 12.54, (10) 5.83, , zu (3): python sql processing: , * select from (arbitrary) Snowflake table, * insert into another Snowflake table,  ==&gt; inkl. „TCL“ (= begin, rollback, commit), Ausführungszeiten (sec): , (1) 4.42, (2) 6.52, (3) 35.08, (4) 3.91, (5) 3.69,, (6) 4.95, (7) 4.52, (8) 18.99, (9) 4.94, (10) 3.89, , Man kann sehen, dass es „Ausreißer“ gibt, die auch nicht durch Snowflake erklärt werden können. , Hinweis: , Eine weitere Variante ist der Aufruf von SQL-Dateien bzw. Skripten, die mehrere SQL-Statements beinhalten und mit Hilfe von ""execute_stream()"" Statement für Statement ausgeführt werden können, gemäß folgendem Python-Snippet. Hierbei fehlen allerdings Messwerte für die Ausführungszeiten:, , try:, Es gibt einen nativen Python-Connector von Snowflake, mit dem manuelle ETL-Prozesse auf eine bequeme Art und Weise möglich sind. Python stellt trotzdem eine Option unter vielen dar, grafische ETL-Tools bieten mittlerweile ebenfalls native Konnektoren an., Je nach (eigenem) Wunsch und Rahmenbedingungen sind Use Cases auch unter Python implementierbar. Wenn Python bereits „im eigenen Hause“ eingesetzt wird, sind Importvorgänge schnell eingebaut. Man kann modular implementieren und Packages aufbauen. Diese Möglichkeit bietet Snowflake mit den Stored Procedures nicht, es sind keine Frameworks in JavaScript möglich. Man hat am Ende eine Menge von Procedures, die nebeneinander existieren, und keine Modularität., Da man sich in der Cloud befindet, gibt es für ein Logging kein Filesystem und man muss auf Tabellen ausweichen. JavaScript in Snowflake stellt für SQL-Entwickler eine neue Umgebung bzw. eine neue „Low-Level“-Sprache dar, d.h., es gibt eine Lernkurve, die man nicht unterschätzen darf. Das kann man mit Python „umgehen“ und in Python „bleiben“, statt Stored Procedures in JavaScript zu verwenden., Die Ausführungszeiten für Python sind akzeptabel, weil es den nativen „Snowflake-Connector“ für Python gibt. Dieser tunt das SQL bereits intern auf dem Weg zu Snowflake – das ist beim JDBC-Treiber nicht der Fall., ETL mit Python im Kontext Snowflake wird empfohlen, wenn bereits Python-Kenntnisse „im eigenen Hause“ existieren. Es können damit auch schnell Testszenarien umgesetzt werden., , © 2021 b.telligent";https://www.btelligent.com/blog/snowflake-cloud-db-und-python-als-gute-freunde/;B-Telligent;Oliver Gräfe
04.10.2018;            Mit Data Science den passenden Strampler kaufen        ;"Die Digitalisierung ist in meiner kleinen Familie weiter als in vielen Unternehmen. Wir organisieren uns auf Threema und synchronisieren die Einkaufsliste per Wunderlist. Lediglich die Wunderwelt der Predictive Analytics haben wir noch nicht genutzt – bis jetzt. Als sich meine Familie kürzlich um einen sehr süßen und sehr kleinen Menschen vergrößert hat, habe ich meinen Spieltrieb während der Elternzeit auf ein Vorhersagemodell gelenkt, das ein häufig auftretendes Problem frischgebackener Eltern löst., Wer kleine Kinder hat, kennt von den ersten Arztbesuchen an die Wachstumskurven, die Körpergrößen und Gewicht der Kinder in 10-Prozent-Schritte (Perzentile) einordnen. Sie ermöglichen Aussagen wie „40% der Jungen haben eine kleinere Geburtsgröße als unserer.“ (oder entsprechend für Mädchen; die Trennung ist nötig, weil Jungen und Mädchen einfach unterschiedlich schnell wachsen). Wer diese Kurven nicht kennt, findet hierein Beispiel., Als Data Scientist sehe ich darin auch ein zuverlässiges Prognosemodell für die Körpergröße: Ein Kind, das zum Beispiel mit einer Körpergröße geboren ist, die dem 40%-Perzentil entspricht, wird meistens auch im weiteren Verlauf seines Wachstums eine Körpergröße haben, die größer ist als diejenige von 40% der Kinder gleichen Geschlechts., Im nächsten Schritt rechnet man für jeden Lebensmonat des Kindes aus, welcher Körpergröße dieser Z-Score entspricht. Anschließend hat man noch ein wenig Datumsrechnerei, um die Kalenderdaten festzustellen, zu denen der Lebensmonat anfängt und aufhört (wir rechnen der Einfachheit halber übrigens mit Lebensmonaten, die einheitlich 30 Tage lang sind), und fertig ist die Prognose. Ach ja, am Schluss muss man natürlich die prognostizierte Körpergröße noch in eine Kleidergröße umrechnen. Dabei ist es gut, zu wissen, dass die Kindergrößen in Deutschland immer das obere Ende der Körpergrößen angeben, für die die Kleidergröße passt. Ein Strampler in Größe 62 passt also Kindern von 56 cm (das ist die nächstkleinere Größe) bis 62 cm Körpergröße. Um die Prognose allerdings sinnvoll zu nutzen, auch für andere Leute, die Python vielleicht nur als Würgeschlange kennen, muss man noch ein wenig Arbeit investieren. In diesem Fall war das eine gute Gelegenheit, mich weiter in Vegaeinzuarbeiten. Für die, die Vega noch nicht kennen: Meiner bescheidenen Meinung nach ist es das kommende Framework für die Visualisierung im Bereich Data Science und darüber hinaus. Vergesst ggplot, matplotlib und Bokeh, Vega ist das kommende Ding. JavaScript-basierte Visualisierungen lassen sich damit einfach in JSON konfigurieren – und neben der eigentlichen Visualisierung bringt das Framework auch Funktionalitäten für Benutzerinteraktion und Datenaufbereitung mit., Für den Moment nur so viel: Man kann das ganze Vorhersagemodell rein in Vega realisieren., P.S.: Hier geht's zum vollständigen Interview mit patschehand.de, *Noch mehr Spaß am Kleidergrößenrechner habt ihr mit allen Browsern außer dem Internet Explorer ;), , © 2021 b.telligent";https://www.btelligent.com/blog/mit-data-science-den-passenden-strampler-kaufen/;B-Telligent;Dr. Michael Allgöwer
11.09.2018;"            Recommender Systems - Teil 1: Motivation &amp; Grundlage        ";Dieser Blogbeitrag beantwortet zwei Fragebereiche:, Ich fokussiere mich in diesem Beitrag auf einen soliden Überblick. Details beschreibt mein KollegeJosef Bauer in dendarauf aufbauenden Teilen zwei und drei., Ein Recommendation System spricht gezielt die individuellen Interessen einzelner Konsumenten an. Andersherum formuliert, verhindert es den Abbruch einer Customer Journey durch Überforderung oder Frust. Deshalb sind Recommendation Systems gerade bei großen Waren- und Dienstleistungsangeboten zentral. Dabei ist es wichtig, das Benutzungsverhalten der Kunden zu berücksichtigen. Während manche Seiten explizit zum Stöbern einladen wollen, steht bei anderen Seiten das schnelle Auffinden des optimalen Produkts im Mittelpunkt., Digitale B2C-Geschäftsmodelle profitieren am meisten von Recommendation Systems, denn dort beeinflussen sie unmittelbar die wichtigsten Key Performance Indicators (KPIs). Eine Einführung steigert das Nutzungserlebnis deutlich. Zudem erwarten Konsumenten gute Empfehlungen zunehmend als Standard-Feature. Die Optimierung eines bestehenden Recommendation Systems dient zudem der Erschließung ungenutzter Potenziale., Neben diesem Haupteffekt haben Recommendation Systems eine Reihe von Nebeneffekten:, Viele Details bestimmen darüber, wie gut dieses Instrument genutzt werden kann. Man kann Recommendation Systems dennoch grob in zwei Geschmacksrichtungen unterteilen:, Die erste Option orientiert sich eng an den verfügbaren Produkten. Der Konsument sieht dabei Produkte, die dem bereits betrachteten ähneln. „Ähnlichkeit“ kann in diesem Zusammenhang viele verschiedene Dinge meinen. Beispiele sind die Produktkategorie, der Preis oder auch der Hersteller. Selbstverständlich ist auch eine Kombination verschiedener Dimensionen denkbar., Bei der zweiten Option steht nicht die Suche nach ähnlichen Produkten, sondern nach ähnlichen Kunden im Mittelpunkt. Die daraus entstehenden Empfehlungen können der ersten Option stark oder überhaupt nicht ähneln. Ein solcher Ansatz versucht, die hinter den Kaufentscheidungen liegenden Präferenzen und Handlungslogiken der Kunden zu modellieren und weniger die „objektiven“ Ähnlichkeiten zwischen Produkten., In Zeiten der DSGVO ist die Auswahl nicht unwesentlich von Datenverfügbarkeit und rechtlich zulässiger Nutzbarkeit getrieben. Allerdings bietet sich für die Einführung ein produktbasierter Ansatz an. Aus praktischer Sicht lässt sich diese Option gut als Tabelle vorhalten, die in festen Intervallen neu berechnet wird. Das erleichtert die Integration in mobile Webseiten und Apps enorm. Der konsumentenbasierte Ansatz bietet größere Potenziale, hängt jedoch von einer verlässlichen Streaming-Infrastruktur ab., Um Recommendation Systems in die Tat umzusetzen, fallen Bestandsaufnahmen auf drei verschiedenen Ebenen an:, Wenn diese drei Fragen ausreichend beantwortet sind, kann die eigentliche Arbeit beginnen. Die Details beschreibtJosef Bauer in seinem nächsten Beitrag, , © 2021 b.telligent;https://www.btelligent.com/blog/recommender-systems-1-motivation-grundlage/;B-Telligent;Dr. Michael Allgöwer
20.06.2018;            Predictive Analytics World - Tag 2        ;Die Vielfalt der in sechs Tracks und über dutzende von Sessions abgedeckten Themen bei der PAW 2018 in Berlin kann nur mit einem klaren Fokus sinnvoll zusammengefasst werden. Deshalb konzentriere ich mich hier auf drei Präsentationen, die von verschiedenen Seiten eine Frage beleuchten, die Dean Abbot in seiner heutigen Keynote gestellt hat: Ist es möglich, fertige Rezepte für Data Science zu definieren?, Dean Abbot selbst bezog sich dabei hauptsächlich auf das Verhältnis von Datenaufbereitung und der späteren Modellierung. Es wäre selbstverständlich hocheffizient, eine einzige Datenpipeline zu erstellen und deren Ergebnis zum Training verschiedenster Modelle nutzen zu können. Aus diesen Modellen würden wir dann das beste auswählen und es benutzen. Klingt verlockend? Ist es auch! Allerdings gibt es zahlreiche Unterschiede darin, wie Algorithmen etwa mit unterschiedlichen Skalenniveaus oder fehlenden Werten umgehen, um nur zwei seiner fünf Beispiele zu nennen. Natürlich gibt es dafür Richtlinien und Best Practices. Trotzdem betonte Dean Abbot, dass es keine allgemeinen Rezepte gibt, die einfach blind angewandt werden sollten., Hans Werner Zimmermann ergänzte diese Perspektive mit einer anregenden Präsentation über das Erstellen von Vorhersagemodellen. Als Resultat seiner langjährigen Erfahrung war es ihm möglich, theoretische Überlegungen mit praktischen Anwendungen zu kombinieren. Die Liste an Herausforderungen in diesem Bereich ist äußerst umfangreich. Um zwei Punkte herauszustellen:, Aus Zimmermanns Perspektive hilft ein Mehr an Daten hier nicht weiter. Stattdessen argumentierte er für theoretische Überlegungen, kombiniert mit tiefem Fachwissen., Wie das Anwendungsgebiet die besonderen Herausforderungen eines Machine-Learning-Problems formt, zeigte zudem Malte Pietsch im Bereich des NLP (Natural Language Processing). Sein Vortrag konzentrierte sich auf das Problem der sogenannten Named Entity Recognition bzw. NER. In den letzten Jahren gab es in diesem Bereich beeindruckende Fortschritte., Leider gibt es dort immer noch einen ärgerlichen Trade-off. In den Bereichen, in denen am meisten Text verfügbar ist, fehlen die Business Use Cases. Dort wo die Use Cases zu finden sind, fehlt allerdings eine ausreichende Textmenge. In jüngerer Vergangenheit wurden Deep-Learning-Architekturen um verschiedene Embedding-Ansätze erweitert, um dieses Problem anzugehen. Malte Pietsch verwies beispielsweise auf das BERT-Projekt von Google, das vor kurzem öffentlich verfügbar gemacht wurde. Das dazugehörige Github-Repository hat innerhalb von nur zwei Wochen fast 8.000 Sterne gesammelt!, Für mich nehme ich vor allem drei Beobachtungen mit:, Wir freuen uns bereits auf die Predictive Analytics World 2019!, , © 2021 b.telligent;https://www.btelligent.com/blog/predictive-analytics-world-2018-tag-2/;B-Telligent;Dr. Michael Allgöwer
18.04.2018;            Predictive Analytics World 2018 - Tag 1        ;"Die PAW ist mittlerweile auf drei parallele Tracks angewachsen und dementsprechend inhaltsreich. Meine Favoriten an diesem ersten Tag gehören meist zum längeren (genauer: einstündigen) Vortragsformat der Deep Dives, bei denen es richtig zur Sache und in die Details geht. Mehr Zeit, mehr Input., Sven Crones Deep Dive drehte sich um Zeitreihenprobleme: wie man sie modern und souverän angeht, aber auch, wie man darüber einen Vortrag hält, der keine Sekunde langweilig ist. Anders als viele andere Zeitreihenspezialisten tat Sven dabei nicht so, als wären klassische statistische Zeitreihenmethoden (ARIMA &amp; Co.) der einzig richtige Weg, Zeitreihenprobleme zu lösen. Stattdessen zeigte er, wie man neuronale Netze sinnvoll zur Zeitreihenprognose einsetzt. Neuronale Netze hier einzusetzen, ist keine hypegetriebene Modeentscheidung, sondern eine Konsequenz der Tatsache, dass neuronale Netze bei diesen Problemen sehr viel besser funktionieren als viele der anderen robusten Arbeitspferde wie Random Forest. Spannend dabei: Tiefe Netze bieten bei Zeitreihenproblemen typischerweise wenig Mehrwert. Flache Architekturen liefern eine ähnliche Genauigkeit bei deutlich geringerer Komplexität. Tiefe neuronale Netze lassen sich sinnvoll einsetzen, indem man auf die Metaebene wechselt: Man kann Deep Learning benutzen, um anhand der Zeitreihendaten zu entscheiden, welcher Algorithmus für diese Daten am besten funktioniert., Dr. John Snow hat mit Datenvisualisierung Menschenleben gerettet. Das war zu Zeiten der Choleraepidemie in London, als er mit Hilfe einer Karte von Todesfällen in einem besonders heimgesuchten Viertel die Ursache der Häufung finden und unschädlich machen konnte. Mit diesem Beispiel stieg Alfred Inselberg in seinen Vortrag zur interaktiven Datenvisualisierung mit parallelen Koordinaten ein. Ich bin ehrlich gesagt noch nicht sicher, wie nützlich ich Visualisierungen mit parallelen Koordinaten für meine Arbeit finde – bisher habe ich immer draufgeschaut und nichts gesehen. Gelernt habe ich, dass das möglicherweise daran lag, dass ich statische Plots angeschaut habe. Wenn es funktioniert, dann nur mit interaktiven Plots, die zudem mit anderen Grafiken kombiniert werden (Scatterplots zum Beispiel), die sich gleichzeitig und konsistent mit aktualisieren., Ein (ehemaliges?) Modethema hatte sich Phil Winters vorgenommen: Bots. Nicht gerade mein Lieblingsthema, um ehrlich zu sein: zu viel Hype, zu wenig praktischer Nutzen bis jetzt. Aber ich dachte mir, wenn Phil es behandelt, wird’s schon passen: Er hat ein Talent dafür, Hypethemen zu erden und mit viel gesundem Menschenverstand auf ihren Praxisnutzen zu testen. Der eigens für den Vortrag geschaffene Bot Emil diente als durchgängiges Beispiel für eine erfreulich detaillierte Darstellung, verbunden mit einem überzeugenden Plädoyer für mehr Einbeziehung von Nutzerfeedback in prädiktive Modelle (Active Learning)., Last, but not least gab’s ein Thema, das ich, ehrlich gesagt, zunächst im Verdacht hatte, vielleicht nicht ganz seriös zu sein. Ich musste mein Vorurteil revidieren: Jonathan Mall gab eine spannende, fundierte und extrem gut präsentierte Einführung in die psychologisch geschickte Nutzung von Word Embeddings für Marketingzwecke. Für mich einer der spannendsten Vorträge, weil er zwei Lieblingsthemen kombinierte: Word Embeddings und Psychologie. Er zeigte, wie sich Word Embeddings nutzen lassen, um Begriffsassoziationen länder- und sprachspezifisch zu quantifizieren, passende Assoziationen zu finden und unpassende zu modifizieren., , © 2021 b.telligent";https://www.btelligent.com/blog/predictive-analytics-world-2018-tag-1/;B-Telligent;Dr. Michael Allgöwer
15.03.2018;            AnaCredit - Kosteneinsparungen durch Kreditmeldewesen verzeichnen        ;"In der Vergangenheit wurden Banken in Europa von den jeweiligen nationalen Aufsichtsbehörden einzig unter Berücksichtigung des nationalen Aufsichtsrechts kontrolliert. Mit der Umsetzung des Einheitlichen Aufsichtsmechanismus (engl. Single Supervisory Mechanism - SSM) im November 2014 wurde erstmals der Europäischen Zentralbank (EZB) die alleinige Verantwortung zur Kontrolle und Regulierung aller Banken in der Eurozone übertragen., Die Erhebung von statistischen Daten zum Kreditbestand war bisher nur in wenigen europäischen Staaten mit unterschiedlichen Vorgaben geregelt. Als Folge konnten Meldungen zum Kreditbestand nur teilweise national erfasst und schwer bis gar nicht auf europäischer Ebene aggregiert werden. Mit der Einführung des Analytical Credit Datasets (AnaCredit) und der geplanten Einrichtung eines zentralen Kreditregisters (engl. Central Credit Register - CCR) wird eine Optimierung der Aufgaben zur Geldpolitik und der Finanzmarktstabilität auf europäischer Ebene angestrebt mit dem Ziel, Systemrisiken frühzeitig erkennen und gegensteuern zu können., Mit Beschluss der EZB am 18. Mai 2016 wurde die Umsetzung von AnaCredit verordnet. Im Zuge von AnaCredit sollen Detaildaten von Kreditnehmern, Kreditgeschäften sowie deren Risikoeinschätzung an die nationalen Aufsichtsbehörden gemeldet werden., Konkret besteht eine Meldepflicht, wenn, Der Sitz des Schuldners ist hierbei irrelevant. Es zählt nur, dass der Kredit innerhalb der Eurozone aufgenommen wurde., Hinweis an dieser Stelle:, Detaildaten zu juristischen Personen müssen gemeldet werden, zu natürlichen Personen muss nur deren Beteiligung anonym gemeldet werden., Die Meldung soll hierbei insgesamt 95 Felder, davon 89 fachspezifische und sechs technische Identifikationsmerkmale, aus den folgenden Bereichen abdecken:, , (Zu Details hierzu siehe auch dasAnaCredit Reporting Manual), Grundsätzlich gilt die Aussage, dass die AnaCredit-Meldungen von, abzugeben sind., Da die Umsetzung der AnaCredit-Anforderungen jedoch eine große Herausforderung für viele Institutionen bedeutet, gelten in Deutschland folgende reduzierte Anforderungen:, Niederlassungen außerhalb der Eurozone müssen 13 Felder weniger liefern., Der Zeitplan zur Einführung von AnaCredit , , Die erste Stufe von AnaCredit soll ab Anfang Oktober 2018 komplett produktiv sein. Ausgehend von den Herausforderungen, die die erste Stufe mit sich bringt, den noch vorhandenen reduzierten Anforderungen sowie einer „straffreien“ Einführungszeit wird sich die Realisierung der ersten Stufe jedoch nicht überall pünktlich abschließen lassen., Für die Stufen 2 und 3 ist aktuell vor allem eine Ausweitung der Finanzinstrumente geplant., Die Einführung von AnaCredit bedeutet eine große Herausforderung für jede betroffene Institution, auch unter Berücksichtigung von eventuell verringerten Meldeanforderungen., Herausforderungen für die Core-Banking-Systeme:, Herausforderungen für die BI-Landschaft:, Governance und Strategie:, Die Architektur für die Datenversorgung und Verarbeitung kann man grob in zwei mögliche Szenarien aufteilen., Zentrales Szenario, , Alle Core-Banking-Systeme liefern in eine BI-Landschaft an; diese konsolidiert die Daten und stellt sie über eine Präsentationsschicht für die Extraktion in das Meldewesen zur Verfügung., Vorteile:, Nachteile:, Dezentrales Szenario mit zentraler Konsolidierung, , Eine Finanzinstitution besteht aus mehreren Tochtergesellschaften oder Filialen, die wiederum eine eigene IT- und BI-Infrastruktur besitzen. Die Daten für das Meldewesen werden dort bereits konsolidiert und an den Mutterkonzern oder eine führende Gesellschaft zur Gesamtkonsolidierung und Meldung weitergeleitet., Vorteile:, Nachteile:, AnaCredit ist eine regulatorische Vorgabe aus mehreren Themen, die in den letzten Jahren zur Umsetzung adressiert wurden. Es ist nicht verwunderlich, dass viele Themen miteinander in Verbindung stehen und sich Synergien ergeben, wenn eine entsprechende Umsetzung bereits stattfand. So sind regulatorisch direkt davon betroffen:, Langfristiger Ausblick:, Da AnaCredit eine Konsolidierung im Meldewesen anstrebt, ist langfristig, wenn auch noch nicht beschlossen, von einer Konsolidierung ähnlicher Meldungen (z.B. Mio. Kredit) im deutschen Bankenbereich auszugehen., Durch die strikte Einführung und nachhaltige Umsetzung von Data-Governance- und Data-Strategy-Maßnahmen im BI-Umfeld, aber auch in den zugehörigen IT-Systemen sowie den Prozessen der Fachbereiche ergeben sich aus den Herausforderungen auch Chancen., Vielfach werden langfristig Kosten gespart durch, , © 2021 b.telligent";https://www.btelligent.com/blog/mit-anacredit-kosten-einsparen/;B-Telligent;Stefan Steinhaus
01.03.2018;            Live vom Spark Summit 2018 in London        ;Ich darf in diesem Jahr wieder „live“ vom Spark Summit Europe berichten. Dieses Jahr findet er in London statt. Die Veranstaltung ist in den letzten Jahren in Inhalt und Umfang gewachsen, so dass nun schon dieselben Räumlichkeiten in London genutzt werden, die sonst größere Veranstaltungen wie die Strata beherbergen. Die Schätzung beläuft sich auf 1.500 Teilnehmer gegenüber 1.100 im Vorjahr., Genau genommen ist es dieses Jahr auch nicht mehr nur der Spark Summit. Die Veranstaltung ist neu gebrandet als „Spark + AI Summit“. In meinen Augen völlig zu Recht und nicht bloß zur Schmückung mit Buzzwords. Vor drei bis vier Jahren hat man Spark von außen noch als eine weitere Big-Data-Technologie neben anderen gesehen. Tatsächlich ist es jedoch in meinen Augen die einzige, die in diesem Umfang die Entwicklung von Data-Science-Anwendungen end-to-end von der Datenintegration bis hin zum Machine-Learning-Prozess abdeckt, und das bei schier unbegrenzten Datenmengen. Nahezu jeder Anwendungsfall aus den Vorträgen des Summits beinhaltet ein Machine-Learning-Modell, mit einem stark steigenden Anteil von Deep-Learning-Techniken., Die einleitenden Keynotes am Vormittag vertieften den Fokus auf AI. Ein eindrucksvoller Anwendungsfall von Shell zeigte zum Beispiel, wie Deep-Learning-basierte Bilderkennung genutzt werden kann, um in Tankstellen schnellstmöglich auf Gefahrenherde (z.B. Brandursachen) hinzuweisen., Ein weiterer Fokus der Keynotes lag dieses Jahr auf der Zusammenarbeit von Data Scientists und Data Engineers. Ali Ghodsi, ein Co-Founder von DataBricks, behauptet, dass nur 1% der Unternehmen wirklich erfolgreich im Einsatz von AI sind. Die anderen scheitern wohl daran, dass die Daten noch nicht für AI geeignet integriert sind, dass die Daten in Silos vorliegen, oder letztendlich daran, dass es keinen gemeinsamen Workflow von Data Scientists und Engineers gibt., Der Wunsch nach mehr Zusammenarbeit auf einer gemeinsamen Plattform wird von DataBricks als Veranstalter allerdings nicht ganz uneigennützig heraufbeschworen. „Unifying Data Science + Engineering“ ist der neue Slogan der DataBricks-Plattform. Ich bin dennoch überzeugt, dass DataBricks mit dieser Ausrichtung den Kern des Problems richtig einschätzt., Natürlich gab es bei den Keynotes auch wieder Neuankündigungen von Produkten/Projekten und Features. Zunächst ein Update der künftigen Releases: Spark 2.4 wird vermutlich innerhalb der nächsten Tage veröffentlicht. Neuerungen gibt es vor allem bei Details im Bereich Structured Streaming. Gleichzeitig wird für 2019 ein Spark 3 angekündigt, allerdings wurde noch nichts über die Inhalte berichtet., Der Großmeister Matei Zaharia gab selbst ein Update zu ML-Flow. Ich hatte vor wenigen Wochen über dieses neue Open-Source-Projekt von DataBricks berichtet. Diese Machine-Learning-Plattform soll den Data-Science-Workflow von der Datenaufbereitung über das Training bis zur Implementierung und zurück deutlich vereinfachen und standardisieren. Die Vielfalt an unterschiedlichen Technologien und Frameworks soll standardisiert abgekapselt werden. Ich konnte Matei gestern noch persönlich darauf ansprechen und er betonte, dass er ML-Flow sogar als Template für ML-Projekte sieht. Pünktlich zur Präsentation wurde gestern auch eine neue Version 0.7 veröffentlicht. Diese enthält nun zusätzlich zur ursprünglichen Python-API auch eine Java/Scala-API. Darüber hinaus gibt es wohl eine ML-Flow seitens des RStudios., , Das neue Projekt Hydrogen versucht, die Performance von Deep-Learning-Anwendungen auf Spark zu verbessern. Diese stützen sich meist auf Python und durch den ständigen Wechsel der Java-Runtime von Spark zu Python können rund 92% der Rechenleistung nur durch den Austausch zwischen den Technologien verloren gehen. Eine Weiterführung der Pandas-UDFs aus Spark 2.3, ein neues Scheduling-Konzept sowie andere Optimierungen sollen diesen Bottleneck dramatisch auf ein Hundertstel schrumpfen lassen., , Mit einem zwinkernden Auge wurde verkündet, dass DataBricks nun die Zeitreise erfunden hat. Leider dürfen vorerst nur die Daten reisen und auch nur in die Vergangenheit. Konkret bedeutet das, dass eine Verzahnung von DataBricks Delta, der Plattform für Datenintegration, mit ML-Flow es ermöglicht, alle Machine-Learning-Berechnungen berichtstreu zu gestalten. Das heißt, ein Modell-Training, das jeden Morgen auf einer neu aktualisierten Datenquelle ausgeführt wird, kann bei Bedarf komplett aus der Perspektive eines vergangenen Tages mit dem damaligen Datenstand neu ausgeführt werden. Damit wird das Debuggen von plötzlichen Veränderungen in der Performance des Modells dramatisch erleichtert., Neben den Keynotes gibt es natürlich noch zwei Tage vollgepackt mit über 100 Präsentationen von „Technical Deep Dives“ bis hin zu exotischen Anwendungsfällen von Spark. Allgemein ist in diesem Jahr deutlich die Ausweitung von DataBricks auf Microsofts Azure-Cloud zu spüren. Microsoft tritt als großer Sponsor auf und viele vorgestellte Anwendungsfälle wurden bereits dort durchgeführt., Zuletzt scheint sich die Rolle von Python im Spark-Universum weiter zu stärken. Natürlich ändert sich nichts daran, dass die performantesten und produktionssichersten Spark-Anwendungen immer noch in Scala geschrieben werden. Man erfährt auch im Austausch, dass immer noch viele mit PySpark starten, aber irgendwann zu Scala migrieren. Dennoch ist es ein Statement, dass ML-Flow vor dem aktuellen 0.7-Release nur in Python veröffentlicht wurde und hart daran gearbeitet wird, die Performance-Lücken von PySpark zu Scala-Spark zu schließen. Python entwickelt sich zur Standardsprache für Deep-Learning-Frameworks und deswegen wurde auch der Großteil der Live-Demonstrationen in den Keynotes mit Python-Code durchgeführt. Es bleibt spannend, wie die technologische Ausrichtung sich für Spark 3 entwickeln wird., , , © 2021 b.telligent;https://www.btelligent.com/blog/live-vom-spark-summit-2018/;B-Telligent;Stefan Seltmann
15.02.2018;            MLflow: die neue Machine-Learning-Plattform        ;Jeder, der mit Machine-Learning-Anwendungsfällen zu tun hat, kennt vermutlich dieses Problem: Es steht mittlerweile eine große Fülle an Lösungen und Engines, vor allem Open Source, zur Verfügung. Schnell können erste Analysen und Modelle entwickelt werden, aber die Integration der Lösung in die Geschäftsprozesse erfordert nochmals deutlichen Mehraufwand. Oft bleibt es dann leider nur bei einem POC und ein trainiertes Modell findet nie Anwendung. Zur Produktivsetzung gehört der Transfer von Datenaufbereitungen, gebildeten Merkmalen und trainierten Modellen in einen Deployment- bzw. Anwendungsprozess. Dieser findet häufig auf anderer Infrastruktur und im Worst Case sogar basierend auf einer anderen Technologie statt. Bestehende Arbeitsschritte müssen dann neu implementiert werden, in der Regel von anderen Personen, die das Modell entwickelt haben – Business Logic wird mehrfach vorgehalten., Was dabei on top noch meist vernachlässigt wird, ist die ständige Versionierung der Modellstände sowie das Monitoring der Modellperformance. Meist gibt es immer nur das aktuellste Modell und keine Historie oder nachvollziehbare Entwicklung. Ergebnisse sind womöglich nicht mehr reproduzierbar., Ein Team um Matei Zaharia, den Spark-Gründer, hat sich nach eigenen Erfahrungen mit Kunden und Anwendern nun diesem Problem angenommen und eine Python-basierte Open-Source-Machine-Learning-Plattform namens MLflow veröffentlicht (www.mlflow.org). Sie wurde bereits im Juni auf dem Spark-Summit vorgestellt., Ich finde es schade, dass die Veröffentlichung nicht die Wellen geschlagen hat, die die Vision dieser Plattform verdient. Daher stelle ich hier nochmal kurz die Eckdaten von MLflow vor. Das Produkt ist noch im Alpha-Status, also noch weit weg von produktivem Einsatz, aber der aktuelle Funktionsumfang und der generische Ansatz des Frameworks begeistern mich schon jetzt. Gerade erst gab es ein neues Release 0.5, das wiederum den Scope erweitert., Das Framework besteht aktuell aus drei Komponenten:, Das Tracking stellt einen Funktionsumfang in Python bereit, mit dem von Hyperparametern über Ergebnisse (Modell-Performance) bis hin zu Artefakten, wie sogar Charts, alles geloggt werden kann., Dazu gibt es eine schicke Benutzeroberfläche über einen beinhalteten Tracking-Server, der die Ergebnisse darstellt und durchsuchbar macht. Dieses Tracking bietet nun die Möglichkeit, für jeden Trainings- oder Scoring-Lauf eines Modells Voreinstellungen, Parameter und Ergebnisse für ein kollektives Monitoring zu sammeln. Unter Python verwendet man einfache Funktionsaufrufe, ähnlich einem Logging-Call, um Werte für das Tracking zu sammeln. Alternativ ist es auch möglich, den Tracking-Server direkt über eine REST API anzusprechen, was die Verwendung jenseits von Python ermöglicht., Dieses Modul ermöglicht das Abkapseln von ML-Projekten zu einem abgeschlossenen Paket oder Git Repository. Der Anwender des Projekts muss dabei gar nichts mehr über die interne Funktionsweise wissen. Die Einstiegspunkte und die Basiskonfigurationen werden über ein YAML-File festgelegt. Darüber kann z.B. auch gesteuert werden, wie ein notwendiges Conda-Environment aussieht, das dann von MLflow bei Bedarf erstellt wird., Das bedeutet, dass man ein ML-Modell samt Datenaufbereitung über einen einzigen Kommandozeilenaufruf unter Angabe eines Git Repositorys starten kann. Der konkrete Datensatz wird dabei z.B. über seinen Pfad als Parameter übergeben. Diese feste Git-Verknüpfung ermöglicht dann auch problemlos, verschiedene Versionsstände eines Modells auszuführen., Das dritte Modul übernimmt die Schnittstelle zu nachgelagerten Technologien und ermöglicht ein vereinfachtes Deployment. „Models“ bietet dabei verschiedene Geschmacksrichtungen von Modellen (wörtlich „Flavors“ genannt). Ein Projekt wird dabei in einer der Flavors in einem Binary-File abgespeichert, z.B. als reine Python-Funktion oder als sklearn-, Keras-, TensorFlow-, PyTorch-, Spark-Mlib- oder H2O-Modell., Zusätzlich gibt es bereits Unterstützung für die Cloud, z.B. für AzureML und Amazons Sagemaker. Ein begleitendes Config-File hilft analog zu „Projects“ dem Konsumenten wieder, das bereitgestellte Modell auf neue Daten anzuwenden., Im Moment kann MLflow primär als Python-Modul genutzt werden. Ich habe die Installation bereits selbst über pip an einem Conda-Environment durchgeführt und die Beispiele aus dem Git Repository ließen sich problemlos starten. MLflow nutzt auch selbst den Conda Package Manager, um für Projekte einen vordefinierten Python-Interpreter vorzuhalten., Die finale Anwendung muss allerdings nicht zwingend über Python passieren, da MLflow neben einem Command Line Interface noch eine REST API bietet. Diese kann sowohl für das Ausführen von Modellen als auch für das Loggen von Infos über den Tracking-Server genutzt werden., Die Plattform ist, wie oben erwähnt, noch in einem frühen Alpha-Stadium. Dennoch lässt sich der Funktionsumfang sehen, und dieser ist erst der Anfang. Die Vision, ein Framework zu schaffen, das andere Frameworks vereinigt, ist sicher nicht neu und nicht zwingend von Erfolg gekrönt. Die angebotene Lösung, ML-Projekte verschiedenster Frameworks so abzukapseln, dass sie agnostisch in Bezug auf die Implementierung verteilt und angewendet werden können, muss sich erstmal beweisen – sowohl im Hinblick auf die Kompatibilität mit Zielumgebungen als auch im Hinblick auf die Performance und Reaktionsschnelligkeit., Mich stimmen dabei aber dennoch zwei Dinge sehr zuversichtlich:, Matei, der verantwortliche Kopf hinter Spark, ist persönlich am Projekt beteiligt und war wohl symbolisch der erste Committer der Codebase. Wir haben es hier im aktuellen Stadium mit einem Open-Source-Produkt zu tun, aber die Crew dahinter hat es mit Spark bereits geschafft, das größte Open-Source-Projekt im Big-Data-Umfeld zu verwirklichen., Ich habe daher keine Angst, dass MLflow nach einer anfänglichen Euphorie im Sand verlaufen wird. Im Gegenteil – ich erwarte, dass im Hinblick auf eine mögliche zukünftige kommerzielle Variante die Plattform schnell die Alpha-Phase verlassen wird., In einem früheren Blogbeitrag hatte ich bereits vor Jahren dafür geworben, Python als zukünftige Lingua franca der Data Science zu erlernen. Die Entwicklung, die Python dabei in den letzten drei Jahren hingelegt hat, stellt jedoch meine kühnsten Erwartungen in den Schatten. Im „Economist“ wurde im Juli erst ein Beitrag veröffentlicht, dass Python sich zur beliebtesten Programmiersprache entwickelt., Wenn man mal von Spark absieht, wo die aktuellsten Features immer zuerst für die primäre Scala-API bereitgestellt werden, scheinen sich die ML-Welt und vor allem auch die Deep-Learning-Frameworks auf Python als gemeinsame Sprache zu einigen. Dass MLflow trotz der Spark-/Scala-Historie der Entwickler von vornherein auf Python setzt, ist für mich ein wichtiges Signal. Python is here to stay., , © 2021 b.telligent;https://www.btelligent.com/blog/mlflow-die-neue-machine-learning-plattform/;B-Telligent;Stefan Seltmann
08.02.2018;            Machine Learning Conference - die Highlights 2018        ;Das Data-Science-Team von b.telligent hat es sich nicht nehmen lassen, an der dreitägigen Machine Learning Conference in München teilzunehmen, um sich über die Themen und Trends von heute zu informieren und mit Gleichgesinnten auszutauschen!, Die Highlights und wichtigsten Erkenntnisse hat Dr. Sebastian Petry zusammengefasst:, Deep Learning und Artificial Intelligence (AI) sind voll im Machine Learning angekommen. Kaum ein Vortrag, der sich nicht mit diesem Thema beschäftigte. Es wurde in vielen Talks klar, dass die Möglichkeiten sehr groß sind, aber das Thema insgesamt noch in einem sehr frühen Stadium ist. Deep-Learning-Projekte sind auch immer Forschungsprojekte. Zum einen sahen wir uns in unserer Wahrnehmung bestätigt, auf der anderen Seite war es schön, unsere Erfahrungen beispielsweise mit Tensorflow bestätigt zu sehen., Wie messe ich Intelligenz bei AI. Eine sehr spannende Frage, für die ein methodologisches Framework vorgestellt wurde. Diese Frage wird mich noch ein wenig beschäftigen, da ich gute Möglichkeiten sehe, dieses Framework für die Evaluation von Bots oder anderen AIs einzusetzen., Data Science und Softwareentwicklung rücken näher zusammen. Smarte Algorithmen werden immer enger mit Produkten verknüpft, was den Data Scientist immer näher an die Softwareentwicklung bringt. Ich bin gespannt, ob sich das Bild des Data Scientists nicht in zwei Richtungen entwickelt:, Ob es angesichts des Arbeitsmarktes genug Data Scientists gibt, die beide Felder in gleichem Maße bedienen können, wird sich zeigen, aber ich sehe hier eine mögliche sinnvolle Trennung., In jedem Fall war die dreitägige Machine Learning Conference in München eine gelungene Veranstaltung mit einem interessanten Vortragsprogramm!, , © 2021 b.telligent;https://www.btelligent.com/blog/machine-learning-conference-in-muenchen/;B-Telligent;Dr. Sebastian Petry
18.01.2018;            Data-Warehouse-Automatisierung (Teil 2: Development)        ;"Große Teile an bisher manueller Programmierung können durch DWA-Tools abgelöst oder zumindest stark vereinfacht werden. Welche Teile der Entwicklung dabei genau automatisiert werden können, kann von Tool zu Tool stark differieren. So gibt es Ansätze von reinen Code-Generatoren, mithilfe derer Datenbankstrukturen und ETL-/ELT-Prozesse automatisch generiert werden können („design-time“). Auf der anderen Seite existieren umfangreiche Integrationssuiten, die den gesamten DWH-Lebenszyklus, von der Bereitstellung der Daten in den Quellen bis hin zu den Data Marts, generieren, aber auch verwalten können („run-time“)., Bei der Entwicklung gibt es eine Reihe von Aufgaben, bei denen ein DWA-Tool unterstützen kann. Im Folgenden wird insbesondere auf die Bereiche Reverse Engineering und Kompatibilität, Analyse, Implementierung und Rahmenbedingungen eingegangen., DWA kann in unterschiedlichen Projektszenarien eingesetzt werden. Mögliche Szenarien reichen von der Erweiterung über die Modernisierung eines DWH bis hin zu einer (Teil-)Neuerstellung., , Mögliche DWH-Projektszenarien, In den meisten Fällen existiert bereits eine DWH-Lösung. Dementsprechend kann es für die Auswahl eines DWA-Tools entscheidend sein, inwieweit eine Kompatibilität mit dem alten DWH gegeben ist. Wo einige Tools das bestehende DWH lediglich als Quelle nutzen, können andere an dem bereits entwickelten Datenmodell anknüpfen und dieses weiterentwickeln., Bestimmte DWA-Tools fahren den Ansatz, ausschließlich auf einer bereits vorbereiteten Datenbasis aufzusetzen („Pre“-Stage). Dadurch wird eine systematische Trennung zwischen der Datenbereitstellung und der Datenverarbeitung verfolgt. Die Stage fungiert somit als Schnittstelle, bei der alle notwendigen Vorbereitungen bereits getroffen wurden. Hier kann die Notwendigkeit weiterer, externer Tools zur Datenmanipulation und zur Beladung der Stage-Tabellen bestehen., Andere DWA-Tools bieten einen ganzheitlicheren Ansatz. Dazu gehören spezifische Konnektoren, mit denen direkt auf Quellsysteme zugegriffen werden kann, sowie Möglichkeiten nachträglicher Datenmanipulationen. Dadurch wird die Chance erhöht, die gesamte Datenverwaltung allein über ein DWA-Tool abbilden zu können., Beim Bereitstellen von Daten aus den Quellsystemen können DWA-Tools gewissen Einschränkungen unterliegen, wie:, Mithilfe einer vorgelagerten Bestandsaufnahme bestehender sowie potentieller Quellsysteme sollten DWA-Tools vorab auf mögliche Kompatibilitätsprobleme und weitere Einschränkungen geprüft werden., Damit Datenbankstrukturen und Datenbewirtschaftungsprozesse automatisiert erstellt werden können, muss die Beschaffenheit von abgebenden Quellsystemen analysiert werden. DWA-Tools bieten zumeist ein Reverse Engineering von Datenbankstrukturen an, bei dem Basisinformationen gesammelt werden können. Dabei muss auch hier wieder die Kompatibilität mit verschiedenen Datenbanken geprüft werden. Die Basis dabei bilden übliche Metainformationen von DBMS wie Spaltentypen, Integritätsbedingungen, Beziehungen, aber auch erweiterte Objektstatistiken. Jedoch existieren in der betrieblichen Praxis nur selten Quellsysteme, die über eine lückenlose und allumfassende Datenbasis verfügen. Sowohl plattformabhängige Optimierungen als auch nutzergetriebene Modifikationen können die automatisierte Analyse von Quellsystemen erschweren. Die Qualität des erzeugten DWH-/ETL-Codes ist jedoch maßgeblich davon abhängig, wie detailliert Metainformationen dem DWA-Tool zur Verfügung stehen. Hier muss das DWA-Tool dem Entwickler möglichst praktikable Werkzeuge an die Hand geben, um fehlende Informationen in die Metadaten nachzutragen. Ein Großteil der gefühlten Usability ist von diesem Aufgabenfeld abhängig, da es sich hierbei um eine der wichtigsten Stellschrauben handelt, über die der Entwickler das Verhalten des Tools beeinflussen kann. So kann die fehlende Berücksichtigung der Bedeutung einzelner Objekte und ihrer Beziehungen zur späteren Restrukturierung ganzer Systemausschnitte führen., Neben der strukturellen Analyse können Datenbestände durch Profiling-Funktionalitäten vorab analysiert werden. Die Unterstützung der DWA-Tools reicht hier vom reinen automatisierten Absetzen verschiedener Abfragen über die anschließende Ausgabe der Ergebnisse bis hin zu Designvorschlägen. So bietet es sich möglicherweise an, Objekte mit großen Datenaufkommen und hoher Änderungsrate im Deltaverfahren und in (Near-)Realtime von den Quellen zu transferieren und andere hingegen als Vollabzug auf Tagesbasis., Weiterhin sollte ein Data Lineage auf DWH-Seite vereinfacht möglich sein, da alle hierfür notwendigen Metainformationen dem Tool bereits vorliegen. Den entscheidenden Unterschied machen dabei die Form der Visualisierung des Lineages und die Möglichkeit des Entwicklers zur Interaktion. Erschwert wird ein lückenloses Lineage durch bestimmte Freiheitsgrade des Entwicklers. Wird an einer bestimmten Stelle des ETL-Prozesses beispielsweise dynamisches SQL oder externer Code prozessiert, ist es für das DWA-Tool (und nebenbei in einigen Fällen auch für Menschen) nahezu unmöglich, den Prozessfluss lückenlos nachzuvollziehen. An dieser Stelle ist es Aufgabe des DWA-Tools und des Entwicklers, ein gesundes Mittelmaß an Freiheitsgraden und Standardisierung zu finden., DWA-Tools sind bis dato noch nicht in der Lage, das eigentliche DWH vollständig zu abstrahieren, sodass es dem Entwickler/Fachanwender wie eine Blackbox vorkommt. Die eigentliche Entwicklertätigkeit innerhalb der verschiedenen DWA-Tools kann sich je nach Grad der Automatisierung in Umfang und Komplexität stark unterscheiden. Je breiter der Einsatz des DWA-Tools gewählt wird, desto mehr nicht- oder teilautomatisierte Eingriffe des Entwicklers können durchgeführt werden:, , Manuelles Coding bei Data-Warehouse-Automatisierungs-Tools, Wie in der Abbildung zu sehen, können über die gesamte DWH-Schichtenarchitektur hinweg manuelle Eingriffe notwendig sein. Beim Übergang von den Quellsystemen in die Abstraktionsschicht müssen sog. „Hard Rules“ definiert werden. Quellspezifische Datentypen müssen möglicherweise konvertiert und harmonisiert werden, oder eine für das DBMS ungeeignete Struktur muss vorstrukturiert werden. Hier können spezifische Fragestellungen und Trade-offs eine Rolle spielen, die eine menschliche Entscheidung unabdingbar machen. Im Optimalfall müssen Hard Rules nur einmal je Quellsystem definiert werden und das DWA-Tool bietet bereits vordefinierte Lösungswege an., Während des Übergangs von Abstraktionsschicht zu Integrationsschicht werden hingegen typischerweise „Soft Rules“ angewandt. Hier finden die eigentlichen Integrationsaufwände statt, die für eine einheitliche und konzernweite Datenbasis, das Core DWH, notwendig sind. Dazu zählt u.a. das Harmonisieren von Business Keys.1 Wird eine Teilung des Core DWH verfolgt, wie es bspw. mit dem RAW- und Business-Vault bei der Data-Vault-Modellierung vorkommen kann, verlagern sich die Soft Rules auf den Übergang vom RAW-Vault in den Business-Vault. Aufgaben wie die technische Historisierung der Daten können hingegen vereinfacht mithilfe des DWA-Tools gelöst werden., Für die Präsentationsschicht werden die Daten so aufbereitet, dass sie in Form und Darstellungsweise den Wünschen des Fachbereichs entsprechen. Ergeben sich Aufgabenstellungen, die die einfache technische Generierung eines Star-Schemas oder einer einfachen View übersteigen, muss auch hier eingegriffen werden. Einige Tools gehen noch einen Schritt weiter und können Datenstrukturen für gängige Reporting-Werkzeuge vorgenerieren., Bei manuellen Eingriffen ist das Synchronisieren dieser mit dem DWA-Tool entscheidend. Eingriffe müssen nicht nur ermöglicht oder vereinfacht werden, sondern in die Arbeitsweise des DWA-Tools eingebettet werden. Andernfalls könnten diese bei erneuter Codegenerierung überschrieben werden oder sogar zu weitreichenderen Komplikationen wie inkonsistenten Daten führen. Beschränkt sich das Tool lediglich auf das Generieren des Codes zum Erzeugen der Datenbankobjekte sowie der Datenbewirtschaftungsprozesse eines bestimmten Systemausschnitts (bspw. das Staging), so müssen wenige Eingriffe getätigt werden. Jedoch muss ein Großteil der notwendigen Schritte mit separater Software, wie einem Datenbankentwicklungswerkzeug, erstellt werden., Im Projektgeschäft kann es vorkommen, dass nicht nur mehrere Entwickler eines Unternehmens, sondern weitere Entwicklerteams anderer Unternehmen an denselben Datenbankstrukturen arbeiten. Dabei sind Metadaten des DWA-Tools genauso betroffen wie die Datenbankstrukturen der DWH-DB, wodurch sich gewisse Anforderungen ergeben. Es muss sichergestellt werden, dass Änderungen eines Entwicklers nicht die Änderungen eines anderen Entwicklers konterkarieren. Um dies zu erreichen, nutzen einige DWA-Tools die Funktionen von Version Control Systems (VCS). Nutzt das DWA-Tool OS-basierte Konfigurationsdateien, können die VCS-typischen Funktionalitäten wie das Ein- und Auschecken von Objekten sowie MERGE genutzt werden, um die verteilte Entwicklung zu erleichtern. Andere DWA-Tools speichern ihre Metainformationen ausschließlich in Datenbankschemata. Hier kann das mehrfache Editieren desselben Objekts durch die GUI des DWA-Tools sowie durch die Transaktionssicherheit der Datenbank reglementiert werden. Über DB-Backups und Flashbacks kann zusätzlich zu einem früheren konsistenten Stand zurückgerollt werden, was jedoch nur bedingt auf Einzelsatzbasis funktioniert., Zu Beginn einer DWA-Initiative sollte feststehen, welche Teilbereiche der Entwicklung vom DWA-Tool abgedeckt werden sollen. Werden über die reine (Teil-)Generierung von Code hinaus bspw. erweiterte Profiling-Funktionalitäten benötigt, macht eine umfangreiche Integrationssuite mehr Sinn, als verschiedene Tools mit teils überlappendem Funktionsumfang zu nutzen. Je allumfassender ein DWA-Tool jedoch ausgestattet wird, desto stärker muss der Entwickler auch mit dem Tool interagieren können. Die wesentliche Herausforderung für DWA-Tools ist dabei, trotz einer immer breiteren Abdeckung an Funktionalitäten eine intuitive und eingängige Haptik zu bieten, sodass Vorteile der Automatisierung überhaupt flächendeckend und konsistent genutzt werden können. Besonders in der Art und Weise, in der die unterschiedlichen DWA-Tools den Entwickler bei der Definition von Soft Rules unterstützen, existiert noch viel Luft nach oben; das könnte in Zukunft zu einem wichtigen Differenzierungsmerkmal werden., Es wäre falsch anzunehmen, dass mit dem Einsatz von DWA-Tools weniger fachspezifisches Know-how für die Entwicklung eines DWH notwendig sein wird. Wie zuvor beschrieben sind weiterhin viele Entscheidungen während der DWH-Entwicklung zu treffen und viele Problemstellungen zu lösen. Dagegen findet eher eine Verschiebung der Aufgabenstellungen statt, weg von technischen Detailfragen wie der Historisierung von Datenbankstrukturen hin zu konzeptuellen Fragestellungen. Die Entwicklungstätigkeit mit einem DWA-Tool ist somit verstärkt davon bestimmt, neue Standards einzuführen sowie bestehende konstant in Frage zu stellen, um die besten Lösungen für ein individuelles DWH-System zu entwickeln. Die Praxis zeigt jedoch auch, dass DWA-Tools gewisse Standards auch hart einfordern, wodurch der Handlungsspielraum beschränkt werden kann. So kann u.a. eine gewisse Layer-Architektur, Modellierungsvariante, Namenskonvention oder auch Beladungsstrategie vorgegeben werden. Nicht selten führt dies zu einem Alignment von Prozessen und Entwicklerverhalten an das DWA-Tool, was besonders von Unternehmen mit bestehenden DWH-Lösungen oder stark ausgeprägter Standardisierung berücksichtigt werden sollte., 1 Wird Data-Vault für die Modellierung des Core DWH verwendet, wird dieser Schritt typischerweise während des Übergangs vom RAW-Vault in den Business-Vault vollzogen., , © 2021 b.telligent";https://www.btelligent.com/blog/data-warehouse-automatisierung-teil-2/;B-Telligent;Dominik Schuster
11.01.2018;            7 Faktoren für eine erfolgreiche Marketing-Automation-Einführung         ;"Durch die verstärkte Nutzung des Internets und des Smartphones ändert sich zunehmend auch das Konsumentenverhalten der User. Mit zunehmender Digitalisierung der Kanäle und Touchpoints ist der Kunde heute besser denn je informiert und damit schwerer zu fassen. Das Konsumentenverhalten auch digital einzufangen, auszuwerten und zu verstehen, dazu muss der Marketingverantwortliche heute zielgenaue Segmente bilden und eine darauf abgestimmte Strategie für potentielle Customer Touchpoints erstellen, um den Kunden auf dem richtigen Kanal zum richtigen Zeitpunkt mit dem richtigen Angebot anzusprechen und ihn auf der anderen Seite nicht mit E-Mails oder Push-Nachrichten zu überfluten., Generell lassen sich Marketing-Automation-Tools vier Hauptaufgaben zuschreiben:, Welche konkreten Schritte sind notwendig, um eine Marketing-Automation-Lösung erfolgreich einzuführen?, Im ersten Schritt macht es Sinn, eine Bestimmung des internen Reifegrads vorzunehmen. Welche Abläufe sind manuell, welche sind bereits teilautomatisiert, welche Prozesse fehlen noch gänzlich? Wohin möchte sich das Unternehmen entwickeln? Auch hier ist es sinnvoll, sich langsam Schritt für Schritt weiterzuentwickeln. Zu einem vollumfänglich selbstlernenden und automatisierten System kann es ein langer Weg werden, sei es in technischer, fachlicher oder organisatorischer Hinsicht., Welche weiteren Module (z.B. für Datenanalyse oder Budgetverwaltung) werden zusätzlich benötigt oder ist die Einführung mit den Standardmodulen möglich? Wie gut sind zusätzliche Module in die Kampagnenlösung integriert? Gibt es möglicherweise Referenzkunden, die Auskunft über den Praxiseinsatz der Lösung geben können?, Bei einer Anbieterevaluierung sollten stets folgende Aspekte im Auge behalten werden: Definition der fachlichen und technischen Anforderungen an die Lösung und die Umsetzbarkeit der anzubindenden Kanäle (z.B. Kanäle wie Call-Center oder Mobile Push). Eine Zusammenstellung eines Projektteams (z.B. aus IT, CRM) kann helfen, Anforderungen zu bündeln, gemeinsame Synergien zu schaffen und eine für alle zufriedenstellende Lösung auszuwählen., Der zeitliche Rahmen des Projekts sollte ebenfalls Beachtung finden (Bis wann soll die Entscheidung fallen, wann soll die Lösung einsatzbereit sein?). Die Planung eines maximalen Budgets inkl. interner und externer Implementierungsaufwände und Lizenzgebühren ist ebenso erforderlich wie die Vorabdefinition der Bereitstellung der Datenbewirtschaftung bzw. der Datenstrategie. Im Voraus erstellte Use Cases sollte der Anbieter, neben mehreren Referenzen, während seiner Präsentation live zeigen., Die Umsetzung der Datenbewirtschaftung der Kampagnenlösung sollte im Voraus festgelegt sein; darunter fallen die z.B. zur Selektion nötigen Kundenstammdaten, Transaktions- oder Vertragsdaten oder auch Bewegungs- und Verhaltensdaten aus dem Bestell- oder ERP-System. Gibt es viele Datenquellen, so sollte über die Erstellung einer zentralen integrierten 360-Grad-Sicht nachgedacht werden. Folgende Grafik stellt die funktionale Referenzarchitektur einer CRM-/Customer-Intelligence-Landschaft dar:, Folgende Checkliste soll helfen, die richtige Marketing-Automation-Lösung unter Berücksichtigung der wichtigsten Faktoren auszuwählen und einzuführen:, Woran misst sich die erfolgreiche Implementierung einer Marketing-Automation-Lösung?, Die Lösung ist noch neu im Einsatz, die Abläufe sind ungewohnt, andere und evtl. neue Personen beteiligen sich jetzt an den Marketing-Prozessen, einiges geht jetzt schneller, andere Prozesse dauern dafür (noch) länger. Nach einer gewissen Eingewöhnungszeit sollte die Einführung der Marketing Automation daraufhin geprüft werden, ob sich bestimmte Dinge gebessert haben:, Sie möchten mehr über das Thema Marketing Automation erfahren oder haben weitere konkrete Fragen zu einer erfolgreichen Implementierung? Kontaktieren Sie uns gerne!, , © 2021 b.telligent";https://www.btelligent.com/blog/7-erfolgsfaktoren-fuer-marketing-automation/;B-Telligent;Laurentius Malter
09.01.2018;            Erfolgsfaktor „Daten“ im Performance-Marketing - Teil 2        ;Im ersten Teil des Blogbeitrags erfahren Sie, wie sich für den Marketeer mit aussagekräftigen Daten bei gleichem Budget mehr Umsatz erzielen lässt. Der zweite Teil des Beitrags zeigt hingegen auf, wie wichtig es ist, den potenziellen Kunden gut einschätzen zu können., Schon immer nutzen Kunden kanalübergreifend dass für sie ideale Informationsmedium. Im Alltag ist der Kunde darüber hinaus ständig weiteren Impulsen aus Radio, TV, City-Lights oder Großflächenwerbung ausgeliefert, die seine Kaufentscheidungen beeinflussen. Um auf Seiten der Anbieter mehr Vergleichbarkeit zwischen den Kanälen und Konsistenz zwischen den Daten-Silos zu schaffen, bildet der Kunde selbst mit seinen Aktionen den logischen Dreh- und Angelpunkt. Cookies, Logins, Ad-ID, Online- und Offline-Einkäufe, Versandadresse, Hotline-Anfragen, Laufwege und viele weitere Faktoren kommen dabei ins Spiel. Auch an dieser Stelle gilt es, die Datenpunkte aufeinander abzustimmen, um ein kanalübergreifendes Bild vom Kunden zu skizzieren., Um den Kunden möglichst genau visualisieren zu können, arbeitet man auch im Performance-Marketing mit Personas beziehungsweise Segmenten, die aufgrund ihrer umfangreichen Beschreibung helfen sollen, sich in die Lage der potenziellen Nutzer zu versetzen. Diese Vorgehensweise hilft dabei, das Verständnis dafür zu schärfen, wie sich der Stammkunde vom Spontankäufer, der Stadtmensch vom Landmensch oder der Single vom Familienvater unterscheidet., Sind die Vorarbeiten erledigt, beginnen die eigentlichen Herausforderungen:, Ein A/B-Testing hilft dabei, die aufgestellten Thesen zu überprüfen, und liefert darüber hinaus wertvolle Datenpunkte von beiden Gruppen, die sich auch in anderen Szenarien auswerten lassen., Mit den daraus gesammelten Erkenntnissen kann der Werbetreibende neuen Blickwinkeln folgen, Daten messen, auswerten, justieren und Maßnahmen anpassen. Durch dieses Vorgehen kristallisieren sich nach und nach Schwellwerte heraus, die das Setup im Detail optimieren. Ein Beispiel:, Mein „Kanal X“ ist mit 200.000 Euro budgetiert: Das Verhältnis Neukunde (NK) zu Bestandskunde (BK) beträgt 40 Prozent zu 60 Prozent. Gemäß den Analysen teilen sich 120.000 Euro für die BKs wiederum auf 15 Prozent spontane Käufer, 20 Prozent „Wenig-Käufer“, 40 Prozent reguläre Käufer und 25 Prozent Stammkunden auf. Analysen ergeben, dass Kanal X auf spontane Käufer und Stammkunden keinen Einfluss hat. Folglich lassen sich 48.000 Euro – 24 Prozent des gesamten Budgets – in wirkungsvollere Kundensegmente oder andere Kanäle transferieren und so deren Wirkung erhöhen., Je nach Unternehmen, Branche und Markt sind unterschiedliche Messpunkte und Feinheiten einzubeziehen. Wer Neukunden in der Messung weniger stark berücksichtigt, da ihr Verhalten zu Beginn aktiver ist und sich zu stark von dem der Bestandskunden unterscheidet, der sollte das bewusst tun – oder sie in der Segmentierung komplett ausschließen. Es bleibt nicht aus, dass Unschärfen bei der Messung der Aktivität über verschiedene Endgeräte hinweg entstehen. Schließlich handelt es sich beim Nutzer eines Tablets nicht immer um ein und dieselbe Person., Die Antwort auf die Frage, ob ich mein Marketing-Budget richtig einsetze, erhalte ich demnach definitiv über das kontinuierliche Messen, Auswerten und Nachjustieren meiner Daten. Dem Marketeer ist es dank der Digitalisierung möglich, nahezu jedes Kundenverhalten klar zuzuordnen oder mit Hilfe von Attributionsmodellen Näherungswerte zu schaffen. Dadurch ist eine Grundlage für ein erfolgreiches datengesteuertes Marketing gegeben., , © 2021 b.telligent;https://www.btelligent.com/blog/erfolgsfaktor-daten-im-performance-marketing-teil-2/;B-Telligent;Christian Endres
14.12.2017;            How-to: CSV nach Kafka mit Python und confluent_kafka (Teil 2)        ;"Im ersten Teil dieses Blogs ging es darum, möglichst einfach eine CSV-Datei nach Avro zu serialisieren und das Ergebnis in Kafka abzulegen, wobei das Schema in der Schema-Registry registriert werden sollte., Dazu haben wir die CSV-Datei mittels csv.DictReader geöffnet. Dadurch haben wir schon ein Dictionary vorliegen, das wir verwenden können, um unseren Datensatz manuell zusammenzubauen. Nehmen wir an, dass in Spalte 1 ein float, in Spalte 2 ein int und in Spalte 3 ein Stringwert steht: ,  Damit haben wir wieder einen funktionierenden Producer. ,  Leider muss dieses Skript für jedes einzelne Schema angepasst werden. Dabei haben wir alle notwendigen Informationen schon vorliegen., Parsen wir also einfach das Schema, das liegt schließlich als JSON vor. Dazu verwenden wir die JSON-Bibliothek,  und parsen Feld für Feld:,  Mittels len(fields'type'0) &gt; 1 wird in dem Fall geprüft, ob ‚type‘ ein Array ist oder ein String. Das kommt daher, dass der Datentyp einer NOT-NULL-Spalte als String {""name"":""column_01"", ""type"": ""string""}, der Datentyp einer NULL-Spalte aber als Liste angegeben wird: {""name"":""column02"", ""type"":""string"", ""null""},  Jetzt können wir prüfen, welchen Datentyp eine Spalte hat, und entsprechend konvertieren. Dazu öffnen wir zunächst wieder wie gehabt unsere CSV-Datei:,  Mittels reader.filenames können wir uns eine Liste aller Headereinträge zurückgeben lassen.,  Und gehen dann Spalte für Spalte vor:,  Das Ergebnis können wir dann wieder an Kafka übergeben:,  Das komplette Skript sieht also wie folgt aus:, , Wie man sehen kann, ist das Python-Confluent-Kafka-Framework eine einfache und doch mächtige Möglichkeit, Daten nach Kafka zu serialisieren., , © 2021 b.telligent";https://www.btelligent.com/blog/how-to-csv-nach-kafka-mit-python-teil-2/;B-Telligent;Oliver Gräfe
30.11.2017;            Drei Ansätze für die Inter-Company-Abstimmung in der Planung        ;Ein wichtiger Baustein für ein Planungssystem ist der Umgang mit dem Thema Intercompany (IC). IC tritt immer dann auf, wenn unterschiedliche Wirtschaftseinheiten eines Unternehmensverbundes untereinander Waren oder Dienstleistungen austauschen. Die daraus resultierenden Aufwände und Erträge, aber auch Forderungen und Verbindlichkeiten müssen transparent und gegeneinander abgrenzbar sein. Da die Planung durch eine Vielzahl von Planungsteilnehmern durchgeführt wird, ist die IC-Abstimmung und auch die darauffolgende IC-Eliminierung nicht trivial., Die Herausforderungen können inhaltlich und prozessual gegliedert werden., Inhaltliche Herausforderung:, Prozessuale Herausforderungen:, Wenn eine Legal-Einheit (LE) einen IC-Geschäftsvorfall plant, wird üblicherweise eine Partnergesellschaft (LE) angegeben. Um den wirtschaftlichen Erfolg der jeweiligen Legal-Einheit beurteilen zu können, müssen im Planungsergebnis und in der Planbilanz die IC-Bestandteile ausgewiesen werden. Um hingegen den wirtschaftlichen Erfolg eines Konzerns oder eines Teilkonzerns beurteilen zu können, ist es wichtig, dass die IC-Bestandteile eliminiert werden, da ansonsten in Summe zu viel Ertrag oder Aufwand in dem Planungsergebnis ausgewiesen wird bzw. zu viele Forderungen oder Verbindlichkeiten in der Planbilanz., Die Abbildung zeigt, an welcher Stelle, in Abhängigkeit der jeweiligen LEs, die Eliminierung stattfinden muss. Die Eliminierung plant in der Regel nicht der jeweilige Planer, sondern sie wird im Planungssystem erzeugt., , Der IC-Abweichungsbericht findet dann Anwendung, wenn die IC-Beziehungen von unterschiedlichen Planern geplant werden und es somit zu Abweichungen kommen kann., Der IC-Abweichungsbericht wertet die Summen sämtlicher IC-Beziehungen aus und macht Abweichungen zwischen IC-Partnern transparent., , Der Abstimmungsprozess lässt sich dadurch steuern, dass in regelmäßigen Abständen anhand der Abweichungen aus dem Bericht die jeweiligen Planungspartner informiert werden und die Aufgabe bekommen, sich bezüglich ihrer IC-Beziehungen abzustimmen., Der erste Ansatz lässt IC-Abweichungen innerhalb des Planungssystems zu, die über einen Abstimmungsprozess behoben werden. Der zweite Ansatz zielt darauf ab, dies zu vermeiden., Die Systematik setzt eine Eindeutigkeit der IC-Beziehungen voraus. Die Eindeutigkeit der Beziehung muss nicht unbedingt eine 1:1-Beziehung sein, über Zuordnungstabellen können auch n:m-Beziehungen abgebildet werden., Der Ansatz vermeidet im Kern die IC-Abweichungen, wodurch der Abstimmungsprozess entfällt. Dies setzt aber implizit voraus, dass in den IC-Prozess kein weiterer Planer eingreift, der eine IC-Abweichung erzeugen könnte., Mit dem dritten Ansatz werden ebenfalls IC-Abweichungen vermieden, allerdings bleibt im Kern die Interaktion zwischen den Planern erhalten., Die Planung von IC-Aufwand oder IC-Ertrag wird bei diesem Ansatz in einen eigenen Prozessschritt ausgegliedert und mit einem Workflow unterstützt. Sobald eine IC-Beziehung erfasst wurde, wird diese bei dem Partner angezeigt. Dieser kann jetzt zusätzliche Informationen beisteuern, wie z.B. die gewünschte Gegenposition. Wichtig ist, dass er im Anschluss die IC-Beziehung bestätigt. Dann erst werden beide Buchungen in die jeweilige Planung übertragen und im Planungsergebnis ausgewiesen., , Wenn die IC-Abstimmung im aktuellen Planungsprozess große Probleme bereitet oder man dabei ist, die Planung grundlegend zu überarbeiten, sollte man darüber nachdenken, den Ansatz für die IC-Abstimmung zu wechseln., Stellt man diese drei Ansätze gegenüber, wird man feststellen, dass jeder einzelne Ansatz seine Vor- und Nachteile hat. Nicht jeder Ansatz ist für jedes Unternehmen geeignet., Bei der Wahl des geeigneten Ansatzes müssen folgende Faktoren berücksichtigt werden:, , © 2021 b.telligent;https://www.btelligent.com/blog/intercompany-abstimmung-in-der-planung/;B-Telligent;Stefan Kersten
15.11.2017;            How-to: CSV nach Kafka mit Python und confluent_kafka (Teil 1)        ;"Auch in modernen Umgebungen ist CSV noch ein häufig anzutreffendes Austauschformat, da viele bestehende Systeme mit moderneren Alternativen nicht umgehen können. Zur Weiterverarbeitung in einer Big-Data-Umgebung sind jedoch andere Formate besser geeignet. Im Zusammenhang mit Kafka ist das vor allem Avro. Avro bietet ein platzsparendes Datenformat mit vielen Features, in dem das Datenschema ebenfalls mit übertragen wird. Um die Handhabung zu verbessern, kann zudem das Schema in einem Schema-Repository registriert werden., Will man das mit Python erledigen, so bietet sich die Bibliothek python-confluent-kafka der Kafka-Entwickler Confluent an., Zunächst muss die Bibliothek python-confluent-kafka installiert werden. Dies schlägt unter Windows fehl, da eine Abhängigkeit nach librdkafka nicht aufgelöst werden kann. Offiziell unterstützt confluent_kafka auch nur macOS und Linux. , Entscheiden wir uns für Debian, kann python-confluent-kafka einfach aus dem Debian-Repository installiert werden. Dafür reicht ein:,  Im Python-Skript müssen wir zunächst die benötigten Bibliotheken importieren:,  Danach können wir das Avro-Schema laden,  und den Avro-Producer konfigurieren:,  Der zweite Eintrag dient dazu, die Adresse der Schema-Registry bekanntzumachen, so dass später das Schema registriert werden kann.,  Mit dieser Konfiguration können wir jetzt unseren Producer erzeugen:,  Jetzt sind wir so weit, dass wir die CSV-Datei öffnen können:, row enthält nun ein Dictionary der Form {‚Headername‘: ‚Spalteninhalt‘},  Dieses können wir direkt dem Avro-Producer übergeben:,  Der Befehl schreibt den Inhalt von row im Avro-Format nach Kafka und registriert das Schema im Repository. Der verwendete Schemaname ist dabei immer &lt;TOPIC_NAME&gt;-data, in diesem Fall also „mein_topic-data“., Das Schema wird dabei auch überprüft. Sollte also das in value_schema übergebene Avro-Schema nicht zu den Daten im verwendeten CSV passen, so wird ein entsprechender Fehler produziert., Das heißt aber leider auch, dass dieses Skript nur funktioniert, wenn alle Spalten des Schemas aus Strings bestehen.,  Das komplette Skript sieht wie folgt aus:, , Das Konvertieren der Daten in andere Datentypen behandele ich im 2. Teil., , © 2021 b.telligent";https://www.btelligent.com/blog/how-to-csv-nach-kafka-mit-python-teil-1/;B-Telligent;Oliver Gräfe
14.11.2017;            Zum neuen Jahr: eine Ode an die logistische Regression        ;Data Scientists halten keine Neujahrsansprachen und Jahresrückblicke. Zu rasant ist die Entwicklung von Spark, KI und Co., als dass wir uns eine Ruhepause gönnen könnten, um einen Moment innezuhalten. Zu rational vielleicht auch die Profession für Gefühlsduselei und rückschauende Sentimentalität. Müsste ich in diesen Tagen jedoch etwas Pathetisches schreiben, ich würde es der logistischen Regression widmen!, Sie ist unser Arbeitspferd und Fixstern. Grundbaustein und gemeinsamer Nenner. Paradebeispiel und sicherer Hafen. Könnte ich drei Modelle auf eine einsame Insel mitnehmen: Eines wäre zweifelsohne die logistische Regression., Hier strahlt sie in ihrer ganzen Schönheit:, Die logistische Regression ist Arbeitspferd, weil sie verlässlich ihre Arbeit leistet, ohne dabei für viel Aufregung zu sorgen. Im Scheinwerferlicht stehen die Rockstars unserer Zunft, wie rekurrente neuronale Netze oder Gradient Boosted Trees. Sie gewinnen die Data-Science-Wettbewerbe, lösen neue (KI-)Revolutionen aus und bedrohen, laut Elon Musk, sogar unsere Zivilisation. In den Maschinenräumen der Industrie läuft häufig jedoch, stillschweigend und beflissen, die logistische Regression. Und das aus guten Gründen:, Schnelles Prototyping: , Die Frequenz der Entwicklungszyklen in (agilen) Data-Science-Projekten steigt. Häufig werden erst sukzessive neue Datenquellen erschlossen und interpretiert. Die logistische Regression kann durch ihre geringe Berechnungskomplexität den Modellierungsprozess wesentlich verkürzen. Zudem dienen die Modellierungsergebnisse als Grundlage und Benchmark für die Entwicklung komplexerer Modelle. Die Gefahr, am Ende des Projekts mit leeren Händen dazustehen, sinkt., Fokus auf Feature Engineering und Data Understanding:, Neuronale Netze und Entscheidungsbäume sind verführerisch, denn sie erledigen das Feature Engineering gleich mit. Der geneigte Data Scientist überlässt die Arbeit dem Lernalgorithmus und lehnt sich zurück. Doch auch hier ist Gefahr im Verzug: Data Science ist Erkenntnisgewinn aus Daten. Dies bedarf der Auseinandersetzung mit Daten, im wahrsten Sinne des Wortes. Die logistische Regression zwingt zur Sorgfalt und führt erfahrungsgemäß zu lukrativen Einsichten in datengenerierende Prozesse, Modellvalidität und Wirkungszusammenhänge., Interpretierbarkeit und Kommunikation:, Das maschinelle Lernen von funktionalen Zusammenhängen bewegt sich häufig im Spannungsfeld von Vorhersagbarkeit und Interpretierbarkeit. Flexiblere Modelle erhöhen in der Regel die Prognosegüte, reduzieren jedoch auch die Interpretierbarkeit der Ergebnisse. Häufig interessieren sich die Stakeholder jedoch gerade für „actionable insights“, also diejenigen Erkenntnisse, die sich konkret in Handlungsanweisungen ummünzen lassen. Die logistische Regression knüpft hier an: Ihre Interpretation über Eintrittswahrscheinlichkeiten erlaubt leicht verständliche Einblicke in das Beziehungsgeflecht beteiligter Variablen., Kommunikation ist wesentlicher Bestandteil jedes Data-Science-Projekts. Komplexe Modelle werden häufig als Black Box wahrgenommen und stoßen dann auf wenig Vertrauen. Die Interpretierbarkeit der logistischen Regression dient also der Akzeptanz von Projektergebnissen., Grundbaustein für Advanced Analytics oder „Wer fliegen will, muss erst lernen zu laufen“, Der Einsatz der logistischen Regression empfiehlt sich gerade beim Aufbau neuer Data-Science-Abteilungen und in deren Weiterbildung. Viele Konzepte können anschaulich und nicht-trivial anhand ihrer Modellmechanik erklärt werden, wie z.B. Gradient Descent, Maximum Likelihood Estimation oder Regularisierung. Vielfach, und oft überraschend, wird sie als Einführung in das maschinelle Lernen verwendet. Mit ihrem geschlossenen Interferenzkonzept lässt sich zusätzlich eine Brücke in die klassische Statistik schlagen., Vielfältigkeit und didaktisches Können zeigt die logistische Regression ganz aktuell in der Entwicklung von neuronalen Netzen. Konzeptionell können tiefe neuronale Netze als hintereinandergeschaltete logistische Regressionen verstanden werden. An sich ist die logistische Regression bereits ein klitzekleines neuronales Netz, was der folgende Computergraph veranschaulicht:, Schalten wir nun eine Vielzahl dieser „Neuronen“ hintereinander und lassen ihren Output als Input weiterer Neuronen agieren, erhalten wir die grundlegende Struktur eines neuronalen Netzes:, Im Kontext von neuronalen Netzen übernimmt die logistische Regression, aka Sigmoid-Funktion, die Rolle einer Aktivierungsfunktion. Die Speerspitze der Aktivierungsfunktionen haben auch hier wieder andere übernommen, aber es ist und bleibt beeindruckend: Hat man die Funktionsweise der logistischen Regression erst verstanden, kann man sich rasch ein Bild von neuronalen Netzen machen., Wer sich in den Hype-Zyklen rund um Data Science, Artificial Intelligence oder Machine Learning zurechtfinden will, ist gut beraten, sich an der logistischen Regression zu orientieren. Trends kommen und gehen, die logistische Regression bleibt. Seit über fünfzig Jahren strahlt sie am Data-Science-Himmel und leuchtet uns den Weg. Dieser Text ist für sie., In diesem Sinne wünscht das Data-Science-Team von b.telligent ein frohes und spannendes Jahr 2018!, , © 2021 b.telligent;https://www.btelligent.com/blog/ode-an-die-logistische-regression/;B-Telligent;Dr. Sebastian Petry
10.11.2017;            Erfolgsfaktor „Daten“ im Performance-Marketing - Teil 1        ;"Im ersten Teil des Blogbeitrags erfahren Sie, wie aussagekräftige Daten für mehr Überblick und Transparenz sorgen. Der zweite Teil des Beitrags zeigt auf, wie sich durch genau diese Daten bei gleichem Budget potenziell mehr Umsatz erzielen lässt., Marketeers stehen heutzutage immer wieder vor komplexen Fragestellungen, wie etwa: Welche Datenanalysen helfen mir, mein Budget effizienter zu nutzen? Und welche Gemeinsamkeiten in puncto Daten helfen mir dabei, verlässliche und aussagekräftige Kennzahlen zu generieren?, Eine gute Datenstruktur ist die Basis für Erfolg im digitalen Marketing. Zukunftsfähige Kampagnen lassen sich nur umsetzen, wenn zuvor das Messen und Optimieren der Daten höchste Priorität hatte. Dabei ist es ausschlaggebend, zu wissen, wie man unterschiedliche Datenquellen miteinander vergleicht und synchronisiert. Arbeitet der Marketeer an dieser Stelle genau und konsequent datengetrieben und richtet seine Maßnahmen an den Bedürfnissen seiner Kunden aus, dann wird er mit dem gleichen Budget mehr Umsatz erzielen oder Budget bei gleichbleibendem Umsatz einsparen. Im Vorhinein müssen allerdings die richtigen Weichen gestellt werden, um ein optimales Ergebnis erzielen zu können., Da sich viele Käufer heutzutage an dem Prinzip „Research online – Purchase offline“ orientieren, hat der Werbetreibende die Möglichkeit, das Kundenverhalten genauer zu analysieren. Liegen alle erforderlichen Daten aus dem Kassen-, dem CRM-System und der Onlinekampagne vor, gilt es jedoch, zusätzlich einige organisatorische Komplikationen in Bezug auf die Datenverarbeitung zu vermeiden. Um die erforderlichen Datenschutzauflagen gewissenhaft zu erfüllen, hilft ein neutraler Datendienstleister, der die regulatorischen, technischen und methodischen Voraussetzungen mitbringt., Jeder Kanal folgt in puncto Kennzahlen seinen eigenen Regeln; sei es der Point of Sale, die sozialen Netzwerke oder auch das E-Mail-Marketing. Ein ausführliches Reporting zeigt in der Regel alle wichtigen Kennzahlen auf einen Blick, doch dabei ergeben sich schnell Probleme mit den verfügbaren Datenquellen. So wird sich jeder Marketeer fragen, ob die Kenngrößen überall gleich definiert und die Aussagen somit untereinander vergleichbar sind., Für das Direkt-Marketing sind vor allem die Öffnungsraten entscheidend, wohingegen die Branding-Verantwortlichen bei Video-Ads auf Unique Visitors beziehungsweise Ad-Impressions und Abspieldauer achten. Es ist also wesentlich, ein Reporting zu erstellen, das einheitliche Aussagen ermöglicht und einen engen Bezug zu den ökonomischen Kenngrößen des Kerngeschäftes herstellt., Eine der schwierigsten Aufgaben ist es wohl, die Begrifflichkeiten und Messmethoden auf einen Nenner zu bringen. So gibt es innerhalb von einzelnen „Transaktionen“ bereits sehr unterschiedliche Momentaufnahmen:, a) Zeitpunkt der Aktion – Kunde löst eine Bestellung aus, b) Zeitpunkt, an dem die Transaktion nicht widerrufen werden kann, Elementar ist es, Messwerte wie Uhrzeit und Datum zu vereinheitlichen, vor allem wenn mehrere Systeme die Rohdaten liefern. Ist die Qualität der Daten zufriedenstellend, ihre Konsistenz sichergestellt und die Analyse aussagekräftig, so lässt sich die Effizienz des eingesetzten Budgets einfacher bewerten., Erschafft man im Vorhinein ein konkretes Bild von dem potenziellen Kunden, fällt die Aussage darüber leichter, für welches Budget und mit welchem Motiv oder Angebot (Creative oder Call to Action) wer genau angesprochen und gewonnen werden soll. Das Verhalten der Kunden, also wie sich wer auf der eigenen Website und im Online-Shop verhält, wird ebenso einfacher vorhersagbar. Dadurch kann der potenziell erzeugte Wert (Customer Lifetime Value) eines Kunden relativ genau bestimmt werden., Wie erstelle ich ein konsistentes und kanalübergreifendes Bild des Kunden und wie optimiere ich mein Media-Budget? Erfahren Sie mehr dazu im zweiten Teil des Blogbeitrags von Christian Endres., , © 2021 b.telligent";https://www.btelligent.com/blog/erfolgsfaktor-daten-im-performance-marketing-teil-1/;B-Telligent;Christian Endres
10.11.2017;            Kickoff zur EU-DSGVO - Was Sie jetzt wissen müssen!        ;, Die EU-Datenschutz-Grundverordnung (DSGVO) ist in den Unternehmen in aller Munde. Denn Stichtag ist der 25.05.2018! Dieser Blogbeitrag soll Ihnen Werkzeuge für Fach- und IT-Bereiche an die Hand geben, um am 25.05.2018 startklar zu sein., Nach Art. 1 der DSGVO handelt es sich dabei um:, Dabei ersetzt die EU-DSGVO nicht das deutsche Bundesdatenschutzgesetz (BDSG), sondern setzt vielmehr darauf auf. Bei der Schaffung der europäischen Verordnung war das BDSG die wesentliche Grundlage, die in einigen Punkten verschärft wurde. Diese Anforderungen gilt es nun zu erfüllen., Die Einführung der EU-DSGVO hat natürlich Auswirkungen für Unternehmen, die mit personenbezogenen Daten arbeiten. Dabei gelten für alle Unternehmen dieselben Vorschriften und Rechte. So müssen beispielsweise Maßnahmen eingeführt werden, um personenbezogene Daten natürlicher Personen bei der Verarbeitung zu schützen und ihre Grundrechte und Grundfreiheiten zu wahren. Das bedeutet, dass Daten schon in allen Systemen und Prozessen, zum Beispiel in einem DWH, datenschutzkonform aufbereitet werden müssen. Personenbezogene Daten können beispielsweise bei der Speicherung und Verarbeitung von, anfallen. Ist dies der Fall, sollten Sie sich die neue Grundverordnung definitiv zu Herzen nehmen!, Die EU-DSGVO ist dabei auch für Unternehmen verpflichtend, die ihren Unternehmenssitz nicht in der EU haben, aber dort Dienstleistungen anbieten. Auch wenn die angebotene Dienstleistung unentgeltlich erfolgt, muss die Verordnung eingehalten werden. Sollte die Verordnung ab dem 25.05.2018 nicht eingehalten werden, werden Strafen in Form von Geldbußen durchgesetzt. Das Gesetz sieht dann vor, Geldbußen in Höhe von 10 bis 20 Mio. Euro oder aber 2 bis 4 Prozent des weltweiten Unternehmensumsatzes zu verhängen., Die neue Datenschutzverordnung enthält Vorschriften für Unternehmen, die sie gegenüber ihren Kunden erfüllen müssen. So haben Kunden zum Beispiel:, Sollten die Rechte der Betroffenen nicht erfüllt werden, drohen die bereits besagten Bußgelder., Außerdem müssen Unternehmen von nun an penibel die Datenverarbeitung dokumentieren. Dazu muss ein sogenanntes Verarbeitungsverzeichnis geführt werden, in dem der Zweck der Datenverarbeitung festgehalten wird. Ebenfalls müssen die Verarbeitung, die Herkunft (Erhebung) und, falls dies zutrifft, die Weitergabe der Daten beschrieben werden. Bitte beachten Sie in den Verarbeitungsprozessen immer, dass der einzelne Betroffene einen Anspruch auf Widerruf hat und gegebenenfalls einzelne Daten wegfallen. Wer darf eigentlich auf welche Daten zugreifen und wurde sichergestellt, dass jeder Zugriff auf einer rechtlichen Grundlage erfolgt?, Zu diesem Zeitpunkt steckt die DSGVO in vielen Unternehmen noch in den Kinderschuhen. Doch die Zeit rennt und nach dem Stichtag gibt es keinen Spielraum, denn die Übergangsfrist ist damit bereits abgelaufen. Die ersten Schritte sollten mindestens Folgendes beinhalten:, Ein Tipp von meiner Seite: Bilden Sie ein unternehmensweites Core-Team, das sich auf die Transformation spezialisiert. Es sollte aus einem, bestehen., Ein jedes Unternehmen, das mit personenbezogenen Daten arbeitet, egal in welcher Form, muss sich den neuen Richtlinien bis spätestens 25.05.2018 angepasst haben. Dazu gilt es alle Unternehmensanforderungen zu beachten, wie zum Beispiel die Einführung eines Verarbeitungsverzeichnisses, sodass Behörden jederzeit die Datenverarbeitung prüfen können. Setzen Sie sich am besten noch heute mit dem Thema auseinander, bevor es Sie einholt!, , © 2021 b.telligent;https://www.btelligent.com/blog/kickoff-zur-eu-dsgvo/;B-Telligent;Björn Ouni
03.11.2017;            Next Best Activity in der Energieversorgung: Herausforderung und Chance im Inbound-Call-Center        ;Interaktionen mit Kunden im Bereich der Energieversorgung sind selten – ein einziger Kontakt im Jahr ist keine Seltenheit. Entsprechend wichtig ist die bestmögliche Steuerung des Kontakts. Im Fokus des vorliegenden Projekts steht der Kommunikationskanal Inbound-Call-Center., Sie möchten mehr über den NBA Advisor von b.telligent erfahren? Schauen Sie einfach auf unserer Website vorbei!, Im Status quo erfolgt die Steuerung der Call-Center-Agenten über grundsätzliche, generische Handlungsanweisungen, die um aktuelle strategische Initiativen und vertriebliche Kampagnen ergänzt werden. Die gelebten Prozesse führen in vielen Situationen zu Handlungen der Agenten, die aus Deckungsbeitragssicht oder Perspektive der Kundenbindungsstrategie suboptimal sind:, Das Next-Best-Action-System (NBA) ermöglicht hier, Agenten im Customer Management mit auf den Kunden individualisierten Aktivitäten und Informationen zu unterstützen und bewusst zu führen. Eine (Next-Best-)Aktion1 kann den Agenten grundsätzlich zu einer Service- oder einer vertrieblichen Tätigkeit oder Ansprache auffordern., Der NBA-Ansatz ermöglicht, unter allen für einen einzelnen Kunden potentiell möglichen Aktionen nach deren Priorisierung die beste auszuwählen und im Moment der Interaktion im Kommunikationskanal auszuspielen. NBA realisiert damit im Customer Management und Direktmarketing einen Wechsel von der isolierten, kampagnenzentrischen hin zu einer kundenzentrischen Sichtweise. Der NBA-Prozess besteht im Grundsatz aus fünf Säulen, die folgende Schwerpunkte enthalten:, , , Im vorliegenden Fall wird dem Call-Center-Agenten eine Auswahl der drei besten möglichen Aktionen im Call-Center-User-Frontend angezeigt. Sobald der Agent das Serviceanliegen des Kunden gelöst hat, soll ein Einstieg in eine der drei besten Aktionen erfolgen. Eine Next-Best-Aktion liefert dem Agenten:, , Ergänzend dazu wird das Customer-Self-Care-Onlineportal mit NBA-gesteuerten Teaser-Platzierungen bespielt. Ziel ist die abgestimmte Ansprache des Kunden auf den verschiedenen Kanälen. Hier bietet NBA die zentrale Intelligenz, die dem Kunden eine kontaktkanalübergreifend konsistente Customer Journey und Ansprache mit Aktionen und Kampagnen liefert., Einführung einer Next-Best-Action-Logik zur individuellen, gezielten und analytisch priorisierten Kundenansprache mit den Zielen:, Die Ausspielung der NBAs erfolgt reaktiv in den Kanälen Inbound-Call-Center und Online-Customer-Self-Care., Der Projektansatz gliedert sich in untenstehende Phasen. Abhängig von der Kapazität der Projektteams ist eine Einführung von NBA MVPs in vier bis acht Monaten umsetzbar., , Die Systematik NBA erfordert Innovation und Change Management in Kampagnenmanagement-Prozessen End-to-End. Beginnend mit der Konzeption von einzelnen Aktivitäten bis hin zu deren Darstellung und Responsemessung im Frontend der Agenten im Call-Center werden Teams über Organisationseinheiten hinweg vor besondere Herausforderungen gestellt:, NBA realisiert eine effektive und effiziente Steuerung der Ansprache eines Kunden mit Aktionen und Kampagnen reaktiv im Kundenkontakt. Jedoch bergen die Voraussetzungen der Vorberechnung und analytischen Priorisierung von Aktionen und Vorschlägen eine gewisse Komplexität in der Umsetzung, die in der Organisation ein Umdenken und einen Perspektivenwechsel in den beteiligten Teams bedingt. Wird dieser Change jedoch erfolgreich gelebt, bietet NBA perspektivisch die Chance, Kundenkontakte auch proaktiv in Outbound-Kampagnen mit optimierten NBA-Vorschlägen zu steuern – ein Kampagnenportfolio und Customer Management in einem dauerhaft optimierten Zustand sind damit möglich!, , © 2021 b.telligent;https://www.btelligent.com/blog/next-best-activity-im-inbound-call-center/;B-Telligent;Dr. Wolfgang Leußer
03.11.2017;            Die Highlights des Vortragsprogramms an Tag 2        ;, Dr. Christian Elsässer von Swiss Re stellte in seinem Vortrag „Der Wert von externen Daten für Predictive Analytics im B2B-Umfeld am Beispiel von Swiss Re“ einige Lessons Learned bei der Entwicklung der Global Motor Risk Map vor. Hier fließen zum Beispiel Bevölkerungsdaten, Nachtlichtemissionsdaten (als Proxy für wirtschaftliche Aktivität), Daten zum Wetter und Informationen über das Straßennetz ein. Diese Map bietet die Swiss Re Erstversicherern an, um Unfallrisiken auf regionaler Ebene besser einschätzen zu können., Joachim Gaschler von Concardis und Andreas Kulpa von DataLovers stellten in ihrem Beitrag „Predictive Alerting im Kreditmanagement am Beispiel der Concardis GmbH“ eine Alerting-Lösung vor, die Concardis als Anbieter von bargeldlosen Bezahl-Services zur besseren Einschätzung von Kreditausfallrisiken verhilft. Hierbei geht es um mögliche Rückforderungen von Endkunden, die (in der Regel) 120 Tage nach der Transaktion stattfinden können, etwa wenn ein Produkt nicht geliefert wird. Die Erstellung der Alerting-Lösung basiert auf Google Alerts und es kommen verschiedene Algorithmen zum Einsatz, die teils routinemäßig gegeneinander getestet werden (Word2Vec, neuronale Netze und Random Forest). Concardis hat somit eine Art Frühwarnsystem dafür, welche ihrer (über hunderttausend) Kunden sie sich genauer ansehen sollten., Christian Deger von AutoScout24 stellte in seinem Vortrag „Predictive Analytics for Vehicle Price Prediction – Delivered Continuously at AutoScout24” die Umgebung und Methodik vor, auf der die geschätzten Preise basieren, die AutoScout24 seinen Kunden zur Verfügung stellt. Spannend war, was auf der technologischen Seite zum Einsatz kam, um das analytische Modell des Data-Science-Teams (R-basierte Schätzung, Random Forest) in eine sich kontinuierlich aktualisierende Preisschätzung für eine Web-Anwendung zu integrieren, auf die die Webseiten-Nutzer Zugriff haben., Dr. Markus Groß von INWT Statistics stellte unter dem Titel „Nach der Wahl ist vor der Wahl: Predictive Analytics für Wahlprognosen“ eine Analyse zur Bundestagswahl 2017 vor, die auf gecrawlten Umfrageergebnissen der Befragungsinstitute und Monte-Carlo-Simulationen basiert. Hierbei ging es um eine Makro-Prognose zur Bestimmung von Wahrscheinlichkeiten für bestimmte Ereignisse bezüglich des Wahlausgangs und weniger um die genauen Wahlergebnisse. , , In der Abschluss-Keynote der Predictive Analytics World „SpiegelMining – Data Science auf Spiegel Online“ stellte David Kriesel Ergebnisse seiner Analyse von fast 100.000 Spiegel-Online-Artikeln seit Mitte 2014 vor. Neben ein paar Fun Facts und deskriptiven Statistiken zur Anzahl der Artikel im Zeitverlauf gab es auch viele bunte Einblicke in die Nähe verschiedener Themen zueinander. Als heikel wurde das „Feature“ dargestellt, ob ein Artikel kommentiert werden darf, und hierzu wurden Daten im Zeitverlauf gezeigt. Dass es nach dem ersten öffentlichen SpiegelMining-Vortrag von David Kriesel im Dezember 2016 zu einem relativen Anstieg der kommentierbaren Artikel kam, wurde miteinander in Zusammenhang gebracht. Um darin eine Wirkung des Vortrags zu sehen, fehlt – würde ich sagen – der Beweis, aber vielleicht hat die Spiegel-Online-Redaktion für diese Schlussfolgerung auch noch ein paar „Small Data“ (in Form von Gesprächen) beigetragen., Insgesamt eine sehr spannende Konferenz!, , © 2021 b.telligent;https://www.btelligent.com/blog/predictive-analytics-world-2017-tag-2/;B-Telligent;Dr. Barbara Hofmann
26.10.2017;            Meine Highlights von der Predictive Analytics World 2017 - Tag 1        ;"Die Predictive Analytics World ist die Konferenz im deutschsprachigen Raum, wenn es um Data Science und Predictive Analytics geht. Das diesjährige Motto „Das Erwachen der künstlichen Intelligenz“ weckt hohe Erwartungen; wenn man „künstliche Intelligenz“ nicht nur als Marketingwort für „Data Science“ verstehen will, kann damit nur ein Themenschwerpunkt im Bereich tiefe neuronale Netze gemeint sein. Indes, ein Themenschwerpunkt ist es noch nicht, was sich dazu im Programm findet – von den 18 Vorträgen am ersten Tag beschäftigen sich bei großzügiger Zählung drei schwerpunktmäßig mit tiefen neuronalen Netzen., Einer davon ragte allerdings heraus, nämlich der Vortrag von Dr. Ralph Grothmann: „Deep Recurrent Networks: Theory and Industrial Applications at Siemens“. Das Interesse war groß. Die rund 35 Sitzplätze in dem Raum waren bis auf den letzten Platz belegt, etwa zwanzig Zuhörer folgten dem Vortrag stehend, und weitere wurden wegen Überfüllung an der Tür abgewiesen. Der konventionelle Vortragseinstieg mit sattsam strapazierten Unterscheidungen von „Predictive vs. Prescriptive Analytics“ wurde bald gefolgt von sehr viel spannenderen Inhalten. Dr. Grothmann zeigte sehr anspruchsvolle Anwendungen vor allem für industrielle Großanlagen, aber auch z.B. die Vorhersage von Türversagen in ICEs. Es waren Anwendungen, die eher für klassische Data-Science-Methodik prädestiniert waren, wenn man mal davon absieht, dass man bei einigen wohl bezweifeln muss, dass man sie mit „klassischen Methoden“ (Ensemblemodelle, Support Vector Machines, statistische Regressionsansätze, Zeitreihenanalyse etc.) noch mit vertretbarem Aufwand hätte lösen können., Aber noch interessanter als die Anwendungen war die Methodik, mit der sie angegangen wurden. Anders als viele Protagonisten im Deep-Learning-Umfeld, die oft nach dem Motto verfahren: „Was schert mich die Businesslogik, wenn du mir genug Daten gibst“, betonte Dr. Grothmann die Wichtigkeit des Verständnisses der meist ingenieurwissenschaftlich-physikalischen Logik der Aufgabe. Sein Ansatz folgt der Idee, das neuronale Netz eng anhand der Anwendungslogik zu konstruieren. Die dabei entstehenden Netze sind typischerweise (je nach Anzahl der Eingangsvariablen) deutlich kleiner als die oft riesigen Netze, die z.B. in der Bildverarbeitung verwendet werden, eher hunderte bis tausende Knoten, nicht die hunderttausende, die man in typischeren Deep-Learning-Anwendungen sieht., Der Vortrag wurde von einer sehr interessanten Frage-Antwort-Session abgeschlossen, die unter anderem auch sehr pointierte Einsichten zur Vermeidung von Overfitting lieferte („Regularisation is the old school answer!“). , Unter dem Strich hat sich durch diesen Vortrag allein schon die Anreise nach Berlin gelohnt. Den Ansatz, Wissen über das Anwendungsproblem nicht zugunsten von Bergen von Trainingsdaten zu ignorieren, sondern in den Aufbau der zu verwendenden neuronalen Netze einzubeziehen, halte ich für sehr sinnvoll und erfolgversprechend, wenn man Deep Learning in die klassische Predictive Analytics bringen möchte, und ich freue mich darauf, ihn in eigenen Projekten zu erproben., , © 2021 b.telligent";https://www.btelligent.com/blog/predictive-analytics-world-2017-tag-1/;B-Telligent;Dr. Michael Allgöwer
25.10.2017;             Industrie 4.0 und Business Analytics – mehr Raum für Innovationskraft        ;Die Welt der mittelständischen Unternehmen bewegt derzeit vor allem ein Thema: Industrie 4.0. Denn der Druck wächst enorm, da der Anschluss an die Digitalisierung nicht verpasst werden darf, um auch langfristig erfolgreich am Markt interagieren zu können. Was die meisten Unternehmen aber nicht wissen: Es müssen nicht sämtliche internen Abläufe über Bord geworfen werden, um bei den Themen Big Data und Industrie 4.0 mithalten zu können. Viel wichtiger ist es, sich die Neugier seiner eigenen Mitarbeiter zu Nutze zu machen und die Datenkultur in alle Abteilungen zu tragen. Erfahren Sie in diesem Blogbeitrag, wie auch Sie den Wandel zur Digitalisierung Schritt für Schritt meistern können., Von Zeit zu Zeit stehen Unternehmen vor der Herausforderung, ihre internen Prozesse zu optimieren. Bevor allerdings erste Maßnahmen ergriffen werden, sollte man sich im Klaren sein, welche Ziele, Absichten und welcher Nutzen damit verfolgt werden sollen. Möchte ich beispielsweise meine Umsätze erhöhen oder das Verhalten meiner Käufer besser verstehen? Dies gilt vor allem auch für die Einführung einer Business-Intelligence-Strategie. Es lohnt sich daher, zu Beginn der Zusammenarbeit mit dem Kunden einen Workshop aufzusetzen, in dem genau diese Faktoren gemeinsam erarbeitet und definiert werden sollen. Auf diese Weise gelangt man meist viel schneller zu Ergebnissen, auftretenden Missverständnissen wird vorgebeugt, und neue Denkweisen werden diskutiert, auf die man selbst meist gar nicht gekommen wäre. Wichtig: konzentrieren Sie sich nicht nur auf das Optimieren von bestehenden Prozessen, sondern versuchen Sie, in neue Richtungen zu denken. Das elektrische Licht ist nicht durch die Optimierung des Kerzenlichts entstanden., Immer wieder ein Thema: die allseits bekannten „low hanging fruits“. Hier verdient das eigene CRM-System eine besondere Aufmerksamkeit. Datenleichen sind ein No-Go, weshalb das System stets gepflegt werden will. Erst im nächsten Schritt kann ich mir überlegen, wie ich es mit weiteren Kunden- oder Produktionsdaten verknüpfen kann oder welche Trends und bereits bestehenden Verbindungen mir weiterhelfen. Ist das CRM-System so weit in Schuss, kann ich wundervolle Insights zu meinen Kunden herausfinden: Bricht zum Beispiel ein Kunde im Online-Shop einen Kauf ab, weil eine bestimmte Zahlungsart nicht angeboten wird? Gibt es andere Gründe, warum Abschlüsse nicht zustande gekommen sind? Doch die Möglichkeiten ohne weitere Hardwareintegration sind begrenzt. Hier hilft uns das Stichwort IoT weiter: Durch dieses Helferlein fällt eine Verknüpfung mit den Bestandsdaten und eine Auswertung wesentlich leichter, als komplett neue Daten zu sammeln. Das heißt: genau hinschauen und gezielt neue individualisierte Angebote entwickeln., Erst einmal auf der unternehmenseigenen Website integriert, sind Chatbots ein wahrer Segen – für Kunden und Unternehmen. Interessierte Kunden müssen nicht die Internetseite nach Informationen durchforsten, sondern können ihre Frage direkt in den Chatbot eingeben, der ihnen dann sofort eine Lösung ausspuckt. Das Beste daran: Unternehmen werden auf diese Weise gleichzeitig im Kundenservice stark unterstützt. Besonders Sinn macht ein Chatbot vor allem dann, wenn er mit den Daten im DWH verbunden ist: So wird nicht sofort ein Servicemitarbeiter mit der Anfrage konfrontiert, nur weil diese über die Logik des Systems hinausgeht. So können komplexe Fragen über den Bestellstatus oder Angebotsdetails schnell abgerufen werden. Ein klarer Mehrwert für den Kunden. Hierzu ein Best-Practice-Beispiel von Microsoft: Auf der Plattform Microsoft Azure funktioniert das NLU (Natural Language Understanding) mittlerweile sogar so gut, dass Kunden ihre Fragen nicht mehr in einer strengen Syntax oder „Chatsprache“ stellen müssen, sondern der Chatbot den richtigen Kontext erkennt und eine passende Lösung anbietet, insbesondere, wenn dieser mit der Cloud verbunden ist. Das Beste daran: Mit dieser Maßnahme erzielen Sie schnell gute Ergebnisse und das Ganze kann mit wenig Aufwand integriert und muss nicht komplett selbst entwickelt werden. Mit der Auswertung des Kundenfeedbacks gelingt es Ihnen dann noch, Ihre eigenen Prozesse zu verbessern und anzupassen., Im Rahmen einer Business-Intelligence-Beratung bekommen unsere Kunden eine neue Sicht auf ihre Daten. So entstehen häufig Ideen für Services, die vorher so nicht möglich waren. Zur Steigerung der Kundenzufriedenheit kann ein Einblick von außen in die Produktionsstraße dem Kunden Transparenz vermitteln und beispielsweise Informationen darüber liefern, wann ein Produkt geliefert wird. Seit Jahrzehnten stecken deutsche Unternehmen viel Mühe in die Planung und Organisation der Liefer- und Produktionsketten, weshalb viele deutsche Mittelständler nicht umsonst in ihrem Bereich Weltmarktführer sind. Deswegen sollten Sie im ersten Schritt Ihre vor- und nachgelagerten Prozesse betrachten, um in Zeiten der Digitalisierung und Globalisierung dem Druck, die eigenen Kunden im höchsten Maß zufriedenzustellen, gerecht werden zu können., Im Rahmen unserer Workshops erleben wir immer wieder, wie durch eine gesunde Motivation der Mitarbeiter schnell gute Ergebnisse erzielt werden können. Bereits durch eine initiale Analyse von unterschiedlichen Daten gewinnen Mitarbeiter aus den verschiedenen Fachabteilungen im Rahmen eines Workshops erste Erkenntnisse und erkennen so schnell Möglichkeiten, was mit welchen Abfragen und Analysen erreicht werden kann und welchen Nutzen diese haben. Auf diese Weise entwickeln alle Beteiligten eine gesunde Neugier, wodurch die neuen Maßnahmen aus eigener Motivation weiter mitgetragen werden. Erkennen Sie das Verbesserungspotential Ihrer Maßnahmen und gelangen Sie gemeinsam schnell zu guten Ergebnissen., Um die gewonnene Motivation und Neugierde der Mitarbeiter auch weiterhin aufrechtzuerhalten, macht es in jedem Fall Sinn, bereits von Anfang an Projektverantwortliche zu benennen. Denn es können weitere Erfolge erzielt werden, wenn es jemanden gibt, der die Prozesse durch seine Entscheidungsbefugnis sowie die entsprechende Budgetverwaltung weiter vorantreiben kann. In unseren diversen agilen BI-Projekten konnten wir feststellen, dass die Fachbereiche sehr gute Ergebnisse erzielen, wenn sie selbstständig Daten auswerten und daraus neue Erkenntnisse gewinnen können – natürlich mit dem richtigen Werkzeug. Wird diese Kultur gelebt, wird im besten Fall der Erfolg des Projektes in andere Fachbereiche getragen, wodurch übergreifende Unternehmensziele konsequenter weiterverfolgt und auch erreicht werden., In der Zusammenarbeit mit unseren Kunden und vor allem während der Workshops legen wir immer großen Wert darauf, ein ganzheitliches Denken zu initiieren. Nur wenn man von Anfang an eine ganzheitliche Sicht bewahrt, können Fehler im Projekt vermieden werden, bevor sie auftauchen. Es passiert immer wieder, dass die Datenstrategie zu eng gewählt wird. Das kann dazu führen, dass das Datenmodell später wieder angepasst werden muss. Schauen Sie mehr auf das Große und Ganze als nur auf den einzelnen Fachbereich, um wichtige Verbindungen zwischen den Abteilungen frühzeitig zu erkennen und zu berücksichtigen., Die After-Sales-Phase wird in vielen Unternehmen unterschätzt. Dabei kann gerade hier die Kundenloyalität gestärkt werden. Mit dem richtigen Einsatz von Industrie 4.0 und Business-Analytics-Maßnahmen erhalten Sie ein besseres Verständnis für Ihre Kunden und haben zu jeder Zeit die richtige Antwort auf Fragen wie ‚Welches Produkt ist das richtige und wie kann ich dafür sorgen, dass der Kunde auch nach der Lieferung zufrieden bleibt?‘ oder ‚Wie kann ich meinen Kunden immer neue und bessere Dienste anbieten?‘. Bei der Auswertung des Kundenfeedbacks muss jedoch langfristig gedacht werden: Die Kundenloyalität kann nicht von heute auf morgen eine 180°-Wende hinlegen. Doch auf lange Zeit gesehen werden Kundenbeziehungen gestärkt, und dies bringt klare Wettbewerbsvorteile für die Zukunft., Ziel eines jeden Unternehmens sollte es sein, den manuellen Aufwand von Prozessen möglichst gering zu halten. Chatbots und die Etablierung eines DWH sowie das Anbieten von weiteren Services sind in jedem Fall Schritte in die richtige Richtung. Mit automatisierten Auswertungen und neuen Systemen gelingt es Ihnen darüber hinaus, die Fehlerquote maßgeblich zu senken. Auf diese Weise werden die Prozesse für alle Beteiligten vereinfacht, und gleichzeitig wird freie Zeit für Ideen und Innovationen geschaffen., , © 2021 b.telligent;https://www.btelligent.com/blog/industrie-40-und-business-analytics-mehr-innovation/;B-Telligent;Dr. Sebastian Petry
04.10.2017;            Von wegen untypisiert!        ;"Die Behauptung „Python ist keine typisierte Sprache“ erhöht meinen Puls mittlerweile genauso wie das vor vielen Jahren übliche „Python ist nur eine Scriptsprache“., Man muss nur schnell eine Python-Konsole öffnen und 1+""1"" eingeben. Das Ergebnis ist nicht, wie z.B. bei PHP, eine 2, sondern ein TypeError. Python ist sehr wohl stark typisiert und unterscheidet zudem auch noch zwischen mutable und immutable types. Da der Code in Python erst zur Laufzeit kompiliert wird, fällt der gerade genannte Additionsfehler an sich nicht beim Programmieren, sondern erst bei der Ausführung auf., , In Python soll trotz starker Typisierung zur Laufzeit in der Programmierung keine zwingende Festlegung auf einen Variablentyp erfolgen. Man folgt dem Prinzip des Duck-Typings. Wenn ein Vogel „Quack“ machen kann, dann wird es wohl eine Ente sein, egal ob Stockente oder Kragenente. Genauso ist es zunächst egal, ob eine Variable einen Integer- oder Float- oder Double-Wert beinhaltet, wenn ich addieren und multiplizieren will., , Den Variablentyp bereits beim Programmieren zu erkennen, führte vor über 30 Jahren zu Coding-Standards wie der „Hungarian Notation“, bei der man den Typ dem Variablennamen als Prefix beifügt. Zum Beispiel nCustomers soll dem Entwickler mitteilen, dass es sich um eine numerische Kundenzahl handelt. Dieses aus heutiger Sicht kuriose Relikt aus dem letzten Jahrtausend ist durch moderne Entwicklungsumgebungen natürlich längst überflüssig. Dennoch habe ich das ein oder andere Unternehmen gesehen, das es tatsächlich noch lebt. Bitte nicht, das gehört ins Museum., Heute werden Entwickler durch ausgefeilte IDEs unterstützt. Ich habe für Python vor 10 Jahren mit der Eclipse-IDE angefangen, bin irgendwann auf die Wing-IDE umgestiegen und schließlich vor rund 3 Jahren bei PyCharm von Jetbrains gelandet. Seitdem ich nun das Jetbrains-Package auf dem Rechner habe, benötige ich keine anderen IDEs mehr, nicht mal für SQL. Für Python bietet PyCharm immer die volle Unterstützung der neuesten Features und ab Python 3.5 eben auch für Type-Hinting., Das neue Type-Hinting ist von den Python-Entwicklern, allen voran Guido von Rossum, explizit nur als optionale Unterstützung gedacht. In der PEP484 steht: “It should also be emphasized that Python will remain a dynamically typed language, and the authors have no desire to ever make type hints mandatory, even by convention.”, , Der hier gezeigte Code enthält eine einfache Beispiel-Klasse mit einer Reihe von Funktionen., , , Was passiert, wenn ich mich nicht daran halte?, , , PyCharm interpretiert laufend in Echtzeit den Code, bereitet die Type-Hints auf und gibt dem User Hinweise auf mögliche Fehler, ohne den Code ausführen zu müssen., Die IDE geht hier sogar noch einen Schritt weiter und interpretiert den Code nach gewissen Regeln. Nachfolgender Code zeigt z.B., wie mit dem Parameter ""base"" im Methodenaufruf umgegangen wird. Es ist zunächst kein Typ angegeben, aber die IDE erkennt am Code der nächsten Zeile, dass damit multipliziert wird. Also wird abgeleitet, dass, im Sinne von Duck-Typing, es zunächst egal ist, was übergeben wird, solange es die buildin-Methode für Multiplikation ""__mul__"" beinhaltet., , , Die Methodenansicht im letzten Screenshot erhält man übrigens am schnellsten, wenn man eine Methode oder Funktion mit dem Cursor selektiert und einfach STRG+Q drückt. Alternativ kann man auch beim Tippen, sobald man die Klammer für die Parameter geöffnet hat, STRG+P drücken und man erhält sofort einen Tooltip mit den Parametern und den dazu definierten Typen. Wie hier:, , Natürlich ist das nicht notwendig. Wie oben erwähnt, dient das Type-Hinting nur zur optionalen Unterstützung. Wenn der Python-Interpreter den Code ausführt, ignoriert er alle Type-Hints gnadenlos, da sie nur als Hinweise für die IDEs gedacht sind, und schiebt sie in ein Metadatenobjekt namens ""__annotations__""., Ich persönlich sehe aber eine große Chance darin, einen Code präziser zu dokumentieren. Immer wenn ich Type-Hints setze, dann teile ich sofort anderen Usern und mir in der Zukunft mit, wie der Code optimal zu bedienen ist. Natürlich gab es das schon immer auch über Code-Kommentare, aber diese hatten wenig Konsequenz darin, schon beim Entwickeln auf Fehler hinzuweisen., Python in der Data Science bedeutet für mich Pandas und PySpark. Ich persönlich schreibe meinen Code immer in einem Editor in PyCharm und schicke den Code dann per Tastenkombination selektiv zum Interpreter. Hier habe ich ständig das Problem, dass die Autovervollständigung hinterherhinkt, da die verwendeten Methoden nicht immer die Rückgabewerte korrekt angegeben haben. In Pandas z.B. passiert es schnell, dass ein Dataframe nicht mehr im Editor als solches erkannt wird. Dafür kann die IDE nichts, wenn das importierte Modul die Metadaten nicht mitliefert. Sobald ich aber hier und da ""df: DataFrame = ..."" statt einfach nur ""df = ..."" schreibe, nimmt die IDE den Faden sofort wieder auf und bietet mir in der Autovervollständigung wieder alle Infos., , PyCharm deckt noch nicht alle Fälle von Type-Hinting mit User-Hinweisen ab. Das heißt, der Editor unterstützt zwar alle neuen Schreibweisen, verhält sich aber noch etwas konservativ, was Fehlerhinweise angeht. Man muss auch bedenken, dass die Vorschläge für das eindeutige Format von Type-Hints zwar schon recht alt sind, aber die Unterstützung erst mit Python 3.5 eingeführt wurde. Selten findet man in Unternehmen eine so aktuelle Python-Installation vor, so dass es erstmal ein Thema der eigenen Entwicklungsumgebung sein wird. Bei den quartalsweisen Updates von PyCharm ist aber zu erwarten, dass die Entwicklerunterstützung stetig weiter reift., Die Funktionalität des typing-Moduls reicht noch viel weiter als das, was ich hier vorgestellt habe. Mehr Infos findet man direkt in der Python-Doku unter docs.python.org/3.6/library/typing.html ., , © 2021 b.telligent";https://www.btelligent.com/blog/von-wegen-untypisiert/;B-Telligent;Stefan Seltmann
07.09.2017;            Anlage externer HANA-Views        ;Wenn ein BW-System eine SAP-HANA-Datenbank im Einsatz hat, ist es möglich, die sogenannten „Mixed Scenarios“ zu nutzen. Bei solchen Szenarien kann man Daten zusammenspielen, die sowohl im BW-System als auch in SAP HANA mit SAP-HANA-Werkzeugen modelliert wurden., Es ist möglich, HANA-Views von InfoCubes, DataStore-Objekten, InfoObjekten und Queries als InfoProvider und CompositeProvider in BW-SAP-HANA-Views zu generieren. Dafür muss nur das entsprechende Häkchen im HANA Studio gesetzt werden., , Nach der erfolgreichen Aktivierung wird ein HANA-View angelegt. Standardmäßig landen die neu erzeugten HANA-Views im HANA-Content-Ordner (Paketstruktur des Content-Pakets: system-local.bw.bw2hana)., , In der Transaktion RS2HANA_VIEW kann jedoch ein anderer Ablageort definiert werden., , Des Weiteren können hier generelle Spezifikationen zur Erstellung der HANA-Views sowie zum Aufbau/zur Replikation von entsprechenden Berechtigungen eingestellt werden., Welche HANA-Views aus einem BW-System generiert worden sind und welchem Status sie unterliegen, kann in der Transaktion RS2HANA_ADMIN überprüft werden., , Hier können schon die vorhandenen HANA-Views repariert werden, wenn z. B. Änderungen an dem darunterliegenden Objekt durchgeführt wurden., Über den CompositeProvider ist es möglich, auf SAP-HANA-Views zuzugreifen. Dazwischen wird immer eine Calculation View gebaut, da die vom System automatisch generierten HANA-Views nicht geändert werden sollten. Dafür reicht im Prinzip eine einfache Calculation View., , Natürlich können hier beliebig komplexe Datenmodellierungen mit allen Funktionalitäten der HANA-nativen Werkzeuge aufgebaut werden., Die HANA-Views dürfen dann ganz normal zu einem CompositeProvider in BW hinzugefügt werden, womit sich der Mixed-Scenario-Kreis wieder schließen lässt., , Mit dem SAP HANA Transport for ABAP (HTA) können Sie Objekte von ABAP-for-SAP-HANA-Applikationen transportieren. Das sind ABAP-Applikationen, die auf SAP HANA basieren und bei denen ABAP- und SAP-HANA-Objekte über das Change and Transport System im ABAP transportiert werden sollen., Die Objekte und Pakete aus dem SAP HANA Repository müssen Sie zunächst in das Repository des SAP HANA Transport for ABAP (HTA Repository) im ABAP-System übertragen und einen Transportauftrag hinzufügen. Dieser Vorgang wird „Synchronisierung“ genannt., Nach der Freigabe kann der Transportauftrag mit dem Transport Management System im Zielsystem importiert werden. Beim Import werden die Pakete und Objekte im SAP HANA Repository angelegt und aktiviert. Dieser Vorgang wird „Deployment“ genannt., Auf diese Weise können Sie auch kleinere Änderungen von SAP-HANA-Objekten und -Paketen transportieren., Es wird der Zustand zur Ausführung der Synchronisation übernommen. Vor dem Release sollte kontrolliert werden, ob alle „Ampeln“ auf Grün sind. Gelb zeigt an, dass das Objekt seit dem letzten Release geändert wurde., , Weitere Informationen zu den HANA-Transporten für ABAP können hier entnommen werden., , © 2021 b.telligent;https://www.btelligent.com/blog/anlage-externer-hana-views/;B-Telligent;Markus Sontheimer
24.08.2017;            Regularisierungspfade mit HTML-Widgets übersichtlich visualisieren -  Eine übersichtliche Alternative zur Spaghettiknäuelvisualisierung        ;Eine bessere Visualisierung muss also her. Alle Informationen gleichzeitig darzustellen, wie man das in einem statischen Plot muss, stößt bei vielen Variablen an natürliche Grenzen. Irgendwann ist die Zeichenfläche halt voll. Ein Ausweg ist die Visualisierung mit HTML und JavaScript. Sie bietet die Möglichkeit, bei Berührung mit der Maus einen einzelnen Regularisierungspfad hervorzuheben, so dass sein Verlauf im Spaghettiknäuel deutlich wird. Gleichzeitig kann man zusätzliche Informationen anbieten, in unserem Fall den Variablennamen. Auf diese Weise wird das Spaghettiknäuel entzifferbar., Nun ist es eine der Krankheiten von R, dass es zwar für alles ein Paket gibt, dass aber meist jedes Paket seine sehr eigenen Befehlsvarianten verwendet. Die Syntax ist weit entfernt von einer Einheitlichkeit, wie man sie (trotz Package-Vielfalt) bei Python findet. Es ist also begrüßenswert, wenn sich irgendwo zarte Ansätze zur Vereinheitlichung zeigen. Für die Visualisierung ist das beim ggplot2-Universum der Fall. ggplot2 kennt fast jeder, der mit R arbeitet. Es ist (neben den eingebauten Grafikmöglichkeiten von R) das verbreitetste R-Paket zur Datenvisualisierung. Warum ich hier gleich von einem „Universum“ spreche, ist vielleicht noch nicht jedem klar. ggplot2 bietet seit einiger Zeit eine Schnittstelle an, mit der sich das Paket um zusätzliche Funktionalitäten erweitern lässt. Unter www.ggplot2-exts.org gibt es eine Übersicht über die bereits recht umfangreiche Sammlung., Eine dieser Erweiterungen nennt sich „ggiraph“ und bietet HTML/JavaScript-Varianten bekannter Standardplots an. Gegenüber anderen Erweiterungen, mit denen sich so etwas auch realisieren lässt, hat ggiraph den Vorteil, dass man keine komplett neue Syntax lernen und bestehenden Code für ggplot2-Plots nur leicht anpassen muss, wenn man HTML-Widgets produzieren möchte. Für mich ist das ein schlagendes Argument für ggiraph. Ich muss allerdings zugeben, dass ich Alternativen wie metricsgraphics, plotly, highcharter oder rbokeh nicht ausprobiert habe. Wenn jemand damit Erfahrungen gesammelt hat, bin ich neugierig, davon zu hören., Um unser konkretes Visualisierungsproblem zu lösen, müssen wir eine Alternative zur Funktion im Elastic-Net-Package bauen, die an Stelle eines Standardplots ein HTML-Widget erzeugt. Damit am Ende etwas herauskommt, was mit dem Original vergleichbar ist, hat es Sinn, sich an die entsprechende plot-Routine des Elastic-Net-Packages zu halten und diese in die Syntax von ggiraph zu übersetzen. Leider verwendet Elastic Net nicht ggplot2 zur Visualisierung, sondern die Basisgrafik von R, sonst wäre diese Aufgabe noch leichter. Aber auch so ist sie leicht erledigt, und es bleibt noch Zeit, ein paar kleine Zusatzfeatures einzubauen, die den Überblick weiter erleichtern. Zum einen haben wir eine Möglichkeit eingebaut, den dargestellten Wertebereich auf der y-Achse mit Hilfe eines Parameters ylimits einzuschränken. Zum anderen gibt es einen weiteren Parameter max_abs_coeff_range, der es ermöglicht, einzuschränken, welche Variablen dargestellt werden. Es werden nur solche Variablen aufgenommen, für die der maximale Betrag des Koeffizienten zwischen max_abs_coeff_range1 und max_abs_coeff_range2 liegt. Das kann man benutzen, um allzu unübersichtliche Plots in mehrere Teile zu zerlegen: einen für Variablen, die an irgendeiner Stelle ihres Regularisierungpfades hohe Koeffizienten aufweisen, und einen für solche, die eher kleine Koeffizienten haben. Damit vermeidet man, dass die Regularisierungspfade der letzteren Variablen sich in der Visualisierung so dicht an die x-Achse schmiegen, dass sie nicht zu unterscheiden sind. Außerdem haben wir uns noch Parameter title und ylab eingebaut, die es erlauben, die Titel der Grafik und der y-Achse zu setzen., Der wiederverwendbare Teil des Codes ist die Funktion plot_regupath. Sie lehnt sich eng an die Funktion plot.glmnet aus dem glmnet-Package an sowie an die dazugehörige interne glmnet-Funktion plotCoef. Neben den schon oben erwähnten Zusatzparametern verlangt die Funktion dieselben Parameter x (für das gefittete Modell) und xvar (für die Art der Darstellung) wie die plot-Funktion aus dem glmnet-Package. Eine ausführlichere Erläuterung dieser Parameter ist in der glmnet-Dokumentation zu finden., Der Rest des Codes (außer der Funktion plot_regupath) demonstriert ihre Funktion an einem Beispiel. Die Datengrundlage stammt aus dem Videospiel FIFA 2017 von Electronic Arts. Die Daten kann man bei Kaggle herunterladen:, https://www.kaggle.com/artimous/complete-fifa-2017-player-dataset-global/version/2, Sie enthalten für jeden Spieler diverse Variablen, und eine Auswahl davon benutzen wir in unserem Codebeispiel, um eine binäre Zielvariable mittels logistischer Regression vorherzusagen. Die Zielvariable haben wir aus den Daten konstruiert, indem wir einem Spieler eine 1 zugewiesen haben, wenn die Variable „Rating“ über dem Durchschnitt liegt, und 0 sonst. Wir versuchen also die besonders guten Spieler zu identifizieren. Wir reduzieren Datenaufbereitung und Feature Engineering auf ein kaum mehr wahrnehmbares Minimum, um die Funktionsweise der Visualisierung mit möglichst wenig Code demonstrieren zu können. Am Schluss wird mit saveWidget die Grafik als HTML-Datei gespeichert. Diese kann man dann im Browser anschauen (siehe auch die Abbildung). Darüberfahren mit der Maus nicht vergessen!, , © 2021 b.telligent;https://www.btelligent.com/blog/regularisierungspfade-mit-html-widgets-visualisieren/;B-Telligent;Dr. Michael Allgöwer
17.08.2017;            Howto: Anwendung der zellbasierten Eingabesteuerung für Planungslösungen in Longview         ;Mit der neuen Funktionalität „zellbasierte Eingabesteuerung“ ist es möglich, zellbasiert das Eingabeverhalten in Longview zu steuern. Sobald die Funktionalität aktiviert wurde, kann über ein Referenzobjekt die manuelle Eingabe sowie das Kopieren und das Einfügen gesperrt bzw. freigegeben werden., Weitere Vorteile der oben genannten Funktionalität sind die benutzerfreundliche Usability und die einfacheren Entwicklungen. Denn da für diese Anwendungen kein weiterer Programmieraufwand vonnöten ist, sind die Implementierungszeiten wesentlich kürzer. Zusammen mit der dynamischen Formatierung ist die Funktion sehr gut einsetzbar, da hierdurch ein Hervorheben der Zellen nach Status einfach zu definieren ist., Folgende Tabelle sei gegeben:, , , Ziel ist es, anhand des zu wählenden Monats die Planungseingabe für die vergangenen Monate zu sperren und nur eine Planung für die kommenden Monate zu erlauben. Mithilfe der „dynamischen Formatierung“ werden durch eine visuelle Kennzeichnung des Eingabebereichs bzw. durch die Sperrung der Eingabe die freien bzw. gesperrten Planungsmonate hervorgehoben., Besonders geeignet ist die Anwendung dieser Funktionalität in Planung, Budgetierung, Forecast-Anwendungen usw., Die bisherige Variante besteht aus folgenden Schritten:, Diese Variante muss in Form von Codes in den jeweiligen Ereignissen implementiert werden., Es folgt ein Beispiel für die Tabellen:,   ,  2. Tabelle zum Steuern des Eingabeverhaltens: , , In der Tabelle unter 1. öffnen wir die Eigenschaften und schreiben in das Feld „Definiert durch“ den Namen der Tabelle 2., , , Somit wird erreicht, dass:, Wir definieren zwei dynamische Formate: Pa_open für editierbare Zellen und Pa_blocked für gesperrte Zellen., Unsere Tabelle für die dynamische Formatierung sieht wie folgt aus:, , , Die Einstellungen der dynamischen Formatierungen sehen folgendermaßen aus:, , , Als letzten Schritt verbinden wir unsere Tabelle mit den dynamischen Formatierungen der Datentabelle. Dazu klicken wir auf „Objekt formatieren“ und in den Reiter „Zellen“ geben wir unsere Tabelle ein., , , Das Ergebnis sieht dann zum Beispiel so aus:, , Beide Umsetzungsvarianten führen zur gewünschten Darstellung. Jedoch sparen wir mit den Funktionalitäten „dynamische Formatierung“ und „Eingabeverhalten“ sehr viel Zeit. Folgende Vorteile ergeben sich durch die neuere Variante:, , © 2021 b.telligent;https://www.btelligent.com/blog/eingabesteuerung-fuer-planungsloesungen-in-longview/;B-Telligent;Stefan Kersten
10.08.2017;            Die Highlights des Spark Summit Europe 2017        ;Ich freue mich, dieses Jahr mit diesem Blog-Eintrag wieder live vom Spark Summit Europe berichten zu können. Es ist der dritte Summit für den europäischen Raum und findet nach Amsterdam 2015 und Brüssel 2016 dieses Jahr in Dublin statt., , Der Summit erstreckt sich über drei Tage, beginnend mit einem Tag für eine Reihe von angebotenen Intensivschulungen. Die offizielle Eröffnung findet jedoch am Morgen des zweiten Tages mit den ersten Keynote Speeches statt. Eröffnet wird der Teil der Veranstaltung wie immer vom ursprünglichen Spark-Entwickler Matei Zaharia persönlich., Dieses Jahr sind etwa 1.200 Teilnehmer vor Ort bei einem Angebot von 100 Vorträgen in 5 separaten Tracks mit den Schwerpunkten Developer, Streaming, Data Science, Technical Deep Dives und Data Engineering. Trotz allem ist es, finde ich, noch eine nahezu familiäre Veranstaltung, wo sich die ursprüngliche Spark-Crew in den Pausen unters Volk mischt. Während der Keynote Speech saß schräg hinter mir beispielsweise Holden Karau, Entwicklerin der ersten Stunde und Autorin mehrerer Spark-Bücher., Aber zurück zu den angebotenen Inhalten. Vor meinem ersten Spark Summit habe ich ihn mir sehr Big-Data-lastig vorgestellt. Tatsächlich ist es im Kern jedoch eine spezialisierte Data-Science-Veranstaltung. Nahezu jeder praxisbezogene Vortrag birgt einen komplexen Anwendungsfall für Machine Learning. Alle Anwendungen beinhalten Big-Data-Quellen, aber es ist genau der Benefit von Spark, diesen Aspekt durch die Performance und Usability in den Hintergrund treten zu lassen., , In der ersten Keynote Speech von Matei Zaharia wurden die neuesten Features für das kommende Spark 2.3 vorgestellt. Es gibt dabei zwei aktuelle Entwicklungsschwerpunkte von Databricks und der Spark-Community: Streaming und Deep Learning. Beides soll durch Spark für den Enduser nochmal vereinfacht werden., , Ziel ist es, eine „High Level End-to-End API“ mit einem sehr simplen Interface zur Verfügung zu stellen. Ein Stream soll dadurch analog einem Data Frame abrufbar sein. Also auch in Form von Spark SQL und verknüpfbar mit Data Frames aus Batch-Berechnungen. Abgerundet wird Structured Streaming mit dem Versprechen eines „Exactly once Processing“, also einer ausfallrobusten Stream-Verarbeitung, , Auch hier soll mit Spark 2.3 eine neue API bereitgestellt werden, die Deep Learning Pipelines. Dadurch sollen mehr User in die Lage versetzt werden, Deep Learning produktiv einzusetzen. Die API basiert auf den ML Pipelines und unterstützt vor allem Tensorflow and Keras., Eine Databricks-Mitarbeiterin führte hierzu ein Deep-Learning-Beispiel mit der Klassifikation von Schuhen über die Bilder aus Online-Shops vor. Das Beispiel benötigt nur 7 Zeilen Code (!) und war in wenigen Minuten gut vermittelbar., , PySpark-Nutzer werden sich riesig über die Ankündigung freuen, dass ab Spark 2.3 eine der größten PySpark-Performance-Bremsen gelöst wird. Und zwar wird es möglich sein, vektorisierte Pandas-ähnliche Funktionen als UDFs (User Defined Functions) zu verwenden. Darüber hinaus wird es weitere Performance-Steigerungen für Python und R sowie einen besseren Support für Kubernetes geben., Der CEO von Databricks, Ali Ghodsi, konnte das Publikum mit der Premiere von Databricks Delta überraschen., Delta ist ein vereinheitlichtes System für Datenmanagement und erweitert das bisherige Databricks-Ökosystem. Es wird dabei auf Basis einfacher Massenspeicher (z.B. S3, HDFS mit Parquet) eine strukturierte Abstraktionsschicht geboten, die ACID-Konformität, hohe Performance und niedrige Latenz verspricht., In den Worten von Ali: „A Unified Management System for Real-time Big Data: Combining the best of data warehouses, data lakes and streaming.“, In einer Live-Demo konnte eindrucksvoll gezeigt werden, wie mit SQL ein Kafka-Topic abgefragt wurde und sich die Ergebnisse in Echtzeit aktualisierten., , Streaming und Deep Learning sind auch die Schwerpunkte in den weiteren Vorträgen der beiden Tage. Gerade beim Streaming gibt es gegenüber Spark mittlerweile ernste Konkurrenz (Kafka Streaming, Apache Flink). Laut Databricks wird Spark mit dem nächsten Release 2.3 wieder enorm aufholen und in Bezug auf Performance sogar deutlich überholen. Es bleibt spannend, welche Technologie sich bei Streaming durchsetzen wird. In Bezug auf komplette „End-to-End“-Anwendungen, die über das Streaming hinaus auch Machine-Learning-Modelle beinhalten, sehe ich Spark allerdings jetzt schon konkurrenzlos., Damit verabschiede ich mich aus Dublin und eile zum nächsten Vortrag!, , , © 2021 b.telligent;https://www.btelligent.com/blog/spark-summit-europe-2017-live-bericht-aus-dublin/;B-Telligent;Stefan Seltmann
10.08.2017;            How-to: Wer sollte ETL-Anwendungen prüfen?        ;Nach jahrelanger Arbeit an der Entwicklung von ETL-Anwendungen kann ich sagen, dass sie üblicherweise schlechter als Transaktionssysteme geprüft werden. Hierzu tragen eine Menge technischer Faktoren bei, wie z. B.:, Neben den oben genannten Faktoren besteht eine erhebliche organisatorische Schwierigkeit: Die klassische Verteilung von Entwicklern und Prüfern funktioniert für ETL-Projekte nicht gerade gut. Dies wird durch die ungewöhnlich hohe Qualifikation verursacht, die für Prüfer erforderlich ist. Der betreffende Prüfer muss:, In der Tat handelt es sich hierbei um Anforderungen eines Entwicklers mittleren Niveaus. Allerdings ist es fast unmöglich, solch qualifizierte Prüfer zu finden. Denn oftmals mangelt es an deren Bereitschaft, „nur“ als Prüfer zu arbeiten., Daher können Peer-Review-Techniken in Betracht gezogen werden, wenn sowohl vom Entwickler als auch vom Prüfer vergleichbare Qualifikationen erwartet werden. Laut meiner Erfahrung funktionieren Peer-Reviews allerdings auch nicht. Dies liegt insbesondere an folgenden Gründen:, Aufgrund dieser Kritik schlage ich eine andere Methode vor. Ziel ist es, die Prüfqualität durch die Abtrennung von Entwicklungs- und Prüfpflichten zu verbessern. Die Methode verlangt zwei Funktionen: Nennen wir sie einmal Analyst und Entwickler. Der Entwickler ist für die tatsächliche Entwicklung von Gegenständen, die Integration, Leistungsabstimmung usw. zuständig. Der Analyst liefert Input für den Entwickler und – hier besteht der grundlegende Unterschied – prüft die Ergebnisse der Tätigkeit des Entwicklers. Somit besteht die Hauptidee darin, dem Analysten die Verantwortung für die Datentests zuzuweisen. Dies funktioniert gut, da:, Die beschriebene Methode beinhaltet keine zusätzliche Hierarchieebene im Projekt: Analysten und Entwickler weisen für gewöhnlich dasselbe Positionsniveau auf. Meiner Erfahrung nach ist ein Verhältnis von einem Analysten je 2 bis 3 Entwickler angemessen., Zudem bietet dies weitere Vorteile:, Der Hauptgrundsatz beim Durchführen von Tests besteht darin, dass unter keinen Umständen eine Einzelperson sowohl für die Umsetzung als auch die Tests verantwortlich sein darf. Um dies zu erreichen, wird bei DWH-Projekten den Analysten die Verantwortung für das Prüfen der Daten zugewiesen., , © 2021 b.telligent;https://www.btelligent.com/blog/how-to-wer-sollte-etl-anwendungen-pruefen/;B-Telligent;Oliver Gräfe
03.08.2017;            SAP HANA – Early Unload – Speichernutzung        ;"Noch einmal möchte ich mich mit dem Thema Speichernutzung auseinandersetzen. In anderen Blogartikeln wie „SAP HANA – kein Speicher mehr? Bewusster Early Unload!“ ist bereits die Relevanz der korrekten Einstellung der SAP-BW-Objekte für die vorgesehene Nutzung aufgezeigt worden. Zusammengefasst: Immer alles frühzeitig aus dem Speicher schieben, was nicht immer und sofort für Abfragen benötigt wird. Gesteuert wird dies über die „Early Unload Priority“, die in der HANA-Datenbank auf Tabellenebene gesetzt wird. Da diese Einstellungen nicht im ABAP-Transportwesen (CTS) enthalten sind, muss der Entwickler oder der Betrieb sicherstellen, dass diese Einstellung immer korrekt gesetzt ist. Ansonsten liegen nur für das Staging benötigte Daten aus dem „Data Acquisition Layer“ sehr performant im RAM, was unnötig teuer ist und das System belastet., Eine HANA-Datenbank lagert Tabellen anhand ihrer Priorität vom Hauptspeicher auf das physische Storage aus (genannt UNLOAD). Damit stehen die Daten zwar auch nicht mehr direkt für Abfragen zur Verfügung, für Staging-Tabellen kann dies jedoch vernachlässigt werden, da diese z. B. häufig nur in der Nacht benötigt werden, wenn eine Prozesskette läuft. Die Reihenfolge von Auslagerungen wird über die Priorität pro Tabelle eines Objekts gesteuert, die in der Transaktion RSHDBMON manuell gesetzt werden kann. Einige SAP-BW-Objekte wie PSA-Tabellen, Change Logs von normalen ADSOs oder schreiboptimierte ADSO-Objekte haben das Early-Unload-Flag auch im Standard bereits komplett oder in Teilen gesetzt, je nach der von SAP vorgesehenen Nutzung., Für SAP-BW-Objekte kann nur die Priorität 5 (aktive Daten fürs Reporting) und 7 (Early Unload, inaktive Daten wie Change Logs …) gesetzt werden., , Die SAP beschreibt dabei auch, dass die manuell in der RSHDBMON gesetzten Einstellungen erhalten bleiben, wenn man ADSOs ändert. Das ist aber nicht vollständig korrekt. Führt ein Transport dazu, dass eine Tabelle über DROP &amp; CREATE neu erstellt wird, enthalten die aktuellen Support Packages von SAP BW 7.5 die Standardeinstellungen für Early Unload. Diesentspricht somit nicht mehr den notwendigen Einstellungen., Wie man dieses Problem lösen kann: Da wir im SAP-HANA-Umfeld durch die fortschreitende Integration kompletten Zugriff auf die Verwaltungsfunktionen der Datenbank haben, kann analog zur Transaktion RSHDBMON ein natives SQL-Kommando genutzt werden, um die Early Unload Priority korrekt zu setzen. Am besten funktioniert dies, wenn ein sauberes Namenskonzept verfolgt und die Layer der LSA++ entsprechend benannt sind (z. B. P* für Propagation Layer, D* für Datamart Layer, …). Über dieses Prefix im Namen kann ein Programm die ADSOs ermitteln, die im P* Layer liegen, und die Early Unload Settings entsprechend korrigieren. Abrunden lässt sich das Prinzip noch über die Nutzung von Ausnahmetabellen, falls z. B. ein Reporting auf einem Objekt im Propagation Layer aufgesetzt wird. Die LSA++ erlaubt dies explizit., Ausgeführt nach einem Release ist somit sichergestellt, dass alle Objekte die korrekten Einstellungen besitzen., Für eine Implementierung wurde die hier genutzte AMDP-Klasse zum Bereinigen des Hauptspeichers um eine Funktion erweitert, die das Ändern der Early Unload Priority kapselt. Leider hat die SAP dafür keine API implementiert und programmiert dies aktuell einzeln aus. Zudem wurde zusätzlich zu der Klasse ein ABAP-Report implementiert, der diese aufruft und dem Entwickler oder Betrieb eine Maske bereitstellt, die auch eine Simulation der Änderungen ermöglicht., Die Klasse ermittelt die zugehörigen Tabellen des übergebenen ADSOs und stellt diese korrekt ein. Zu Beginn werden die notwendigen Datentypen definiert, die Übergaben geprüft und die Datenbankinformationen des SAP-BW-Schemas auf der HANA-DB ermittelt., , Danach wird die eigentliche Programmlogik ausgeführt. Diese ermittelt die betroffenen ADSOs aus der SAP-BW-Tabelle „rsoadso“, die alle ADSOs enthält mit einem statischen Filter auf die Layer. Danach werden die Tabellen zu den ADSOs über eine API ermittelt. Und zuletzt werden die Tabellen entsprechend der Vorgabe über den Aufruf der privaten Methode _set_unload_priority auf Early Unload gestellt., , Die private Methode kapselt den SQL-Zugriff für das Setzen der Early Unload Priority. Damit ist sichergestellt, dass nicht zufällig fremde Tabellen gesetzt werden und die Funktion bei weiteren Erweiterungen der Klasse wiederverwendet werden kann. (Bitte dort besonders aufpassen, Änderungen lassen sich nicht einfach wiederherstellen!), , Quellcode zur Erstellung des Reports, , Der Report ermöglicht über einen sehr einfachen Dialog die Steuerung der Klasse und auch die Simulation. Auswirkungen auf die Objekte können vorab geprüft werden. Zudem fällt auf, wenn ein Objekt falsch benannt wurde oder fälschlicherweise erkannt wird. Eine Ausnahmetabelle existiert hier nicht, die Funktionalität ist somit noch vielschichtig und beliebig erweiterbar., Beim Aufruf des Reports können die Tabellen noch über SQL-Wildcards gefiltert werden, ansonsten greift mindestens der statische Filter auf die Tabellen-Prefixes aus dem Namenskonzept, was Fehler vermeidet., , , Diese einfache Funktionalität stellt sicher, dass der Speicher der HANA-DB nicht unnötig belastet wird, und verringert auch die Aufwände bei einem Release bzw. vereinfacht die Prüfungen, ob alle Einstellungen noch den Erwartungen entsprechen. Manuelle Anpassungen entfallen und der Speicher der HANA-Datenbank wird optimal ausgenutzt., , © 2021 b.telligent";https://www.btelligent.com/blog/sap-hana-early-unload-speichernutzung/;B-Telligent;Markus Sontheimer
03.08.2017;            Donald Trump und Kosinskis Bombe (Teil 2)        ;", Im ersten Teil des Blogbeitragshaben wir bereits einiges zu Kosinskis Methode erfahren. Zum Beispiel, wie Kosinski die Daten (also die Facebook-Likes) beschafft und wie er sie analysiert hat. Hier spielen die OCEAN-Persönlichkeitsmerkmale eine wichtige Rolle, wobei zwischen „Traits“ und vorübergehenden „States“ differenziert werden muss. Cambridge Analytica hat sich Kosinskis Methode zunutze gemacht und unter anderem im US-amerikanischen Präsidentschaftswahlkampf zu Gunsten Donald Trumps angewandt. Nun stellt sich die Frage, was man noch alles mit einem Cambridge-Analytica-Fragebogen herausfinden kann und wie präzise die so gewonnenen Ergebnisse die Persönlichkeit eines Menschen tatsächlich abbilden können., Wenn man den Cambridge-Analytica-Fragebogen ausfüllt, fallen ein paar Fragen auf, die in einen OCEAN-Fragebogen nicht hineingehören. Das liegt daran, dass die beschriebene Methode eben nicht nur mit einem OCEAN-Fragebogen funktioniert. Vielmehr kann man so ziemlich alles abfragen und anhand von Facebook-Likes vorherzusagen versuchen, wofür es einen halbwegs sinnvollen Fragebogen gibt. Zum Beispiel die politische Orientierung. Oder die Zufriedenheit mit dem eigenen Leben. Oder den „Finanz-IQ“. Oder den Musikgeschmack. Oder die sexuelle Zufriedenheit. Der Fantasie sind kaum Grenzen gesetzt. Wer?s gern mal selbst ausprobieren will, findet auf dieser Seite der Universität Cambridge eine reichhaltige Auswahl. Das Muster ist dabei immer dasselbe: Wenn jemand einen XY-Test macht und einwilligt, mir seine Facebook-Likes zugänglich zu machen, kann ich versuchen, die Eigenschaft XY aus dem Test anhand der Likes vorherzusagen. Nicht immer sind die damit gewonnenen Erkenntnisse sonderlich tiefsinnig (Likes für Obama sprechen zum Beispiel dafür, dass man mit den Demokraten sympathisiert. Wer hätte das gedacht?). In ihrer Summe kann man diese Erkenntnisse aber durchaus unheimlich finden: eine lange Liste von interpretierten Likes, die alle zusammen den Menschen, zu dem sie gehören, schon recht genau beschreiben können., Aber wie genau ist das eigentlich alles? Genau genug, um sich wirklich zu gruseln? Kosinskis oben erwähnter Artikelzeigt ein paar Beispiele. Unter anderem geht es dort um die oben erwähnten OCEAN-Merkmale. Kosinski vergleicht die Sicherheit, mit der er die Testergebnisse aus den Likes vorhersagen kann (quantifiziert als Korrelation zwischen den Testergebnissen und den Ergebnissen aus der Like-Analyse), mit der Test-Retest-Reliabilität des benutzten Fragebogens. Für vier der fünf etablierten Persönlichkeitsdimensionen erreicht er knapp die Hälfte der Test-Retest-Reliabilität des benutzten Fragebogens, für die Persönlichkeitsdimension „Offenheit“ kommt er sogar auf beachtliche 78 % der Test-Retest-Reliabilität. Das klingt beeindruckend., Allerdings kann man einen wichtigen Punkt in Kosinskis Studie leicht überlesen. Der Fragebogen, den er für die Ermittlung der Persönlichkeitsmerkmale verwendet, ist eine sehr kleine Variante mit lediglich 20 Fragen – deutlich weniger als die 100 und mehr Fragen, die ein „voller“ Test beinhaltet. Dementsprechend geringer ist die Test-Retest-Reliabilität des Fragebogens, die er als Vergleichsmaßstab für seine eigenen Ergebnisse benutzt. Die Abbildung zeigt einen Vergleich zwischen den von Kosinski erreichten Ergebnissen, der Test-Retest-Reliabilität des von ihm verwendeten Tests und der Test-Retest-Reliabilität eines langen OCEAN-Fragebogens (diese kann man hiernachlesen; der Erstautor Robert R. McCrae ist einer der wissenschaftlichen Väter des OCEAN-Modells). Die Abbildung zeigt, differenziert nach den fünf Dimensionen des OCEAN-Modells, die Test-Retest-Reliabilität (Pearson Correlation Coefficient) der drei Instrumente, die hier eine Rolle spielen:, Man tut Kosinski sicherlich kein Unrecht, wenn man zu dem Schluss kommt, dass er mit der Wahl des Fragebogens die Latte so tief wie möglich gelegt hat, was den Vergleichsmaßstab für seine Methode angeht. Wenn man die dunklen Balken in der Abbildung nicht, wie in Kosinskis Artikel geschehen, mit den mittleren vergleicht, sondern mit den hellen, wird deutlich, dass sie von der Genauigkeit eines langen OCEAN-Fragebogens weit entfernt sind., Kosinskis Ergebnisse erlauben also eine wesentlich weniger präzise Einschätzung von Persönlichkeitsmerkmalen, als es auf den ersten Blick den Anschein hat. Allerdings sind sie auch nicht auf maximale Genauigkeit bei der Vorhersage optimiert, sondern eher eine Art wissenschaftlicher Proof of Concept., Kosinskis Methoden haben also nicht den US-Wahlkampf entschieden. Aber sie haben das Potential, in zukünftigen Wahlkämpfen eine wichtige Rolle zu spielen. Wirklich perfide werden diese Methoden, wenn man sie mit Fake News kombiniert, so wie das im US-Präsidentschaftswahlkampf passiert sein soll. Allerdings ist mein Gefühl dabei, dass das Perfide daran vor allem die Fake News sind – auch schon für sich genommen, ohne psychologisches Targeting., Michal Kosinski hat übrigens nicht nur gezeigt, dass es die Bombe gibt, wie er bescheiden behauptet hat. Er hat auch eine recht komplette Bauanleitung ins Netz gestellt. Wer zuhause sein eigenes Bömbchen bauen möchte, findet die Anleitung hier., , , © 2021 b.telligent";https://www.btelligent.com/blog/donald-trump-und-kosinskis-bombe-teil-2/;B-Telligent;Dr. Michael Allgöwer
27.07.2017;            Mythos oder Wahrheit: Feldroutinen in BW-Transformationen sind langsam        ;Nicht selten habe ich auch von erfahrenen SAP-BW-Beratern gehört, dass Feldroutinen aufgrund von schlechter Performance in BW-Transformationen nicht angewendet werden sollten. An der Stelle möchte ich dieser These widersprechen. Ich glaube, dass Feldroutinen schneller sind. Bei Feldformeln, die direkt auf einer HANA-Datenbank ausgeführt werden, ist das nicht zu bezweifeln. Aber auch ABAP-Feldroutinen und -Formeln sind performanter als Endroutinen., Dies will ich anhand des folgenden Beispiels verdeutlichen., , Wir wollen für die Ausprägung einer Kennzahl ein Flag setzen, und zwar, wenn die Kennzahl gleich 1 ist., , Die Feldformel dafür würde so aussehen., , Um zu verstehen, wie Feldformeln oder Feldroutinen auf dem ABAP-Applikationsserver tatsächlich funktionieren, schauen wir uns das generierte Programm der Transformation an. Diese wird auch dann erzeugt, wenn eine Transformation nur direkte Zuweisungen enthält., , Ab dem folgenden Schritt im Programm werden die Datensätze verarbeitet., , Hierdurch wird deutlich, dass BW-Transformationen auch ohne Start- oder Endroutinen standardgemäß eine Schleife über jeden Datensatz durchführen. Innerhalb dieser Schleife wird dann unsere Feldroutine ausgeführt., , Für die Performance ist also zwischen einer direkten Zuweisung und einer einfachen Feldroutine kaum zu differenzieren. Aber auch der Unterschied zwischen einer Feld- und einer Endroutine wird an dieser Stelle offenbart. Denn wenn man Endroutinen schreibt, erzeugt man einen zusätzlichen, unter Umständen überflüssigen LOOP., Man kann also daraus schließen, dass es – solange möglich – besser ist, Feld- anstatt Endroutinen anzuwenden, da man sich dadurch den überflüssigen LOOP spart., Hinzu kommen noch zwei weitere relevante Aspekte., Erstens ist es oft einfacher, Programmierlogik aus einer Feldroutine zu verstehen als aus einer Endroutine, da dort in der Regel nur Informationen für das bestimmte Feld enthalten sind. Allgemein gesprochen verbessert sich durch die Anwendung von Feldregeln und -routinen die Wartbarkeit im System., Zweitens kann man durch den eingebauten und oft unbemerkten Ausführungsknopf Feldroutinen und Feldformeln ohne Weiteres debuggen. Dies führt zur Beschleunigung des Entwicklungsprozesses., , An dieser Stelle ergibt sich außerdem die Frage, ob man Feldroutinen nicht so manipulieren sollte, dass diese Programmierlogik auch für weitere Felder ermittelt. Dies gilt für Situationen, in denen man normalerweise eine Endroutine nutzen würde., So könnte man zum Beispiel, wenn man mehrere Datenfelder aus einem Lookup ermittelt, das Auslesen dieser – in einer in der Startroutine definierten Struktur – bei der ersten Feldroutine durchführen. Daraufhin werden in den restlichen Feldroutinen einfach die entsprechenden Strukturkomponenten zugewiesen. Auch wenn sich das nicht ganz „sauber“ anfühlt, könnte es vielleicht bei riesigen Datenmengen doch einen spürbaren Performancegewinn durch die Ersparnis der zusätzlichen Schleife in der Endroutine zur Folge haben., Vielleicht hat das schon jemand von Euch ausprobiert? Gerne könnt Ihr Eure Erfahrungen in den Kommentaren mitteilen., , © 2021 b.telligent;https://www.btelligent.com/blog/mythos-oder-wahrheit-feldroutinen-sind-langsam/;B-Telligent;Markus Sontheimer
20.07.2017;            Der effektive Einsatz von Partition Pruning zur Optimierung der Abfragegeschwindigkeit (Teil 3)        ;In dem vorhergehenden Artikel dieser Serie wurde ein praxisnaher und effektiver Ansatz zur Nutzung von Partition Pruning eingehend erläutert. Mit Hilfe dieser einfach umzusetzenden Methode können die Abfragezeiten deutlich optimiert werden.Um jedoch die effiziente und effektive Nutzung der vorgestellten Methode zu gewährleisten, müssen, wie so häufig, einige Details beachtet werden. In diesem Aspekt halten wir es mit Theodor Fontane, der schon im 19. Jahrhundert festgehalten hat, dass der Zauber immer im Detail steckt., Im Folgenden werden deshalb einige essentielle Details näher erläutert, die beim Einsatz von Partition Pruning beachtet werden müssen:, , © 2021 b.telligent;https://www.btelligent.com/blog/der-effektive-einsatz-von-partition-pruning-teil-3/;B-Telligent;Oliver Gräfe
06.07.2017;            Donald Trump und Kosinskis Bombe (Teil 1)        ;"Viel wurde Anfang des Jahres in den Medien der Artikeldiskutiert, der im Schweizer „Magazin“ erschienen ist. Schnell konzentrierte sich die Diskussion auf eine Frage: Haben Kosinskis Methoden, angewandt von einer zweifelhaften Firma namens Cambridge Analytica, die amerikanische Präsidentenwahl entschieden? Im Vordergrund stand nicht mehr Kosinski und seine Forschung, sondern Cambridge Analytica und vor allem die Gestalt des Alexander Nix. Nix soll ein brillanter Verkäufer sein. Die Aufmerksamkeit, die er für Cambridge Analytica zu erzeugen versteht, spricht dafür, dass diese Zuschreibung stimmt., Die Frage, ob Cambridge Analytica die Wahl entschieden hat, ist weniger schwierig zu beantworten, als es zunächst scheint. Die Zusammenarbeit von Cambridge Analytica mit Donald Trump hat erst gegen Ende der Primaries begonnen, im Juni also. Sie hat, mit anderen Worten, weniger als ein halbes Jahr gedauert, ein halbes Jahr zudem, in dem Cambridge Analytica nur eine von mehreren einschlägigen Firmen war, die Trump unterstützt haben und die sich oft uneins waren. Das ist zu wenig Zeit, um eine solche Kampagne ausreichend vorzubereiten, zu wenig Zeit also, um ein wichtiger Einflussfaktor zu werden., Viel interessanter als der Einfluss auf die letzte US-Wahl ist der Ausgangspunkt der Diskussion: Michal Kosinski und seine Forschung. Was sind es für Methoden, die er entwickelt hat? Wie wirkungsvoll sind sie? Dahinter steht die eine, die wirklich große Frage: Welche Rolle werden Kosinskis Methoden bei zukünftigen Wahlen spielen, auch, aber nicht nur in den USA?, Kosinski ist, im Gegensatz zu Nix, Forscher, und als solcher publiziert er seine Arbeit. Es liegt also nahe, seine Arbeiten einfach mal zu lesen. Ein guter Startpunkt dafür ist ein Artikel, der 2013 veröffentlicht wurde und dessen Titel den Kern von Kosinskis Entdeckung nüchtern auf den Punkt bringt: „Private traits and attributes are predictable from digital records of human behavior“. Darin beschreibt Kosinski, welche Daten er verwendet und wie er sie analysiert hat., Das Muster, nach dem die Daten gewonnen wurden, ist ebenso einfach wie genial: Man bietet auf Facebook einen Persönlichkeitstest an. Wer neugierig auf die Ergebnisse ist, „bezahlt“ mit seinen Daten – im Fall von Kosinskis Tests spendeten die Probanden diese Daten freiwillig der Wissenschaft. Wer das nicht wollte, bekam wohl die Testergebnisse trotzdem., Wer einen solchen Test ausfüllt und seine Daten (also seine Facebook-Likes) zur Verfügung stellt, liefert gleichzeitig die Ergebnisse des Tests, also (je nach Test) einen mehr oder weniger detaillierten Einblick in die eigene Persönlichkeit., Aber von was für einem Einblick sprechen wir da genau? Das Problem mit echten psychologischen Testverfahren liegt darin, dass sie in der Entwicklung sehr komplex und in der Durchführung meist langwierig sind. Was man dann tatsächlich gemessen hat, ist dabei oft nicht ganz frei von Interpretationen. Es ist ähnlich wie mit der Intelligenz. Auf die Frage hin, was tatsächlich Intelligenz ist, wird in Psychologenkreisen gerne geantwortet: Intelligenz ist das, was der Intelligenztest misst., Ganz ähnlich ist es bei Persönlichkeitstests: Es gibt keine klar benennbaren, unumstößlichen Dimensionen, nach denen man die Persönlichkeit eines Menschen vermessen könnte. Was diesem Ziel noch am nächsten kommt, sind die Big Five oder OCEAN-Persönlichkeitsmerkmale. Auf diese bezieht sich Kosinski ebenso wie Cambridge Analytica. Die OCEAN-Merkmale wurden nicht durch sorgfältiges Sezieren von Gehirnen oder intensive Beobachtungen von Menschen in freier Wildbahn entdeckt, sondern durch Fragebögen, ausgewertet mit enorm umfangreichen faktoranalytischen Berechnungen auf Basis einer Vielzahl an Fragen und Befragten. Das Endresultat sind fünf Cluster an Fragen. Jedes Cluster fasst Fragen zusammen, deren Antworten untereinander hochkorreliert (überspitzt: kenn ich eine Antwort, kenn ich alle), aber gleichzeitig ziemlich unabhängig von den Antworten aus anderen Clustern sind., Wenn man diese Cluster anschaut, stellt man fest, dass es inhaltliche Beziehungen zwischen den Fragen gibt, die zu demselben Cluster gehören. Beispielsweise enthält das erste Cluster Fragen, die alle irgendwie zu tun haben mit einer gewissen Offenheit für neue Erfahrungen. Darum steht das „O“ in OCEAN für „Openness“. Genauso wurden die anderen vier Dimensionen gewonnen. Außer Openness sind das noch Conscientiousness (Gewissenhaftigkeit), Extraversion (im Deutschen ist das derselbe Begriff), Agreeableness (Verträglichkeit) sowie Emotional Stability (emotionale Stabilität; diese Dimension hat man früher Neurotizismus genannt, daher das „N“ in OCEAN). Diese inhaltlichen Beschreibungen der Cluster sind also Interpretationen, man kann sie weder berechnen, noch kann man beweisen, dass sie richtig sind. Sie sind lediglich plausibel., Man kann also nicht beweisen, dass zum Beispiel „Offenheit“ eine angemessene Interpretation gewisser Antworten ist. Was man dagegen schon empirisch nachweisen kann, ist eine gewisse Stabilität der Ergebnisse, wenn man die Persönlichkeit eines Menschen nach dem OCEAN-Schema vermisst. Man bezeichnet die fünf Persönlichkeitsmerkmale aus dem OCEAN-Test deshalb auch nicht als (vorübergehende) States, sondern als sogenannte Traits, also stabile, über die Zeit nur wenig veränderliche Persönlichkeitsmerkmale., „Traits“ ist auch die Begrifflichkeit, die auf der Website von Cambridge Analytica verwendet wird. Mit etwa 50 Fragen kann man dort die eigene Persönlichkeit sofort vermessen lassen. Genau hier trennt sich nun aber das beworbene „wissenschaftliche Vorgehen“ von der echten wissenschaftlichen Theorie. Um einen Trait seriös zu messen, muss man viele Fragen einsetzen, um Stabilität zu erzielen. Fragestellungen aus dem Web-Fragebogen, wie z. B. „I waste my time“ oder „I find it difficult to get down to work“, können bei mir zumindest allein im Wochenverlauf zwischen Montagmorgen und Freitagnachmittag deutlich schwanken. Dieser Effekt wird in der psychologischen Skalenkonstruktion vermindert, indem man mehrere leicht variierende Fragen mit ähnlichen Inhalten stellt. Diese Variationen sind nicht beliebig, sondern müssen empirisch daraufhin geprüft werden, dass sie tatsächlich ähnliche Inhalte abdecken. Erst mit einem Fragebogen, der weit über 100 Fragen beinhaltet, nähert man sich einer zuverlässigen Messung, die bei Wiederholung nach einem halben Jahr vergleichbare Resultate liefert. Erst dann kann ich auch besser tatsächliches Verhalten vorhersagen, da die breiteren Skalen genügend Fragen beinhalten, um auch Unterfacetten abbilden zu können., Was Cambridge Analytica hier tut, ist eher die Messung von States und nicht von Traits. Sie messen also eher den aktuellen Gemütszustand bei der Beantwortung der Fragen als Aspekte einer dauerhaften Persönlichkeitsstruktur. Wenn man auf dieser Basis Werbung steuert, läuft man Gefahr, Launen anzusprechen, die bereits lang vergangen sind. Das heißt noch lange nicht, dass das nicht tatsächlich die Meinung und das Wahlverhalten beeinflussen kann. Man sollte sich aber etwas von der Vorstellung lösen, hier wie beworben das Resultat langjähriger psychologischer Forschung am Werk zu sehen., Was kann man eigentlich alles mit einem Cambridge-Analytica-Fragebogen abfragen? Wie präzise können die so gewonnenen Ergebnisse die Persönlichkeit eines Menschen beschreiben? Und was bedeutet das alles für zukünftige Wahlkämpfe? All diese Fragen werden im zweiten Teil des Blogbeitrags beantwortet., , © 2021 b.telligent";https://www.btelligent.com/blog/donald-trump-und-kosinskis-bombe-teil-1/;B-Telligent;Dr. Michael Allgöwer
06.07.2017;            Staging-Area: Potentiale gegenüber Quellsystemen        ;, © 2021 b.telligent;https://www.btelligent.com/blog/staging-area-potentiale-gegenueber-quellsystemen/;B-Telligent;Oliver Gräfe
29.06.2017;            Der effektive Einsatz von Partition Pruning zur Optimierung der Abfragegeschwindigkeit (Teil 2)        ;"Nachdem im ersten Beitrag dieser Blogreihe die herkömmlichen Wege zur Aufbewahrung historischer Daten dargestellt wurden, möchte ich in diesem zweiten Teil eine weitere, effektivere Möglichkeit zur Partitionierung einer historischen Tabelle vorstellen. Wesentlich hierbei ist, dass ein erheblicher Anstieg der Größe vermieden wird. Nennen wir diese Möglichkeit partitionierte Intervalltabelle. Der Algorithmus ist sehr einfach:, Sehen wir anhand eines Beispiels, wie dies funktioniert. Für den ersten Monat ergibt sich kein Unterschied gegenüber einer Intervalltabelle:, , , KONTO_ID, GÜLTIG_VON, GÜLTIG_BIS, SALDO_BETRAG, Partition I, , Sobald jedoch der nächste Monat beginnt, sollten wir einen neuen Datensatz erstellen und den vorherigen Datensatz für jedes Konto ""schließen"", unabhängig davon, ob die Attribute aktualisiert wurden (33333333333) oder nicht (11111111111 und 22222222222):, , , KONTO_ID, GÜLTIG_VON, GÜLTIG_BIS, SALDO_BETRAG, Partition I, , , , , Partition II, ,  Bis zur Erstellung der nächsten Partition werden in der Partition II weitere Aktualisierungen vorgenommen, was ebenfalls wie in einer normalen Intervalltabelle erfolgt., , , KONTO_ID, GÜLTIG_VON, GÜLTIG_BIS, SALDO_BETRAG, Partition I, , , , , Partition II, , Selbstverständlich sind bei dieser Lösung redundante Zeilen erforderlich, was unüblich ist, wenn Sie die Leistung optimieren möchten. Es gibt jedoch einen weiteren riesigen Vorteil: Wir können sicher sein, dass für alle Zeilen, die für die Auswahl der Salden an dem bestimmten Datum erforderlich sind, das Datum GÜLTIG_VON in dem gleichen Monat liegt wie das angegebene Datum. Dementsprechend können wir beispielsweise die folgende Abfrage verwenden, um die Salden auszuwählen:, , , Die Regel der Transitivität ermöglicht es Oracle, festzustellen, dass das Datum GÜLTIG_VON zwischen dem 01.06.2016 und dem 15.06.2016 liegen sollte, was bedeutet, dass nur die Partition II gelesen werden sollte, um die Abfrage durchzuführen. Grundsätzlich sollte nur eine Partition anstelle der ganzen Tabelle durchsucht werden. Aus diesem Grund könnten unsere Kennzahlen anhand der folgenden Formeln berechnet werden:, TRC = CA * (FC + 1 - FC/30) * HD * 12, RSR = CA * (FC + 1 - FC/30), Und in Zahlen:, , , , Intervall, Snapshot, Partitionierte Intervalle, Anzahl der Gesamtzeilen, , , , Zeilen pro einzelnem Lesevorgang, , , , , Abschließend ist festzuhalten, dass die partitionierte Intervalltabelle die Vorteile der zuvor genannten beiden Methoden kombiniert: Zwar ist eine minimal größere Tabelle als eine normale Intervalltabelle erforderlich, die Leistung der Auswahlmöglichkeiten ist jedoch mit derjenigen einer Snapshot-Tabelle vergleichbar. Zudem ist es verhältnismäßig einfach, Filter in Berichten anzupassen, um die Vorteile der Partitionierung zu nutzen., Die Effektivität hängt in der Tat von der Häufigkeit der Änderungen von historisierten Attributen ab. In der folgenden Tabelle werden die oben genannten Maßnahmen auf der Grundlage der Häufigkeit der Änderungen verglichen:, , , Anzahl an verschiedenen Schlüsseln, Anzahl an Monaten, Anzahl an Änderungen pro Monat, , Option 1 – Intervalle, Zeilen insgesamt, Zeilen pro einzelnem Lesevorgang, , Option 2 – Tägliche Snapshots, Zeilen insgesamt, Zeilen pro einzelnem Lesevorgang, , Option 3 – Partitionierte Intervalle, Zeilen insgesamt, Zeilen pro einzelnem Lesevorgang, , Wie Sie sehen, sind die partitionierten Intervalltabellen für die Häufigkeit der Änderungen von einmal alle zwei Monate bis zu fünfmal pro Monat am effektivsten. Bei weniger häufigen Änderungen können Sie versuchen, größere Partitionen zu verwenden (die gleiche Logik funktioniert für vierteljährliche oder jährliche Partitionen zusätzlich zu monatlichen Partitionen). Wenn die Anzahl an Änderungen 5 übersteigt, können Sie wöchentliche Partitionen ausprobieren; wenn die Anzahl um die 20 liegt, ist die Snapshot-Tabelle die effektivste Lösung., Es geht tatsächlich immer darum, wie partitionierte Intervalltabellen allgemein funktionieren könnten, aber ""der Teufel steckt im Detail"". Deshalb sind wichtige Kleinigkeiten zu beachten, die im nächsten Blogbeitrag dieser dreiteiligen Serie dargestellt werden., , © 2021 b.telligent";https://www.btelligent.com/blog/der-effektive-einsatz-von-partition-pruning-teil-2/;B-Telligent;Oliver Gräfe
15.06.2017;            Data-Warehouse-Automatisierung (Teil 1: Einleitung)        ;Die Automatisierung von immer wiederkehrenden Aufgabenstellungen gehört zu den grundlegendsten Prinzipien der modernen Welt. Bereits Henry Ford erkannte daraus resultierende Vorteile, wie sinkende Fehleranfälligkeit, kürzere Fertigungszyklen und eine gleichbleibende, einheitliche Qualität. Eben diese Vorteile lassen sich bei Data-Warehouse-Initiativen anwenden., Die Speicherung, Pflege und Erweiterung der unternehmensweiten Daten lässt sich mithilfe von Data-Warehouse-Automatisierungs-Werkzeugen (DWA-Tools) in wenigen Klicks realisieren, was zuvor noch einen Großteil der gesamten Entwicklungszeit erfordert hätte. Dabei gehen moderne DWA-Tools noch einen Schritt weiter und bieten tiefgreifende Impact- und Lineage-Funktionalitäten sowie automatisierte Regressions- und Qualitätstests, effiziente Beladungsroutinen, vereinfachte Deployments zwischen Umgebungen und eine weitreichende Generierung von Dokumentation an., Um die genannten Möglichkeiten umzusetzen, werden die Informationen über Quelldaten, DWH-Strukturen und Beladungsprozesse vom DWA-Tool als Metadaten in einer Datenbank hinterlegt und verwaltet., In der nachfolgenden Abbildung werden die grundsätzlichen Themenbereiche der Automatisierung noch einmal übersichtlich dargestellt:, , Auf dem Markt haben sich bereits diverse DWA-Tools etabliert. Grundlegend lassen sich zwei Strömungen unterscheiden, nach denen vorgegangen wird (Eckerson, 2015, S. 5 ff):datengetriebene und modellgetriebene Ansätze., Mithilfe von datengetriebenen DWA-Tools soll die physische Erstellung von Data-Warehouse-Strukturen stark automatisiert werden. Der Entwickler kann sich vereinfacht auf den Daten bewegen und SQL-Statements, Prozeduren, Skripte und sonstige Datenbankstrukturen automatisch generieren lassen. Für Fachbereiche hingegen ist die datennahe Darstellung eher ungeeignet, wodurch die Kommunikation und Abstimmung mit den Entwicklern vor allem über die prototypische Entwicklung von Ergebnismengen realisiert wird:, , Die Ergebnismengen können über BI-Tools oder Excel visualisiert werden., Besonders bei gut strukturierten und leicht zugänglichen Daten können rein datengetriebene DWA-Tools effektiv eingesetzt werden und einen Mehrwert bieten., Modellgetriebene DWA-Tools hingegen verfolgen das Ziel, komplexe Sachverhalte auf eine höhere Abstraktionsebene zu heben und zu visualisieren. Auf diese Weise sollen sowohl Entwickler als auch Fachbereiche mit den Daten vereinfacht arbeiten können. Mithilfe des bereitgestellten GUI können Datenbewirtschaftungsprozesse intuitiv aufgebaut werden, wobei vorgefertigte Bausteine u.a. für das Erstellen von Dimensionen, Faktentabellen und Hierarchien zur Verfügung stehen., ,  Welches DWA-Tool letztendlich das geeignete ist, sollte außer an den beschriebenen konzeptuellen Unterschieden der einzelnen Tools an weiteren Faktoren festgemacht werden. Auf Grundlage der in Abbildung 1 dargestellten Bereiche, bei denen grundsätzlich eine Automatisierung von Nutzen sein kann, werden in kommenden Blogeinträgen weitere Faktoren näher beleuchtet. Ziel dabei ist es, eine Übersicht zu geben, die als Entscheidungshilfe dienen kann., , © 2021 b.telligent;https://www.btelligent.com/blog/data-warehouse-automatisierung-teil-1/;B-Telligent;Dominik Schuster
31.05.2017;            Die erstmalige Aggregation der selektierten Daten        ;Wir wissen jetzt, wie wir die Daten richtig selektieren, welche Tabellenart wir bei Lookups nutzen sollten und wie wir sicherstellen können, dass wir nur relevante Datensätze durchlesen., In der Praxis ist es aber oft so, dass man erstmals eine größere und/oder nicht eindeutige Datenmenge von der Datenbank selektieren muss, die dann nach bestimmten Regeln fürs performante Nachlesen aggregiert werden sollte., Abhängig vom Aggregationsverfahren wird unterschiedlich vorgegangen. Beim Summieren, Errechnen des Zählers oder des Durchschnitts muss man i. d. R. durch jeden Datensatz der internen Tabelle schleifen. Beim Ermitteln des Minimal- bzw. Maximalwertes geht es aber auch schneller. Dafür muss man die erhaltenen Datensätze sortieren und daraufhin mit der Anweisung DELETE ADJACENT DUPLICATES bereinigen. Dabei hat man die Möglichkeit, mittels des Zusatzes COMPARING zu bestimmen, welche Felder für den Vergleich berücksichtigt werden sollen., Dies wird anhand des folgenden Beispiels verdeutlicht. Wir haben eine Datenquelle dbTab. Diese beinhaltet unter anderem 3 Schlüsselfelder, ein Lookup-Feld zum Nachlesen und ein Datumsfeld. Unser Ziel ist es, den Lookup-Wert nur für den letzten, aktuellsten Datensatz zu selektieren, damit wir diesen später performanter nachlesen können., Zuerst selektieren wir alle o. g. Felder aus dbTab, für die eine entsprechende Schlüsselkombination in unserem result_package vorhanden ist., , Hiermit haben wir mehrere Datumsfelder pro Schlüsselfeld, wobei wir nur das aktuellste benötigen., key_field, key_field2, key_field3, date_field, lookup_field, A, B, C, 20170101, 1, B, C, D, 20170101, 2, C, D, E, 20170101, 3, A, B, C, 20170301, 4, B, C, D, 20170301, 5, C, D, E, 20170301, 6, A, B, C, 20170501, 7, B, C, D, 20170501, 8, C, D, E, 20170501, 9, , Ganz wichtig ist es, die Ergebnistabelle zu sortieren, da sonst DELETE ADJACENT DUPLICATES nicht funktionieren würde., , Somit haben wir sichergestellt, dass die interne Tabelle iTab so aussieht., key_field, key_field2, key_field3, date_field, lookup_field, A, B, C, 20170501, 7, A, B, C, 20170301, 4, A, B, C, 20170101, 1, B, C, D, 20170501, 8, B, C, D, 20170301, 5, B, C, D, 20170101, 2, C, D, E, 20170501, 9, C, D, E, 20170301, 6, C, D, E, 20170101, 3, , Nach der Anweisung DELETE ADJACENT DUPLICATES bleiben dann nur die relevanten Datensätze übrig und wir können uns auf ein performantes Nachlesen dieser Datensätze freuen. :), key_field, key_field2, key_field3, date_field, lookup_field, A, B, C, 20170501, 7, B, C, D, 20170501, 8, C, D, E, 20170501, 9, , © 2021 b.telligent;https://www.btelligent.com/blog/performante-lookups-in-bw-transformationen-teil-5/;B-Telligent;
11.05.2017;            Anwendung der SCD-Methodik durch den ODI 12         ;"Wie im vorherigen Blogeintrag dargestellt, bietet der Oracle Data Integrator (ODI) eine eingebaute Lösung, um Daten mit der SCD-Methodik (Slowly Changing Dimension) zu historisieren. Bei näherer Betrachtung und der praktischen Beladung einer Integrationsmenge in eine Zieltabelle mithilfe des Integration Knowledge Modules (IKM) SCD fällt auf, dass der ODI gewisse ""Default Values"" für das Gültigkeitsende des Datensatzes verwendet. Dies kann im folgenden Szenario veranschaulicht werden:, Aus der Staging-Tabelle STAGE_TEST_SCD werden Daten in die Dimensionstabelle DIM_TEST_SCD geschrieben., Um die Ausgangssituation leichter nachzuvollziehen, wird in den folgenden Testcases beschrieben, wie die Funktionen dargestellt werden können., , In der Staging-Tabelle sind drei Datensätze angekommen, die in die leere Tabelle DIM_TEST_SCD per IKM SCD geschrieben werden sollen., , , Wie im Ergebnis zu sehen ist, wird die Gültigkeit des Datensatzes vom IKM serienmäßig auf den 01.01.2400 gesetzt, um eine unendliche Gültigkeit darzustellen., , Nun liefert die Staging-Tabelle Änderungen. Der Datensatz mit NK=1 bleibt unverändert (ATTR), bei NK=2 wurde ATTR auf ""C"" geändert, NK=3 wird nicht mehr geliefert, und NK=4 ist neu hinzugekommen., , , Im Falle einer Änderung wird der ursprüngliche Satz in seiner Gültigkeit terminiert und ein neuer Datensatz mit unendlicher Gültigkeit eingefügt. Wie in diesem Beispiel zu sehen ist, wird im Falle der Terminierung die Gültigkeit des Datensatzes auf den gleichen Wert gesetzt wie der Gültigkeitsbeginn beim Einfügen eines Datensatzes., Wie aus den Beispielen deutlich geworden ist, fügt das IKM SCD serienmäßig für die unendliche Gültigkeit den 01.01.2400 und für die Terminierung eines Datensatzes das Ladedatum ein. In vielen Situationen kann es jedoch gewünscht sein, dass für die unendliche Gültigkeit ein eigens definierter Wert eingefügt oder im Falle der Terminierung die Gültigkeit des Datensatzes auf ""Ladedatum - 1"" gesetzt wird., Damit die Gültigkeit des Datensatzes auf die gewünschten Werte gesetzt wird, sind kleinere Anpassungen im IKM SCD durchzuführen. Für unser Beispiel nehmen wir an, dass das Datum für die unendliche Gültigkeit auf den 31.12.9999 und das Datum bei einem zu terminierenden Satz auf ""Ladedatum - 1"" gesetzt wird. Im ODI können die IKM im Designer-Tab unter dem entsprechenden Projektfolder durch einen Doppelklick geöffnet und editiert werden. Unter dem Reiter ""Tasks"" lassen sich die folgenden Tasks identifizieren, die für die gewünschten Änderungen angepasst werden müssen:, , Bei den Tasks in den grünen Kästchen wird die Gültigkeit des Datensatzes jeweils angepasst. Dafür werden im Bereich ""Target Command"" die Pünktchen neben dem dargestellten Codeausschnitt angeklickt., , Oben wurde eine exemplarische Anpassung an dem Task ""Flag rows for update"" durchgeführt. Damit auch die Gültigkeit des Datensatzes bei einer Terminierung auf den gewünschten Wert gesetzt wird, muss im Task ""Historize old rows"" die Gültigkeit um einen Tag reduziert werden:, , , Im Anschluss an die Anpassungen kann das IKM SCD gespeichert und die Testcases wiederholt werden. Nach Ausführen des ODI-Mappings in Testcase 2 ergeben sich nun folgende Konstellationen:, , , Wie gewünscht wurde die Gültigkeit des Datensatzes auf den 31.12.9999 und im Falle einer Terminierung auf das ""Ladedatum - 1 gesetzt""., Im nächsten Blogeintrag zum Thema ""Adjustments to the IKM Slowly Changing Dimensions"" wird aufgezeigt, wie im IKM SCD durch weitere Anpassungen auch Deletes berücksichtigt werden können., , © 2021 b.telligent";https://www.btelligent.com/blog/anwendung-der-scd-methodik-durch-den-odi-12/;B-Telligent;
11.04.2017;            Der effektive Einsatz von Partition Pruning zur Optimierung der Abfragegeschwindigkeit (Teil 1)        ;"Data Warehouses nehmen im Zuge der digitalen Transformation von Unternehmen eine immer zentralere Rolle ein. Denn im Zuge der Entscheidungsfindung hält ein gutes Data Warehouse für jegliche Fragestellung entscheidungsrelevante Informationen bereit. Ein DWH kann also als ein strategisches Management-Instrument gesehen werden, dessen reibungslose Funktionsweise essentiell für den zukünftigen Unternehmenserfolg ist. In diesem Blogbeitrag wird auf eines der wichtigsten Performancefeatures eingegangen, das Partition Pruning., In diesem Artikel schlage ich eine Möglichkeit zur physischen Organisation von historischen Tabellen vor, die es ermöglicht, Partition Pruning effektiv einzusetzen, um die Abfragegeschwindigkeit zu optimieren. Diese Möglichkeit wurde speziell für Datenbanken entwickelt, weshalb relativ komplizierte Datenmengen und dennoch ergiebige Auswahlmöglichkeiten vorausgesetzt werden., Ich werde zwei Messgrößen verwenden, um die Effektivität jeder der beschriebenen Lösungen zu beurteilen, Bei der Anzahl der Gesamtzeilen wird die Gesamtgröße einer Tabelle ermittelt. Die Messgröße ""Zeilen pro einzelnem Lesevorgang"" vermittelt uns eine Vorstellung davon, wie viele Daten durchsucht werden sollten, um einer typischen Abfrage zu genügen. Bei einer typischen Abfrage einer historischen Tabelle handelt es sich meiner Ansicht nach um eine Abfrage, bei der Werte von historisierten Attributen eines bestimmten Datums zurückgegeben werden. Dies hat Auswirkungen auf einen bedeutenden Teil von Schlüsselwerten, wodurch der Indexzugriff ineffektiv wird., Es ist klar, dass die Effektivität des physischen Designs von der Demographie der Daten abhängt. Für einen ersten Vergleich wird eine Tabelle herangezogen, die den Verlauf von Kontosalden bei einer Bank darstellt. Ziel ist es, darzustellen, wie Änderungen innerhalb der Demographie der Daten die Messgrößen beeinflussen können. Die Daten haben den folgenden Umfang:, Rufen wir uns zunächst die herkömmlichen Wege zur Aufbewahrung historischer Daten in Erinnerung. Bei der am häufigsten eingesetzten Lösung handelt es sich um eine Intervalltabelle. Jede Zeile der Tabelle enthält zwei Daten, die das Gültigkeitsintervall eines Datensatzes angeben., , KONTO_SALDO_INTERVALL, KONTO_ID, ZAHL (18), GÜLTIG_VON, DATUM, GÜLTIG_BIS, DATUM, SALDO_BETRAG, ZAHL (38,6), KONTO_ID, GÜLTIG_VON, GÜLTIG_BIS, SALDO_BETRAG, , Der Saldo für ein bestimmtes Datum könnte durch eine Abfrage ermittelt werden, wie sie in der nachstehenden Abbildung dargestellt ist:, Auf der Grundlage dieser Abfrage können wir den größten Nachteil von Intervalltabellen erkennen: Es besteht keine Möglichkeit, Datenpartitionierung einzusetzen, da GÜLTIG_VON/GÜLTIG_BIS von notwendigen Zeilen uneingeschränkt von dem angegebenen Datum abweichen können., Die Leistungskennzahlen könnten anhand der folgenden Formeln berechnet werden:, TRC = CA * FC * HD * 12, RSR = TRC (da die ganze Tabelle gelesen werden sollte), Insgesamt ergibt sich:, Bei der zweiten Möglichkeit handelt es sich um eine Snapshot-Tabelle, was bedeutet, dass für jedes Konto für jeden Tag Salden gespeichert werden., , KONTO_SALDO_SNAPSHOT, KONTO_ID, ZAHL (18), SALDO_DT, DATUM, SALDO_BETRAG, ZAHL (38,6), KONTO_ID, SALDO_DT, SALDO_BETRAG, , Wenn man auf diese Weise vorgeht, werden viele redundante Informationen gespeichert. Es werden jedoch Abfragen mit ""Equity""-Bedingung ermöglicht, um Salden eines bestimmten Datums auszuwählen:, , , Für diese Art von Abfragen kann Oracle leicht Partition Pruning anwenden, wenn die Basistabelle nach BALANCE_DT unterteilt ist. Aus diesem Grund können wir die Leistungskennzahlen wie folgt berechnen:, TRC = CA * HD * 365, RSR = CA (es gibt einen Datensatz je Konto je Partition), Wird diese Vorgehensweise angewandt, so werden folgende Daten ermittelt:, , , Intervall, Snapshot, Anzahl der Gesamtzeilen, , , Zeilen pro einzelnem Lesevorgang, , , , Jeder einzelne Lesevorgang kostet für eine Snapshot-Tabelle wesentlich weniger als für eine Intervalltabelle. Dieser Vorteil wird jedoch durch einen Anstieg der Gesamtgröße der Tabelle zunichtegemacht. Für eine Tabelle dieser Größe bestehen weniger Chancen zur Zwischenspeicherung, und es ist mehr Zeit für Backup-, Wiederherstellungs- und Wartungsvorgänge erforderlich., Mit welcher Lösung diese Faktoren umgangen werden können, erfahren Sie in unserem nächsten Blogbeitrag!, , © 2021 b.telligent";https://www.btelligent.com/blog/der-effektive-einsatz-von-partition-pruning-teil-1/;B-Telligent;
06.04.2017;            Performante Lookups in BW-Transformationen – Die relevanten Datensätze finden        ;Nachdem wir uns mit relevanten Selektionstechniken und mit den unterschiedlichen Arten von internen Tabellen auseinandergesetzt haben, sind die wichtigsten Performanceoptimierungen für die Lookups in unseren BW-Transformationen zunächst einmal sichergestellt., Hiermit ist das Thema jedoch nicht komplett abgedeckt.Denn bis jetzt sind wir davon ausgegangen, dass nur die relevanten Informationen in unseren Lookup-Tabellen durchsucht werden. Wie können wir dies aber sicherstellen?, , , Anhand unseres Beispielswird in diesem Artikel verdeutlicht, wie man aus der Datenbanktabelle Y nur die relevanten Datensätze in der internen Tabelle Z zum Durchsuchen selektiert. Die Größe der Datenbanktabelle Y ist hier von zentraler Bedeutung. Wenn in der Transformation die Datenpakete 50.000 Datensätzen beinhalten, die Y-Tabelle allerdings Millionen von Datensätzen enthält, muss sichergestellt werden, dass diese nicht bei jedem Datenpaket komplett durchsucht werden., Um dies zu gewährleisten, wenden wir folgende Vorgehensweise an. Wir nutzen bei der SELECT aus der Datenbank die Anweisung FOR ALL ENTRIES. Dabei sollte man unbedingt folgende Punkten beachten:, Außerdem gelten bei der Verwendung dieses Zusatzes mehrere Einschränkungen, die berücksichtigt werden sollten. Unter anderem kann man damit keine SINGLES, keine UNIONS und keine GROUP BY-Selektionen durchführen., Da FOR ALL ENTRIES keine Standard-SQL-Anweisung ist, sondern eine SAP-Erweiterung darstellt, wird bei der Verarbeitung auf den unterschiedlichen Datenbanken letztendlich die Anweisung vom ABAP-Stack ins Standard-SQL übersetzt. Bei sehr komplexen FOR ALL ENTRIES bestehen aus dem Grund weitere Optimierungsmöglichkeiten mithilfe von Parametrisierungen oder Hinweisen. Wenn man eine SAP-HANA-Datenbank im Einsatz hat, kann sogar ein Funktionsmodul (RSDU_CREATE_HINT_FAE) Anwendung finden, das die optimale Durchführung von FOR ALL ENTRIES vorbereitet. Weitere hilfreiche Informationen zu dem Thema können den folgenden SAP-Hinweisen entnommen werden:, Im nächsten Artikel werden wir uns der oftmals notwendigen Aggregation bzw. Verdichtung von nicht eindeutigen Datensätzen widmen. Dies ergibt sich beispielsweise, wenn die aus der Datenbank selektierten Informationen in unterschiedlichen Gruppenverteilt werden müssen., , © 2021 b.telligent;https://www.btelligent.com/blog/performante-lookups-in-bw-transformationen-teil-4/;B-Telligent;Markus Sontheimer
16.03.2017;            Machine Learning in der Cloud mit Microsoft Azure        ;In diesem Blogbeitrag schauen wir uns Azure Machine Learning an. Das Tool eignet sich für Data-Science-Einsteiger genauso wie für Experten, da die graphische Oberfläche intuitiv bedienbar und gleichzeitig flexibel ist., ,                               Quelle: Microsoft (https://studio.azureml.net/), , Mittlerweile bietet Microsoft mit der Azure Cloud viele Dienste, die bisher in der Regel on premise – also auf den eigenen Servern – gehostet wurden. Ebenso gibt es für Machine Learning ein gutes Tool aus dem Hause Microsoft für die Cloud und ganz ohne Installationen., Um mit dem Microsoft Azure Machine Learning Studio zu arbeiten, ist lediglich eine Anmeldung auf der folgenden Seite Voraussetzung. Anschließend kann direkt mit dem Tool gestartet werden:, Die so genannten Experimente sind kostenfrei, eine Berechnung der CPU-Zeit findet hier nicht statt. Erst bei größeren Projekten wird die CPU-Zeit kostenpflichtig. Die genauen Preise können bei Microsoft nachgelesen werden: https://azure.microsoft.com/de-de/pricing/details/machine-learning/., Im Übrigen sind die hier gezeigten Screenshots immer in englischer Sprache. Das hat in der Benutzung den Vorteil, dass man bei Schwierigkeiten mit einzelnen Funktionen automatisch den richtigen Begriff auf Englisch googelt und mehr Ergebnisse erhält., Um die Möglichkeiten und die Bedienung des AzureML-Studios zeigen zu können, bediene ich mich des Titanic-Datensatzes. Der Datensatz ist der Klassiker für den Test von Machine-Learning-Algorithmen und beinhaltet u.a. folgende Informationen über die Passagiere der Titanic: Name, Alter, Buchungsklasse, Geschlecht und Überlebensstatus., Um einen neuen Datensatz hochzuladen (z.B. als csv-Datei), geht man im Studio erst in den Bereich der Datensätze und klickt dann unten links auf „New“., Anschließend wählt man „from local file“ aus und gibt die Rahmendaten der hochzuladenden Datei an., , Das neue Experiment kann über die linke Navigationsleiste aufgerufen bzw. erzeugt werden. Zunächst lädt man die gerade hochgeladenen Daten in diese Ansicht, indem man bei „Saved Datasets“ den Datensatz heraussucht und per Drag and Drop auf die graue Fläche zieht., , , Es werden in den nächsten Schritten Daten sowohl für das Trainieren eines Modells als auch für die Auswertung des Modells benötigt. Daher sollte der Datensatz zunächst einmal in zwei Teile gesplittet werden. Die dafür vorgesehene Funktion findet sich unter „Data Transformation“., , , Nachdem diese Funktion in das Experiment gezogen wurde, können die Eigenschaften noch konfiguriert werden. In diesem Fall werden 70% der Daten für das Trainieren des Modells genutzt und 30% für die spätere Auswertung., , Im nächsten Schritt sucht man sich ein passendes Modell aus den zur Verfügung stehenden aus – in diesem Fall wird das „Two-Class Neural Network“ genutzt und mit dem Baustein „Train Model“ verknüpft. An dieser Stelle muss zudem festgelegt werden, welche Spalte in unseren Daten dieses Modell später voraussagen soll. Für den zugrundeliegenden Titanic-Datensatz ist das die Spalte „Survived“., , , Zusätzlich kann man den hochgeladenen Datensatz vor dem Trennen noch bereinigen, sodass nur die wirklich relevanten Daten herausgefiltert werden. In diesem Fall werden nur Spalten ausgeschlossen, die für die Auswertung uninteressant sind (z.B. Namen)., Für eine testweise Ausführung mit dem anderen Teil der Daten werden diese zusammen mit dem trainierten Modell dem Baustein „Score Model“ zugeführt. Nach der Ausführung, die durch den Klick auf „Run“ gestartet wird, kann die Visualisierung der Ergebnisse durch einen Rechtsklick auf den grauen „Ausgang“ des Bausteins „Score Model“ angezeigt werden., , , Die teilweise geringen Prozentpunkte der Ergebnisse verwundern auf den ersten Blick. Warum entscheidet sich das trainierte Modell für „nicht überlebt“, obwohl die zugehörige Wahrscheinlichkeit unter 13 Prozent liegt? Die Antwort: Weil die Wahrscheinlichkeit für „überlebt“ noch geringer gewesen wäre. Man beachte hier, die Gegenwahrscheinlichkeit „-1-Wahrscheinlichkeit“ entspricht: P (überlebt) ? 1 – P (nicht überlebt)., , , Im Baustein „Evaluate Model“ wird die Ausführung des Modells statisch ausgewertet. Die dort gezeigten Daten eignen sich gut, um die Ergebnisse mit anderen Experimenten zu vergleichen. So könnte man bei sonst gleichen Voraussetzungen ein anderes Modell wählen und prüfen, welches bessere Ergebnisse liefert., Der Titanic-Datensatz ist vergleichsweise klein und die Ergebnisse sind entsprechend schwach. Auch hier gilt der Spruch: „Garbage in, Garbage out“. Mit anderen Worten: Die Ergebnisse können nicht besser sein als die Daten, auf denen sie beruhen., , Das gesamte Experiment umfasst schlussendlich sieben Bausteine und kann ohne höhere Mathematik innerhalb von etwa 30 Minuten problemlos aufgesetzt werden. Es eignet sich auch gut als Vorlage für weitere, ähnlich einfach gehaltene Projekte., Auf diese Weise können aber auch komplexe Machine-Learning-Algorithmen ausprobiert, angelernt und für Datenauswertungen genutzt werden – ohne hierfür teure Hardware und Softwarelizenzen anschaffen zu müssen., , , © 2021 b.telligent;https://www.btelligent.com/blog/machine-learning-in-der-cloud-mit-microsoft-azure/;B-Telligent;Dr. Sebastian Petry
28.02.2017;            Wie bekomme ich auch HANA langsam?        ;"Die gute Performance einer HANA-Datenbank kommt von der konsequenten Orientierung auf eine hauptspeicherbasierte Datenhaltung sowie durch die Nutzung moderner Kompressions- und ColumnStore-Algorithmen. Somit muss die Datenbank bei Aggregationen über große Datenmengen vergleichsweise wenige Daten lesen und kann dies im Hauptspeicher auch noch außerordentlich schnell erledigen., Jedoch kann bei einem suboptimalen Design des Datenmodells einer dieser Vorteile sehr schnell hinfällig werden. So gehen sowohl der HANA-Datenbank als auch den Anwendern große Vorteile hinsichtlich der Laufzeit und Agilität während der Nutzung verloren., Eine Ablage der Daten erfolgt in der HANA-Datenbank in zwei Schritten: Zuerst werden die Daten im Delta Store geschrieben (zeilenbasiert) und dann beim Delta Merge in den Main Store überführt. Dieses Verfahren wird auch von anderen Herstellern angewandt. Ausschlaggebend dafür ist, dass komprimierte ColumnStore-Index-Schreiboperationen nicht direkt erlaubt sind. Denn diese sind einerseits komprimiert und andererseits müssen die Daten ""aufwändig"" in das spaltenbasierte Format überführt werden., , , Der ColumnStore ist insbesondere dann vorteilhaft, wenn wenig Spalten gelesen und viele Zeilen zusammengerechnet werden sollen (bei BI-Anwendungen ist dies im Gegensatz zu ERP-Anwendungsfällen nicht unüblich), und verringert wesentlich die Zugriffe auf das Storage-Medium, da die Daten bereits in Spalten organisiert sind., Ein klassischer RowStore muss dagegen immer alle betroffenen Zeilen in voller Breite lesen. So entsteht beim Aggregieren ein großer Overhead. Folglich kann festgehalten werden, dass ein ColumnStore-Index für das Zusammenrechnen vieler Daten sehr schnell ist, da die notwendigen Informationen voraggregiert abgelegt sind., , Die HANA-Datenbank muss also wenig arbeiten, wenn ich wenige Daten anzeige, aber viele aggregiere. Was passiert jedoch, wenn ich zwei Tabellen über eine granulare Spalte verbinde?, , , Es ist deutlich zu erkennen, dass die beiden Tabellen auf der Ebene des Join-Kriteriums gelesen werden, bevor eine Aggregation stattfindet. Dies hat zur Folge, dass die HANA-Datenbank viel mehr Daten aus dem Hauptspeicher lesen muss und diese gleichzeitig ""on the fly"" zu verarbeiten hat. So wird der Vorteil eines ColumnStore durch das Datenmodell zunichtegemacht und die Laufzeit erhöht sich im Vergleich zum persistierten Datamart stark. Bei der Datenmodellierung ist also eine Abwägung zwischen der Flexibilität des Modells und der Performance essentiell., Ein CompositeProvider joint zwei ADSOs mit einerseits Auftragskopf- und andererseits Auftragspositionsdaten, um diese gemeinsam im Reporting zur Verfügung zu stellen. Wir haben 40.000.000 Kopfdaten und 180.000.000 Positionen. Jede Ausführung (und auch Navigation) der Query dauert 5 Sekunden (was in Anbetracht der Datenmenge sehr schnell ist, aber HANA liest ja auch aus dem Hauptspeicher). Persistieren wir nun den Join in einem weiteren ADSO, sodass die Join Engine nicht mehr genutzt wird, zeigt die Query eine Laufzeit von unter 1s. Der Grund ist trivial: Die HANA-Datenbank liest einfach weniger bei gleichem Ergebnis., Ein Beispiel ist der Distinct Count (Ausnahmeaggregation: Zähler für alle detaillierten Werte, die nicht null sind oder die null sind oder die Fehler enthalten). Die Query liest die SIDs vom InfoObjekt zur Ermittlung der Kennzahl. Dies geschieht über einen Join. Ist das InfoObjekt sehr groß, macht sich das in der Laufzeit der Query bemerkbar, wenn die Kennzahl zur Anzeige kommt. Eine Möglichkeit zur Optimierung ist die Übernahme der SIDs in den DataStore, womit ein Join unterdrückt wird. Details zu diesem Thema werden in einem späteren Artikel erscheinen, der auf die Optimierung von Distinct-Count-Kennzahlen eingeht., , © 2021 b.telligent";https://www.btelligent.com/blog/wie-bekomme-ich-auch-hana-langsam/;B-Telligent;Markus Sontheimer
24.02.2017;            Performante Lookups in BW-Transformationen - Die richtige Tabellenart wählen        ;"Dies ist vielleicht die grundlegendste aller ABAP-Fragen, und zwar nicht nur, wenn man sich mit performanten Lookupsauseinandersetzt. Sobald man etwas in ABAP macht, wird man auf diese Frage stoßen., Die am häufigsten verwendete interne Tabellenart ist die Standardtabelle. Dies ist zum Teil historisch bedingt, da Hash- und sortierte Tabellen erst seit dem Release 4.0 verfügbar sind. Des Weiteren sind die Standardtabellen schnell zu befüllen, da jeder neue Datensatz ohne weiteres am Ende der Tabelle hinzugefügt werden kann (siehe APPEND). Bei sortierten oder Hash-Tabellen muss dagegen erst einmal der richtige Ort gefunden (siehe INSERT) bzw. der Hash-Schlüssel generiert werden. Zum Schreiben ist die Standardtabelle somit zu bevorzugen., Für Lesezugriffe sind Standardtabellen allerdings ein Performancekiller. Denn diese werden im Gegensatz zu den anderen Tabellen linear durchsucht. Wird also eine Standardtabelle mit 50.000 Datensätzen durchsucht, beginnt die Suche immer beim allerersten Datensatz. Im ungünstigsten Fall, wenn der gesuchte Datensatz an der letzten Stelle steht, müssen alle vorhergehenden 49.999 Datensätze auch gelesen werden., Wie sich das bei Lookups auswirkt, wird anhand des nachstehenden Beispiels verdeutlicht:, , Nehmen wir an, wir haben eine interne Tabelle X mit 50.000 Datensätzen, bei der jeder Datensatz mit Informationen einer Datenbanktabelle Y ergänzt werden muss. Im besten Fall haben wir nur die relevanten Datensätze aus Datenbanktabelle Y in einer internen Tabelle Z geladen.Diese enthält somit für jeden unserer X-Datensätze genau einen entsprechenden Datensatz(auch 50.000)., Die Art der Tabelle X ist zweitrangig, da wir dadurch LOOPEN müssen und dabei jeden einzelnen Datensatz auslesen. Wenn Z aber eine Standardtabelle ist, werden wir für unser LOOKUP insgesamt 1.250.025.000 (50.000!) Lesezugriffe haben. Bei SAP BW on HANA verarbeitet man oft sogar Datenpakete mit 200.000 oder mehr Datensätzen. In unserem Beispiel würden wir bei der Anwendung einer Standardtabelle für Z dann bei 20.000.100.000 Lesezugriffen pro Datenpaket landen., Selbstverständlich können auch die Standardtabellen aufsteigend sortiert und dann mit BINERY SEARCH durchsucht werden. Warum dann aber nicht gleich eine sortierte oder Hash-Tabelle nehmen? Eine Daumenregel ist, dass bei ca. 10.000 Datensätzen beide Tabellenarten ungefähr gleich schnell sind. Bei deutlich größeren Tabellen ist die Hash-Tabelle nicht zu schlagen, da die Zugriffszeit, unabhängig von der Größe, nahezu konstant ist. Bei weniger als 10.000 Datensätzen gibt es in der Regel keine Performanceprobleme., Hash-Tabellen haben aber den ""Nachteil"", dass sie nur eindeutige Datensätze enthalten sollen. Wenn es also nicht möglich ist, eindeutige Tabellenschlüssel zu erzeugen, zum Beispiel durch die Aggregation der selektierten Daten, sollte man mit sortierten Tabellen arbeiten., , © 2021 b.telligent";https://www.btelligent.com/blog/performante-lookups-in-bw-transformationen-teil-3/;B-Telligent;Markus Sontheimer
15.12.2016;            Performante Lookups in BW Transformationen - Die Nutzung interner Tabellen vs. SELECTS aus der HANA-Datenbank        ;"Indieser Seriewollen wir uns mit Implementierungstechniken für Lookups auseinandersetzen, bei denen jeder Datensatz der zu durchsuchenden Tabelle überprüft werden sollte. Je größer unsere Datenpakete und unsere Lookup-Tabellen, desto wichtiger ist eine performante Implementierung., Weil man in BW-Transformationen normalerweise Datenpakete mit einer Größe von 50.000 oder mehr Datensätzen verarbeitet, ist es nicht zu empfehlen, einzelne SELECT-Anweisungen innerhalb der LOOP-Schleife in ABAP-Routinen durchzuführen. Dies würde dazu führen, dass für jeden Datensatz aus dem Datenpaket eine Datenbankselektion durchgeführt wird. Auch wenn man eine schnelle In-Memory-Datenbank wie SAP HANA hat, geschieht das Nachlesen von Millionen Datensätzen daraus nicht automatisch blitzschnell. Dies liegt auch an dem Overhead, der bei jedem einzelnen Zugriff entsteht. Besser ist es, vor der LOOP-Schleife der Transformation die relevanten Daten mit einer oder wenigen SELECT-Statements in internen Tabellen zu speichern und diese später mit der READ-Anweisung auszulesen. Am besten sollte man dafür Hash-Tabellen verwenden., Ob man dann mehrere SELECT-Anweisungen pro Datenpaket (1) durchführt …, ""Bei Bedarf Sortierung/Aggregation/Reduktion der Felder on itab_group1 implementieren, ""Bei Bedarf Sortierung/Aggregation/Reduktion der Felder on itab_group1 implementieren, … oder eine SELECT-ENDSELECT-Schleife (2) nutzt, um mehrere interne Tabellen zu befüllen, ist an der Stelle für die Geschwindigkeit unerheblich., ""Ab diesem Punkt ist das Feld mit der Gruppe nicht mehr wichtig =&gt; ergo geben wir nur die restlichen Zeilen weiter, ""Bei Bedarf Aktualisierung der Zeile von itab_group1 implementieren, ""Wie oben, aber für itab_group2, Bei einem DTP mit Paketgröße 200.000 und knapp 16 Mio. zu beladenden Datensätzen haben wir mit (1) 43m 39s und mit (2) 42m 25s gebraucht. Dabei haben wir aus einer Tabelle mit knapp 190 Mio. Datensätzen nachgelesen und müssten insgesamt 5 Gruppen von Daten in internen Tabellen aufbauen, die beim Lookup in der LOOP-Schleife der Transformation durchgesucht werden mussten. Und ja, diese Beladungen wurden auf BW on HANA durchgeführt., Zum Vergleich, wenn wir an der Stelle bei den 5 Lookups innerhalb der LOOP-Schleife aus der Datenbank nachlesen würden, würden wir für die Beladung derselben Datenmenge ca. 5 Stunden verbrauchen., Dass die Methoden (1) und (2) vergleichbar schnell laufen, bestätigt auch die These (Kapitel 4.4.2), dass der gängige schlechte Ruf von SELECT-ENDSELECT-Schleifen nicht gerechtfertigt ist. Diese haben zwar gewisse Nachteile, performancetechnisch sind sie aber nicht unbedingt langsamer als SELECT-INTO-Statements. Vor allem, wenn man dadurch eine einfachere und besser nachvollziehbare Programmierlogik aufbauen kann, sollten diese bevorzugt werden. Bei In-Memory-Datenbanken werden die Nachteile von SELECT-ENDSELECT-Schleifen irrelevanter., Wenn man auch noch darauf achtet, nur die Datensätze und Felder zu selektieren, die man später benötigt, kann man sich in der Regel auch auf performante Transformationen freuen. Der Implementierungsaufwand an der Stelle lohnt sich!, , © 2021 b.telligent";https://www.btelligent.com/blog/performante-lookups-in-bw-transformationen-teil-2/;B-Telligent;Markus Sontheimer
24.11.2016;            SAP HANA – kein Speicher mehr? Bewusster Early Unload!        ;"Die Ausnutzung des Hauptspeichers ist bei SAP-HANA- und Data-Warehouse-Szenarien immer ein spannendes Thema im Vergleich zu ERP-Anwendungen bzw. Anwendungen mit einem sich gering verändernden Datenvolumen. Somit muss man eines im Hinterkopf behalten: Lasse niemals den freien Hauptspeicher ausgehen., Eine HANA-Datenbank bringt bei der Berechnung des Hauptspeichers einige interessante Eigenschaften gegenüber klassischen Datenbanksystemen mit sich. Einerseits ist der Hauptspeicher die zentrale Ablage der Daten (klassisch: Disk -&gt; Hauptspeicher, HANA: Hauptspeicher -&gt; Auslagerung auf Disk), und andererseits wird der Hauptspeicher auch für die Ausführung von Abfragen genutzt. Somit konkurriert der persistente Bereich mit dem dynamischen Bereich für Abfragen. Dies wird auch von der Berechnungsgrundlage der SAP für HANA-Datenbanken berücksichtigt. Vereinfacht setzt sich diese zusammen aus RAM = Data Footprint * 2 / 7. Zur Berechnung nimmt man die Byte-Breite der Spalten mal die Anzahl der zu erwartenden Zeilen. Diese werden mit dem Faktor 2 multipliziert, damit immer die Hälfte des Hauptspeichers für Abfragen zur Verfügung steht, und durch 7 dividiert, da von einem durchschnittlichen Kompressionsfaktor von 7 ausgegangen wird., Aus der Erfahrung hat sich gezeigt, dass gerade breitere Faktentabellen aufgrund von Spalten mit niedriger Kardinalität (also wenig verschiedenen Werten) sich sehr gut komprimieren lassen, sodass dort teilweise Kompressionsraten von 30 auftreten. Tabellen mit hoher Kardinalität, wie z.B. hochzählende IDs, haben dabei jedoch einen geringeren zu erwartenden Kompressionsfaktor. Dieses Verhalten resultiert darin, dass Tabellen mit wenig Spalten plötzlich viel mehr Speicher belegen können als sehr breite Tabellen., Um auch sehr große Datenmengen verarbeiten zu können, existieren innerhalb einer HANA-Infrastruktur viele Möglichkeiten, den Hauptspeicher der HANA-Datenbank zu entlasten. Möglichkeiten mit SAP BW on HANA: Auslagerung auf SSD und Einbindung von Archivservern (Nearline Storage). Die Daten bleiben im Zugriff, belasten jedoch den Hauptspeicher der HANA-Datenbank nicht mehr., 1: HOT à Daten im Hauptspeicher 2: Warm à Daten auf SSD in der HANA-DB – Early Unload oder Extended Tables (SAP HANA Dynamic Tiering) 3: Cold à Daten ausgelagert im NLS, , Aber auch bei einer einfachen HANA-Instanz ohne Extended Storage (das Datenmodell wird klassisch auf Platte ausgelagert), Nearline Storage etc. lässt sich das Nutzungsverhalten des Hauptspeichers optimieren, sodass auch ein Data Warehouse mit mehreren Schichten (wie z.B. ein SAP BW mit LSA++) nicht unnötig Daten dort vorhält. Somit lassen sich auch Umgebungen mit 1-TB-Data-Footprint in eine 1-TB-HANA-Instanz laden. Der Trick ist die Nutzung und explizite Steuerung des Early Unload., Eine HANA-Datenbank lagert Tabellen anhand ihrer Priorität vom Hauptspeicher auf das physische Storage aus (genannt UNLOAD). Damit stehen die Daten zwar auch nicht mehr direkt für Abfragen zur Verfügung, für Staging-Tabellen kann dies jedoch vernachlässigt werden, da diese z.B. häufig nur in der Nacht benötigt werden, wenn eine Prozesskette läuft. Somit kann ein SAP BW on HANA gerne auch mehr Speicher belegen als in der von SAP angesprochenen Empfehlung und sauber funktionieren, da Daten im Hauptspeicher bei einem hohen „Memory Pressure“ automatisch ausgelagert werden. Die Reihenfolge von Auslagerungen wird über eine Priorität pro Tabelle eines Objekts in der Transaktion RSHDBMON gesteuert. Einige SAP-BW-Objekte wie PSA-Tabellen, Change Logs von normalen ADSOs oder schreiboptimierte ADSO-Objekte haben das Early-Unload-Flag auch im Standard bereits komplett oder in Teilen gesetzt, je nach der von SAP vorgesehenen Nutzung., Für SAP-BW-Objekte kann nur die Priorität 5 (aktive Daten fürs Reporting) und 7 (Early Unload, inaktive Daten wie Change Logs …) gesetzt werden., Die Steuerung des Unloads von der HANA-Datenbank selber klappt im Allgemeinen sehr gut und auch ohne große Auswirkung auf Abfragen, da nur schreibend auf den physischen Storage zugegriffen wird und die Abfragen aus dem Hauptspeicher bedient werden. Es gibt jedoch einen Punkt, an dem die automatische Handhabung des Unloads nicht optimal funktioniert, und zwar bei der Aktivierung von sehr großen Requests. Die Ursache ist, dass alle Tabellen der betroffenen Objekte in den Hauptspeicher geladen werden. Bei einem normalen ADSO sind dies z.B. die Eingangstabelle, die aktive Tabelle und das Change Log zum Teil. Des Weiteren wird der Hauptanteil der Datenbankoperationen in einer Transaktion ausgeführt und somit der Hauptspeicher stark belastet. Ein automatisch getriggerter Unload kann häufig nicht rechtzeitig durchgeführt werden, bis es zum Rollback kommt und die Aktivierung abbricht (siehe: SAP Hinweis 2399990 – How-To: Analyzing ABAP Short Dumps in SAP HANA Environments)., Mit zwei Vorgehensweisen kann die HANA-Datenbank aber bewusst unterstützt werden:, 1. Die Beladung wird in kleineren Paketen über mehrere DTPs mit disjunkten Filtern ausgeteilt (kleinere Datenbanktransaktionen)., 2. Alle nicht benötigten Tabellen werden proaktiv aus dem Speicher geladen (bewusster Early Unload)., Eine Beispielrechnung:, , Eine HANA-DB würde somit den maximal verfügbaren Hauptspeicher für die physische Datenhaltung belegen: bei einer Größe von 512Gbyte ca. 460Gbyte aufgrund des Erreichens der Grenze, ab der ein Unload von Tabellen beginnt., Die PSA, Data Acquisition Layer und EDWH Layer sind nur zum Teil geladen. Diese haben normalerweise das Early-Unload-Flag bereits gesetzt und wurden somit schnell aus dem Hauptspeicher entfernt. Der Zustand ist in der Transaktion DB02 in der Spalte „Loaded“ sichtbar., , Wollen wir nun den Hauptspeicher prophylaktisch um alle Tabellen bereinigen, die ein Early-Unload-Flag gesetzt haben, würden auch alle von der HANA-DB optional im Hauptspeicher gehaltenen Tabellen entladen werden. In unserem Beispiel hätten wir vereinfacht nur noch eine Belegung von 200 Gbyte und könnten mit dem gewonnenen Speicherplatz auch größere Aktivierungen in dem System durchführen. Auch kann mit diesem Vorgehen die Belastung tagsüber reduziert werden. Es ist dabei aber zu beachten, dass sich die Laufzeit von Prozessketten dadurch verlängern kann, denn in der Nacht müssen die Tabellen für das Staging ggf. wieder in den Hauptspeicher geladen werden., Umsetzen lässt sich ein bewusster Early Unload sehr einfach mit einer ABAP Managed Stored Procedure und ist damit auch im normalen Transportwesen auf die Systeme verteilbar. Auch die Integration in Prozessketten oder Reports lässt sich durch die ABAP-Integration einfacher umsetzen., , , Abschließend möchte ich jedoch noch festhalten, dass dieses Vorgehen nur in speziellen Fällen Anwendung finden sollte: um bei Fullloads eine besondere Behandlung in mehreren DTPs zu vermeiden oder um bei einem nicht erwarteten Datenwachstum die Systemstabilität sicherzustellen. Eine Dauerlösung ist es nicht, jedoch ein guter Weg, sich Zeit zu verschaffen, um eine Erweiterung der Infrastruktur oder eine zu Beginn beschriebene Archivierung von historischen Daten vorzubereiten., , © 2021 b.telligent";https://www.btelligent.com/blog/sap-hana-kein-speicher-mehr/;B-Telligent;Markus Sontheimer
23.11.2016;            Performante Lookups in BW-Transformationen aus der Praxis - Einführung        ;"Performanceoptimierungen können nicht in Stein gemeißelt werden. Denn Optimierungen, die bei einem Unternehmen mit bestimmter Systemarchitektur und bei gewissem Datenvolumen super funktioniert haben, müssen nicht zwingend an einer anderen Stelle genauso gut klappen. Kurzum: Es müssen individuelle Lösungen erarbeitet werden. Prinzipiell geht es aber immer darum, die Balance zwischen Arbeitsspeicherauslastung und Datenbankauslastung sowie zwischen Implementierungskomplexität und Wartbarkeit zu finden. Dabei steht die Verarbeitungszeit stets im Zentrum., , , Wie wir aus dem vorangehenden Artikel erfahren haben, ergibt es weiterhin Sinn, auch beim Ansatz einer In-Memory-Datenbank wie HANA, Cache-Tabellen in SAP-BW-Transformationen zu verwenden. Mit Cache-Tabellen sind in diesem Kontext interne ABAP-Tabellen gemeint, die zur Laufzeit von Daten-Transformations-Prozessen (DTP) erstellt und befüllt werden. Dies ist vor allem dann der Fall, wenn Implementierungslogik in ABAP-Routinen umgesetzt werden muss, weshalb die Transformationen nicht in der SAP-HANA-Datenbank verarbeitet werden können., Oft braucht man die Datenzeilen nur zu duplizieren oder zu löschen. Dies geschieht am besten mit Blockoperationen. Im Gegensatz dazu muss in anderen Fällen mit Lookups gearbeitet werden:, Bei Lookups wird im Besten Fall jeder relevante Datensatz mit Informationen aus einer oder mehreren weiteren Datenbanktabellen bereichert. Genau in diesen Fällen ist der richtige Aufbau der Lookups kritisch für die Performance der BW-Transformation., Anhand eines Beispiels, das mich zuletzt häufiger in der Praxis begleitet hat, werden wir uns in dieser Serie mit möglichen Implementierungstechniken für den Aufbau performanter Lookups auseinandersetzen., Freuen Sie sich also auf unsere fünfteilige Serie zum Thema ""Perfomante Lookups in BW-Transformationen aus der Praxis"". Dabei werden die häufigsten, praxisrelevanten Fragen im Detail geklärt. Folgende Inhalte werden in den nächsten Teilen dieser Blogserie geklärt:, Erfahren Sie in den nächsten Wochen mehr zum Thema und schauen Sie auf unserem Blog vorbei., , © 2021 b.telligent";https://www.btelligent.com/blog/performante-lookups-in-bw-transformationen-teil-1/;B-Telligent;Markus Sontheimer
21.11.2016;            Enterprise Data Warehouse und agiler SQL Data Mart: SAP BW on HANA kann beides – das „Mixed Scenario“        ;Bei dem Einsatz eines SAP Business Warehouses hat es in der Vergangenheit häufig verschiedene Ansätze in Unternehmen gegeben, die zu dem Aufbau einer parallelen Infrastruktur führen konnten. Die Betreuung dieser Infrastruktur liegt dabei stärker im Fachbereich als in der IT. Verbreitet sind Lösungen wie QlikView, SQL Server, Oracle oder TM1. Diese erfüllen ihre Aufgaben in der jeweiligen Situation sehr gut, ansonsten würde es sicher nicht den Drang geben, sie einzusetzen., Rückblickend ist das dem angedachten Einsatzzweck von SAP BW als Data Warehouse zuzutragen, das einige Anpassungen nicht „mal eben so“ unterstützt. In den meisten Fällen ist das auch ein Vorteil: Denn so können einerseits Probleme mit falschen Datentypen nachhaltig vermieden sowie ein konsistenter Stand der Daten gewährleistet werden, und andererseits lassen sich technisch relevante KPIs an zentraler Stelle überwachen. SAP hatte bereits mit den „BW Workspaces“ den ersten Schritt gemacht, dem Anwender die Freiheit zu geben, selbst Daten in SAP BW zu laden und mit den bestehenden Datenmodellen zu verbinden. Mit dem unter SAP BW on HANA eingeführten „Mixed Scenario“ geht SAP noch weiter: Denn hier wird das Enterprise Data Warehouse in Form von SAP BW mit einer flexiblen SQL-orientierten Ebene verbunden, die gerade bei sich ständig verändernden Fachbereichsanforderungen in vielen Fällen schnelle Reaktionszeiten ermöglicht. Das SAP BW wird dadurch jedoch keinesfalls überflüssig, sondern beide Szenarien ergänzen sich mit den besten Eigenschaften aus beiden Welten. Eine Integration kann dabei in beide Richtungen erfolgen. Über externe HANA Views (von SAP BW automatisch generierte Views) werden multidimensionale InfoCubes, Queries oder Stammdaten in der HANA-nativen Ebene verfügbar gemacht. Tabellen aus HANA können über Open ODS Views direkt in Reports integriert werden. Dabei werden alle wichtigen Bereiche wie z.B. Berechtigungen im SAP BW mit berücksichtigt., , Das zuvor beschriebene Szenario bringt wesentliche Vorteile mit sich. Beispielsweise entfällt häufig der mit der Drittanbieterlösung notwendige Datentransfer und gleichzeitig können auch die Berechtigungen in den zentralen Punkten wiederverwendet werden. Trotz der Trennung beider Komponenten muss weiterhin eine enge Abstimmung erfolgen und im BI Competence Centerdarauf geachtet werden, dass kein Aufbau eines parallelen DWHs erfolgt. Diese neue Flexibilität kann ähnlich der von uns an anderer Stelle dargelegten Report-Lebenszyklengesteuert werden. Hier werden aktuelle Self-Service-Lösungen betrachtet, die sowohl die Anforderungen der Fachbereiche an Flexibilität als auch die Überführung in eine Standardisierung steuern. Die Anforderungen und Prozesse der IT werden durch die Trennung in einem eigenen Tenant nicht gestört., Eine technische Übersicht der Möglichkeiten finden Sie in dem SAP-Blog unter folgender Adresse: https://blogs.sap.com/2016/10/31/sap-hana-bw-mixed-scenarios-architecture/, , © 2021 b.telligent;https://www.btelligent.com/blog/sap-bw-on-hana-mixed-szenario/;B-Telligent;Markus Sontheimer
27.10.2016;            Das Auslesen einer Hierarchie mit der Funktion ZEILENBEZUG        ;Folgende Hierarchie sei gegeben:, , , © 2021 b.telligent;https://www.btelligent.com/blog/howto-der-zeilenbezug-in-longview/;B-Telligent;Stefan Kersten
06.10.2016;            b.telligent auf der Predictive Analytics World 2016        ;Erdbeeren dabei zuzusehen, wie sie langsam verschimmeln, ist eine Vorstellung, die auf die meisten Menschen nicht sehr attraktiv wirkt. Dr. Ralf Herbrich allerdings kann von verschimmelnden Erdbeeren derart anregend erzählen, dass seine Keynote im Gedächtnis blieb. Herbrich leitet das Amazon Development Center Germany in Berlin, und die Erdbeeren sind ein Teil von Amazons Vorstoß ins Geschäft mit frischen Lebensmitteln. Wer selbst frische Lebensmittel liefern will, tut gut daran, zu wissen, wie lange die Äpfel, Gurken und Erdbeeren noch halten werden, die man im Lager hat. Heute weiß das niemand. Amazon wird es bald wissen: Data Scientists am Berliner Standort bauen Machine-Learning-Pipelines, die aus Bildern von Obst und Gemüse die restliche Haltbarkeit schätzen können. Dies ist jedoch nur ein Beispiel der Amazon-Philosophie, wo immer möglich die zahlreichen Entscheidungen, die täglich millionenfach zu treffen sind, mit Machine Learning zu unterstützen und zu steuern., In der anschließenden Kaffeepause war zu merken, dass die Veranstaltung wieder enorm gewachsen ist. Auch die großen Konzerne haben die Konferenz für sich entdeckt, neben Amazon waren auch Microsoft und IBM mit Vorträgen vertreten. Glücklicherweise ging das Wachstum nicht auf Kosten der einzigartigen inhaltlichen Tiefe, die die Predictive Analytics World unangefochten zur besten Data-Science-Konferenz im deutschsprachigen Raum macht. Dazu trug auch das neue Konzept der Deep Dives bei, einstündiger Einheiten mit Voranmeldung, die sich an diejenigen Experten wenden, die keine Angst davor haben, sich auch mit den kopfschmerzträchtigen Details des jeweiligen Vortragsthemas auseinanderzusetzen., Auch der Dauerbrenner Marketingattribution war prominent mit einem interessanten Vortrag aus dem Otto-Konzern vertreten. Das methodische Niveau war hoch (Shapley-Regression etc.), doch blieb gerade im Wechselspiel der fachlichen Referentin mit dem Statistiker das seltsame Gefühl, dass das Modell eine sehr ausgefeilte Antwort ist, die aber nur bedingt zur fachlichen Frage passt, ein häufiges Problem im Umfeld der Marketingattribution. Ein weiteres hochkarätiges Thema wurde gleich im Anschluss behandelt, es ging um Pricing. Ein sehr interessanter Akzent lag auf den prozessmäßigen Voraussetzungen von Predictive Pricing im E-Commerce, also z.B. dafür zu sorgen, dass bei Retouren wirklich der Preis gutgeschrieben wird, zu dem der Kunde bestellt hat, selbst wenn sich dieser ständig ändert. Ein weiterer oft vernachlässigter Aspekt war die vorbereitende Einschätzung, wie hoch das Potenzial für Predictive Pricing in welchen Teilen des Sortiments überhaupt ist und wie ich dafür sorgen kann, dass automatisch angepasste Preise für den Endkunden auch sinnvoll und seriös wirken., Ein Highlight des zweiten Konferenztages war ohne Zweifel der Vortrag von Paul Mlaka. Seinen Erfolg hat er nicht durch ein besonders ausgefeiltes Modell oder große Datenmengen erreicht, sondern durch tiefe fachliche Kenntnis und eine erstaunliche Datenquelle. Aber der Reihe nach: Wie einige der besonders interessanten Vorträge kam auch dieser von einem Exoten - in dem Sinne, dass Paul Mlaka mit seinem Studium des Bauingenieurwesens an der amerikanischen Militärakademie West Point einen völlig anderen Hintergrund hat als die meisten anderen Konferenzteilnehmer. Mlaka ist mittlerweile als Berater für Unternehmen unterwegs, die vor allem von Aufträgen der öffentlichen Hand leben. In dieser Eigenschaft hat er ein junges Bauunternehmen bezüglich dessen Gebotsstrategie bei öffentlichen Ausschreibungen beraten. Mit Hilfe einfacher prädiktiver Modelle in Excel (!) konnte er dafür sorgen, dass sein Auftraggeber die Gebote der Wettbewerber sehr genau vorhersagen und sie jeweils knapp unterbieten konnte. Die Grundlage dafür lieferte ihm zum einen sein profundes fachliches Wissen über die Kalkulationsgrundlagen in der Branche, das es ihm erlaubte, die richtigen Prädiktoren zusammenzustellen. Mindestens ebenso erstaunlich wie die Prädiktoren ist in diesem Beispiel aber die Zielvariable: die historischen Gebote vergangener Ausschreibungen. Man sollte denken, dass solche Daten schlicht nicht verfügbar sind, weil sie nur der ausschreibenden Behörde zur Verfügung stehen und von dieser unter Verschluss gehalten werden. Seit einem Gesetz, das von Präsident Obama erlassen wurde, sind diese Daten jedoch öffentlich zugänglich! Eine wahrhaft erstaunliche Datenquelle also - und ein schöner Abschluss für diese kurze Sammlung von Eindrücken aus Berlin., , , © 2021 b.telligent;https://www.btelligent.com/blog/predictive-analytics-world-2016-in-berlin/;B-Telligent;Dr. Michael Allgöwer
06.10.2016;            SAP BW on HANA – macht ein Cache noch Sinn bei ABAP Routinen?        ;"Mit der Einführung von SAP BW on HANA im Jahre 2010 wurden viele bisherigen Maßnahmen zur Performancesteigerung in BW-Systemen obsolet; gleichzeitig drängen sich aber viele neuen Fragen bezüglich der neuartigen Plattform auf. Von großer Relevanz ist dabei auch die Frage, ob es immer noch sinnvoll ist, die sogenannten ""Advanced Business Application Programming-Routinen"" zu cachen. Denn mit HANA werden die Daten einerseits in der unter einem Applikationsserver liegenden Datenbank im Hauptspeicher abgelegt und andererseits für Abfragen optimiert. Hinzu kommt, dass die Abfragen in Routinen systembedingt auf dem Applikationsserver ausgeführt werden. Die Frage nach der Sinnhaftigkeit der Nutzung eines Caches für ABAP-Routine-Abfragen soll deshalb im nachfolgenden Blogbeitrag eingehend erläutert werden:, Bei häufig wiederkehrenden Daten lässt sich dies grundsätzlich bejahen. Denn wenn beispielsweise das Attribut ""Kontinent"" von dem Info-Objekt ""Land"" hinzugelesen werden soll, ist der zeitliche Overhead eines Zugriffs durch den SQL Parser, das Netzwerk, etc. auf HANA wiederkehrend für jede Zeile zu hoch. Zwischen dem ABAP-Programm und den eigentlichen Daten liegen etliche technische Layer, welche damit wiederholend ausgeführt werden. Ist es jedoch notwendig, mehrere Joins zwischen Tabellen durchzuführen oder ist die Anzahl der zu lesenden Zeilen sehr groß, kippt der Vorteil wieder in Richtung der HANA-Datenbank., Nach meinen Erfahrungen bei Kunden mit großen Datenmengen beschleunigt ein Cache im ABAP die DTP-Ausführung in einem SAP BW on HANA System teils um den Faktor 3. Dies ist natürlich immer abhängig von der Situation (z.B. Datenverteilung, Homogenität der Daten etc.), sowie von der aufgebauten Infrastruktur. Alles noch ohne Einsatz des Shared Memory. Dieser führt für alle Datenpakete zusammen, also pro Beladung, nur eine Abfrage auf die Datenbank aus. Im Handling ist dieser aber unnötig kompliziert., In einer internen Tabelle wird die gesamte bzw. der notwendige Teil einer Datenbanktabelle geladen und steht während der DTP Ausführung zur Verfügung. Ein Zugriff auf die Datenbank pro Lookup erfolgt nicht mehr. Dabei sollte jedoch beachtet werden, dass eine SORTED oder HASHED Tabelle verwendet wird, um die größtmögliche Performance sicherstellen zu können., Werden dynamische Berechnungen oder Lookups, welche bei einer gewissen Anzahl von Datensätzen sich wiederholen, durchgeführt, macht es Sinn, diese nicht mehrmals auszuführen. Das Ergebnis wird in einem KEY à VALUE Modell abgespeichert, in unserem Fall eine SORTED Tabelle. Bei der DTP-Ausführung kann durch die geänderte Datenbankstruktur der HANA-Datenbank sowie durch die semantische Partitionierung eines DTP dafür gesorgt werden, dass gleichartige Datensätze innerhalb eines Pakets verarbeitet werden. Dieses Vorgehen optimiert den Stack-Cache ohne die Nutzung vom Shared Memory enorm., Die einfachste Form des Caching ist gut mit der semantischen Partitionierung zu nutzen. Diese liefert die markierten InfoObjekte sortiert und somit kann über eine einfache Struktur und einer Fallunterscheidung geprüft werden, ob die Struktur neu beladen werden muss. Als Beispiel können z.B. die Auftragspositionen nach den Aufträgen semantisch partitioniert abgefragt werden. Damit werden die Aufträge auch in den Start- und Endroutinen sortiert geliefert. Im ABAP kann nun einfach geprüft werden, ob die neue Zeile noch den gleichen Auftrag beinhaltet und ob eine Aktualisierung der Struktur erforderlich ist. Damit kann je nach Größe der Gruppierung die Anzahl der SQL Abfragen einfach aber stark verringert werden., Um zu veranschaulichen, wie sich die Verarbeitungslaufzeiten bei der Nutzung bzw. nicht Nutzung eines Caches unterscheiden, soll im folgenden Teil eine Ausführung mit unterschiedlichen Cache-Modellierungen durchgeführt und verglichen werden. Die Ausführung erfolgt dabei jeweils mit über 17.516.672 Datensätzen. Die Anzahl der gegen die HANA DB abgesetzten SQL Statements hat eine Paketgröße von 200.000 Datensätzen., Um die Unterschiede in der Verarbeitungslaufzeit aufzuzeigen, wurden verschiedene mögliche Umsetzungen verglichen. Zur Ermittlung eines Startwertes wurde zunächst die Beladung ohne Ableitungen (einfache Transformation mit direkter Zuordnung) mit und ohne HANA Optimierung ausgeführt. Danach wurde noch ein Nachlesen mit einer SQL Abfrage pro Datenzeile und einmal mit einem einfachen ""Full Table""-Cache pro Paket betrachtet (also ohne Shared Memory)., Die SAP HANA optimierte Ausführung mit Pushdown benötigt 2 Minuten und 9 Sekunden., , , Die Ausführung ohne SAP HANA Optimierung benötigt bereits die dreifache Laufzeit, genau 6 Minuten und 11 Sekunden., , , Die Ausführung mit einer SQL Abfrage pro Datenzeile (SELECT SINGLE) führt beinahe zu einer Verzehnfachung der Laufzeit des DTPs., , Nun zum spannenden Ergebnis der Untersuchung: die Nutzung mit einem „Full Table“-Cache statt der vielfachen Ausführung eines SQL Statements ist wesentlich effektiver. D.h. pro Paket (mit einer Größe von 200.000 Zeilen) wurde nur eine SQL Abfrage benötigt, um eine interne Tabelle zu füllen. Die Laufzeit ist marginal höher, als die Ausführung ohne HANA Optimierung. Worin liegt dies nun begründet, dass es trotz der HANA In-Memory Verarbeitung zu einer so starken Diskrepanz kommt? , , Vergleichen wir die Zugriffe auf die M-Tabelle, aus der wir Daten nachlesen, sieht man deutlich, dass jede Abfrage eine gewisse Grundlaufzeit besitzt. In dem Testfall sind dies ca. 300ms. Obwohl sich das erstmal nach sehr wenig anhört, ist genau diese Tatsache bei der millionenfachen Ausführung einer der entscheidenden Faktoren. Begründen lässt sich dies mit den normalen Latenzen, wie Netzwerk, ABAP Open SQL Abstraktion etc., Die vereinfachte Laufzeitberechnung sieht dann folgendermaßen aus:, 17.516.672 * 300ms / 3-fach parallele Ausführung = 29.19 Minuten Overhead für das Nachlesen, was in etwa auch der gemessenen Laufzeitsteigerung entspricht., Schaut man sich das zweite Beispiel mit Cache an, ist sofort ersichtlich, dass mit 800ms die Abfragedauer etwas höher ist, die Ausführung jedoch nur paketweise erfolgt, also einmal auf 200.000 Zeilen. Die Verarbeitung in der internen Tabelle ist dann wieder enorm schnell und tritt nur marginal hervor., , Somit ist deutlich zu erkennen, dass ein Cache in ABAP zu benutzen weiterhin Sinn ergibt. Vorteil ist jedoch, dass man komplexere mehrstufigere JOINs oder andere Logiken, bei denen die Verarbeitung in der Datenbank früher länger gedauert hat, nun einfach im SQL abbilden kann. Jedoch kommt es bei wiederkehrenden gleichartigen Abfragen zu einem organisatorischen Overhead, welcher sich in Summe doch bemerkbar macht., Ab der Version 7.5 SP4 steigt die Anzahl der technischen Möglichkeiten weiter an. Es ist z.B. die kombinierte Nutzung von HANA optimierter Ausführung zusammen mit ABAP Ausführung möglich, wenn die beiden Transformationen durch eine InfoSource verbunden sind (partielle SAP HANA Ausführung). Details zu den Neuerungen erfahren Sie im Detail hier., Zusammenfassend ergibt sich die Erkenntnis, dass weiterhin über die optimierte Ausführung von Ableitungen und Regeln nachgedacht und der richtige Ort für die Ausführung des Codes gesucht werden muss, selbst wenn ein Einsatz von HANA erfolgt. SAP wird mit der immer tieferen Integration von HANA ins SAP BW mit jedem Support Package weitere spannende Lösungen schaffen, worüber wir Sie in diesem Blog mit informieren werden., , Anbei noch ein Codebeispiel, für einen „Full-Table“-Cache in vereinfachter Form. Generell empfiehlt sich Lookups in Klassen zu implementieren, da man diese für Testfälle auch einfach direkt ausführen kann und auch Erweiterungen einfacher zu implementieren sowie eine Share Memory Nutzung einfach möglich ist. Falls Sie Fragen haben, kommen Sie gerne auf mich zu., „Full Table“-Cache Beispiel einer Formel-Routine: Globale Deklaration, , , „Full Table“-Cache Beispiel einer Formel-Routine: auszuführender Code, , , © 2021 b.telligent";https://www.btelligent.com/blog/sap-bw-on-hana-caching-abap/;B-Telligent;Markus Sontheimer
05.10.2016;            Die Highlights des Spark Summits 2016 in Brüssel        ;"Diesen Blog-Eintrag verfasse ich mal nicht in einer ruhigen Minute in unseren b.telligent-Büros, sondern live aus Brüssel vom Spark Summit.Für Data Scientists bietet es einen enormen Umfang an Machine-Learning-Verfahren, sowohl klassisch für statische Datensätze als auch für Streamingdaten in Echtzeit. Jeder mit Praxiserfahrung in der Python-Bibliothek sklearn wird sich sofort zuhause fühlen, da diese als Vorbild verwendet wurde., Spark sollte man eigentlich nicht mehr vorstellen müssen. Dennoch für alle, die erst spät zugeschaltet haben: Spark ist ein Open-Source-Framework für verteiltes Rechnen. Man nennt es häufig in einem Satz mit vielen Big-Data-Technologien und gerne wird es mal fälschlicherweise als Hadoop-Konkurrent bezeichnet. Tatsächlich ist es eher eine sinnvolle Ergänzung dazu, denn während Hadoop eher ein Framework für auf Cluster verteiltes und robustes Speichern von Massendaten ist, kann Spark diese Datenmengen höchst performant verteilt für Berechnungen verarbeiten. Spark selbst mag man zwar zu den Big-Data-Technologien rechnen, aber es hat per se keine eigene Datenhaltung., , Spark ist noch sehr neu in Maßstäben herkömmlicher kommerzieller Software. Der erste Wurf wurde erst vor vier Jahren veröffentlicht, aber in den USA wird es bereits seit einem Jahr als die ""Taylor Swift of Big Data software"" gefeiert und es gab bereits mehrere internationale Spark-Messen. Ich befinde mich nun gerade auf einer solchen, dem europäischen Spark Summit 2016. Das ist eine von mittlerweile drei Veranstaltungen im Jahr und die einzige außerhalb der USA. Hier wird mit etwa 1.000 Besuchern und 100 Vortragenden aus Wirtschaft und Forschung über Weiterentwicklung/Trends, technische Feinheiten und Anwendungsfälle von Spark berichtet., , Für mich ist es beeindruckend, die Keynote Speech von Matei Zaharia mitzuerleben. Matei hat Spark in seiner ersten Fassung ja erst vor ein paar Jahren als universitäre Arbeit während seiner Promotionszeit in Berkeley veröffentlicht. Nun hat er eine Professur in Stanford inne und ist Mitgründer und CTO von Databricks. Das Unternehmen trägt den Großteil der Entwicklungsarbeit an Spark und ist weltweit führend in der Bereitstellung von kommerziellen Arbeitsumgebungen für Spark. Der Summit in Brüssel ist sozusagen eine Databricks-Hausmesse., Matei lässt es sich auf den Spark Summits nicht nehmen, selbst die Eröffnungsrede zu halten und seine neuesten Visionen für Spark mit dem sichtlich begeisterten und ehrfürchtig lauschenden Publikum zu teilen. Er strahlt dabei die Aura eines IT-Nerds aus, ist hoch konzentriert auf sein Thema, spricht ohne Punkt und Komma, ... man hört ihn regelrecht Atem holen zwischen langen Sätzen. Dafür ist er routiniert und sehr sicher in seinem Vortrag. Man kann schon sagen, dass der Spark Summit letztlich seine Veranstaltung ist., , Der aktuelle Summit ist der erste seit dem Launch von Spark in der Version 2.0. Diese Version bringt zunächst deutliche Performancegewinne durch weitere grundlegende Optimierungen (Project Tungsten: ""Bringing Apache Spark Closer to Bare Metal"") sowie Vereinheitlichungen in den APIs mit sich, vor allem bei der Zusammenführung von DataFrames und DataSets., Darüber hinaus wurde die neue Schnittstelle für Streamdaten vorgestellt. Hierzu wurde in der zweiten Keynote eine Livedemonstration mit Twitterfeeds gezeigt, für uns Europäer zum Brexit statt zum Clinton-Trump-Wahlkampf. Das Machine-Learning-Modell, das zunächst für statisch eingelesene Daten bzw. Batchberechnungen erstellt wurde, konnte einfach mit Anpassung einer Zeile auch für eine Streamingquelle genutzt werden. Der Vortragende wechselte dabei auch laufend im selben Notebook zwischen Code in Scala und Code in Python. Durch diese stärker generische API muss der Spark-User sich also nicht mehr im Vorfeld entscheiden, ob er für Streaming oder Batchverarbeitung entwickelt., Die auf die Keynotes folgenden Vorträge sind in vier Tracks aufgeteilt, nach Data Science, Developer, Spark Ecosystem und Enterprise. Mit insgesamt etwa 1.000 Summit-Besuchern sind die Vorträge immer gut besucht. Die inhaltliche Bandbreite reicht von Predictive-Maintenance-Themen, z.B. Vorhersage von Weichenausfällen in Eisenbahnnetzen, über die Verknüpfung von Spark mit geeigneten Technologien (R-Wrapper, Sparkling Water/H20, Apache Ignite) bis hin zu ""Internet of Things""-Themen. Ein Vortrag von mmmmoooogle berichtete sogar vom Einsatz von Spark für die Analyse von unzähligen biometrischen Daten an vernetzten Kühen zur Steigerung der Milchproduktion, also ein ""Internet of Cows""., , Spark verbreitete sich sehr schnell in den letzten drei Jahren, sowohl in Forschung wie auch in Industrie und Wirtschaft. Ein leitender Business Developer von Databricks erzählte mir bei einem Termin, dass eigentlich keiner der Databricks-Kunden Spark ausprobiert hat und dann zum Schluss gekommen ist, es würde nicht wie gewünscht funktionieren oder für ihn ungeeignet sein. Er erklärte mir weiterhin, dass die Anfragen und Ideen für neue Features in Spark so gewaltig sind durch die rasant wachsende Community, dass Databricks zunächst immer erst priorisieren muss, welche Entwicklung der größte Benefit für möglichst viele User darstellt, bevor sie sich eines Themas annehmen., Bei Spark von einem Hype zu sprechen, tut der Technologie in meinen Augen etwas unrecht. Hype impliziert für mich immer etwas eine Blase um das Thema. Aber bei Spark wird nichts versprochen, was es nicht halten kann, und der Erfolg sowie die schnelle Verbreitung sprechen für sich. Es bleibt spannend., , Die aktuellen Keynotes am zweiten Tag des Spark Summits bringen einige interessante Ankündigungen mit sich., , Databricks hat soeben ein neues Produkt angekündigt, und zwar TensorFrame. Wer ein Databricks-Notebook benutzt, kann sich auf den komfortablen Service verlassen, dass innerhalb von Minuten bei Bedarf ein skalierbarer Cluster von Spark-Nodes in der AWS Amazon Cloud (oder bald auch in Microsoft Azure!) gestartet wird. Das nimmt dem User die komplette aufwendige Infrastruktur ab und stellt komplexe vorkonfigurierte Rechensysteme quasi sofort zur Verfügung. Dieser Status quo wird erweitert um neu verfügbare Spark-Nodes, die auf GPUs, also Grafikkarten, basieren und maßgeschneidert sind für Deep-Learning-Aufgaben.Der Vortragende konnte eindrucksvoll eine Livedemonstration zeigen, in der in kurzer Zeit Googles DeepDream-Algorithmus über Tensorflow innerhalb eines Databricks-Spark-Notebooks auf ein frisch aus dem Netz gezogenes Bild von Boris Johnson angewendet wurde. Impressive!Spark arbeitet hier übrigens mit dem Grafikkartenhersteller NVIDIA zusammen, um die Technologien weiter aufeinander abzustimmen., IBM stellte in der zweiten Keynote seine neue Machine-Learning-Werkbank vor. Das Produkt benötigte nur ein halbes Jahr Entwicklung bis zur jetzigen Veröffentlichung als ""Closed Beta""-Version. Es gehört zur IBM-Watson-Familie und bietet eine ""end to end collaboration platform"" für Machine Learning. Man spricht sogar von einer Demokratisierung von Machine Learning, da die Bedienung so einfach sei, dass jeder damit arbeiten könne. Gleichzeitig bietet es genügend Optionen und Eingriffsmöglichkeiten für professionelle Data Scientists. Dieses Produkt wurde komplett in Spark realisiert. Es zeigt das Potential von Spark, dass es erst etwa drei Jahre nach der Veröffentlichung als Kern erfolgreich für ein IBM-Watson-Produkt verwendet wurde., , , © 2021 b.telligent";https://www.btelligent.com/blog/spark-summit-2016-in-bruessel/;B-Telligent;Stefan Seltmann
16.09.2016;            Analyse oder App - was stellt ein Data-Science-Team eigentlich her?        ;"Eine besonders fruchtbare aktuelle Diskussion (siehe auch den Blogeintrag meines Kollegen Simon Nehls) dreht sich um die Frage, was ein Data-Science-Team eigentlich sinnvollerweise herstellt. Die beiden Möglichkeiten sind dabei schnell benannt: Auf der einen Seite steht die ""Analyse"", also ein einmalig erstelltes, eher statisches Endergebnis; die meisten denken hier sofort an eine PowerPoint-Präsentation. Auf der anderen Seite steht die ""App"", also ein interaktives, ständig mit frischen Daten versorgtes Endprodukt, häufig in Form einer Website oder einer Mobile App., Bei dieser Diskussion sind die Befürworter der App momentan in der Offensive. Ihnen spielt die Tatsache in die Hand, dass das vor zehn Jahren noch von statischen Webseiten geprägte Internet immer interaktiver und dynamischer wird und damit unser aller Erwartungshaltung in Richtung der App verschiebt. Wenn mein Informationsmedium Nummer eins interaktiv und ständig aktuell ist, sehe ich nicht ein, warum das beim Output meines Data-Science-Teams anders sein sollte. Alles andere ist ja gar nicht mehr zeitgemäß. Dieses Argument ist beliebt, aber oberflächlich und pauschal. Wenn man sich ein wenig mehr Differenzierung gönnt, wird die Diskussion merklich spannender. Die App als Endprodukt verändert nämlich die Arbeitsweise eines Data-Science-Teams tiefgreifend. Dass es sich für jedes Data-Science-Team lohnt, sich auf diese Veränderung einzulassen, will dieser Blogeintrag zeigen. Es geht dabei nicht darum, sich auf die App als Arbeitsergebnis festzulegen, sondern geschickt auf der Klaviatur der möglichen Endprodukte zu spielen., Wenn die Medien über den Data Scientist berichten, wird dabei häufig der Singular verwendet - möglicherweise, weil Data Scientists immer noch so selten sind, dass automatisch das Bild vom einsamen Datenhelden im Kopf entsteht. Fest steht aber: Der Data Scientist in der Einzahl produziert im Regelfall keine Apps. Die App ist meist eine Teamleistung, weil ein interaktives und dynamisches Endprodukt gegenüber der Analyse zusätzliche Kompetenzen verlangt in Webtechnologien, Softwareentwicklung und dem Design von User Interfaces. Diese Zusatzanforderungen sorgen dafür, dass die ohnehin umfangreiche Palette von Fähigkeiten, die ein Data Scientist braucht, sich so weit verbreitert, dass sie endgültig unrealistisch wird. Es braucht also mindestens zwei Kollegen mit unterschiedlichen Schwerpunkten, die arbeitsteilig zusammenarbeiten. Das einfachste Modell ist dabei ein linearer Ablauf (mit Schleifen), bei dem entlang einer Kette von Anforderungsdefinition, Datenbeschaffung, Datenaufbereitung, Modellierung, Visualisierung, Deployment (unten mehr zu diesem Stichwort) nicht mehr einer oder eine alles macht, sondern unterschiedliche Personen für unterschiedliche Teile der Kette verantwortlich sind., Für die Entwicklung eines Data-Science-Teams haben Apps darum zwei sehr interessante Effekte. Der eine (naheliegendere) ist eine Veränderung der Teamkultur und Zusammenarbeit. Man ist sehr viel mehr aufeinander angewiesen, und das hilft, das Durchlaufen der klassischen Teamprozesse von Forming, Storming, Norming, Performing zu beschleunigen. Da Data-Science-Teams fast immer relativ frisch und noch im Aufbau begriffen sind, ist das ein wichtiger Aspekt., Der andere Effekt ist wichtig für den schnellen Aufbau eines schlagkräftigen Teams: Eines der Hauptprobleme bei der Suche nach neuen Mitarbeitern im Data-Science-Bereich ist die Tatsache, dass ein Data Scientist eine sehr breite Palette von Fähigkeiten mitbringen muss. Wenn ich, zumindest wenn das Endprodukt eine App ist, ohnehin einen arbeitsteiligen Ablauf habe, dann kann ich die Arbeitsteiligkeit auch noch ein kleines bisschen weiter treiben, als das unbedingt notwendig wäre (also drei oder vier Spezialisten für eine App statt zwei oder drei), und komme zu schlankeren Anforderungsprofilen für jeden der Beteiligten. Die Chancen, jemand Passenden zu finden, werden dadurch deutlich größer werden. Ein Webentwickler (also ein deutlich weniger rares Profil) zum Beispiel kann sich durchaus in ein Data-Science-Team hineinentwickeln, wenn dieses Team häufig Apps herstellt. Er wird dadurch im Regelfall nicht zum Data Scientist, aber er trägt zur Teamproduktivität erheblich bei., Eine gut gemachte App, die regelmäßig zu treffende, wichtige Entscheidungen unterstützt, hat eine Art Leuchtturmfunktion. Sie kann im Unternehmen nachhaltig für das Data-Science-Team werben und als greifbares Beispiel für den Nutzen der Arbeit bei Budgetdiskussionen sehr hilfreich sein. Nicht ohne Grund gibt es dort, wo Aufmerksamkeit gemessen wird und Geld bringt, einen Trend zur App: im Datenjournalismus der Onlinemedien., Interessante Beispiele aus dem Alltag gibt es hier:, Auch öffentliche Institutionen, die ihre Arbeit vor dem Steuerzahler rechtfertigen und sich um Transparenz bemühen müssen, haben den Wert von interaktiven Apps erkannt. Da wäre zum Beispiel die schöne Appzu nennen, mit der die Federal Reserve Bank of New York die geographische Verteilung der öffentlichen Ausgaben für Schulen in New York nachvollziehbar macht. Gut gemachte Apps sind unmittelbar ansprechend, es macht Spaß, mit ihnen zu arbeiten, und gerade für immer wiederkehrende Entscheidungssituationen in Unternehmen oder in der Interaktion mit Endkunden sind sie eine gute Wahl., Bei allen Vorteilen hat die App als Endprodukt für das Data-Science-Team auch Nachteile. Es besteht die Gefahr, dass das Team hinter dem Produkt verschwindet. Dass die Oberfläche wahrgenommen wird, aber nicht konzeptionelle Arbeit dahinter, dass die tiefeSachkenntnis verborgen bleibt, die essentiell ist, um eine gute App zu bauen. Die Technik ist sichtbar, nicht die Kompetenz dahinter. Und es bleibt ein klassischer Bestandteil der Data Science auf der Strecke (zumindest, falls denn in unserer jungen Disziplin irgendetwas schon klassisch ist): das Storytelling. Eine interaktive App mag den Anwender ""führen"", aber letztlich ist sie immer eine individuelle Entdeckungsreise. Eine Geschichte aber ist ein linearer Ablauf, der ein lineares Medium braucht - die statische Analyse also. Ich weiß, dass jetzt viele aufheulen, die mit Recht darauf hinweisen, dass es sehr kluge Versuche gibt, auch das ""Geschichtenerzählen in interaktiven Medien"" zu erforschen und zu lehren, insbesondere im Umfeld der Videospiele. Die Tatsache bleibt jedoch, dass das ein sehr hoher Anspruch ist, der noch sehr viel schwieriger zu realisieren ist als das lineare Data Science Storytelling - ein Anspruch, der im Alltag unter Zeitdruck nur in Ausnahmefällen realisierbar ist., Die klassische Analyse ist dort eine große Chance für die Sichtbarkeit des Teams, wo am Schluss nicht einfach eine PowerPoint-Datei verschickt wird, sondern der Data Scientist sie vorstandstauglich präsentiert. Das ist keine Aufgabe für den ""reinen Techniker"", sondern für den rhetorisch versierten, mit tiefem Businesswissen ausgestatteten Data Scientist mit Storytelling-Talent und analytischer Tiefe. Wenn er diese Aufgabe gut löst, kann er für die Sichtbarkeit des Teams einen Beitrag leisten, den keine App ersetzt: auf Fragen mit tiefem Wissen antworten. Gesicht zeigen. Die Kompetenz des Teams persönlich sichtbar machen. Ein Vorstand wird sich bei wichtigen Richtungsentscheidungen in komplexen Situationen nie nur allein auf ""die Daten"" verlassen. Die Kompetenz derjenigen, die diese Daten richtig zu befragen und zu interpretieren wissen, wird er nicht mehr missen wollen, wenn er sie einmal persönlich kennengelernt hat. Auch in dieser Weise sollte sich ein Data-Science-Team strategisch im Unternehmen positionieren., Die Wahl des Endproduktes beeinflusst auch die Geschwindigkeit, mit der es geliefert werden kann. Die Analyse ist im Regelfall schneller erstellt als die App (das kommt allerdings auch auf Prozesse und Toolunterstützung an - aber das ist ein Thema für einen separaten Blogartikel). Sie kann daher Wunder wirken in Fällen, in denen sehr schnell eine Antwort hermuss. Gleichzeitig kann die App dort Last vom Team nehmen, wo absehbar ist, dass sich dringende Anfragen in ähnlicher Weise wiederholen. Es lohnt sich zudem, im Entwicklungsprozess die beiden Endprodukte miteinander zu verbinden. Die Analyse kann als Prototyp für die App dienen, und wenn man die Analysen der letzten Monate durchgeht, wird man vielleicht eine Häufung ähnlicher Aufgaben feststellen, die sich als App bereitstellen lassen., Es lohnt sich für jedes Data-Science-Team, die eigene ""App-Fähigkeit"" herzustellen und zu lernen, sowohl die Analyse als auch die App geschickt und flexibel einzusetzen. Wer sich auf eines der beiden Endprodukte festlegt, verschenkt Chancen., , © 2021 b.telligent";https://www.btelligent.com/blog/was-stellt-ein-data-science-team-eigentlich-her/;B-Telligent;Dr. Michael Allgöwer
16.08.2016;            Kleine SAP Schule – Die RSADMIN – Teil 1        ;In dieser Serie werden wir uns die Einstellungen der RSADMIN genauer anschauen. Die Tabelle RSADMIN wird als globale Konfigurationstabelle in verschiedenen BW-Prozessen und -Funktionalitäten benutzt. Sie besitzt bereits standardmäßig Einträge, es ist aber auch durchaus möglich und üblich, eigene Einträge in der RSADMIN zu erzeugen. Der erste Blogeintrag ist dem Setting NO_TP_SCHEDULE_IMMEDIATE gewidmet., Standardmäßig werden Prozessketten nach dem Transport sofort eingeplant. Bei Prozessketten, die auf Sofortstart stehen, bedeutet dies aber auch, dass diese sofort laufen., Dieses Verhalten kann unerwünscht sein, wenn dadurch zum Beispiel Jobs zum Laden von Daten getriggert werden., Damit die Prozessketten nicht automatisch gestartet werden, gibt es zwei Möglichkeiten. Die erste Möglichkeit wäre, die Einplanung so einzustellen, dass die Prozesskette nur über eine Metakette oder eine API gestartet werden kann., Dies hat jedoch den Nachteil, dass die Prozesskette nach dem Transport auf dem jeweiligen System wieder umgestellt werden muss, um sie auch manuell ausführen zu können., Die elegantere Lösung an der Stelle ist die Einstellung NO_TP_SCHEDULE_IMMEDIATE in der RSADMIN. Diese Einstellung sorgt dafür, dass die Prozessketten, die auf Sofortstart stehen, nicht ausgeführt werden., Die RSADMIN kann über die Transaktion SE38 mit dem Programm SAP_RSADMIN_MAINTAIN bearbeitet werden., , , In der sich öffnenden Pflegemaske wird bei ObjectNO_TP_SCHEDULE_IMMEDIATE eingegeben und bei Value entsprechend X. Bei den darunter stehenden Optionen muss noch Update ausgewählt werden, um die bestehende Einstellung zu überschreiben., , , Mit einem Klick auf Bestätigen wird die Einstellung ausgeführt und in der Datenbank persistiert., Um die Einstellungen auf den anderen Systemen verfügbar zu machen, müssen sie noch transportiert werden. Dazu wird in der Transaktion SE01 ein Transport angelegt. In den Einstellungen kann dann das folgende Objekt hinzugefügt werden., , , Bei Tabelleninhalten muss immer ein Schlüssel der zu transportierenden Inhalte mit übergeben werden. Die Einstellung dafür kann mit einem Klick auf den Button mit dem Schlüssel geöffnet werden. Als Schlüssel wird NO_TP_SCHEDULE_IMMEDIATE gewählt, um die Einstellung für die Prozessketten zu transportieren., , , Nachdem alle Einstellungen für den Transport gespeichert wurden, ist dieser bereit und kann in die verschiedenen Systeme durchgeführt werden., Im nächsten Teil dieser Serie werden wir uns den Parameter MD_ENABLE_KEY_UPDATE genauer anschauen., , © 2021 b.telligent;https://www.btelligent.com/blog/kleine-sap-schule-die-rsadmin-teil-1/;B-Telligent;Markus Sontheimer
11.08.2016;            Wie der stationäre Handel mit PoS-Trackingdaten gegenüber Onlineshops aufholt        ;, , , © 2021 b.telligent;https://www.btelligent.com/blog/wie-der-stationaere-pos-gegenueber-onlineshops-aufholt/;B-Telligent;Harald Mösel
21.07.2016;            Der lokale Verbindungspfeil von Longview BI (ehemals arcplan)        ;Lässt man die Tabelle, die hier nur zur Verdeutlichung dienen sollte, weg, sieht das Ergebnis „Jahresumsatz der Länder, die in Q2 Umsätze gemacht haben“ nun so aus., , © 2021 b.telligent;https://www.btelligent.com/blog/der-lokale-verbindungspfeil-von-longview-bi/;B-Telligent;Stefan Kersten
19.07.2016;            Anpassen des Knowledge Modules IKM Oracle Slowly Changing Dimension – Teil 1        ;"Im ersten Teil des ODI Blogs wurde gezeigt, wie man mithilfe des Knowledge Modules IKM Oracle Slowly Changing Dimension eine Slowly Changing Dimension in ODI umsetzen kann., Das Knowledge Module IKM Oracle Slowly Changing Dimension deckt schon viele Anwendungsfälle ab. So werden zum Beispiel neue Datensätze gegen die bereits bestehenden geprüft. Bei Änderungen kann automatisch ein neuer Datensatz erstellt (SCD2) oder der bestehende Datensatz überschrieben werden (SCD1)., Sollen jedoch technische Felder wie die ID des Ladejobs oder der Insert- bzw. Updatezeitpunkt gepflegt werden oder möchte man einen anderen SCD_END-Standardwert verwenden, kommt das KM an seine Grenzen., Hier kommt der Vorteil von ODI zum Tragen, die Knowledge Modules beliebig anpassen zu können., Wird ein unbegrenzt gültiger Datensatz geschrieben, so wird das SCD_END-Datum auf den 01.01.2400 gesetzt. In vielen Umgebungen ist aber ein anderer Standardwert definiert (z.B. der 31.12.9999)., Um möglichst flexibel zu sein, setzen wir nicht einfach ein neues SCD_END-Datum, sondern führen eine neue Option in das Knowledge Module ein., Dazu wird das Knowledge Module geöffnet und der Reiter Options ausgewählt., , , Und über Plus legen wir eine neue Option an., , , Diese wird als Type Text mit dem Namen SCD_END_DATE angelegt und mit dem gewünschten Datumswert belegt., , , Danach wechseln wir in den Tasks-Reiter und klappen den Baum bei Execution Unit Main auf., , , Geändert werden müssen folgende Tasks:, Ersetzt wird jeweils der Wert:, 01-01-2004, Durch:, &lt;%=odiRef.getOption(""SCD_END_DATE"")%&gt;, Zum Beispiel Änderung von:, , , nach:, , , Mit dieser Änderung kann das SCD_END-Datum beliebig gewählt und an die Bedürfnisse einer vorhandenen Architektur angepasst werden., Es muss nur darauf geachtet werden, dass das Datum im korrekten Format angegeben wird, in diesem Fall also als 'mm-dd-yyyy'., Lesen Sie im nächsten Teil dieses Blogs ""Anpassen des Knowledge Modules IKM Oracle Slowly Changing Dimension - Teil 2"", wie man auch im IKM Slowly Changing Dimension technische Felder wie Ladelauf-ID oder Insert- bzw. Updatezeitstempel korrekt behandeln lassen kann., , © 2021 b.telligent";https://www.btelligent.com/blog/anpassung-knowledge-module-ikm-im-odi-teil-1/;B-Telligent;Oliver Gräfe
11.07.2016;            Der Bedarf an kundenzentrischem Datawarehousing und die Chancen, die dieses eröffnet        ;"Die zentrale Rolle des Kunden für die Unternehmensausrichtung wird in der Wissenschaft seit Jahrzehnten diskutiert:, Heute ist die Rolle des Kunden von noch größerer Bedeutung, insbesondere für Unternehmen, die in wettbewerbsintensiven Industrien mit einer hohen Zahl an Endkunden tätig sind. Sehen Sie sich hierzu gerne unser Webinar auf unserem YouTube-Kanal an, welches das Thema auch anhand von Praxisbeispielen mit SAP verdeutlicht!, , , Historisch betrachtet, wurden viele Geschäftsmodelle nach dem Markt, nach dem Produkt oder nach den Wettbewerbern ausgerichtet., Zentrale Fragen dabei waren:, Der Kunde wurde, wenn überhaupt, erst später in den Prozess mit einbezogen. Entsprechend wurden die Kundenbedürfnisse auch zum größten Teil bei der Führung und Steuerung vieler Unternehmen vernachlässigt., Gegenwärtig sehen sich Firmen mit neuen Herausforderungen konfrontiert. Sie agieren in einem sehr dynamischen Umfeld, das sowohl durch den konstanten technischen Fortschritt als auch den intensiven Datenaustausch geprägt ist. Die Grenzen zwischen Unternehmen, Lieferanten und Kunden verschwimmen immer mehr. Marktanteile sind durch stetig intensivierte Konkurrenz hart umkämpft, worunter die Profitabilität leidet. Unternehmen, die auch noch mit vielen Einzelkunden interagieren, sind zusätzlich mit hohen Kosten konfrontiert, die durch inkonsistente und falsche Kundendaten verursacht werden. Diese schlagen sich als Multiplikator durch das ganze Unternehmen - von der Durchführung von Marketingkampagnen über Serviceleistungen bis hin zur falschen Planung und Steuerung., Laut der 2015 Experian Digital Marketing Survey 4 haben 89% der befragten Unternehmen Probleme, ein einheitliches Kundenprofil zu erstellen. Als Hauptgrund (43%) dafür wurde vor allem die schlechte Datenqualität benannt. Für knapp ein Drittel der Befragten ist dagegen die effektive und unternehmensweite Anwendungsintegration (32%) die größte Herausforderung bei der Implementierung einer kanalübergreifenden Marketingstrategie. Diese scheitert darüber hinaus auch an der Organisationsstruktur (31%) sowie am Mangel an unternehmensweit etablierten, einheitlichen und kundenzentrischen Prozessen und KPIs., In manchen Fällen können auch Akzeptanzprobleme bei den Entscheidungsträgern oder sogar die gewollte Intransparenz gegenüber dem Team und der Führungsebene für Probleme bei der Kampagneneinführung sorgen. Dies ist vor allem dann der Fall, wenn Industrien privatisiert und somit in kurzer Zeit einem ungewohnt hohen Konkurrenzdruck ausgesetzt werden. Mit der Einhaltung von etablierten Prozessen und Steuerungsmodellen werden immer öfter die von der Unternehmensführung gesetzten Ziele verfehlt., Um erfolgreich dagegen vorzugehen, müssen Unternehmen die Kunden ins Zentrum der Geschäftsausrichtung setzen. Zunächst sollten die eigenen Prozesse analysiert und durch die Integration eines starken Kundenfokus optimiert werden. Dies setzt allerdings voraus, dass alle kundenzentrischen Daten und Informationen ""über Bereichs- und vor allem Applikationsgrenzen hinweg"" zu einer gesamtheitlichen Sichtweise auf den Kunden zusammengeführt werden. 5, Des Weiteren müssen auch Pflege- und Bereinigungsprozesse für sämtliche Daten entwickelt werden. Auch wenn die Einführung einmalig geschieht, sind diese regelmäßig durchzuführen, um auch in Zukunft die Datenstringenz und -konsistenz sicherstellen zu können., Darüber hinaus ""sind die informationstechnisch realisierten Funktionalitäten der Unternehmung auf die kundenzentrischen Prozesse hin anzupassen und applikationsübergreifend zu vernetzen."" 6, Mit der Erstellung eines einheitlichen Kundenprofils für jeden einzelnen Kunden wird die schnelle, individuelle und adäquate Reaktion auf die ständig wechselnden Kundenbedürfnisse für alle Unternehmensbereiche ermöglicht., Auf diese Weise werden sowohl die Kundenzufriedenheit als auch der Umsatz erhöht und gleichzeitig die Servicekosten gesenkt. Erst ab diesem Zeitpunkt können Unternehmen mit Hilfe von erfahrenen Data Scientists auch richtig von Predictive- und Realtime-Analytics-Lösungen sowie Machine-Learning-Algorithmen profitieren., Als eine fokussierte Unternehmensberatung, die auf die Einführung und Weiterentwicklung von Business Intelligence, Customer Relationship Management, DWH und Big Data spezialisiert ist, steht b.telligent mit Rat und Tat an der Seite vieler Branchenführer aus den Bereichen Telekommunikation, Finanzdienstleistung, Handel und Industrie, die einen starken Kundenfokus in ihrer Strategie, ihren Prozessen und Datenbanksystemen aufbauen wollen., Als technologieunabhängiger Anbieter ist b.telligent sowohl beim Aufbau neuer als auch beim Ausbau existierender Softwarelösungen genau der richtige Ansprechpartner. Schließlich stellt b.telligent auch sicher, dass Fach- und Führungskräfte das volle Potenzial der optimierten IT-Lösungen ausschöpfen können. Dafür werden Experten in interdisziplinären Teams aus den Bereichen Business Intelligence, Business Process Management, CRM, E-Commerce, Finance und Controlling sowie Data Science zusammengeführt. Diese greifen auf eine Vielzahl von Best Practices und Referenzarchitekturen zurück, wodurch sie in der Lage sind, methodisch individualisierte Kundenlösungen zu gestalten., , Quellen (abgerufen im April 2016): , 1 Cook (2002), S. 109 in Lüchinger, F. (2002), http://bit.ly/1RUNier, 2 Pink, D.H. (1999) in 3, 3 Ponelis, S.R. und Britz, J.J. (2002), S. 1, http://bit.ly/1qzHRen, 4 Experian (2015), http://bit.ly/1RUSgYE, 5 Fridgen, M. und Heinrich, B. (2002), S. 5, http://bit.ly/1MjQfIA, 6 Klier, M. (2007), S. 14, http://bit.ly/23hN5MQ, , © 2021 b.telligent";https://www.btelligent.com/blog/chancen-des-kundenzentrischen-datawarehousing/;B-Telligent;Markus Sontheimer
11.07.2016;            Digitaler Wandel im Handel - die Chancen der Digitalisierung am Point of Sale nutzen        ;Handelsunternehmen mit stationären PoS beschäftigen sich derzeit, ebenso wie Hersteller mit eigenem Retail und Betreiber von Shopping-Centern, intensiv mit der Digitalisierung des PoS., , © 2021 b.telligent;https://www.btelligent.com/blog/digitaler-wandel-im-handel/;B-Telligent;Harald Mösel
30.06.2016;            SCD-Handling mit dem Oracle Data Integrator 12        ;"Seitdem das Supportende für den OWB offiziell bekannt gemacht wurde, ist der Oracle Data Integrator (ODI) das ETL-Tool der Wahl in der Oracle-Welt. Die Entwicklung ist zu der Version 12 fortgeschritten, die einige Änderungen und Verbesserungen gebracht hat. Die GUI hat sich dem OWB noch weiter angenähert, jedoch stehen einige Möglichkeiten zur Verfügung, die der OWB so nicht bot., In diesem Eintrag wollen wir uns mit der Implementierung von Slowly Changing Dimensions im ODI beschäftigen., Der Oracle Data Integrator (ODI) bietet eine elegante Lösung, um Slowly Changing Dimensions zu befüllen. Das grundsätzliche Vorgehen entspricht dabei dem Vorgehen in ODI 11, jedoch wurde die Konfiguration der Zieltabelle überarbeitet., Die Dimensionstabelle wird zunächst wie jedes andere Modell auch per ""reverse engineer"" im ODI angelegt., Zunächst muss die Dimensionstabelle als SCD eingestellt werden. Dazu öffnet man den Data Store und stellt ""OLAP Type"" auf ""Slowly Changing Dimension""., , Die Spalten der Dimensionstabelle findet man im Reiter ""Attributes"":, , , Hier wird für jede Spalte der Wert ""SCD Behavior"" eingestellt., Für eine SCD Typ 2 müssten die Spalten wie folgt eingestellt werden:, , , Im Beispiel sieht das wie folgt aus:, , Das IKM für Slowly Changing Dimensions ist standardmäßig in ODI 12 nicht mehr vorhanden. Es kann jedoch aus dem zu ODI mitgelieferten XML importiert werden. Zu finden ist das Knowledge Module im Ordner $ORACLE_HOME/odi/sdk/xml-reference., Hier ist das IKM entsprechend dem Typ der Zieldatenbank zu wählen. Für eine Oracle-Datenbank wäre es z.B. die ""IKM Oracle Slowly Changing Dimension""., , Ist die Zieltabelle konfiguriert, kann sie ganz normal im Mapping eingebunden werden., In der Zieltabelle muss jetzt der Integration Type ""Slowly Changing Dimension"" ausgewählt werden., , , Nun sollte im Physical Layer das IKM geprüft werden:, , , Dieses IKM setzt zwingend ein CKM voraus., , , Sollte sich hier kein CKM auswählen lassen, so muss es, genau wie das IKM, zunächst importiert werden (siehe Schritt 3)., Damit ist die Implementierung der SCD2-Beladung abgeschlossen und kann getestet werden., Erfahren Sie in einem der nächsten Blogbeiträge mehr dazu, wie sich das IKM Slowly Changing Dimension noch weiter auf Ihre Umgebung anpassen lässt., , © 2021 b.telligent";https://www.btelligent.com/blog/scd-handling-mit-dem-oracle-data-integrator-12/;B-Telligent;Oliver Gräfe
19.05.2016;            Willkommen auf dem Data Warehouse und BI Blog        ;"Unser neuer Blog wird Sie in Zukunft zu aktuellen Problemen und Lösungsvorschlägen rund um die Themen ""DWH und BI"" auf dem Laufenden halten. Unsere Blogbeiträge spiegeln Situationen aus unserem Arbeitsalltag wider und sollen Ihnen Anregungen und Hilfestellungen bei Ihren aktuellen Projekten geben., Regelmäßig berichten unsere Consultants ab jetzt über spannende und innovative Themen, die dem Leser einen praxisrelevanten Mehrwert bieten und darüber hinaus Hinweise zur Umsetzung von DWH- und BI-Projekten liefern., Die Grundlage für unsere Beiträge liefern unsere aktuellen Projekte sowie Neuheiten und Updates bei den Technologien, die wir im Bereich Data Warehouse und BI verwenden. Es wird konkret sowohl der Umgang mit Tools und mit Datenbanken näher beleuchtet als auch einzelne Lösungsansätze, die maßgeblich zum Projekterfolg beigetragen haben. Profitieren Sie also in Zukunft von den Erfahrungswerten unserer Experten und lesen Sie regelmäßig Neues zu unseren Methoden und Best Practices in unterschiedlichen Branchen und Projekten., Unser Blog befasst sich sowohl mit den allbekannten Datenbanken wie Oracle- und MS-SQL-Server-Datenbanken und deren ETL-Tools (SQL Server Integration Services und Oracle Data Integrator) als auch mit den neuen Trends in der DWH-Automatisierung (z.B. WhereScape). Zudem werden SAP-Lösungen wie das SAP Business Warehouse (on HANA), SAP Data Services und SAP HANA betrachtet. Die Anbindung von einem EDWH an Big-Data-Plattformen wird dabei ebenfalls behandelt., Neben den DWH-Tools wird in dem Blog auch auf unsere Erfahrungen mit den Modellierungstechniken (Data Vault, 3 NF, dimensionale Modellierung) und dem BI-Projektmanagement eingegangen., Warum DWH- und BI-Themen in Zukunft für alle Unternehmen immer wichtiger werden? Weil die Bedeutung von Daten und deren Auswertung in jeder Branche wächst und die Geschäftsprozesse zunehmend komplexer werden, ist es wichtig, Datenmengen möglichst effektiv zu verzahnen., Nutzen Sie unsere individuell dargestellten Problemstellungen und angewandten Lösungsstrategien für die richtige Vorgehensweise in Ihren eigenen Projekten., Bei Fragen zu unseren Beiträgen oder bei Interesse an Unterstützung im DWH- und BI-Bereich können Sie jederzeit Kontakt mit uns aufnehmen. Wir freuen uns auf Ihr Feedback!, , © 2021 b.telligent";https://www.btelligent.com/blog/der-data-warehouse-und-bi-blog/;B-Telligent;Helene Fuchs
21.04.2016;            Aufbau eines schlagkräftigen Data-Science-Teams        ;"Data Science erlebt in den letzten Jahren eine zunehmende Professionalisierung und Standardisierung. Der oft intrinsisch motivierte Datenbastler und Frickler, der die Nische ""Analyse"" in seinem Unternehmen mit sehr hohem unternehmensinternen Daten- und Prozesswissen besetzt, kommt an seine Grenzen., Zunehmende Anforderungen, gerade im Zuge der stärkeren Kundenfokussierung über alle Branchen hinweg, zwingen Unternehmen dazu, die Strukturen im Bereich Data Science zu professionalisieren: Dies reicht vom Wissen über zur Verfügung stehende Datenquellen und deren Aufbereitung bis zu schon im Unternehmen genutzten Data-Science-Produkte., So erleben wir in unserem Beratungsalltag immer häufiger die Anfrage nach Beratungsbedarf beim Aufbau eines schlagkräftigen Data-Science-Teams. Schnell stellt sich nun beim Aufbau einer Organisation die Frage, wie man die oben beschriebene ""One-Man-Show"" institutionalisiert und damit in das Netzwerk eines Unternehmens bzw. dessen Organigramm einpasst., Entscheidend ist hier die Frage, ob man die Data-Science-Kompetenz in einem Team bündeln sollte oder dezentral über verschiedene Abteilungen verteilt. Hierum soll es in diesem Blogartikel gehen., Dies ist sicher nur eine der Fragen von Relevanz, sollte aber früh geklärt werden. Am Ende des Artikels folgen weitere in diesem Kontext relevante Fragestellungen, wie z.B. die Verortung auf dem Organigramm: Soll es sich bei einem möglichen Team um eine Stabsstelle handeln oder sollte es Teil einer Fachabteilung sein?, Welches Ziel wird mit dem Aufbau eines Data-Science-Teams verfolgt? Die Antwort auf diese Frage ist kritisch für deren spätere Ausrichtung: Wird das Data-Science-Team in einer Fachabteilung zentralisiert oder installiert man einzelne Data Scientists in den jeweiligen Abteilungen., Bei der Definition des Ziels bzw. des Data-Science-Produkts ist die Unterscheidung zwischen Ad-hoc-Analyse und analytischer Applikation entscheidend:, Ad-hoc-Analyse: Eine Analyse ist statisch und wird nicht produktiv genutzt bzw. ist nicht in den Kontext einer Anwendung eingebunden. Ein klassisches Beispiel wäre hier eine Kundensegmentierung, die auf einem Snapshot der Kundenbasis zu einem gewissen Zeitpunkt entstanden ist. Es werden Handlungsempfehlungen abgeleitet, umgesetzt und zu einem späteren Zeitpunkt evaluiert. Dies geschieht aber nicht fortlaufend., Am anderen Ende des Kontinuums die analytische Applikation: Die Segmentierung wird genutzt, um Kunden, die auf die Webseite kommen, zu kategorisieren und die Nutzererfahrung so zu individualisieren. Die Verarbeitung der dafür nötigen Information kann zur Laufzeit geschehen. Der Kunde einer Data-Science-Abteilung muss jedoch nicht immer der Endkunde sein; so gibt es die Möglichkeit, interaktive Applikationen für einen kleinen Kreis, z.B. nur im Intranet zur Verfügung zu stellen. Eine interaktive Visualisierung einer Szenariorechnung ist ein Beispiel., Die zwei extremen Ausprägungen sind also auf der einen Seite ein Data Science Competence Center sowie auf der anderen Seite fachabteilungsinterne Data Scientists mit jeweiligen Vor- und Nachteilen., , , Sobald sich das analytische Zielbild einer Organisation auf der Achse Applikation vs. Data Product weiter in Richtung Applikation entwickelt, wird eine Zentralisierung in ein Data Science Competence Center notwendig. Da ist zum einen eine verstärkte Spezialisierung, die bei der Entwicklung einer Applikation notwendig ist: Es sollten im Team neben der Kompetenz, das Geschäftsmodell zu verstehen, Daten aufzubereiten und Modelle zu entwerfen und zu implementieren, auch die Fähigkeiten der Software-Entwicklung vorhanden sein. Hierzu bedarf es komplett eigener Kenntnisse, nicht nur in entsprechenden Frontend-Sprachen, sondern unter Umständen auch in technischer Architektur und dem Verständnis für einen Software-Entwicklungs-Lifecycle. Dieses umfangreiche Aufgabenportfolio und damit Anforderungsprofil an entsprechende Skills lässt sich schwer von einem Mitarbeiter alleine bewältigen., Der größte Kritikpunkt an einer zentralen Einheit ist das vermeintlich fehlende Verständnis für die Geschäftsprozesse in einer spezifischen Abteilung, z.B. Marketing: ""Wie kann jemand, der sonst Logistikoptimierung macht, nun plötzlich Bid-Management übernehmen"", ist dann ein häufig genannter Einwand. Um dem Fachbereich einen kompetenten Ansprechpartner an die Hand zu geben, der auch als Sparringspartner dienen kann, ist wiederum eine Spezialisierung innerhalb der Data-Science-Teams nötig., So wird nicht nur für eine kontinuierliche Weiterentwicklung der Data Scientists Sorge getragen, sondern durch die Teamstruktur auch der für die Modellierung benötigte Freiraum erwirkt. Wir erleben sehr häufig, dass in der Praxis gute Data Scientists nicht zu ihrer eigentlichen Aufgabe kommen, sondern primär kurzfristige Analyse- und Reporting-Aufgaben erfüllen., Wie man aus den Zeilen erkennt, ist der erfolgreiche Aufbau eines Data-Science-Teams keine leichte, aber doch sehr kritische Entscheidung für das Business. Die Entscheidung für die richtige Organisationsform, die im Idealfall das Zielbild meiner Organisation widerspiegelt, ist nur eine der relevanten Angelegenheiten: Früher oder später tauchen dann Fragen auf wie: ""Welche Prozesse und Methodiken bzw. Standards bieten sich an?"" oder ""Wie funktioniert Projektmanagement im Kontext des 'Working with Uncertainty'?"" - dazu aber mehr in späteren Blogbeiträgen., , © 2021 b.telligent";https://www.btelligent.com/blog/aufbau-eines-schlagkraeftigen-data-science-teams/;B-Telligent;Dr. Sebastian Petry
09.03.2016;            Von SAS zu R und zurück: SAS-Daten nach R transferieren        ;"SAS und R sind Themen, die dicht beieinanderliegen: Beides sind populäre Werkzeuge für Leute wie uns, die Probleme aus dem Umfeld von Statistik und Machine Learning auf mehr oder weniger großen Datenmengen lösen möchten. Trotz dieser scheinbaren Nähe gibt es wenig Berührungspunkte zwischen den beiden Communitys, und nur wenige arbeiten mit beiden Werkzeugen. Als passionierte Über-den-Tellerrand-Blicker finden wir das schade und möchten mit diesem Blogeintrag eine Miniserie starten, in der wir uns in loser Folge mit Themen beschäftigen, die beide Welten miteinander verbinden. Für diesen ersten Blogeintrag werden wir uns mit den Möglichkeiten beschäftigen, Daten zwischen den Systemen auszutauschen. Da es eine große Vielzahl an Wegen gibt, beschränken wir uns für diese Folge auf den Transfer von SASzu R; der entgegengesetzte Weg folgt in einem späteren Eintrag., Es gibt diverse Wege, SAS-Daten in ein R-System hineinzubekommen. Wir teilen sie in drei grobe Kategorien: , Mit ""generisch"" ist hier gemeint, dass diese Methoden generell für den Datentransfer zwischen allen möglichen unterschiedlichen Systemen taugen, nicht nur für R und SAS. Hier sind vor allem zwei Methoden zu nennen: der Transfer über CSV-Dateien und der gemeinsame Zugriff auf dieselbe relationale Datenbank., Die erste Methode hat den Vorteil, dass sie wenig Spezial-Know-how benötigt. Die entsprechenden Befehle sind für die meisten Anwender gute alte Bekannte: PROC EXPORT auf SAS-Seite und read.csv in R bzw. der fread-Befehl aus dem data.table-Package als bessere Alternative nicht nur für große Datenmengen. Diejenigen, die diese Befehle schon öfter verwendet haben, wissen allerdings auch, dass sehr oft zeitaufwendige manuelle Feinarbeit notwendig ist, bis der Datenaustausch wirklich funktioniert. Das fängt bei der sinnvollen (und auf beiden Seiten übereinstimmenden) Wahl des Trennzeichens an. Insbesondere wenn zum Beispiel in Text-Mining-Anwendungen längere Freitexte ausgetauscht werden sollen, hört es damit aber noch lange nicht auf. Von der richtigen Wahl der Stringbegrenzer (die in Freitexten nicht vorkommen sollten) bis zur Behandlung von Problemen mit Encodings (insbesondere beim plattformübergreifenden Transfer von länderspezifischen Sonderzeichen) gibt es hier jede Menge Möglichkeiten, sich mit scheinbar kleinen Details lange zu beschäftigen. Eine unangenehmeOption also für ungeduldige Menschen wie mich., Der gemeinsame Zugriff auf dieselbe relationale Datenbank ist wesentlich angenehmer als der Transfer via CSV. Die Probleme mit Trennzeichen, Stringbegrenzern und teilweise auch Encodings fallen weg, und Datentypen können wesentlich leichter mitübertragen werden. Überdies bietet sich die Datenbank an, um die Daten nicht nur zu transferieren, sondern auch damit zu arbeiten. Es ist zudem leicht, beim Transfer gleich nur diejenigen Daten auszuwählen, die tatsächlich relevant sind. Allerdings ist diese Methode auch höchst voraussetzungsreich. Der Zugriff auf dieselbe Datenbank von beiden Systemen aus ist oft nicht gegeben. Er ist aber oft leichter herzustellen als eine direkte Verbindung zwischen R und SAS, wie sie die nächste Methode benötigt. , Viele Methoden zum Einlesen von SAS-Daten in R setzen den Zugriff auf eine lokale SAS-Installation voraus. Weil diese Voraussetzung oft nicht erfüllt ist, gehen wir hier nicht näher auf diese Möglichkeiten ein. , Das SAS7BDAT-Format ist das Standardformat auf SAS-Seite. Es existiert in verschiedenen Varianten; insbesondere gibt es die Möglichkeit, die Komprimierung ein- oder auszuschalten. Wenn man versucht, diese Dateien mit dem R-Package ""foreign"" zu lesen, erlebt man eine Überraschung. Es geht nämlich nicht. foreign ist zwar das erste Paket, das einem einfällt, wenn man fremde Dateiformate in R einlesen will, und es unterstützt auch SAS-Dateien. Aber leider beschränkt sich dieser Support auf ein uraltes Dateiformat (SAS XPORT), das in der SAS-Community kaum mehr benutzt wird. Das Format weist geradezu groteske Beschränkungen auf, vor allem dürfen Variablennamen nicht länger sein als acht Zeichen. Leider ist es das einzige SAS-Dateiformat, zu dem SAS eine Spezifikation offengelegt hat, was auch der Grund dafür sein dürfte, dass sich die Macher von foreign darauf beschränken. Sinnvoll nutzbar ist dieses Format nicht; es bietet gegenüber schlichten CSV-Dateien nur Nachteile., Glücklicherweise hat Matt Shotwell dieSisyphusaufgabe auf sich genommen, das Dateiformat SAS7BDAT zu analysieren. Er hat auf Basis der durch Reverse Engineering erworbenen Erkenntnisse ein R-Package gleichen Namens programmiert, das diese Dateien lesen kann (Schreiben ist nicht möglich). Leider kommt das Package nur mit der unkomprimierten Variante des Dateiformats zurecht. Für Alternativen, die auch komprimierte Dateien lesen können, siehe unten., Zum Glück ist eseinfach, die Komprimierung beim Erzeugen der Datei abzuschalten: einfach im DATA-Step die Option ""COMPRESS=NO"" setzen. Allerdings schreiben sehr viele SAS-Systeme defaultmäßig die komprimierte Variante (das Defaultverhalten lässt sich mit OPTIONS konfigurieren). Das bedeutet, dass man in vielen Fällen nicht einfach eine SAS-Datei nehmen und in R einlesen kann. Vielmehr muss man eine in R lesbare Datei extra erzeugen, indem man die Kompression abschaltet. Dann funktioniert das Einlesen zuverlässig, wenn auch nicht übertrieben schnell., Aufbauend auf der Arbeit von Matt Shotwell wurde später eine Java-Library entwickelt, die auch mit komprimierten Dateien zurechtkommt. Diese Java-Library hat Matt Shotwell wiederum in einem R-Package namens sas7bdat.parso zugreifbar gemacht. Dieses Package ist allerdings etwas schwieriger zu installieren (es benötigt rJava), und es ist nicht über CRAN verfügbar, sondern nur über GitHub. Ähnlich wie das SAS7BDAT-Package ist es eher langsam., Der beste Weg ist also, wie so oft, von der genauen Situation abhängig. Persönlich würde ich den Weg über eine gemeinsam genutzte Datenbank präferieren, und wenn das nicht möglich ist, auf eines der beiden Packages von Matt Shotwell ausweichen. Wenn nichts anderes geht, würde ich mir schließlich mit CSV-Dateien behelfen., , © 2021 b.telligent";https://www.btelligent.com/blog/von-sas-zu-r-und-zurueck/;B-Telligent;Dr. Michael Allgöwer
11.02.2016;            Best Practice: Kampagnendurchführung        ;, © 2021 b.telligent;https://www.btelligent.com/blog/best-practice-kampagnendurchfuehrung/;B-Telligent;Dr. Wolfgang Leußer
04.02.2016;            Boosting für den naiven Bayes-Klassifikator        ;"Es gibt viele Bereiche, in denen sich die Neurowissenschaft und das maschinelle Lernen überlappen. Einer davon ist das Kombinieren des Lernens während mehrerer Lernepisoden mit kleinen Erfolgen, um am Ende ein daraus verschmolzenes, stärkeres, gelerntes Modell für eine bestimmte Aufgabe zu nutzen. Dieser Vorgang wird im maschinellen Lernen als ""Boosting"" (auf Deutsch ""Verstärken"") bezeichnet. Gerade in der IT-Branche ist das Entwickeln von Lösungen dieser Art ein sehr interessantes Thema, weshalb nachstehend eine kurze Einführung in das maschinelle Lernen erfolgen soll, die die Grundideen sowie die Anwendung des naiven Bayes-Klassifikators in R darstellt., Vor allem in den 90er Jahren wurde in den Bereichen der Neurowissenschaft und des maschinellen Lernens intensiv über die Idee diskutiert, verschiedene Modelle aus Lernepisoden bei einer bestimmten Art von Aufgabe zu kombinieren (siehe Referenzen). Diese Lernprozesse stelle ich im Folgenden erst einmal kurz und intuitiv vor:, Der ursprüngliche Algorithmus, der die oben beschriebene Logik umfasst, wurde von Freund und Schapire (1996) entworfen und heißt ""AdaBoost"" (steht für ""Adaptable Boosting"" oder ""anpassbare Verstärkung"")., Welche Voraussetzung für ein Modell gegeben sein muss, damit es ""boosted"" (verstärkt) werden kann? Ein ""schwacher"" Lerner muss die Lernepisoden durchführen. Das heißt, das Kriterium großer Varianz muss erfüllt werden (z.B. Entscheidungsbäume mit sehr niedriger Tiefe), denn das Boosting bei Modellen mit niedriger Varianz (siehe Referenzen) ist wirkungslos. Modelle des Bayes?schen Lernens wurden damals in den 90er Jahren u.a. wegen ihrer eleganten mathematischen Formulierung und der experimentellen Übereinstimmungen hochgeschätzt. Heutzutage sind naive Bayes-Klassifikatoren als produktive Lösungen z.B. für die Identifizierung von E-Mail-Spam sehr bekannt und durchschaubar. Der naive Bayes-Klassifikator bietet sich als ""schwaches"" Modell an, um ihn mit dem AdaBoost-Algorithmus zu verstärken., Nach einer kurzen Internetrecherche findet man zwar R-Pakete z.B. für das Boosting von Entscheidungsbäumen, aber nicht für den naiven Bayes-Klassifikator. Es gibt in der Literatur mehrere Varianten des AdaBoost-Algorithmus, aber ich habe mich diesmal dafür entschieden, den ursprünglichen ""AdaBoost.M1"" von Freund und Schapire mit dem naiven Bayes-Klassifikator in R zu programmieren. Dank der R-Syntax und der R-Pakete ist dies echt simpel und man lässt sich schnell von der Idee begeistern, den einfachen Klassifikator mit seiner verstärkten Version zu vergleichen., Problem: Klassifikation zweier Klassen {0, 1} mit 52 Merkmalen., Datenquelle: Ich habe 1.000 Beobachtungen von 52 Prädiktoren ohne fehlende Werte und der Zielvariablen aus dem ""Weight Lifting Exercises Dataset"" selektiert. Diese Datensätze sind hier zugänglich und können, laut deren ""CC BY-SA""-Lizenzierung, frei selektiert, verarbeitet und sowohl akademisch als auch kommerziell benutzt werden, solange die darauf basierenden Werke auch unter den Bedingungen von ""CC BY-SA"" veröffentlicht sind., Datenaufbereitung: Die Datensätze wurden von Anfang an 70%/30% für Training und Test getrennt, derselbe Seed wurde immer benutzt etc., Vergleichskriterium: ""Accuracy"" aus der R-Funktion ""confusionMatrix()""., Ergebnisse:, 1. Normaler naiver Bayes-Klassifikator:, 2. Boosted naiver Bayes-Klassifikator:, Das Ergebnis mit den Testdaten war zwar nicht unerwartet, aber trotzdem sehr erfreulich. Das Gute ist, dass jeder meinen R-Code mit seinen eigenen Daten testen kann. Sogar die Klassifikationsmethode kann durch eine andere dank des R-Pakets ""caret"" sehr schnell ersetzt und getestet werden! , Und hier unten füge ich noch meinen R-Code bei:, , © 2021 b.telligent";https://www.btelligent.com/blog/boosting-fuer-den-naiven-bayes-klassifikator/;B-Telligent;Dr. Michael Allgöwer
21.01.2016;            Deep Learning im Scheinwerferlicht        ;Der allgemeine Zeitgeist im Bereich künstlicher Intelligenz dreht sich seit geraumer Zeit um das Thema Deep Learning. Fast wöchentlich werden neue Anwendungsbereiche im Sturm erobert. So auch der nun bekannt gewordene Erfolg des Google-ProgrammsAlphaGo, das in einem Wettkampfgegen einen europäischen Go-Profi alle der insgesamt fünf gespielten Durchgänge gewann. Die Kombination aus Deep-Learning-Methoden und dem Monte-Carlo-Suchalgorithmus war der Grundstein, um fast 20 Jahre nach der ersten gewonnenen Schachpartie gegen einen Profi diesen Meilenstein auch in Go zu erreichen. Aufgrund der sehr komplexen Entscheidungsmuster und der Bewertungen der Züge im Go-Spiel wurde es als letzte Bastion des Menschen gegenüber der künstlichen Intelligenz angesehen, und dieses Ziel galt damit unter vielen Experten noch für die nächsten 10 Jahre als nicht erreichbar. Mehr Informationen zu diesem sich selbst trainierenden Programm finden Sie imBlogeintragvon Google., Die mittlerweile schon alltäglichen Berührungspunkte mit Deep Learning umfassen beispielsweise die Gesichtserkennung von Bildern bei Facebook, die Spracherkennung und Verarbeitung von Siri, Google Now oder Cortana sowie die Videoverarbeitung von selbstfahrenden Autos. Ein sehr interaktives und Menschen verbindendes Einsatzfeld ist zum Beispiel auch die Echtzeitübersetzung von gesprochener Sprache durch denSkype Translator. In dem sehr heterogenen Anwendungsbereich der Mensch-zu-Mensch-Kommunikation werden maschinelle Spracherkennung und maschinelle Übersetzung mit Hilfe von Deep-Learning-Verfahren derart stark verbessert, dass die damit erreichte Reduktion der Fehler und gleichzeitig auch die Erhöhung der Robustheit erst den Einsatz ermöglichen., Die Lernphasen der Skype-Translator-Modelle verwendeten bis zu 3 Milliarden sehr komplexe und unterschiedliche Datensätze wie übersetzte Webseiten, benannte Videos und bereits übersetzte Einzelgespräche. Modelle bilden die frei gefundenen Beziehungen ab und können sich mit neuen Daten immer weiter entwickeln. Diese Datenmengen und die daraus resultierenden Modelle können nur noch mit verteilter und GPU-basierter Infrastruktur verarbeitet werden. Die sehr zeitaufwendigen Lernphasen für die verbundenen Schichten im tiefen Netzwerk konnten durch die Parallelisierung der Verfahren auf beliebig viele GPUs von mehreren Tagen auf einige Stunden reduziert werden. Dies ermöglicht die erforderliche Interaktivität bei der Testbarkeit der Modelle., Eine sehr anschauliche und einfache Einführung zu Deep Learning - Überblick, Anwendung, Verfahren, Modelle und Tools - ist im YouTube Channel vonDeepLearning.TVzu finden., Als Grundlage des Skype Translators sowie des Sprachassistenten von Windows 10 dient das vor wenigen Tagen ins Open-Source-Lager überführteComputational Network Toolkit(CNTK)von Microsoft, das verteilte GPU-Berechnungen über mehrere Einheiten ermöglicht und damit momentan ein Alleinstellungsmerkmal zu den anderen bekannten Deep-Learning-Bibliotheken wieCaffe,Torch(Facebook),Theano,DL4JoderTensorFlow(Google) bietet. Gleichzeitig integriert sich CNTK in das Azure-Ökosystem und wird den GPU-Instanzen eine größere Aufmerksamkeit zukommen lassen, wenn die Skalierung des Modells in der Lernphase relevant ist. Somit tritt CNTK in direkte Konkurrenz mit TensorFlow, wenn es um die Verknüpfung von Standard-Tools und den sich daran anschließenden produktiven Einsatz im Cloud-Ökosystem geht., Das CNTK wird momentan von Microsofts internem Benchmark als schnellstes Deep-Learning-Tool für die bereitgestellten Methoden dargestellt. Eine ausführliche Einführung lässt sich imNIPS Tutorialvom Dezember 2015 nachlesen. Die bereits integrierteNVIDIA-CUDA-Deep-Neural-Network-Bibliothek(cuDNN) in Version 4 und das optimierte1-Bit-Stochastic-Gradient-Descent-Verfahren(Frank Seide et al.), das jedoch eine Microsoft-Lizenz hat, sind hier die Hauptfaktoren. Die meisten Open-Source-Tools verwenden momentan noch die ältere und langsamere cuDNN-Version 3 (d.h. Torch) oder sogar Version 2 (TensorFlow). Torch und TensorFlow haben zum jetzigen Zeitpunkt die cuDNN-Version 4 in der Entwicklung und sollten in diesem Punkt zeitnah zum CNTK aufschließen. Facebook AI Research (FAIR) hat durch die Veröffentlichung einiger interner Pakete Ende Januar 2016 die Lücke im Bereich Geschwindigkeit weiter geschlossen und bietet jetzt weitere Parallelisierungsoptionen für Modelle und Daten sowie das 1-Bit-Stochastic-Gradient-Verfahren an (zum Nachlesen in diesem Blogeintrag). Gleichzeitig wird durchiTorch, als Schnittstelle von Torch und iPython,der einfache Einstieg in das Tool erleichtert. Die spannendste Frage wird allerdings sein, wie schnell und vor allem wann Google die Verteilung über mehrere GPU-Einheiten in TensorFlow überführt. Diversen Blogkommentaren ist zu entnehmen, dass dies für Mitte 2016 geplant ist, wodurch dem stärksten Ökosystem in Sachen Dokumentation, Tutorial und Open-Source-Support ein weiterer Auftrieb gegeben wird., Auch ein spannender Aspekt des CNTK-Benchmarks ist die gewählte Mini-Batch-Größe der zu lernenden Daten. Die Anzahl der verwendeten Datenpunktefür das SGD-Verfahren wurde auf8.192gesetzt. Diese Größe ist für die meist wenigen Merkmale in der Sprachverarbeitung realistisch, jedoch wäre ein Test für kleinere Datengrößen wie 512 oder 128 hier sehr informativ. Im SGD-Verfahren wird der Gradient über die Mini-Batch-Größe gemittelt. Somit beeinflusst die Größe, wie verrauscht der Gradient ist und welcher Pfad bei der Minimierung im Netzwerk verfolgt wird. Dies wirkt sich wiederum auf die Geschwindigkeit, die Genauigkeiten und Problemstellungen (d.h. Overfitting) der Lernphasen aus., Eine sehrübersichtliche Evaluierungnach Modellen, Schnittstellen, Bereitstellung, Architektur sowie Ökosystem der gängigen Open-Source-Tools im Deep-Learning-Umfeld helfen bei der Wahl des Tools. Die Vorstellung einiger Tools auf demYouTube Channel vonDeepLearning.TVunterstützt hier nochmal enorm das Verständnis., Spannend für den Bereich Data Science bleibt auch die Adaption und die einfache Verbindung zuApache Spark. Wie der Beitrag vonDatabrickszeigt, ermöglicht die Kombination aus TensorFlow und Spark, dass moderate Modellgrößen sehr einfach und schnell gelernt und evaluiert werden können., Sollte einer der folgenden Anwendungsbereiche von Interesse sein, lohnt es sich, weiter in den Themenbereich Deep Learning einzutauchen und unsere folgende, praxisorientierte Deep-Learning-Reihe in den nächsten Blogbeiträgen zu lesen:, Text, Sound, Bilder, Video, Zeitreihen, , © 2021 b.telligent;https://www.btelligent.com/blog/deep-learning-im-scheinwerferlicht/;B-Telligent;
19.01.2016;            Ein Korb voller Schlangen: Python Module für Data Science        ;"Wer meine früheren Blogeinträge gesehen hat, der weiß, dass ich sowohl ein großer Fan von R als auch von Python in der täglichen Arbeit bin., So mächtig R auch im Funktionsumfang für Datenanalyse und Modellierung ist, so schnell wird der Elan beim ""number crunching"" auch gedämpft, wenn der Arbeitsspeicher auf Oberkante läuft., Eine schöne Serverinstallation mit viel Blech (z.B. 96Gig-RAM) wirkt dabei Wunder., Da diese Option nicht immer zur Verfügung steht, habe ich aus der Not eine Tugend gemacht und mich der performanteren Alternative, nämlich den Python-basierten R-Alternativen zugewandt, zumal ich eh schon seit langem Python für ETLs und Datenaufbereitungen einsetze., Klingt leichter gesagt als getan. Wie so häufig im Open-Source-Umfeld offenbarte sich dort ein neues Ökosystem an Tools mit einer reichhaltigen, exotischen und internationalen Flora an Code-Gewächsen. Es war auf den ersten Blick gar nicht so einfach, die Abkürzungen für die einzelnen Tools und ihre Schwerpunkte klar voneinander abzugrenzen. Zusätzlich wurde auch sehr schnell klar, dass die Motivationen für die Entwickler und die fachlichen Hintergründe der ersten User ganz klar andere waren als die eines SPSS- oder SAS-Jockeys. Sie waren ganz klar an Machine Learning orientiert. Damit meine ich z.B.:, Machine Learning in Python hat diese Punkte nicht für sich allein reserviert, da sie genauso auch für R-Nutzung zutreffen können, wenn auch häufig nur mit entsprechenden Erweiterungen. Es ist vielmehr mein subjektiver Eindruck, der entsteht, wenn ich die Dokumentationen, die Tutorials und die Schwerpunkte der Funktionsumfänge sehe. Allein wenn man sich das Cheat-Sheet für scikit-learn-Algorithmen zu Gemüte führt, sieht es auf den ersten Blick so aus, als würden herkömmliche Regressionsverfahren gar nicht auftauchen. Es wird direkt der Sprung zu regularisierten Methoden gemacht und statt logistischer Regression werden SVMs für ein binäres Klassifikationsproblem nahegelegt., Auch wenn dieses ganze Sammelsurium an Python-Modulen etwas wirr und unzugänglich erscheint - ein Korb voller Schlangen -, sehe ich in dieser Technologie gewaltiges Potential. Es ermöglicht sehr performante, formal stringente, höchst automatisierbare, zu großen Anwendungen skalierbare und leicht deploybare Machine-Learning-Anwendungen., Folgende Auflistung von Elementen soll für Einsteiger eine kleine Orientierungshilfe durch den Jungle an Abkürzungen geben:, NumPy stellt höchst performante Datenstrukturen bereit. Es eignet sich hervorragend für die Verarbeitung von Daten in sehr umfangreichen Vektoren und Matrizen. Die Grundlagen wurden in C und Fortran implementiert, worauf die gute Performance zurückzuführen ist. NumPy beinhaltet auch bereits Möglichkeiten zu mathematischen Berechnungen, dies gehört aber nicht zum primären Einsatzzweck. Die Datentypen und Datenstrukturen von NumPy bilden die Grundlagen für alle weiteren hier aufgeführten Python-Technologien., SciPy setzt auf NumPy auf und erweitert es um Möglichkeiten für Scientific Computing. SciPy mit NumPy darunter kann sozusagen als kleine Python-Alternative für Matlab gesehen werden., Für das alltägliche Analysieren oder im Bereich von Machine Learning ist SciPy direkt eher uninteressant., Pandas baut ebenfalls auf NumPy auf und verbessert die Usability hin auf Datentransformation und Analyse., NumPy-Strukturen werden in Objekte verpackt, die den R-Datentypen recht ähnlich sind. Es gibt also Series (R-Vektoren) und Dataframes. Das Subsetting/Filtern, Neuberechnen von Variablen, Aggregieren und Verknüpfen von Dataframes funktioniert relativ intuitiv und ist von der Bedienung ganz klar an R angelehnt., Pandas-Objekte erleichtern die Arbeit, indem sie über Methoden zur deskriptiven Statistik und zur Visualisierung verfügen. Die Standards für eine explorative Datenanalyse sind also schon gleich mitgeliefert. Es lohnt sich bei der Fülle an Methoden ab Werk, öfter mal die Doku zu überfliegen., Dieses Paket ist die zentrale Machine-Learning-Bibliothek von Python. Sie bietet viele moderne Verfahren für jeden Bereich des Machine Learnings. Dadurch, dass jede Art der Modellierung über dasselbe Interface angesprochen wird, kann man sehr einfach mit noch generischerem Code als in R verschiedene Modelle gegeneinander testen. Bemerkenswert ist auch das beinhaltete Set an Standardtools für Variablentransformationen. Ein Wermutstropfen dabei ist die Tatsache, dass die scikit-Modellierung zunächst nur modelliert und nicht viel mehr macht. Es gibt keine Modellzusammenfassung, keine Charts, selbst einfachste Diagnostiken wie die Residuen muss man sich z.T. selbst berechnen und so fort., Der Name scikit stammt von ""SciPy Toolkit"". Es wurde also ausgehend von SciPy entwickelt, da SciPy selbst nicht über entsprechende Methoden verfügt., Statsmodel ist eine sehr neue Alternative zu scikit-learn, die deutlich näher an R angelehnt ist und den Schwerpunkt mehr auf klassische Modellierung bzw. Datamining legt. Hier erhält man nach Berechnung eines Modells eine schöne Zusammenfassung mit einer nützlichen Auswahl an Kennzahlen. Das Modellobjekt enthält auch viele weiterverwertbare Berechnungen, wie man es aus R gewohnt ist. Es existiert sogar ein Interface, das Modellformeln in derselben Notation wie in R akzeptiert., IPython ist zunächst eine interaktive Kommandozeile für Python. Die intelligente Autovervollständigung erleichtert das Explorieren von Daten dramatisch. Man darf die Konsole aber nicht automatisch mit dem IPython Notebook gleichsetzen. Das Notebook hebt die IPython-Konsole nochmals auf eine webbasierte Arbeitsumgebung, die viele Usability-Features enthält, Charts und Output während des Arbeitens im Code visualisiert und Veröffentlichungsmöglichkeiten im Sinne von ""reproducible research"" bietet. Wer also eine Python-Alternative zu Rs knitr sucht, landet zwangsläufig beim IPython Notebook., Machine Learning unter Python ist erst eine richtig runde Sache mit schicken Visualisierungen und hier springt matplotlib in die Bresche. Die Grafikbibliothek, die ursprünglich unabhängig von den oben genannten Technologien entstand, ermöglicht mit ausreichendem Skill des Entwicklers jede erdenkliche Visualisierung, da alle noch so kleinen Objekte in einem Chart selektierbar und modifizierbar bleiben. Gleichzeitig bietet es aber auch für Einsteiger genügend ""High-Level""-Funktionen, um schnell die ersten Charts auf Publikationsniveau zu erstellen., Der Hype mit Machine Learning in Python startete wohl erst so richtig parallel zum Buzzword ""Data Science"". Das soll aber nicht den Eindruck erzeugen, dass die aufgeführten Technologien ebenso brandneu sind. Im Gegenteil, die meisten Module, mit Ausnahme von Statsmodels, reichen in der Historie viel weiter zurück und numpy z.B. kann auf 20 Jahre zurückblicken. Ich selbst habe beispielsweise matplotlib vor etwa 7 Jahren bereits produktiv für Präsentationscharts eingesetzt. Man kann mittlerweile durchaus behaupten, dass für einen fachkundigen User die Technologien an einem belastbaren Entwicklungsstand angelangt sind. NumPy und Pandas werden unter anderem auch in der Finanzwelt eingesetzt, und da ist Belastbarkeit definitiv kein bloßes ""Nice-to-have"". Mein Hinweis auf ""fachkundige User"" begründet sich damit, dass man aktuell noch häufig mit nicht sehr schlüssigen Fehlermeldungen konfrontiert wird und ein gewisser Erfahrungsstand sehr hilfreich ist, um das Problem zu umschiffen., Wenn man selbst diese Technologien ausprobieren möchte, scheitert man leider ziemlich sicher schon bei der ersten Installation von Paketen, unter Windows zumindest. Normalerweise reicht auf einer aktuellen Python-Installation ein Befehl wie ""pip install sklearn"". Dieser bricht aber wieder ab, wenn nicht ein Sammelsurium an Compilern, z.B. Visual Studio, installiert ist. Solange man keinen Informatikhintergrund besitzt und Ähnliches anderweitig benötigt, möchte man seine Installation nicht mit diversen Compilern zumüllen. Eine Alternative wäre eine Installation von bereits kompilierten inoffiziellen Binaries, wie ich es gelöst habe., Der beste Einstieg dagegen ist vermutlich gleich die Installation einer erweiterten Python-Distribution wie Anaconda. Diese beinhaltet alle oben genannten Pakete und noch viel mehr. Sie ist etwas umfangreicher als eine normale Python-Installation, dafür ist der Umfang für Datenanalyse maßgeschneidert und es sind sogar von Continuum Analytics Enterprise-Versionen verfügbar., Der Überblick hier ist natürlich nur eine subjektive Sicht ohne Anspruch auf Vollständigkeit. Abgesehen davon entwickelt sich die Python-Community so schnell, dass der Post morgen schon wieder veraltet ist., Ich hoffe, dieser Post hat dennoch Interessierten geholfen, sich etwas im Jungle der Begrifflichkeiten zurechtzufinden., , © 2021 b.telligent";https://www.btelligent.com/blog/python-module-fuer-data-science/;B-Telligent;Stefan Seltmann
21.12.2015;            Zeitreihenanalyse leicht gemacht – ganz ohne Analysetool        ;Als Nächstes steht b.telligent vor der Aufgabe, eine Prognose auf Wochenebene zu erstellen, wobei sich völlig neue und andere Herausforderungen ergeben!, , © 2021 b.telligent;https://www.btelligent.com/blog/zeitreihenanalyse-leicht-gemacht/;B-Telligent;Katharina Moltz
17.12.2015;            HowTo: Verbinden von Zellen mit arcplan 8.6        ;"Als Datengrundlage wird eine Tabelle herangezogen, die Werte für Kostenstellen, Kostenarten und Kosten eines Unternehmens liefern soll., , , In arcplan Version 8.5 und niedriger könnte eine Tabelle auf Basis dieser Daten folgendermaßen gestaltet worden sein:, , , Mit Hilfe der neuen Funktion ""Zellen zusammenfügen"" in arcplan 8.6 , sieht eine mögliche Darstellung in einer Tabelle wie folgt aus:, , , Die Tabelle ist jetzt merklich aussagekräftiger und verständlicher. Im Folgenden sollen nun die nötigen Schritte zur Zusammenfügung der Zellen erläutert werden., Zunächst wird eine neue Tabelle erstellt, in der die Zellen, die später verbunden werden sollen, noch getrennt sind., , Zunächst soll das vertikale Verbinden der Zellen gezeigt werden. Für die vertikale Verbindung wird die Primärzelle, die im ersten Schritt zusammengefügt werden soll, in der Matrix durch eine 2 dargestellt. Zellen darunter, die verbunden werden sollen, müssen durch eine 1 gekennzeichnet werden., , Nun muss der Tabelle noch mitgeteilt werden, dass die Matrix die Parameter zum Zusammenfügen beinhaltet. Dies erreicht man (während man im Designmodus ist) durch: Rechtsklick auf die Überschriftstabelle -&gt; ""Objekt formatieren..."" -&gt; Reiter ""Zellen"" -&gt; als Anweisungsobjekt die Matrix referenzieren., , die horizontale Verbindung wird die Primärzelle durch eine 4 und Zellen, die mit dieser zusammengefügt werden sollen durch eine 3 markiert. Es folgt die vollständige Matrix und die fertige Überschriftstabelle:, , Das Endergebnis könnte dann in arcplan 8.6 folgendermaßen aussehen:, , arcplan Version 8.6 bietet die Möglichkeit, Zellen innerhalb eines Tabellen-, Spalten- oder Zeilenobjektes zu verbinden. Hierfür muss eine Matrix erstellt und in der Formatierung des Objektes, welches die zu verbindenden Zellen beinhaltet, als Anweisungsobjekt referenziert werden., Primärzellen werden in der Matrix durch eine 2 gekennzeichnet. Die Zellen, die mit der Primärzelle vertikal verbunden werden sollen, werden durch eine 1 markiert., Primärzellen werden in der Matrix durch eine 4 gekennzeichnet. Die Zellen, die mit der Primärzelle horizontal verbunden werden sollen, werden durch eine 3 markiert., , © 2021 b.telligent";https://www.btelligent.com/blog/howto-mit-arcplan-86-zellen-einfach-verbinden/;B-Telligent;Stefan Kersten
15.12.2015;            Predictive Analytics und die Präsidentschaftswahl        ;"Das Duell zwischen Barack Obama und Mitt Romney bei der letzten Präsidentschaftswahl endete miteinem sehrklaren Ergebnis, einem Sieg für Obama und einer krachenden Niederlage für Romney. Die Teilnehmer meines Webinars zu dem Thema wissen, dass das nicht nur an den rhetorischen Fähigkeiten der Kandidaten, dem Klima in der amerikanischen Gesellschaft und den politischen Programmen der Kontrahenten lag, sondern dass es noch einen weiteren großen Unterschied gab., Dieser Unterschied war Obamas sehr massiver, sehr teurer, sehr professioneller und sehr erfolgreicher Einsatz eines ganzen Bündels an Predictive-Analytics-Methoden im Wahlkampf. Diese Methoden führten dazu, dass Obamas Wahlhelfer jederzeit wussten, an welche Haustür sie klopfen und zu welchem der Themen auf Obamas politischer Agendasie die Bewohner in ein Gespräch verwickeln mussten. Sie konnten genau berechnen, wie sie sichere Obama-Befürworter zum Wählen bewegen und politischUnentschlossene auf Obamas Seite ziehen konnten. Gleichzeitig verschwendeten Romneys Unterstützer ohne vergleichbare Informationen ihre Zeit in Straßenzügen, von denen man bei Einsatz prädiktiver Methoden gewusst hätte, dass dort vorwiegend eingefleischte Demokraten zu finden sind, die kein Wahlhelfer auf Romneys Seite würde ziehen können., Statistische Modelle erlaubten es Obamas Team auch sehr genau einzuschätzen, in welchen der ""Swing-States"" sie das Rennen ohnehin schon gewonnen hatten. So konnten sie ihre Ressourcen auf die verbleibenden Staaten konzentrieren. Es stellte sich am Wahltag heraus, dass diese Modelle derart genau waren, dass sie für jeden einzelnen Bundesstaat mit einer Abweichung von maximal einem Prozentpunkt das richtige Ergebnis prognostizierten. Während also Obamas Data-Science-Team den Wahltag als ""Model Validation Day"" feierte, hatten Romneys Leute nichts Vergleichbares aufzubieten., Eine der Stärken der Obama-Kampagne war die Datenbasis, auf der sie ihre verschiedenen Modelle erstellten. Anders als in den meisten europäischen Ländern gibt es in den USA recht gut zugängliche und umfassende Daten darüber, wer bei welcher Wahl seine Stimme abgegeben hat. Dieses sogenannte Voter File, eine amerikanische Besonderheit, ergänzte Obamas Team in einer Weise, die man in Europa schwer nachahmen könnte: Daten zu politischen Präferenzen der Wähler lassen sich in den USA durch die Registrierungen als Unterstützer einer bestimmten Partei gewinnen. Diese Registrierung ist deswegen wichtig und relativ beliebt, weil sie zur Teilnahme an den Vorwahlen berechtigt, bei denen die Parteien ihre Kandidaten auswählen. Darüber hinaus verwendete Obamas Team zugekaufte Marketingdaten, die in den USA ebenfalls in einer extremen Vielfalt und Detailtiefe verfügbar sind. Dazu kamen umfangreiche eigene Telefonbefragungen, wie sie sich nur eine finanzkräftige Organisation wie die Obama-Kampagne leisten kann., Diese einzigartige Datenbasis ermöglichte extrem hilfreiche und wertvolle Prognosen. Die dazu verwendeten Modellierungsmethoden waren vor allem bei der Vorhersage der Wahlergebnisse auf Bundesstaatsebene innovativ. Bei der Steuerung der Wahlhelfereinsätze genügten an gängige Standards des Direktmarketings angelehnte Modelle, um die Obama-Kampagne mit unerreichter Präzision zu steuern., Die Republikaner haben aus dieser Niederlagedie Konsequenzengezogen. Beim Vortrag von Amelia Showalter bei der Predictive Analytics World 2015in Berlin war zu erfahren, dass alle republikanischen Kandidaten für die nächste Präsidentschaft jeweilsein hochkarätiges Data-Science-Teamaufgestellt haben, um das Data-Science-Desaster vom letzten Mal nicht zu wiederholen., , © 2021 b.telligent";https://www.btelligent.com/blog/predictive-analytics-und-die-praesidentschaftswahl/;B-Telligent;Dr. Michael Allgöwer
05.11.2015;            Data-driven Marketing á la Weihnachtswichtel        ;, , , , , © 2021 b.telligent;https://www.btelligent.com/blog/data-driven-marketing-weihnachtswichtel/;B-Telligent;Anne-Kathrin Stolz
04.11.2015;            Predictive Analytics World: Tag 2        ;"Der zweite Tag der Predictive Analytics World: Einbrecher, sensible Daten und was man daraus über das Data-Science-Storytelling lernen kann., Der zweite Tag der Predictive Analytics World brachte sogar noch einmal eine deutliche Steigerung gegenüber dem ersten. Gleich der erste Vortrag war etwas Besonderes. Es ging um Predictive Policing. Der Vortragende Thomas Schweer ist ein waschechter Kriminologe, ein Experte für organisierte Kriminalität und Terrorismus. Predictive Policing allerdings ist eher etwas für die Massendelikte, und so ging es im Vortrag vor allem um Einbrüche., Das Muster, das hier Vorhersagen erlaubt, ist das sogenannte ""Near-Repeat-Phänomen"". Darunter versteht man die Tatsache, dass nach einem Einbruch eine hohe Wahrscheinlichkeit besteht, dass innerhalb der nächsten 72 Stunden in der unmittelbaren Nachbarschaft erneut eingebrochen wird. Es gibt Indizien dafür, dass diese Serien auf professionelle Einbrecherbanden zurückgehen. Allerdings ist die Aufklärungsquote bei Einbruchsdelikten mit rund 15% sehr gering, so dass man diese Vermutung letztlich nicht verifizieren kann., Was den Vortrag sehr interessant und kurzweilig machte, waren weniger methodische Schmankerl; der prognostische Algorithmus hinter dem vorgestellten System ist, so darf man vermuten, eher schlicht. Es waren die Einblicke in die Kriminologie des Einbruchs und in die Zusammenarbeit mit der Polizei. Man erfuhr viel über die Vorgehensweise von Profis (""nehmen nur mit, was in eine Socke passt"") und die Erfolgsaussichten der Einbruchsprävention (""manchmal gibt es sogar Festnahmen""). Auch die aus dem geschäftlichen Kontext sattsam bekannten schiefen Verteilungen tauchen hier wieder auf: Man geht davon aus, dass 4% der Täter 40% der Einbrüche begehen. Der Vortrag war aber nicht nur wertvoll, weil er so kurzweilig war, sondern auch weil er implizit an eine wichtige Tatsache erinnerte: dass fachliche Kenntnisse ebenso wichtig sind wie statistisch-algorithmische., , Der darauffolgende Vortrag von Philip O?Brien war ein schönes Beispiel dafür, wie ein im Umgang mit prädiktiven Methoden erfahrenes Unternehmen diese umsichtig und mit viel Fingerspitzengefühl auch in einem sehr sensiblen Bereich sinnvoll nutzen kann. Das Unternehmen ist Paychex, ein internationaler Anbieter von Lohnabrechnungsoutsourcing und verwandten Dienstleistungen, und der sensible Bereich ist ein Churnscore. Sensibel deshalb, weil es um Kündigungen nicht von Kunden, sondern von eigenen Mitarbeitern ging., Umsicht und Fingerspitzengefühl waren als Erstes bei der Wahl der Prädiktoren gefragt. Alle Prädiktoren, die die Gefahr der Diskriminierung mit sich brachten, wurden von vornherein ausgeschlossen: Alter, Geschlecht, Nationalität etc. Eine weitere hochsensible Klasse von potentiellen Prädiktoren wurde ebenfalls von der Modellierung ausgeschlossen: alle Informationen, die mit dem Gehalt zu tun haben. Das vereinfachte zum einen die Freigabe der Rohdaten für die Nutzung im Modell. Zum anderen verhinderte es die (im Gehaltsthema immer große) Gefahr, dass das Modell als Argumentationshilfe für Eigeninteressen Einzelner missbraucht wird., Die Modellierung selbst war eine unspektakuläre logistische Regression. Für die Nutzung im Unternehmen werden daraus lediglich fünf (und nicht wie andernorts zehn oder zwanzig) Scoreklassen gebildet, und zwar in Anlehnung an angloamerikanische Schulnoten von A (am besten, hier: niedrigste Kündigungsgefährdung) bis F (am schlechtesten, hier: höchste Kündigungsgefährdung)., , In diesem Fall allerdings haben die Ergebnisse auf individueller Ebene die Data-Science-Abteilung nie verlassen. Verwendet wurden nur auf räumliche Organisationseinheiten aggregierte Informationen; eine weitere umsichtige Entscheidung. Philip O?Brien hat brillant formuliert, warum es keine gute Idee gewesen wäre, neben jeden Mitarbeiternamen eine Scorestufe von A bis F zu schreiben: ""We feared that people might tell themselves stories about their score."" Und er hat Beispiele genannt, was für Geschichten das sein könnten: ""Ich bin in Stufe A eingeordnet worden. Jetzt denkt mein Chef, dass er mich ja ohnehin sicher hat, und kümmert sich nicht mehr richtig um mich."" ""Ich habe ein E bekommen, aber ich will doch bleiben! Jetzt bekomme ich eine Art von Aufmerksamkeit, die ich nicht will."" ""Ich habe ein C. Das ist so aussageloses Mittelmaß. Jetzt sind alle anderen wichtiger: Die Loyalen mit den As und Bs, auf die man bauen kann, genau wie die Unzufriedenen mit den Es und Fs, um die man sich kümmern muss."", Diese Auswahl von Beispielen konzentriert sich natürlich auf die problematischen Geschichten und lässt die positiveren außer Acht; dieser Fokus auf Risikovermeidung ist der Sensibilität des Themas angemessen. Noch interessanter ist aber die Tatsache, dass man an diesem Beispiel etwas darüber lernen kann, warum Storytelling ein integraler Bestandteil von Data Science ist und nicht bloß eine aufgepfropfte Verkaufsveranstaltung. Wenn nämlich nicht wir, die Data Scientists, eine Geschichte erzählen, die die Resultate in den richtigen Kontext einbettet und die Interpretation leitet, dann heißt das nicht, dass es keine Geschichte gäbe. Nein, vielmehr wird der Adressat unserer Bemühungen sich selbst eine Geschichte erzählen, wie es Philip O?Brien so treffend formuliert hat. Das Problem ist nur, dass dieser Adressat normalerweise den Kontext nicht kennt, aus dem die Daten stammen, den Hintergrund, den man sehen muss, um zu angemessenen Interpretationen zu kommen. Er wird sich also einen Hintergrund erfinden, und leider wird dieser mehr zu tun haben mit unserem Adressaten, seinen Erfahrungen und seiner Geschichte als mit den Daten. Die Konsequenz ist, dass ein Großteil unserer Bemühungen sinnlos sein wird, weil unsere Ergebnisse gesehen, aber im falschen Kontext interpretiert werden. Diesen Bezug zum Storytelling hat Philip O?Brien nicht explizit hergestellt, aber mit seinem spannenden Vortrag angeregt., , , © 2021 b.telligent";https://www.btelligent.com/blog/predictive-analytics-world-tag-2/;B-Telligent;Dr. Michael Allgöwer
03.11.2015;            Predictive Analytics World: Tag 1        ;"Am ersten Tag der Predictive Analytics Worldsind es die kürzeren Vorträge abseits der Keynotes, die besonders überzeugen., Amelia Showalters Vortrag über Obamas digitalen Präsidentschaftswahlkampf war, wie erhofft, einer dieser Höhepunkte. Es war zum Beispiel sehr interessant zu sehen, dass bereits das sogenannte ""Voter File"" sehr viele Merkmale enthält, noch bevor es aus anderen Quellen angereichert wird. Hieraus kann nachvollzogen werden, wer wann an welcher Wahl teilgenommen hat (was indirekt, zumindest bei häufigen Wählern, Rückschlüsse auf die Umzugshäufigkeit zulässt), wer sich als Anhänger einer bestimmten Partei registriert hat (um an den Vorwahlen teilnehmen zu können) und welche diversen soziodemographischen Daten ein Wähler bzw. eine Wählerin innehat. Ebenso spannend waren auch die Geschichten aus der Kampagne. Beispielsweise die einer Kampagnenhelferin, die dank Obamas Analytics Team eine genaue Laufliste der Haushalte hatte, die sie besuchen sollte, und das jeweils mit einem spezifischen Auftrag: In diesem einen Haushalt sollte sie versuchen, die Leute von Obama zu überzeugen, indem sie bestimmte, vorgegebene Themen ansprach. Ein anderer Haushalt hatte sich für eine Briefwahl registriert und wurde nach dem zuvor erfolgten Scoring für einen sicheren Obama-Wähler gehalten. In diesem Fall sollte die Kampagnenhelferin den gesamten Haushalt daran erinnern, die Unterlagen auch wirklich abzugeben etc. Der Witz an der Sache war, dass gleichzeitig auf der anderen Straßenseite ein Team von republikanischen Wahlkampfhelfern unterwegs war, das an allen Türen klopfte, immer dieselben Sprüche aufsagte und oft auf verschlossene Türen oder felsenfeste Obama-Anhänger traf., Ausgesprochen spannend war auch der Vortrag über Roboter-Journalismus von Marcel Hager und Alexander Siebert. Es ging um automatisch generierte Spielberichte für die unteren Fußballligen, die in Deutschland so zahlreich sind, dass unmöglich menschliche Reporter alle diese Spiele begleiten können. Es gibt in Deutschland insgesamt 1.571 (!) Amateurfußballligen. Allein im Verbreitungsgebiet zum Beispiel der Fuldaer Zeitung sind es 60 Ligen. Zu diesen Spielen gibt es üblicherweise keine Spielberichte, jedenfalls keine, die von Menschen regelmäßig erstellt werden. Was es allerdings gibt, sind die Daten der Spiele. Nicht nur das Ergebnis, sondern auch gelbe und rote Karten, Torschützen, Auswechselungen etc. Auf dieser Basis kann man tatsächlich Kurzberichte erstellen, die in der Qualität den von Menschen erstellten kaum nachstehen. Man bereitet dazu in sehr aufwendiger, in großen Teilen manueller Arbeit eine Stichprobe von Trainingstexten (mit manuell erfassten Zusatzinformationen) vor, auf denen man dann ein Machine-Learning-Modell trainiert, das sozusagen lernt, wie der Schreibstil ist, die Reihenfolge, in der Informationen dargeboten werden etc., Als weitere Zutat müssen dann noch, ebenfalls in aufwendiger, großteils manueller Arbeit, die Regeln und Besonderheiten des Spiels und der Liga modelliert werden. Das Ergebnis ist ein Modell, das Spieldaten in Kurzberichte umsetzen kann. Wer Lust hat, sich selbst ein Bild von der Qualität zu machen, findet solche Spielberichte zum Beispiel unter fussifreunde.de., Last, but not least habe ich auch selbst einen Vortrag gehalten, gemeinsam mit Juliane Homuth von unserem Kunden myToys. Es ging um unser Customer-Lifetime-Value-Projekt, das wir Anfang des Jahres durchgeführt haben. Der Vortrag hat eine Menge Spaß gemacht. Ein derart engagiertes und fachkundiges Publikum wie bei der Predictive Analytics World ist auf anderen Veranstaltungen höchst selten anzutreffen. Dementsprechend ertragreich und spannend waren die Fragen und die Diskussionen in den anschließenden Kaffeepausen., ,  Nach den interessanten Vorträgen von heute freue ich mich bereits jetzt auf den morgigen Tag!, , © 2021 b.telligent";https://www.btelligent.com/blog/predictive-analytics-world-tag-1/;B-Telligent;Dr. Michael Allgöwer
22.10.2015;            Predictive Analytics World Berlin: der Vorabend        ;"Morgen beginnt die Predictive Analytics World, und der Vorabend ist ein guter Zeitpunkt, um schon mal die Erwartungen zu justieren., Ein Blick auf das Programm zeigt diesmal parallel zum Vortragsprogramm diverse Hands-on-Workshops zu Themen wie Uplift Modelling, Predictive Analytics in der Cloud, Process Mining etc. Ich werde trotzdem den ""normalen"" Vorträgen den Vorzug geben, die weniger lang sind und daher eine größere Vielfalt an Themen und Ideen versprechen., Gerade die Vielfalt ist eine der Stärken der Predictive Analytics World, Vorträge zu Themen wie Roboterjournalismus oder Predictive Lead Management springen beim Blick auf die Agenda ins Auge. Besonders gespannt bin ich auf den Vortrag von Amelia Showalter. Sie war als Director Digital Analytics ein Teil des ebenso großen wie großartigen Data-Science-Teams, das an Obamas Sieg bei den letzten Präsidentschaftswahlen einen entscheidenden Anteil hatte. Die Teilnehmer meines Webinars zu dem Thema wissen, dass Obamas Sieg über Mitt Romney zu einem beträchtlichen Teil dem überlegenen Team für Data Science und digitale Wahlkampfführung zu verdanken war, das die Demokraten eingesetzt haben., Amelia Showalter wird zu den Data-Science-Methoden, die Obamas Team eingesetzt hat, hoffentlich weitere, bisher unbekannte Details enthüllen. Außerdem hat sie einen Ausblick darauf angekündigt, wie die Digitalisierung amerikanische Wahlkämpfe weiter prägen und verändern wird. Eines ist gewiss: Die Republikaner werden alles daransetzen, den amateurhaften Auftritt in der digitalen Wahlkampfführung nicht zu wiederholen, den sie bei den letzten Präsidentschaftswahlen hingelegt haben. Die Demokraten werden also nachlegen müssen, wenn sie ihre Überlegenheit behalten wollen. Was sie dafür tun werden und viele andere spannende Dinge erfahren wir hoffentlich morgen und übermorgen bei der Konferenz - und Sie gleich anschließend hier im Blog., Und hier schon ein kleiner Ausblick der der Predictive Analytics World von heute Morgen:, , , © 2021 b.telligent";https://www.btelligent.com/blog/predictive-analytics-world/;B-Telligent;Dr. Michael Allgöwer
20.10.2015;            Hochleistungs(denk)sport mit R        ;", Vor ein paar Jahren habe ich einen TV-Werbespot gesehen, in dem behauptet wurde, dass der sogenannte ""Fosbury Flop"" die Sportwelt des Hochspringens revolutioniert hat. Also nahm ich mir vor, mir eines Tages die Daten der Weltrekorde beim Hochspringen zu beschaffen und sie genauer zu analysieren. R bietet sich dazu als Werkzeug an. Dank der kompakten R-Syntax kann ich mir die Weltrekorde vom Hochsprung echt leicht zeigen lassen und einen etwas tieferen Blick ins Thema werfen., Als Quelle habe ich auf Wikipedia zugegriffen, und nach ein paar manuellen Textformatierungen mit Notepad++ konnte ich Daten über Rekorde, Athleten, Länder und Jahre in .CSV-Dateien speichern. Ich lud die Daten in R hoch, sowohl von Männern (m) als auch von Frauen (w), und schon konnte ich mein Vorhaben in die Tat umsetzen!, , 1. Durchbruch nach dem ""Fosbury Flop"", Man kann tatsächlich sehen, dass die Entwicklung der Weltrekorde beim Hochspringen Mitte der 60er Jahre eine Art Grenze erreicht hatte. Erst ab Mitte der 70er Jahre hatte sich Dick Fosburys (USA) neue Technik, zusammen mit der Nutzung von dicken weicheren Landematten, langsam etabliert und die Rekorde weiter gesteigert. Der Trick ist, den Schwerpunkt des Körpers während des Sprungs so tief wie möglich unter der Latte zu halten. Das erreicht man beim Springen und Landen auf dem oberen Rücken. Aber Moment mal! Hätte die Weiterverwendung von Sandgruben zum Landen die Erfolge dieser neuen Technik nicht gebremst? Wahrscheinlich schon..., 2. Das Muster gab es aber bereits vorher, In den Daten sieht man auch, dass es schon einen vorherigen Durchbruch Ende der 50er, Anfang der 60er Jahre bei der Verwendung der ""Straddle""-Technik (Wälzsprung) statt der ""Western Roll""-Technik (Rollstil) gegeben hatte. Das Halten des Körperschwerpunkts unter der Latte war schon hier der Knackpunkt. Nur interessanterweise war die Steigung der Rekorde mit der Wälzsprungtechnik in dieser Zeitperiode deutlich steiler als später in der Zeit des ""Fosbury Flop"". Warum?, 3. Die 2,00-m-Marke, Dieser Rekord wurde von Männern bereits ab 1914 gebrochen. Bei den Frauen dauerte es etwas länger, sodass die Höchstmarke erst ab 1977 überboten wurde. Deshalb dachte ich, es wäre gerechter, sich die durch eine durchschnittliche Körpergröße skalierten Höhen anzuschauen (siehe Plot rechts in der Abbildung). Bei Frauen wären es ca. 1,60m und bei Männern 1,74m. Diese Durchschnitte sind aber sehr grob geschätzt und es können andere wichtige Faktoren, wie z.B. Anteil der Muskelmasse und Mitochondrien in der Muskelzelle, eine Rolle spielen. Deshalb ist der Vergleich hierbei nicht ganz präzise und vollständig., 4. Geteilte Rekorde, In der Geschichte des Hochsprungs kam es nur viermal dazu, dass sich zwei Männer den Rekord teilten. Bei den Frauen passierte es sogar zehnmal und dabei gab es bis zu drei Athletinnen, die sich eine Bestmarke teilten (siehe oben rechts im linken Plot). Insgesamt also mehr als doppelt so häufig wie bei den Männern, was ich als Zeichen eines ausgeglichenen Wettbewerbs werte. Das macht das Zuschauen spannender und ist übrigens heutzutage ein ganz wichtiger Punkt in Bezug auf die Popularität von Spielen und Wettkämpfen., 5. Keine neuen Rekorde ab den 90er Jahren, Bei den Frauen gibt es ab 1987 keine neuen Weltrekorde, bei den Männern ab 1993. Warum ist das so? Trotz aller Weiterentwicklungen von Trainingsmethoden, Professionalisierung des Sports, Ernährung für Sportler, Sportmaterialien usw. ... Bringen all diese Maßnahmen tatsächlich etwas? Ist es Zeit für die Erfindung einer neuen Sprungtechnik? Sind wir hier auf eine menschliche Grenze gestoßen? Oder sind die Rekorde aus den 80er und 90er Jahren unter die Lupe zu nehmen?, All diese Gedanken habe ich mir gemacht, weil ich ein paar Plots von Weltrekorden beim Hochspringen mit Hilfe von R gesehen habe, nachdem ich mit der Behauptung im TV-Werbespot über die Revolution in dieser Sportart nicht ganz zufrieden war., Und hier unten füge ich meinen R-Code bei:, , , © 2021 b.telligent";https://www.btelligent.com/blog/hochleistungsdenksport-mit-r/;B-Telligent;Dr. Michael Allgöwer
17.09.2015;            Bridging Worlds – Omnichannel-Handel im digitalen Zeitalter        ;, © 2021 b.telligent;https://www.btelligent.com/blog/omnichannel-handel/;B-Telligent;Mary Heinze
19.08.2015;            Customer Journey Analytics im E-Commerce mit Markov-Ketten        ;Ein Großteil der deutschen Wohnbevölkerung nutzt das Internet, um sich über Produkte zu informieren und diese auch zu kaufen. So wurden 2014 allein in Deutschland ca. 42 Mrd. Euro im Onlinehandel umgesetzt. Kundengewinnung und -bindung durch das Internet werden daher zu strategischen Aspekten der Unternehmensführung. Ich befasste mich 2013/14 am Lehrstuhl für Statistik und Ökonometrie der Universität Erlangen-Nürnberg in Kooperation mit der GfK SE Marketing Science mit der Modellierung des Internetnutzungsverhaltens im Reisemarkt. Die daraus resultierende Abschlussarbeit wurde im Frühjahr vom Bundesverband der deutschen Marktforschung als beste Arbeit des Jahres ausgezeichnet. Demnach ermöglicht ein besseres Verständnis der Internetnutzer zum Beispiel eine tiefer gehende Personalisierung von Webseiten oder auch eine zielgerichtete Aussteuerung von Werbeinhalten. Im wissenschaftlichen Interesse stehen vor allem die eigentliche Struktur des Nutzerverhaltens und die dahinterliegenden Determinanten., Die Arbeit nutzt einen bisher in diesem Kontext noch nicht verwendeten Datenpool der GfK SE, der Webseitenkontakte innerhalb des Reisemarktes protokolliert und mit einer großen Menge an Zusatzinformationen (Buchungs- und Individualcharakteristika) anreichert. Hierbei kommen Daten aus Beobachtungs- (GfK Media Efficiency) und Befragungsquellen (GfK TravelScope) zum Einsatz. In das Modell gehen nicht nur die erwähnten Charakteristika mit ein, sondern auch Recherchen von Kunden, die schließlich ein Produkt offline gekauft haben., Nach einer umfassenden Datenvorbereitung wird eine Markov-Kette modelliert: Ansatzpunkt zur Modellierung von Heterogenität ist die Übergangswahrscheinlichkeit, die hier für jede Etappe der Journey pro User separat betrachtet wird. Durch den Einsatz von Variablenselektionsverfahren können Variablen mit einem hohen potenziellen Beitrag zur Erklärung der Heterogenität im Modell identifiziert werden und die große Menge an potentiell zu schätzenden Koeffizienten lässt sich reduzieren. Es werden Stepwise-Regression und Lasso-Schätzungen eingesetzt. Die Auswertung der Kreuzvalidierungsstudien zeigt jedoch, dass sich in diesem Kontext die Lasso-Schätzung besser zur Auswahl von signifikanten Kovariaten eignet., , Es lassen sich beispielsweise folgende Erkenntnisse festhalten:, Die gewonnenen Erkenntnisse bringen wichtige Implikationen für die Praxis der Marktforschung und der Werbetreibenden mit sich. So werden Hinweise für den optimalen Einsatz von Webseiten-Personalisierungs- und Cross-Selling-Maßnahmen gegeben. Darüber hinaus werden Grundlagen für die weitere Forschung und vor allem die Datenaufbereitung sowie -vorverarbeitung im Umfeld der Customer Journey geschaffen., , © 2021 b.telligent;https://www.btelligent.com/blog/customer-journey-analytics/;B-Telligent;Dr. Michael Allgöwer
19.08.2015;            HOWTO: Große Dateien verarbeiten mit Standard-Python        ;"Häufig werde ich mit bereitgestellten Rohdaten für Analysen konfrontiert, welche sich unkomprimiert durchaus auf Dateien von einem halben Gigabyte oder mehr erstrecken. Ab einem Gigabyte kommen die Desktop-gestützten Statistik-Tools langsam ins Schwitzen. Es gibt natürlich je nach Tool Möglichkeiten, nur einen Teil der Spalten zu selektieren oder nur die ersten 10.000 Zeilen zu laden usw., Aber was macht man, wenn man aus der Datenlieferung nur eine zufällige Stichprobe ziehen möchte? Man darf sich nie darauf verlassen, dass die Datei zufällig sortiert ist. Sie kann durch Prozesse im Datenbankexport bereits systematische Reihenfolgeeffekte beinhalten. Es kann aber auch vorkommen, dass man z.B. nur ein Zehntel einer Gruppierung analysieren möchte, wie etwa die Einkäufe jedes zehnten Kunden. Dazu muss die komplette Datei gelesen werden, sonst kann man nie sicherstellen, dass alle Einkäufe der gefilterten Kunden berücksichtigt wurden., Nun, das Öffnen solcher Dateien unter Windows mit diversen Text-Editoren oder gar Excel ist selten von richtigem Erfolg gekrönt, da die komplette Datei erstmal in das Tool geladen werden muss. Das Programm verweigert dabei irgendwann den Dienst, schneidet die Datei ab oder braucht zumindest sehr lange., Ich habe es zuletzt mal wieder mit einem kurzen Python-Script gelöst. Nehmen wir mal den Versuch, die Datei einfach stumpf in 3 große Partitionen aufzuspalten., Zunächst müssen wir die Datei öffnen. Das passiert über einen sogenannten Filehandle, mit dem wir die Datei ansprechen können. Die simple Herangehensweise, die häufig auch funktioniert, ist, den Inhalt zu lesen, ihn in Zeilen zu trennen, diese aufzuteilen und ihn dann wieder zu speichern. Etwa so:, Jetzt haben wir erstmal eine Liste mit allen Zeilen und drei leere Listen für die Zieldateien. Ich nehme mal die einfache Herangehensweise und teile die Zeilen gleichmäßig auf die neuen Dateien auf:, , Der Code ist jedoch extrem holzig. Zunächst sollten wir das ""with""-Statement verwenden, dann müssen wir die Dateien nicht mehr explizit schließen. Dieses Literal bewirkt, dass bei getaner Arbeit ein Aufräumprozess gestartet wird, vor allem auch, wenn ein Fehler auftritt im With-Bereich. Im Falle der Filehandles gehört zum Aufräumen das Schließen der Dateien. Das schützt davor, dass Prozesse abbrechen, aber die Dateien trotzdem als ""in Arbeit"" gesperrt sind. Viel hässlicher ist noch, dass wir mit read() die komplette Datei in den Arbeitsspeicher lesen! Die Testdatei für dieses Script hatte 650MB und der Arbeitsspeicher wurde mit etwa 1,6 Gigabyte belastet!! Hier kann man auch einfach die Datei Zeile für Zeile lesen und verarbeiten! Nächster Anlauf:, Das sieht schon besser aus, ist insgesamt kürzer und läuft schmäler im Arbeitsspeicher, braucht aber immer noch knapp einen Gigabyte. Kein Wunder, in den Listen newFile01, newFile02 und newFile03 steckt ja wieder die gesamte Datei für einen gewissen Zeitraum. Das geht noch besser, indem wir direkt in die Dateien schreiben. Es ist übrigens ein oft übersehenes Feature der Print-Funktion in Python3, dass man das Ausgabeziel wählen kann. Per default ist es immer die console, es kann aber auch eine Datei sein., So, das ganze ist noch kürzer und überstieg im Lauf nie 16MB im Arbeitsspeicher! D.h. hier kann man auch bedenkenlos Dateien verarbeiten, welche den Arbeitsspeicher schnell sprengen würden! Ich hätte hier jetzt keine Angst, Files mit hunderten von Gigabyte zu verarbeiten., Jetzt haben wir immer noch nicht das Szenario gelöst, wenn wir z.B. nach Kundennummern trennen wollen, das behandle ich vielleicht in einem separaten Post. Genauso das Problem, dass der Datenheader jetzt nur in der ersten Datei zu finden ist. PS: Es geht übrigens noch kürzer, aber dann ist wohl Schluss:, , , © 2021 b.telligent";https://www.btelligent.com/blog/dateien-verarbeiten-mit-standard-python/;B-Telligent;Stefan Seltmann
17.08.2015;            QuickSteps mit arcplan 8.5        ;"Diese Schlüssel finden wir in dem Objekt ""QS PlanIst - Istwerte"" wieder. In dem Formatierungsobjekt der Grafik wird nun die Kategorie unter Verweis auf dieses Objekt dynamisch formatiert., , Beispielsweise benötigen wir für die Verwendung der Plan-Ist-QuickStep-Grafik in einem neuen Dokument eine Reihe von Plan- und Istwerten als Datenbasis., , Nun wird der QuickStep ausgewählt ..., , ... und die Grafik an der gewünschten Stelle aufgezogen, und die Objekte werden mit den Basisdaten im Dialog angegeben., , Alle benötigten Objekte werden automatisch auf Ebene 3 angelegt., , , © 2021 b.telligent";https://www.btelligent.com/blog/quicksteps-mit-arcplan-85/;B-Telligent;Stefan Kersten
24.07.2015;            Mobile Customer Engagement - Teil 2        ;, © 2021 b.telligent;https://www.btelligent.com/blog/mobile-customer-engagement-teil-2/;B-Telligent;Laurentius Malter
22.06.2015;            Uplift-Modeling als Zusatz zum klassischen Response-Modeling        ;"Uplift-Modeling kann Kampagnenmanager bei der Kampagnensteuerung und -planung unterstützen, da es das klassische Response-Modell des Kampagnenscorings ergänzt., Uplift-Modeling geht von der Grundidee aus, dass man die Kampagnenreagierer in zwei Gruppen einteilt: diejenigen, die auch ohne die Kampagne reagiert hätten, und die, die das ohne die Kampagne nicht getan hätten. Während klassisches Scoring unterschiedslos auf beide Gruppen abzielt, versucht das Uplift-Scoring, exklusiv nur die zweite Gruppe zu isolieren und dabei die erste möglichst zu ignorieren. Zu diesem Zweck werden die Responseinformationen aus der Kontrollgruppe verwendet, die im klassischen Kampagnenscoring ungenutzt bleiben., Beispielhaft zeigt sich das in der Grafik, die den Vergleich zweier Kundengruppen darstellt (hier KMU- vs. private Kunden) und deren Reaktion auf eine Kampagne, z.B. ein Mailing. Während das Mailing bei den KMU-Kunden nur zu einem geringen Anstieg der Take-Rate geführt hat, funktioniert es bei den Privatkunden sehr viel besser., Genau darum geht es: gute Prädiktoren, nicht nur für die Prognose, sondern exklusiv für den Uplift zu finden!, , , Anwenden lässt sich das Modell in vielen Bereichen der Kampagnensteuerung, sei das Ziel nun Retention, Prevention oder Cross- bzw. Upsell., Es gibt für das Uplift-Modeling drei Ansätze: Der erste modelliert die Responses in beiden Gruppen unabhängig voneinander und bildet anschließend Differenzen der Reaktionswahrscheinlichkeiten.Der Uplift wird also nur indirekt modelliert und so wird folglich auch nicht nach ihm optimiert. Ergebnisse sind eher zufällig, da nicht garantiert ist, dass in beiden Modellen die gleichen Variablen selektiert werden. Der Ansatz ist also wenig ernst zu nehmen., Alle anderen Ansätze bauen daher auf einem einheitlichen Datensatz auf, der beide Gruppen umfasst. Die Gruppenzugehörigkeit wird durch eine Indikatorvariable (0/1) modelliert., Der zweite Ansatz benutztEntscheidungsbäume mit einem modifizierten Splittingkriterium. Dieses Kriterium misst dieGüte einer potentiellen Splitvariablen an der Unterschiedlichkeit der Verteilungen der Responsevariablen zwischen Test- und Kontrollgruppe. Die Unterschiedlichkeit wird dabei mit Hilfe einer informationstheoretischen Maßzahl gemessen, der Kullback-Leibler-Divergenz. Auch dieser Ansatz einer entscheidungsbaumbasierten Modellierung hat so, wie meist implementiert, seine Tücken: So geht dieser implizit davon aus, dass sich das Splittingkriterium bei einer Größe der Kontrollgruppe von null zu einem normalen Splittingkriterium, wie bei einer klassischen Response-Modellierung, reduziert. Formal bewiesen ist dies nicht, der Ansatz liefert jedoch in der Praxis brauchbare Ergebnisse.                                          , Der dritte Ansatz benutzt logistische Regression und zielt dann auf die Interaktionseffekte der Responsevariablen mit der Indikatorvariablen für die Zugehörigkeit zur Testgruppe. Besonderer Wert wird auf die Variablenselektion gelegt: Hier wird meist die Responsevariable ersetzt durch eine besondere Verknüpfung zwischen Responsevariable und Gruppenzugehörigkeit. Diese modifizierte Zielvariable ist 1, wenn eine Response stattgefunden hat und der Datensatz zur Testgruppe gehört oder wenn keine Response stattgefunden hat und der Datensatz zur Kontrollgruppe gehört; in allen anderen Fällen ist sie 0. Dies hat den Vorteil, dass man trotz realistischer Conversion- oder Take-Rates im Bereich von 1-3% eine je nach Kontrollgruppengröße gleichmäßigere Verteilung der Zielvariablen erreicht. Hier kommen nun gängige Variablenselektionsmethoden (Wrapper- oder Embedded-Verfahren) zum Einsatz. Das eigentliche Modell wird dann mithilfe der ursprünglich kodierten Zielvariablen geschätzt. Dies besteht aus zwei Teilen:, , Uplift-Modeling kann in einer Kampagnensteuerung jedoch definitiv Mehrwert generieren, denn es zeigt den ""inkrementellen Unterschied"", den eine Kampagne gemacht hat, und versucht dessen Herkunft zu erklären., Dieser Ansatz sollte jedoch nur zusätzlich zu im Kampagnenkontext bekannten Instrumenten genutzt werden, um weitere Informationen zur Selektion und Steuerung zu gewinnen., , © 2021 b.telligent";https://www.btelligent.com/blog/uplift-modeling/;B-Telligent;Stefan Seltmann
10.06.2015;            Anleitung: HICHERT (IBCS) out of the Box        ;Als Nächstes wählt man die passende Grafik aus den Quickstep-Vorlagen aus, ...,  ... zieht die Grafik wie gewohnt an der gewünschten Position auf und füllt per Klick die Grafikparameter:, ,  Damit ist die Grafik schon angelegt. Alle notwendigen Objekte liegen automatisch auf Ebene 3 und können dort auch weiterbearbeitet werden., , Zentrale Formatierungen lassen sich im Administrationsfenster mit einem Klick auf das Symbol vornehmen., In dem Fenster können zentral die Eigenschaften (und Schlüssel) für Nulllinie, Balken, Farben, Muster etc. gepflegt, verändert oder erweitert werden. Es lassen sich auch neue, eigene Regeln für Formatierungen hinzufügen., , Als Ergebnis erhält man nun nach wenigen Klicks eine IBCS-konforme Grafik.,  Selbstverständlich lässt sich der Bericht (die Applikation) ganz normal weiterentwickeln und ausbauen. Wie man eigene Quicksteps (Grafiken) erstellt, erfahren Sie in unserem nächsten Blogbeitrag!, , © 2021 b.telligent;https://www.btelligent.com/blog/anleitung-hichert-ibcs-out-of-the-box/;B-Telligent;Stefan Kersten
27.05.2015;            Mobile Customer Engagement - Teil 1        ;, © 2021 b.telligent;https://www.btelligent.com/blog/mobile-customer-engagement-teil-1/;B-Telligent;Laurentius Malter
19.05.2015;            Howto: Transaktionssichere Eingaben        ;Die Umsetzung erfolgt in drei Schritten:, Zunächst wird eine Locktabelle im SQL-Server erstellt. Hier soll zum Zeitpunkt der Eingabe ein Datensatz geschrieben werden, der die Person, den Zeitpunkt, die Filterkombination und die Information über die Session enthält. Durch die Existenz eines solchen Datensatzes wird eine Sperre (Lock) markiert. Sobald die Eingabe beendet wird, wird der Datensatz wieder gelöscht., Sofern Bedarf an einem Logging von Eingaben besteht oder eine solche Funktion vorhanden ist, sollte geprüft werden, ob die beiden Funktionen kombiniert werden können., Die Tabelle hat in unserem Szenario folgende Spalten:, Notwendig in der Tabelle sind die Filterwerte, die spid und die login_time. Die weiteren Spalten werden für Informationszwecke bzw. für Ausgabetextegenutzt., Die Tabelle ist zunächst und auch zu den meisten Zeitpunkten leer. Die Datensätze werden dann beim Öffnen der Kommentareingabe geschrieben., , In dem beschriebenen Szenario wurde gezeigt, wie man eine transaktionssichere Eingabe gestalten kann. Ziel war es, die Überlegungen hierzu sowie eine grundsätzliche Darstellung der Funktionalität aufzuzeigen., Konkret wurde viel mit SQL-Server-Funktionalitäten gearbeitet. Dies ist allerdings nicht zwingend notwendig. Die meisten Funktionen können auch innerhalb von arcplan oder auch in anderen Datenbanken umgesetzt werden. Der Aufbau wäre dann aber relativ ähnlich., , © 2021 b.telligent;https://www.btelligent.com/blog/howto-transaktionssichere-eingaben-in-arcplan-gestalten/;B-Telligent;Arno Velden
13.05.2015;            R Tipps und Tricks - Teil 1        ;"Vor etwa drei Jahren bin ich von kommerziellen Statistiklösungen, wie SPSS, auf R umgestiegen. Mittlerweile kann ich mit Überzeugung sagen, dass ich erstmal kein anderes Tool mehr für Advanced Analytics brauche. Vor allem in Verbindung mit der IDE ""R-Studio""hat die Software einen Reifegrad erreicht, um sie bedenkenlos in großen Data-Science-Projekten einzusetzen., Man braucht sich allerdings nicht vormachen, dass man R einfach installiert und loslegt. Die Lernkurve ist vergleichsweise steil und es gibt nicht nur in Bezug auf die verschiedenen Pakete viele unterschiedliche Wege, dasselbe zu tun. Nicht selten hab ich mich geärgert, dass ich mitten im Auswerten plötzlich über einen banalen Schritt gestolpert bin, dessen Umsetzung ich für R erst recherchieren musste. Ich möchte daher in diesem und hoffentlich vielen folgenden Teilen Tipps und Tricks für R aufgreifen, die ich gerne schon früher als Einsteiger gekannt hätte., Fangen wir mit etwas Banalem an, der Spaltenselektion in Dataframes. Sie wird natürlich in allen Einführungswerken behandelt, aber ich war ehrlich gesagt nicht immer von den Erläuterungen überzeugt. R unterscheidet drei Schreibweisen, um eine Spalte zu selektieren. Tatsächlich sind es nur zwei, nämlich..., Das hat mich zu Beginn immer etwas verwirrt. Die Schreibweise data$spaltenname ist natürlich die klar bevorzugte, da man nicht mit lästigen Klammern und Anführungszeichen hantieren muss. Noch wichtiger ist die Autovervollständigung unter R-Studio, wenn man STRG+SPACE drückt. Diese funktioniert halt nur so., Nun habe ich ständig die Situation, in der ich den Namen einer Spalte als String in einem Skalar vorliegen habe, z.B. wenn ich eine Schleife über eine Liste von Variablen laufen lasse. Dort hab ich dann als Anfängerfehler häufig die Schreibweise dataspaltenNamenString statt dataspaltenNamenString gewählt, weil ich es aus anderen Programmiersprachen einfach so gewohnt bin. Manchmal funktioniert es trotzdem auf wundersame Weise, meistens aber wirft irgendetwas einen Fehler., Die Variante mit doppelten Klammern ist technisch identisch mit der Variante mit $. Die Variante mit einfachen Klammern dagegen ist nicht wirklich ein Weg, um eine Spalte aus einem Dataframe zu extrahieren. In Tutorials und einem Buch hab ich die Erläuterung gefunden, dass damit ein neues Dataframe mit der genannten Spalte darin zurückgegeben wird. Das ist nur insofern richtig, als es nicht notwendigerweise ein Dataframe sein muss. Es kann auch irgendein anderer Datencontainer sein, der diese Schreibweise unterstützt., Aber wozu eigentlich? Warum sollte ich mit data'spaltenName' eine Kopie des ursprünglichen Dataframes mit nur einer Spalte ziehen? In dieser Form ist das Literal tatsächlich eher nutzlos, das liegt aber daran, dass es sich um einen Spezialfall eines anderen Anwendungsfalles handelt., R bietet ja die Möglichkeit, ein Spaltensubset eines Dataframes zu selektieren, eben mit dieser Schreibweise, z.B. datac('spalte_A', 'spalte_B', 'spalte_C'), oder auch nur über die Indizes, z.B. datac(1,2,89,99) oder data1:10., Nun ist R so gestrickt, dass es keine eigentlichen Variablen in dem Sinne eines typisierten singulären Wertes gibt. Es gibt stattdessen Skalare, also Vektoren mit Elementen des gleichen Typs. Wenn man spaltenName&lt;-'spalte_a' schreibt, dann legt man eben einen Character-Vektor der Länge 1 an, gleichbedeutend mit spaltenName&lt;-c('spalte_a')., Somit wird das Resultat von data'spaltenName' verständlicher, da es datac('spaltenName') entspricht., Früher habe ich Kreuztabellen immer so formuliert, da ich dann die Autovervollständigung in R-Studio beim Tippen nutzen konnte: table(datensatz$spalteA~datensatz$spalteB). Ich hab mich dann immer geärgert, dass ich am Output jedes Mal überlegen musste, was denn nun in den Spalten und was in den Zeilen gezählt wird, da die Beschriftung fehlte. Mit dem Wissen von oben ist es eigentlich klar, warum. Die Daten aus Spalte A und Spalte B werden aus dem Datensatz als Vektoren extrahiert, die an sich keine Namen haben., Wenn ich die Kreuztabelle aber so schreibe: table(datensatzc('spalteA', 'spalteB')), dann übergebe ich nicht zwei anonyme Vektoren, sondern einen data.frame mit zwei Spalten, die ihre Namen behalten. Dann klappt es auch mit der Beschriftung., Abschließend möchte ich in diesem Zusammenhang noch eine Best-Practice-Empfehlung geben, und zwar: Die Schreibweisen mit der direkten Angabe von Indizes, wie data1:10, sollte man besser gleich wieder vergessen, sie sind zu fehleranfällig und unleserlich. Ich definiere mittlerweile über verschiedene Wege immer einen Vektor mit den Spaltennamen, z.B. predictors_relevant = c('Predictor01','Predictor03','Predictor05','Predictor09') und wähle im Anschluss nur über diesen Vektor aus, z.B. datapredictors_relevant. Das macht den Code dynamischer und ich weiß bei sprechenden Titeln sofort, was ich eigentlich gerade auswähle., Genauso halte ich nicht viel von der Minus-Schreibweise, um Spalten auszuschließen. Ich verwende stattdessen setdiff, dann sind die beteiligten Spalten alle im Klartext leserlich, z.B. wenn ich alle Prädiktoren außer 'predictor03' verwenden will: Datasetdiff(predictors_relevant, c('Predictor03'))., , © 2021 b.telligent";https://www.btelligent.com/blog/r-tipps-und-tricks-teil-1/;B-Telligent;Stefan Seltmann
08.04.2015;            Kampagnenmanagement im Mobile-Engagement-Umfeld – Schaffung von Kampagnenintelligenz im Mobile-App-Kanal        ;, © 2021 b.telligent;https://www.btelligent.com/blog/kampagnenmanagement-im-mobile-engagement-umfeld/;B-Telligent;Laurentius Malter
31.03.2015;            IMHO, vergesst die Handlungsrelevanzmatrizen        ;"Vor einiger Zeit war ich in einem Projekt erneut mit Kundenzufriedenheitsbefragungen konfrontiert. Die ""Kuzu"" und ich, ... ja, man kann sagen, dass wir alte Freunde sind. Meine erste ehrliche Anstellung vor zehn Jahren war als Projektmanager in einem Marktforschungsinstitut, wo ich quasi nichts anderes gemacht habe, als den Kunden die vielen Variationen von ""wie zufrieden sind Sie mit ... gemessen auf einer Skala von ..."" entlocken zu lassen., Kuzu-Befragungen erfreuen sich natürlich nach wie vor höchster Beliebtheit, obwohl ich ihren Nutzen in ihrer gewöhnlich ausgeführten Form als mittlerweile sehr beschränkt erachte. Ein Chef in einer späteren Anstellung propagierte völlig richtig, dass man stattdessen Unzufriedenheitsbefragungen machen müsse. Mal ehrlich, wer weiß, welche Maßnahmen wirklich zu ergreifen sind, wenn sich z.B. der Mittelwert der ""Zufriedenheit mit der persönlichen Betreuung"" von 1,5 auf 1,7 verschlechtert? Ich weiß es zumindest nicht, ... dafür weiß ich, dass die Zielvereinbarungen vieler bemitleidenswerter Angestellter an so etwas gekoppelt sind, aber das alles ist ein anderes Thema., Was bei der Wiederbegegnung mit einer Kuzu meinen Unmut erregte, war ein Summary-Chart, das ich als ""Handlungsrelevanzmatrix"" kennengelernt habe. Es erinnert so ein bisschen an die vier Boston-Consulting-Quadranten. Zunächst gibt es eine x-Achse, die die Bedeutung verschiedener Zufriedenheitsmerkmale aufzeigt. Dazu gekreuzt, wird die Ausprägung der Merkmale auf der y-Achse dargestellt., , Eigentlich ganz toll. Es wird mir mit dem Chart suggeriert, ich könne sofort bewerten, was die wichtigsten Treiber für eine Globalzufriedenheit oder Weiterempfehlung sind und vor allem wo die größten Versäumnisse dabei liegen. Ich gestehe, dass ich das auch dachte, als ich diese Darstellung das erste Mal sah. Nachdem ich dutzende bis hunderte davon erstellt und in schicken Präsentationen verbaut hatte, bröckelte die Begeisterung ab. Die meisten dieser Charts sind sehr ähnlich. Im Grunde reichen mir als Input die Branche und die Auswahl der Zufriedenheitsfaktoren und dann kann ich wohl Teile des Charts bereits erraten., Ein grundlegender Fehler liegt darin, die Bedeutung der Merkmale über eine Korrelation zu ermitteln. Meist wird per Voreinstellung eine Produkt-Moment-Korrelation verwendet. Das ist ja nichts Ausgefallenes, sondern die ganz normale Pearson-Korrelation, die zum Einsatz kommt, wenn ich in einem Tool einfach auf ""Korrelation"" drücke., Dies ist in der Tat eine vortreffliche Wahl, wenn die Skalen der Merkmale alle äquidistant sind, d.h. der Abstand von Bewertungsstufe zu Stufe immer der gleiche ist. Das wird natürlich einfach angenommen, sonst könnte man ja auch keinen Mittelwert berechnen und dann wäre man definitiv ein Spielverderber. Gewiss, die Verzerrung ist meist vernachlässigbar, wie einige wissenschaftliche Arbeiten durch Simulationen bewiesen haben., Eine weitere Voraussetzung wäre eine lineare Beziehung zwischen den Merkmalen. Die Pearson-Korrelation reagiert durchaus robust, falls so etwas nicht ganz wie geplant eintritt., Sobald ich aber Zufriedenheitsmerkmale in sogenannte Begeisterungs- und Hygienefaktoren einteilen kann (siehe Kano-Modell oder Herzbergs Zwei-Faktoren-Theorie), ist die Ausnahme die Regel. Die Zusammenhänge sind dann unterschiedlich stark, je nachdem, in welchem Wertebereich ich mich bewege. Hygienefaktoren wirken stärker durch Unzufriedenheit, Begeisterungsfaktoren durch Höchstnoten., Das alles führt nicht zwangsweise zu einem falschen Ergebnis. Auch wenn die Rangkorrelation die bessere Wahl wäre, wird das Resultat damit nicht viel anders aussehen., Das massivere Problem liegt im Konzept der Korrelation allgemein. Sie ist nämlich abhängig von der Spannweite der Zufriedenheitswerte und deren Begrenzungen. Wenn ich Zufriedenheit und Weiterempfehlung auf einer 5er-Skala messe, dann ist nun mal bei 5 Schluss, und wenn die Unternehmen nicht alles falsch gemacht haben, dann sind bei den eigenen Kunden wohl mehr als die Hälfte der Urteile in den beiden Topkategorien zu erwarten. Wenn man dazu noch ein bisschen Rauschen, Erfassungsfehler und individuelles Antwortverhalten unterrührt, kann man da keine überwältigenden Korrelationen erwarten., Eine Korrelation mit stetigen Daten braucht jedoch eine gewisse Varianz der Werte. Ein Merkmal, wie Zufriedenheit mit Preis/Leistung, korreliert meist recht hoch mit einer Globalzufriedenheit, weil es in der Tat ein wichtiges Merkmal ist, aber auch, weil es oft vergleichsweise schlecht bewertet wird und eine breitere Verteilung der Werte erhält., Wie bedeutend ein Merkmal die Globalzufriedenheit im Allgemeinen beeinflusst, kann ich umso besser messen, je mehr Kunden damit unzufrieden sind. Auch möglich sind Bereiche mit gravierender Konsequenz, aber geringer Korrelation, weil sie zu selten auftreten. Angenommen, wir würden Stromkunden befragen, dann wäre in der Handlungsrelevanzmatrix ein Merkmal namens ""Versorgungsstabilität mit Strom"" in unserer Ersten Welt top bewertet und die Relevanz wäre sehr niedrig. Sobald wir aber für einen sauberen A-B-Test kurz vor der Befragung ganze Kundengruppen mit gezielten Stromausfällen beglücken, sieht unsere Matrix deutlich anders aus. Kunden, deren Beschwerden nicht ernst genommen werden oder deren Schaden bei einer Versicherung nicht reguliert wird, sind extrem unzufrieden, aber kommen in Befragungen meist so selten vor, dass ihnen in einer Handlungsrelevanzmatrix nicht ausreichend Rechnung getragen wird., Zusammenfassend: Die Handlungsrelevanzmatrix erklärt uns die Zusammensetzung einer Globalzufriedenheit in den konkreten Befragungsdaten, aber sie kann nicht bedingungslos auf die tatsächliche Bedeutung der Merkmale jenseits der Befragung generalisiert werden. Sie stellt das Zusammenspiel zwischen einer Teilzufriedenheit und der Globalzufriedenheit nur anhand eines Punktes dar, obwohl der Wirkmechanismus meist komplexer ist. Sie stützt sich nicht wie gewöhnlich auf explizite Aussagen der Bedeutung von Merkmalen, sondern leitet sie fehlerbehaftet indirekt über Korrelationen ab., Nun darf ich natürlich nicht alles schlechtreden und kritisieren, ohne Alternativen zu bieten. Nun, der Weisheit letzten Schluss habe ich selbst noch nicht gefunden. Ich löse das Dilemma gerne mit dem Ansatz der Penalty Reward Contrast Analysis. Diese untersucht die Wirkung der Extremsturteile, positiv wie negativ. Damit kann ich Merkmale mehrdimensional vergleichen, nämlich inwieweit sie zur Verärgerung oder auch zur Begeisterung beitragen., , © 2021 b.telligent";https://www.btelligent.com/blog/imho-vergesst-die-handlungsrelevanzmatrizen/;B-Telligent;Stefan Seltmann
17.03.2015;            Der Customer Lifetime Value: Populäre Irrtümer und ungeschminkte Wahrheiten        ;"Lange Zeit war der Customer Lifetime Value in wissenschaftlichen Veröffentlichungen wesentlich häufiger anzutreffen als in der Realität. Doch mittlerweile findet er seinen Weg in die praktische Anwendung. Triebfeder ist vor allem die Digitalwirtschaft. Die zunehmende Verbreitung des Customer Lifetime Values eröffnet große Chancen für eine gezielte Akquisition und Steuerung von Kundenbeziehungen. Sie legt aber auch die Tatsache offen, dass gewisse Missverständnisse weit verbreitet sind., Eher nicht. Der Customer Lifetime Value ist nicht der große Vereinfacher, als der er gelegentlich willkommen geheißen wird. Er summiert die zu erwartenden Umsätze mit einem Kunden über einen gewissen Zeitraum, der je nach Anwendung und Branche von einigen Wochen bis zu mehreren Jahren reichen kann. Er leistet daher gute Dienste, wenn man Vertriebskanäle beurteilen oder den langfristigen Erfolg von Marketingmaßnahmen messen will. Klassische Marketingkennzahlen (Customer Acquisition Cost, Responseraten etc.) wird er dabei allenfalls modifizieren, aber nicht verdrängen. Der Customer Lifetime Value lädt aufgrund seines Namens geradezu zu Assoziationen über verschiedenste Vorstellungen von Kundenwert ein. Wer mit dem Customer Lifetime Value effektiv arbeiten möchte, darf ihn jedoch nicht als Projektionsfläche für die eigenen Vorstellungen vom Kundenwert benutzen. Wie bei jeder anderen Kennzahl auch ist es nicht der Name, der die Interpretationsmöglichkeiten festlegt, sondern die Definition. Und die macht sehr klar, dass es bei dieser Kennzahl um die Umsatzerwartungen geht, die ich an einen Kunden haben darf (oder, in manchen Varianten, um die Erwartungen an die Marge)., Das wäre ein gutes Beispiel dafür, dass man auch auf Basis der richtigen Kennzahl die falschen Entscheidungen treffen kann. Wenn eine Kundengruppe einen hohen Customer Lifetime Value hat, so ist das sicher erfreulich. Es sagt allerdings nichts darüber aus, ob sich dieser mit Hilfe von Marketingmaßnahmen weiter steigern lässt. Und das sind die zentralen Fragen einer Direktmarketingstrategie, die sich auf den Customer Lifetime Value stützt: Wie lässt sich der Customer Lifetime Value steigern? Mit welchen Maßnahmen? Bei welcher Kundengruppe? Beantworten lassen sie sich nur durch Experimente. Genauer gesagt: durch Testkampagnen und eine kontrollgruppengestützte Erfolgsmessung. Bei dieser Methodik, die ganz ähnlich auch in klinischen Medikamententests angewandt wird, vergleicht man die Entwicklung einer Kennzahl (zum Beispiel des Customer Lifetime Values) zwischen einer Testgruppe (die in den Genuss einer Marketingmaßnahme gekommen ist) und einer Kontrollgruppe (die diese Maßnahme nicht bekommen hat, aber identisch zusammengesetzt ist wie die Testgruppe). Das ist der einzige Weg, sichere Aussagen über den Erfolg von Marketingmaßnahmen zu generieren. Die richtige organisatorisch-technische Umsetzung von Kontrollgruppen ist ein Thema für sich, das als Basis für ein erfolgreiches datengetriebenes Direktmarketing unerlässlich ist. Noch anspruchsvoller wird es, wenn man auch dort eine zuverlässige Erfolgsmessung installieren will, wo es unmöglich ist, mit Kontrollgruppen zu arbeiten, wie etwa bei den meisten Online-Kampagnen; in solchen Fällen arbeitet man idealerweise mit Ansätzen aus der Bayes-Statistik. Man braucht also Voraussetzungen, um einen Customer Lifetime Value wirklich optimal nutzen zu können. Das bringt uns auch gleich zum letzten Irrtum:, Man kann natürlich einen Customer Lifetime Value auch gleich im ersten Schritt einführen. Allerdings muss man dann damit leben, dass die Anwendungsmöglichkeiten sich auf die Vertriebskanalsteuerung beschränken. Das ist in vielen Geschäftsmodellen eine sehr wichtige und profitable Anwendung, die zunächst durchaus genügen mag. Um aber den vollen Nutzen aus einem Customer Lifetime Value zu ziehen, muss man ihn mit einer Kampagnenerfolgsmessung kombinieren, die mit nachweisbar zuverlässigen statistischen Methoden arbeitet. Beim planvollen Aufbau eines datengetriebenen Direktmarketings bietet es sich daher an, mit den Prozessen zur Erfolgsmessung zu beginnen, die bereits für sich genommen einen hohen Wertbeitrag leisten., Der Customer Lifetime Value ist also keine Wunderkennzahl, die das Marketing im Alleingang revolutioniert. Was übrig bleibt, wenn man den überschüssigen Optimismus abzieht, ist ein enorm wertvolles Instrument zur Steuerung von Vertriebskanälen und Marketingmaßnahmen, das bestehende Kennzahlen ergänzt und seinen vollen Wert dann zeigt, wenn man es mit modernen Methoden der Erfolgsmessung kombiniert., , © 2021 b.telligent";https://www.btelligent.com/blog/customer-lifetime-value-irrtuemer-und-wahrheiten/;B-Telligent;Dr. Michael Allgöwer
11.03.2015;            Fehlende Werte in logistischer Regression        ;"Die logistische Regression ist neben Entscheidungsbäumen das Arbeitspferd in der Modellierung, um das Eintreten eines Ereignisses vorherzusagen. Nun sind beide Verfahren zum Glück so ausgelegt, dass man im Grunde jede Art von Prädiktor für die Vorhersage einsetzen kann, egal ob dichotome Kategorien, mehrstufige Kategorien oder stetige Variablen auf Intervallskalenniveau. Speziell die logistische Regression hat allerdings keine Möglichkeit, sinnvoll mit fehlenden Werten umzugehen. In der sozialwissenschaftlichen Forschung oder der Marktforschung behilft man sich häufig damit, Analysen nur auf vollständige Datensätzezu beschränken. Dieses Vorgehen birgt immer die Gefahr, dass man systematisch eine Kundengruppe vernachlässigt. Allein die Tatsache, dass Werte fehlen, kann bereits Aufschluss über einen Kunden oder ein Untersuchungsereignis geben., Im Umgang mit Unternehmensdaten sind die Fallzahlen in der Regel groß genug, um auch die fehlende Werte für ein Scoring zu verwenden. Bei Kategorien bildet man einfach eine neue Kategorie, welche diese Fälle umfasst., Doch wie geht man hier mit Fehlenden bei stetigen Prädiktoren um? Angenommen man möchte einen Churn-Score entwickeln, verwendet das Kundenalter als Prädiktor und bemerkt, dass etwa ein Sechstel der Kunden kein Alter gepflegt haben. Häufig entdeckt man dann auch noch viele weitere Kunden mit fehlerhaften Dateneingaben, die man ebenfalls auf ""fehlend"" setzen muss., Wäre das Alter vollständig, würde man es als stetigen Einflussim Modell berücksichtigen, sodass jedes weitere Jahr das Chancenverhältnis zur Kündigung verändert. Diese Beziehung ist dabei stets linear steigend oder fallend., In der Literatur gibt es eine Vielzahl von Techniken zum Umgang mit fehlenden Daten, die alle ihre Berechtigung und ihr Einsatzgebiet besitzen. Softwarepakete wie SPSS bieten an, fehlende Werte mit dem Median oder dem Mittelwert zu ersetzen. Je nach Anteil Fehlender an der Gesamtheit, setzt dieses Vorgehen viele, vielleicht falsche Annahmen voraus., Ich empfehle klar als Best Practice, stetige Prädiktoren mit fehlenden Werten in der linearen Regression auf eine Anzahl Kategorien umzuschlüsseln. Die Variable Alter aus dem Beispiel würde ich zunächst auf 9 oder 10 gleich große Perzentile aufteilen und die Fehlenden als weitere Kategorie berücksichtigen. Für den Referenzpunkt, also welche Kategorie als Basis zählt, wählt man am besten eine Randkategorie, also höchster oder niedrigster Rand. Wenn tatsächlich eine deutliche stetige Beziehung zwischen dem Alter und der Churn-Wahrscheinlichkeit besteht, dann wird man sie auch anhand der Koeffizienten der 9 bzw. 10 Einzelkategorien entdecken. Wenn sich benachbarte Kategorien nicht unterscheiden, ist es sinnvoll, sie zu einer größeren Kategorie zusammenzufassen. Diese Vorgehen birgt auch den Vorteil, eventuell Zusammenhänge mit Minima und Maxima, also kurvilineare Zusammenhänge, zu entdecken. Es könnte ja sein, dass vor allem junge und alte Kunden Churn-gefährdet sind und die Mitte sich eher durch Stabilität auszeichnet. Unabhängig davon kann man mit diesem Ansatz auch Vorhersagen für die Kunden mit fehlendem Alter treffen. Sollte sich der Koeffizient für diese Kategorie deutlich von den anderen Alterskoeffizienten unterscheiden, deutet es sehr auf einen systematischen Effekt hin. In diesem Fall hat man eine eigene Kundengruppe identifiziert, die man durch andere Methoden unter den Teppich kehren würde., Diese Vorgehensweise ist allerdingsnicht ganz frei von Nachteilen. Abgesehen davon, dass man seine Datensätze mit horrenden Mengen von Dummy-Variablen vollmüllt (heute kein Problem mehr), besteht immer noch die Gefahr von Overfitting. Genauso kann man bei der drastisch erhöhten Menge an Prädiktoren schnell der Alpha-Inflation und damit falschen Zufallsergebnissen auf den Leim gehen.Das Verfahren wählt man am besten nur bei ordentlichen Fallzahlen und man löst sich dabei besser von der kindischen Jagd nach 5%-Signifikanzen. Alles über p von 0.1% ist in meinen Augen dabeiuninteressant., , © 2021 b.telligent";https://www.btelligent.com/blog/fehlende-werte-in-logistischer-regression/;B-Telligent;Stefan Seltmann
06.03.2015;            Neue Funktion: ZUORDNEN        ;"Ein sehr häufiger Anwendungsfall im Reporting ist dasVerknüpfen und Anzeigen von Informationen, die entweder aus unterschiedlichen Tabellen oder sogar aus unterschiedlichen Datenquellen stammen. Häufig ist dazu entweder ein Datenbank-View oder sogar ein Zwischenschritt über ein Datenbank-Layer notwendig, in dem die Daten aus den unterschiedlichen Quellen gemergt werden. arcplan bietet mit der Funktion ""Zuordnen"" eine einfache und schnelle Möglichkeit, Daten im Bericht und in Realtime zu verknüpfen., Für eine Analyse wollen wir aus dem System 1 die Umsätze aus dem letzten Monat abfragen und bekommen dafür eine bestimmte Menge an Datensätzen zurück., , , In einer anderen Datenquelle (System 2) sind alle Stammdaten gespeichert. Diese Liste ist deutlich länger und anders sortiert (alphabetisch)., , , Für ein Vertriebsreporting sollen nun den Lieferanten mit Umsätzen Adressinformationen hinzugefügt und richtig zugeordnet werden:, , , Mit der arcplan Funktion ""zuordnen"" lässt sich das ganz einfach erreichen:, , , © 2021 b.telligent";https://www.btelligent.com/blog/die-arcplan-funktion-zuordnen/;B-Telligent;Stefan Kersten
18.02.2015;            Wandflächenberechnung vektorisiert in R        ;Ich bin vor kurzem umgezogen und wurde von meiner alten Hausverwaltung gebeten, die Wände und die Decken der verlassenen Wohnung zu streichen. Ich habemich sogleichim Internet um Vergleichsangebote von Malerbetriebenbemüht. Dort sollte ich gleich zu Beginn die zu streichende Fläche in Quadratmetern angeben ...mmh, ich hätte natürlich sofort die Grundfläche und die Anzahl Räume angeben können und gehofft, dass die Betriebe eine einfache Hochrechnung für die Angebotserstellung verwenden. Aber direkt die zu streichende Fläche zu ermitteln, schien mir etwas komplexer als eine Schätzung aus dem Stegreif., Früher hätte ich das Problem in einer Excel-Mappe gelöst, aber diesmal wollte ich R als Taschenrechner einsetzen., Zunächst habe ich ein paar Eckdaten als Variablen definiert:, Dann habe ich die Grundfläche der Räume angegeben. Für das Berechnen der Deckenfläche kann ich natürlich die bekannten Quadratmeter des Bodens verwenden, aber für die Wände muss ich für jeden Raum Länge und Breite mit der Deckenhöhe verrechnen. Da ich also Länge und Breite einmal multiplikativ und einmal additiv kombiniere, habe ich mich dafür entschieden, alle Raummaße zunächst als Vektoren zu speichern., Von der später zu errechnenden Gesamtfläche muss ich natürlich noch Türen und Fenster abziehen. Die Türen nehme ich doppelt für beide Seiten und ziehe eine Seite für die Haustüre ab. Die Flächenberechnung erfolgt über die prod()-Funktion, die mir das Produkt der Elemente eines Vektors, hier die Maße in Metern, berechnet., Damit es nicht zu einfach wird, muss ich für das Bad und für das WC noch Wände mit Fliesen unterschiedlicher Höhe abziehen. Dafür nehme ich wieder Länge und Breite der Räume und multipliziere sie mit der gemessenen Fliesenhöhe. Natürlich möchte ich die Maße der Räume nicht zweimal angeben und redundant vorhalten, daher hole ich mir die Vektoren aus der Raumauflistung von weiter oben., Zuletzt fehlt noch die Küchenzeile, die sich vereinfacht nur über eine Wand erstreckt, Nun zur Wandfläche insgesamt. Ich habe die Berechnung in eine Funktion gekapselt., Da ich mich über die Jahre mit mehreren Programmiersprachen intensiver auseinandergesetzt habe, vor allem mit Python, war ich zunächst versucht, die Berechnung aller Räume über eine Schleife zu lösen. Diese würde alle Räume ablaufen und auf jedes Element die Funktion anwenden. Etwas wie Folgendes:, Es funktioniert zwar wunderbar, ist aber gerade in R die falsche Vorgehensweise. Der Hintergrund dazu ist, dass die Schleife schrittweise Wiederholungen durchführt. Es ist deutlich performanter, die Berechnung in R vektorisiert durchzuführen. Bei unserer Handvoll Räume ist das völlig egal. Bei größeren Datensätzen mit tausenden von Zeilen wird es dann schon spürbarer. Die folgende Zeile verwendet die sapply-Funktion, die eine Funktion auf einen Vektor in einem Rutsch anwendet und das Ergebnis wieder als Vektor zurückmeldet., Die Maße der alten Wohnung hatte ich leider nicht genau vorliegen und musste einiges schätzen. Als finaler Check sollte die Fläche aber der tatsächlichen Fläche von etwa65m² entsprechen. Daher berechnete ich die Gesamtfläche aus den Angaben wieder mit der sapply-Funktion und wendete auf den Vektor die prod-Funktion an, um das Produkt aus Länge und Breite zu ermitteln., Für meine finale Wandfläche muss ich nun nur noch meinen Vektor der Einzelflächen aufaddieren und die Spezialflächen, wie Fenster und Türen, abziehen., So, das Ergebnis lautet etwa 235m²., PS: Ich hatte mittlerweile einen Vor-Ort-Termin mit einem Malermeister. Er hat die Wohnung grob vermessen und hat über eine Faustregel 220m² ermittelt. Auch nicht schlecht!, , © 2021 b.telligent;https://www.btelligent.com/blog/wandflaechenberechnung-vektorisiert-in-r/;B-Telligent;Stefan Seltmann
11.02.2015;            Standardisierung eines Erfolgskontrollprozesses in einem Versicherungsunternehmen        ;, © 2021 b.telligent;https://www.btelligent.com/blog/standardisierung-eines-erfolgskontrollprozesses/;B-Telligent;Laurentius Malter
04.02.2015;            SPSS Wertelabels für die Ausgabe umformatieren        ;"Kategoriale Variablen können in SPSS als ursprüngliche Texte verwendet werden, was bei größeren Datenmengen beträchtliche Performance-Einbrüche mit sich bringt,oder als numerische Codes mit Labels. Der zweite Weg ist nicht nur drastisch performanter, sondern auch der richtige Weg, weil es den Code in der SPSS-Syntax zwar schlechter lesbar macht,aber dafürvöllig immun gegenüber Änderungen in Schreibweisen ist., In SPSS istvoreingestellt, ob man bei Ergebnisausgaben, z.B. beim FREQUENCIES-Befehl, als Beschriftung die Zahlencodes, die Labels oder beides angezeigt bekommt. Alles hat Vor- und Nachteile ..., Man kann zwischen den verschiedenen Formaten in den Optionen wechseln. Unter ""Bearbeiten -&gt; Optionen -&gt; Ausgabe"" gibt es links den Bereich ""Gliederungsbeschriftung"". Hier kann man für die Variablennamen und die Variablenwerte über Pulldowns zwischen Labels, Werten/Namen und beidem wechseln., Jetzt ist es schon recht umständlich, jedes Mal diesen Menüpunkt aufzurufen, um die Einstellungen nach Bedarf zu ändern. Einfacher geht es in der Syntax direkt mit Optionsbefehlen., Man kann also in einer laufenden Syntax schnell für eine einzelne Ausgabe zwischen zwei Schreibweisen wechseln:, Hier ein konkretes Beispiel mit Automarken. Die Spalte ""Marke"" im Datensatz enthält Automarken als numerischen Code mit Labels., Der oben ausgeführte Code führt zu folgenden drei Varianten in der Ausgabe:, , © 2021 b.telligent";https://www.btelligent.com/blog/spss-wertelabels-fuer-die-ausgabe-umformatieren/;B-Telligent;Stefan Seltmann
28.01.2015;            IMHO lernt Python        ;"Anfang der 90er konnte ich mich als Kind das erste Mal an einer Programmiersprache versuchen, damals Turbo Pascal. Mittlerweile habe ich durch meine Berufserfahrungen und private Interessen kleinere Codes in C++, etwas größere Codes in Java, Perl und JavaScript, ausgereifte Apps für Endkunden in VBA und für professionelle Projekte über 5 Jahre mit PHP entwickelt. Dennoch, keine dieser Programmiersprachen hat mich vom ersten Moment an so überzeugt wie Python., Sicher ist Python in seiner normalen Distribution als ""High-Level""-Sprache nicht geeignet, um etablierten Sprachen wie C++ oder Java in Sachen Performance das Wasser zu reichen. Letztere sind einfach schneller und optimierter und haben den Vorrang in der professionellen Softwareentwicklung., Kein Problem für uns ... Data Scientists müssen vermutlich selten komplexe Anwendungen entwickeln. Warum sollten wir uns überhaupt mit Programmiersprachen auseinandersetzen?, Ich habe vor kurzem mehrere Stellenausschreibungen für ""Data Scientists"" gesehen, die nur Erfahrung in einem Statistiktool sowie leichte SQL-Kenntnisse voraussetzen. Kein Wunder, dass man im Zusammenhang mit ""Data Scientists"" von einem Hype spricht, wenn man es nur als Rebranding von reinen Statistikerinnen/Statistikern versteht., Nun, jeder echte Data Scientist ist ständig mit der Aufgabe konfrontiert, seine Daten erst mal aufzubereiten, Analyseergebnisse anderen Usern zugänglich zu machen und teilweise Reportings oder Präsentationen vorzubereiten oder selbst zu erstellen. Ich zumindest stoße dabei ständig auf kleinere und größere Probleme, die ich ohne die Hilfe einer kleineren Programmierung nur umständlich oder mit hohem händischen Aufwand lösen kann. Zusätzlich bieten mir Programmierungen die Möglichkeit, die Prozesse zu automatisieren und dabei Flüchtigkeitsfehler zu vermeiden., Genau hier kommt Python als Skriptsprache ins Spiel. Man kann sich schnell und ohne großes Setup ein kleines Skript zusammenschustern, das im Nu das Problem löst. Klar, für eigentlich alle diese Anwendungsfälle hätte ich auch Perl verwenden oder PHP missbrauchen können. Aber kein Konkurrent zu Python besitzt dessen Potential nach oben. Python darf nicht als eine von vielen Skriptsprachen abgetan werden. Python vereinigt eine Vielzahl der großen Programmierparadigmen. Die Sprache ist beispielsweise völlig objektorientiert, unterstützt aber auch funktionale Programmierung. Python kann vorgeworfen werden, die besten Ideen aus anderen Sprachen geklaut zu haben. Der Schöpfer Guido von Rossum antwortete darauf einmal sinngemäß, dass es ja genau die Absicht war, die Highlights anderer Sprachen zu bündeln., Gerade in den letzten 5 Jahren habe ich eine rasant wachsende Verbreitung der Sprache erlebt. Jeder, der sich im Web bewegt, hat mit Sicherheit schon Dienste genutzt, die u.a. Python nutzen, allen voran z.B. Google, YouTube und Dropbox., Für Data Scientists wächst ständig das Angebot an dedizierten Modulen zur Datenaufbereitung (z.B. http://pandas.pydata.org/), Analyse (z.B. http://www.scipy.org/) und Reporting (z.B. http://matplotlib.org/). Vor kurzem bin ich sogar über einen Artikel gestolpert, der die Sprache als künftige Ablösung von Tools wie R sieht: http://readwrite.com/2013/11/25/python-displacing-r-as-the-programming-language-for-data-science., So extrem würde ich es bei weitem nicht sehen. Es gehört schon viel Erfahrung im Umgang mit der Sprache dazu, um etablierte Lösungen wie R abzulösen., Mich persönlich überzeugt die Sprache sehr, weil ..., Ich habe Python in den letzten etwa 7 Jahren im Arbeitsumfeld für Dinge verwendet wie:, und so manches mehr ..., Genug geschwärmt. Hier ein kleines Beispiel. Die Nachricht aus der ersten Zeile soll in ""IMHO lernt Python"" umgewandelt werden. Oben gibt es die langsame Variante, welche die Wörter abläuft und nur die ersten 4 umwandelt., Zunächst wird die Nachricht anhand eines Leerzeichens in eine Liste von Wörtern überführt (Split-Befehl). Später wird diese Liste in einer Schleife abgelaufen. Wenn das jeweiligeWort zu den ersten 4 gehört, wird fürjedes dieser Wörter nur der erste Buchstabe ausgewählt und der neuen Nachricht angehängt. Die restlichen Wörter werden unverändert mit einem führenden Leerzeichenübernommen. Da die neue Nachricht im Moment noch eine Liste von Wörtern ist, wird sie zum Ausgeben im Print-Befehl noch mit Hilfe eines leeren Strings als Kleber zwischen den Wörtern zu einem einzigen String verknüpft (Join-Befehl)., Weiter unten gibt es die Variante für Fortgeschrittene, alles in einer Zeile :-D, , , © 2021 b.telligent";https://www.btelligent.com/blog/imho-lernt-python/;B-Telligent;Stefan Seltmann
08.01.2015;            Roll-Out-Strategien für Kampagnenmanagement – think global, act local        ;, © 2021 b.telligent;https://www.btelligent.com/blog/roll-out-strategien/;B-Telligent;Laurentius Malter
08.01.2015;            Ein Blick in die Data-Science-Werkzeugkiste        ;In diesem Eintrag möchte ich gemeinsam mit Ihnen einen Blick in unsere Werkzeugkiste werfen. Das Thema bietet Stoff für mehr als einen Eintrag, und wir werden in diesem Blog immer mal wieder darauf zurückkommen., Als Berater hat man es immer dann leicht, wenn der Kunde bereits eine umfangreiche Data-Science-Infrastruktur besitzt. Aber wiepraktiziere ich Data Science, wenn mein Kunde nicht bereits über eine etablierte Softwareumgebung mit Statistiktools, Datenbanken und Visualisierungswerkzeugen verfügt? Für diesen Zweck benutzen wir unser „Data Science Survival Kit“. Das isteine Zusammenstellung von Softwaretools, die darauf ausgelegt ist, schnell mit unserer Arbeit beginnen zu können,wenn auf Kundenseite wenig zur Verfügung steht. Es handelt sich um Tools, die idealerweise mitgeringem Installationsaufwand und ohne Lizenzprobleme daherkommen, die aber dennoch leistungsfähig sind und gut miteinander zusammenspielen., Da gibt’s natürlich die üblichen Verdächtigen wie Python, über die wir auch noch schreiben werden – einsteigen ins Survival Kit möchte ich jedoch an einer ganz anderen Stelle: mit der Geovisualisierung. Landkartendarstellungen sind äußerst wertvoll, um Ergebnisse handhabbar zu machen. Moderne Geofinformationssysteme sind darüber hinaus nicht nur Visualisierungs- sondern auch mächtige Verarbeitungswerkzeuge. Unsere Werkzeugkiste enthält an dieser Stelle das Geoinformationssystem QGIS (siehe Screenshot). QGIS ist Open Source, schnell installiert und sehr mächtig., , , Besonders wichtig ist uns die hervorragende Datenbankintegration sowie die Möglichkeit, das Tool mit Hilfe von Python selbst zu erweitern. Einfache Visualisierungen wie der postleitzahlbasierte Screenshot sind damit zügig zu erstellen, aber auch komplexe und ungewöhnliche Kartendarstellungen sind machbar., Neben der reinen Geovisualisierung eignet sich QGIS hervorragend zur Aufbereitung von Geodaten vor weiteren Analyseschritten außerhalb von QGIS. Es stehen diverse Geoalgorithmen zur Verfügung, von kürzesten Wegen über die Festellung von geometrischen Beziehungen (Zuordnungen von geographischen Punkten zu Flächenstücken wie PLZ-Gebieten zum Beispiel) bis hin zu verschiedenen Trendberechnungen. Da QGIS auch Zugriff auf die Algorithmen anderer Open-Source-Projekt wie SAGA bietet, lassen sich damit auch anspruchsvolle Geo-Data-Science-Aufgaben wie die Klassifizierung von Satellitenbildern durchführen., Das nur als Appetithappen -- in späteren Einträgen werden wir von konkreten Beispielen berichten, wie sichQGIS in Projekte einbinden lässt., , © 2021 b.telligent;https://www.btelligent.com/blog/ein-blick-in-die-data-science-werkzeugkiste/;B-Telligent;Dr. Michael Allgöwer
17.12.2014;            Das Monty-Hall-Dilemma/Ziegenproblem in 10 Python-Zeilen        ;"So manch einer erinnert sich an die Spielshow ""Geh aufs Ganze"" aus den 90ern, in der Kandidaten sich für eines von drei Toren entscheiden mussten. Hinter einem Tor war stets der Preis versteckt und hinter den anderen Toren waren Nieten in Form des Zonk, bzw. in den USA bei Moderator Monty Hall waren es Ziegen. Der Kandidat wählt zu Beginn immer ein Tor aus, hinter dem er den Preis vermutet. Der Moderator kann anschließend versuchen, ihn auf andere Tore mit Geldangeboten umzustimmen. Er kann auch Tore öffnen, um die Spannung zu erhöhen., Dieses Öffnen von Toren fordert den Kandidaten auf, bei seiner bisherigen Wahl zu bleiben oder auf das letzte verbleibende Tor zu wechseln. Genau hier liegt der Kern des Dilemmas. Intuitiv vermutet die Mehrheit, dass es egal ist und die Gewinnchance beide Male bei einem Drittel bleibt. Tatsächlich verdoppelt sich die Gewinnchance auf zwei Drittel, wenn man wechselt. Eigentlich ist es ja ganz einfach, denn wenn ich nicht wechsle, ist es genauso, als ob man mein Tor sofort geöffnet hätte, also ein Drittel. Die verbleibende Alternative muss zwei Drittel Chance besitzen, denn es muss insgesamt auf 100%, also drei Drittel aufgehen., Als ich es vor über 10 Jahren zum ersten Mal gelesen habe, wollte ich es erst nicht so recht glauben, drum habe ich es einfach in Java als Simulation programmiert und wurde schließlich von den Fakten eines Besseren belehrt. In diesem Blog-Eintrag möchte ich das konkrete Beispiel nutzen, um so eine Simulation mit Python zu demonstrieren, und zwar so schlank wie möglich. Hier zunächst der Code:, Ich finde es beeindruckend, dass Python so eine Simulation in nur 10 Zeilen und relativ lesbar ermöglicht. Ich stütze mich hierbei auf die etwas vernachlässigten SETs in Python. Diese können in Programmierungen zu Datenauswertungen sehr hilfreich sein, weil sie performant das Abgleichen von Mengen ermöglichen. SETs sind in Python Anordnungen wie Listen oder Tupel mit der Ausnahme, dass sie jeden Wert nur einmal beinhalten dürfen und die Reihenfolge zweitrangig ist. Hier die Erläuterung des Codes:, Das Ergebnis liegt nahe 0,667%, also bei zwei Drittel. Nun kann man sich fragen, warum ich in Zeile 7 so aufwendig die verbleibende Tür per Zufall gezogen habe, wenn es doch eh nur noch eine gibt. Nun, ich habe das Problem hier nicht nur in 10 Zeilen simuliert, sondern auch noch die Möglichkeit geschaffen, das Problem in einem Spiel mit 4 oder mehr Türen zu simulieren. Das Ergebnis: Bei 4 Toren ist das Ergebnis 0,60%, bei 5 Toren 0,57%. Mit steigender Anzahl sinkt erwartungsgemäß der Nutzen durch den Torwechsel, da die Restwahrscheinlichkeit sich auf mehrere Alternativen aufteilt., , © 2021 b.telligent";https://www.btelligent.com/blog/das-monty-hall-dilemma/;B-Telligent;Stefan Seltmann
02.12.2014;            Datenqualitätsmanagement auch in Zeiten der Big-Data-Ansätze als CRM entscheidende Voraussetzung        ;"Um ein wirksames Customer Relationship Management einsetzen zu können, ist es, besonders im Bereich des Kampagnenmanagements, von großer Bedeutung, aussagekräftiges Wissen über die eigenen Kunden und Kundenstrukturen zu haben. Persönliche Informationen über Kunden, Ereignisse und Veränderungen in Kundensituation und -verhalten in einem CRM-System können als Anlass für eine auftretende Kommunikation genutzt werden. Mit Hilfe dieser vorhandenen Daten wird eine personalisierte Interaktion mit dem Kunden, wie Angebote und Services, ermöglicht., Viele Unternehmen beginnen derzeit, erste Erfahrungen mit dem Einsatz von Big-Data-Methoden zu machen. Diese Methoden versprechen, aus großen Mengen an meist unstrukturierten Daten in kürzester Zeit mittels statistischer Methoden zusätzliches Wissen über die Kunden zu generieren. Häufig werden dazu automatisch generierte Daten eingesetzt, wie zum Beispiel Logfiles oder auch Clickstreams, die aus der Nutzung der Webseite, eines Webshops oder den Daten aus sozialen Netzen erhoben werden. Dieses Wissen kann wertvoll für CRM-Aktivitäten eingesetzt werden. Dabei stellt sich die Frage der Bedeutung von Datenqualität bei Einsatz von Big-Data-Methoden. Denn: Die Analyse und damit das generierte Wissen kann nur so gut sein wie die Qualität der dazu verwendeten Daten., Eine automatisierte Generierung von Daten und daraus Wissen, wie durch Big-Data-Methoden, kann dazu verleiten, davon auszugehen, dass Bemühungen zur Verbesserung der Datenqualität keine so große Bedeutung mehr haben. Dies ist ein Fehlschluss, denn das Gebot des Datenqualitätsmanagements bleibt bestehen, auch wenn sich die Schwerpunkte verlagern., Zwei typische Probleme aus der Praxis: Unter anderem durch Systemausfälle kann es bei der automatisierten Datengenerierung zu sporadisch lückenhaften Daten kommen. Mit diesen Lücken ist umzugehen, um nicht systematische Verzerrungen zu erhalten. Auch kommt es oftmals zu einer fehlerhaften fachlichen Interpretation von Analyseergebnissen, wenn eine Analyse des Prozesses der Datengenerierung unterbleibt und auf Basis von Annahmen Schlüsse gezogen werden., Um Datenqualitätsprobleme anzugehen, sollte eine systematische Herangehensweise verfolgt werden. Gerade die Analyse und Beseitigung von Ursachen für erkannte Probleme ist die Basis für nachhaltigen Erfolg. In einer Untersuchung zum Datenqualitätsmanagement im CRM (Leußer, 2011) wurden bei einer Befragung von CRM-Experten in DACH-Gebiet folgende allgemeine Ursachenfelder identifiziert:, Diese Ursachenfelder lassen sich auch auf Big-Data-Methoden anwenden:, Das Veralten der Daten ist gerade bei stark volatilen Daten, die häufig eine kurze Werthaltigkeit haben, ein wichtiges Thema. Genau dazu wurden ja entsprechende Big-Data-Methoden wie MapReduce entwickelt, um Wissen schnell generieren zu können. Für eine schnelle Reaktion auf neue Erkenntnisse benötigt es aber neben der Analyse auch eine entsprechend konzipierte Architektur sowie für die Kommunikation Systeme der Marketing-Automation. Damit ist auch das oben genannte Ursachenfeld ""Design von IT-Systemen"" betroffen., Ein häufiger Kritikpunkt an Big-Data-Analysen ist die falsche Interpretation der Ergebnisse. So werden oftmals Korrelationen mit Kausalität gleichgesetzt oder Beziehungen, wie Facebook-Freundschaften, überbewertet. Die Ursachen liegen hier zumeist in fehlendem Wissen im Umgang mit Analyseergebnissen und unzureichender Dokumentation., Wie man sieht, ändern sich die Schwerpunkte der Ursachen, die hinter Datenqualitätsproblemen im Zusammenhang mit Big-Data-Analysen liegen, etwas. Damit hat dies natürlich auch Auswirkungen auf die folgenden Maßnahmen. Die eingangs erwähnte Untersuchung nennt für CRM-relevante Daten allgemein folgende Maßnahmenkategorien:, Auch diese Kategorien lassen sich auf Big-Data-Analysen anwenden. Teilweise können Ergebnisse dieser Analysen die traditionelle Datenerfassung unterstützen und dabei bisherige Informationslücken als Datenqualitätsproblem schließen. So war zum Beispiel das Verhalten auf Webseiten bisher aufgrund der Masse an Tracking-Informationen keine schnell zugängliche Information. Über eine entsprechende Analyse können nun für die Interaktion und Kommunikation wertvolle Inhalte generiert werden., Eine weitere Einsatzmöglichkeit von Big-Data-Methoden liegt im Datenqualitäts-Monitoring. Hierbei geht es um die Entdeckung von Unregelmäßigkeiten in den Datenströmen. Ein entsprechendes Monitoring sichert dabei sowohl die Datenbasis für traditionelle Daten und Methoden wie auch für Big-Data-Analysen. Bei Unregelmäßigkeiten, wie Ausreißern oder fehlenden Daten, werden Benachrichtigungen angesteuert, um Ursachenanalysen und Maßnahmen zu veranlassen., Durch den Einsatz von Big-Data-Methoden kann für das CRM wertvolles Wissen generiert werden. Gerade in Feldern wie der sozialen Vernetzung oder dem Suchverhalten, die durch bisherige Daten nicht abgedeckt sind, bereichert dies das Bild vom Kunden. Dieses Wissen stabil und nachhaltig in Wert zu verwandeln, setzt aber ein Datenqualitätsmanagement voraus. Ein fortlaufendes Datenqualitäts-Monitoring, die Dokumentation der Generierungsprozesse und die Vermittlung des Methodenwissens stellen wichtige Maßnahmen dar, die in jedem Unternehmen umgesetzt sein sollten., , © 2021 b.telligent";https://www.btelligent.com/blog/datenqualitaetsmanagement/;B-Telligent;Laurentius Malter
26.11.2014;            SPSS Häufigkeitsauswertungen in R selbstgemacht        ;"Seit meinem Psychologiestudium war ich ein intensiver SPSS-Nutzer. Was mich über alle Versionen in dieser Zeit begleitet hat, waren die einfachen, knappen Befehle, um mir deskriptive Statistiken anzeigen zu lassen. Diese kurzen Kommandos gehen schnell in Fleisch und Blut über und ermöglichen ein schnelles Sichten der Daten., Aktuell liegt mein Tool-Schwerpunkt bei R. Es ist eine hervorragende Alternative, aber trotz umfangreicher Erfahrung mit diesem Open-Source-Tool fehlt mir immer noch ein bisschen die gefühlte Usability von SPSS. Mir fehlen schlicht und einfach meine kurzen Kommandos. Nun ist es relativ leicht, SPSS-ähnliche Befehle selbst als Funktionen in R zu ergänzen., Als Beispiel nehme ich den Frequencies-Befehl. In SPSS reicht es, für Befehle nur die ersten 3 bis 4 Buchstaben zu verwenden, solange es eindeutig ist. ""FREQ kategorie."" z.B. stellt mir für die Spalte ""kategorie"" für jeden distinkten Wert die absolute Häufigkeit, die prozentuale Häufigkeit, die prozentuale Häufigkeit an gültigen Fällen ohne Fehlende sowie die kumulierte prozentuale Häufigkeit dar. Für eine explorative Datenanalyse ist das sehr hilfreich, da ich sofort sehe:, , Wenn man den Befehl noch anpasst, z.B. ""FREQ kategorie /FORMAT AFREQ."", dann werden die Häufigkeiten aufsteigend statt absteigend dargestellt., So, genau das hätte ich jetzt auch gerne in R. Der Befehl lässt sich wie folgt über eine benutzerdefinierte Funktion umsetzen:, Was passiert hier?, Das Endergebnis gefällt mir schon sehr gut! Zu einem späteren Zeitpunkt werde ich wohl noch weitere Funktionalitäten integrieren, z.B. einen ""limit""-Befehl, um nur die Top-X-Kategorien anzuzeigen usw. Aber in der Zwischenzeit hat die neue Funktion ""freq"" mir die Arbeit schon sehr erleichtert :-)., Das Beispiel unten zeigt ein Ergebnis der Funktion. Ich habe bewusst die Funktion so von SPSS abweichen lassen, dass fehlende Werte in den kummulierten Prozenten (perc_cum) berücksichtigt werden, da ich es so öfter benötige., , © 2021 b.telligent";https://www.btelligent.com/blog/spss-haeufigkeitsauswertungen/;B-Telligent;Stefan Seltmann
06.11.2014;            Predictive Analytics World, Zweiter Tag        ;, © 2021 b.telligent;https://www.btelligent.com/blog/predictive-analytics-world-zweiter-tag/;B-Telligent;Dr. Michael Allgöwer
05.11.2014;            Predictive Analytics World        ;"Die Predictive Analytics World in Berlin ist eine kleine, aberbrillante Veranstaltung. Sie ist das wahrscheinlich beste Treffen der Predictive Analytics Community in Deutschland, hochkonzentriert auf anspruchsvolle Vorträge und ein spezialisiertes Publikum. Ein passender Anlass also für den ersten Eintrag inunserem Predictive AnalyticsBlog., Vorausschicken muss ich, dass mich die Predictive Analytics World jedes Mal fertigmacht. Im positiven Sinne. Es schwirren auf dieser Veranstaltung so viele Anregungen und Ideen durch die Räume, dass ich um Nachsicht bitte dafür, dass dieser Eintrag unter starkem Endorphineinfluss entsteht., Der Höhepunkt des Vormittags war der Vortrag von Hendrik Wagenseil und Nina Weigel von der GfK zum Thema „From Smart Phones to Smart Places to Smart Profiles“. Sie berichteten über die Entwicklung eines zukünftigen Produkts der GfK, das einen Einblick geben soll in die geographischen Bewegungen von Zielgruppen, die für das Marketing interessant sind. Eine Luxusvariante von Passantenfrequenzerhebungen sozusagen. Die Grundidee besteht in der Zusammenführung von Geopositionsdaten von Mobilfunknetzbetreibern auf der einen mit Marketingprofilen auf der anderen Seite. Dass und wie das selbstverständlich datenschutzkonform geschehen muss, ist nur eines der spannenden Probleme, die angeschnitten wurden. Erleichtert wurde das vermutlich dadurch, dass es in dem Vortrag primär um US-Daten ging; aber natürlich wären vergleichbare Produkte auch in Deutschland interessant., Mindestens ebenso hörenswert waren die methodischen Ideen zur Fusion und Qualitätssicherung der beiden sehr unterschiedlichen Datenquellen. Die potentiellen Fehlerquellen sind vielfältig, z.B. Stichprobenfehler, weil nicht jeder ein Handy bei sich trägt, beschränkte Genauigkeit der rein auf Mobilfunkzellen basierenden Lokalisierung etc. Die Ansätze zur Minimierung dieser Fehler kamen nicht von der Stange, sondern waren auf die Besonderheiten der Datenlage sorgfältig zugeschnitten. Stichprobenfehler beispielsweise wurden durch geschickten Rückgriff auf die (von Volkszählungsdaten bekannte) Demographie des Wohnorts der Handynutzer minimiert., Dabei wurde im Vorbeigehen auch die Sensibilität der verwendeten Daten immer wieder deutlich, etwa wenn darauf hingewiesen wurde, dass sich aus lokalisierten Einbuchungsereignissen mit Zeitstempeln mit hoher Wahrscheinlichkeit auf den Wohnort zurückschließen lässt. Der geographische Ort mit einer Häufung nächtlicher Einbuchungsereignisse ist offensichtlich meist der Wohnort., Einen völlig anderen Akzentsetzte Prof. van der Aalst von der TU Eindhoven in seinem Vortrag über Process Mining. Wir legen in unseren Beratungsprojekten seit jeher Wert darauf, die Prozesse zu verstehen, die die Daten erzeugen, mit denen wir arbeiten. Trotzdem hat mich der Reifegrad der Methoden undSoftwaretools überrascht und begeistert, die Prof. van der Aalst vorgestellt hat. Die unmittelbare Konsequenz für mich war, dass diese Tools in unser Data-Science-Toolkit aufgenommen werden. Von ersten Erfahrungen werde ich sicher demnächst in diesem Blog berichten. Das Fazit für mich aus diesem Vortrag war, dass es dort eine spezialisierte Community gibt, die Großartiges leistet, aber im Data-Science- und auch im Business-Intelligence-Umfeld weitgehend unbekannt ist., Die Methoden, diein dieserCommunityentwickelt wurden, erlauben es, aus Logdaten die dahinterliegenden Prozesse automatisiert zu rekonstruieren, mit einem frei wählbaren Detailgrad. Diese tatsächlichen Prozesse – einschließlich aller inoffiziellen Abkürzungen, Fehler und Besonderheiten – lassen sich dann als Grundlage nutzen, um sie entweder mit „offiziellen“ Prozessenzu vergleichen oder diese erst zu entwickeln.Sie bilden auch eine solide Grundlage für Vorhersagen – eine wertvolle Ergänzungfür Predictive Analytics, die durch keine der Standardmethoden aus Statistik oder Machine Learning ersetzbar ist, sich damit aber wunderbar kombinieren lässt. Der Anwendungsbereich dieser Methoden ist enorm und umfasst längst nicht nur die industriellen Prozesse, an dieman vielleicht als Erstes denkt. Auch und gerade Prozesse im Bereich Kundenservice sind hierfür ein dankbares Anwendungsfeld, aber auch beispielsweise bei Prozessen in Krankenhäusern haben sich diese Methoden bewährt., , Neben diesen Highlights gab es natürlich weitere spannende Vorträge und konzentrierten Austausch in den Pausen. Morgen geht?s weiter mit dem zweiten Teil der Konferenz und dieses Eintrags ..., , © 2021 b.telligent";https://www.btelligent.com/blog/predictive-analytics-world-erster-tag/;B-Telligent;Dr. Michael Allgöwer
30.10.2014;"            arcplan meets Hichert - Grafiken mit ""outline bars""        ";", , , Wichtig: Außer unten setzen wir alle Ränder auf ""0"", um eine starre Grafikgröße und Balkenpositionierung zu erhalten., , Im nächsten Schritt duplizieren wir die Grafik:, , Jetzt wird nur noch die zweite Grafik leicht nach unten versetzt über die erste Grafik geschoben, so dass die schwarzen Balken fast vollständig von den weißen Balken verdeckt werden. Der Bereich der schwarzen Balken, der noch hervorschaut, sieht nun wie ein Rand der weißen Balken aus und wir haben optisch ""outline bars"". Als Letztes werden für eine saubere Darstellung bei der ""oberen"" Grafik die Skala unten und die Datenbeschriftung entfernt (beides wird nur durch die untere Grafik dargestellt)., Werden semantische Achsen verwendet, so wird nun auch noch die ""Nulllinie"" entsprechend angepasst. Hier ist die Achse ein ""Einzelfeld"" - zunächst schwarz., Und mit weißer Farbe und schwarzem Rahmen (oben und unten) korreliert die Achse mit den Balken., Eine zweite Variante wird im nächsten arcplan-Blogbeitrag vorgestellt., , © 2021 b.telligent";https://www.btelligent.com/blog/arcplan-meets-hichert-outline-bars/;B-Telligent;Stefan Kersten
16.10.2014;            Marketing Resource Management - Effizienzsteigerung in Beratungsleistungen über eine integrierte Workflow- und Collaboration-Plattform        ;In Toollandschaften der Mediaplanung mit regionalem Fokus finden sich Insellösungen, die eine Vereinheitlichung von Prozessen unmöglich machen. Die Mediagattungen benötigen für ihre Planung oftmals spezifische Tools zur Budgetallokation, Verfügbarkeitsprüfung und Buchung von Platzierungen. Die Planungsergebnisse werden manuell übertragen und über ein Geoinformationssystems (GIS) auf Zielgruppenpassung hin überprüft und visuell aufbereitet. Alle Outputs werden im Folgenden einzeln im Format Excel exportiert, manuell zusammengeführt und auf die CI des Kunden formatiert - auf Wunsch wird dies zusätzlich auch in PowerPoint umgesetzt. Führt ein Kundenfeedback zu einer Anpassung, wird der Prozess erneut durchlaufen., Diese Faktoren verursachen neben manuellen Prozessen einen hohen Abstimmungs- und Kommunikationsaufwand in der Interaktion von Kundenberater, Mediaplaner, Kreativagenturen und dem werbetreibenden Kunden - es entstehen Ineffizienzen und hohe prozessseitige Kosten für alle Beteiligten., Diese Situation ist Ansatzpunkt eines Optimierungsprojekts über die Einführung eines Marketing Resource Management Systems (MRM) bei einer führenden Mediaagentur in Deutschland. Sie stellt mit ihren Schwesterunternehmen das tragende Element innerhalb eines der global größten Netzwerke von Planungs- und Einkaufsagenturen., Mit Hilfe eines speziell auf den Kunden abgestimmten MRM-Tools können erhebliche Effizienzsteigerungen erzielt werden. Dies macht sich vor allem bei der Prozessunterstützung und der wesentlich schnelleren Abstimmung der beteiligten Ressourcen bemerkbar. Zentrales Element ist dabei eine offene Collaboration- und Workflow-Plattform, die über verschiedene Self-Service-Elemente und Kommunikationsfunktionen verfügt. Beteiligte Fachbereiche und operative Prozesse werden nach funktionalen Entitäten strukturiert und nach einer End-to-End-Prozessanalyse eingebunden. Hierüber wird eine gemeinsame Integration der verschiedenen Informations-, Planungs- und Buchungssysteme im Mediaplanungsprozess und der Interaktion zwischen Werbekunden, Kundenberatern, Kreativagenturen und Publishern erreicht. Zusätzlich zu den Handlungsfeldern wird ein mediagattungsspezifisches Wissensmanagement-Tool spezifiziert und in die Prozesse eingebettet., Zielsetzung in diesem MRM-Projekt ist die Anforderungsanalyse und Ableitung der Soll-Spezifikation einer Collaboration-Plattform mit Self-Service-Bereichen für den vollständigen crossmedialen Mediaplanungsprozess mit Geo-Analyse-Fokus. Darüber hinaus wird die Spezifikation für eine mediaspezifische Wissensdatenbank zur Sammlung und Bereitstellung von Wissen zu regionalen Mediaverfügbarkeiten sowie ein Konzept für ein Revisionssicherheit bietendes Dokumentenmanagement erstellt., Analyse des vollständigen crossmedialen regionalen Mediaplanungsprozesses, Operativ tätige Mitarbeiter und die zugehörige Management-Ebene der beteiligten Funktionsbereiche sollten bei der Analyse gemeinsam einbezogen werden. Dies und eine zusätzliche intensive Kommunikation des Projekts im Unternehmen verbessert die Qualität, Praktikabilität und Realisierbarkeit der erarbeiteten Anforderungen und unterstützt so ein erfolgreiches Change Management., Für eine erfolgreiche Spezifikation wird bei der Erfassung von Prozessen ein abgestimmtes Einverständnis bezüglich des Soll-Zustands aller Beteiligten benötigt. Dies unterstützt die effektive und effiziente Erfassung von Anforderungen zu Kollaboration und Workflows in der Analyse. Die Darstellung von Zwischenergebnissen und Entwicklungsstufen über Mock-ups und Click Dummies verbessert die Kommunikation der geplanten Workflows und beschleunigt die Artikulation und Qualität der Anforderungen im Entwicklungsprozess., Die Implementierung eines MRM Collaboration Systems zur Unterstützung des Beratungsprozesses in der Mediaplanung schafft effizientere Workflows und senkt damit die Prozesskosten. Nach der Etablierung der neuen Prozesse sollte die Integration weiterer Funktionsbereiche des Unternehmens erfolgen. Die darüber ermöglichte integrierte Sicht auf vorhandene Informationen, gezielte Analytik und das Schaffen von aufsetzender Intelligenz können helfen, weitere Potentiale zur Erhöhung der Wettbewerbsfähigkeit durch gezieltes Management der Unternehmensressourcen zu heben., , © 2021 b.telligent;https://www.btelligent.com/blog/mrm-effizeinzsteigerungen-in-beratungsleistungen/;B-Telligent;Laurentius Malter
03.09.2014;            Alt trifft neu: Client-Technologien für arcplan 8        ;, © 2021 b.telligent;https://www.btelligent.com/blog/client-technologien-fuer-arcplan-8/;B-Telligent;Stefan Kersten
15.07.2014;            Location Based Advertising        ;, © 2021 b.telligent;https://www.btelligent.com/blog/location-based-advertising/;B-Telligent;Laurentius Malter
18.06.2014;            PROSET - Ein Forschungsprojekt        ;"Am PROSET-Forschungsprojekt arbeiteten die TU München, die ETH Zürich und b.telligent für insgesamt drei Jahre, von Februar 2011 bis Februar 2014. Im Mittelpunkt standen Fragestellungen, für deren Bearbeitung im gewöhnlichen Arbeitsalltag meist keine Zeit bleibt. Dabei werden oftmals Fragen behandelt, die nicht nur für die Praxis relevant sind, sondern auch neue Erkenntnisse für die Forschung ans Licht bringen. Im PROSET-Projekt haben wir uns der Frage der Produktivitätssteigerung durch Service Experience Management gewidmet., , , Um für die Wissenschaft und Wirtschaft interessante Fragestellungen bearbeiten und erschließen zu können, ist es zunächst notwendig, ""echte Daten"" zu analysieren. Diese für das Projekt PROSET relevanten und echten Daten stellte das Telekommunikationsunternehmen o2 zur Verfügung, das wir erfolgreichals Praxispartner gewinnen konnten. Für die von o2 bereitgestellten anonymisierten kunden- und transaktionsbezogenen Daten erhielt der Praxispartner im Gegenzug wertvolle Einsichten in das eigene Customer Experience Management sowie einen einmaligen, objektiven Einblick in die Customer-Relationship-Management-Prozesse bzw. in das Zusammenspiel von Kontaktkanal und Kundenwert im Geschäftsmodell., Die Kooperation mit der Telefónica Germany GmbH startete im Februar 2013. Nach einer circazweimonatigen Voranalysephase wurde mit der Bearbeitung der Forschungsfragen begonnen., Bearbeitet wurden folgende Themen:, Im Rahmen des Projekts entstanden zwei Abschlussarbeiten an der TU München., Nach einem Jahr gelungener Kooperation wurde das Teilprojekt im Februar 2014 erfolgreich abgeschlossen. Die Ergebnisse des Forschungsprojekts werden mit der Unterstützung unseres Medienpartners Call Center Verband in einer Gesamtveröffentlichung voraussichtlich im Juli 2014 publiziert., Was ist PROSET:, Gefördert wurde das Projekt vom Bundesministerium für Bildung und Forschung, vom Projektträger im DLR (Deutsches Zentrum für Luft­ und Raumfahrt) sowie von der strategischen Partnerschaft ""Produktivität von Dienstleistungen""., An dieser Stelle möchte b.telligent noch einen herzlichen Dank an Prof. Dr. Florian von Wangenheim von der ETH Zürich und Prof. Dr. Rainer Kolisch von der TU München sowie an die vielen hilfsbereiten Ansprechpartner von o2 für die gemeinsame Durchführung des Projekts aussprechen!, , © 2021 b.telligent";https://www.btelligent.com/blog/proset-forschungsprojekt/;B-Telligent;Laurentius Malter
11.06.2014;            arcplan meets Hichert: Einheitliche Skalierung von Grafiken        ;Grafiken in Präsentationen und im Reporting haben vor allem die Aufgabe, Informationen schnell und effizient zu vermitteln. Das heißt, Grafiken sollen auf einen Blick wichtige Aussagen, wie zum Beispiel Trends, Abweichungen, Muster und Auffälligkeiten wie Peaks, Ausreißer etc. verdeutlichen. Insbesondere gilt dies, wenn es das Ziel ist, in Präsentationen und im Reporting die Hichert©SUCCESS-Regeln anzuwenden., Arcplan Enterprise ist ein hervorragendes Tool, um in wenigen Schritten Grafiken zu erstellen, die entsprechende Aussagen deutlich vermitteln können, wie es hier an einer Abweichungsanalyse für Produkte zu sehen ist:, , Wenn mehrere (gleiche) Informationen – in diesem Fall Produkte – dargestellt und miteinander verglichen werden, dann ist es wichtig, dass die einzelnen Grafiken eine einheitliche Skalierung erhalten, damit die Balkenlänge in der richtigen Proportion zu den Werten aller Grafiken steht., ,  Das bedeutet, dass alle Grafiken – in diesem Fall Produkt A bis D – denselben Maximalwert haben müssen, damit die Längen der Balken in der richtigen Proportion zueinander stehen., , , Im letzten Schritt wird der gesamtheitliche Maximalwert „max alle“ in jeder Grafik verwendet., , © 2021 b.telligent;https://www.btelligent.com/blog/arcplan-meets-hichert-einheitliche-grafikenskalierung/;B-Telligent;Stefan Kersten
22.05.2014;            Mobile vs. PC        ;, © 2021 b.telligent;https://www.btelligent.com/blog/mobile-vs-pc/;B-Telligent;Laurentius Malter
17.04.2014;            Best Practice CRM: Evaluierung Kampagnenmanagementsysteme Finanzsektor        ;, © 2021 b.telligent;https://www.btelligent.com/blog/evaluierung-kampagnenmanagementsysteme-finanzsektor/;B-Telligent;Laurentius Malter
20.03.2014;            Effiziente CRM-Erfolgsmessung bei einem Video-on-Demand-Anbieter        ;, © 2021 b.telligent;https://www.btelligent.com/blog/effiziente-crm-erfolgsmessung-video-on-demand-anbieter/;B-Telligent;Laurentius Malter
17.03.2014;"            Tipps &amp; Tricks: Individuelle Auswahl als Performance Optimierer oder Killer        ";Hier die Ergebnisse der Abfrage im SQL Server Management Studio., , Die Kennzahldimension wurde ausgewählt und eingeschränkt. Das System ermittelt nun ohne jegliche Einschränkung die Kennzahlwerte und -namen., Als Resultat hätte dieser Bericht folglich ab arcplan 7 zwei Datenabfragen mehr. Je nach Komplexität kann dies natürlich beliebig anwachsen., , , , © 2021 b.telligent;https://www.btelligent.com/blog/tipps-tricks-performancesteuerung-von-arcplan/;B-Telligent;Stefan Kersten
20.02.2014;            Auswirkung von Cross- und Up-Selling-Strategien auf den Erfolg im Call-Center        ;, © 2021 b.telligent;https://www.btelligent.com/blog/auswirkung-cross-up-selling-strategien-im-call-center/;B-Telligent;Laurentius Malter
23.01.2014;            Customer Intelligence im Bestandskundenmanagement        ;, © 2021 b.telligent;https://www.btelligent.com/blog/customer-intelligence-bestandskundenmanagement/;B-Telligent;Laurentius Malter
16.01.2014;"            Tipps &amp; Tricks: Gleichzeitiges Anmelden an mehreren Datenbanken        ";, © 2021 b.telligent;https://www.btelligent.com/blog/tipps-tricks-gleichzeitiges-anmelden-bei-datenbanken/;B-Telligent;Stefan Rann
19.12.2013;            Zukunftsorientiertes Kampagnenmanagement im Online-Gaming        ;, © 2021 b.telligent;https://www.btelligent.com/blog/zukunftsorientiertes-kampagnenmanagement-online-gaming/;B-Telligent;Dr. Wolfgang Leußer
21.11.2013;"            Retention &amp; WinBack        ";, © 2021 b.telligent;https://www.btelligent.com/blog/retention-winback/;B-Telligent;Laurentius Malter
07.11.2013;"            Tipps &amp; Tricks: Die Funktion MENÜHIERARCHIEPOPUPZEIGEN        ";, © 2021 b.telligent;https://www.btelligent.com/blog/tipps-tricks-die-funktion-menuehierarchiepopupzeigen/;B-Telligent;Arno Velden
23.10.2013;            Steigerung der Kampagnenprofitabilität durch die Einführung von SAS Marketing Optimization        ;, © 2021 b.telligent;https://www.btelligent.com/blog/steigerung-der-kampagnenprofitabilitaet/;B-Telligent;Laurentius Malter
19.09.2013;            Ankündigung Marktübersicht: Kampagnenmanagementsysteme für den deutschen Markt        ;, © 2021 b.telligent;https://www.btelligent.com/blog/marktuebersicht-kampagnenmanagementsysteme/;B-Telligent;Laurentius Malter
16.09.2013;"            Tipps &amp; Tricks: arcplan-Hintergrundbericht zum Anmelden an Datenbanken        ";"arcplan bietet die Möglichkeit, Texte verschlüsselt in Felder einzugeben.(Objekteigenschaften F2 - ""Verschlüsselte Eingabe""anhaken.), Der Login-Button enthält dann die Anmeldung an die Datenbank., ""DB_AdventureWorks"" = Name der Verbindungsdatei, , © 2021 b.telligent";https://www.btelligent.com/blog/tipps-tricks-der-arcplan-hintergrundbericht/;B-Telligent;Stefan Kersten
22.08.2013;            PROSET – Produktivitätssteigerung durch Service Experience Management        ;, © 2021 b.telligent;https://www.btelligent.com/blog/proset-cross-und-up-selling-massnahmen/;B-Telligent;Laurentius Malter
30.07.2013;"            Tipps &amp; Tricks: arcplan und die Bibliotheken        ";, © 2021 b.telligent;https://www.btelligent.com/blog/tipps-tricks-arcplan-und-die-bibliotheken/;B-Telligent;Stefan Kersten
18.07.2013;            Customer-Intelligence-Quick-Check        ;, © 2021 b.telligent;https://www.btelligent.com/blog/customer-intelligence-quick-check/;B-Telligent;Laurentius Malter
26.06.2013;"            Intelligentes Cross- &amp; Up-Selling durch den Einsatz von Next-Best-Activity        ";, © 2021 b.telligent;https://www.btelligent.com/blog/intelligentes-cross-up-selling-next-best-activity/;B-Telligent;Laurentius Malter
19.06.2013;            Step by step: Einrichten des IIS7 und der für arcplan notwendigen Komponenten        ;Überprüfen ob der IIS7 installiert ist, , © 2021 b.telligent;https://www.btelligent.com/blog/step-by-step-der-iis7-und-weitere-arcplan-komponenten/;B-Telligent;Stefan Rann
18.06.2013;            Herzlich willkommen bei Customer Intelligence Insights        ;, © 2021 b.telligent;https://www.btelligent.com/blog/willkommen-bei-customer-intelligence-insights/;B-Telligent;Laurentius Malter
15.05.2013;"            Tipps &amp; Tricks: Sortieren von Tabellen und Hierarchien        ";, © 2021 b.telligent;https://www.btelligent.com/blog/tipps-tricks-sortieren-von-tabellen-und-hierarchien/;B-Telligent;Stefan Rann
02.04.2013;"            Tipps &amp; Tricks: Wechsel zwischen Hierarchien        ";, © 2021 b.telligent;https://www.btelligent.com/blog/tipps-tricks-den-wechsel-zwischen-hierarchien-meistern/;B-Telligent;Stefan Kersten
05.02.2013;"            Tipps &amp; Tricks: METADATENLESEN() und METADATENSETZEN()        ";, © 2021 b.telligent;https://www.btelligent.com/blog/tipps-tricks-die-funktionen-metadatenlesen-setzen/;B-Telligent;Arno Velden
18.12.2012;            Best Practice: arcplan Entwicklungsrichtlinien zum Aufbau von Dokumenten        ;Eine der für uns mittlerweile wichtigsten Regeln ist eine funktionsabhängige Färbung der Objekte. Dazu legen wir auf Ebene 1 die Farben und ihre Bedeutung fest., , Eine weitere Regel ist das Gruppieren von Objekten. Um die Ebenen übersichtlicher zu gestalten, ist es sinnvoll, zusammengehörige Objekte z.B. bei der Datenabfrage auch nah beieinander anzuordnen und sie durch ein weiteres Objekt, das als Rahmen dahintergelegt wird, als zusammengehörig zu kennzeichnen., , , © 2021 b.telligent;https://www.btelligent.com/blog/best-practice-arcplan-richtlinien-zum-dokumentenaufbau/;B-Telligent;Stefan Rann
26.11.2012;"            Tipps &amp; Tricks: Wechseln zwischen Datenquellen        ";"Eine Möglichkeit ist das Schreiben eigener SQL-Statements. Hierzu wird in einem Einzelfeld das Statement zusammengesetzt und über die SELECT(;)-Formel an die Datenbank geschickt., Die Nutzung der SQL-Statements bietet verschiedene Vor- und Nachteile. Vorteilhaft ist in der Regel, dass die Abfragen sehr performant sind und sie können durch das Zusammensetzen einzelner Textbausteine sehr variabel aufgebaut werden. Zudem können über das SQL-Statement sehr komplizierte Abfragen durchgeführt werden und ein Wechsel der Datenbanken ist theoretischauch während der Laufzeit möglich, sollte aber aus Konsistenzgründen vermieden werden. Nachteilig ist vor allem die schlechtere Übersicht und Lesbarkeit der Anwendung und es gibt kein Syntaxhighlighting für die SQL-Statements, was das Lesen erschwert. Zudem müssen Anwendungsdesigner zwingend SQL-Kenntnisse haben und die einfache Handhabung der Verbindungspfeile zur Abfrageerstellung entfällt. , , © 2021 b.telligent";https://www.btelligent.com/blog/tipps-tricks-wechsel-von-datenquellen-bei-arcplan/;B-Telligent;Stefan Kersten
18.10.2012;            Best Practice: Warum Pfeile nicht über Ebenen gezogen werden sollten        ;, © 2021 b.telligent;https://www.btelligent.com/blog/best-practice-die-beziehung-zwischen-pfeil-und-ebene/;B-Telligent;Arno Velden
28.08.2012;            Neu mit arcplan 7: Die Funktion AUSDRUCKERSETZEN()        ;", Zur Umsetzung dieser Anforderung muss in das MDX an geeigneter Position das Ausdruck NON EMPTY in das Statement eingebunden werden. Die Funktion AUSDRUCKERSETZEN arbeitet wie folgt: , AUSRUCKERSETZEN(&lt;Suchausdruck&gt;;&lt;Zu ersetzender Ausdruck&gt;;&lt;Anzahl der Ersetzungen, optinal&gt;), Somit verhält sich die Funktion ähnlich der in Word vorhandenen Funktionalität zum Suche und Ersetzen von Texten., Ermittlung des zu ergänzenden Ausdrucks„NON EMPTY“ hinter „ON COLUMNS,“ einfügen, Ermittlung des Ankerpunktes für die Suchen und Ersetzen Funktionalität.Tipp: Da das MDX von arcplan dynamisch generiert ist, ist auf die Eindeutigkeit besonderer Wert zu legen, damit die Änderung immer zu einem korrekten Ergebnis führt. ";;;
20.07.2012;"            Tipps &amp; Tricks:Der S-Verweis zur Zuordnung von Informationen        ";"arcplan bietet mehrere Möglichkeiten, einen klassischen S-Verweis, wie er in Excel oft genutzt wird, aufzubauen. Eine dieser Möglichkeit ist unter dem BlogeintragTipps &amp; Tricks: Die Wiederholen-Schleife als Performancekillerzu finden., Die hier beschriebene Methode arbeitet über das Kreuzprodukt zweier Tabellen und stellt als Mengenoperation einen „Left Outer Join“ dar, wogegen die Schnittmenge einen „Inner Join“ darstellt., Nehmen wir an, die Daten kommen aus zwei unterschiedlichen Datenquellen. In der einen liegen die Informationen über Stück- und Verkaufspreis, in der anderen die Daten über die Bestellmengen., In unserem Beispiel kommen die Daten aus dem SAP-System, daher wird das Kreuzprodukt über die Schlüssel hergestellt; dies ist vor allem dann wichtig, wenn die Bezeichnungen der Produkte sich ändern bzw. von vornherein unterschiedlich sind., , Für das Kreuzprodukt aus den Schlüsseln werden die Informationen aus der Tabelle „Schlüssel_1“ und „Stückpreis“ in Zeilen transformiert (OBJ29 und OBJ30). In der Tabelle selbst wird bei gleichem Inhalt in Spalte OBJ28 und Zeile OBJ29 der Wert aus Zeile OBJ30 genommen. Bei Ungleichheit bleibt die Zelle leer., Das Ergebnis des Kreuzprodukts kann über die Funktion MAX() in eine Spalte zusammengezogen werden., , , Bei der Nutzung der Funktion ist zu beachten, dass arcplan alle Informationen der Berichte, die für die Anzeige der Daten auf der Report-Ebene benötigt werden, in den Speicher lädt. Wenn also mehrere Kreuzprodukte in der Form der oberen Abbildung gebildet werden, werden die Tabelleninhalte alle in den Speicher geladen., Um dies zu verhindern, kann die Funktion auch direkt in einer Spalte abgebildet werden. Hierzu sind die Funktionen der oberen Abbildungen in einem Objekt koordiniert., , , Prinzipiell gilt allerdings immer: Was über die Abfrage schon zusammengefügt werden kann, sollte auch dort zusammengefügt werden., , © 2021 b.telligent";https://www.btelligent.com/blog/tipps-tricks-der-s-verweis-zur-informationszuordnung/;B-Telligent;Stefan Rann
24.04.2012;"            Tipps &amp; Tricks: Stammdatenabfrage bei SAP BW beschleunigen        ";Somit sieht das Menü nach außen für den End-User vollkommen gleich aus., Im Hintergrund wird jedoch der für kleine Hierarchien deutlich performantere Zugriff über den Funktionsbaustein erzeugt. Der Join mit den Fakten wird dadurch vermieden., , , © 2021 b.telligent;https://www.btelligent.com/blog/tipps-tricks-stammdatenabfrage-sap-bw-beschleunigen/;B-Telligent;Markus Sontheimer
06.03.2012;"            Tipps &amp; Tricks: Prüfen, ob ein Objekt vorhanden ist        ";"In einer Anwendung gibt es immer zentrale Funktionen, die einen generischen Ablauf steuern. Diese Funktionen rufen aus dem zentralen Bereich berichtsspezifische Funktionen auf. Wenn ein Bericht z.B. nur bei Bedarf Daten abfragt, aber bei dem Aufruf eines Favoriten direkt aktualisiert werden soll, muss die Aktualisierung von dem zentralen Dokument getriggert werden., Dabei ergibt sich die Problemstellung, dass es auch Berichte geben kann, welche immer aktuell sind. Die zentrale Funktion darf dann den Button zur manuellen Aktualisierung nicht aufrufen, da dieser in diesem Bericht nicht vorhanden wäre. Folglich käme es zu einer Fehlermeldung., Ein Lösungsweg für diese Problematik besteht darin, eine Steuerungstabelle in einer Datenbank zu pflegen, welche die Information enthält, wann eine manuelle Aktualisierung notwendig bzw. sinnvoll ist. Im Rahmen der Berichtsaktualisierung könnte diese Information dann abgefragt werden. Allerdings ist der Aufwand zur Pflege dieser Informationen relativ hoch. Die Funktionalität und auch die Möglichkeit der Berichtsauswertung sind trotz des Aufwandes und der Fehleranfälligkeit gegeben. Eleganter ist allerdings der Weg über eine dynamische Prüfung, ob ein Objekt vorhanden ist.arcplan stellt dafür keine Funktion bereit. Doch kann die Prüfung mit folgender Formel erfolgen:, ISTLEER(OBJEKT(&lt;Bericht_Aktualisieren_Button&gt;;&lt;Berichtsname&gt;)), Diese Formel ermittelt aus dem Bericht &lt;Berichtsname&gt;das Objekt mit dem Namen &lt;Bericht_Aktualisieren_Button&gt;.Wenn dieses Objekt nicht existiert, gibt die Prüfung mit ISTLEER eine 1 zurück, ansonsten eine 0. Wichtig ist hierbei, dass die Funktion ISTLEER() natürlich den Inhalt eines Objektes prüft. Im Beispiel der Aktualisierung ist das zu prüfende Objekt ein Button. Der Inhalt eines Buttons ist der Text, der in der „Ausgabe“-Eigenschaft (der Beschriftung) des Buttons hinterlegt ist. Dieser darf also nicht leer sein. Für alle anderen Objekte gilt daher ebenso: Nur mit vorhandenem Inhalt kann eine Überprüfung funktionieren., Eine Prüfung, ob ein Objekt leer ist oder nicht, wird häufig auch mit folgendem Konstrukt durchgeführt:, OBJEKT(&lt;Bericht_Aktualisieren_Button&gt;;&lt;Berichtsname&gt;) = LEER(), Diese Art der Prüfung funktioniert jedoch nicht für den hier beschriebenen Fall, da ein „= LEER()“ nur auf Objekte mit dem „Berechnen-Ereignis“ angewendet werden kann. Daher sollte zur Prüfung nur die eingangs genannte Formel genutzt werden., , , © 2021 b.telligent";https://www.btelligent.com/blog/tipps-tricks-vermeidung-von-fehlermeldungen/;B-Telligent;Stefan Kersten
07.02.2012;"            Tipps &amp; Tricks: Die Wiederholen-Schleife als Performancekiller        ";Die Wiederholen-Schleife ist grundsätzlich eine schnelle Funktion, die erst bei mehreren tausend Durchläufen merklich langsam wird. Die derzeitige arcplan-Realisierung bei Zuweisung (wie bei dem Zähler notwendig OBJx := OBJx +1) führt aber dazu, dass abhängig von der summierten Anzahl der Objekte der geöffneten Dokumente die Laufzeit bei der Zuweisung von Werten anwächst. (Dies ist ein Problem der grundlegenden Programmierung von arcplan Enterprise bei der Ermittlung des Objekts im Objektstack, der alle Objekte aller offenen Dokumente enthält.) Bei komplexen Anwendungen ist daher die Laufzeit der Wiederholen-Schleife stark erhöht, so dass diese unter Umständen selbst für nur wenige Einträge Sekunden statt Millisekunden läuft. Daher sollten alle Funktionalitäten für eine gute Performance möglichst ohne Wiederholen-Schleife umgesetzt werden., Nachfolgend ein Beispiel, das häufig mit der Wiederholen-Schleife realisiert wird, obwohl eine Umsetzungsoption mit anderen Formeln existiert., Der S-Verweis ist durch den Umgang mit Excel bekannt: Sollen Werte aus einer Basis-Tabelle zu abhängigen Werten ermittelt werden, z.B. um einzelne Steuerparameter zu lesen, wird in Excel der S-Verweis genutzt. Einzelne Schlüssel werden angegeben, zu denen die abhängigen Werte ermittelt werden sollen., In arcplan ist dies auch mit einer Funktion möglich, dies ist aber nicht sofort ersichtlich. Daher wird häufig eine Kombination der Funktionen WIEDERHOLEN und FILTER eingesetzt., Eleganter zu lösen ist dies durch die Funktion SCHNITTMENGE, die den aus Excel bekannten S-Verweis 1:1 nachbilden kann., , , Im Beispiel enthält OBJ5 den Schlüssel und OBJ3 den zugehörigen Parameter. In OBJ2 stehen die Schlüssel, für welche die Werte benötigt werden. OBJ4 enthält den eigentlichen S-Verweis, der aus OBJ3 die Werte zugehörig zu den in OBJ2 angegebenen Schlüsseln ermittelt. Diese Funktion kann für mehrere tausend Einträge unabhängig von der Komplexität der Anwendung genutzt werden und bietet auch bei großen Inhalten eine sehr gute Performance., , © 2021 b.telligent;https://www.btelligent.com/blog/tipps-tricks-performanceoptimierung-durch-arcplan/;B-Telligent;
04.01.2012;"            Tipps &amp; Tricks: Eine Anwendung aus der Anwendung schließen        ";"Parallel zu MS Word 2010 war bei älteren Versionen als arcplan Enterprise 5 ein Schließen über das Menü, vergleichbar mit MS Word 2010 über „Datei“ und „Beenden“, notwendig, um die Verbindungen zur Datenbank vollständig zu beenden. Wird dies nicht durchgeführt, können ggf. Objekte in SAP gesperrt oder Ressourcen unnötig geblockt sein., Da dies aber ab arcplan Enterprise 5 und jünger nicht mehr notwendig ist, handelt es sich hierbei nur noch um ein kosmetisches Vorgehen., Die Funktion BEENDEN beendet die Verbindung zur arcplan-Session. Jedoch wird dadurch nicht, wie von anderen Programmen gewohnt, das Browserfenster mit dem Java-Client geschlossen. Dies muss manuell durch den Anwender durchgeführt werden. Damit hat die Funktion BEENDEN für sich alleine in einer Webanwendung keinen funktionalen Sinn, da mit dem Schließen des Browserfensters auch direkt die arcplan-Session beendet würde., Um die Funktion im Web zum wirklichen „Beenden“ einer Anwendung zu nutzen, ist es notwendig, noch den Browser zu schließen. Dafür kann auf den JavaScript-Befehl „window.close()“ in der ausführenden Website zurückgegriffen werden. Folgender JavaScript-Code muss hierfür in der Startseite der Anwendung ergänzt werden (ab arcplan Enterprise 7 wird dieser auch nicht mehr durch den Administrator überschrieben):, …. &lt;HEAD&gt; …&lt;script type=""text/javascript""&gt;…Function FensterSchliessen() { Window.close(); } … &lt;/script&gt; … &lt;/HEAD&gt; &lt;BODY&gt;…, Über die in arcplan Enterprise 6 eingeführte Formel STARTENJS ist diese JavaScript-Funktion nun einfacher aufrufbar. Der Befehl lautet: STARTENJS(„FensterSchliessen“), Kombiniert bilden diese beiden Funktionen eine saubere Möglichkeit, eine Anwendung aus sich selbst heraus zu schließen:, STARTENJS(„FensterSchliessen“) BEENDEN(), Vielleicht ist Ihnen aufgefallen, dass nach dem Schließen des Browsers noch die Session beendet wird. Das ist nicht zwingend notwendig, sorgt aber dafür, dass direkt nach dem Schließen des Fensters die Session beendet wird. Ansonsten könnte es auftreten, dass noch nachgelagerte Funktionen ausgeführt werden, die ein Schließen der Session verlängern., Wird beispielsweise eine Datenbankabfrage nach STARTENJS ausgelöst, wird die Session erst nach der kompletten Ausführung beendet. Mit dem direkten Beenden der Anwendung wird dies aktiv unterbunden. (arcplan Enterprise erkennt das Schließen erst, wenn alle folgenden Aktionen fertig ausgeführt wurden. Das heißt, selbst wenn der Browser geschlossen ist, könnte arcplan noch Aktionen auf dem Server ausführen. Die Anwendung bzw. die User-Sessions werden nicht auf dem Client, sondern auf dem Server ausgeführt.), , © 2021 b.telligent";https://www.btelligent.com/blog/tipps-tricks-anwendung-aus-anwendung-schliessen/;B-Telligent;Stefan Kersten
31.10.2011;            Best Practice: Eindeutigkeit von Pop-up-Menüeinträgen sicherstellen        ;Pop-up-Menüs werden in arcplan häufig verwendet, um dem Nutzer einer Anwendung eine kontextbezogene Selektion eines Wertes anzubieten. Sie finden häufig Verwendung bei Dateneingaben, in denen nur ein bestimmter Wertebereich zulässig ist, oder dienen der Auswahl einer gewünschten Aktion beim Anklicken eines Objektes., Häufig kommt es hierbei vor, dass die nutzerfreundliche Darstellung der Inhalte des Pop-up-Menüs nicht eindeutig ist., Da bei Pop-up-Menüs in arcplan nicht die ausgewählte Zeilennummer zurückgegeben werden kann, wie es bei den übrigen Menüobjekten möglich ist (AKTUELLEZEILE()), muss die Eindeutigkeit der Auswahl auf andere Weise sichergestellt werden. Im Folgenden wird anhand eines Beispiels mit fiktiven Personennamen die Problematik eines Pop-up-Menüs beschrieben und eine praxisbewährte Lösung vorgestellt., In der beschriebenen Situation (siehe Abbildung 1) wird dem User über das Pop-up-Menü eine Auswahl von Personennamen gegeben. Diese Auswahl wird zur Weiterverarbeitung in ein Einzelfeld geschrieben und ist im Nachhinein schwer zuzuordnen, weil die enthaltenen Texte, die den Rückgabewert des Pop-up-Menüs darstellen, nicht eindeutig sind. In der Praxis muss also sichergestellt sein, dass Personen mit gleichen Namen technisch weiterhin eindeutig identifiziert werden können., , Um die Eindeutigkeit innerhalb des Pop-up-Menüs sicherzustellen, kann dem Namen (z.B. Hans Müller) ein eindeutiger Schlüssel vorangestellt werden. In diesem Beispiel ist dies die Personalnummer (4611Hans Müller). Entscheidend ist, dass die Nummer in allen Einträgen eine feste Anzahl an Zeichen besitzt. In unserem behandelten Beispiel besteht die Personalnummer immer aus vier Zeichen. Für das Pop-up-Menü wird als Basis folgende Tabelle verwendet, um die eindeutige Identifizierung zu gewährleisten:, , , Um diese technischen Informationen vor dem Endanwender zu verbergen, kann die etwas versteckte Funktion des „Ausblendens“ in arcplan verwendet werden., In dem Pop-up-Menü kann diese Einstellung nicht durchgeführt werden, allerdings lässt sich das „Ausblenden“ über eine Konfiguration der zugrundeliegenden Objekte erreichen. Hier sorgt der Eintrag „\04“ unter der Formateinstellung „Präfix“ – wie in der nächsten Abbildung dargestellt – für den gewünschten Effekt: die ersten vier Zeichen werden ausgeblendet und sind somit für den Endanwender nicht weiter sichtbar., , , Dadurch wird für die Anzeige zwar der vierstellige Schlüssel abgeschnitten, existiert aber weiterhin im Hintergrund., Nach dem Ausführen des Pop-up-Menüs wird also wieder der komplette Inhalt der Selektion angezeigt. In der nachfolgenden Abbildung ist zu sehen, dass auch der Schlüssel im Ergebnisobjekt angezeigt wird. Somit ist der gewählte Eintrag über den Schlüssel eindeutig zu identifizieren., , , Ein weiterer Anwendungsfall für die Verwendung von eindeutigen Schlüsseln zur Weiterverarbeitung der Selektion tritt häufig in mehrsprachigen Anwendungen auf. Hier ist die Zuordnung der Selektion im richtigen Sprachkontext relativ komplex. Die Verwendung einer ID erleichtert also auch in diesem Fall die Verarbeitung des Ergebnisses., Die Funktion arbeitet auch in die andere Richtung. Dreht man den Schrägstrich um („/04“), werden die ersten vier Zeichen angezeigt. Gleiches gilt für das Postfix. Fügt man die Zeichen „\04“ im Postfix ein, werden die letzten vier Zeichen abgeschnitten. Fügt man „/04“ im Postfix ein, werden nur die letzten vier Zeichen angezeigt., , © 2021 b.telligent;https://www.btelligent.com/blog/best-practice-eindeutigkeit-von-popup-menueeintraegen/;B-Telligent;
28.09.2011;"            Tipps &amp; Tricks: Unnötige Datenbankabfragen bei Einschränkungen mit Einzelfeld verhindern        ";Die Aktualisierungsroutinen in arcplan funktionieren gewöhnlicher Weise sehr gut. Bei hochkomplexen Anwendungen ist es allerdings teilweise nötig, die Aktualisierung manuell zu beeinflussen, um eine performante Anwendung sicherzustellen., In arcplan lassen sich hierfür verschiedene Arten von dynamischen Datenbank-Einschränkungen nutzen. Hierbei ist es wichtig zu wissen, dass die Abhängigkeiten in arcplan immer automatisch aufgelöst werden, wenn sich eine Einschränkung verändert: die automatische Aktualisierung. Wenn das Verhalten der automatischen Aktualisierung nicht detailliert berücksichtigt wird, kann es durch dieses Feature zu nicht gewünschten Aktualisierungen auf der Datenbank kommen., , , Das Objekt mit der manuellen Eingabe wird wieder mit einem bereits vorhandenen Wert gefüllt. Die arcplan Aktualisierungsroutine wertet dies als eine Veränderung des Objektes, weil ein Ereignis ausgelöst wurde. Dadurch wird die Datenbankabfrage erneut angestoßen, obwohl der Anwender bereits im in Vorfeld weiß, dass diese Abfrage keine neuen Ergebnisse liefern wird., Das gleiche Verhalten tritt auf, wenn statt einer manuellen Eingabe z.B. eine sich aktualisierende Formel genutzt wird., , , Das Radio-Button Objekt enthält eine fixe Formel. Wird das Objekt in irgendeiner Weise aktualisiert – sei es von einem anderen Objekt oder weil sich der Inhalt ändert – wird wiederum automatisch die Datenbankabfrage ausgeführt, obwohl keine signifikanten Veränderungen stattgefunden haben., Umgehen lässt sich dieses Verhalten, indem die Aktualisierung entkoppelt wird. Dies sollte aber nur bewusst und im Einzelfall durchgeführt werden, nämlich wenn Aktualisierungen unnötigerweise ausgeführt werden., , , Das Radio-Button Objekt wird entsprechend erweitert: Nach einer Aktualisierung wird nun nur die Auswahl kopiert, wenn sich der Wert ergebnisbeeinflussend geändert hat. Somit ist sichergestellt, dass auch bei einer Änderung des Inhalts immer der korrekte Wert angezeigt wird. Durch die Kopie beim Umschalten des Objektes wird die neue Auswahl in die Einschränkung übertragen., Somit ist die Einschränkung der Datenbank immer aktuell. Abfragen werden jedoch nur bei Bedarf ausgeführt., Die Entkopplung des automatischen Aktualisierungsverhalten in arcplan sollte nur im Einzelfall angewendet, um eine dauerhaft qualitativ hochwertige Wartung der Anwendung zu gewährleisten: Es muss klar sein, dass die Änderungen das Ergebnis nicht beeinflussen, die Aktualisierung muss zweifelsfrei als „unnötig“ kategorisiert sein., , © 2021 b.telligent;https://www.btelligent.com/blog/tipps-tricks-unnoetige-datenbankabfragen-verhindern/;B-Telligent;Stefan Kersten
30.08.2011;"            Tipps &amp; Tricks: Doppelte Einträge aus einer Spalte/Zeile entfernen        ";Um aus einer Liste von Daten mit doppelten Einträgen eine Liste ohne Duplikate zu erstellen, gibt es bei der Arbeit mit arcplan unterschiedliche Methoden.  In vielen Fällen kann eine entsprechend gruppierte Abfrage gegen die Datenbank diese Aufgabe übernehmen. Trotzdem gibt es immer wieder Fälle, in denen es erforderlich ist, die Duplikate mithilfe von arcplan Funktionen zu entfernen. Zwei einfache Methoden, die dasselbe Endergebnis liefern sind:, Im Folgenden sollen die zwei arcplan internen Methoden an einem Beispiel verdeutlicht werden:, Als Ausgangssituation dient eine Spalte mit Städtenamen, die mehrfach in der Liste vorkommen., , , © 2021 b.telligent;https://www.btelligent.com/blog/tipps-tricks-entfernung-von-duplikaten-aus-listen/;B-Telligent;Stefan Rann
01.08.2011;"            Neu bei arcplan 7: Die Option ""Nur geänderte Dateien übertragen""        ";"Eine kleine, aber interessante Funktion bringt der arcplan Administrator seit der Version 7 mit., In den vorherigen Versionen wurden bei der Aktualisierung einer Anwendung durch den Administrator immer alle Dateien aus dem lokalen Verzeichnis auf den Server übertragen. Dies kann insbesondere bei großen Anwendungen und der Verwendung des xml-Dateiformates recht lange dauern: In diesem Fall wird nämlich sowohl die Übertragung zum Server als auch die Konvertierung in das binäre Dateiformat für alle Dateien durchgeführt. Und das auch wenn nur eine einzelne Datei in der Entwicklung bearbeitet wurde., Besonders aus der Sicht eines Entwickler ist die unvorteilhaft, wenn dieser eine Neuentwicklung oder Fehlerbehebung online testen möchte, da hierbei zusätzlich mehrere Iterationen notwendig sind. Seit der Version 7 bietet der Administrator in dem Dialog zur Anwendungsaktualisierung die neue Option: ""Nur geänderte Dateien übertragen"", , , Mit dem Setzen dieser Option werden dann lediglich die geänderten Dateien zum Server übertragen, so dass der Prozess der Anwendungsaktualisierung deutlich schneller erfolgt., , © 2021 b.telligent";https://www.btelligent.com/blog/arcplan-7-option-nur-geaenderte-dateien-uebertragen/;B-Telligent;Stefan Rann
26.07.2011;            Herzlich Willkommen auf dem b.telligent Reporting Blog         ;, © 2021 b.telligent;https://www.btelligent.com/blog/herzlich-willkommen/;B-Telligent;Stefan Kersten
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
;;;;;
