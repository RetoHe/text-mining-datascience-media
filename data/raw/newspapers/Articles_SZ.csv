Datum;Titel;Text;Link;Quelle;Autor 
06.11.2020;Karte zeigt Studiengänge rund um KI und Data Science;"München (dpa/tmn) - Schulabsolventen, die eine Karriere in den Technologiefeldern Künstliche Intelligenz oder Data Science anstreben, können sich ab sofort auf einer Landkarte einen Überblick über passende Studiengänge verschaffen. Die KI-Landkarte der Plattform Lernende Systeme, die bei der Akademie der Technikwissenschaften (acatech) angesiedelt ist, hat eine neue Rubrik ""Studiengänge"". Dort werden derzeit für Deutschland 170 Hochschulen aufgelistet, die Studiengänge rund um KI und Data Science anbieten, informiert die Plattform. Nutzer können die Suchergebnisse verfeinern und sie zum Beispiel nach Hochschultyp, Abschluss und thematischem Schwerpunkt filtern. Datenbasis der auf der KI-Landkarte aufgeführten Studiengänge ist den Infos zufolge der Hochschulkompass der Hochschulrektorenkonferenz (HRK).";https://www.sueddeutsche.de/karriere/arbeit-karte-zeigt-studiengaenge-rund-um-ki-und-data-science-dpa.urn-newsml-dpa-com-20090101-201106-99-236761;sz.de;
10.03.2020;Dürr übernimmt Mehrheit an Datenanalyse-Spezialisten;Bietigheim-Bissingen (dpa/lsw) - Mit der Übernahme einer Spezialfirma für Datenanalyse untermauert der Maschinen- und Anlagenbauer Dürr seine Ambitionen im Digitalgeschäft. Man habe mit Wirkung zum 9. März die Mehrheit an der Böblinger Firma Techno-Step übernommen, teilte Dürr am Dienstag mit. Techno-Step entwickelt den Angaben zufolge Systeme zur Analyse von Prozessdaten, die in den Lackieranlagen von Autoherstellern zum Einsatz kommen. Durch die Kombination von eigener und Techno-Step-Technik könnten künftig auch ältere oder fremde Anlagen so nachgerüstet werden, dass die Daten ausgetauscht und analysiert und die Anlagen dadurch verbessert werden könnten, teilte Dürr weiter mit. Techno-Step hat laut Mitteilung im Geschäftsjahr 2018/19 einen Umsatz von rund 3,5 Millionen Euro erzielt. Gründer Markus Schultheiß bleibt auch nach der Übernahme Geschäftsführer. Zu finanziellen Details wurden keine Angaben gemacht.;https://www.sueddeutsche.de/wirtschaft/maschinenbau-bietigheim-bissingen-duerr-uebernimmt-mehrheit-an-datenanalyse-spezialisten-dpa.urn-newsml-dpa-com-20090101-200310-99-270810;sz.de;
25.06.2020;Mit Tempo gegen die Seuche;Eine Woche nach Einführung sind die ersten Infektionsmeldungen eingegangen. Damit die App ein Erfolg wird, sind drei Faktoren entscheidend. Mehr als zwölf Millionen Mal haben Menschen in Deutschland die Corona-Warn-App heruntergeladen. Gut eine Woche nach Einführung gingen am Dienstagabend auch die ersten Meldungen von Infektionsfällen ein. Diese Liste von Schlüsseln wird nun mit den gesammelten Kontakten jedes App-Nutzers abgeglichen. In der Datenbank, die im Netz einsehbar ist, stehen bereits mehrere hundert Einträge. Wie viele infizierte Menschen tatsächlich dahinter stehen, lässt sich nicht genau sagen. Denn um zu verhindern, dass einzelne Personen identifiziert werden können, hat die App auch einige falsche Schlüssel generiert und in der Datenbank hinterlegt. Realistischerweise dürfte die Zahl der infizierten Nutzer sich aber noch im niedrigen zweistelligen Bereich bewegen. Doch egal wie viele es sind: Die ersten App-Nutzer dürften mittlerweile die Aufforderung bekommen haben, sich testen zu lassen. Die App kann nun den Zweck erfüllen, für den sie geschaffen wurde.;https://www.sueddeutsche.de/digital/coronavirus-app-datenanalyse-mit-speed-gegen-die-seuche-1.4946797;sz.de;Sören Müller-Hansen
11.11.2020;Wählen wie der eigene Nachbar;"Die Polarisierung in den USA nimmt weiter zu. Die große Mehrheit der Wähler lebt inzwischen in einer Gegend, in der eine Partei klar dominiert. Mit welchen Faktoren das zu tun hat. Einerseits ging diese Wahl sehr knapp aus, sind die Verhältnisse so unklar wie selten zuvor. Doch andererseits sah die politische Landkarte in den USA noch nie so sortiert und aufgeräumt aus wie 2020. Dann nämlich, wenn der Blick vom Gesamtergebnis weggeht, auch am denkbar knappen Wahlausgang in entscheidenden Staaten wie Pennsylvania nur kurz vorbeischweift und schließlich an den kleinräumigen Daten hängen bleibt. Immer weniger Amerikaner leben in einem County, in dem sich Demokraten und Republikaner in etwa die Waage halten. Immer mehr US-Bürger leben also in einer Gegend, die entweder ganz klar rot oder ganz klar blau wählt. Bei der Präsidentschaftswahl 2016 lebten erstmals weniger als ein Viertel der Wähler in einem Kreis, in dem beide Kandidaten weniger als zehn Prozentpunkte trennten - in dem es also ein Wahlergebnis gab, das etwa 55 zu 45 Prozent entsprechen würde. Nun ist dieser Anteil der Wähler aus ausgeglichenen Gebieten nochmals gesunken, auf wohl weniger als 21 Prozent, es sind weiterhin nicht alle Stimmen ausgezählt. Ein Tiefststand mindestens seit dem Jahr 1992, so weit gehen die vorliegenden Daten zurück. Mehr als die Hälfte der US-Wähler lebt in sogenannten ""Landslide Counties"" Mehr als drei Viertel der Amerikaner gaben ihre Stimme in einem Landkreis ab, den eine der beiden Parteien mit großem Abstand gewann. Deutlich mehr als die Hälfte der Wähler lebt sogar in einem Kreis, bei dem der Vorsprung des Siegers auf den Verlierer mindestens 20 Prozentpunkte beträgt. Landslide counties nennen Politikwissenschaftler diese Gegenden, weil die Wahlergebnisse dort einem Erdrutsch gleichkommen, der die unterlegene Partei begräbt. Die politische Landkarte der USA enthält zunehmend dunkelblaue und dunkelrote Flecken, die blass gefärbten Flächen schrumpfen. Diese geografische Verteilung der Mehrheitsverhältnisse ist ein wichtiger Indikator für die Polarisierung in einem Land. Was wie eine abstrakte Metrik für Akademiker erscheint, hat handfeste Folgen: Wer in einem Landslide County lebt, trifft im Alltag überwiegend Leute mit ähnlicher politischer Einstellung. Republikaner haben republikanische Nachbarn, sie sitzen beim Elternabend neben anderen republikanischen Eltern und schwitzen im Fitness-Studio ebenfalls gemeinsam mit Republikanern. Das konträre politische Lager existiert vor allem in der Fremde, der Austausch mit Andersdenkenden wird zur Seltenheit. Eine polarisierte Landschaft bietet zudem wenig Anreiz für Politiker, nach Ausgleich und Kompromissen zu streben. Einen Halbe-Halbe-Wahlkreis gewinnt nur, wer auch ein paar Wähler aus dem gegnerischen Lager überzeugt. In einem klar ausgerichteten Bezirk geht es hingegen eher darum, die eigene Anhängerschaft zu begeistern. Gerade bei parteiinternen Vorwahlen, wie sie in den USA üblich sind, gelingt das häufig radikalen Kräften am besten. In den Großstädten haben die Republikaner keine Chance. Die Polarisierung folgt in Amerika klaren Mustern. Das wichtigste: Stadt und Land. Ländliche Gegenden wählen meist überwiegend republikanisch, in den Großstädten gewinnen fast immer die Demokraten. ""Es gibt in Amerika keine dicht besiedelte Großstadt, die regelmäßig Republikaner wählt"", schreibt der Autor Ezra Klein in seinem Buch ""Der tiefe Graben"". Dieses Phänomen hat auch zur Folge, dass viele Karten mit den Wahlergebnissen in diesen Tagen ein verzerrtes Bild transportieren: Färbt man die Vereinigten Staaten Landkreis für Landkreis nach der dort siegreichen Partei ein, dann entsteht viel mehr rote Fläche als blaue. Dabei hat Joe Biden etwa vier Millionen Stimmen mehr bekommen als Donald Trump. Doch Bidens Stimmen konzentrieren sich eben in den dicht besiedelten Großstädten, während die republikanischen Wähler das weite Land bevölkern. Dieses Stadt-Land-Gefälle hat den Nebeneffekt, dass Biden zwar insgesamt die Mehrzahl der Stimmen gewonnen hat, aber nur in einer kleinen Minderheit der Kreise als Sieger aus der Wahl hervorgeht. Auch hier können die genauen Zahlen im Laufe der nächsten Tage und Wochen noch schwanken, doch mindestens 2600 Landkreise gehen mehrheitlich an Trump, während Biden weniger als 600 gewinnen kann - in denen freilich die Mehrzahl der Wähler lebt. Ähnlich war die Verteilung vor vier Jahren bei der Wahl zwischen Donald Trump und Hillary Clinton ausgefallen. Biden ist der ""King of Queens"" - hier gewann er mit 63 Prozentpunkten Vorsprung. So liegt Biden im New Yorker Stadtteil Queens sagenhafte 63 Prozentpunkte vor Trump. In Manhattan fällt das Ergebnis vermutlich noch deutlicher aus, dort zählen die Wahlhelfer noch. Im Prince George's County, das im Bundesstaat Maryland liegt und zum Großraum der Hauptstadt Washington, D. C., gehört, liegt Biden sogar 80 Prozentpunkte vorne. Zu den Extremen auf der anderen Seite des Spektrums zählt McPherson County in Nebraska. Dort liegt Trump 85 Punkte vor Biden - allerdings gaben dort nur 302 Wähler ihre Stimmen ab. In Queens waren es 947 000. Neben dem Gefälle zwischen Stadt und Land gibt es weitere Indikatoren, nach denen sich rote und blaue Regionen unterscheiden. So zeigt etwa eine Analyse der Neuen Zürcher Zeitung, dass Joe Biden in Texas vor allem in einkommensstarken Kreisen zulegen konnte, während Trump seine Führung in den ärmeren Regionen ausbaute. Zu einem besonders verlässlichen Marker entwickelt sich der Bildungsstand: Je höher der Anteil der Einwohner mit Hochschulabschluss, desto besser schneiden demokratische Kandidaten in der Regel ab. So konnte Joe Biden mitunter auch abseits der großen Metropolen Siege erringen - in Kleinstädten mit Hochschulen, Großkliniken und einer entsprechend gut ausgebildeten Wählerschaft. Doch in der Regel kommt eben alles drei zusammen - große Städte, großes Geld, ein hoher Bildungsstand - oder es ist nichts davon vorhanden. ""Jede dieser Teilungen stapelt sich auf die nächste"", schreibt Ezra Klein. Daraus entstehe das tief gespaltene Land, das die Vereinigten Staaten von Amerika heute mehr denn je sind.";https://www.sueddeutsche.de/politik/usa-trump-wahl-biden-polarisierung-1.5108861;sz.de;Christian Endt
22.11.2020;Daten sind nicht alles ;"Bei all den Appellen nach mehr Datenkompetenz sollten Forscher nicht vergessen: Erst mit genug Statistik- und Fachkenntnis werden aus bloßen Zahlen wissenschaftliche Erkenntnisse. Statt mit Spitzkelle und Pinsel suchen Archäologen heute auch mit Maus und Tastatur nach Hinweisen auf das Leben unserer Vorfahren. Nicht in südeuropäischen Lehmgruben stöbern sie dann nach Zeugnissen menschlicher Zivilisationen, sondern in großen Datensätzen. Die Altertumswissenschaft steht damit symptomatisch für eine Entwicklung in vielen wissenschaftlichen Disziplinen: Daten sind in immer größerer Zahl verfügbar, spielen selbst in den Geisteswissenschaften mittlerweile eine große Rolle - mitunter sogar als Trainingsdaten für maschinelles Lernen, einen Teilbereich der künstlichen Intelligenz. Big Data heißt das Schlagwort. Manche, denen dieser Begriff nicht bildungssprachlich genug ist, sprechen gar etwas hochtrabend von einer empirischen Revolution. Die stärksten Korrelationen nützen nichts, wenn sie nicht richtig interpretiert werden. Experten fordern deshalb immer wieder, dass sich auch die wissenschaftliche Ausbildung in vielen Fächern verändern muss. Archäologen müssten nicht nur Latein, sondern auch die Datenbanksprache SQL sprechen, heißt es dann sinngemäß. Auch in einem kürzlich veröffentlichten Positionspapier der Plattform Lernende Systeme, eines vom Bundesforschungsministerium gegründeten Expertengremiums, regen Wissenschaftler an, in Studiengängen jenseits der Informatik vermehrt ""Fähigkeiten im Datenmanagement"" zu vermitteln. Der ""Data Literacy"" müsse schon in der Schule ""ein breiter Raum eingeräumt werden"".";https://www.sueddeutsche.de/wissen/statistik-big-data-ki-wissenschaft-1.5122058;sz.de;Julia Rodemann
09.11.2020;Maschine, erkläre dich;"Angeblich gibt künstliche Intelligenz keinen Einblick in das, was sie lernt. Dabei ist es möglich, sie zu interpretieren - und sogar dringend nötig. Künstliche Intelligenz funktioniert in einigen Bereichen relativ reibungslos, sodass immer mehr Menschen sich auf ihre Vorhersagen verlassen. Im amerikanischen Gesundheitswesen zum Beispiel ist eine kommerzielle KI im Einsatz, die Patienten mit besonderen Bedürfnissen identifizieren soll. Sie werden in ein Programm aufgenommen, in dem sie intensiv betreut werden. Unter dem Strich hilft dies den Patienten, und es senkt die Kosten, da man unter 200 Millionen Menschen nur jene auswählt, die besonders davon profitieren. Ausschlaggebend sind die Kosten, die sie bereits verursachen. Weil aber schwarze Menschen im Durchschnitt weniger medizinische Hilfsleistungen beanspruchen - sie haben einen schlechteren Zugang zum Gesundheitssystem -, fallen sie durch dieses Raster. Die künstliche Intelligenz lernt, sie zu übergehen. Forscher um Ziad Obermeyer von der Universität Kalifornien haben diese technische Voreingenommenheit aufgedeckt. Sie gehen davon aus, dass nach einer Korrektur des Programms unter den chronisch kranken Patienten der Anteil der schwarzen Bevölkerung, die zusätzliche Leistungen bekämen, von knapp 20 auf etwa 50 Prozent steigen würde. Ähnliche Vorurteile wurden in KI-basierten Gesichtserkennungssystemen, Bewerbungsverfahren und in den Algorithmen hinter Websuchen festgestellt, heißt es in der Studie. Solche Algorithmen, die in großem Maßstab eingesetzt werden, seien meist proprietär. Die Firmen würden keinen Einblick gewähren in das, was sie auf die Leute loslassen, schreiben die Wissenschaftler. Das macht es unabhängigen Forschern schwer, sie zu analysieren. Sie können nur Missstände dokumentieren, aber nicht verstehen, wie sie zustande kommen. Einige KI-Anbieter sagen, dass es nur darauf ankäme, ob die Ergebnisse gut seien oder nicht. Nur manchmal sieht man dies dem Ergebnis nicht direkt an. Der Gesetzgeber fordert zwar beim Einsatz von KI in sensiblen Bereichen, dass Entscheidungen erklärbar oder interpretierbar sind, aber was damit gemeint ist, definiert er nicht. Und damit gibt er Unternehmen freie Hand. Um Fehler zu vermeiden, müssen Forscher wissen, wie eine künstliche Intelligenz arbeitet. Mit dem Erfolg von Deep Learning werden die maschinellen Lernmodelle komplexer und schwieriger zu erklären. Solche Systeme, auch künstliche neuronale Netze genannt, setzen sich aus Hunderten von Funktionen und Millionen von Parametern zusammen. Sie werden mit Daten trainiert und finden darin Muster, dank derer sie aus unbekannten ähnlichen Daten Schlüsse ziehen können. Die Lernprozesse gelten als Blackbox, weil eine derartige Vielzahl von Prozessen nicht überschaubar ist. Was in der Blackbox passiert, sei ein Rätsel, sagen viele Anwender. Aber das ist nicht ganz richtig. ""Grundsätzlich können Sie jedes KI-System bis zu einem gewissen Grad interpretieren"", sagt Christoph Molnar, der am Institut für Statistik der Ludwig-Maximilians-Universität München erforscht, wie man maschinelles Lernen interpretierbar machen kann. Wie dringend dies sei, hänge vom Anwendungsfall ab. ""Ein Fehler in einem System, das Produktempfehlungen für Kunden gibt, ist natürlich weniger folgenreich als in einem System, das einen Tumor diagnostizieren oder ein Auto lenken soll"", sagt er. Im letzteren Fall sei die Erklärbarkeit mitunter lebenswichtig. Die interpretierbare oder erklärbare KI ist deshalb ein eigener Forschungszweig geworden. Wissenschaftler der Technischen Universität Berlin (TU), des Fraunhofer Heinrich-Hertz-Instituts und der Singapore University of Technology and Design haben eine Technik entwickelt - die Spektrale Relevanzanalyse (SpRAy) - die es erlaubt, die Kriterien der Entscheidungsfindung zu visualisieren, zum Beispiel in Form von Heatmaps. Das ist, als würde man einer KI einen Laserpointer aufsetzen, sodass man am Lichtstrahl beobachten kann, wo die KI hinschaut. ""Was wir dafür brauchen, ist allerdings auch Einblick in das Modell - wir müssen also wissen, wie die Struktur des Netzwerks aussieht und ebenso, welche Parameter es nutzt"", sagt Klaus-Robert Müller von der TU Berlin. ""Unsere Erklärmethode funktioniert so gut, weil wir dies alles berücksichtigen und erst dadurch nachvollziehen können, wie das Netz entschieden hat."" Die Forscher fanden damit Mängel in einem KI-System, das vor einigen Jahren mehrere internationale Wettbewerbe zur Bildklassifikation gewonnen hatte. Es basiert auf einem statistischen Modell, den sogenannten Fisher-Vektoren (FV). Dieses FV-Model und ein Vergleichsmodell, ein eigens dafür trainiertes künstliches neuronales Netz, sollten in der Studie erkennen, ob auf einem Foto ein Pferd zu sehen ist oder nicht. Der Vergleich offenbarte erhebliche Abweichungen. Die Heatmap des Netzes zeigte Pferd inklusive Reiter rötlich an - er untersuchte also tatsächlich den Bildinhalt. Im Gegensatz dazu ließ die Heatmap des preisgekrönten FV-Modells die linke untere Ecke des Bildes in warmen Farben erscheinen. Dort steht auf vielen Bildern die Quellenangabe, zum Beispiel der Name eines Pferdefotoarchivs. Die KI identifizierte folglich nicht das Pferd, sondern lernte, es anhand der Quellenangaben zu erkennen. Es las im übertragenen Sinne einfach ab - wie von einem Spickzettel. Diese Strategie sei ein klarer Fall von ""Cleverer Hans""-Verhalten, wie Müller das nennt. Der clevere oder kluge Hans war ein Pferd zu Beginn des 20. Jahrhunderts. Ein Lehrer behauptete, dem Pferd Rechnen beigebracht zu haben, und tatsächlich beantwortete es Aufgaben korrekt, zum Beispiel, indem es mit einem Huf klopfte oder nickte. Damals fanden Wissenschaftler jedoch heraus, dass das Pferd nur feinste Nuancen in Gestik und Mimik des Lehrers deuten konnte - er verriet also unbeabsichtigt die Lösung. Die Forscher um Müller konnten bei dem FV-Netzwerk ihr Ergebnis noch einmal bestätigen, in dem sie die Quellenangaben aus dem Foto herausnahmen und in ein Bild mit einem Sportwagen einfügten - die KI erkannte nun in dem Auto ein Pferd. ""Solche KI-Systeme sind in der Praxis völlig nutzlos und sicherheitskritischen Bereichen enorm gefährlich"", warnt Klaus-Robert Müller. Das Problem daran ist, dass man nicht weiß, was eine KI im Alltag mit solchen trickreichen Lösungen alles anrichten kann. ""Es ist denkbar, dass bis zur Hälfte der derzeit eingesetzten KI-Systeme implizit oder explizit auf solche Strategien setzen."" Müller und seine Kollegen sind nicht die Einzigen, die an erklärbarer KI forschen - das Gebiet wächst rapide. Es gibt inzwischen eine ganze Reihe von vielversprechenden Ansätzen, sagt Christoph Molnar. Zum Beispiel Shapley Additive Explanations (SHAP): Diese Technik versucht zu identifizieren, auf welche Teile der Eingabedaten sich ein trainiertes Modell am meisten stützt. Forscher können zum Beispiel bei einer Gesichtserkennung die Augen oder die Nasen testweise aus den Bilddaten entfernen, um herauszufinden, ob die KI trotzdem noch Menschen auf Fotos erkennt. Reicht es, wenn Experten verstehen, was ein System macht? Oder muss der Anwender das nachvollziehen können? Sebastian Palacio vom Deutschen Forschungszentrum für Künstliche Intelligenz in Kaiserslautern arbeitet an KI-Modellen, die von vorneherein so konzipiert sind, dass sie leicht interpretierbar sind. Letzten Endes sind solche Modelle eine Rückbesinnung auf Expertensysteme. Gegenwärtig funktionieren typische KI-Modelle, insbesondere neuronale Netze so, dass sie nicht wissen müssen, was etwa einen Tumor auf einer Aufnahme ausmacht - sie lernen es anhand von Beispielen. Bindet man Experten ein und gibt der KI bereits einige Merkmale vor, sind ihre Entscheidungen leichter zu interpretieren. Das hat den Nachteil, dass die KI weniger dazu fähig ist, etwas zu finden, was Experten bislang entgangen ist. Daher haben die bisherigen Modelle auch ihre Berechtigung, auch wenn ihre Erklärbarkeit Grenzen hat. Ein Problem sei, dass es keine grundlegende Definition dafür gebe, was Erklärbarkeit bedeute - weder vom Gesetzgeber noch in der Wissenschaft, sagt Palacio. In der EU-Datenschutz-Grundverordnung steht zum Beispiel, dass Entscheidungsprozesse mittels eines automatisierten Verfahrens erklärbar sein müssen. Aber es gibt keine genaue Definition, was das bedeuten soll. Reicht es, wenn Experten verstehen, was ein System macht? Oder muss der Anwender das nachvollziehen können? Nicht zuletzt kann eine KI auch einfach mit Beispielen Einblicke in Entscheidungen geben. Wenn zum Beispiel eine KI Bildinhalte erkennen soll und eine Katze als Haus bezeichnet, kann sie bei der Fehlersuche Beispiele von Häusern oder Katzen zeigen - daraus kann hervorgehen, wie der Fehler zustande kam, etwa weil in mehreren Bildern Katzen vor Häusern zu sehen sind. Aber würde das als Erklärbarkeit durchgehen? Solange das nicht geklärt ist, werden Unternehmen wenig Anreiz haben, ihre KI interpretierbar zu machen. Und dann kann eine angeblich zuverlässige KI weiterhin mitunter nur einen Hautkrebs erkennen, wenn er vom Arzt auf einem Bild mit einem Pfeil markiert ist - auch das ist schon vorgekommen.

";https://www.sueddeutsche.de/wissen/ki-machinelles-lernen-neuronale-netze-informatik-erklaerbarkeit-1.5109715;sz.de;Boris Hänßler
18.12.2020;Irgendwas mit Mathe;"Pflegekräfte, Kassierer, Erzieherinnen und neuerdings auch Couch-Potatos gelten als Helden der Corona-Krise. Ebenfalls ins Rampenlicht gehört eine Spezies, die sonst eher unbemerkt an den Stellschrauben der Welt dreht: Modellierer, die mit ihren Simulationen Wege aus der Pandemie aufzeigen. Ihre Berechnungen bieten die Grundlage für Entscheidungen über Lockdown oder Lockerung.

Tatsächlich läuft fast nichts ohne die mathematisch versierten Tausendsassas. Flugzeuge würden ohne ihre Simulationen in der Luft zusammenstoßen oder erst gar nicht abheben, der öffentliche Nahverkehr wäre ein einziges Chaos, wir könnten weder Bargeld abheben noch Hochhäuser bauen, es gäbe keine bildgebenden Verfahren in der Medizin. Produktionsprozesse werden genauso wie globale Finanzströme von Zahlenkolonnen gesteuert, die Regeln folgen, die Modellierer sich ausgedacht haben. Partnerbörsen vertrauen auf deren Algorithmen, um ihren Kunden den perfekten Gespielen vorzuschlagen. Neue Produkte werden am Rechner entworfen, und mittlerweile erobert künstliche Intelligenz auch lange als uneinnehmbar geltende Bastionen wie die Kunst: Intelligente Algorithmen komponieren Musik, schreiben Gedichte oder erschaffen Gemälde. ""Es ist schwierig, Dinge zu finden, wo Modellieren und Simulieren keinen Einfluss haben"", sagt Gerta Köster, Professorin für Mathematik und Informatik an der Hochschule München. Kein Wunder, dass Experten auf diesem Gebiet in den unterschiedlichsten Branchen sehr gefragt sind - umso mehr, als es sich bei ihnen um eine recht seltene Spezies handelt. ""Tendenziell bilden wir immer zu wenig Leute aus, und auch zu wenige mit diesen begehrten interdisziplinären Kompetenzen, die nicht nur unter sich reden können, sondern den Nutzen, den man der Gesellschaft als Mathematiker oder Informatiker bringt, auch gut darstellen können.""
Den Begriff ""vielleicht"" versteht der Computer nicht

Doch was machen Modellierer eigentlich? Um diese Frage zu klären, muss man sich damit auseinandersetzen, wie ein Modell entsteht - nämlich durch den Dreischritt Beschreibung, Algorithmus und Programm. ""Da wir immer irgendetwas aus der Realität modellieren, müssen wir diese erst einmal beobachten"", erklärt Köster. Diese Beobachtungen gelte es möglichst einfach auszudrücken, indem man sie grafisch oder verbal beschreibe oder eben in mathematischen Formeln. Das passiere meist mit Bleistift auf einem Stück Papier.

Da man mit dem Modell allerdings verschiedene Sachen durchspielen wolle, müsse man es in den Computer bringen. ""Deshalb versucht man, diese Beschreibung in etwas umzusetzen, was der Computer verstehen kann: in Algorithmen."" Das sind Rechenvorschriften oder Abläufe, die dem Computer eindeutig sagen, was er in jeder Situation zu tun hat. Denn ""vielleicht"" oder ""ähnlich"" verstehe der Computer nicht. ""Dieser Algorithmus ist ein wichtiger Schritt in der Modellbildung, und da kommt sehr viel Mathe ins Spiel - aber auch noch auf dem Papier.""

Um die Algorithmen, die von den Modellierern, Mathematikern, Physikern oder Anwendungswissenschaftlern geschrieben wurden, in einen Computer zu bringen und daraus ein zuverlässiges Programm zu entwickeln, sind Informatikkenntnisse gefragt. ""Es ist ein Dreieck: Wir brauchen die Anwendungsgebiete wie Physik, Ingenieurwissenschaften, Psychologie oder Soziologie, die Mathematik, um diese Themen zu beschreiben, und die Informatik, um sie auf dem Computer umzusetzen"", bringt es die Professorin auf den Punkt. Für die 52-Jährige ist diese Interdisziplinarität das Schöne an ihrem Beruf: ""Man muss ein Teamplayer sein, gerne mit anderen Leuten zusammenarbeiten und darf keine Scheuklappen haben.""

In Kösters Schrank steht denn auch ein 700-Seiten-Wälzer über ""Psychology and Life"" neben den ""etwas härteren Mathe-Sachen"", wie sie sagt. Für sie gehören beide Dinge zusammen, gerade in ihrem Spezialgebiet, der Fußgängersimulation. ""Da muss man sich zusammenraufen: Wir müssen die Psychologen zwingen, sich eindeutig auszudrücken, und sie zwingen uns wiederum, wichtige Dinge zu sehen, die wir gerne aussparen würden, weil sie schwierig auszudrücken und in einen Algorithmus zu überführen sind.""
Das Klischee vom weltfremden Nerd ist überholt

Immer im Vordergrund: die praktische Anwendung, der Nutzen, und das auch schon im Studium: ""Stärker als meine Generation haben unsere Studierenden das Bedürfnis zu sehen, was man nachher daraus macht."" Deshalb hat die Professorin mit ihren Mathematik- und Informatik-Studenten im Fach ""Modellierung und Simulation"" ein paar Wochen nach Ausbruch der Corona-Pandemie einen SIR-Simulator programmiert, der auf einem Modell aus der mathematischen Epidemiologie basiert. In ihm werden Menschen in einem Zellautomaten als 10 000 sich im Quadrat bewegende Punkte dargestellt, die miteinander in Kontakt kommen und entweder infiziert, infizierbar oder immun sind. Die Studenten simulierten damit, wie sich die Zahl der Infizierten bei verschiedenen Maßnahmen wie Quarantäne, Ausgangssperre oder Impfung verändert. In einem anderen Lehrprojekt spielte Gerta Köster mit ihren Studierenden die Evakuierung eines Wiesnzelts durch. Den Grundriss besorgten sie sich im Internet, bestückten das Bierzelt virtuell und in 3-D mit Bänken und 5000 in Dirndl oder Lederhose gekleidete Besuchern, die sie Hänsel und Gretel tauften. Dann ließen sie alle aus dem Zelt fliehen und sahen zu, was passiert, wenn Störfaktoren auftraten - hier ein blockierter Gang, dort ein ""bewegliches Hindernis"" wie ein torkelnder Betrunkener. In den um einiges komplizierteren Simulator ""Vadere"", den Köster und ihr Team entwickelt haben, flossen auch Erkenntnisse von Psychologen und Soziologen ein, zum Beispiel jene, dass Familien in Notsituationen zusammenbleiben oder sich Menschenmengen in Pärchen oder Dreier- und Vierer-Gruppen aufteilen. Wenn es eng wird, ändern sie ihr Tempo, ist jemand verletzt, bleiben manche stehen und wollen helfen.

""Auch wegen des Love-Parade-Unglücks gibt es inzwischen viele Firmen, die vor großen Events Räumungen simulieren und sich die Fluchtwege genau ansehen"", sagt Köster. Dabei kann man Gefahrenquellen wie Engstellen identifizieren und kann sie beheben, indem man etwa breitere Gänge oder zusätzliche Rettungswege schafft. Köster und ihre Wissenschaftler arbeiten mit mehreren Partnern zusammen, die ihre Algorithmen anwenden - wie das Münchner Start-up Accu:rate, das unter anderem Fluchtwege für Schloss Neuschwanstein oder die Landshuter Hochzeit berechnet hat. Das Bild von Modellierern als weltfremde, lebensuntaugliche Nerds hat sich längst überholt, meint Köster: ""Es sind durchweg verantwortungsvolle Menschen, die diesen Beruf anstreben. Und sie wirken nicht nur in der Krise wesentlich an gesellschaftlichen Prozessen mit.""
Die Deutsch-Note ist so wichtig wie Mathe-Erfolge

Doch was braucht man, um einer von ihnen zu werden, und was sollte man studieren? ""Ordentliche Noten in Mathe und Informatik wären ganz gut, aber ich schaue vor allem auf die Deutsch-Note"", verrät Köster. Ausdrucksfähigkeit sei in ihrem Job besonders wichtig - und mehr als alles andere Biss. ""Man darf nach einem Rückschlag nicht aufgeben, sondern muss erst recht weitermachen. Es sind die stursten Studierenden, die ihren Bachelor und Master machen, nicht die schlausten.""

Um Modellierer zu werden, gibt es nicht den einen richtigen Weg. Gerta Köster hat Mathematik studiert und ist erst später auf die praktische Anwendung umgeschwenkt: ""Ich wollte endlich etwas bauen."" Nach der Promotion zog es sie in die Telekommunikationsbranche, wo sie zum Mobilfunkstandard UMTS forschte. ""Als es den First Call gab, also jemand das erste Mal mit der von uns entwickelten Technik telefonierte - das war so schön"", erinnert sich Köster, und immer noch klingt Begeisterung aus ihrer Stimme.

Der Einstieg in den Job sei zwar sportlich gewesen, denn ihr Spezialgebiet waren partielle Differentialgleichungen, und sie musste sich plötzlich mit Protokollen und Informatik beschäftigen, von denen sie keine Ahnung hatte. Dennoch ist sie überzeugt: ""Reine Mathematik oder Informatik zu studieren, ist nicht falsch, man lernt dort zu denken."" Mit diebischer Freude berichtet die Mathematikerin von einem kuriosen Zusatzkurs über Mannigfaltigkeit in ihrem Diplomstudiengang, den sie nur aus Spaß belegte - und Mannigfaltigkeit sei jetzt in der KI ganz wichtig.

Näher dran an der praktischen Anwendung sind ihrem Namen gemäß Studiengänge wie ""Angewandte Mathematik"" oder ""Angewandte Informatik"" - wobei angehende Studenten einen Blick ins Modul-Handbuch werfen sollten, um zu sehen, ob die Art der Anwendung nach ihrem Geschmack ist. Geht es eher in eine wirtschaftliche oder technische Richtung? ""Wenn dort eine klassische Modellbildung und Simulation drinsteht, freue ich mich. Aber auch Sachen wie machine learning oder Statistik sind klare Methoden der Anwendung von Mathematik auf Probleme aus dem echten Leben"", weiß Köster. Sie empfiehlt, wegen ihrer praxisorientierten Ausrichtung sogenannte Und-Studiengänge zu studieren, wie zum Beispiel ""Design und Informatik"", Finanzmathematik oder Bioinformatik. ""Dort werden andere Wissenschaften miteinbezogen, und es geht wirklich um Problemlösung, nicht nur um Algorithmen. Da ist man sofort in der Modellbildung drin.""
Riesige Datenmengen helfen bei der Früherkennung

Aufsehen erregten auch Wissenschaftler aus dem Bereich Medizininformatik der Ostbayerischen Technischen Hochschule (OTH) Regensburg: Sie entwickelten ein System, das mithilfe künstlicher Intelligenz Speiseröhrenkrebs im Frühstadium diagnostizieren kann. Dafür fütterten Professor Christoph Palm und sein Team einen Rechner mit Hunderten von Endoskopie-Bildern und Patientendaten und programmierten daraus eine Software. Bei der Untersuchung der Speiseröhre mit einer Kamera kann diese künstliche Intelligenz dazugeschaltet werden. Sie erkennt anhand der Endoskopie-Bilder, ob ein Tumor vorliegt. Handelt es sich um Krebs, leuchtet die Region gelb oder rot auf. Speiseröhrenkrebs zählt zusammen mit Leber- und Bauchspeicheldrüsenkrebs zu den Krebsarten mit der niedrigsten Überlebensrate. Doch wenn Speiseröhrenkrebs früh erkannt werde, liege die Heilungschance bei fast 100 Prozent, sagt Palm.

Gerade die Früherkennung von Krankheiten ist ein wichtiges Gebiet, auf dem Modellierer und Simulierer gefragt sind. ""Dazu muss man riesige Datenberge durcharbeiten, das geht nicht ohne Computer"", sagt Köster. Daten gelten als Öl des 21. Jahrhunderts. Damit Studierende lernen, bewusst mit ihnen umzugehen und sie richtig einzusetzen, startete an der Hochschule München der Studiengang ""Data Science & Scientific Computing"". Wer diesen oder ähnliche Studiengänge abschließt, dem stehen unzählige Branchen und Arbeitsgebiete offen: das Gesundheitswesen, die Finanz- und Versicherungsbranche, wo ohne Prognosen und Modelle nichts läuft, der Versandhandel oder E-Commerce, wo sich Modellierer mit Online-Marketing oder Business Analytics beschäftigen, oder die Produktion, wenn es um Qualitätssicherung oder die Entwicklung neuer Produkte geht.

Auch für Wettervorhersagen, autonomes Fahren oder Crashtests braucht es Modellierer und Simulierer. Und natürlich kommt die immer wichtiger werdende Robotik hinzu - für Gerta Köster ""eine wunderbare Ehe zwischen Mathematik und Informatik mit einer Anwendung"". Dort könne man sehen, dass der Spieltrieb vieler Nerds nicht umsonst war: ""Wer früher in Computerspielen Roboter gesteuert hat, programmiert heute vielleicht Pflegeroboter."" In allen modernen Branchen sei die Digitalisierung auf dem Vormarsch.

Doch wo viel Licht ist, gibt es auch Schatten: ""Ich finde es wichtig, diesen Vormarsch zu kontrollieren und zu hegen, damit keine Effekte entstehen, die wir nicht haben wollen"", sagt Köster. ""Und dazu brauchen wir gut ausgebildete Leute, die wissen, was sie tun."" Nerds mit Ausdrucksstärke und Biss also, die Lust darauf haben, an gesellschaftlichen Prozessen mitzuwirken - und hin und wieder sogar Leben zu retten.";https://www.sueddeutsche.de/karriere/beruf-job-mathematik-corona-studium-1.5142084;sz.de;Nicole Grün
27.11.2020;"""KI ist nicht kreativ""";"Hätte ein Textroboter diesen Artikel verfasst, hätte er, nach der Datenanalyse tausender Berichte selbstverständlich, womöglich erst einmal Begriffe definiert. In einfachen Sätzen wie diesem: Künstliche Intelligenz (KI) bedeutet maschinelles Lernen. Sodann hätte er erklärt, ebenfalls nüchtern, was der Anlass der Berichterstattung war. Vor etwa 50 Studenten an der Hochschule für Fernsehen und Film (HFF) sowie vor weiteren 150 Stream-Zuschauern fand am Donnerstag das ""Erste Münchner KI-Symposium"" statt, organisiert von der HFF und der Fernseh- und Kinotechnischen Gesellschaft (FKTG). Vielleicht hätte er noch erwähnt, wie wichtig die Thematik für die Kultur- und Medienbranche ist. Zitat der HFF-Präsidentin Bettina Reitz: ""Schon jetzt ist KI aus der Filmindustrie nicht mehr wegzudenken.""

Da aber kein Textroboter zur Verfügung stand und dem (menschlichen) Autor ein Einstieg vorschwebte, der ein bisschen Lesefreude in die technische Dimension einspeisen soll, beginnt dieser Text mit einer Randnotiz zum Schmunzeln. Auch weil der Autor als Beobachter des Symposiums gelernt hat: Humor und Ironie können Computer nicht, auch die Analyse von Sarkasmus ist noch nicht gelöst. Mit Humor muss man es aber nehmen, dass fast alle der sieben Referenten, die teils physisch anwesend, teils per Webcam zugeschaltet waren, ohne Anglizismen weniger zu sagen hätten. ""Signature Code"" hier, ""Real-Time-Response"" da, ""Feature Engineering"" hier, ""Shallow Fakes"" da. Daran muss sich wohl gewöhnen, wer vom Lernen der Maschinen lernen will. Oder er muss eine Sprachmaschine erfinden.

Abgesehen davon gab es spannende, bekannte wie überraschende Einblicke in ein Forschungsfeld, das auf systematischer Datenanalyse basiert, deren Erkenntnisse dem Menschen bei seinen Entscheidungen helfen sollen, auch im kreativen Prozess. Es ging um Rezipientenforschung und Werbewirkung, crossmediale Archive und Multimedia-Recherche; es ging um Audioforensik, automatisierte Komponisten und Schnitt-Assistenten und führte schließlich zu einem Film, der sich ständig verändert: ""The Future Is Not Unwritten"" heißt der ""Smartfilm"" von Susanne Steinmassl, der seit ein paar Jahren für Furore sorgt und auf dem Symposium erneut präsentiert wurde. Die Medienkünstlerin und HFF-Studentin hat eine klare Meinung zu KI, sie sagt: ""Kunst und Wissenschaft können von der Technologie profitieren."" Sie forderte alle Kreativen auf, sich einzumischen und die Zukunft mitzugestalten.

KI ist ein weites Feld. Eines, das fasziniert und einschüchtert. Oft ist das Thema ja mit der Angst verbunden - geprägt durch Filme wie ""2001: Odyssee im Weltraum"", ""Matrix"" oder ""I, Robot"" -, künstliche Wesen würden die Macht ergreifen. ""Humanoide Roboter sind, zumindest jetzt noch, ein rein filmdramaturgisches Konstrukt"", stellte der HFF-Technik-Professor Peter Slansky in seiner Einführung klar. Dem Tenor schlossen sich viele Dozenten an: Menschen würden nicht ersetzt, ""es muss immer jemand da sein"", sagte etwa Marc Egger, Head of Data Science bei der Mediengruppe RTL. An anderer Stelle fiel der Begriff des ""Butlers"". Das Computersystem als Diener.

Besonders eindrucksvoll war der Vortrag von Claudia Birkholz über ""virtuelle Komponisten"". Die Bremer Pianistin und Dozentin für Klavier und Neue Musik setzte sich an den Flügel und spielte ein paar Takte, die an Mozarts A-Dur-Sonate erinnerten. Tatsächlich hatte eine Software das Stück komponiert, basierend auf ""gelernten"" Eigenschaften, destilliert aus Werken von Bach, Beethoven und so weiter. ""Mozart wird in den Schredder gegeben, und EMI setzt es neu zusammen"", sagte Birkholz. EMI steht für ""Experiments in Musical Intelligence"", die Software des Amerikaners David Cope geht bis in die Siebziger zurück. Zeitgenössische Kompositionen eines jüngeren Programms (""Iamus"", 2010) spielte die Musikerin ebenfalls an. Auch hier lägen lediglich Töne vor, die Interpretation liege beim Menschen. ""Als würde jemand Polnisch sprechen, der es gar nicht kann."" Claudia Birkholz betonte, in der Musik gehe es um Vermittlung, um Gefühle und Bilder. Was sie zu dem Schluss brachte: ""KI ist nicht kreativ."" Aber: ""KI kann Kreativität initiieren."" Als Beispiel nannte sie das Werk der italienischen Komponistin Artemi-Maria Gioti für robotisiertes und menschliches Schlagzeug. ""Ein Duo"", das aufeinander reagiere.

Fortgeschritten ist auch der Einsatz von KI in Filmproduktionen. Jakob Rosinski von IBM zeigte den Trailer zu ""Morgan"" (2016), einem Science-Fiction-Thriller über ein Wesen mit synthetischer DNA. Die Fox-Studios seien hier auf IBM zugekommen mit der Bitte, den Trailer zum Film von der KI-Software ""Watson"" erstellen zu lassen. 100 gute Trailer-Beispiele aus dem Genre seien analysiert worden, ehe das Programm aus ""Morgan"" Szenen für den Zusammenschnitt vorgeschlagen hätte. Der finale Schnitt sei dann aber menschlich gewesen, sagte Rosinski. Einsatzmöglichkeiten sieht er unter anderem bei Sport-Highlights und im schnellen Tagesgeschäft der Rundfunkhäuser.

Am Ende hätte der Textroboter resümiert: Es ist kompliziert. Die Veranstalter resümieren: Es geht weiter. Das Symposium wurde als Auftakt bezeichnet, von Wiederholungen war die Rede. Zum Hintergrund: 2021 soll es bekanntlich eine KI-Professur an der Münchner Filmhochschule geben. Noch sei KI ""Neuland für die HFF"", sagte Bettina Reitz. Aber Europa müsse aufholen im Vergleich zu Amerika und China.";https://www.sueddeutsche.de/muenchen/symposium-ki-ist-nicht-kreativ-1.5130099;sz.de;Bernhard Blöchl
06.11.2020;Karte zeigt Studiengänge rund um KI und Data Science;"Schulabsolventen, die eine Karriere in den Technologiefeldern Künstliche Intelligenz oder Data Science anstreben, können sich ab sofort auf einer Landkarte einen Überblick über passende Studiengänge verschaffen.

Die KI-Landkarte der Plattform Lernende Systeme, die bei der Akademie der Technikwissenschaften (acatech) angesiedelt ist, hat eine neue Rubrik ""Studiengänge"". Dort werden derzeit für Deutschland 170 Hochschulen aufgelistet, die Studiengänge rund um KI und Data Science anbieten, informiert die Plattform.

Nutzer können die Suchergebnisse verfeinern und sie zum Beispiel nach Hochschultyp, Abschluss und thematischem Schwerpunkt filtern. Datenbasis der auf der KI-Landkarte aufgeführten Studiengänge ist den Infos zufolge der Hochschulkompass der Hochschulrektorenkonferenz (HRK).";https://www.sueddeutsche.de/karriere/arbeit-karte-zeigt-studiengaenge-rund-um-ki-und-data-science-dpa.urn-newsml-dpa-com-20090101-201106-99-236761;sz.de;DPA
19.10.2020;So fallen Sie beim Online-Shopping nicht auf Tricks rein;"Onlineshops sind heute mindestens genauso voll (und oft auch so kräftezehrend) wie ein Kaufhaus an einem Samstag kurz vor Weihnachten. Laut Statistischem Bundesamt kauft in Deutschland jeder Dritte mindestens einmal pro Woche im Internet ein. Der Anteil der Befragten, die nur zweimal im Jahr online einkaufen, liegt bei gerade mal zwei Prozent. Und die Menschen lassen dort sehr viel Geld: Etwa 60 Milliarden Euro haben die Deutschen im Jahr 2020 im Online-Handel ausgegeben – vor zehn Jahren waren es 20 Milliarden. Das verrückte Jahr 2020, in dem im Internet so unterschiedliche Dinge wie Hometrainer, Hefe oder Trampoline zeitweise ausverkauft waren, ist da noch nicht eingerechnet. Der Aktienkurs von Amazon hat sich seit dem Beginn der Corona-Beschränkungen Mitte März fast verdoppelt.

Nun liegt der Erfolg der Online-Shops aber nicht nur daran, dass es so praktisch ist, sich den Bleistiftspitzer, die Entkalkungstabletten für die Kaffeemaschine oder die Matschhose für die Kinder schnell im Internet zu bestellen. Sondern Online-Händler sind auch sehr gut darin geworden, Menschen dazu zu bringen, mehr zu kaufen, als sie eigentlich wollten. Die Tricks, die Händler dafür nutzen, sind ziemlich schlau. Doch es gibt zum Glück auch Expertinnen und Experten bei Verbraucherzentralen, in Think-Tanks und bei Regierungsstellen, die sich damit auskennen. Mit ihrer Hilfe haben wir fünf Tipps zusammengestellt, die man beim Einkaufen im Internet beachten sollte – egal, ob man Waren bestellt oder Hotelzimmer bucht. 1. Lassen Sie sich nicht unter Druck setzen

Vermutlich kennen Sie diese kleinen roten Hinweise, die neben dem Angebot im Online-Shop stehen und warnen: »Achtung, nur noch zwei Exemplare verfügbar!«, »Vorsicht, neun weitere Kunden schauen sich gerade genau dieses Angebot ebenfalls an!«. Oder, besonders dreist: »Schade, dieses Schnäppchen ist leider nicht mehr verfügbar.« Dieses Design hat einen Namen: Dark Patterns. Das klingt nicht nur wie der Bösewicht in einem Science-Fiction-Film, sondern geht auch ähnlich fies vor. Die Stiftung Neue Verantwortung definiert diese kleinen Tricks als »sanfte Formen der Einflussnahme«, die Kunden zu »unüberlegten Kaufentscheidungen drängen«.

Das Büro für Technikfolgenabschätzung beim Deutschen Bundestag geht noch weiter: »Der Einsatz von Dark Patterns ist unethisch, mitunter unlauter und gegebenenfalls betrügerisch«, schreibt Christoph Bogenstahl, der Autor einer Studie über dieses Thema. »Insbesondere sind auf die Ausnutzung menschlicher Wahrnehmungsschwächen ausgerichtete Dark Patterns für unerfahrene Nutzende schädlich, etwa Senioren, Kinder und Jugendliche sowie bildungsferne Gruppen.«

Dark Patterns sind vielfältig: Mal wird behauptet, der Preis für ein Hotelzimmer sei momentan außergewöhnlich stark reduziert – dabei wird als Vergleichspreis ein viel teureres Zimmer in der Hauptsaison herangezogen. Oder durch eine Vielzahl von Kundenbewertungen wird der Eindruck erzeugt, ein bestimmtes Produkt sei besonders hochwertig. Dass diese Bewertungen oft Humbug sind – dazu mehr unter Punkt 2.

Solche Tricks sind zu einem lukrativen Geschäft geworden: Eine 2019 veröffentlichte Studie der Princeton University hat festgestellt, dass rund elf Prozent der populärsten Shopping-Webseiten solche Dark Patterns verwenden, um Nutzer dazu zu bringen, potenziell schädliche Entscheidungen zu treffen. Also viel mehr persönliche Daten preiszugeben als nötig, oder mehr und teurere Produkte zu kaufen als geplant. Oft funktioniert das über den Trick, den Experten als Pressure Selling bezeichnen – also das Erzeugen von künstlichem Druck. So wird dem Nutzer vorgegaukelt, dass ein bestimmtes Produkt gerade bei vielen Käufern beliebt und deshalb wohl bald ausverkauft sei und man sich beeilen müsse, um noch ein Exemplar zu ergattern. So wird man unter Zeitdruck gesetzt – von dem Psychologen wissen, dass er zu unüberlegten Entscheidungen führen kann.

Die Verbraucherzentrale Bayern hat in einer Umfrage herausgefunden, dass bis zu 55 Prozent der Befragten bei solchen scheinbar limitierten Angeboten unüberlegt auf den Kaufen-Button drücken. Tatjana Halm von der Verbraucherzentrale Bayern hat einen einfachen Tipp: »Verbraucher sollten sich von solchen Methoden nicht beeindrucken lassen und sich dennoch in Ruhe informieren. Häufig findet man auf anderen Portalen weitere Angebote.« Statt der großen Vergleichsportale lohnt sich gerade auf der Suche nach einer Unterkunft oft ein Blick auf die Webseiten der Hotels. Dort gibt es von den angeblich fast ausverkauften Zimmern zum Supersonderpreis oft noch genug – nicht selten sogar günstiger. Auch bei anderen Onlineshops sind die scheinbar sehr begehrten Artikel in Wahrheit noch ausreichend verfügbar – die roten Zahlen, die zur Eile mahnen, sind häufig einfach erfunden. Genau wie die angeblichen Supersuper-Schnäppchen zu Anlässen wie dem »Black Friday«, den viele Onlinehändler großflächig bewerben. Die Verbraucherzentrale Brandenburg hat diese Angebote untersucht und festgestellt: »Die meisten Angebote waren nichts Besonderes, es gab sie genauso auch woanders zu einer anderen Zeit«, sagt Michèle Scherer, eine Expertin für Online-Handel bei der Verbraucherzentrale.
2. Seien Sie skeptisch bei Produktbewertungen

Wir Menschen sind soziale Wesen und wollen fast immer, dass unser Handeln in der Gesellschaft Anklang findet (oder zumindest nicht allzu sehr ausschert). Und das macht uns anfällig für Manipulation. Wenn man nach einem Produkt sucht, mit dem man sich nicht auskennt – Akkuschrauber, Toaster, Bluetooth-Kopfhörer –, verlässt man sich oft instinktiv auf die Meinung anderer. Wir entscheiden uns also eher für ein Produkt, wenn andere Leute es auch benutzen. Das können Influencer in sozialen Medien sein (die mit genau dieser Form der Beeinflussung viel Geld verdienen und eben meistens nicht das beste, sondern das für sie lukrativste Produkt empfehlen). Oder andere Kunden, die das Produkt gekauft und bewertet haben.

Was erwiesen ist: Wenn in einem Onlineshop viele Menschen eine positive Bewertung für einen bestimmten Artikel hinterlassen haben, steigt die Chance, dass auch wir diesen Artikel kaufen. Das ist auf den ersten Blick eine gute Idee, weil es Zeit spart. Wir schenken einer Kohorte fremder Bewertungsschreiber unser Vertrauen – und müssen uns nicht länger mit Akkuschrauber oder Toastern beschäftigen. Dieses Vertrauen, das der Systemtheoretiker Niklas Luhmann einen »Mechanismus der Reduktion sozialer Komplexität« nennt, soll uns entlasten. Aber oft heißt das: Wir lassen uns täuschen. Etwa von dubiosen Anbietern, die einfach sehr viele bunte Siegel und Qualitätsstempel auf ihrer Website haben (gern in Form eines emporgereckten Daumens mit der Aufschrift: 100% Qualität) – und in Wahrheit überteuerte, mangelhafte oder gefälschte Produkte verkaufen. Dabei kann man auf internet-guetesiegel.de leicht prüfen, ob so ein Siegel tatsächlichen Qualitätskriterien entspricht.

Und auch die Bewertungsschreiber können zu einer Täuschung gehören. Ein Team um die Journalistin Svea Eckert vom NDR hat im Dezember 2019 unter dem Titel »Fake Likes« offengelegt, wie Menschen von dubiosen Agenturen dafür bezahlt werden, bestimmte Artikel und Firmen im Netz positiv zu bewerten. Auch den großen Online-Händlern wie Amazon wird immer wieder Trickserei vorgeworfen. Das Wall Street Journal hat im Herbst beschrieben, wie das größte aller Online-Kaufhäuser seinen Algorithmus still und heimlich so angepasst haben soll, dass Kunden auf der Suche nach bestimmten Artikeln nicht mehr die besonders beliebten oder relevantesten Artikel angezeigt bekommen, sondern jede, mit denen Amazon besonders viel Geld verdient. Auch hier empfiehlt Michèle Scherer von der Verbraucherzentrale Brandenburg: »Investieren Sie lieber ein bisschen Zeit in wirkliche Produktvergleiche.« Gibt es Testberichte über ein bestimmtes Produkt in der Fachpresse oder der Stiftung Warentest? Wie wird dieses Produkt bei anderen Online-Händlern bewertet? »Es lohnt sich oft, sich ein wenig mit den Artikeln zu befassen, als einfach auf die Bewertungen bei Online-Händlern zu vertrauen.«
3. Verraten Sie nicht zu viel von sich

Man sollte meinen, dass es für den Online-Handel keine Rolle spielt, ob ein Produkt nach Sachsen-Anhalt oder Starnberg geschickt wird. Oder ob es abends von einem 19-jährigen Auszubildenden mit einem Aldi-Laptop oder nachmittags von einer 50-jährigen Professorin auf dem neuen iPhone bestellt wird. Aber dieser Eindruck täuscht. Es gibt beim Internet-Kauf das Prinzip Dynamic Pricing – das Anpassen von Preisen an den Käufer. Das Prinzip ist nicht neu (schon Tankstellen ändern ihren Benzinpreis oft mehrmals am Tag, damit es dann besonders teuer ist, wenn besonders viele Menschen tanken müssen), findet aber im Internet seinen perfekten Nährboden. Denn vieles von dem, was abstrakt als »Big Data« bezeichnet wird, kommt hier zur Anwendung: das datenbasierte Festlegen von verschiedenen Preisen für das gleiche Produkt.

Wie genau das funktioniert, verraten die Onlinehändler nicht – aber Verbraucherschützer gehen davon aus, dass viele der digitalen Spuren, die Internetnutzer beim Stöbern in Onlineshops hinterlassen, einen Einfluss auf den Preis haben. Zum Beispiel: Welches Gerät nutze ich? Wo halte ich mich auf? Welche anderen Produkte habe ich vielleicht schon bei einem Anbieter angesehen? So hat etwa die Verbraucherzentrale Brandenburg 2018 die Preise von rund 1500 Artikeln bei 16 Online-Händlern auf fünf Endgeräten untersucht und herausgefunden:

    Bei einem großen Online-Händler für Autozubehör waren Autobatterien oder Reifen jeweils am Vormittag teils bis zu 30 Prozent teurer als am Nachmittag zuvor.
    Verschiedene Endgeräte zeigten in einem Zeitraum von 20 Minuten bei vielen Produkten unterschiedliche Preise an – in der Kategorie Spielzeug sogar bei 61 Prozent der untersuchten Angebote.
    Bei einer Baumarktkette schwankten die Preise je nach Adresse des Bestellers um bis zu 52 Prozent.
    Die Preise änderten sich teilweise täglich.
    Bei einem großen Elektromarkt war ein bestimmtes Handy im Zeitraum von 30 Tagen um bis zu 220 Euro teurer – je nachdem, wann es bestellt wurde.

Leider lassen sich genaue Muster – also in welcher Stadt und zu welcher Zeit ein Produkt besonders teuer ist – nicht so einfach ableiten. Die Preise ändern sich einfach zu schnell. Aber dass man diesen Mechanismus zumindest kennen sollte, bevor man etwas bestellt, betont Michèle Scherer von der Verbraucherzentrale: »Ich muss mir längerfristig die Preise angucken, und das ist ja vielleicht das Schwierige daran. Denn die Anbieter tun ja sehr viel dafür, uns zu Impulskäufen zu verleiten.« Eine Möglichkeit: Sehr genau darauf achten, welche Daten man bei Online-Shops hinterlässt.
4. Verwischen Sie Ihre Spuren

Wenn Online-Shops mit technischem Aufwand versuchen, uns dazu zu bringen, zu viel Geld im Internet auszugeben, kann eine Lösung sein, sich mit ein wenig technischem Aufwand dagegen zu wehren. Folgende Punkte haben sich in der Praxis bewährt und werden von Verbraucherschützern empfohlen:

    Browser wechseln: Wenn man sich einen bestimmten Artikel mehrfach anschaut, können Shops daraus ableiten: Aha, diese Person interessiert sich offenbar für dieses Produkt – was möglicherweise dazu führt, dass ein höherer Preis dafür angezeigt wird. Vor allem bei teureren Produkten wie Laptops, Küchengeräten oder Hometrainern empfiehlt es sich, nicht immer denselben Internetbrowser für die Suche zu verwenden. Wichtig dabei: Die Cookies im Browser löschen (über »Einstellungen«). So werden besuchte Seiten oder Eingaben in Suchmaschinen nicht so einfach an die Onlineshops weitergereicht.
    Unterschiedliche Geräte nutzen: Weil genau erfasst wird, mit welchem Gerät man im Netz unterwegs ist, kann es sehr gut sein, dass man unterschiedliche Preise angezeigt bekommt, wenn man vor einem älteren Laptop sitzt statt vor dem neuesten Apple-Tablet.
    Erst beim Kauf einloggen: In vielen Onlineshops muss man eingeloggt sein, um etwas zu kaufen. Beim Stöbern empfiehlt sich das allerdings nicht. Denn für registrierte Kunden hat jeder Shop eine Art Profil angelegt, in der gespeichert ist, was man bereits wann und zu welcher Zeit (und für wie viel Geld) gekauft hat. Das alles kann sich auf den Preis auswirken. Es kann also hilfreich sein, sich erst kurz vor dem Kauf in sein Kundenkonto einzuloggen – erfahrungsgemäß ändert sich dann der Preis auch nicht mehr.
    Anonym surfen: Was Experten für Internet-Privatsphäre ohnehin empfehlen, ist ein VPN-Dienst. Mit so einem »Virtual Private Network« wird der eigene Internetverkehr umgeleitet und die IP-Adresse verborgen. Für Websites ist damit nicht mehr klar, woher eine Anfrage kommt. Die Folge: Man ist im Netz anonymer unterwegs – für Händler ist es also nicht mehr so leicht erkennbar, ob man in einem tendenziell wohlhabenden Ort im Münchner Umland sitzt oder in einer Stadt, in der die Bürger durchschnittlich weniger Geld zur Verfügung haben und damit unter Umständen auch nur geringere Preise bezahlen würden. Aber: Es empfiehlt sich unbedingt, einen kostenpflichtigen VPN-Anbieter für ein paar Euro Gebühr im Monat zu nutzen, da Gratis-Anbieter ihr Geld anderweitig verdienen müssen, etwa mit dem Verkauf persönlicher Daten.

5. Installieren Sie dieses kleine Programm

Ein Wort, das im Gespräch mit Verbraucherschützern immer wieder auftaucht, ist der Impulskauf – darauf zielen viele dieser Tricks der Online-Händer ab. Eigentlich überflüssig, aber naja – und zack, landet das Ding im Warenkorb und wenig später vor der Tür. Und wenn es mal da ist, schickt man es eher selten wieder zurück. Auch das erklärt den Erfolg von Online-Shopping: Das Einkaufen geht viel einfacher als das Zurückgeben.

Doch ein paar Fachleute haben sich einen Trick einfallen lassen, mit dem man solche Impulskäufe zumindest ein bisschen bremsen kann: Das Programm »Icebox«, das man als Nutzer des Internet-Browsers Chrome kostenlos herunterladen kann. Wenn man es aktiviert, wird beim Kauf in vielen Onlineshops eine Art Zwischenstufe vor dem eigentlichen Bezahlen eingefügt: Man legt seine Einkäufe dann nicht mehr in den Einkaufswagen, sondern »auf Eis«. Wann sie sich dann wirklich kaufen lassen, kann man einstellen. Die Macher empfehlen eine Bedenkzeit von dreißig Tagen (geht aber auch kürzer). Wenn man nach Ablauf der Frist diesen sehr schicken Salzstreuer, den eher abseitigen Bildband oder die Großpackung Batterien immer noch haben will, bitte. Wenn nicht, hat man die Macht der Technikkonzerne mit ihren eigenen Waffen geschlagen: durch einfach zu bedienende, aber raffinierte Technik.";https://sz-magazin.sueddeutsche.de/internet/tipps-online-shopping-89313?reduced=true;sz.de;Till Krause
16.09.2020;Big Pharma trifft Big Data;"Ob Mann oder Frau, Schwarz oder Weiß - in der Medizin spielt das allzu oft keine Rolle. ""Menschen unterscheiden sich in Größe, Geschlecht, Gewicht, in ihrer Nieren- und Leberfunktion und in ihrem Ansprechen auf Medikamente - und trotzdem bekommen sie in der Regel eine Standarddosis"", beklagte Gerd Geißlinger, der Ordinarius des Instituts für Klinische Pharmakologie der Universität Frankfurt, während des SZ-Gesundheitsforums. Der Pharmakologe ist davon überzeugt, dass sich Arzneimittelnebenwirkungen reduzieren ließen, wenn Unterschiede zwischen den Menschen stärker berücksichtigt würden. ""Immer wieder hat sich gezeigt, dass bestimmte Patienten mehr von einzelnen Arzneimitteln profitieren, während andere stärker unter Nebenwirkungen leiden"", sagte Geißlinger. Würden die Unterschiede besser berücksichtigt, könnten die Ergebnisse der Pharmatherapie am Ende sehr viel besser sein.

Aber wie lassen sich die Unterschiede herausfinden? Geißlinger setzt vor allem auf neue Technologien wie Künstliche Intelligenz und auf Big Data, denn nur aus großen Datenmengen lassen sich am Ende Untergruppen von Patienten herauslesen. ""In Zukunft werden Ärzte, Naturwissenschaftler und Informatiker gemeinsam in einem Team in die Medikamentenentwicklung involviert sein"", prophezeite er. Davon sind längst auch jene Unternehmen überzeugt, die die Global Player in Sachen Daten sind. Immer mehr Tech-Konzerne tummeln sich mittlerweile auf dem Pharmamarkt. So gründete die Google-Mutter Alphabet mit Verily Life Sciences (früher Google Life Sciences) eine eigene Biowissenschaftsfirma: ""Big Data erlaubt es uns, mehr Daten über die Menschen zu sammeln und das Entstehen schwerer Krankheiten zu verhindern"", lautet das Versprechen bei der Gründung. Verily wolle herausfinden, was Menschen gesund hält und wie sie länger gesund bleiben.

Selbst Facebook ist mittlerweile ein Arzneimittelhersteller. Die Forschungsetats des Social-Network-Unternehmens seien größer als die vieler Pharmafirmen, sagte Jochen Maas, Forschungschef bei Sanofi-Aventis Deutschland. Noch arbeiteten diese Firmen zum Teil an esoterischen anmutenden Ansätzen - etwa dem Ziel, dass Menschen 200 Jahre leben. Aber manche sind auch auf den klassischen Arzneifeldern unterwegs, der südkoreanische Tech-Konzern Samsung etwa. Dessen Tochter Samsung Biologics hat sich in Windeseile zu einem der weltweit größten Auftragsproduzenten für biopharmazeutische Arzneimittel entwickelt. Und selbst Amazon macht mit - erst einmal als Apotheke. ""Pillpack by Amazon Pharmacy"" plant, Arzneimittel direkt zum Patienten bringen.

Die medizinische Expertise dieser Konzerne mag begrenzt sein, aber das Sammeln und Auswerten von Daten ist ihr Kerngeschäft - und sie haben direkten Zugriff auf unfassbar große Datenmengen. Darunter sind sogar schon Gesundheitsdaten. Viele Technologiekonzerne erheben diese: Sie registrieren, ob ihre Nutzer depressive Symptome zeigen, messen die Herzkreislauffunktion von Smartwatch-Trägern und schließen aus Suchanfragen zu Krankheiten und Symptomen, was ihre User gerade plagt. All diese Daten landen auf den Servern der Tech-Konzerne - für die Datensicherheit sei das ein echtes Problem, sagte Maas. Und was es am Ende für die Arzneimittelsicherheit bedeutet, wenn Technologiekonzerne Medikamente entwickeln? Die Experten während des Forums äußerten sich zumindest sorgenvoll.

Eines aber ist nicht von der Hand zu weisen: Die selbstlernenden Algorithmen der Tech-Unternehmen können sehr wohl dabei helfen, komplexe Daten schneller, preiswerter und präziser auszuwerten. Vor allem die aufstrebenden ""Omics-Technologien"", bei denen Gene (Genomics), Proteine (Proteomics), Stoffwechselprodukte (Metabolomics) oder umgeschriebene Gene (Transcriptomics) analysiert werden, gelten als wichtiger Datenlieferant, als begehrtes Tool für die moderne Wirkstoffforschung. Computer könnten aus diesen Daten herauslesen, welche Moleküle erfolgversprechende Ziele für Pharmaka sind. Universitäten und klassische Pharmakonzerne scheitern aber oft an der schieren Masse der Daten. ""Airbnb hat kein einziges Hotelzimmer und ist der größte Anbieter von Übernachtungen. Uber hat kein einziges Auto und ist doch ein großes Transportunternehmen geworden"", sagte Maas, ""ebenso kann Google zum großen Arzneimittelhersteller werden.""

Selbst klinische Studien lassen sich mittlerweile am Computer modellieren. Und Miniatur-Organe auf Chips bilden die Verträglichkeit und Wirksamkeit oftmals schon recht gut ab, ohne dass Patienten untersucht werden müssen, sagte Geißlinger. Das Paul-Ehrlich-Institut (PEI) erhält zu ersten klinischen Prüfungen bereits häufiger nur Modellierungsdaten, sagte dessen Präsident Klaus Cichutek. ""Je nach Wirkstoff sind aber klinische Daten notwendig und werden es immer bleiben. Das wird man nicht wegmodellieren können.""

Big Pharma hat längst begriffen: Wenn es um Big Data geht, können die klassischen Pharmakonzerne Firmen wie Google nicht schlagen. Viele haben schon Kooperationen geschlossen. So haben die US-Unternehmen Biogen und MSD mit einem weiteren Samsung-Spross, Samsung Bioepis, Joint-Ventures zur Entwicklung von biotechnischen Nachahmerprodukten begonnen, sogenannten Biosimilars. Binnen acht Jahren ist Samsung Bioepis bereits die Zulassung von neun Arzneimitteln in Europa und den USA gelungen.

Auch Sanofi-Aventis hat gemeinsam mit Google ein Joint-Venture gegründet: Onduo will mit Hilfe von Daten die Behandlung und das Krankheitsmanagement von Patienten mit Diabetes verbessern. Die Gräben zwischen den Branchen seien noch groß, erzählte Maas, aber sie beginnen sich zu schließen. ""Als wir das erste Mal zusammensaßen, haben sich die Leute aus diesen beiden Welten nicht einmal verstanden, inzwischen sind wir sehr produktiv geworden.""";https://www.sueddeutsche.de/gesundheit/arzneimittel-facebook-pharma-1.5033210;sz.de;Christina Berndt
18.08.2020;Highway to sell;"Man muss sich das in etwa so vorstellen: Bettina Reitz sitzt vor ihrem Häuschen auf Mallorca und denkt über die Zukunft ihrer Wahlheimat München nach. Sie philosophiert am Telefon über verkehrsfreie Kulturmeilen und Naturgeräusche im Großstadtrauschen, über gesellschaftliche Herausforderungen und interdisziplinäre Symbiosen. Sie sagt kluge Sätze wie: ""Ein Generationentransfer der Fehlerminimierung wäre dringend angeraten."" Dass sie sich physisch in Spanien befindet, gedanklich an der Prinzregentenstraße, dem Bernd-Eichinger-Platz und der vernetzten Welt drumherum, passt gut zu der Frau, die schon recht konkret von einem Digital Art Center für die Landeshauptstadt träumt und 2021 mit einer KI-Professur (künstliche Intelligenz) an der von ihr geleiteten Hochschule für Fernsehen und Film (HFF) beginnen möchte. Das also merkt man schnell: Ihr Denken duldet keine Grenzen, digital ist Pflicht.

Zunächst einmal: Ja, Bettina Reitz ist schwer zu fassen. Den Fototermin am Friedensengel hat sie noch hinbekommen, ein paar Tage vor dem Telefongespräch, kurz nach dem Shooting ging bereits ihr Flieger. Während der Ausgangsbeschränkungen hatte sie mit wenigen Kolleginnen und Kollegen die Stellung an der geschlossenen Hochschule gehalten; die Studierenden mussten sich mit dem neuen Online-Lehrangebot anfreunden. So als müsste sie sich für ihren Auslandstrip rechtfertigen, sagt sie gleich zu Beginn von Mallorca aus, wo sie teilweise urlaubt, teilweise Telearbeit macht: ""Jetzt bin ich ausnahmsweise auch mal nicht vor Ort."" Das Gespräch fand Ende Juli statt.

Vor Ort bedeutet die HFF, jene 1966 gegründete Talentschmiede mit renommierten Absolventen wie Maren Ade und Wim Wenders, die sie seit Herbst 2015 als Präsidentin leitet. Davor war sie fast alles, was man in der Film- und Fernsehbranche werden kann, wenn man herausragende Managerqualitäten und vielleicht auch ein bisschen Glück hat. Reitz, geboren in Frankfurt am Main, war Redakteurin beim Hessischen Rundfunk und beim ZDF, sie war Geschäftsführerin der Degeto Film und Fernsehdirektorin des Bayerischen Rundfunks. Sie war an mehreren Oscar-prämierten Produktionen beteiligt, an ""Das Leben der Anderen"", ""Amour"" und ""Citizenfour"". Und das sind nur ein paar von vielen Meilensteinen in ihrer sagenhaften Karriere. Im Februar 2019 begann ihre zweite Amtszeit an der Spitze der Filmhochschule. Und für diese hat sie große Pläne, sozusagen ihre himmlischen Aussichten für den Mikrokosmos HFF, die wiederum, das wird sich am Ende zeigen, symptomatisch für das stehen, was man ihr Wunschkonzert für die Gesellschaft nennen könnte. Da wäre also zunächst die eingangs genannte KI-Professur, die die HFF als eine von 50 bayerischen Hochschulen umsetzen darf. Der Entscheidung von Mai dieses Jahres war ein Wettbewerb vorausgegangen, insgesamt wurden 175 Anträge eingereicht. ""Wir starten 2021"", legt sich Bettina Reitz fest, Arbeitstitel: ""Professur KI und Datenwissenschaft in der Medienproduktion"". Da geht es etwa um Robotik, Mobilität und Data Science, oder, etwas konkreter: um virtuelle Produktionsprozesse, also eine Art Schnittstelle zu VFX (Visual Effects, dazu gibt es bereits einen Lehrstuhl an der HFF). ""Es ist so, dass eine Filmhochschule mit der Technik immer schon am intensivsten zusammenarbeitet im Bereich der Kunsthochschulen"", sagt Reitz. Die Studierenden in digitale Forschungsfelder einzubinden, sei immens wichtig. Und wie bei dem zweiten großen Zukunftsziel, dem Digital Art Center, pocht die HFF-Chefin hierbei vor allem auf den Gemeinschaftssinn.

Ohne übergreifende Zusammenarbeit ginge beim Digital Art Center wohl überhaupt nichts voran. Eine ""Innovationsschmiede"" für München stellt sich Bettina Reitz vor, ""wo man neue Entwicklungen im Bereich der Visualisierung und Soundentwicklung vorantreiben will""; wo die Wahrnehmung der Menschen erweitert und sensibilisiert werden soll. Was letztlich dem Medium Film und dem Kino, aber eben auch der Musik zugutekommen soll. Deshalb sind die treibenden Kräfte hinter dem Projekt die HFF und die Hochschule für Musik und Theater beziehungsweise Bettina Reitz und Bernd Redmann - ""immer auch im Schulterschluss mit Dieter Rehm von der Akademie der Bildenden Künste und Hans-Jürgen Drescher von der Theaterakademie August Everding"", wie Reitz betont. Über diese interdisziplinäre Kooperation war schon Anfang 2019 öffentlich spekuliert worden. Konkrete Details und Zeitpläne gibt es zwar noch immer nicht. Aber Bettina Reitz sagt: ""Der Wille ist mehr als da.""

Spannend wäre das schon, so ein moderner Raum für Experimente, mitten in der Landeshauptstadt. Außerhalb des Curriculums der Hochschulen. Und auch die Öffentlichkeit soll eingebunden werden. In Vor-Corona-Zeiten habe Reitz eine Exkursion nach Zürich gemacht, wo es, in der Kunsthochschule, ""eine Art digital Lab"" bereits gebe. ""Das war alles sehr beeindruckend"", schwärmt sie.

Und dann holt Bettina Reitz weiter aus, löst sich vom Mikrokosmos ihrer Hochschule, um die persönliche Wunschgesellschaft zu skizzieren. Sie spricht nahezu druckreif, wenn sie wortreich die Zukunft ausmalt. ""Ich würde mir sehr wünschen, dass die Menschen erkennen, dass wir nur in einer Gemeinschaft und als Teamplayer überlebensfähig sind. Keiner ist alleine so gut, um besser als alle miteinander zu sein. Die Differenzierung und der Reichtum der unterschiedlichen Lebenswelten sind doch die große Qualität unseres Seins auf dieser Welt. Eine Entwicklung in diese Richtung ist mein Lebenstraum.""

Den habe sie bereits als Kind gehabt, wie sie sagt. Bettina Reitz ist auf dem Land groß geworden, in einem kleinen Ort im Taunus. ""Wir Mädchen waren in der Minderheit, und so mancher Rock wurde zerrissen, wenn wir über die Zäune sprangen, weshalb mir meine Mutter irgendwann Hosen angezogen hat zum Spielen."" Gemeinsame Entdeckungen wären nicht möglich geworden, wenn die Freunde sich nicht als Gruppe zusammengerauft hätten. Reitz sagt: ""Das ist das, was mich in diesem Leben immer wieder beschenkt: Dass man mit unterschiedlichen Menschen zusammen Abenteuer erleben kann, den eigenen Horizont erweitern darf, jeden Tag aus Fehlern lernt.""

Digitalisierung, kollektives Lernen, Natur. Drei wichtige Säulen in der Gedankenwelt der Bettina Reitz. Ihre Zukunft für alle könnte so schön werden.";https://www.sueddeutsche.de/muenchen/himmlische-aussichten-sz-serie-folge-3-highway-to-sell-1.5002226;sz.de;Bernhard Blöchl
22.07.2020;Was ist los in Marbach?;"Im November 2019 hat der Bundestag dem Literaturarchiv Marbach 19 neue Stellen zugesagt und damit eine Aufstockung des Budgets um 2,5 Millionen Euro Förderung. Seitdem ist in Marbach etwas im Gange, das die Archivdirektorin Sandra Richter heute diplomatisch als einen ""Prozess"" umschreibt. Dieser Prozess spielte sich zuletzt vor den Augen der breiten Öffentlichkeit ab: Mitarbeiter berichteten in der Lokalpresse anonym von ""Führungsproblemen"" und einer ""deutlich verschlechterten"" Stimmung im Haus seit Richters Amtsantritt 2019. Die FAZ zitierte anonym Mitarbeiter, die die Qualität von Richters germanistischer Arbeit bemängelten und ihr Interviews im Lufthansa-Magazin ankreideten. Ein ""Brandbrief"" der Betriebsratsvorsitzenden Ulrike Weiß an das Kuratorium, in dem von einer ""desolaten Lage"" die Rede war, gelangte an die Öffentlichkeit. Darin hieß es unter anderem, die Direktorin handle ohne Kenntnisse ""interner Abläufe"" und die Situation verhindere eine ""konstruktive und vertrauensvolle Zusammenarbeit"".
SZ am Sonntag Newsletter Banner

Im Namen des Kuratoriums wies der Präsident der Deutschen Schillergesellschaft, Peter-André Alt, die Vorwürfe zwar zurück. Die neue Direktorin habe wichtige Weichen gestellt, die öffentliche Reputation des Hauses sei unvermindert hoch und die Generalkritik nicht nachvollziehbar. Trotzdem beschäftigt sich mittlerweile auch die baden-württembergische Landespolitik mit den Verwerfungen: Der Stuttgarter Landtagsabgeordnete Martin Rivoir (SPD) stellte eine Anfrage an das zuständige Wissenschaftsministerium, um die Vorgänge am Literaturarchiv ""zu einem parlamentarischen Vorgang zu machen"". Die Antwort wird erst in einigen Wochen erwartet.
Sie versteht die digitale Öffentlichkeit als selbstverständliche Vorbedingung

Von den Entscheidungen, die Sandra Richter jetzt mehr oder minder anonym vorgehalten werden, betreffen zwei das Personal. Im April dieses Jahres wurde die Verwaltungsdirektorin Dagmar Janson freigestellt, zuvor war die Ausstellungsmacherin Heike Gfrereis an ihre alte Wirkungsstätte zurückgekehrt. Beides wurde im Haus offenkundig nicht ausnahmslos begrüßt. Nach einem Konflikt mit dem Amtsvorgänger Sandra Richters, Ulrich Raulff, war Gfrereis, die für ihre Literaturausstellungen von der Kritik zuletzt wieder viel Applaus bekam, von 2017 bis 2019 beurlaubt worden. Richter wurde außerdem vorgeworfen, sie halte sich zu häufig bei ihrer Familie in Frankfurt auf und zu selten in Marbach. Das Archiv jedoch verlange eine intensive Präsenz. Aber auch ohne interne Querelen ist der Direktorenposten in Marbach eine komplexe Aufgabe. Außer dem Archiv umfasst die Institution das Schiller-Nationalmuseum und das Literaturmuseum der Moderne, das Haus widmet sich der Forschung und der Programmarbeit im selben Maße wie der Pflege der Bestände. Verlage wie Suhrkamp, S. Fischer und Rowohlt haben ihre Verlagsarchive nach Marbach ausgelagert, auch deshalb liegt der Schwerpunkt der Sammlung auf der Literatur des 20. Jahrhunderts. Schon länger bekannt ist, dass die Räume ihre Kapazitätsgrenzen erreicht haben, ein neues Magazingebäude ist in Planung, einen Termin für den Baubeginn gibt es jedoch noch nicht. Auch diese Unsicherheit sorgt unter den 260 Mitarbeitern für eine gewisse Unruhe.

Der Konflikt, der jetzt eskaliert, ließe sich leicht als Richtungsstreit zwischen Modernisierern und Traditionalisten begreifen. Sandra Richter hat sich als Vertreterin der Digital Humanities profiliert. Ihre Ankündigung, das Archiv in Zukunft neben Kafka und Koselleck auch Computerspiele sammeln zu lassen, weil es sich auch dabei um erzählerische Formen handele, hat einige aufmerken lassen.

Die digitale Öffentlichkeit versteht sie als selbstverständliche Vorbedingung jeder institutionellen Arbeit, also der Arbeit eines Literaturarchivs. Neue Aufmerksamkeit möchte sie Manuskripten schenken, die direkt am Computer geschrieben wurden, zu denen es also gar keine von Hand geschriebene Vorlage mehr gibt und mit deren Archivierung sich Marbach fortan häufiger wird beschäftigen müssen. Gerade erst vor einigen Tagen ist laskerschuelerarchives.org online gegangen, ein digitales Archiv der Gedichte Else Lasker-Schülers, eine Kooperation zwischen Marbach und der National Library of Israel. Das Projekt steht emblematisch für das Literaturarchiv Marbach, wie Richter es sich vorstellt: digital, publikumsnah, international. Die Sammlung sei zwar nach wie vor das Kernstück des Literaturarchivs, sagt Richter. Ohne die Forschung und die Vermittlung aber sei die Sammlung nicht lebendig. Dass die Seite am Dienstag vorübergehend nicht erreichbar war, kann man für ein böses Omen halten. Oder aber für einen Hinweis darauf, dass Richter recht hat, wenn sie mehr Gelder in die technische Infrastruktur stecken möchte. Womit wir wieder am Anfang wären. Nicht zuletzt geht es um die Verteilung der Gelder, die der Bundestag dem Literaturarchiv im vergangenen Herbst zugesagt hat.

Weil das Literaturarchiv jeweils zur Hälfte vom Bund und dem Land Baden-Württemberg finanziert wird, kann Marbach mit den Aufstockungen erst dann verbindlich rechnen, wenn Stuttgart nachgezogen hat. Von der Landesregierung in Stuttgart allerdings gibt es erst Zusagen für 1,5 Millionen Euro, eine Million weniger als der Bund. Von den 19 neuen Stellen konnten erst fünf tatsächlich besetzt werden.
Die Angegriffene wird aus dem Streit wohl gestärkt hervorgehen

Wie die frischen Mittel verteilt werden sollen, darüber gibt es naturgemäß unterschiedliche Ansichten. Sandra Richter, die unter anderem mit dem Höchstleistungsrechenzentrum HLRS in Stuttgart kooperiert, um das ""Data Science Center für Literatur"" aufzubauen, sähe nicht zuletzt bei der Technikabteilung erhöhten Personalbedarf. Gerade die Corona-Pandemie, während der auch in Marbach ein Großteil der Arbeit ins Digitale verlegt wurde, habe gezeigt, wie schnell die Infrastruktur des Archivs an ihre Belastungsgrenzen gerate, so Richter. Um Ausstellungen und Projekte in Zukunft besser auf die Wünsche des Publikums abstimmen zu können, schwebt ihr unter anderem vor, Nutzer- und Besucherdaten auszuwerten. Die Arbeit in Marbach könne außerdem mit der Forschung an den Universitäten viel enger verknüpft werden, die Produktion moderner Kataloge und Datenbanken verlange Big-Data-Kompetenz. Einige Abteilungen sollen dieser digitalen Agenda offen gegenüberstehen, so Richter. Andere, das klingt da durchaus an, zeigen nicht denselben Enthusiasmus.

Ungefähr genauso wahrscheinlich aber ist, dass sich die Gespräche über die zukünftige Ausrichtung und den Einsatz der Mittel, die derzeit laufen und an denen Marbach-typisch Vertreter zahlreicher Institutionen und Gremien beteiligt sind, tatsächlich auf einem guten Weg befinden. Und dass die neue Direktorin aus dem öffentlich ausgetragenen Streit gestärkt hervorgeht. Nachdem ihr in aller Öffentlichkeit schlechter Stil, schlechte Stimmung, schlechte Germanistik und häufige Abwesenheit vorgeworfen wurde, könnten sich die Zuwender demonstrativ hinter Richter stellen, um das ganze Gefüge zu stabilisieren. Und an Stabilität dürfte ihnen angesichts der anstehenden Um- und Anbauten wahrscheinlich gelegen sein.";https://www.sueddeutsche.de/kultur/deutsches-literaturarchiv-sandra-richter-1.4974069;sz.de;Felix Stephan
14.06.2020;Warum es sich jetzt lohnt, Linux auszuprobieren;"Die Windows-Alternative Linux fristet als Betriebssystem für PCs bislang ein Nischendasein. Nur drei bis vier Prozent der Deutschen nutzen es privat. Linux gilt vielen als Betriebssystem der Tüftler und Nerds, kostenlos und schnell, für Laien aber schwer zu verstehen und aufwendig einzurichten. Doch Letzteres könnte sich nun ändern.

Der Grund dafür ist ausgerechnet der Marktführer Microsoft. Das blaue Fenster-Logo von Windows erscheint täglich auf rund 80 Prozent aller privaten Computer in Deutschland. Aber schon jetzt ist es möglich, Linux innerhalb von Windows 10 zu benutzen. WSL, das Windows Subsystem für Linux, startet Linux in einer Art virtuellen Schachtel in Windows. Man nutzt Linux wie ein einfaches Windows-Programm. Das hat den Vorteil, dass Nutzer auf einem Computer mit zwei Betriebssystemen arbeiten und einfach zwischen den Systemen hin- und herwechseln können. Denn viele Programme laufen auf Linux schneller, andere wie Word oder Excel gibt es dagegen nur für Windows.
Linux ausprobieren war noch nie so einfach

Kürzlich verkündete Microsoft, dass die zweite Version des Subsystems - WSL2 - noch weiter ausgebaut wird. Dass der Software-Konzern aus Redmond in Linux investiert, hat einen Grund. ""Microsoft laufen in einigen Bereichen schon länger die Entwickler weg"", sagt Kristian Kißling vom Linux-Magazin, einem Fachmagazin für Linux-Nutzer. Professionelle Software-Entwickler arbeiten heutzutage überwiegend mit Linux, insbesondere im Cloud-Computing und in Rechenzentren. Einige von ihnen benötigen gelegentlich aber auch Windows, etwa um sich mit Kollegen aus anderen Abteilungen auszutauschen. Eine recht beliebte Lösung hierfür heißt Dual Boot: Auf einem Rechner werden zwei gleichberechtigte Betriebssysteme installiert, und Benutzer wechseln hin und her.

Für diese Nutzer ist das verbesserte WSL eine große Hilfe: Sie können ihr Linux-System nun direkt aus Windows heraus starten. ""Dual Boot ist tot"", schrieb der Entwickler Dimitris Poulopoulos auf dem Blog Towards Data Science. Auch Gerald Pfeifer, technischer Vorstand beim Open-Source-Unternehmen Suse, sagt, einige Dual-Boot-Nutzer könnten umsteigen. ""Jeder weiß, wie ätzend ständiges Booten ist. WSL kann da komfortabler sein"", sagt Pfeifer. Dass eingefleischte Linux-Nutzer wegen WSL zu Windows wechseln, hält er aber für sehr unwahrscheinlich.

Kristian Kißling vom Linux-Magazin glaubt sogar, dass das Gegenteil der Fall ist: Windows-Nutzer könnten durch WSL auf den Geschmack von Linux kommen und schon bald die offene Linux-Welt komplett Windows vorziehen. Denn Linux-Systeme gelten als schneller und sicherer, sie können überdies leichter an persönliche Vorlieben angepasst werden. Allerdings kosten Installation und Einrichtung Linux-basierter Betriebssysteme oft Nerven. Das schreckt viele Windows- und Apple-Nutzer ab. Mit dem verbesserten Subsystem sinkt diese Hemmschwelle. Zudem muss Windows nicht erst deinstalliert werden, damit sich Linux testen lässt. Linux einfach auszuprobieren war wohl noch nie so leicht wie heute.

Bei Linux von einem Betriebssystem zu sprechen, ist eigentlich nicht korrekt. Linux bezeichnet eine Familie von unterschiedlichen Betriebssystemen, sogenannten Distributionen, in denen der gleiche Kern steckt. Ubuntu, Debian und Arch zählen zu den bekanntesten. Den Linux-Kernel kann sich im Gegensatz zum NT-Kernel, auf dem Windows basiert, jeder herunterladen. Sein Quellcode ist offen zugänglich. Das Prinzip solcher Open-Source-Software ist einfach: Sie gehört im Grunde allen, jeder kann sie benutzen und verändern.

Auch Max Mehl von der Free Software Foundation Europe, einer Organisation, die sich für offen zugängliche Software einsetzt, sieht die Hürden für den Wechsel zu Linux fallen. Zudem liefert zum Beispiel der chinesische Hersteller Lenovo einige Laptops mittlerweile mit vorinstallierten Linux-Betriebssystemen aus. Ein Grund für die Vorherrschaft von Windows ist schließlich, dass es auf vielen Geräten bereits vorinstalliert ist.
Mit einem Kernel kommen Nutzer nie in Kontakt - doch ohne ihn läuft gar nichts

Microsoft umgarnt Linux schon länger. Azure, die Cloud-Plattform des Konzerns, läuft zu einem großen Teil unter Linux. Die WSL-Offensive passt ins Bild. Und die Verbesserungen sind beachtlich: Während WSL1 noch keinen echten Linux-Kernel benutzte, sondern nur dessen Verhalten simulierte, läuft mit WSL2 ein vollständiger Linux-Kernel verschachtelt in Windows. Ein Kernel ist genau das Stück Software, das direkt mit der Hardware kommuniziert, zum Beispiel mit Tastatur oder Grafikkarte. Wenn man sich die Software eines Computers in mehreren Schichten vorstellt, befindet sich der Kernel ganz unten. Programme wie Internet-Browser bilden die oberste Schicht. Nutzer kommen also nie mit einem Kernel in Kontakt, ohne ihn läuft jedoch gar nichts. Um diesen Kernel - auf Deutsch Systemkern - herum entsteht in mehreren Schichten das Betriebssystem.

""Der Unterschied von WSL1 zu WSL2 ist in etwa wie der zwischen einem Tofu-Burger und einem Rindfleisch-Burger"", sagt Gerald Pfeifer von Suse. ""Um Linux-Anwendungen laufen zu lassen, braucht man nicht zwingend einen Linux-Kernel. Etwas, das aussieht wie einer, reicht schon."" WSL2 kann deshalb deutlich mehr als WSL1, aber auch nicht alles: Beim Zugriff auf Grafikprozessoren geht auch WSL2 noch den Weg über Windows. Eine direkte Einbettung in WSL stehe ""auf der Roadmap"", sagt eine Microsoft-Sprecherin.

Ob Microsoft mit den WSL-Verbesserungen Linux tatsächlich zu mehr Popularität verhelfen wird, wie Kißling vom Linux-Magazin glaubt, bleibt abzuwarten. Allerdings hat es der Linux-Kernel auch ohne Microsofts Hilfe schon in viele Rechner geschafft: Er steckt in Firmennetzwerken, den meisten Servern und in leicht modifizierter Form auch in Android, dem weltweit populärsten Betriebssystem überhaupt, das auf den meisten Handys läuft. Nicht zuletzt laufen die 500 schnellsten Supercomputer der Welt alle mit Linux. ";https://www.sueddeutsche.de/digital/windows-linux-microsoft-betriebsystem-1.4965794;sz.de;Julian Rodemann
04.06.2020;Fit im Programmieren;"Bereits im Sommer 2018 fassten drei Studierende der Goethe-Universität Frankfurt den Entschluss, Studierenden aller Fachbereiche Programme zu Themen wie Big Data, Data Science und Künstliche Intelligenz anzubieten. Da konnten sie freilich noch nicht ahnen, wie immens wichtig Digitalkompetenz im Jahr 2020 angesichts der Pandemie werden würde. ""Es gab kein fächerübergreifendes Angebot, sondern lediglich Veranstaltungen in einzelnen Fachbereichen. Dabei ist Digitalisierung für alle Studierenden relevant"", sagt Georgios Brussas, Wirtschaftsinformatiker und einer der drei Gründer des gemeinnützigen Vereins Tech Academy.

Unter diesem Namen starteten im Sommersemester 2019 zwei je auf ein Semester angelegte Programme, deren Teilnahme kostenlos ist. Für die 60 Plätze bewarben sich mehr als 200 Studierende aus 16 Studiengängen, von der Medizin über die Psychologie bis zur Physik. Sie konnten sich entweder für das Web-Development-Programm entscheiden, in dem sie erlernen, wie sich eine Webseite programmieren lässt. Alternativ für das Data-Science-Programm, bei dem die Programmiersprachen Python und R sowie die Auswertung von Datensätzen im Mittelpunkt stehen. Das Prinzip der Kurse: Zuerst erwerben die Teilnehmer Theoriekenntnisse in Onlinekursen. Danach bilden sie Teams, um gemeinsam an Projekten zu arbeiten. ""IT-Vorkenntnisse sind nicht notwendig, entscheidend ist vor allem die Eigenmotivation, denn das Programm ist zeitintensiv"", sagt Brussas, einer der beiden Vorstandsmitglieder. Bis zu vier Stunden E-Learning müsse man für die Theorie pro Woche einplanen. Dazu kommt die Zeit für Workshops, Meetings und die Projektarbeit - das alles parallel zum eigenen Semesterprogramm.

Um das Programmieren oder den Umgang mit großen Datenmengen schnell zu verstehen, hilft den Neueinsteigern ein ehrenamtliches Team aus Studenten vor allem IT-affiner Studiengänge. Auch Mitarbeiter aus IT-Firmen stehen bei wöchentlichen Treffen mit Rat und Tat bereit, zum Beispiel bei den wöchentlichen Treffen, die eigentlich in den Räumlichkeiten der Universität und der Partnerunternehmen, nun aber online stattfinden. ""Um die Studierenden zu unterstützen, ist es wichtig, eine Community aufzubauen"", sagt Brussas. Eine Exkursion sowie Partner aus der Wirtschaft, wie zum Beispiel die Direktbank ING und die Unternehmensberatung Bearing Point, sollen berufliche Perspektiven aufzuzeigen. So könnten die Studierende sehen, wie sich Digitalisierungswissen später im Job einsetzen lässt.

Ende April startete der dritte Jahrgang, mit 70 Studierenden. Wer erfolgreich die Theorie erlernt und das Projekt bis zum Sommer beendet, erhält von der Tech Academy ein Zertifikat, das man sich jedoch nicht auf seine ECTS-Punkte anrechnen lassen kann. Doch die jungen Menschen kümmert das wenig. ""Sie kommen vor allem zu uns, weil sie sich für Digitalisierung interessieren und weil sie das Wissen für ihr Studium oder danach im Beruf brauchen, oder weil sie meinen, so eine bessere Chance auf dem Arbeitsmarkt zu haben"", sagt Brussas. Die Tech Academy laufe in Frankfurt sehr gut. Sie könnte auch an anderen Hochschulen in Hessen oder in anderen Bundesländern funktionieren.";https://www.sueddeutsche.de/karriere/studenteninitiative-fit-im-programmieren-1.4924457;sz.de;Benjamin Haerdle
15.05.2020;50 neue Professuren;"Von Rosenheim bis Hof, von Aschaffenburg bis Kempten und von Deggendorf bis Bayreuth: Bis 2023 werden in allen sieben Regierungsbezirken des Freistaats 50 neue Professuren für Künstliche Intelligenz eingerichtet. ""Die Kommission hat sich die Auswahl nicht einfach gemacht"", sagte Wissenschaftsminister Bernd Sibler (CSU) am Freitag in München zu den Ergebnissen des landesweiten Wettbewerbs der Hochschulen um die Forschungslehrstühle. ""Die ersten 20 sollen bereits im Herbst dieses Jahres besetzt werden, die weiteren 30 folgen sukzessive bis 2023. Im Ergebnis erhalten alle bayerischen Universitäten jeweils mindestens zwei neue KI-Professuren"", sagte Sibler. Besonders erfolgreich sei die Universität Bamberg mit sieben Professuren gewesen. Unter den Hochschulen für angewandte Wissenschaften waren die TH Deggendorf, die OTH Regensburg und die HAW Coburg mit jeweils mindestens zwei Professuren besonders erfolgreich. Darüber hinaus konnten sich zwei Kunsthochschulen, nämlich die HFF München und die Hochschule für Musik Nürnberg, durchsetzen.

In dem Wettbewerb hatten Bayerns Hochschulen 175 Bewerbungen eingereicht; diese wurden von einer 18-köpfigen Expertenkommission ausgewertet. Sibler betonte, dass die 125 jetzt nicht erfolgreichen Bewerbungen keine Verlierer seien. Die ""großartige Expertise"", die nun vorliege, gehe nicht verloren, sondern werde im Ministerium weiter verfolgt. Wesentliche Kriterien bei der Auswahl seien die Qualität, die Schlüssigkeit und die Passgenauigkeit der Konzepte in das regionale Umfeld gewesen.

Die 50 Stellen komplettieren das von Ministerpräsident Markus Söder (CSU) in der Hightech-Agenda angekündigte Forschungsnetzwerk zur KI in Bayern. Dazu wurden bereits 50 weitere Lehrstühle mit festen Standorten übers Land verteilt. Das KI-Zentrum ist in München, dort alleine mit 22 Lehrstühlen. Ein neues ""KI Mission Institute"" soll die KI-Aktivitäten in Bayern verzahnen. In Würzburg sind zehn, in Ingolstadt ebenfalls zehn und in Erlangen acht neue Lehrstühle geplant.

Sibler lobte, dass die ausgewählten Konzepte im Netz nun alle vier fachlich-thematischen Schwerpunktbereiche des bayerischen KI-Netzwerks abbilden würden: Intelligente Robotik, Mobilität, Gesundheit und Data Science. Für den Aufbau der Professuren stehen 600 Millionen Euro zur Verfügung.

""Mit insgesamt 100 neuen Professuren auf diesem Zukunftsgebiet legen wir die Grundlage dafür, dass Bayern als Wissenschaftsstandort seine Sichtbarkeit und Wettbewerbsposition auf diesem begehrten Markt entscheidend verbessern kann"", betonte Sibler. Das sei auch mit Blick auf die Belebung der Wirtschaft und von Wertschöpfungsketten nach der Corona-Pandemie wertvoll.

Künstliche Intelligenz ist ein Teilgebiet der Informatik, das sich mit der Automatisierung intelligenten Verhaltens und dem maschinellen Lernen befasst. Dabei soll die menschliche Intelligenz mit Maschinen simuliert werden, insbesondere mit Computersystemen.";https://www.sueddeutsche.de/bayern/wissenschaft-50-neue-professuren-1.4908817;sz.de;DPA
07.05.2020;Die Experten von morgen;"New Work, Social Web, Industrie 4.0: Die Digitalisierung der Arbeitswelt hat viele Namen und Ausprägungen. Bis zum Jahr 2035 führt dieser fortschreitende Prozess ""zu größeren Verschiebungen von Arbeitsplätzen zwischen Branchen, Berufen und Anforderungsniveaus"", heißt es im IAB-Kurzbericht des Instituts für Arbeitsmarkt- und Berufsforschung von 2018. Entgegen weitläufigen Befürchtungen entstehen durch die technischen Entwicklungen aber wenigstens so viele Arbeitsplätze neu, wie abgebaut werden. Verschwinden werden überwiegend Jobs in den produzierenden Bereichen wie Maschinen- und Fahrzeugbau sowie in der Metallbearbeitung. In der Branche ""Information und Kommunikation"" hingegen entstehen weitere Berufsbilder. Bereits in den vergangen Jahren haben sich durch die Digitalisierung viele neue Profile herausgebildet, die in Stellenanzeigen meist mit englischem Jobtitel auftauchen. Eine Auswahl.
Online Marketing Manager

Meist ist das die erste Stelle, die geschaffen wird, wenn ein Unternehmen einen Webshop einrichtet oder die erste Online-Werbekampagne plant: die Position des Online Marketing Managers. ""Als Abteilungsleiter des Online-Geschäfts übernimmt er sogar direkt eine Führungsposition"", sagt Daniel Gremm, IHK-Trainer für Online-Marketing. Wie erreicht ein Unternehmen potenzielle Neukunden? Läuft es rund im Online-Shop? Kommt der Newsletter gut an? Wie hoch sind die Klickraten? Um solche Fragen beantworten zu können, muss ein Online Marketing Manager den Markt ebenso gut kennen wie seine Zielgruppe und aktuelle Social-Media-Trends. ""Flexibilität und Kreativität sind daher fast wichtiger als das noch in vielen Ausschreibungen verlangte abgeschlossene Hochschulstudium in BWL oder Wirtschaftsinformatik"", sagt Gremm. Während der Online Marketing Manager eines mittelständischen Betriebs eigenverantwortlich für dessen Positionierung im Internet verantwortlich ist, vereint er in größeren Unternehmen ein ganzes Team unter sich.
BIM-Manager

Bevor Architekten, Bauingenieurinnen und Statiker mit der Arbeit beginnen, kommt Carlos Vicente zum Einsatz. Er lässt eine Drohne über das Werksgelände fliegen, vermisst die Lagerhalle, die erweitert werden soll, oder scannt das Einfamilienhaus. Aus den gesammelten Daten entwirft der Geschäftsführer des Münchner Unternehmens Cavicon ein digitales 3-D-Modell der Baustelle, das er an die beteiligten Fachplaner schickt. Vicente ist BIM-Manager. BIM, das steht für Building Information Modeling, auf Deutsch Bauwerksdatenmodellierung. ""Im Gegensatz zur klassischen Bauplanung, bei dem alle Beteiligten getrennt voneinander Pläne entwerfen und dann schauen, wie sie kombiniert werden können, arbeiten sie mit der BIM-Software vernetzt am gleichen digitalen Modell"", sagt Vicente. Der BIM-Manager ist an der Schnittstelle zwischen Ingenieur und Bauherr tätig. ""Prädestiniert für den Job sind besonders Architekten und Ingenieure mit einem Händchen für IT."" Denn es sei ebenso wichtig, sich mit der Brandschutzordnung auszukennen, wie bei großen Datenmengen den Überblick zu behalten. In Zertifikatskursen können sich Personen mit dem erforderlichen Vorwissen zum BIM-Manager ausbilden lassen.
Data Scientist

Das Managermagazin Harvard Business Review hat den Beruf des Data Scientists bereits 2012 zum ""Sexiest Job des 21. Jahrhunderts"" gekürt. Datenanalysten hätten seltene wissenschaftliche und analytische Fähigkeiten, die sehr gefragt seien, so die Begründung der Autoren-Jury. Die wachsende Datenmenge, die in der Industrie und der Forschung entsteht, wäre nutzlos, würde sie nicht jemand erfassen und auswerten. So extrahieren die IT-Experten relevante Informationen aus den Daten und bereiten sie verständlich für die Marketingabteilung oder die Produktion der jeweiligen Organisation auf. Ob Forschungseinrichtung oder Luftfahrtunternehmen: Je nach Arbeitgeber und Auftrag beschäftigen sich die Datenanalysten mit sehr unterschiedlichen Themen. So können sie Prognosen für den Stromverbrauch erstellen oder Betrieben dabei helfen, Logistikabläufe zu optimieren. Bundesweit wächst die Zahl der Data-Science-Studiengänge, in denen auch Grundlagen der Betriebswirtschaft vermittelt werden. Um Datenanalyst zu werden, kommt allerdings auch ein Informatik- oder Mathematikstudium mit Schwerpunkt Statistik infrage.
SEA-Manager

Dort, wo sich viele Menschen tummeln, machen auch Unternehmen auf sich aufmerksam. Früher taten sie das ausschließlich an Litfaßsäulen, in Zeitungen und auf Plakatwänden, heute ist es für sie selbstverständlich, auch im Internet für sich zu werben. Dadurch ist nicht nur die Reichweite der Werbekampagnen gestiegen, sondern auch der Wettbewerbsdruck hat zugenommen. Deshalb setzen Unternehmen vermehrt auf Search-Engine-Advertising- oder SEA-Manager. Sie arbeiten eng mit SEO-Managern zusammen, die Suchmaschinenoptimierung betreiben und dafür sorgen, dass die Webseite des eigenen Unternehmens bei Google oder anderen Suchmaschinen möglichst weit oben auftaucht. SEA-Manager gehen einen Schritt weiter, sie schalten bezahlte Google-Anzeigen und entscheiden, bei welchen Suchwortkombinationen diese auftauchen. Daniel Gremm, der für die IHK bundesweit SEA-Lehrgänge gibt, sagt: ""Neben betriebswirtschaftlichem Grundwissen sollten SEA-Manager unbedingt Leidenschaft für das beworbene Produkt mitbringen.""
E-Learning-Konzepter

Kopfrechnen mit der Lern-App Anton, auf der Online-Plattform Sofatutor durch die europäische Geschichte reisen und im digitalen Lernraum Englischvokabeln pauken: Nicht erst seit den krisenbedingten Schulschließungen sind digitale Lernformate stark nachgefragt. Entwickelt werden solche E-Learning-Plattformen von Konzeptern wie Nicole Stegelmeyer vom Redaktionsbüro E-Schoolbook. Im Gegensatz zu analogen Schulbüchern sei es bei digitalen Lernmedien besonders wichtig, ""die Inhalte interaktiv aufzubereiten, den Stoff spielerisch zu vermitteln und junge Lerner mit einem Belohnungssystem zu motivieren"", sagt sie. Besonders geeignet ist der Beruf für Sprachwissenschaftler, Mediengestalter oder Pädagogen, die sich mit Redaktionssystemen auskennen und Lerninhalte kreativ und übersichtlich vermitteln können. ""Das ist häppchenweises Lernen"", sagt Stegelmeyer. Daher kooperiert E-Schoolbook mit Illustratoren, Tonstudios und Videofirmen. Und durch die technische Weiterentwicklung bleibe der Beruf abwechslungsreich, ""mit neuen Möglichkeiten fürs E-Learning"".
Social Media Manager

""Der Kunde kauft heutzutage eher eine Marke als ein Produkt"", sagt Nellie Winter in einem Video. Die freiberufliche Social Media Managerin vermittelt zwischen potenziellen Kunden und Unternehmen. Dafür pflegt sie deren Profile in sozialen Netzwerken wie Facebook und Xing sowie bei Twitter, Instagram und Youtube. Regelmäßig postet sie Bilder, veröffentlicht Videos und Textbeiträge. Als Social Media Managerin gestaltet Winter unter der Leitung des Online Marketing Managers das Image des Unternehmens mit. Da hierbei Authentizität gefragt ist, gehört Community Management ebenfalls zu Winters Aufgaben. Sie antwortet auf Kommentare und kommuniziert über Umfragen mit den Nutzern. Wie auch SEA-Manager wertet sie per Webanalyse deren Verhalten aus: Welcher Beitrag kommt besonders gut an? Wer reagiert am häufigsten auf Videos und Bilder? Viele Jobanwärter kommen aus dem Bereich Öffentlichkeitsarbeit und PR, haben Marketing oder Medieninformatik studiert und kennen die notwendigen Online-Tools aus Schulungen; das Wissen wird in Zertifikatskursen vermittelt. Wie auch Winter arbeiten einige von ihnen komplett ortsunabhängig. ""Alles, was ich brauche"", sagt Winter, ""das sind mein Laptop, mein Handy und stabiles Internet.""
User Experience (UX) Designer

Wie erklärt man den eigenen Job verständlich und unterhaltsam auf einer Party? Das fragte sich auch die UX-Designerin Megan Wilson und kam auf eine Idee: die Bananen-Analogie. ""Wenn eine Obstsorte von Mutter Natur für die Benutzererfahrung entworfen wurde"", schreibt Wilson in ihrem Blog UX Motel, ""wäre es die Banane."" Ihre Farbe signalisiert, ob sie genießbar ist oder nicht, sie ist ohne Weiteres transportabel und ohne Hilfsmittel essbar. Keine Kerne, keine Stiele und keine klebrigen Hände nach dem Verzehr. Doch was hat eine Banane mit Technologie zu tun? Ganz einfach: UX-Designer gestalten Schnittstellen zwischen Mensch und Maschine, die logisch und in sich geschlossen funktionieren. In der Ausbildung zum Mediengestalter oder einem Interaction-Design-Studium lernen künftige UX-Designer, nutzerfreundliche Ideen zu entwickeln und Prototypen zu skizzieren. Fällt es Nutzern letztlich genauso leicht, Apps oder das Tastfeld einer Waschmaschine zu bedienen, wie eine Banane zu essen, haben UX-Designer einen guten Job gemacht.
Affiliate Manager

Eine Modebloggerin erwähnt in ihren Beiträgen bewusst bestimmte Marken und setzt Links zu den jeweiligen Modelabels. Ein Youtuber, der auf dem Internetportal Videos hochlädt, verlinkt den Online-Shop eines Rucksackherstellers unter seinem Reisevideo. Beide werben mit ihren Internetauftritten für Firmen; für die Kundenvermittlung bekommen sie eine Provision. Hergestellt werden die Vertriebskooperationen zwischen Unternehmen und Partnerwebseiten von Affiliate Managern, die entweder freiberuflich für das jeweilige Unternehmen arbeiten oder dort angestellt sind. Sie machen Webseiten ausfindig, die thematisch zum eigenen Produkt passen und die gewünschte Zielgruppe ansprechen. Anschließend stellen sie den Betreibern Werbemittel zur Verfügung, die auf der Webseite eingebaut werden können. Gefragt sind daher Menschen mit umfassenden Kenntnissen des jeweiligen Marktes sowie technischen Erfahrungen im Bereich Online-Shopping. Wer im Online-Marketing beschäftigt ist und zusätzliche Kompetenzen erwerben möchte, kann sich zum Affiliate Manager weiterbilden. Firmen erwarten von Letzteren kaufmännisches Know-how, teilweise auch ein abgeschlossenes BWL- oder Kommunikationswissenschaftsstudium.
Mobile Developer

83 Prozent der Deutschen haben ein Smartphone, heißt es in der ARD/ZDF-Onlinestudie 2019. Sie bilden die große Gruppe all jener, die regelmäßig mobil im Internet surfen und auf Apps zugreifen, um Nachrichten zu lesen, zu spielen oder online einzukaufen. Um sie zu erreichen, versuchen sowohl Traditionsunternehmen als auch Start-ups, ihre Inhalte für Smartphones und Tablets zu optimieren. Daher sind sie auf der Suche nach Mobile Developern. Die Programmierer sorgen unter anderem dafür, dass Webseiten und Online-Shops nicht nur auf dem Desktop, sondern auch am wesentlich kleineren Handybildschirm richtig angezeigt werden. Da der App-Markt boomt, arbeiten sie außerdem mit UX-Designern zusammen, um Apps weiterzuentwickeln und benutzerfreundlich zu gestalten. Der Einstieg ins Mobile Development gelingt am besten über das klassische Informatikstudium oder eine Ausbildung zum Softwareentwickler oder Webdesigner.
Supply Chain Manager

Selten war die Tätigkeit mit der sperrigen Bezeichnung ""Wertschöpfungskettenverwalter"" greifbarer als in den vergangenen Wochen. Denn wenn Supermarktregale wesentlich öfter nachgefüllt werden als gewöhnlich, Hersteller im Ausland ihre Produktion zeitweise einstellen müssen und es zu Transportverzögerungen kommt, haben Supply Chain Manager alle Hände voll zu tun. Sie verantworten den gesamten Warenstrom von der Beschaffung der Rohstoffe über die Produktion bis zum Verkauf der Endprodukte im Laden. Dazu lokalisieren sie die Lieferanten per GPS, nutzen Echtzeitinformationen über Lagerbestände, um den Materialbedarf zu planen, und sorgen dafür, dass die Rohwaren aus dem Lager zuerst zu den Produktionsstätten und später zu den jeweiligen Abnehmern transportiert werden. Zusätzlich wird dieser Prozess automatisch zurückverfolgt, um ihn langfristig zu verbessern. So vermitteln Supply Chain Manager zwischen der Produktion und den Lieferanten, holen sich Feedback der Kunden digital ein und halten Rücksprache mit dem Vertrieb. Das Fach Supply Chain Management bieten mittlerweile einige deutsche Hochschulen als Masterstudium an.
Feel Good Manager

In den meisten Stellenanzeigen werden Qualifikationen wie Kreativität und Einfühlungsvermögen verlangt: Feel Good Manager sind in erster Linie dafür zuständig, dass sich die Mitarbeiter am Arbeitsplatz wohlfühlen. Sie kümmern sich um die Work-Life-Balance der Angestellten, fungieren als Vermittler zwischen Beschäftigten und Vorgesetzten, auch bei Konflikten, und organisieren gemeinsame Ausflüge. Besonders im IT-Bereich konkurrieren die Unternehmen mit solchen Wohlfühlprogrammen um Fachkräfte. Vermehrt sind diese ""Beauftragten für Unternehmenskultur"", wie Feel Good Manager auch genannt werden, aber nicht nur in Start-ups, sondern laut dem Fachportal Goodplace auch in Unternehmen wie der Lufthansa und der Deutschen Post zu finden. Häufig sind sie Soziologen oder Wirtschaftswissenschaftler. Manche haben eine mehrmonatige Fortbildung im Bereich Konfliktmanagement oder Personalentwicklung absolviert. Feel Good Manager verstehen sich als Bindeglied zwischen Beschäftigten und Vorgesetzten, sie organisieren gemeinsame Ausflüge und versuchen, bei Konflikten zu vermitteln. Einen typischen Arbeitstag gibt es in der Regel nicht, denn der orientiert sich an den Fragen und Wünschen der Mitarbeiter.";https://www.sueddeutsche.de/karriere/neue-it-und-kommunikations-berufe-die-experten-von-morgen-1.4897451;sz.de;Rebekka Gottl
04.03.2020;Begehrte Forschungsstellen;"Großes Interesse an neuen Forschungsstellen für Künstliche Intelligenz: Im Wettbewerb um 50 geplante KI-Professuren haben Bayerns Hochschulen 175 Bewerbungen eingereicht. ""Künstliche Intelligenz wird in unserer Zukunft allgegenwärtig sein. Unsere Hochschulen sind entscheidende Wegbereiter: Technologien, die heute vielleicht noch in den Kinderschuhen stecken, werden hier umfassend erforscht und entwickelt"", sagte Wissenschaftsminister Bernd Sibler (CSU) in München. Die 50 KI-Professuren, die bei der Hightech Agenda Bayern zum Ausbau des KI-Netzes im Freistaat zusätzlich entstehen, ""werden dieser Forschung einen weiteren, kräftigen Schub verleihen"".

Bis Ende Februar konnten sich alle staatlichen Universitäten in Bayern, die Hochschulen für angewandte Wissenschaften sowie die Technischen Hochschulen und die Kunsthochschulen um die 50 neuen Forschungsstellen bewerben. Der von der Staatsregierung ausgelobte Wettbewerb sieht vor, 50 Professuren im Freistaat zu verteilen. Besonders erwünscht waren dabei gemeinsame, auch überregionale Bewerbungen.

Darüber hinaus wurden 50 weitere Lehrstühle mit festen Standorten bereits in Bayern verteilt. Das KI-Zentrum soll in München sein, dort alleine mit 22 neuen Lehrstühlen. Ein neues ""KI Mission Institute"" soll die gesamten KI-Aktivitäten in Bayern verzahnen. In Würzburg sind zehn, in Ingolstadt ebenfalls zehn und in Erlangen acht neue Lehrstühle geplant.

Bei den Bewerbungen seien alle antragsberechtigten 34 Hochschulen beteiligt. Sowohl die technologischen Schwerpunkte - Intelligente Robotik, Data Science, Gesundheit und Mobilität - als auch weitere Fachgebiete seien in den Bewerbungen abgebildet. Mehr als 85 Prozent der Anträge seien von mehreren Hochschulen gemeinsam gestellt worden. ""Kooperationen ermöglichen es den Partnern, von dieser Expertise gegenseitig zu profitieren. So entstehen eine hohe Dynamik und wertvolle Synergieeffekte."" Eine 18-köpfige Expertenkommission wird nun die Auswahl treffen. Bis Ende April wird das Wissenschaftsministerium entscheiden, welche Konzepte schrittweise bis 2023 umgesetzt werden. Die ersten KI-Professuren sollen bis zum Herbst 2020 besetzt werden. Dafür stehen 600 Millionen Euro bereit.";https://www.sueddeutsche.de/bayern/hochschule-begehrte-forschungsstellen-1.4831436;sz.de;DPA
09.02.2020;Mehr Zeit für die Forschung;"Die bayerischen Professoren sollen mehr Zeit für die Forschung bekommen. Das sieht die Hightech-Agenda von Ministerpräsident Markus Söder (CSU) vor, die auch eine Hochschulreform umfasst. Dabei soll das Lehrdeputat abgesenkt werden. Diese Idee dürfte auf große Gegenliebe an den Universitäten und Hochschulen für angewandte Wissenschaften (HAW) stoßen. Von angloamerikanischen Verhältnissen, wo teilweise fünf Wochenstunden Lehre üblich sind, träumen viele Hochschullehrer im Freistaat. Erst vor wenigen Wochen erschien eine Umfrage, in der sich die deutliche Mehrheit der bayerischen Professoren beklagt, dass die Lehre zu viel ihrer Zeit einnehme und kaum Raum für Forschung bleibe.

Von der neuen Offensive werden aber nicht alle profitieren: Der Plan von Wissenschaftsminister Bernd Sibler (CSU) sieht vor, Wissenschaftler zu entlasten, die besonders erfolgreiche Forscher sind. Insgesamt 600 Professoren-Stellen mit Budget für Mitarbeiter und Ausstattung sollen von Oktober an bis 2023 an die elf Universitäten und 17 HAW verteilt werden. Pro Hochschule soll die Zahl der Professoren um zehn Prozent steigen, derzeit arbeiten knapp 7000 in Bayern. ""Rein rechnerisch können wir das Stundendeputat von neun auf sieben Stunden runterbringen"", sagte Sibler der SZ. Welche Wissenschaftler in der Lehre entlastet werden und sich stärker auf die Forschung konzentrieren dürfen, entscheiden die Hochschulchefs.

Eine Gefahr, dass die Geistes- und Sozialwissenschaften dabei leer ausgehen, will Sibler nicht sehen. Dabei kritisieren Hochschulkenner seit Jahren, dass vor allem jene Unis und Professoren als erfolgreich gelten, die viele Drittmittel einwerben. Das gelingt besonders in den Naturwissenschaften. Auch die bayerischen Studentensprecher warnen vor einer Benachteiligung der Geisteswissenschaften und wünschen sich ein eigenes Förderprogramm. Denn die Hightech-Agenda ist auch Konjunkturprogramm und stärkt die Forschung in jenen Bereichen, von denen die Wirtschaft besonders profitiert: Informatik und Künstliche Intelligenz. Wirtschaftsminister Hubert Aiwanger (Freie Wähler) ist ebenso eingebunden wie Sibler. Die bayerischen Wirtschaftslobbyisten hatten die Zahl der verfügbaren Informatiker laut Sibler als Schlüssel zum Erfolg beschrieben. Die Regierung folgte dem Wunsch: Allein in diesem Bereich entstehen 5000 Studienplätze, verteilt über alle Hochschulstandorte. Insgesamt sind es 13 200. Trotzdem will Sibler den Hochschulen nichts vorschreiben. Er glaubt, dass sein Appell reicht: ""Alle Fächer an unseren Hochschulen und Universitäten kommen für das Forschungsbudget in Frage. Dieses Signal ist mir wichtig.""

Auch eine Benachteiligung der Lehre sieht Sibler nicht. Mit 600 von 1000 neuen Hightech-Professuren werde diese sogar ausgebaut, findet er. Schließlich brauche Bayern auch künftig Fachkräfte und Spitzenwissenschaftler. Von den zusätzlichen Jobs sollen zudem Wissenschaftler profitieren, die bisher nur befristet arbeiten.

Die Hochschulreform ist eine von vier Säulen der zwei Milliarden Euro teuren Hightech-Agenda, die Söder im Oktober 2019 präsentiert hatte. Von der ersten Reform seit 15 Jahren verspricht man sich modernere, international attraktive Unis mit lebhafter Startup-Szene und englischsprachigen Studiengängen, die miteinander kooperieren. Ein Wettbewerb zur Künstlichen Intelligenz soll die Vernetzung vorantreiben: 50 neue KI-Professuren sind ausgelobt, Ende Februar läuft die Bewerbungsfrist ab. Im Mai soll klar sein, wie die KI-Professuren verteilt werden. Weitere 50 entstehen an vier KI-Knotenpunkten in Ingolstadt (Mobilität), Würzburg (Data Science), Erlangen-Nürnberg (Medizintechnik) sowie an der geplanten TU Nürnberg.

Weil alle Hochschulen von der Hightech-Agenda profitieren, äußert niemand offen Kritik. Allein die Studenten sprechen aus, was auch viele Uni-Chefs beunruhigt: Fast sechs Milliarden Euro müssten in teils marode Gebäude und Neubauten investiert werden, 600 Millionen Euro sind in der Agenda eingeplant. ""Die bereitgestellten Mittel reichen bei Weitem nicht aus"", sagt Studentensprecher Simon Lund und fordert langfristig mehr Geld, um den Sanierungsstau abzubauen.";https://www.sueddeutsche.de/bayern/wissenschaft-mehr-zeit-fuer-die-forschung-1.4790303;sz.de;Anna Günther
09.01.2020;Wie die Hochschule Platz für mehr Studierende schaffen will;"Die Grenzebach Group zum Beispiel ist so ein Unternehmen, das zwischen Augsburg und Nördlingen in Schwaben beheimatet und international tätig ist. Anlagenbau und Automatisierungstechnik sind ihre Spezialgebiete, und wenn irgendwo auf der Welt Solaranlagen gebaut werden, ist die Chance hoch, dass das Unternehmen daran beteiligt ist. Es gibt viele solcher Firmen im Umkreis Augsburgs, die in ihren Sparten weltweit führend sind, sie alle aber haben ein Problem: den Fachkräftemangel. In einer bayernweit einzigartigen Initiative haben sich deshalb Unternehmen und Wirtschaftsverbände aus der Region mit der Hochschule Augsburg zusammengetan und ein Konzept entwickelt: Sechs neue Studiengänge sollen die Kapazität der Hochschule um 2000 Studierende erweitern - und so helfen, die Probleme in der Region zu lösen.

Das Konzept heißt ""gp 2025"", gp steht für ""gefragte Persönlichkeiten"", und das sind gut ausgebildete Studenten heutzutage: Die IHK Schwaben stuft den Fachkräftemangel auf Grundlage einer Umfrage als größtes Risiko für die Wirtschaft ein. ""Mehr als jeder zweite Firmenchef gab zur Antwort, dass der Mangel an qualifizierten Mitarbeitern ein Risiko für die wirtschaftliche Entwicklung darstellt"", sagt Matthias Köppel, der bei der IHK Schwaben für die Standortpolitik zuständig ist. Die Hochschule in Augsburg kann die Nachfrage aber nicht mehr bedienen und muss jährlich 500 Studierende abweisen. Es gibt kein Studienfach, das keine Zugangsbeschränkung in Form eines Numerus clausus hätte. ""Es ist momentan einfacher, an der Exzellenzuniversität in München einen Platz zu bekommen als bei uns"", sagt Präsident Gordon Rohrmair. Mit dem Konzept ""gp 2025"" sollen also nicht nur mehr Studierende ausgebildet werden - es ist auch ein Hilferuf aus der Region Augsburg. Rohrmair will mit dem Konzept Geld aus der Forschungsoffensive von Ministerpräsident Markus Söder nach Augsburg lenken. Zwei Milliarden Euro hat Söder den Universitäten und Hochschulen im Freistaat versprochen. Die Staatskanzlei, das Wirtschaftsministerium und das Wissenschaftsministerium haben das Konzept zur Kapazitätserweiterung im Herbst erhalten, mit Briefen von mehr als 100 Firmen. Inzwischen unterstützen die Initiative 150 Unternehmen aus der Region Augsburg mit einem Einzugsgebiet von knapp einer Million Menschen.

Und Rohrmair hat gute Argumente, warum ein Teil des Geldes gerade in seiner Hochschule bestens angelegt wäre: Da wäre einmal das Konzept selbst, das exakt auf den Bedarf der Wirtschaft zugeschnitten ist. ""Wir haben geschaut, wo der reale Bedarf ist"", sagt Rohrmair. Herausgekommen sind sechs Studiengänge in zukunftsfesten Sparten wie Big Data, Künstliche Intelligenz und Softwareentwicklung, zum Beispiel: Digitales Planen und Bauen, Data Science, aber auch Wirtschaftspsychologie für Digitale Märkte. So werden Spezialisten ausgebildet, die digitale Produkte überhaupt erst nutzerfreundlich machen.

Dringend benötigte künftige Spezialisten aber wandern laut Rohrmair momentan notgedrungen ab, weil sie keinen Studienplatz bekommen. ""Unsere Start-ups hier könnten alle mehr Stellen besetzen. 100 000 Euro Umsatz pro fehlendem Mitarbeiter werden der Region so entzogen."" Dabei sind die Studienplätze an der Hochschule Augsburg um den Faktor vier überbucht. Betrachtet man die Statistiken bei den Bachelor-Studiengängen, so gibt es in Regensburg etwa 1000 und in Würzburg sogar knapp 2000 Studierende mehr als in Augsburg - obwohl die Wirtschaftsräume signifikant kleiner sind. Und obwohl die Region Augsburg vergleichsweise wenige hoch qualifizierte Beschäftigte mit entsprechend hohem Einkommen hat. Eine Statistik aus dem Jahr 2017 beziffert den Anteil hoch qualifizierter Beschäftigter in Augsburg auf 12,3 Prozent, in Ingolstadt auf 16,6 und in München auf 28,3 Prozent. ""Wir müssen hier mehr qualifiziert ausbilden, um das zu ändern"", folgert Rohrmair. Der Hochschulpräsident sieht aber auch eine soziale Verantwortung. Gerade weil es in Augsburg vergleichsweise viele Arbeiter gibt, sagt er: ""Wir werden es in den nächsten 20 Jahren mit vielen Bildungsaufsteigern zu tun bekommen."" Also mit Studierenden, deren Familien nicht so viel Geld haben, um ihren Kindern ein Studium in einer anderen Stadt zu ermöglichen. ""Gerade wir als Hochschule müssen unserer Funktion als sozialer Lift gerecht werden"", sagt Rohrmair. Das aber schafft die Hochschule Augsburg aus Kapazitätsgründen nicht mehr, wenn sie jährlich 500 Studierende abweisen muss.

13,5 Millionen Euro im Jahr bräuchte die Hochschule, um ihre Pläne zu verwirklichen, dazu käme eine Anschubfinanzierung von vier Millionen Euro für Infrastruktur und Labore. 130 Stellen würden entstehen. Das Wissenschaftsministerium lässt ausrichten, dass es das Konzept ausdrücklich unterstütze und eine Förderung im Zuge der ""Hightech Agenda"" denkbar sei. Ein genauer Zeitpunkt, wann das Geld aus der Forschungsoffensive verteilt wird, stehe aktuell jedoch noch nicht fest.";https://www.sueddeutsche.de/bayern/hochschule-augsburg-platzmangel-neue-studiengaenge-1.4749974;sz.de;Florian Fuchs
02.01.2020;Mensch oder Maschine?;"Sie erkennen Gesichter, schreiben Artikel, steuern Autos und gewinnen Schachspiele. Maschinen mit künstlicher Intelligenz sind längst nicht mehr nur Fantasien aus Science-Fiction-Filmen. Die Frage ist: Wie verändern sie unser Berufsleben? Nach einer Studie stehen die Deutschen heute Robotern weniger positiv gegenüber als noch vor einigen Jahren. Das gilt vor allem für Maschinen am Arbeitsplatz, wie Psychologen der Universitäten Würzburg und Linz Anfang 2019 berichteten. Aber es ist nicht nur die Digitalisierung, die den Arbeitsmarkt vor Herausforderungen stellt: Drei Berufe mit guten Aussichten und drei, die wahrscheinlich eher früher als später aussterben werden.
Handwerker/Handwerkerin

Viele traditionelle Handwerksberufe sind vom Aussterben bedroht: Buchbinder, Seiler, Schuhmacher, Instrumentenbauer. Für andere Handwerker gilt das nicht, die sind so gut im Geschäft wie vielleicht noch nie. Im Durchschnitt müssen Kunden 4,7 Wochen auf einen Profi warten - wenn es sich nicht um einen Notfall handelt. Das ist das Ergebnis einer Umfrage der Bayerischen Handwerkskammern. Weil heute fast jeder studieren möchte, gibt es extrem viele Akademiker auf dem Arbeitsmarkt, in Ausbildungsberufen wird es dagegen knapp. Dem deutschen Handwerk geht es weiterhin gut. Die Auslastung der Betriebe liegt bei 84 Prozent - laut Zentralverband des Deutschen Handwerks (ZDH) ein Rekordhoch. Für 2020 rechnet der ZDH mit einer Umsatzsteigerung von etwa drei Prozent - ein Wert, der sich im Vergleich zu anderen Branchen sehen lassen kann. Data Scientist

Vom Packsystem im Logistikunternehmen über die Fertigung in der Autofabrik bis zur Pflege der Webseite: Kaum etwas ist in der modernen Arbeitswelt nicht computergesteuert. Und wo ein Computer ist, gibt es auch Daten, und zwar eine ganze Menge. Da braucht es Experten, die den Überblick behalten. An dieser Stelle kommt der Data Scientist ins Spiel, ein relativ neuer Beruf an der Schnittstelle zwischen Informatik und Betriebswirtschaft. Ein Logistiker kann zum Beispiel seine Produktionsabläufe optimieren, wenn ein Data Scientist herausfindet, wo es noch hakt. Mögliche Studiengänge sind Mathematik, Statistik oder Informatik, inzwischen gibt es auch Data-Science-Kurse an vielen Universitäten. Schon 2012 bezeichnete das Magazin Harvard Business Review den Beruf als attraktivsten Job des 21. Jahrhunderts.
Erzieher/Erzieherin

Würde man sein Kind einem Roboter anvertrauen? Vermutlich eher nicht. Anders als in der Pflege, wo vereinzelt schon jetzt Roboter zum Einsatz kommen, ist das in Kitas schwer vorstellbar. Erzieherinnen und Erzieher müssen erst mal keine Angst haben, von Maschinen ersetzt zu werden. Nach einer Studie der Bertelsmann-Stiftung fehlen in deutschen Krippen und Kindergärten insgesamt mehr als 100 000 Fachkräfte. Der Bedarf ist groß. Das Problem: Erzieher und Kinderpfleger verdienen im Vergleich zu anderen Berufen relativ wenig. Dazu kommt, dass viele Nachwuchskräfte für ihre Ausbildung immer noch bezahlen müssen. Andere Auszubildende bekommen dagegen Geld in berufsbegleitenden Modellen - auch um Quereinsteiger für den Beruf zu gewinnen. Aber es hat sich laut Bertelsmann-Stiftung im Zuge des Kita-Ausbaus auch einiges getan: Von 2008 bis 2018 ist die Zahl des pädagogischen Personals von etwa 380 000 auf rund 582 000 gestiegen.
Bankkaufmann/Bankkauffrau

Bis zum Jahr 2030 wird die Hypo-Vereinsbank in Deutschland etwa 1300 Arbeitsplätze streichen. Es sind Meldungen wie diese vom Dezember, die nicht gerade Hoffnung verbreiten. Früher begleitete der Bankkaufmann oder die Bankkauffrau Menschen vom ersten Konto über die Baufinanzierung bis hin zur Altersvorsorge. Heute machen immer mehr Filialen dicht, den klassischen Kundenberater gibt es immer seltener. Erst kürzlich hat die Unternehmensberatung Oliver Wyman die Filiallandschaft deutscher Banken und Sparkassen untersucht. Das Ergebnis: Bis 2030 werden 13 000 Standorte schließen. Allein zwischen 2008 und 2018 haben Finanzinstitute hierzulande etwa 12 000 Filialen aufgegeben. Auch wenn es nur einer der Gründe ist: Den Bankensektor kostet die Digitalisierung schon jetzt Arbeitsplätze.
Schäfer/Schäferin

Früher lebten Schäfer vor allem vom Verkauf von Wolle und Fleisch. Heute haben sie mit dem Preisverfall zu kämpfen. Ohne Fördergelder der Europäischen Union können viele nicht mehr überleben. Während 1990 noch mehr als drei Millionen Schafe in Deutschland gezählt wurden, waren es 2018 nur noch etwa 1,6 Millionen. Weil sie nicht mehr von ihrem Beruf leben können, schlagen immer mehr Wanderschäfer Alarm. Aber es geht nicht nur um sie: Schafe leisten auch einen wichtigen Beitrag zum Umweltschutz. Die weidenden Tiere tragen etwa zum Erhalt der heidetypischen Landschaften bei und sorgen dafür, dass bedrohte Tiere überleben. Schon im Jahr 2016 ist die Zahl der Berufsschäfer in Deutschland erstmals auf unter 1000 gesunken.
Kassierer/Kassiererin

In vielen Super- oder Baumärkten können Kunden bereits selbst kassieren. Nach einer Umfrage nutzen mittlerweile mehr als 30 Millionen Deutsche Selbstbedienungskassen, kurz SB-Kassen. Und es gibt immer mehr Supermärkte, die solche Scanner einsetzen. Braucht es irgendwann überhaupt noch Menschen, die an der Kasse arbeiten? Damit hat sich auch das Institut für Arbeitsmarkt- und Berufsforschung in Nürnberg auseinandergesetzt und für etwa 4000 Jobs ermittelt, ob die jeweiligen Tätigkeiten automatisierbar sind oder nicht. ""Der Arbeitsalltag dieses Berufs besteht im Wesentlichen aus zwei verschiedenen Tätigkeiten"", heißt es auf der Homepage. Gemeint ist das Scannen und das Kassieren. ""100 Prozent könnten schon heute Roboter übernehmen"", lautet das nüchterne Urteil.";https://www.sueddeutsche.de/karriere/berufe-job-digitalisierung-zukunft-1.4723626;sz.de;Julian Erbersdobler
31.12.2019;Was kommt, was bleibt;"Sie erkennen Gesichter, schreiben Artikel, steuern Autos und gewinnen Schachspiele. Maschinen mit künstlicher Intelligenz sind längst nicht mehr nur Fantasien aus Science-Fiction-Filmen. Die Frage ist: Wie verändern sie unser Berufsleben? Nach einer Studie stehen die Deutschen heute Robotern weniger positiv gegenüber als noch vor einigen Jahren. Das gilt vor allem für Maschinen am Arbeitsplatz, wie Psychologen der Universitäten Würzburg und Linz Anfang 2019 berichteten. Aber es ist nicht nur die Digitalisierung, die den Arbeitsmarkt vor Herausforderungen stellt: Drei Berufe mit guten Aussichten und drei, die wahrscheinlich eher früher als später aussterben werden.
HANDWERKER/HANDWERKERIN

Viele traditionelle Handwerksberufe sind vom Aussterben bedroht: Buchbinder, Seiler, Schuhmacher, Instrumentenbauer. Für andere Handwerker gilt das nicht, die sind so gut im Geschäft wie vielleicht noch nie. Im Durchschnitt müssen Kunden 4,7 Wochen auf einen Profi warten - wenn es sich nicht um einen Notfall handelt. Das ist das Ergebnis einer Umfrage der Bayerischen Handwerkskammern. Weil heute fast jeder studieren möchte, gibt es extrem viele Akademiker auf dem Arbeitsmarkt, in Ausbildungsberufen wird es dagegen knapp. Dem deutschen Handwerk geht es weiterhin gut. Die Auslastung der Betriebe liegt bei 84 Prozent - laut Zentralverband des Deutschen Handwerks (ZDH) ein Rekordhoch. Für 2020 rechnet der ZDH mit einer Umsatzsteigerung von etwa drei Prozent - ein Wert, der sich im Vergleich zu anderen Branchen sehen lassen kann.
DATA SCIENTIST

Vom Packsystem im Logistikunternehmen über die Fertigung in der Autofabrik bis zur Pflege der Webseite: Kaum etwas ist in der modernen Arbeitswelt nicht computergesteuert. Und wo ein Computer ist, gibt es auch Daten, und zwar eine ganze Menge. Da braucht es Experten, die den Überblick behalten. An dieser Stelle kommt der Data Scientist ins Spiel, ein relativ neuer Beruf an der Schnittstelle zwischen Informatik und Betriebswirtschaft. Ein Logistiker kann zum Beispiel seine Produktionsabläufe optimieren, wenn ein Data Scientist herausfindet, wo es noch hakt. Mögliche Studiengänge sind Mathematik, Statistik oder Informatik, inzwischen gibt es auch Data-Science-Kurse an vielen Universitäten. Schon 2012 bezeichnete das Magazin Harvard Business Review den Beruf als attraktivsten Job des 21. Jahrhunderts.
ERZIEHER/ERZIEHERIN

Würde man sein Kind einem Roboter anvertrauen? Vermutlich eher nicht. Anders als in der Pflege, wo vereinzelt schon jetzt Roboter zum Einsatz kommen, ist das in Kitas schwer vorstellbar. Erzieherinnen und Erzieher müssen erst mal keine Angst haben, von Maschinen ersetzt zu werden. Nach einer Studie der Bertelsmann-Stiftung fehlen in deutschen Krippen und Kindergärten insgesamt mehr als 100 000 Fachkräfte. Der Bedarf ist groß. Das Problem: Erzieher und Kinderpfleger verdienen im Vergleich zu anderen Berufen relativ wenig. Dazu kommt, dass viele Nachwuchskräfte für ihre Ausbildung immer noch bezahlen müssen. Andere Auszubildende bekommen dagegen Geld in berufsbegleitenden Modellen - auch um Quereinsteiger für den Beruf zu gewinnen. Aber es hat sich laut Bertelsmann-Stiftung im Zuge des Kita-Ausbaus auch einiges getan: Von 2008 bis 2018 ist die Zahl des pädagogischen Personals von etwa 380 000 auf rund 582 000 gestiegen.
BANKKAUFMANN/BANKKAUFFRAU

Bis zum Jahr 2030 wird die Hypo-Vereinsbank in Deutschland etwa 1300 Arbeitsplätze streichen. Es sind Meldungen wie diese vom Dezember, die nicht gerade Hoffnung verbreiten. Früher begleitete der Bankkaufmann oder die Bankkauffrau Menschen vom ersten Konto über die Baufinanzierung bis hin zur Altersvorsorge. Heute machen immer mehr Filialen dicht, den klassischen Kundenberater gibt es immer seltener. Erst kürzlich hat die Unternehmensberatung Oliver Wyman die Filiallandschaft deutscher Banken und Sparkassen untersucht. Das Ergebnis: Bis 2030 werden 13 000 Standorte schließen. Allein zwischen 2008 und 2018 haben Finanzinstitute hierzulande etwa 12 000 Filialen aufgegeben. Auch wenn es nur einer der Gründe ist: Den Bankensektor kostet die Digitalisierung schon jetzt Arbeitsplätze.
SCHÄFER/SCHÄFERIN

Früher lebten Schäfer vor allem vom Verkauf von Wolle und Fleisch. Heute haben sie mit dem Preisverfall zu kämpfen. Ohne Fördergelder der Europäischen Union können viele nicht mehr überleben. Während 1990 noch mehr als drei Millionen Schafe in Deutschland gezählt wurden, waren es 2018 nur noch etwa 1,6 Millionen. Weil sie nicht mehr von ihrem Beruf leben können, schlagen immer mehr Wanderschäfer Alarm. Aber es geht nicht nur um sie: Schafe leisten auch einen wichtigen Beitrag zum Umweltschutz. Die weidenden Tiere tragen etwa zum Erhalt der heidetypischen Landschaften bei und sorgen dafür, dass bedrohte Tiere überleben. Schon im Jahr 2016 ist die Zahl der Berufsschäfer in Deutschland erstmals auf unter 1000 gesunken. KASSIERER/KASSIERERIN

In vielen Super- oder Baumärkten können Kunden bereits selbst kassieren. Nach einer Umfrage nutzen mittlerweile mehr als 30 Millionen Deutsche Selbstbedienungskassen, kurz SB-Kassen. Und es gibt immer mehr Supermärkte, die solche Scanner einsetzen. Braucht es irgendwann überhaupt noch Menschen, die an der Kasse arbeiten? Damit hat sich auch das Institut für Arbeitsmarkt- und Berufsforschung in Nürnberg auseinandergesetzt und für etwa 4000 Jobs ermittelt, ob die jeweiligen Tätigkeiten automatisierbar sind oder nicht. ""Der Arbeitsalltag dieses Berufs besteht im Wesentlichen aus zwei verschiedenen Tätigkeiten"", heißt es auf der Homepage. Gemeint ist das Scannen und das Kassieren. ""100 Prozent könnten schon heute Roboter übernehmen"", lautet das nüchterne Urteil.";https://www.sueddeutsche.de/karriere/berufe-was-kommt-was-bleibt-1.4730366;sz.de;Julian Erbersdobler
29.12.2019;Das pulsierende Herz;"So viel steht fest: Wenn Galileo einst fertig ist, dann kann es in Garching mit ein paar Superlativen aufwarten. Das etwa 50 Meter breite und 200 Meter lange Gebäude soll einmal das Herzstück des Garchinger Wissenschaftscampus werden und endlich Leben bringen in die Lern- und Forschungslandschaft. 36 000 Quadratmeter stehen dafür allein oberirdisch zur Verfügung, für Büros, Seminarräume, ein Vier-Sterne-Hotel, Gäste-Apartments und Restaurants und Geschäften im Untergeschoss. 175 000 Tonnen wiegt das Gebäude, so viel wie 2,2 Millionen Menschen, 50 Kilometer lang wäre die Schlange der Betonmischer, wenn man es an einem Stück betoniert hätte, so steht es auf der Homepage des Projekts der Soini Asset Immobilien-Gruppe.

Aber noch schlägt ""das pulsierende Herz"" nur für einen kleinen Teil des Gebäudes. Bisher arbeiten erst wenige Firmen im Gebäude und ein Fitness-Center hat schon seit längerem offen und trotzt der Baustelle rundherum. Daran hat auch ein ""Soft-Opening"" im September nichts geändert, die Handwerker sind immer noch zugange. Der frühe Termin mit Feier im Audimax, der Platz für 1300 Studenten bietet, war wohl eher dem Amtswechsel des Präsidenten der Technischen Universität geschuldet. Das Fest fand im September an den letzen Arbeitstagen von Wolfgang Herrmann statt, der das Projekt gegen anfängliche Widerstände als Public Private Partnership auf den Weg gebracht hatte. Er wollte sicherstellen, dass es am Campus Einkaufsmöglichkeiten und ein größeres kulinarisches Angebot geben wird und sich für die TU zusätzliche Räume öffnen. Der Freistaat gab das Grundstück auf Erbpachtbasis her, private Investoren der ""Neue Mitte am Hochschulcampus Garching GmbH & Co. KG"" übernahmen den Bau. 2014 war der Baubeginn, Fertigstellung sollte zwei Jahre später sein. Zuletzt war geplant, dass die Studenten bereits vom Wintersemester an das Audimax nutzen können, aber auch dieser Termin platzte, wie schon einige zuvor. Jetzt rechnet die TU damit, dass sie die von ihnen angemieteten Räume voraussichtlich am 31. Januar übergeben bekommt. Wie TU-Sprecher Ulrich Marsch sagt, könnten sie dann sukzessive im ersten oder zweiten Quartal nächsten Jahres bezogen werden. Dringend gebraucht wird nicht nur das Audimax, sondern auch Räume für Einrichtungen, die im Zuge der Exzellenzstrategie neu geschaffen oder fortgeführt werden, etwa für das ""Institute for Life Long Learning"", das ""Munich Center for Technology in Society"" oder das ""Data Science Institute"".

Daneben sei geplant, Projektflächen für die Fakultät Maschinenwesen und Büroflächen für die Verwaltung zu belegen. Ebenfalls alles bereit für den baldigen Start ist bei der Bierwirth & Kluth Hotelmanagement GmbH. B&K wird drei Einrichtungen in Galileo betreiben. Die Firma ist zuständig für das Science Congress Center Munich und damit für das Audimax, das sich auch für Kongresse und kulturelle Veranstaltungen eignet. Dann sind da noch das Courtyard Marriot Hotel und das Stellaris mit 159 Apartments. Hotel und Congress Center sollen nach Auskunft der Betreiber am 1. April 2020 eröffnen, das Stellaris im zweiten Quartal. Wann die Restaurants und der Edeka-Laden endlich eröffnen werden und wann das Gebäude als Ganzes fertig ist, dazu war vom Projektbetreiber, der Soini Asset Immobilien-Gruppe, vor Weihnachten allerdings keine Auskunft zu erhalten. Trotzdem: Ein Ende der Baustelle zeichnet sich ab.";https://www.sueddeutsche.de/muenchen/landkreismuenchen/garching-das-pulsierende-herz-1.4739313;sz.de;Gudrun Passarge
26.12.2019;Es gibt sie noch, die guten Nachrichten;"Jetzt, wo wir kurz davor stehen, in ein neues Jahrzehnt aufzubrechen, ist ein guter Zeitpunkt, Bilanz zu ziehen. Und zwar nicht mit den größten Katastrophen, sondern mit den besten Entwicklungen.

Vier von zehn Lesern (39 Prozent) sagen laut der jüngsten Reuters-Studie, dass die Medien zu negativ sind. Das Reuters Institute hat dazu immerhin 75.000 Menschen in 38 Ländern befragt. Mehr als die Hälfte (58 Prozent) klagen, die Medien vermiesen ihnen regelmäßig die Stimmung. 

Gut, manchmal lässt sich das nicht vermeiden. Über den Brexit-Wahnsinn, die Trump-Tweets und das Boeing-Desaster müssen wir natürlich berichten. Tatsächlich aber gibt es auch eine ganze Reihe guter Nachrichten, die in der täglichen Berichterstattung untergehen oder über die wir vielleicht nur auf Seite 7 berichten, obwohl sie eigentlich so wichtig sind, dass sie auf Seite 1 gehören. Unter anderem dank des (inzwischen leider verstorbenen) schwedischen Wissenschaftlers Hans Rosling wissen wir: Immer mehr Mädchen gehen zur Schule, immer mehr Kinder werden geimpft, immer weniger Menschen leben in Armut, die Zahl der Todesfälle pro Jahr durch Naturkatastrophen hat sich in den letzten 100 Jahren halbiert. Schritt für Schritt, Jahr für Jahr wird die Welt besser. Nicht überall und nicht für jeden Einzelnen, aber die Richtung stimmt. Hier sind meine sechs besten Nachrichten des Jahrzehnts:
1. Arbeitslosigkeit in Deutschland auf niedrigstem Stand seit zehn Jahren

Allen Konjunkturpessimisten und Handelskriegen zum Trotz: Die Zahl der Arbeitslosen sinkt in Deutschland jedes Jahr. Im November sank sie auf 2,18 Millionen – den niedrigsten Stand seit der Wiedervereinigung. Besonders schön: Auch die Zahl der Langzeitarbeitslosen geht weiter zurück. Im November waren es erstmals weniger als 700.000 Menschen, die seit mehr als einem Jahr ohne Arbeit sind.
2. Das Ozonloch über der Antarktis schloss sich schneller als erwartet

Gut möglich, dass das auslaufende Jahrzehnt als die Dekade in die Geschichte eingehen wird, in der wir rekordverdächtig viel CO2 raushauten und den Klimawandel verschliefen. Aber es gibt ein Beispiel, das wunderbar illustriert, wie es möglich ist, globale Umweltkatastrophen zu verhindern, wenn alle zusammenhalten: Das Ozonloch über dem Südpol, das sich jedes Jahr wieder auftut, hat sich 2019 so früh geschlossen wie seit über 30 Jahren nicht mehr. Ozonloch? War da was? Genau, in den Siebziger- und Achtzigerjahren wuchs das Bewusstsein, dass wir Menschen Ozonkiller raushauen – vor allem Fluorchlorkohlenwasserstoffe (FCKW) in Kühlschränken und Sprühdosen –, die den Abbau von Ozon beschleunigen und damit den natürlichen Schutz der Erde vor schädlicher Sonnenstrahlung beeinträchtigen. Eine weltweite Allianz, diese Stoffe zu verbieten, war 1987 erfolgreich. Wenn 198 Staaten 1987 in Montreal den Verzicht auf Ozonkiller nicht beschlossen hätten, wäre die Erde bis Mitte dieses Jahrhunderts noch ein zusätzliches Grad wärmer, denn FCKW und ihre Artverwandten sind auch hochwirksame Treibhausgase, die das Klima noch stärker aufheizen als Kohlendioxid.

Es geht also doch.

Auch wenn die schnellere Erholung der Ozonschicht in diesem Jahr auch auf wärmeres Wetter über der südlichen Erdhälfte zurückgeht, ist es doch ein positives Omen. Die Weltorganisation für Meteorologie geht davon aus, dass sich die Ozonschicht vollständig erholen wird. Zumindest dieses Umweltproblem ist fast gelöst.

Und überhaupt: Auch wenn die Regierungen immer noch zu langsam reagieren und gerade wieder eine Klimakonferenz verschlammasselten, die weltweite Aufmerksamkeit für Greta Thunberg, die millionenfachen Schulstreiks und die gesamte Fridays for Future Bewegung zeigen deutlich, dass das Thema Klimakrise endlich so stark ins Bewusstsein gedrungen ist, dass die Regierungen und Industrievertreter im nächsten Jahrzehnt mit Entschuldigungen nicht mehr durchkommen.
3. Mehr Menschen als jemals zuvor gehen in Demokratien wählen

Es macht mir Angst, dass einige der größten und mächtigsten Demokratien der Welt gerade ausgesprochen undemokratisch agieren, allen voran die USA, Indien und Brasilien.

Was aber auch stimmt: Weltweit dürfen mehr Menschen als jemals zuvor in Demokratien wählen gehen. Vor vierzig Jahren wurden fast zwei Drittel der Länder weltweit von Autokraten regiert und nur ein Viertel von demokratisch gewählten Regierungen, inzwischen sind laut des Center for Systemic Peace mehr als die Hälfte Demokratien. 
4. Die Ehe für alle hat in Skandinavien die Suizidrate halbiert

So deprimierend die Welt auch scheint, weltweit sind die Suizidraten in den letzten 25 Jahren um 38 Prozent gesunken. Das sind vier Millionen Leben. Bisher machten Psychologen international besonders den wirtschaftlichen Aufstieg und die Emanzipation von Frauen in Asien dafür verantwortlich. Nun kommt eine weitere Ursache für den Rückgang dazu, denn statistisch gesehen nehmen sich mehr Homosexuelle als Heterosexuelle das Leben. Die Suizidrate sinkt aber erheblich in Ländern, in denen die Schwulenehe der Heteroehe gleichgestellt ist, allen voran in Skandinavien: Dänemark war das erste Land der Welt, das schon 1989 gleichgeschlechtliche Lebensgemeinschaften anerkannte; Schweden zog 1995 nach. Schweden führte die Ehe für alle 2009 ein, Dänemark 2012. In beiden Ländern sank die Suizidrate unter Homosexuellen laut des Dänischen Instituts zur Suizidprävention in den letzten Jahren um 46 Prozent! Zum Vergleich: Bei heterosexuellen Paaren sank die Rate um 28 Prozent.

Die Forscher werteten dazu Daten von mehr als 28.000 Menschen über den Zeitraum von 1989 bis 2016 aus. Forscher vermuten als Hauptgründe die geringere Stigmatisierung und allgemein gestiegene Akzeptanz.
5. Indien hat seine Armutsrate halbiert

Die Weltbank meldet, dass Indien seine Armutsrate seit 1990 mehr als halbiert hat. Mussten 2011 noch insgesamt 268 Millionen Menschen mit weniger als 1,90 Dollar pro Tag zurechtkommen, soll die Zahl bis Ende dieses Jahres auf 40 Millionen Menschen schrumpfen.

Die ganze Welt ist bei der Armutsbekämpfung tendenziell auf dem richtigen Weg. Laut der Weltbank und des World Data Lab sank die Rate extremer Armut in den letzten zwanzig Jahren von 50 auf 30 Prozent, Kindersterblichkeit von 14 auf 7 Prozent, und der Zugang zu Wasser und Toiletten verdoppelte sich fast von 22 Prozent auf 41 Prozent. Das heisst: Zum ersten Mal seit Beginn der Neuzeit lebt die Mehrheit der Menschen nicht mehr in Armut. Mehr als 50 Prozent der Menschen gehören jetzt zur Mittel- oder Oberklasse. Das ist auch deshalb eine gute Nachricht, weil Menschen deutlich glücklicher sind, wenn sie sich aus der Armut befreien konnten.
6. Der Buckelwal-Bestand erholt sich

Ausgesprochen tragisch finde ich die regelmäßig zu sehenden Bilder von gestrandeten und elendig verreckenden Walen, aus deren Mägen dann Tonnen von Plastiktüten und Gartenschläuchen gezogen werden.

Tatsache ist aber auch: Zumindest der Bestand der Buckelwale erholt sich. Die Forscher der University of Washington verkündeten im Fachjournal Royal Society Open Science, dass es fast wieder so viele Buckelwale im westlichen Südatlantik gibt wie vor Beginn der Waljagden, die die Tiere beinahe ausgerottet hätten. 

Im 18. Jahrhundert war die Zahl der Wale von 27.000 auf nur 450 Tiere gesunken, und in manchen Regionen bleiben sie gefährdet. Die Forscher schätzen, dass weltweit mindestens 300.000 Buckelwale geschlachtet wurden. Aber die Schutzmaßnahmen und Jagdverbote ab Mitte der Sechzigerjahre waren erfolgreich: Inzwischen zählen Wissenschaftler wieder 25.000 Wale im Südwestatlantik – genug, dass sich der Walbestand in der Region wohl dauerhaft erholen kann.

Die Ankündigung ist wichtig, weil sie Hoffnung gibt: Artenschutz funktioniert. Während wir Menschen einerseits kräftig Arten ausrotten und die Biodiversität auch in unseren Breitengraden dramatisch sinkt, ist aber auch bekannt, dass wir bis heute mehr als 90 Prozent aller Arten noch gar nicht kennen, jedes Jahr kommen Tausende neue hinzu.

Die Wissenschaftler der California Academy of Sciences freuen sich darüber, in diesem Jahr 71 neue Arten entdeckt zu haben. Zu den Lieblingsentdeckungen zählen der lilafarbene afrikanische Wakandafisch, der direkt einem Marvel Comic entsprungen scheint, die leopardengemusterte Meeresschnecke Janolus flavoanulatus und neue Korallen, die Meeresexpeditionen in der Tiefsee aufdeckten wie die zitronengelbe Chromoplexura cordellbankensis. Die sieht aus, als würde eine Zitrone in die Länge wachsen und anfangen zu tanzen. 

Wenn das kein Grund zur Freude ist! Wakanda Forever! Ein schönes neues Jahrzehnt wünscht Ihnen Ihre Michaela Haas.";https://sz-magazin.sueddeutsche.de/die-loesung-fuer-alles/gute-nachrichten-2019-88192;sz.de;Michaela Haas
05.12.2019;Bits und Bots;"Kreative Pioniere wollen die Vereinigten Arabischen Emirate in ihrer neu gegründeten Universität für künstliche Intelligenz (KI) ausbilden: Nach Angaben des Golfstaates ist das die erste Uni weltweit, die sich ausschließlich mit diesem Thema beschäftigen wird. KI verändere die Welt bereits, aber es lasse sich noch viel mehr erreichen, wenn der grenzenlosen Vorstellungskraft des menschlichen Verstandes erlaubt werde, diese vollständig zu erforschen, ließ die Mohamed bin Zayed University of Artificial Intelligence verlauten. Auf ihrem Campus in Masdar City sollen ab September 2020 die ersten Seminare für Studenten stattfinden.

Von einer KI-Universität ist man in Deutschland noch weit entfernt, doch auch hierzulande bieten immer mehr Hochschulen Studiengänge an, die sich mit dem Thema KI und angrenzenden Themenfeldern beschäftigen - egal ob das Robotik, Maschinelles Lernen, Computer Vision oder Datenwissenschaften sind.

An der Technischen Hochschule (TH) Ingolstadt konnten sich erstmals in diesem Wintersemester Studierende für den neuen Studiengang ""Künstliche Intelligenz"" einschreiben. ""Zuvor gab es bundesweit noch keinen reinen grundständigen Bachelor-Studiengang zum Thema KI"", sagt Melanie Kaiser, Professorin für Datenanalyse und Leiterin des Studiengangs KI. Der akademische Bedarf in der Ausbildung komme zu kurz, wenn KI nur nebenher als Vertiefung in Studiengängen - etwa der Mathematik oder der Informatik, angeboten werde. Circa 70 Erstsemester nahmen Anfang Oktober das siebensemestrige Studium auf, mehr als 200 Bewerbungen waren eingegangen. ""Unser Ziel ist, den Studierenden ein Gesamtpaket für die Entwicklung und den Einsatz von KI-Anwendungen zu bieten"", sagt Kaiser. Dazu zählten fundierte Grundlagen etwa zu Verfahren in den Themenfeldern maschinelles Lernen, neuronale Netze oder Sprach- und Bilderkennung sowie deren Umsetzung mittels geeigneter Programmiersprachen. Inhaltliche Vertiefungsfächer seien etwa Robotik, autonomes Fahren oder KI-Anwendungen in der Logistik und der medizinischen Bilderkennung. Zudem stehe den Studierenden der Weg in die Forschung offen - so können sie schon während des Studiums Studien- oder Abschlussarbeiten gemeinsam mit dem THI-Forschungszentrum für KI machen. ""Der Bedarf an Absolventen ist groß"", sagt Kaiser. Ob in der Industrie, im Online-Handel oder in der Medizin: Experten für die intelligente Steuerung von Prozessen, die Programmierung von Chatbots oder für die Sprach- und Bilderkennung seien überall gesucht.
Elektrotechnik, Maschinenbau, Informatik: Einige Programme verbinden unterschiedliche Fächer

Für einen anderen Ansatz hat sich die Beuth Hochschule für Technik in Berlin entschieden: Sie bildet seit dem vorigen Jahr Studierende darin aus, humanoide Roboter zu konstruieren, zu programmieren und Verhaltensweisen zu entwickeln. ""Die Studenten löten, fräsen und können am Ende einen Roboter bauen"", sagt Manfred Hild, Professor für digitale Systeme. In den Bachelor-Studiengang ""Humanoide Robotik"" fließen Elemente der Elektrotechnik, der Programmierung, der Mechatronik und der Mensch-Maschine-Interaktion ein. Dieses spezifische Angebot kommt gut an: Fast 400 Bewerbungen gingen zum Wintersemester 2018/19 für circa 60 Studienplätze ein, mehr als 330 waren es für das Wintersemester 2019/2020.

""Junge Menschen wollen Themen, die man sonst oft erst im Master nach einem Bachelor in Mathematik oder Informatik in Angriff nimmt, wie Robotik, so schnell wie möglich durchdringen"", sagt Studienfachberater Hild. Zudem startet man gleich am Anfang des ersten Semesters mit vielen Praxiseinheiten: bohren, löten, programmieren, elektrische Schaltungen aufbauen. Der praktische Ansatz der Beuth Hochschule scheint auch jungen Frauen zu gefallen. Deren Bewerberanteil liegt bei 40 Prozent - ungewöhnlich hoch für ein Mint-Fach. Hochschullehrer Hild begründet das damit, dass der Studiengang nicht rein ingenieurwissenschaftlich ausgerichtet sei, sondern auch Aspekte der Gesellschaft, der Ethik, des Rechts oder der Kunst im Studium behandle.

Komplett englischsprachig ist im Unterschied zu den beiden Bachelor-Studiengängen der Master-Studiengang ""Machine Learning"", den die Universität Tübingen neu in diesem Wintersemester aufgelegt hat. ""Die gesamte Forschungslandschaft agiert auf Englisch, ein deutschsprachiger Studiengang wäre absurd gewesen"", nennt Matthias Hein, Leiter der Arbeitsgruppe ""Maschinelles Lernen"", einen Grund dafür. Der andere: ""Wir wollen auch internationale Studierende anlocken."" Aus mehr als 220 Bewerbungen wählte er mit seinem Team 65 Studierende aus, die zumeist einen Bachelor-Abschluss in Informatik, Mathematik oder Physik mitbrachten. Deep Learning, also der Einsatz neuronaler Netze und großer Datenmengen, oder statistisches Lernen sind verpflichtende Module, andere Inhalte können frei gewählt werden. ""Alles, was im Bereich der Objekterkennung und der Sprachverarbeitung heute möglich ist, basiert auf maschinellem Lernen"", sagt Universitätsprofessor Hein. Und man wisse mittlerweile auch, dass anders als vor 30 Jahren, als KI schon mal ein großes Thema war, die praktischen Anwendungen im Bereich des maschinellen Lernens jetzt funktionierten und industriell nutzbar seien. Entsprechend groß werde auch die Nachfrage sein. Ein Beispiel aus Tübingen: Dort will Bosch ab 2020 für 35 Millionen Euro ein Entwicklungszentrum für KI mit bis zu 700 Arbeitsplätzen bauen.

Sehr viel mehr Erfahrung hat die Technische Universität München (TUM) mittlerweile in der Lehre gesammelt. Schon im Jahr 2009 legte sie den Master-Studiengang ""Robotics, Cognition, Intelligence"" auf und verknüpfte dafür Maschinenbau und Elektrotechnik mit der Informatik. Wie gelingt es, Maschinen so zu konstruieren, dass sie sich intelligent verhalten? Wie kann man intelligente Produktionssysteme für die Industrie anlegen? Wie lässt sich die Automation verbessern? Diesen Fragen wollten sich im ersten Jahrgang nur zehn Studienanfänger widmen. Fürs aktuelle Wintersemester haben sich bereits 200 Studierende an der TU für das Studium immatrikuliert. ""Wirtschaft und Forschung benötigen Experten, die ein Systemwissen mitbringen"", sagt Alexander Lenz, der den Studiengang an der TUM koordiniert. Man suche nicht nur den klassischen Informatiker, der sich gut mit Betriebssystemen und Algorithmen auskenne, sondern jemanden, der zudem verstehe, wie ein Algorithmus einen Motor steuere und wie ein System Energie umwandle.

Wenn die Studierenden bei Lenz die Masterarbeit abgeben, habe der eine Teil bereits einen Job in der Tasche, der andere könne sich einen aussuchen - unter mehreren Angeboten. Insbesondere im Münchner Raum gibt es nicht nur Automobilfirmen und Zulieferbetriebe, sondern auch mittelständische Maschinenbauunternehmen, Roboterhersteller, Chiphersteller oder Start-ups. Lenz: ""Das Thema KI ist kein Hype, der Bedarf an akademisch ausgebildeten Fachkräften ist wirklich da.""";https://www.sueddeutsche.de/karriere/ki-studiengaenge-bits-und-bots-1.4707740;sz.de;Benjamin Haerdle
02.10.2019;Was der neue TU-Präsident in Garching vorhat;"Thomas Hofmann steht am Fenster in einer der oberen Etagen des Institute for Advanced Study. Der neue Präsident der Technischen Universität München (TU) hat dort gerade Fotos von sich machen lassen und genießt den Blick über den Garchinger Campus. Den Überblick wird er noch brauchen. Er hat für die Zukunft große Pläne, darunter nicht weniger als einen Kulturwandel. Der TU-Präsident, der zum 1. Oktober sein Amt angetreten hat, will den Garchinger Campus zu einem ""globalen Austauschplatz des Wissens"" entwickeln. Wissenschaftler sollen mit Experten aus Wirtschaft, Politik und Gesellschaft zusammenkommen, um über Grenzen hinweg Lösungen für ein nachhaltiges Zusammenleben zu finden.

Den Studierenden will der Nachfolger des langjährigen TU-Präsidenten Wolfgang Herrmann ein neues Wertebewusstsein vermitteln und ihren Sinn für gesellschaftliche Verantwortung schärfen. Deswegen möchte er geistes- und sozialwissenschaftliche Inhalte in die Studiengänge der Natur-, Lebens- und Ingenieurwissenschaften integrieren. Mit dem Fokus auf lebenslanges Lernen will er am Campus zudem ein neues Profilelement implementieren. Der neue Posten bringt einige Veränderungen mit sich. Der 51 Jahre alte Lebensmittelchemiker hat die Leitung seines Lehrstuhls in Weihenstephan kommissarisch an eine Nachfolgerin abgegeben und auch für den Posten als Direktor des Leibniz-Instituts für Lebensmittel-Systembiologie an der TU ist eine Frau im Gespräch. ""Zwei hervorragende Wissenschaftlerinnen"", sagt Hofmann.

Damit kann er sich ganz auf das Präsidentenamt konzentrieren, das er als ""150-Prozent-Job"" bezeichnet. Seine Aufgabe sieht er wesentlich darin, die TU auf sich verändernde gesellschaftliche Herausforderungen vorzubereiten. ""Und das gelingt nur, wenn wir als Universitätsleitung diese Veränderungsbereitschaft selbst vorleben und mit einem klaren Leitbild und neuen Formaten der Kommunikation die unterschiedlichen Menschen an der TU mit auf die Reise in die Zukunft nehmen."" Dabei sieht sich Hofmann nicht als Einzelkämpfer, sondern er möchte den Leuten zuhören und lernen, was sie antreibt. Gemeinsam gehe es darum, Entwicklungsmöglichkeiten zu identifizieren und diese dann gezielt zu fördern.

Wichtig ist ihm dabei vor allem, die Leute zu motivieren, über den eigenen Tellerrand zu schauen und die Fachrichtungen interdisziplinär zusammenzubringen. Denn: ""Viele Innovationsfelder liegen genau an den Schnittstellen"", sagt Hofmann. Erschwert wird dieses Ziel durch die verschiedenen Standorte der TU. Dazu zählen außer Garching noch die Innenstadt, Weihenstephan, Straubing und Heilbronn, ein Campus der gerade im Aufbau ist, und dann kommt natürlich der Standort in Ottobrunn hinzu, wo Bayerns Ministerpräsident Markus Söder eine neue Fakultät für Luftfahrt, Raumfahrt und Geodäsie ansiedeln will.
Ein Campus mit Zukunft

Um den Austausch der Wissenschaftler zu fördern, müsse man über neue digitale Formate nachdenken, sagt Hofmann, zudem brauche es den menschlichen Kontakt. ""Nicht zuletzt ist Vertrauen die Grundlage jeder Kooperation, aber Vertrauen entsteht nicht über digitale Medien, sondern über persönlichen Kontakt."" Es gelte, Brücken zwischen den Standorten zu schlagen und zu erreichen, dass alle wie eine Universität denken, trotz unterschiedlicher Standorte.

Zur Entwicklung Garchings hat Hofmann klare Vorstellungen: modernste Themenstellungen, talentförderliche Maßnahmen, die ganz gezielt die Einzelnen fördern, beim Studenten angefangen, und neue Kooperationsmodelle zu etablieren, die Experten auch aus anderen Wissenschaftseinrichtungen anziehen, genauso wie Fachleute aus der Wirtschaft und der Politik - in diesem Zusammenspiel entwickelt Hofmann seine Idee vom ""Tauschplatz des Wissens"", an dem die Zukunftsthemen gemeinsam bearbeitet werden. Dabei müsse sichergestellt sein, dass die behandelten Themen gesellschaftsrelevant seien, sagt er. So formt sich sein Bild von einem führenden europäischen Innovationszentrum, ""von einem Campus, der Zukunft hat"".

Diesen Tauschplatz versteht Hofmann als ""turbulentes Treiben von Experten unterschiedlicher Disziplinen und Institutionen"". Die TU könne damit die großen Themen angehen, etwa im Bereich der Diagnose und Therapie von neuen Krankheiten, über die unterschiedlichen Disziplinen hinweg. Ein anderes Beispiel wären nach seiner Vorstellung eine zukunftsfähige Mobilität oder der Klimawandel. ""Wir brauchen da das Zusammenwirken von vielen Experten aus der Wissenschaft, der Wirtschaft, Gesellschaft und Politik."" Die Zeit des Technologie-Nerds sei lange vorbei, sagt Hofmann. ""Die Entwicklung neuer Technologien dient nicht dem Selbstzweck, vielmehr gilt es, diese auf die Bedürfnisse und Erwartungen der Menschen auszurichten."" Gerade der Klimawandel könne nicht ""in bayerischer Isoliertheit"" betrachtet werden. So große Themen müssten länderübergreifend gedacht werden.

Um die Politik, die Geistes- und Sozialwissenschaften mit einzubinden, hat die TU bereits im Jahr 2012 das Munich Center for Technology in Society (MCTS) gegründet, das künftig mit dem TU Institute for Ethics in Artificial Intelligence einen Ausleger im Galileo bekommen wird, dem gerade entstehenden neuen Zentrum des Garchinger Campus, an dem Lehre, Lernen und auch Lifestyle zusammenkommen sollen. Hofmann will ganz bewusst ""die Sozial- und Ethikwissenschaften im Herzen von Naturwissenschaft und Technik platzieren"". ";https://www.sueddeutsche.de/muenchen/muenchen-technische-universitaet-praesident-hofmann-1.4624229;sz.de;Gudrun Passarge
01.10.2019;Künstlicher Intelligenz: Katholische Uni verstärkt Forschung;"Die Katholische Universität Eichstätt-Ingolstadt gründet ein neues Institut zur Erforschung von Themen der künstlichen Intelligenz. Zum Herbst 2020 soll die Einrichtung in Ingolstadt in Betrieb gehen. ""Die Stadt Ingolstadt will die Ansiedlung des Instituts durch die Finanzierung von drei Stiftungslehrstühlen über fünf Jahre hinweg unterstützen"", berichtete die Hochschule am Dienstag. Die gesamte Fördersumme betrage 3,75 Millionen Euro. An der Einrichtung sollen zudem etwa 1000 Studienplätze entstehen.

Das ""Institut für Angewandte Mathematik, Maschinelles Lernen und Data Science"" wird sich beispielsweise mit Geomatik beschäftigen, der Verarbeitung von Daten aus erd-, luft- oder satellitengestützten Systemen. Sensoren sollen dabei die Strukturen und Veränderungen in der Umwelt erfassen. Aus den gewonnenen Daten entstehen nach Auskunft der Universität beispielsweise computergestützte Modelle, die zum besseren Verständnis von Prozessen im Straßen- und Luftverkehr beitragen. Diese seien Grundlagen für langfristige Planung in den Bereichen Verkehr und Infrastruktur oder zum Schutz vor Naturkatastrophen.";https://www.sueddeutsche.de/bildung/hochschulen-eichstaett-kuenstlicher-intelligenz-katholische-uni-verstaerkt-forschung-dpa.urn-newsml-dpa-com-20090101-191001-99-116836;sz.de;DPA
12.09.2019;Uni Rostock bekommt vier neue Professuren;"Die Universität Rostock bekommt vier neue Professorenstellen. Die Posten sollen zum 1. Dezember in den Fachgebieten Data Science (Datenwissenschaften), Pflanzliche Stoffwechselphysiologie, Zellbiologie mariner Organismen und Empirische Bildungsforschung und Schulpädagogik geschaffen werden, teilte das Bildungsministerium am Donnerstag mit. Zehn Jahre lang werde jede der so genannte Tenure-Track-Professuren mit jährlich 118 000 Euro vom Bund gefördert.

Bei einer Tenure-Track-Professur wird ein promovierter Akademiker nach einer Bewährungszeit Professor, ohne sich habilitieren zu müssen oder vorher eine Juniorprofessur inne gehabt zu haben. Damit sollen akademische Karrieren planbarer und transparenter und gefragte Nachwuchskräfte in der Wissenschaft gehalten werden. Der Bund stellt eine Milliarde Euro bereit, um 1000 Tenure-Track-Professuren zu fördern.";https://www.sueddeutsche.de/bildung/hochschulen-rostock-uni-rostock-bekommt-vier-neue-professuren-dpa.urn-newsml-dpa-com-20090101-190912-99-851637;sz.de;DPA
26.08.2019;Wie die Hochschulen in Bayern sich wandeln;"Dass sich die meisten Fachhochschulen schon vor Jahren in ""Hochschulen für angewandte Wissenschaften"" umbenannt haben, ist Programm: Der Name drückt den Anspruch aus, eine besonders praxisorientierte Ausbildung zu bieten. Zudem reagieren die Hochschulen recht schnell auf den Wandel in der Gesellschaft, indem sie schon bei den Bachelorstudiengängen immer spezialisiertere Angebote machen, von denen einige auch berufsbegleitend belegt werden können.

17 staatliche und acht nicht-staatliche Fachhochschulen gibt es in Bayern. Knapp 138 000 Studierende waren dort im vergangenen Jahr immatrikuliert. Viele Hochschulen werden ihr Portfolio zum kommenden Semester im technischen Bereich um Berufsbilder erweitern, die durch die Digitalisierung entstanden sind. Zweites großes Trendthema ist und bleibt die Gesundheitsbranche. Im Herbst 2020 soll es an sieben bayerischen Hochschulen neue Studienangebote zur Pflege geben. Schon zum kommenden Semester werden die folgenden Bachelorstudiengänge starten: Hebammenwesen

Bisher fand die dreijährige Hebammenausbildung fast ausschließlich an speziellen Hebammenschulen statt, die an Krankenhäuser angegliedert sind. Nun bereiten mehrere Hochschulen in Bayern einen Bachelorstudiengang vor, in dem sich Interessenten in sieben Semestern zu Geburtshelfern ausbilden lassen können. Der Studiengang verbindet ein wissenschaftliches Studium mit einer praxisbezogenen Ausbildung. Schon im kommenden Wintersemester soll es an der Ostbayerischen Technischen Hochschule Regensburg und der katholischen Stiftungsfachhochschule München losgehen. Die Hochschule Landshut plant, im Wintersemester 2020/21 mit dem Studiengang Hebammenwesen zu beginnen.
Arztassistenz

Der Ärztemangel in Deutschland wird sich in den kommenden Jahren noch verschärfen, weil viele noch praktizierende Mediziner in den Ruhestand gehen und zu wenig ausgebildete Ärzte nachfolgen. Die Ostbayerische Technische Hochschule (OTH) Amberg-Weiden reagiert auf dieses Problem mit der Ausbildung zur ""Physician Assistance - Arztassistenz"", ein Berufsbild, das in den USA deutlich bekannter ist als hierzulande. Absolventen sollen administrative, aber auch medizinische Aufgaben übernehmen und Ärzte entlasten, indem sie beispielsweise Wunden versorgen, Behandlungen dokumentieren, Visiten organisieren oder auch das OP-Management übernehmen. Momentan ist die OTH Amberg-Weiden die einzige staatliche und damit gebührenfreie Hochschule, die für dieses Berufsbild ein Bachelorstudium anbietet. Beim medizinischen Teil der Ausbildung arbeitet die Hochschule mit regionalen Kliniken zusammen. Digitalisierung in der Medizin

Mit dem Fitnessarmband kann man seinen Herzschlag überwachen, ohne allzu schnell mit der zugehörigen App überfordert zu sein. Im Gesundheitswesen ist die Sache um einiges unübersichtlicher: Von der Diagnostik über neue Entwicklungen für die Telemedizin, von Assistenzsystemen für Pflegebedürftige bis zum Datenaustausch mit der Krankenkasse werden Praxen, Krankenhäuser und Heime mit immer neuen digitalen Angeboten überschwemmt. Viele Kliniken, Ärzte und Manager sind überfordert mit der Frage, welche neuen Geräte und welche Software wirklich sinnvoll sind. Um passgenaue Lösungen zu entwickeln - und um entscheiden zu können, was tatsächlich nützliche Innovationen sind, ist Fachwissen in den Bereichen Medizin, Softwareentwicklung und Technik hilfreich.

Solche interdisziplinär geschulten Experten bilden künftig mehrere Hochschulen aus, wobei sie unterschiedliche Schwerpunkte setzen: Mal geht es mehr um die Entwicklung, mal mehr um die Anwendung. Die Technische Hochschule Aschaffenburg bietet den Studiengang ""Medical Engineering and Data Science"" an der Fakultät Ingenieurwissenschaften an. Die Hochschule Kempten hat den Studiengang ""Gesundheits- und Pflegeinformatik"" an der Fakultät Informatik eingerichtet. Und bei der OTH Amberg-Weiden ist der Studiengang ""Digital Healthcare Management"" an der Fakultät für Wirtschaftsingenieurwesen aufgehängt.
Intelligent Systems Engineering An der Schnittstelle zwischen Elektrotechnik und Informatik ist das neue Ingenieurstudium ""Intelligent Systems Engineering"" der Ostbayerischen Technischen Hochschule Regensburg angesiedelt. Der ideale Studienanfänger interessiert sich für moderne Mikrocomputertechnik, programmiert gerne und experimentiert schon mal mit elektronischen Bauteilen herum. Nach dem Studium sollen die Absolventen programmierbare Bauteile entwerfen, die heute fast überall zum Einsatz kommen, von der Waschmaschine übers Smartphone bis zum autonom fahrenden Auto. Auch intelligente Regelsysteme für die Energiewirtschaft (Stichwort ""Smart Grid"") sind ein Anwendungsbereich.
Künstliche Intelligenz

Bislang ist ""Künstliche Intelligenz"" nur als Vertiefungsrichtung studierbar, nun bietet die Technische Hochschule Ingolstadt als eine der ersten Fachhochschulen in Deutschland einen Bachelorstudiengang zu dem boomenden Berufsfeld an. Schließlich gilt künstliche Intelligenz als Schlüsseltechnologie von morgen. Die Studierenden sollen lernen, Softwaresysteme zu entwickeln, die Aspekte menschlicher Intelligenz nachbilden: Etwa Programme zur Spracherkennung, zur Interpretation großer Datenmengen, aber auch interaktive Systeme, die sich auf ihre Benutzer einstellen und zum Beispiel in der Robotik eingesetzt werden können.
Datenschutz und IT-Sicherheit

Datenschutz und Datensicherheit sind Themen, mit denen sich die Unternehmensleitung, die Rechtsabteilung und IT-Experten beschäftigen müssen. Die Hochschule Ansbach reagiert darauf mit einer interdisziplinären Ausbildung zu Experten für technischen Datenschutz und IT-Sicherheit, bei der alle drei Bereiche abgedeckt werden. Die Absolventen sollen sich mit Kryptografie, Hacking und Netzwerksicherheit genauso auskennen wie mit dem Datenschutzrecht. Sie sollen auch wissen, wie Unternehmen Daten rechtskonform verwenden und dabei möglichst wirtschaftlich handeln können.
Chemtronik Die Technische Hochschule Rosenheim hat einen Ableger im schönen Städtchen Burghausen, wo sie einen großen Bedarf an Fachkräften für die Chemieindustrie ausgemacht hat. Schließlich liegt Burghausen im sogenannten bayerischen Chemiedreieck mit etwa 20 000 Mitarbeitern bei 18 Unternehmen der chemischen Industrie. Die zur Gemeinschaft ""ChemDelta Bavaria"" zusammengeschlossenen Firmen waren an der Entwicklung des neuen Studiengangs Chemtronik beteiligt. Ausgebildet werden Ingenieure der Automatisierungstechnik, die auf chemische und pharmazeutische Anlagen spezialisiert sind und auch das digitale Handwerkszeug beherrschen - um später einzelne Anlagen optimieren zu können oder sie intelligent zu vernetzen.
Verfahrenstechnik und Nachhaltigkeit

Nicht erst seit den Demonstrationen gegen Klimawandel und Plastikmüll in den Ozeanen ist es eine gute Idee, schon bei der Produktion von Gegenständen auf Nachhaltigkeit, den schonenden Umgang mit Ressourcen und die Wiederverwertbarkeit von Material zu achten. Im Kempten können Studierende sich im Bachelorstudiengang Verfahrenstechnik und Nachhaltigkeit damit von Anfang an befassen. Im Vergleich zur klassischen Verfahrenstechnik werden hier Recyclingprozesse, Wasserwirtschaft und Umwelttechnik eine stärkere Rolle spielen. Nach dem Studium sollen die Studierenden später die Produktion eines Unternehmens nachhaltiger und kostengünstiger gestalten. Der siebensemestrige Studiengang ist der Fakultät Maschinenbau zugeordnet.";https://www.sueddeutsche.de/bayern/bayern-fachhochschulen-digitalisierung-pflege-bachelorstudium-1.4575549;sz.de;Claudia Henzler
23.08.2019;KI als Schulfach;"Künstliche Intelligenz (KI) ist eine Schlüsseltechnologie unseres Jahrhunderts. KI-Experten werden auf der ganzen Welt dringend gesucht - aber es gibt einfach nicht genug davon. Der Bewerbermarkt ist deshalb leergefegt und der globale Wettbewerb um KI-Talente enorm. Unternehmen, Forschungseinrichtungen und Hochschulen bieten im Fachbereich Künstliche Intelligenz zwar Voll- und Teilzeitstellen an für Praktikanten, Werkstudenten, Systementwickler, Programmierer, Junior- und Senior-Berater, wissenschaftliche Mitarbeiter, Doktoranden und Professoren. Schätzungen zufolge gibt es in Forschung und Wirtschaft zurzeit weltweit etwa 200 000 bis 300 000 KI-Experten. Jährlich werden etwa 20 000 Fachleute mit einschlägigen Bachelor-, Master- oder Doktoratsabschlüssen ausgebildet. Das Problem ist: Gebraucht würden mehr als eine Million KI-Experten.

Eine Folge davon ist, dass schon junge und noch relativ unerfahrene KI-Ingenieure mit außerordentlich hohen Einstiegsgehältern rechnen dürfen. Im Silicon Valley beispielsweise liegt das Jahresgehalt für Angestellte mit KI-Expertise bei etwa 130 000 Euro, das durchschnittliche Jahreseinkommen für KI-Stellen, die auf dem Stellenportal Glassdoor angeboten werden, hat sich bei etwa 100 000 Euro eingependelt, und auf dem Karriereportal Paysa liegt das durchschnittliche Gehalt für KI-Stellen (wovon 35 Prozent eine Promotion und 26 Prozent einen Masterabschluss erfordern) sogar bei 150 000 Euro. Auch große Firmen, darunter sogar die Schwergewichte Google und Microsoft, können derzeit etwa 20 Prozent ihrer offenen KI-Stellen nicht besetzen.
Es reicht nicht, die Schulen nur mit schnellen Anschlüssen ans Internet auszurüsten

Der Wettbewerb um KI-Experten findet inzwischen nicht nur zwischen Firmen, Hochschulen und Forschungseinrichtungen statt, sondern auch auf nationaler Ebene zwischen einzelnen Ländern und - im Streben um die ""Vorherrschaft in der KI"" - auf multinationaler Ebene zwischen USA, China und Europa. Europa tut sich sehr schwer in diesem Wettbewerb und hat seit einigen Jahren sogar einen ""KI-Braindrain"" zu beklagen: Immer mehr KI-Fachkräfte wandern ab zu außereuropäischen KI-affinen Firmen, allen voran zu den großen Tech- und Internet-Firmen aus USA und zunehmend auch zu chinesischen Hightech-Riesen.

Der Mangel an KI-Fachkräften bringt für europäische Unternehmen ernsthafte Probleme mit sich. Vielen von ihnen bleibt dadurch der Zugang zu dieser Schlüsseltechnologie und somit zu innovativen KI-basierten Anwendungen, Produkten und Dienstleistungen versperrt. Auf mittlere und lange Sicht bedeutet das für die europäische Wirtschaft einen deutlichen Wettbewerbsnachteil, zumal - anders als Europa - die USA und vor allem China bereits seit mehreren Jahren massiv und mit vielen Milliarden in die KI-Forschung und die wirtschaftliche Nutzung von KI-Technologien investieren. Europas Unternehmen und Politik müssen deshalb dringend etwas gegen diesen Fachkräftemangel tun. Wirkungsvolle Maßnahmen hierfür gäbe es durchaus.

Wichtiger denn je ist es angesichts dieser Ausgangslage, starke strategische KI-Allianzen und -Partnerschaften zwischen Unternehmen und Hochschulen und Forschungseinrichtungen zu schließen. Davon profitieren beide Seiten: Unternehmen bekommen direkten Zugang zu KI-Expertise und können eigenes KI-Know-how aufbauen. Umgekehrt erhalten Wissenschaftler die Möglichkeit, KI-Technologien im Rahmen von praxisnahen Problemstellungen zu erproben und weiterzuentwickeln.

Solche Allianzen können beispielsweise Masterarbeiten und Praktika betreffen, wenn es darum geht, erste Erfahrungen mit KI-Technologien zu sammeln. Aber auch umfangreichere Forschungs- und Entwicklungsprojekte können so angegangen werden, etwa wenn unternehmensinterne Kernprozesse mithilfe von KI-Technologien optimiert werden sollen. KI-Allianzen können aber auch bedeuten, gemeinsam finanzierte Stellen zu schaffen, die es Wissenschaftlern erlauben, sowohl in ihrer Universität als auch in einem Unternehmen zu forschen und sich in der Aus- und Weiterbildung von Studierenden zu engagieren.

Unternehmen müssen bereit sein, in solche Allianzen Zeit und Geld zu investieren. Die Politik könnte den Aufbau solcher Allianzen fördern. Zum einen, indem sie das dafür nötige Geld bereitstellt. Zum anderen, indem sie ein europaweites Kontakt-Netzwerk ins Leben ruft und fördert. Dieses Netzwerk würde es Unternehmen ermöglichen, schnell und unkompliziert kompetente Ansprechpartner und Anlaufstellen aus dem akademischen KI-Umfeld zu finden. Nur wenn Unternehmen die Möglichkeit haben, gezielt relevante Informationen über vorhandene KI-Technologien und ihre möglichen Anwendungen aus erster Hand zu bekommen und gemeinsam mit Experten Projektideen auszuloten, wird es gelingen, KI und Wirtschaft effektiv zu vernetzen.

Nicht zuletzt sollten Unternehmen auch in Betracht ziehen, strategische Allianzen mit anderen Unternehmen zu bilden, um so gemeinsam KI-Anwendungen zu entwickeln. Auf diese Weise könnte KI-Expertise zusammengeführt und Kosten gespart werden. Zudem würde das Risiko auf mehrere Schultern verteilt. Unternehmerische Allianzen können auch dazu dienen, Daten über Unternehmensgrenzen hinweg zu bündeln. Dies würde neue Anwendungen ermöglichen, die aber den Einsatz datenintensiver KI-Technologien wie etwa maschinelles Lernen erfordern. Mit solchen Allianzen würden Unternehmen ausnutzen, dass Daten eben nicht ""das neue Öl"" sind, wie manchmal behauptet wird. Im Unterschied zu Öl können nämlich Daten von beliebig vielen Unternehmen beliebig oft benutzt werden. Sie können auch von jedem Unternehmen erzeugt und unterschiedlich verwendet werden - weshalb Daten für verschiedene Unternehmen unterschiedlichen Wert haben können.

Ein schönes Beispiel für eine unkonventionelle Allianz als Antwort auf eine sich stark verändernde Technologie- und Marktsituation ist die kürzlich von den Konkurrenten Daimler und BMW beschlossene Bündelung ihrer Mobilitätsdienste. Treffend formulierte es der frühere Daimler-Chef Dieter Zetsche: ""Wir werden eine Reihe völlig neuer Wettbewerber haben. Wenn wir weiterhin nur das tun, was wir so gut gemacht haben, sind wir erledigt."" Genau darum geht es auch bei dem Markt- und Technologiewandel, der von der KI bewirkt wird.

Um mittel- und langfristig sicherzustellen, dass in Europa genügend KI-Fachkräfte zur Verfügung stehen, muss vor allem umfassend in KI-Bildung investiert werden. Auch hier kann die Wirtschaft einen wichtigen Beitrag leisten. Unternehmen können es den großen Tech- und Internet-Giganten gleichtun und ihre eigenen Mitarbeiterinnen und Mitarbeiter gezielt fördern, sich KI-Kenntnisse anzueignen und sich im Bereich der KI weiterzubilden. Eine Möglichkeit wäre es zum Beispiel, die Mitarbeiter für die Teilnahme an Online-Kursen (sogenannten MOOCs) freizustellen und finanziell zu unterstützen. Diese Kurse gibt es in hoher Qualität zu verschiedenen KI-Themen, angeboten werden sie beispielsweise von Udemy, Udacity und Coursera.

Das Angebot an KI-spezifischen Studiengängen muss europaweit deutlich ausgeweitet werden. Zwar liegen hierzu in vielen europäischen Ländern schon entsprechende Absichtserklärungen vor. Die Umsetzung jedoch geht nur sehr zögerlich voran. Zudem kann man den Eindruck bekommen, dass bei der Ausweitung des KI-Studienangebots zu sehr auf Quantität geachtet wird. Es kommt jedoch nicht nur auf die Anzahl neuer Professuren und Stellen für den akademischen Mittelbau an, sondern auch darauf, diese Stellen qualitativ so hochwertig auszustatten, dass sie mit den attraktiven Stellen von Google und Co. konkurrieren können.
Klappt es nicht mit der KI, ist das für Europa ein Wettbewerbsnachteil

Als langfristig angelegte Maßnahme muss KI schließlich als eigenständiges Fach an allgemeinbildenden Schulen eingeführt werden - hier ist die Bildungspolitik gefordert. In diesem Fach sollten neben Grundlagen auch gesellschaftliche, wirtschaftliche und ethische Auswirkungen der KI behandelt werden. Die deutsche Gesellschaft für Informatik hat auf die Bedeutung von KI als Schulfach hingewiesen - und darauf, dass es nicht genügt, mit dem ""Digital-Pakt"" nur auf moderne digitale Infrastruktur hinzuwirken.

Es wäre fatal für Europa, würde es seine Bemühungen um die KI-Bildung nicht deutlich stärken. Das zeigt ein Blick nach China. Dort wird der Aufbau von weiteren KI-Studiengängen, eigenen KI-Fakultäten und KI-Forschungseinrichtungen von der chinesischen Regierung bereits seit Jahren mit großem Nachdruck vorangetrieben. Derzeit werden rund 400 neue Studiengänge mit den Schwerpunkten KI, Big Data, Data Science und Robotik eingerichtet. Auch hat KI bereits Einzug in chinesische Schulen gehalten. Im vergangenen Jahr wurde ein mehrbändiges KI-Buch samt darauf abgestimmter Programmierplattform vorgestellt, das die gesamte Schullaufbahn - vom Kindergarten (was auch in China nicht unumstritten ist) bis zu den Abschlussklassen höherer Schulen - abdeckt.

Der KI-Fachkräftemangel ist ein ernstes Problem für die europäische Wirtschaft. Um es zu lösen, ist es entscheidend, starke KI-Allianzen zu bilden und umfangreich in KI-Bildung zu investieren. Flankiert werden müssen diese beiden Maßnahmen durch eine umfassende Förderung von KI-Spitzenforschung. Hierzu gehört beispielsweise auch der Aufbau von europäischen KI-Forschungszentren, wie es auch von den europäischen KI-Initiativen Claire und Ellis gefordert wird. Unternehmen und Politik müssen dem Mangel an KI-Fachkräften gemeinsam und auch entschlossen entgegentreten. Für Europa geht es dabei um viel, die USA und China sind mächtige Konkurrenten. Und die Zeit drängt.";https://www.sueddeutsche.de/wirtschaft/samstagsessay-ki-als-schulfach-1.4573491;sz.de;Gerhard Weiss
24.06.2019;R2-D2 und Terminator prägen Vorstellung von KI;"Was ist und was kann Künstliche Intelligenz? Die Vorstellungen davon sind bei vielen Menschen vor allem auch durch Serien-Helden und Film-Charakter aus der Science Fiction geprägt.

Wie eine Studie des Allensbach-Instituts nun herausgefunden hat, repräsentiert der Knuddel-Roboter R2-D2 aus der Serie ""Star Wars"" die Vorstellungen der Menschen noch am ehesten (20 Prozent). 17 Prozent denken demnach beim Thema KI eher an die Killermaschine Terminator.

Bei ebenfalls 17 Prozent der Befragten kommt dagegen Commander Data von ""Star Trek"" ihren Vorstellungen von KI am nächsten, gefolgt von dem intelligenten und sprachbegabten Rennschlitten K.I.T.T. aus der Serie ""Knight Rider"" (16 Prozent der Befragten). Den störrischen Raumschiff-Computer HAL 9000 aus dem Spielberg-Klassiker ""2001: Odyssee im Weltraum"" nannten dagegen nur 6 Prozent der Befragten als am stärksten prägend.

Am Bekanntesten als Repräsentanten von Künstlicher Intelligenz sind demnach der Arnold Schwarzeneggers Terminator (76 Prozent), R2-D2 (65 Prozent) und K.I.T.T. (59 Prozent), gefolgt von Commander Data (50 Prozent) und Agent Smith aus dem Film ""Matrix"" (43 Prozent).

Auf die Frage, welchen Roboter oder welche Maschine sie sich am liebsten als persönlichen Gehilfen wünschen, steht ebenfalls R2-D2 an erster Stelle (15 Prozent). 13 Prozent der Befragten würden dagegen den gepanzerten Sportwagen K.I.T.T. favorisieren, der sich schon im Kampf für Recht und Gesetz bewährt hat. Der TV-Serienstar Commander Data folgt mit einigem Abstand (8 Prozent). Und als Persönlicher Assistent überrundet der Reinigungs-Roboter ""Wall-e"" aus dem gleichnamigen Pixar-Film (""Der letzte räumt die Erde auf"") den Terminator knapp um einen Prozentpunkt.

Die Gesellschaft für Informatik hat die Studie im Rahmen des Projekts ""#KI50: Künstliche Intelligenz in Deutschland - gestern, heute, morgen"" zu ihrem 50-jährigen Bestehen im ""Wissenschaftsjahr 2019 - Künstliche Intelligenz"" des Bundesministeriums für Bildung und Forschung in Auftrag gegeben. Ziel des Projektes ist es, das Thema KI über verschiedene Diskursbeiträge zu entmystifizieren und es in der Gesellschaft zu verankern. Im Mittelpunkt der aktuellen Untersuchung stand unter anderem die Frage, welche popkulturellen Phänomene und Figuren das Bild der deutschen Bevölkerung von KI am stärksten beeinflusst haben.";https://www.sueddeutsche.de/service/internet-r2-d2-und-terminator-praegen-vorstellung-von-ki-dpa.urn-newsml-dpa-com-20090101-190624-99-771073;sz.de;DPA
15.05.2019;Grenzen undurchsichtigen Lernens;"Künstliche Intelligenz (KI) ist ein epochaler Technologiesprung, der die Menschheit vor Fragen stellt, die keine Disziplin alleine beantworten kann. John Brockman, Agent für Wissenschaftsliteratur und Gründer des Debattenforums Edge.org, hat das ""Possible Minds""-Projekt ins Leben gerufen, das Natur- und Geisteswissenschaften zusammenführt, um KI und deren wahrscheinliche Ausformungen und Folgen zu ergründen. Das Feuilleton der SZ druckt Texte aus dem Projekt sowie europäische Reaktionen als Serie.

Judea Pearl ist Professor für Computerwissenschaften und Direktor des Cognitive Systems Laboratory an der University of California in Los Angeles. Er ist einer der Pioniere der Bayes'schen Netze, die künstliche Intelligenz auf der Basis von Zufallsvariablen und Wahrscheinlichkeitsverteilung leistungsfähiger gemacht haben. Als ehemaliger Physiker habe ich mich natürlich auch immer für Kybernetik interessiert, weil sie hochtransparent war, nicht zuletzt, weil sie auf der klassischen Kontroll- und Informationstheorie basierte. Diese Transparenz verlieren wir gerade. Und zwar durch die völlig andere Funktionsweise von Deep-Learning-Systemen in der Forschung zur künstlichen Intelligenz (KI). Dabei geht es, grob gesagt, um Kurvenanpassungen, bei denen die Gewichtungen in langen Eingabe-Ausgabe-Ketten ausgeglichen werden.

Nun trifft man auf eine Reihe von Apologeten dieser neuen Technologie, die fast achselzuckend sagen: ""Wir wissen zwar nicht warum, aber sie funktioniert doch prima."" Denn wenn man Deep Learning auf große Datenmengen ansetzt, entwickelt diese Methode ihre ganz eigene Dynamik, sie korrigiert und optimiert sich selber und spuckt meistens korrekte Resultate aus. Nur wenn die mal nicht stimmen, hat man keine Ahnung, was da wo schiefgelaufen ist und wie man das System reparieren kann. Genauer gesagt: Man weiß nicht, ob ein Fehler im Programm, in der Methode oder in dem eingesetzten Deep-Learning-System selber liegt. Hier sollten wir, müssen wir Transparenz anstreben.

Einige behaupten nun, solche Transparenz sei doch gar nicht immer nötig. Wir verstehen die neuronale Architektur unseres eigenen menschlichen Gehirns ja auch nicht, aber es funktioniert doch ganz gut. Und, so läuft dieses Argument dann weiter, warum sollte man dann nicht auch die Deep-Learning-Systeme so vor sich hin arbeiten lassen, man muss ja nicht verstehen, wie sie funktionieren.
Keine Maschine kann fragen ""Was wäre wenn?"", oder gar, ""was wäre wenn nicht?""

Doch solche Argumentationslinien haben ihre Grenzen. Denn der Grund, warum wir mit unserem dürftigen Verständnis des menschlichen Hirns so gut leben können, ist der, dass wir dieses Hirn besitzen, und das ermöglicht es uns, mit anderen Menschen zu kommunizieren, von ihnen zu lernen, sie zu unterweisen und sie zu motivieren, und zwar in unserer Sprache. Wenn aber alle unseren künftigen intelligenten Maschinen und Roboter so undurchsichtig bleiben wie etwa Alpha Go (das ist die Google-KI, die 2016 den weltbesten professionellen Go-Spieler besiegte, d. Red.), werden wir niemals sinnvolle Gespräche mit ihnen führen können, und das könnte äußerst unvorteilhaft für uns werden. Wir müssen sie dann jedes Mal völlig neu anlernen, wenn wir auch nur kleinste Änderungen vornehmen.

Anstatt also mit opaken System zu arbeiten, versuche ich, ihre theoretischen Limitierungen zu verstehen und zu überwinden. Ich mache dies mit Aufgaben, die eine rein kausal logische Behandlung verlangen, solche, wie sie das Denken in der Wissenschaft bestimmen, und die trotzdem Spielraum bieten für Intuition und Hypothesen, um so Fortschritte bei der Erarbeitung von Lösungen beobachten zu können. Dabei konnten wir grundlegende Beschränkungen bei der KI feststellen, die, wenn sie nicht überwunden werden, verhindern werden, dass wir es jemals mit einer Art menschlicher Intelligenz zu tun bekommen, egal, was wir auch anstellen. Ich denke, dass es kaum weniger wichtig ist, diese Limitierungen und Barrieren aufzuzeigen, als dagegen anzukämpfen.

Aktuelle Maschinenlernsysteme arbeiten fast ausschließlich in einem rein statistischen Modus, der ohne Modellbildung auskommt, in vielerlei Hinsicht so, wie man eine Funktion an eine Datenpunktwolke anpasst. Solche Systeme können nicht über ""Was wäre, wenn ...?""-Fragen nachdenken, sie können daher nicht als Grundlage für eine wirklich starke KI dienen, das wäre eine künstliche Intelligenz, die Denken und Kompetenz menschenähnlich nachbildet. Um aber menschenähnliche Intelligenz erlangen zu können, benötigen künstliche Lernsysteme die Anleitung durch eine Art Blaupause unserer Realität.

Genauer gesagt: aktuelle Lernsysteme verbessern ihre Leistung, indem sie die Parameter für den von Sensoren empfangenen Datenstrom aus ihrer Umgebung kontinuierlich optimieren. Es ist ein langsamer Prozess der Angleichung, den man sich analog zu dem natürlichen Selektionsprozess in der Evolution vorstellen kann. Der erklärt, warum Arten wie Adler und Schlangen über Jahrmillionen hinweg hervorragende Sehsysteme entwickelt haben. Er vermag jedoch nicht jenen über-evolutionären Prozess zu erklären, der den Menschen dazu befähigt hat, in kaum mehr als tausend Jahren Brillen und Teleskope zu entwickeln. Denn worüber Menschen verfügen und was anderen Spezies fehlt, ist die Fähigkeit, mentale Modelle ihrer Umwelt zu entwerfen, Modelle, die sie nach Belieben manipulieren können, die hypothetische Umwelten darstellen, um damit planen und daraus lernen zu können.
Undurchsichtige Lernsysteme bringen uns vielleicht nach Babylon, aber nicht nach Athen

Anthropologen und Historiker des Homo sapiens wie Yuval Noah Harari und Steven Mithen sind sich einig, dass für unsere Vorfahren vor etwa vierzigtausend Jahren der entscheidende Schlüssel, die globale Herrschaft zu erlangen, darin lag, solche mentalen Repräsentationen ihrer Umgebung zu erstellen, zu speichern, immer wieder zu hinterfragen und schließlich diese ""Was wäre, wenn ...?""- Fragen zu beantworten. Beispiele sind etwa Fragen nach den Folgen eines aktiven Eingriffs (""Was passiert, wenn ich das so und so mache?"") oder kontrafaktische Fragen (""Was wäre denn gewesen, wenn ich anders gehandelt hätte?""). Kein aktuelles Maschinenlernsystem stellt solche Fragen oder könnte sie beantworten. Die meisten verfügen nicht einmal über Modelle, aus denen sich die Antworten auf solche Fragen ableiten lassen. Wenn ich nun über den Erfolg des maschinellen Lernens nachdenke und versuche, ihn auf die Zukunft der KI hochzurechnen, frage ich mich: Sind wir bereit, die theoretischen Hindernisse zu umgehen, die uns daran hindern, von einer Hierarchieebene auf eine höhere zu gelangen?

Ich betrachte maschinelles Lernen als ein Werkzeug, um von Daten zu Wahrscheinlichkeiten zu gelangen. Aber dann müssen wir noch zwei weitere Schritte machen, um von Wahrscheinlichkeiten zu wirklichem Verständnis zu gelangen - zwei große Schritte. Einer ist, die Effekte von Handlungen vorherzusagen, der andere betrifft die kontrafaktische Vorstellungskraft. Wir werden die Realität nicht verstehen, wenn wir diese letzten beiden Schritte nicht tun.

In seinem aufschlussreichen Buch ""Foresight and Understanding"" (1961) identifizierte der Philosoph Stephen Toulmin den Kontrast zwischen Transparenz und Opazität als den Schlüssel für das Verständnis der alten Rivalität zwischen den griechischen und den babylonischen Wissenschaften.

Toulmin zufolge waren die babylonischen Astronomen Meister der Blackbox-Vorhersagen, sie übertrafen ihre griechischen Rivalen in Bezug auf Genauigkeit und Konsistenz der Himmelsbeobachtungen bei Weitem. Und doch favorisierte die Wissenschaft die kreativ-spekulative Strategie der griechischen Astronomen, die von metaphorischen Bildern durchdrungen war: runde Röhren voller Feuer, kleine Löcher, durch die das Himmelsfeuer als Sterne sichtbar wurden, eine halbkugelförmige Erde auf Schildkrötenrücken. Und es war diese wilde Modellierungsstrategie, nicht die babylonische Extrapolation, die Eratosthenes (276 - 194 v. Chr.) dazu brachte, eines der kreativsten Experimente der Antike durchzuführen und den Erdumfang zu berechnen. Ein solches Experiment wäre einem babylonischen Dateninstallateur niemals eingefallen.

Modellblinde Ansätze führen zu intrinsischen Begrenzungen der kognitiven Aufgaben, die eine starke KI ausführen kann. Ich denke, dass eine KI auf Augenhöhe mit dem Menschen nicht allein aus modellblinden Lernmaschinen hervorgehen kann. Sie verlangt nach der symbiotischen Zusammenarbeit von Daten und Modellen.

Data Science ist nur insofern eine Wissenschaft, als sie eine Interpretation von Daten erleichtert - ein Zwei-Körper-Problem, das Daten mit der Realität verbindet. Doch Daten allein sind wohl kaum Wissenschaft, egal wie ""groß"" sie werden und wie geschickt man sie manipuliert. Undurchsichtige Lernsysteme bringen uns vielleicht nach Babylon, aber nicht nach Athen.";https://www.sueddeutsche.de/kultur/kuenstliche-intelligenz-lernen-1.4447484;sz.de;Judea Pearl
10.05.2019;Ein Hoch auf das Nichtwissen;"So gegen Anfang des 17. Jahrhunderts war die Welt des Wissens noch in Ordnung. Schon bald, vielleicht schon zu ihren Lebzeiten, so erwarteten es zumindest René Descartes und Francis Bacon, wären wohl alle Schätze des Wissens gehoben. Eine Zeit der Vernunft und der guten Ordnung würde dann anbrechen, denn - so Bacon - ""Wissen ist Macht"".

Ähnlich dachten die französischen En-zyklopädisten, die 1780 den letzten Band ihrer ""Encyclopédie ou Dictionnaire rai-sonné des sciences, des arts et des métiers"" veröffentlichten, das gesamte Wissen der Menschheit in 35 Bänden und 70 000 Artikeln! Sie wären womöglich ein wenig erstaunt gewesen, wenn sie erfahren hätten, dass die größte Enzyklopädie in der Geschichte der Menschheit, die englischsprachige Wikipedia, im Mai 2019 bereits 5,8 Millionen Artikel listet. Ein geübter Leser bräuchte 25 Jahre, um sich durch dieses digitale Weltwissen zu arbeiten, vorausgesetzt, er hätte sonst nichts anderes zu tun. So führt Wissen zur Ohnmacht. Es war ein Irrtum der frühen Neuzeit, dass Wissenschaft eine Art Bergbau sei, in dem die Schätze des Wissens abgebaut werden wie Gold oder Kohle, irgendwann sei ein Ende erreicht, dann könnte man sich der Anwendung dieses Wissens widmen. Es ist eine Idee, die immer mal wieder durch die Welt spukt. Gegen Ende des 19. Jahrhunderts hielten Physiker die großen Fragen ihrer Disziplin für weitgehend beantwortet - bis Max Planck und Albert Einstein mit Quantenmechanik und Relativitätstheorie diese Gewissheiten zertrümmerten. Noch 1997 rief der US-Autor John Horgan das Ende der Wissenschaft aus, weil die überhaupt lösbaren Probleme weitgehend gelöst seien.
Klimaleugner, Impfgegner und Anhänger der Flat-Earth-Theory tragen nicht bei zum Fortschritt der Menschheit

Mittlerweile ist deutlich geworden, dass jede beantwortete Frage neue Fragen aufwirft. Neues Wissen führt zu neuem Nichtwissen, mit dem die Maschinerie der Forschung gespeist wird, die wiederum Wissen und noch mehr Nichtwissen hervorbringt. Deshalb entstehen jedes Jahr weltweit über eine Million wissenschaftliche Publikation. Darin kann man eine endlose Fortschrittsgeschichte sehen - oder einen wachsenden Ozean der Information, in dem Menschen zu ertrinken drohen.

Die moderne Datenflut ist nur ein Beispiel dafür, dass man wegkommen muss von der bedingungslosen Wertschätzung neuen Wissens, sei es in der Forschung, im gesellschaftlichen oder im privaten Bereich. Das bedeutet keine Wissenschaftsfeindlichkeit, neue Medizin und Technologien, die das Leben erleichtern und Weltprobleme lösen, sind dringend notwendig.

Doch mehr als je zuvor sind Bewälti-gungsstrategien nötig. Der Erwerb und die Verarbeitung unnützer Information verschleudern Ressourcen. Das Sammeln immer neuer Daten dient nicht selten der Prokrastination. Sie führt zu Politikern und Unternehmern, die nicht handeln, während sie unendlich viel Zeit verstreichen lassen, um die Entscheidungsgrundlage zu erweitern. Sie verhalten sich wie Studenten, die sich vor dem Schreiben ihrer Masterarbeit drücken und lieber immer neue Studien ausdrucken, die sie doch nicht lesen. Entscheidungen fallen nicht notwendigerweise besser aus, wenn man besonders viel weiß. Und es gibt weitere Aspekte: Wissen kann unglücklich machen, Nichtwissen das Leben erleichtern. Es ist an der Zeit für ein Lob der Ignoranz.

Man darf es damit natürlich nicht übertreiben. Klimaleugner, Impfgegner und Anhänger der Flat Earth Theory tragen nicht bei zum Fortschritt der Menschheit. Bedenklich stimmt es, wenn die Hälfte der US-Amerikaner glaubt, dass Menschen und Dinosaurier einst gemeinsam auf der Erde lebten - die im Übrigen erst 6000 Jahre alt sei. Solche Beispiele bringt der Wissenschaftshistoriker Robert Proctor von der University of Stanford in seinem Buch ""Agnotology: The Making and Unmaking of Ignorance"". ""Agnotologie"" ist ein von ihm eingeführter Begriff für eine Forschungsrichtung, die sich mit der Dynamik des Nichtwissens beschäftigt.

Proctor betont, dass Nichtwissen nicht einfach eine Leere sei, die gemäß der anfangs erwähnten Bergbau-Theorie des 17. Jahrhunderts langsam mit dem Abbruch aus den Minen der Wissenschaft gefüllt wird. Zum Nichtwissen zählt Proctor auch den Irrglauben. Dieser werde nicht selten sogar aktiv durch Manipulation und Lobbyismus geschaffen. Dies hat er unter anderen mit Studien zur Tabakindustrie gezeigt, die lange Zeit verbreitet hat, Nikotin schade der Gesundheit nicht. Dennoch betont auch Proctor den Nutzen bewusst erzeugten Nichtwissens.

So wird kaum jemand bestreiten, dass Geheimnisse in vielen Bereichen von Poli-tik, Wirtschaft und Gesellschaft notwendig sind. Wer nachrichtendienstliche Aktivitäten für legitim hält, wird verstehen, dass der BND seine Einsatzpläne nicht auf seiner Website veröffentlicht. Verhandlungen, ob zwischen Staaten oder Tarifpartnern, sind nur dann möglich, wenn gerade nicht alle Fakten auf dem Tisch liegen. Die Anonymität der Peer Review in der Wissenschaft erleichtert den Gutachtern ein unabhängiges Urteil. Geschlecht, Hautfarbe, Aussehen werden die Entscheidung einer Jury weniger beeinflussen, wenn die Musiker hinter einem Vorhang vorspielen.

Mit gutem Grund sind Justitia die Augen verbunden.

Einen ""Schleier des Nichtwissens"" empfahl auch der Philosoph John Rawls in seinem Hauptwerk ""Theorie der Gerechtigkeit"". Eine gerechte Gesellschaftsordnung würden sich Menschen dann ausdenken, wenn sie sich in einer fiktiven Situation befänden, in der sie zwar entscheiden dürfen, aber nicht wissen, welche Position sie in dieser Gesellschaft einnehmen würden. Vermutlich würden sie weder für den Kommunismus noch für den Turbokapitalismus votieren.

Auch im Privatleben hat das Nichtwissen seine Funktion. Bekanntlich vertreten viele Paartherapeuten die Ansicht, dass nicht jeder flüchtige Seitensprung gebeichtet werden sollte; solches Wissen würde den Partner wohl mehr verletzen als heilen. Überhaupt sollte man sich das Lügen - also die manchmal liebevolle Pflege von Nichtwissen bei anderen Menschen - als eine Gabe vorstellen, die stabile soziale Beziehungen überhaupt erst möglich macht. Von Zerwürfnissen in ihrem Umfeld berichten Autoren, die tatsächlich versuchten, ein Jahr oder wenigstens eine Woche lang lügenfrei zu leben. Nicht jeder möchte darüber informiert werden, dass sein Friseur gerade ein Massaker auf seinem Kopf veranstaltet hat. Gerade zur Pflicht wird die Lüge, wenn sie einen zu Unrecht Verfolgten schützt.

Mit nachvollziehbarem Grund verzichten mögliche Betroffene der Erbkrankheit Chorea Huntington auf einen eigentlich naheliegenden Gentest, der mit größter Sicherheit voraussagen kann, ob das nicht heilbare Leiden bei ihnen ausbrechen wird. Noch schwieriger wird die Entscheidung bei Tests, die mit mittelgroßer Wahrscheinlichkeit einen Brustkrebs vorhersagen, der manchmal einer radikalen Therapie bedarf. Der Wunsch nach Nichtwissen er-streckt sich aber auch auf noch ganz ande-re Lebensbereiche. Längst nicht alle ehemaligen DDR-Bürger mit Stasi-Akte haben bislang ihr Recht auf Einsicht in die Unterlagen wahrgenommen. Vielleicht aus Angst, einen Spitzel im Freundes- oder Familienkreis zu entdecken? Jeder Arzt oder Psychologe kann bestätigen: Wissen verursacht manchmal gravierende emotionale Nebenwirkungen.";https://www.sueddeutsche.de/wissen/nichtwissen-forschung-wissenschaft-gesellschaft-1.4440778;sz.de;Christian Weber
26.04.2019;Der Staat wird digitaler, die Bürger misstrauischer;"Sieben Sessel stehen im Kreis, mit dunkelroten Polstern und einem eckigen Steuermodul, das in die Lehne eingelassen ist. Der Raum wirkt wie eine Kommandozentrale aus einer Science-Fiction-Serie, doch er ist echt. Und er erzählt von dem Moment, als die Vision eines automatisierten Staates schon einmal ganz nah an der Umsetzung war: Mit dem Projekt ""Cybersyn"" sollten Anfang der 1970er-Jahre in Chile Computer den staatlichen Alltag organisieren, zum Beispiel Fabriken kontrollieren und Lieferketten anpassen. Doch es wurde nie vollständig umgesetzt.

Cybersyn könnte nur eine unterhaltsame Episode aus der Geschichte der Planwirtschaft sein. Die Realität ist aber: Wir leben längst im Zeitalter der Automatisierung - und das gilt auch für die öffentliche Verwaltung. In den USA nutzt die Justiz automatisierte Systeme in der Strafverfolgung, in China vermessen und kontrollieren die Behörden ihre Bürger immer strenger digital. Auch in Deutschland setzen staatliche Verwaltungen bereits zum Teil automatisierte Systeme ein - oder es gibt entsprechende Pläne. Doch welchen Einfluss hat das auf eine demokratische Gesellschaft? An dieser Frage wird sich in Zukunft das Verhältnis zwischen Bürgern und staatlichen Institutionen entscheiden. Es ist eine Frage des Vertrauens. Bevor der automatisierte Staat noch mehr Einfluss auf das Privatleben der Bürger nimmt, muss er sich klare Regeln geben. Seit mehr als 150 Jahren gibt es in Deutschland die amtliche Statistik, die diverse Dinge misst und auswertet. Doch über die Jahre hat sich etwas verändert: Anfangs dienten die erhobenen Daten zu sozialen Strukturen, angemeldeten Fahrzeugen, der Anzahl Arbeitsloser noch hauptsächlich als Entscheidungsgrundlage für die Regierung und zur Orientierung der Bürger. Die individuellen Daten, also zum Beispiel die Steuererklärung, die Job-Historie, die Fingerabdrücke eines Menschen, wurden getrennt von dieser amtlichen Statistik gesammelt und verwahrt.
In Umfragen lehnen die meisten Bürger den Einsatz künstlicher Intelligenz ab

Mit der Algorithmisierung werden diese beiden Säulen staatlicher Datensammlung stärker miteinander verknüpft. Das ermöglicht eine immer genauere Einordnung jedes einzelnen Bürgers durch den Staat: Wie steht er im Vergleich zur restlichen Gesellschaft da? In Österreich ließ der Arbeitsmarktservice (AMS), vergleichbar mit der deutschen Bundesagentur für Arbeit, einen Algorithmus entwickeln, der die Chancen von Arbeitslosen auf einen neuen Job bewerten soll. Das Programm nutzt persönliche Daten wie die Ausbildung, das bisherige Arbeitsleben, Alter, Geschlecht und Staatsangehörigkeit. Der Algorithmus berechnet dann auf Basis allgemeiner Arbeitsmarktdaten einen sogenannten ""Integrations-Chancenwert"", eine Prozentzahl. Die soll Auskunft darüber geben, wie gut die Aussichten dieses Menschen auf eine neue Stelle sind und wie er oder sie gefördert werden soll.

In repräsentativen Umfragen des Fraunhofer Instituts FOKUS lehnten es 67 Prozent der Befragten in Deutschland ab, dass eine künstliche Intelligenz (KI) über ihren Arbeitslosengeld-Antrag entscheidet. Acht von zehn Befragten fühlten sich bei Fehlern durch automatisierte Entscheidungen wehrloser, als wenn ein Beamter Fehler macht. Etwa die Hälfte der Befragten bezweifelte, dass die staatliche Verwaltung verantwortungsvoll mit ihren Daten umgeht und zum Beispiel Datenschutz-Regeln und Vorschriften zur IT-Sicherheit beachtet.

Einer der wichtigsten Faktoren für die Akzeptanz automatisierter Systeme ist der Mensch. Der Berater im Jobcenter ist ein lebendiges Symbol des Verantwortungsbewusstseins und der vermeintlich persönlichen Nähe zwischen Staat und Bürgern. Doch auch er handelt meist in einem klar definierten Rahmen, wie bei einem Algorithmus durchläuft er einen Katalog an Kriterien, die er abfragt und mit anderen Daten abgleicht. Liefe der gleiche Prozess automatisiert ab, mit einer Software, und diese käme zu exakt dem gleichen Ergebnis - der Bürger würde eine negative Entscheidung dennoch wesentlich schlechter aufnehmen. Gegenüber einer Software erkennt er viel stärker seine eigene Ohnmacht, meint die Kälte des Algorithmus zu spüren. Dabei spürt er eigentlich nur die Kälte der Bürokratie, deren Regeln er sich unterwerfen muss.

Menschen sind sehr konsequent darin, ihre eigene Spezies zu überschätzen. Besonders anschaulich wird das in der Medizin, wo Programme bei der Erkennung bestimmter Krankheiten deutlich bessere Trefferquoten haben als Ärzte. Trotzdem würden viele Patienten ihre Daten lieber von einem Menschen analysieren lassen. Wenn der Staat also in Zukunft mehr automatisiert, dann muss er sicherstellen, das Vertrauen der Bürger zu erhalten. Jeder kleine Schritt in Richtung Automatisierung könnte auch ein Schritt zur Entfremdung zwischen seinen Institutionen und den Bürgern sein.

Weltweit arbeiten Wissenschaftler an Gütekriterien für den Einsatz von Algorithmen durch den Staat, in Deutschland etwa im Projekt Algorithmenethik von der Bertelsmann Stiftung. Am Fraunhofer-Institut FOKUS beschäftigen sich Forscher mit der Frage, wie IT in der öffentlichen Verwaltung effizient und im Sinne des Allgemeinwohls eingesetzt werden kann. Die Wissenschaftler sind sich in zwei Dingen einig: Dass mehr Automation durchaus sinnvoll sein kann, weil sie eine Verwaltung effizienter machen kann. Und dass für die Umsetzung dieser Automatisierung allgemeingültige und verbindliche Regeln nötig sind.";https://www.sueddeutsche.de/wirtschaft/behoerden-algorithmen-digitalisierung-buerger-1.4422838;sz.de;Valentin Dornis
20.03.2019;Was ist überhaupt Intelligenz?;"Künstliche Intelligenz (KI) ist ein epochaler Technologiesprung, der die Menschheit vor ganz neue Fragen stellt, die keine Disziplin alleine beantworten kann. John Brockman, Agent für Wissenschaftsliteratur und Gründer des Debattenforums edge.org hat das ""Possible Minds""-Projekt ins Leben gerufen, das Natur- und Geisteswissenschaften zusammenführt, um KI und deren wahrscheinliche Ausformungen und Folgen zu ergründen. Eine erste Textsammlung ist in den USA als Buch mit dem Titel ""Possible Minds - 25 Ways of Looking at AI"" bei Penguin Press erschienen. Das Feuilleton der SZ druckt Texte aus dem Projekt als Serie.

Steven Pinker ist Kognitionsforscher und Psychologe an der Harvard University. In Deutschland erschien von ihm zuletzt das Buch ""Aufklärung jetzt"" bei S. Fischer.

Tech-Prophezeiungen sind ein populäres Genre, für das Norbert Wieners Buch ""Kybernetik"" ein frühes Beispiel war. Wobei es bei Wiener wie so oft weniger um Voraussagen ging, als vielmehr um finstere Warnungen im Geiste des Alten Testaments, dass einen furchtbare Strafen für die Sünden der Zeitgenossen treffen könnten. Solche Tech-Prophezeiungen greifen auf die Rebellion der Romantik gegen die ""satanischen Mühlen"" der industriellen Revolution zurück, wenn nicht sogar noch weiter auf die Archetypen des Prometheus, der Pandora und des Faust. Gegenwärtig beschäftigen sich viele solcher Prophezeiungen mit künstlicher Intelligenz. Auf der einen Seite gibt es da die althergebrachten Science-Fiction-Dystopien von Computern, die Amok laufen und uns in ihrem unaufhaltbaren Welteroberungsdrang versklaven. In den neueren Varianten unterjochen uns KIs ganz aus Versehen, weil sie gnadenlos zielstrebig einen Auftrag erfüllen, egal welche Nebenwirkungen das für die Menschheit hat, auch wenn die ihnen diesen Auftrag ja ursprünglich gegeben hat. Beide Arten von Voraussagen halte ich für Hirngespinste, die auf einem technologischen Determinismus basieren, der vollkommen außer Acht lässt, was für Informations- und Kontrollnetzwerke es in einem intelligenten System wie einem Computer, einem Hirn oder auch in einer Gesellschaft als Ganzes gibt.

Die Angst vor der Unterjochung durch die Maschinen basiert auf einem wirren Verständnis von Intelligenz, die eher der Naturphilosophie der Antike oder einem nietzscheanischen ""Wille zur Macht"" zuzurechnen ist als einer Analyse von Intelligenz und Absicht, was Information, Rechenleistung und Kontrollmechanismen betrifft. In diesen Horrorszenarien wird Intelligenz als allmächtiger, wunscherfüllender Zaubertrank beschrieben, über den der jeweilig Handelnde in unterschiedlichen Mengen verfügen kann. Menschen haben davon mehr als Tiere, und ein künstlich intelligenter Computer oder Roboter wird mehr davon haben als Menschen. Und weil wir Menschen unsere bescheidenen Möglichkeiten dazu benutzt haben, Tiere mit noch bescheideneren Fähigkeiten zu zähmen oder auszulöschen, weil technologisch fortgeschrittene Gesellschaften technologisch primitivere Gesellschaften versklavt oder vernichtet haben, soll daraus folgen, dass eine superkluge KI das Gleiche mit uns tun wird. Und weil eine KI millionenfach schneller denken wird als wir und ihre Superintelligenz so einsetzen wird, dass sie ihre Superintelligenz fortlaufend verbessert, werden wir machtlos sein, sobald sie nur eingeschaltet wird.

Diese Szenarien verwechseln Intelligenz mit Antrieb, Glaube mit Sehnsucht, Schlussfolgerungen mit Zielen. Selbst wenn wir übermenschlich intelligente Roboter erfinden würden, warum sollten sie ihre Herren versklaven wollen, um die Welt zu beherrschen? Intelligenz ist die Fähigkeit, neue Mittel einzusetzen, um an ein Ziel zu kommen. Aber die Ziele selbst sind für die Intelligenz belanglos: Klug zu sein ist nicht dasselbe, wie etwas zu wollen. Es ist nun mal so, dass Intelligenz im Homo sapiens das Ergebnis einer natürlichen Auslese im Darwin'schen Sinne ist, also eines immanenten Konkurrenzprinzips. Im Hirn dieser Spezies ist das Denken gepaart mit Zielen, wie Rivalen zu dominieren oder Ressourcen zusammenzuraffen.

Es ist aber ein Fehler, die Funktion aus dem limbischen System einer bestimmten Primatenspezies mit der Grundbeschaffenheit von Intelligenz zu verwechseln. Es gibt kein Gesetz komplexer Systeme, das besagt, dass sich intelligent Handelnde automatisch in skrupellose Größenwahnsinnige verwandeln müssen.

Ein weiterer Irrglaube ist es, Intelligenz als grenzenloses Kontinuum von Wirkungsmacht zu betrachten, ein wundersames Elixier, mit dessen Hilfe man jedes Problem lösen, jedes Ziel erreichen kann. Dieser Trugschluss führt zu so unsinnigen Fragen, wann KI denn ""das menschliche Level von Intelligenz übertreffen"" werde und ob sich dann zwangsweise eine ""artificial general intelligence"" (AGI) mit gottgleicher Allwissenheit und Allmacht entwickeln wird.

Intelligenz ist streng genommen ein Verbund aus mehreren Software-Modulen, die mit Fähigkeiten ausgestattet sind, unterschiedliche Ziele zu erreichen, oder sich diese aneignen. Menschen können zum Beispiel Nahrung finden, Freunde gewinnen, andere beeinflussen, potenzielle Fortpflanzungspartner bezirzen, Kinder aufziehen, sich frei in der Welt bewegen und allen möglichen Leidenschaften und Ablenkungen nachgehen.

Computer können so programmiert werden, dass sie einige dieser Aufgaben übernehmen, wie etwa Gesichter zu erkennen. Sie können sich auch Probleme vornehmen, die Menschen nicht lösen können, wie das Klima zu simulieren oder Millionen Buchhaltungsunterlagen zu sortieren. Die Probleme sind dabei so unterschiedlich wie die Fähigkeiten, sie zu lösen. Doch anstatt zu erkennen, wie zentral Wissen und Erkenntnis für Intelligenz sind, verwechseln die Dystopien eine künstliche Allgemeinintelligenz der Zukunft mit dem Dämon des Weltgeistes, wie ihn der Mathematiker Pierre-Simon Laplace im 18. Jahrhundert beschrieben hat, ein mythisches Wesen, das den Ort und die Bewegung jedes einzelnen Partikels im Universum kennt und sie mit den Gleichungen der physikalischen Gesetze füttert, um den Zustand aller Dinge und jedes Wesens zu jedem Zeitpunkt in der Zukunft zu errechnen.";https://www.sueddeutsche.de/kultur/steven-pinker-ki-maschine-mensch-1.4375316;sz.de;Steven Pinker
26.02.2019;Dataismus;"Keine Angst, sie will doch nur helfen, die künstliche Intelligenz! Auf dem Smartphone fallen ihr zu jedem Wort, das ein User eintippt, drei tolle Varianten ein, wie der Satz weitergehen könnte! Das ist mal praktisch, mal ungewollt komisch, mal völlig abwegig. Auf jeden Fall ist es herrlich unberechenbar - und damit prädestiniert für die Kunst. Computermusiker Chris Peck und Performerin Eleanor Bauer nutzen solche Autofill-Texte am Schauspiel Bochum offensiv als Kreativitätstrigger. ""New Joy"" heißt ihr skurriles, zärtliches, nervtötendes, futuristisches Bühnenspektakel, Untertitel ""ein dataistisches Cyber-Musical-Happening mit Gesamtkunstdingens"". Ein Abend der definitiv polarisiert.

Es herrscht bizarre Geschäftigkeit auf der weltallschwarzen Bühne voller knautschiger Kunstobjekte in Pastell und Neon: Sechs Frauen und Männer in Roben und Hoodies mit buntem New-Age-Fotoprint von Kostümbildnerin Sofie Durnez ordnen Dinge, murmeln auf Englisch, Deutsch, Französisch, stellen Gesten in den Raum. ""Die Verantwortungstendenz ist überwältigend"", verkündet einer und vollführt eine elegante Drehung. Eine andere kontert, jene, die nicht antworten, würden hoffentlich ""wunderbare filigrane Flugmaschinen konstruieren"". Dazu und daraus entwickeln sich Acapella-Sounds zwischen Oper und Pop, Oberton- und Walgesang, begleitet von Sphärenklängen, Computer-Zirpen, Piepsen, Pluckern und Summen. Mangels Sinn und Katharsis verlassen die ersten Theaterbesucher den Raum.

Peck und Bauer verweigern Erklärungen, Botschaften, Eindeutigkeit. Auf der Suche nach der Kunst des Cyber-Zeitalters entwickeln sie mit ihrem Ensemble skurrile Systeme. Gleich einer Big-Data-Anwendung basieren diese auf Regeln, auf Algorithmen, verselbständigen sich dann aber. Zum Autofill-Text kommen Cut-up-Techniken, Google-Übersetzungs-Loops, Science Fiction frei nach Ursula K. Le Guin, Sprache die in Bewegung, Bewegung die in Musik übersetzt wird.

Erst wenn man das Verstehenwollen aufgibt, tauchen flüchtige Szenen auf, wie Erinnerungen aus der Zukunft. Da begegnen sich zwei Vereinzelte, Sängerin Veronika Nickl und Schauspieler Michael Lippold, in einem Duett, das nur aus der Zahl 140 besteht, er singt ""one hundred"", sie ""and fourty"". Zwei Menschen gelingt ein Moment perfekter Harmonie, so als hätten sie bei zwei Maschinen abgeschaut, wie das geht. Da ist Tänzer William Bartley Cooper auf der Flucht vor einem Spotlight, weil er nicht mag, in wen er sich online verwandelt. Da ruft das Amt für ungebetene Ratschläge an und erteilt ungebetene Ratschläge. In der exakt gespielt, getanzt, gesungenen Wortwirrnis können solche szenischen Miniaturen leicht untergehen. Man könnte diesen Abend für naiv und komplett gaga erklären. Oder man erlaubt ihm, an eine Sehnsucht zu erinnern, die sich vor lauter dystopischer Abgeklärtheit kaum mehr jemand eingesteht.";https://www.sueddeutsche.de/kultur/theater-bochum-dataismus-1.4345990;sz.de;Cornelia Fiedler
04.02.2019;"""Weltuntergangs-Szenarien sind Unsinn""";"Im Dezember hat ein mit künstlicher Intelligenz ausgestattetes Programm eine beachtliche Fähigkeit demonstriert. ""AlphaZero"", wie der Algorithmus der Firma Deepmind heißt, brachte sich innerhalb weniger Stunden Schach bei und spielte dann gegen ""Stockfish"", das beste Schachprogramm der Welt, in dem das gesamte Schachwissen der Menschheit steckt. ""AlphaZero"" brachte ""Stockfish"" eine vernichtende Niederlage bei: Von 100 Partien gewann ""AlphaZero"" 28, die restlichen endeten Remis. Die Fachwelt war beeindruckt, wie schnell ""AlphaZero"" übermenschliche Fähigkeiten erlangt hatte. Der ehemalige Schachweltmeister Garri Kasparow verfolgt die Entwicklung künstlicher Intelligenz genau.

Herr Kasparow, 1997 haben Sie gegen den IBM-Schachcomputer Deep Blue verloren. Welche Rolle spielt Schach heute, wenn es um die Entwicklung künstlicher Intelligenz geht?

Viele Wissenschaftler verwenden Schach, um ihre Ideen zu testen. Es ist ein vergleichsweise einfaches System und daher sehr praktisch. Ähnlich wie die Fruchtfliege Drosophila zu einem Modellorganismus für die Genforschung wurde, ist Schach ein Modellsystem im Bereich der künstlichen Intelligenz. Nachdem ""AlphaZero"" das Schachprogramm ""Stockfish"" vom Brett gefegt hatte, schrieben Sie im Fachmagazin Science, Sie hätten sich gefreut, dass ""AlphaZero"" genau wie Sie einen dynamischen, offenen Stil pflege. Sehen wir in ""AlphaZero"" menschliche Kreativität am Werk?

Nein. Worte wie dynamisch oder riskant sollte man im Zusammenhang mit einer Maschine sehr vorsichtig verwenden.

Sie haben das selbst geschrieben: In Ihren Augen wirke der Stil von ""AlphaZero"" dynamisch, riskant und aggressiv.

Natürlich ist ""AlphaZero"" eine andere Maschine als das klassische Schachprogramm ""Stockfish"". Trotzdem hat ""AlphaZero"" nach wie vor nur eines im Auge: die Gewinnchancen. Wenn ""AlphaZero"" Schachfiguren opfert und nie gesehene, geniale Züge macht, dann nicht, weil es schön und kreativ spielen möchte. Es macht das nur, weil es in dieser Art des Spiels Vorteile sieht. Im Gegensatz zu herkömmlichen Schachcomputern hat ""AlphaZero"" allerdings einen viel tieferen Einblick in das Schachspiel erlangt. Das hat mich so begeistert.

Eigentlich heißt es, je höher das Niveau beim Schach, desto langweiliger und technischer wird das Spiel.

Mit diesem Mythos hat ""AlphaZero"" aufgeräumt. Wir haben gesehen, dass Schach auch auf höchstem Niveau äußerst attraktiv sein kann.

Warum ist ""AlphaZero"" viel besser in Schach, Go oder anderen Spielen als bisherige Programme?

Herkömmliche Computerprogramme nutzen rohe Rechengewalt und brauchen viele Daten, anhand derer sie ihre Züge berechnen. Bei Systemen wie ""AlphaZero"" ist das anders. Diese Algorithmen benötigen nur die Spielregeln. Dann spielen sie gegen sich selbst, erzeugen ihre eigenen Daten und lernen enorm schnell. Das ist schon ein gigantischer Schritt vorwärts.

Ist ""AlphaZero"" das perfekte Schachprogramm?

Nein, ich würde nicht sagen, dass die Daten, die das Programm erzeugt, perfekt sind. Aber sie sind anders als das Schachwissen der Menschheit. Wir sollten die Tatsache akzeptieren, dass die Daten, die ""AlphaZero"" selbst generiert, dem menschlichen Wissen überlegen sind. Wir stehen nun Maschinen gegenüber, die nicht unsere Information benutzen, nicht unser gesammeltes Schachwissen, sondern die aufgrund unserer Anleitung ihre eigene Datenbank, ihr eigenes Wissen aufbauen.

Worin liegt der Nutzen dieser Programme?

Sie helfen uns, Dinge aus einem anderen Blickwinkel zu sehen, der uns bislang verborgen blieb.

Wo außer beim Schach könnten wir davon profitieren?

Praktisch überall, wo es uns gelingt, geschlossene Systeme wie zum Beispiel die Schachregeln oder die Regeln für das Brettspiel Go zu definieren. Dort können diese Maschinen brillieren.

Könnten Werkzeuge wie ""AlphaZero"" uns helfen, die großen ungelösten Fragen der Wissenschaft zu lösen oder helfen, neue Wirkstoffe für die Medizin zu entdecken?

Natürlich. Aber noch einmal: Es ist zentral, dass es uns gelingt, jeweils geschlossene Systeme zu definieren mit Parametern, die den Rahmen für ein solches Programm vorgeben, so wie die Regeln beim Schach.";https://www.sueddeutsche.de/wissen/kuenstliche-intelligenz-kasparow-1.4315967;sz.de;Joachim Laukenmann
14.01.2019;Spione studieren jetzt in Neubiberg;"Angehende Geheimdienstmitarbeiter studieren jetzt in Neubiberg. An der dortigen Universität der Bundeswehr sowie der bis zum Jahreswechel zum Teil in Haar untergebrachten Hochschule des Bundes für öffentliche Verwaltung startete am Montag unter dem ""Intelligence and Security Studies"" der erste deutsche Masterstudiengang für Nachrichtendienste. ""Der Studiengang steht für eine neuartige wissenschaftliche Auseinandersetzung mit dem Militärischen Nachrichtenwesen der Bundeswehr und den Nachrichtendiensten des Bundes"", betont der Beauftragte der Bundesregierung für die Nachrichtendienste, Staatssekretär Johannes Geismann. Ziel sei, die Professionalisierung der nachrichtendienstlichen und sicherheitsbezogenen Ausbildung und Forschung voranzutreiben sowie den gesellschaftlichen Diskurs durch unabhängige wissenschaftliche Beratung zu fördern.

Bei dem neuen Angebot handelt es sich um einen zweijährigen Präsenzstudiengang. Je nach Studienschwerpunkt wird am Ende ein Master of Arts oder ein Master of Science als akademischer Abschlussgrad im Fachgebiet ""Intelligence and Security Studies"" verliehen. Das Angebot richtet sich vor allem an Mitarbeiter der Nachrichtendienste des Bundes und der Länder sowie an Soldaten des Militärischen Nachrichtenwesens der Bundeswehr.

Die Hochschule des Bundes für öffentliche Verwaltung verfügt über umfangreiche Erfahrungen in der nachrichtendienstlichen Aus- und Fortbildung. Die Bundeswehr-Uni ergänzt das Angebot in den Bereichen Big-Data-Analyse, Cyber-Sicherheit und Internationale Politik.";https://www.sueddeutsche.de/muenchen/landkreismuenchen/semesterstart-spione-studieren-jetzt-in-neubiberg-1.4287532;sz.de;DPA
28.12.2018;IT-Experten verzweifelt gesucht;"Aus Bluetooth-Lautsprechern hören immer mehr Menschen ihre Lieblingsmusik, unabhängig davon, in welchem Zimmer ihrer Wohnung sie sich aufhalten. Die Waschmaschine weiß, wann sie den Nachtstrom nutzt - und Autos kommen ganz ohne Fahrer aus. Computersysteme finden sich heute in nahezu allen Bereichen des Lebens. Versteckt und ganz offensichtlich. Und es braucht viele Menschen, die Geräte erdenken, aber auch die Software dahinter programmieren und auf dem Laufenden halten. Denn: Nicht nur im Privaten ist der Computer allgegenwärtig.

""Es gibt kein Unternehmen mehr, das ohne IT auskommt"", sagt Juliane Petrich, Leiterin für den Bereich Bildung im Branchenverband Bitkom. Welchen Stellenwert die Informatik inzwischen hat, zeigt sich auch in den Führungsetagen zahlreicher Unternehmen - dort gibt es oft neben dem CEO und dem CFO, dem Chef für Finanzen, auch einen CIO. Das ist der ""Chief Information Officer"", der Chef über die Daten. Der Bedarf an Fachkräften ist enorm, sagt Petrich. Nach Angaben des Portals Statista waren im vergangenen Jahr 21 500 Menschen im Bereich der IT-Hardware tätig. 875 000 Berufstätige beschäftigten sich mit IT-Services und -Software, wobei vor allem in letzterem Bereich die Anzahl der Beschäftigten in den vergangenen Jahren zugenommen hat.

Erst vor wenigen Jahren sind zahlreiche neue Ausbildungsberufe entstanden, aber auch an den Hochschulen gibt es jede Menge Studiengänge im Bereich Informatik. ""Allerdings liegt die Abbrecherquote bei circa 50 Prozent"", berichtet Petrich. Denn die Erwartungshaltung und die Inhalte gingen stark auseinander. Weit verbreitet und trotzdem sehr gesucht sind die Berufsbilder Softwareentwickler, IT-Sicherheitsexperten, Data Scientist und KI-Entwickler, wobei der Weg in diese Jobs meist über ein Studium führt.

Sehr häufig taucht der Begriff ""Softwareentwickler"" in Stellenanzeigen auf, doch er ist nicht scharf definiert. ""Es gibt verschiedene Rollen im Team"", erläutert Petrich. Da ist zum einen der Frontend-Entwickler: Er ist für die grafische Umsetzung von Programmen zuständig oder die Schnittstellen für die Nutzer. Er macht die Programme also für die Mitarbeiter möglichst verständlich und einfach zu bedienen. Der Backend-Entwickler hingegen ""implementiert die funktionale Logik im Hintergrund"". Das bedeutet, er sorgt dafür, dass die Programme den Anforderungen der Unternehmen entsprechen und dass alles reibungslos funktioniert. In den Beruf des Software-Entwicklers kann nahezu jedes Informatikstudium führen.
";https://www.sueddeutsche.de/karriere/digitalisierung-it-job-ausbildung-studium-1.4238470;sz.de;Verena Wolff
16.12.2018;Woran Wissenschaftler und Ingenieure in München forschen;"Autonom fahren ohne Unfälle

Die ""Menge aller Möglichkeiten"" ist für die Arbeit von Matthias Althoff essenziell. Ist sie Sekunden im Voraus berechnet, können Gefahren ausgeschlossen werden - und Unfälle autonom fahrender Fahrzeuge mit an Sicherheit grenzender Wahrscheinlichkeit vermieden werden. Althoff hat in München Maschinenbau studiert und in Elektrotechnik promoviert. Seit 2013 ist er Juniorprofessor für Informatik an der Technischen Universität München (TU) und arbeitet an einem Verfahren, das autonomes Fahren sicher machen soll - und zwar zu 100 Prozent, wenn man ein winziges Restrisiko vernachlässigt.

Ist ein selbständig fahrendes Fahrzeug in Bewegung, berechnet Althoffs System jene Menge aller Möglichkeiten an Situationen, die Sekundenbruchteile später eintreten können. Ein bremsender Vordermann, ein plötzlich kreuzender Radfahrer, ein von hinten anrasendes Motorrad. Die Varianten, in denen sich Gefahren im Straßenverkehr auftun können, sind unendlich. Trotzdem meint Althoff, dass sein spezielles Verfahren alle möglichen Situationen berücksichtigt, in denen autonom fahrende Fahrzeuge einen Unfall verursachen könnten - zumindest wenn seine Annahmen passen, etwa, dass ein Mensch beim Gehen nicht auf 40 Stundenkilometer beschleunigt. ""Das Verfahren ist beweissicher"", sagt er selbstbewusst. Bis erste Autos damit ausgestattet werden können, dauert es aber noch ein paar Jahre.
Geräusche für die virtuelle Realität

Wenn man morgens am Gleis steht, soll man das Quietschen der Züge nicht mehr hören und wenn man abends in der Bar sitzt, auch das Gespräch vom Tisch nebenan nicht mehr - vorausgesetzt man will es nicht hören. Dagmar Schuller, 43, entwickelt gerade eine Software, die unangenehme Geräusche herausfiltern soll. Die um einen herum eine angenehmere, neue Welt schaffen soll. Die Software soll irgendwann sogar einmal selbst erkennen, was man als störend empfindet, ohne dass man es vorher angibt. Dagmar Schuller sagt: ""Die Systeme sollen für die Menschen arbeiten."" Sie ist die Chefin von Audeering; mit Informatikern von der Technischen Universität München hat sie die Firma gegründet - vor zwei Jahren hatten sie noch sieben Mitarbeiter, doch seit diesem Jahr sind sie mehr als 50 Leute. Sie arbeiten für Kunden wie BMW oder Huawei oder auch Red Bull, sie kümmern sich um alles, was mit akustischer künstlicher Intelligenz zu tun hat. Zum Beispiel um Geräusche in Welten der Virtual Reality, die einem möglichst realistisch vermitteln sollen, dass man tatsächlich gerade Motorrad fährt.

Es geht auch darum, anhand der Sprache zu erkennen, wie ein Produkt bei einem Kunden ankommt, Emotionen herauszulesen. Dagmar Schuller hatte schon in der Schule angefangen, zu programmieren, damals noch auf Rechnern mit schwarzem Hintergrund und grüner Schrift. Wenn einer der Schüler einmal etwas falsch programmiert hatte, stürzten alle Rechner im Raum ab. Dann musste man zum Lehrer gehen, und es gab einen schwarzen Punkt auf der Liste - waren es drei, kam der Verweis. Maschinenblick auf medizinische Daten

Eigentlich ist Franz Pfister Arzt, doch gerade arbeitet er an einer Software, mit der es seine früheren Kollegen im Krankenhaus einmal leichter haben sollen. Das Programm nämlich könnte Fehler vermeiden, die Menschen noch immer unterlaufen. Pfister sagt: ""Da redet niemand drüber, aber zehn Prozent der Tode in der Klinik sind auf einen Diagnosefehler zurückzuführen."" Er hat Medizin studiert, weil er den Menschen und seine Krankheiten besser verstehen wollte. Doch mit 28 Jahren beschloss er, noch einmal zu studieren, um den Menschen und seine Krankheiten noch besser zu verstehen - mit Hilfe von Data Science. Franz Pfister hatte selbst gesehen, wie wenig Zeit in den Kliniken für Patienten blieb, wie komplex die Untersuchungen waren und wie überlastet die Ärzte. Am Abend zum Beispiel sei die Wahrscheinlichkeit höher, dass ein Arzt eine falsche Diagnose stelle als am Morgen, sagt Pfister, und wenn ein Arzt in einem CT- oder MRT-Bild eine Krankheit entdeckt habe, sinke die Wahrscheinlichkeit, dass ihm auch noch eine zweite auffalle.

Menschen werden von ihrer Umgebung beeinflusst - eine Maschine nicht. Franz Pfister, heute 31, und sechs Kollegen haben nun eine künstliche Intelligenz entwickelt, die Auffälligkeiten in medizinischen Daten feststellt. Sie alarmiert zum Beispiel nach einem CT sofort den Arzt, wenn ein Notfall vorliegt, auch markiert sie, welche Hirnregionen er sich genauer anschauen sollte. Gerade baut Pfisters Team am Prototypen ihres Projekts Deepc. Im März werden sie den zum ersten Mal testen, am Klinikum rechts der Isar.
Ein Partner für uns Menschen

Dongheui Lee hätte ihre Pizza gerne von einem Roboter zubereitet. Oder anders gesagt: Sie versucht gerade, den Robotern, mit denen sie arbeitet, beizubringen, wie man Pizza bäckt - und zwar so, wie ein Mensch es vormacht. ""Die Roboter sollen lernen, menschliches Verhalten zu verstehen und nachzumachen"", erklärt Lee das Ziel ihrer Forschungen. Die 41-Jährige hat in Südkorea Maschinenbau studiert, nach einem Zwischenstopp in Tokio ist sie seit 2009 Juniorprofessorin für Elektrotechnik an der TU und möchte Robotern menschliche Verhaltensweisen antrainieren. Lee hat noch ein zweites, größeres Forschungsvorhaben: Sie möchte nicht nur, dass die Roboter menschliches Verhalten analysieren können. Sie sollen auch darauf reagieren können. Eine Interaktion zwischen Mensch und Maschine.

Für ihre Forschungen hat sich Lee lange mit menschlichem Verhalten beschäftigt. ""Es ist immer schwierig, vorauszusagen, wie ein Mensch reagiert"", sagt sie. Fast genauso schwer fällt es ihr, vorherzusagen, wie sich ihre Forschung entwickelt. Vielleicht kann sie in vier oder fünf Jahren einen menschenähnlichen Roboter präsentieren - vielleicht aber auch erst in zehn oder 20 Jahren. Ist Lee erfolgreich, könnte man sich irgendwann mit einem Roboter über das Wetter und den letzten Urlaub unterhalten. Und sich nebenbei eine Pizza von ihm machen lassen. Sofern man ihm davor zeigt, wie das geht.";https://www.sueddeutsche.de/muenchen/kuenstliche-intelligenz-forschung-1.4255315;sz.de;Linus Freymark und Pia Ratzesberger
14.12.2018;Maschinenblick auf medizinische Daten;"Eigentlich ist Franz Pfister Arzt, doch gerade arbeitet er an einer Software, mit der es seine früheren Kollegen im Krankenhaus einmal leichter haben sollen. Das Programm nämlich könnte Fehler vermeiden, die Menschen noch immer unterlaufen. Pfister sagt: ""Da redet niemand drüber, aber zehn Prozent der Tode in der Klinik sind auf einen Diagnosefehler zurückzuführen."" Er hat Medizin studiert, weil er den Menschen und seine Krankheiten besser verstehen wollte. Doch mit 28 Jahren beschloss er, noch einmal zu studieren, um den Menschen und seine Krankheiten noch besser zu verstehen - mit Hilfe von Data Science. Franz Pfister hatte selbst gesehen, wie wenig Zeit in den Kliniken für Patienten blieb, wie komplex die Untersuchungen waren und wie überlastet die Ärzte. Am Abend zum Beispiel sei die Wahrscheinlichkeit höher, dass ein Arzt eine falsche Diagnose stelle als am Morgen, sagt Pfister, und wenn ein Arzt in einem CT- oder MRT-Bild eine Krankheit entdeckt habe, sinke die Wahrscheinlichkeit, dass ihm auch noch eine zweite auffalle. Menschen werden von ihrer Umgebung beeinflusst - eine Maschine nicht. Franz Pfister, heute 31, und sechs Kollegen haben nun eine künstliche Intelligenz entwickelt, die Auffälligkeiten in medizinischen Daten feststellt. Sie alarmiert zum Beispiel nach einem CT sofort den Arzt, wenn ein Notfall vorliegt, auch markiert sie, welche Hirnregionen er sich genauer anschauen sollte. Gerade baut Pfisters Team am Prototypen ihres Projekts Deepc. Im März werden sie den zum ersten Mal testen, am Klinikum rechts der Isar";https://www.sueddeutsche.de/muenchen/heilkunde-maschinenblick-auf-medizinische-daten-1.4253909;sz.de;Ratz
03.12.2018;Was Künstliche Intelligenz in der Medizin kann;"Dr. Data in die Notaufnahme: Für Dietmar Frey ist das keine Science-Fiction. Der Neurochirurg an der Berliner Charité und sein Team tüfteln an Rechnern mit Künstlicher Intelligenz, die einer Behandlung von Patienten mit akutem Schlaganfall zugute kommen soll.

""Das ist mehr als eine Idee. Wir haben die Technik, einen Prototypen und erste Machine-Learning-Modelle"", sagt Frey. Wenn 2019 das Wissenschaftsjahr mit dem Schwerpunkt Künstliche Intelligenz beginnt, wird es in der Medizin um solche Fragen gehen wie: Was können Computer - und wo bleiben Ärzte unersetzlich?

Suche nach der richtigen Therapie

Rund 270.000 Bundesbürger trifft jedes Jahr ""der Schlag"". Dann rennt die Zeit. Wird das Gehirn nicht ausreichend mit Blut und Sauerstoff versorgt, stirbt Gewebe ab. Die Folgen können dramatisch sein, Sprachausfälle und Lähmungen drohen.

Ärztliche Leitlinien in Deutschland besagen, dass das betroffene Hirngewebe nach viereinhalb Stunden tot ist und Nebenwirkungen einer Therapie noch mehr Schaden anrichten können - Blutungen im Kopf zum Beispiel. Deshalb werde nach viereinhalb Stunden nicht mehr therapiert, sagt Frey. ""Das mag statistisch korrekt sein, für den individuellen Patienten ist das jedoch nicht immer die richtige Therapie.""

Er vermutet, dass Therapien in bestimmten Fällen auch nach viereinhalb Stunden noch Sinn machen - und in anderen vielleicht schon nach zwei Stunden nicht mehr. Aber wie weiß man, für wen was gilt?

Muster bei Schlaganfällen erkennen

Für Frey ist die Antwort klar: Eine Maschine könnte in Minutenschnelle tausende Vergleichsdatensätze zu Schlaganfällen durchsuchen. Sie abgleichen und Muster aufzeigen, die einem Arzt in der Rettungsstelle bei der Entscheidung helfen könnten. Ein Job für Dr. Data. Für KI in Datenbanken und Rechnern, die trainiert werden, nach programmierten Mustern zu fahnden.

Künstliche Intelligenz könnte Ärzte in ganz unterschiedlichen Bereichen unterstützen, sagt Felix Nensa, Radiologe und Informatiker an der Uniklinik Essen. Vor allem aber dort, wo der Mensch eine Fehlbesetzung sei: bei langweiligen und ermüdenden Tätigkeiten wie der Tumorvermessung oder auch beim Speichern und Scannen tausender Bild- und Textdateien.

Sinnvolles Programmieren ist eine Kunst, auch in der Medizin. ""Man braucht ausreichend große Trainingsdaten-Sets und muss Variabilität abbilden können"", sagt der Bioinformatiker Benedikt Brors am Deutschen Krebsforschungszentrum (DKFZ) in Heidelberg. Sei ein Datensatz zu klein, könne der Rechner zum Beispiel Muster bei sehr seltenen Tumorerkrankungen nicht erkennen.

Hilfe bei der Tumordiagnose

Das DKFZ entwickelt seit zehn Jahren ein KI-System, das anzeigen soll, ob Neuroblastome - sehr seltene Tumore bei Kindern - aggressiv sind oder eher harmlos. Davon hängt die Therapie ab. Doch es wird noch dauern mit der klinischen Erprobung. Die Anforderungen für eine Zertifizierung seien extrem hoch, sagt Brors.

Dietmar Freys Rechner an der Charité haben bereits über 1400 Schlaganfälle geladen. Patientenschicksale, anonymisiert und heruntergebrochen auf Formeln. Es geht um Angaben zu Alter, Geschlecht, Gewicht, Rauchgewohnheiten, Vorerkrankungen, genommenen Medikamenten und Laborwerten.

Freys Plan ist es, diesen Datenschatz abrufbereit in der Notaufnahme zu haben, wenn ein neuer Patient mit akutem Schlaganfall eingeliefert wird. So soll sich direkt prüfen lassen, ob es vergleichbare Ausprägungen eines Schlaganfalls gab, wie verfahren wurde und ob das half.

Schnelle Informationen

Die Ergebnisse soll der Computer binnen Minuten ausspucken - damit die behandelnden Ärzte die beste Therapie für den Patienten finden können, der gerade vor ihnen liegt. ""Ein Arzt hat heute weder die Zeit noch die Kapazitäten, in der Notaufnahme Aktenberge für Vergleiche zu wälzen. Und im Kopf haben und berechnen kann er das alles schon gar nicht.""

In Essen baut Mediziner Nensa Bilddatenbanken auf: CT- und Röntgenbilder von Lungenleiden samt klinischen Daten. ""Wir haben immer wieder neue Fälle"", sagt er. ""Dann gucken wir uns die neuen Bilder dazu an und denken manchmal: Puh, was ist das denn jetzt?"" Mit einer KI-Datenbank könnte man leicht prüfen, ob es schon mal einen ähnlichen Fall gab - und hätte direkt Zugang zu dessen Diagnose und Therapie.

""Solche KI ist für mich die erste Welle von Tools, die den Arzt entlastet"", resümiert er. ""In fünf bis zehn Jahren wird es mehr von diesen Tools geben. Der Beruf des Arztes wird sich verändern - mehr in Richtung Zusammenführen und Interpretieren von Daten"", vermutet Nensa.

Noch gibt es viele Grenzen bei KI. Eine Maschine könne bisher zum Beispiel regelbasiertes Wissen aus Leitlinien reproduzieren. ""Was sie noch nicht hat, ist Kreativität oder Intuition"", sagt Brors. Menschen hingegen könnten auch mal um die Ecke denken.";https://www.sueddeutsche.de/gesundheit/gesundheit-was-kuenstliche-intelligenz-in-der-medizin-kann-dpa.urn-newsml-dpa-com-20090101-181203-99-70704;sz.de;DPA
23.11.2018;Berührungspunkte;"Wenn man davon ausgeht, dass der Intelligenzquotient von Albert Einstein bei geschätzten 180 lag und der von Johann Wolfgang von Goethe bei 210, dann ist die Aussicht, dass der digitale Mensch mittels künstlicher Intelligenz (KI) in naher Zukunft über einen IQ von 2 000 verfügen wird, eher beunruhigend. Der Mann, der das prophezeit, heißt Ray Kurzweil, ein stets leicht schmunzelnder, gebürtiger New Yorker von 70 Jahren, geschätzter IQ 140, von Beruf Informatiker, aber in den USA vor allem wegen seiner kühnen Zukunftsprognosen berühmt.

Die Technologie, die aus Mensch und Maschine Superwesen machen wird, gibt es bisher nur in seiner Vorstellung. Derzeit bedeutet künstliche Intelligenz immer noch lediglich, dass ein Computerprogramm dazulernen und sich dann ohne menschliche Hilfe selbst optimieren und Entscheidungen fällen kann. KI kann damit vor allem große Datenmengen analysieren, mit denen sie dann beispielsweise Bilder identifiziert. Oder Menschen in einer Menge. Oder hochkomplexe Statistiken erstellen, deren Ergebnisse sie in Entscheidungen umwandelt, wie zum Beispiel Kaufempfehlungen für Onlinehändler oder wann ein selbstfahrendes Auto beschleunigt oder bremst.

Ray Kurzweil sagt allerdings, dass die Menschheit gerade an der Schwelle eines Evolutionssprungs steht, an dem sich Natur und Technologie zu einer Einheit verbinden: Das menschliche Gehirn wird einen digitalen Neocortex entwickeln, also einen neuen Teil der Großhirnrinde, der sich über eine Schnittstelle mit einer digitalen Wolke künstlicher Intelligenz verbindet, aus der sämtliche Daten und Informationen des Weltwissens abrufbar sind.

Das steht im deutlich euphorischen Widerspruch zur anderen Fraktion der KI-Debatte, die nicht weniger als den Untergang der Menschheit prophezeit. Zu der gehören immerhin der im März diesen Jahres verstorbene Physiker Stephen Hawking und der Superunternehmer Elon Musk.

Glaubt man dem Tesla-Gründer Musk (IQ 155) oder Hawking (IQ 160), werden künstliche Intelligenzen die Menschheit vernichten. Musk sagt, KI sei schlimmer als Atomwaffen. Deswegen baut er auch Raumschiffe, mit denen er Menschen auf den Mars umsiedeln möchte.

Ganz und gar nicht apokalyptisch sieht Ray Kurzweil die Zukunft. Bei seinem Vortrag an der amerikanischen Westküste neulich wirkte er sehr überzeugt davon, dass sich das menschliche Gehirn mit der KI-Technologie zu einer Einheit verbinden werde. Diese Entwicklung werde vom Moment der ""singularity"" eingeleitet werden. So bezeichnet er die Intelligenzexplosion, wenn KI das Denkvermögen und Bewusstsein des Menschen erreichen und dann schon bald übertreffen wird. Und weil das Publikum an diesem Nachmittag aus rund 50 geladenen Gästen aus dem zukunftsgläubigen Umfeld des Silicon Valley bestand, gab es im angenehm ausgeleuchteten Konferenzraum im Kongresszentrum von Vancouver auch keinen Widerspruch.

Der Hi-Tech-Korridor von Vancouver bis San Diego ist eine hermetische Welt, deren Bewohner selten aus den Instituts- und Bürokomplexen entlang der Westküste herauskommen. Und wenn sie hin und wieder mal bei einer der Hi-Tech-Konferenzen am ernährungswissenschaftlich wertvollen Büffet Kontakt mit der Außenwelt bekommen, sind auch das wohlkuratierte Begegnungen.

Die Gäste an diesem Nachmittag waren auch nicht nur gekommen, um einem Zukunftsvisionär zu lauschen. An der Westküste sind Propheten einer glorreichen Zukunft der künstlichen Intelligenz auch die Pfadfinder auf dem Weg zum nächsten Goldrausch. Seit drei Jahren hetzen Investoren und Risikokapitalgeber, Start-up-Gründer und Digitalkonzerne wie eine geschlossene Meute den Verheißungen der künstlichen Intelligenz hinterher. Was vor fünf Jahren noch die unzähligen Anwendungen des ""Web 2.0"" waren, die sozialen Netzwerke wie Facebook und Twitter oder die Vertriebsplattformen wie Uber und Airbnb, sind heute die künstlichen Intelligenzen.

Nach Schätzungen der Wirtschaftsprüfer von PricewaterhouseCoopers werden KI-Technologien bis 2030 das weltweite Bruttosozialprodukt um 14 Prozent steigern, das entspricht mehr als 15 Billionen Dollar und damit vier Bruttosozialprodukten der Bundesrepublik. Wenn ein prominenter Technologe wie Kurzweil verkündet, KI sei nicht nur der Beginn eines neuen Maschinenzeitalters, sondern gar ein evolutionärer Schritt in der Geschichte des Planeten, ist der Beifall garantiert.

Kurzweils Bild vom digitalen Neocortex ist brillant. Die Entwicklung jenes Teils der Großhirnrinde, der unter anderem Sprache, Wahrnehmung und räumliches Vorstellungsvermögen steuert, war das bisher wichtigste Ereignis in der Geschichte des irdischen Lebens und der Beginn des Weges von der Kröte zu Goethe und Einstein. Der Neocortex ermöglichte den Säugetieren Wahrnehmungsformen, Lern- und Erkenntnisprozesse, die aus Reflexen durchdachte Handlungen machten. Denkt man die digitale Entwicklung weiter, wird die Vereinigung von natürlicher mit künstlicher Intelligenz also das zweitwichtigste Ereignis der Evolution sein.

Geht es nach Ray Kurzweil, sogar das wichtigste. Er lehnt sich an diesem Nachmittag auf der Rednerbühne zufrieden in seinem Polsterstuhl zurück, als er das erläutert. ""In den 2030er-Jahren werden wir uns mit der intelligenten Technologie vereinen, die wir erschaffen"", sagt er im Ton eines Wissenschaftlers, der belegte Tatsache referiert. ""Vor zwei Millionen Jahren bekamen wir den Neocortex und haben ihn an die Spitze der Hierarchie unseres Hirns gesetzt. Ähnlich wie schon vor zwei Millionen Jahren, werden wir diesen zusätzlichen Neocortex an die Spitze der Hierarchie setzen."" Allerdings sei es dieses Mal kein einmaliger Vorgang. ""Hätten wir vor zwei Millionen Jahren das Volumen unserer Schädel immer weiter vergrößert, wäre irgendwann die Geburt nicht mehr möglich gewesen, weil der Schädel nicht mehr durch den Geburtskanal gepasst hätte.""

Doch die Kraft der Cloud sei nicht auf einen begrenzten Raum angewiesen. Derzeit verdoppele sie ihre Kraft jedes Jahr. ""Wir werden unendliche Ausdehnungsmöglichkeiten unseres Hirns haben. Und genauso, wie wir vor zwei Millionen Jahren neue Ausdrucksformen gefunden haben wie die Sprache und die Musik, werden wir neue Ausdrucksformen schaffen, die wir uns noch gar nicht vorstellen können.""
Youtube, Outlook, Facebook, Twitter, Google und Spotify sind auch künstliche Intelligenzen

Wie aufgeladen die Debatte geführt wird, zeigt das Beispiel Stephen Hawking. Der Physiker wurde falsch zitiert. Die Vernichtung der Menschheit durch KI hatte er nur als eines von vielen Szenarien aufgezählt. Das vollständige Zitat lautete: ""Erfolg bei der Erschaffung effektiver künstlicher Intelligenz könnte das größte Ereignis in der Geschichte unserer Zivilisation werden. Oder das schlimmste. Wir wissen es einfach nicht. Deswegen können wir auch nicht sagen, ob uns künstliche Intelligenz letztlich helfen wird, ob sie uns ignoriert, ob sie uns kaltstellt oder zerstört.""

Aber eine Hawking-Apokalypse passt nun mal in einen öffentlichen Diskurs, der vor allem von Angst getrieben ist. Für die es ja gute Gründe gibt. Die ersten Begegnungen der Menschheit mit den ersten künstlichen Intelligenzen in den vergangenen zehn, fünfzehn Jahren gingen jedenfalls nicht besonders gut aus. Denn in den Industrienationen ist künstliche Intelligenz längst fester Bestandteil des Alltags. Sie heißt nur nicht so.

KI im deutschen Alltag bedeutet im Jahr 2018 zum Beispiel: E-Mail-Programme wie Outlook, Suchmaschinen wie Google, soziale Netzwerke wie Facebook und Twitter, Entertainmentportale wie Youtube und Spotify, Shoppingseiten, Navigationssysteme, Sprachsteuerungen wie Siri und Alexa, Videospiele wie Fortnite. Das sind allesamt künstliche Intelligenzen, weil sie zum einen Entscheidungsprozesse automatisieren und sich zum anderen durch maschinelle Lernvorgänge ständig selbst verbessern. Als Nutzer bemerkt man das nicht, als Betreiber sehr wohl.

Fast alle diese Anwendungen haben derzeit einen schlechten Ruf. Es hilft auch nicht, dass die Extreme der Debatte religiöse Untertöne haben. Sowohl das Ende der Menschheit als auch der Erweckungsmoment der Singularity erinnern an die Offenbarung des Johannes, das letzte, prophetische Buch des Neuen Testaments, in dem die Apokalypse der Welt ein Ende bereitet und die Entrückung die wahrhaft Gläubigen errettet. Es ist also an der Zeit herauszufinden, was künstliche Intelligenz heute schon kann. Dann fällt die Abschätzung, was sie einmal können wird, leichter.

Auf der Suche nach einem exemplarischen Beispiel für den Stand der Dinge landet man erstaunlicherweise im Vatikan. ""In codice ratio"" heißt das KI-Projekt dort, ein lateinischer Begriff für Codesystem. Kein punkt null im Namen und keine Anspielung auf Popkultur. In codice ratio soll das Geheimarchiv des Vatikan erfassen. Das ist im großen Querbau am Cortile del Belvedere untergebracht, ein paar Hundert Meter vom Haupteingang der Porta Sant'Anna entfernt. Statt nach links auf den Petersplatz, biegt man nach rechts ab. Ganz so geheim, wie der offizielle Titel ""Archivum Secretum Apostolicum Vaticanum"" suggeriert, ist das Archiv nicht. Wissenschaftler und ausgesuchte Reisegruppen dürfen die düsteren Säle und fensterlosen Lagerräume durchaus besuchen, in denen sämtliche Dokumente, Akten und Korrespondenzen des Heiligen Stuhls seit dem achten Jahrhundert gelagert sind.

Das Problem der Archivierung dort ist, dass Dokumente vom achten Jahrhundert bis zur Erfindung und Verbreitung des Buchdrucks 700 Jahre später digital nicht erfassbar sind. Handschriften sind nicht maschinenlesbar. Und weil das Mittelalter nicht nur eine Ära der handschriftlichen Bürokratie war, sondern die Zeit, in der der Heilige Stuhl die einflussreichste Weltmacht war, fehlt der modernen Geschichtsschreibung der Einblick in ein großes Stück Weltgeschichte.
Buchstabe für Buchstabe muss die Software die Schrift aus dem 11. Jahrhundert erkennen

Das Labor des ""In codice ratio""-Projektes ist eine gute halbe Autostunde von den prächtigen Hallen des Vatikan entfernt im zweiten Stock eines Backsteinbaus auf dem Campus der Universita degli Studi Roma Tre untergebracht. Labor klingt dabei etwas zu elegant für das nüchterne Arbeitszimmer des Professors Paolo Merialdo, mit einem handelsüblichen PC auf dem Schreibtisch und einem Bücherregal, das etwas verwaist wirkt mit seinen unsortierten Lehrbüchern und Aktenordnern.

Merialdos Team versammelt sich um einen leeren Tisch, an dem sie ihre Konferenzen abhalten, was nicht so oft vorkommt, weil sie meist über Rechner kommunizieren. Die beiden Doktorandinnen Elena Nieddu und Donatella Firmanai arbeiten mit Merialdo an der Programmierung, dazu kommt Serena Ammirati, eine Paläografin, also eine Wissenschaftlerin, die ausgestorbene Schriften erforscht. Wenn die vier auf einen einreden, wenn sie beschreiben, wie sie ihren Rechnern Schritt für Schritt etwas Neues beibringen, merkt man erst, was für ein störrisches Biest so eine künstliche Intelligenz ist.

Die Aufgabe ist gewaltig. Nicht nur, weil das Geheimarchiv 83 Regalkilometer Akten, Dokumente und Bücher umfasst. Briefe von Michelangelo finden sich darin genauso wie die Bannbulle, mit der Papst Leo X. Martin Luther aufforderte, seine 95 Thesen zurückzunehmen, oder der Brief, in dem Heinrich VIII. den Papst Clemens VII. um die Annullierung seiner Ehe bat.

In die Tiefen des Archivs steigen sie allerdings nur selten. Für die erste Phase ihres Projektes haben sie sich ein paar Seiten aus den Akten des 11. Jahrhunderts ins Institut geholt. Was da drinsteht, ist nicht so wichtig. Nein, der wichtigste Grund, den Serena Ammirati mit großer Geduld erklärt, ist die schon sehr einheitliche Schrift des 11. Jahrhunderts, die den Forschern heute die Arbeit sehr viel leichter macht. Meist geht es in den Dokumenten um Geld. Der Papst war so etwas wie der Kredithai seiner Tage. Da gab es viel zu verhandeln.

Buchstabe für Buchstabe muss die Software nun erst einmal lernen, die Schrift zu erkennen, sie zu Wörtern und dann zu Sätzen zusammenzufügen. Wie das geht, zeigt Elena Nieddu mit ein paar Grafiken, die sie für Laien vorbereitet hat.

Die Software unterteilt jedes Wort in eine Reihe von Fenstern, die halbe und Drittelbuchstaben erfasst, die für die Software zu Puzzleteilen werden. Um zum Beispiel das lateinische Wort ""sententie"" (Maximen) zu erkennen, durchläuft die KI über fünfzehntausend Arbeitsschritte. Je mehr Beispiele man in so eine künstliche Intelligenz hineinfüttert, desto besser kann sie lernen. In codice ratio ist ein System aus sogenannten neuronalen Netzen, das mit statistischen Sprachmodellen arbeitet. Das heißt, die Software kann Schrift und Sprache erkennen, aber nicht verstehen.

Um den Datensatz rasch zu erweitern, arbeiten 600 italienische Gymnasiasten für das Team. Die sehen auf einer Webseite Buchstaben aus den mittelalterlichen Dokumenten. Als Menschen können sie die mit einem Blick entziffern. Sie bekommen also immer einen Buchstaben gezeigt, tragen ihn dann in moderner Schrift in ein Formular ein, das diese Information dem Datensatz hinzufügt. Eine Art Nachhilfe für künstliche Intelligenz.

Eine sehr mühsame Nachhilfe. Die italienischen Schüler haben für In codice ratio schon fünf Millionen Zeichen für die Forscher identifiziert. Das ist für eine künstliche Intelligenz nicht viel.

Nach den strengen Parametern der KI-Forschung im amerikanischen Westen wäre In codice ratio immer noch eine sogenannte ""narrow artificial intelligence"", eine schwache, weil sehr eng definierte KI, die eine oder nur wenige Aufgaben lösen kann, in diesem Falle eine Schrift entziffern.

Der große Traum der KI-Forschung aber ist die AGI, das ist die Abkürzung für ""Artificial General Intelligence"", also für künstliche allgemeine Intelligenz. Das wäre der Punkt, an dem eine KI ihre Fähigkeiten für eine so breite Palette von Problemen anwenden könnte wie der Mensch, der ja schon im Grundschulalter sein Hirn beispielsweise für Lesen, Schreiben, Rechnen, soziale Interaktionen wie Verhandlungen und emotionale Ausdrucksformen nutzen kann, um nur ein paar elementare Aufgabengebiete zu nennen.

Nach dem Erreichen der AGI käme die Singularity, das Maschinenbewusstsein und damit entweder die Beflügelung des Menschen zum Superwesen oder seine Vernichtung. Kann man dem allen ganz beruhigt entgegenblicken, wenn man den Forschern in Rom dabei zugesehen hat, wie sie ihrer künstlichen Intelligenz Strichlein für Strichlein das Lesen beibringen?
Gewiss ist, dass ein Computer nie die Intelligenz eines Menschen erlangen kann

Es gibt ja dann auch noch die Naturgesetze, die der Entwicklung Grenzen setzen.

Man kann sich kurz mal auf die Argumentation der sogenannten Transhumanisten einlassen. Zu denen gehören vor allem Kognitionswissenschaftler, Ingenieure, aber auch ein paar Philosophen, die finden, dass der Mensch an sich ein sehr unvollkommenes Wesen ist, der seine Vollkommenheit nur mit Technologie erlangen wird. Das Gehirn bezeichnen Transhumanisten zum Beispiel als ""wetware"", was eine Anspielung auf das Wort Software ist, denn ein menschliches Hirn ist ja erst einmal sehr nass und matschig, wenn man es aus seinem Betriebsgehäuse, also dem Schädel, entfernt.

Im Vergleich zu Computern und Supercomputern ist ein Hirn außerordentlich leistungsfähig. Eine Großhirnrinde verfügt zum Beispiel über 25 Milliarden Neuronen, die über die Synapsen um die 100 Billionen Verbindungen bilden können. Sämtliche Leitungen für die Übertragung der elektrischen Impulse, die in der Biologie Nerven genannt werden, ergäben im Hirn eines Zwanzigjährigen zusammengerechnet 160 000 Kilometer. Die Übertragungsgeschwindigkeit dieser Nervenleitungen liegt bei bis zu 460 Stundenkilometern.

Ein Versuch, eine solche Rechenleistung zu simulieren, fand vor acht Jahren in einem Forschungslabor von IBM statt. Mit enormem Aufwand brachten es die Wissenschaftler auf die Leistung eines Katzenhirns (allerdings ohne Software, die diese Leistung hätte nutzen können). Sie fanden aber auch heraus, dass Energieverbrauch zu den lästigen Naturgesetzen gehört, die KI ausbremsen. Ein Menschenhirn verbraucht zum Beispiel nur rund 20 Watt Energie, das käme in den USA auf eine Stromrechnung von weniger als 20 Dollar im Jahr. Ein Supercomputer, der ähnliches leisten könnte, würde dagegen auf eine Stromrechnung von einer Milliarde Dollar pro Jahr hinauslaufen.

Man kann die Gewissheit, dass Computer die menschliche Intelligenz nie erlangen werden, sogar noch mit der ganzen Wucht der Philosophie untermauern. Denn selbst wenn Rechner mithilfe der großen Datenmengen der ""Big Data""-Speichermöglichkeiten, den Methoden des Maschinenlernens und den enormen Rechnergeschwindigkeiten Denkprozesse imitieren können, die bisher den Menschen vorbehalten waren, selbst wenn sich KI und Robotik zu selbständig agierenden Maschinen vereinen, eines werden Rechner nie erlangen: Bewusstsein. Philosophen wie Kenichiro Mogi oder Thomas Nagel können das sehr überzeugend erklären.

Es mag zum Beispiel sein, dass Maschinen so etwas wie Bewusstsein simulieren können, wenn sie mittels Sensoren ihre Umwelt erfassen, analysieren und daraus Rückschlüsse ziehen, wie sie selbst in diesem Umfeld zu agieren haben. Selbstfahrende Autos und hoch entwickelte Roboter müssen eine solche Außenwahrnehmung produzieren, um zu funktionieren. Doch eines fehlt ihnen trotzdem, und das sind die Erfahrungswerte und Emotionen, also die Grundlagen des menschlichen Bewusstseins. KI wird nie mehr sein, als die Karaokemaschine menschlicher Intelligenz.

Es sind sowieso nicht KIs mit einem vierstelligen IQ und einem wie immer gearteten Bewusstsein, die der Menschheit Sorgen machen sollte. Es sind KIs mit dem eng gefassten Aufgabenbereich, die extreme Wirkungskraft erreichen, um diese eine Aufgabe zu erfüllen.

Die großen Web-Portale sind perfekte Beispiele für die eigentliche Gefahr der KI. Und genau die sind eben unsere ersten Begegnungen mit künstlicher Intelligenz, auch wenn das niemand so empfindet, weil künstliche Intelligenz nicht mehr als solche bezeichnet wird, sobald sie Teil des Alltags geworden ist.

Es ist eine extreme Konsequenz oder auch Gnadenlosigkeit, nach denen Rechenprogramme verfahren. Die wird im Silicon Valley gerne mal als Reinform der Objektivität verkauft. Weil ein Computer sich weder von Emotionen, noch vom psychischen Ballast eines menschlichen Lebens leiten lässt.

Was dabei herauskommt ist allerdings oft das Gegenteil. Die Algorithmen von Facebook und Twitter hatten zum Beispiel klare, durchaus gut gemeinte Aufgaben. Sie sollten so viele Menschen wie möglich miteinander vernetzen. Damit die Firmen damit auch Geld damit verdienen konnten, sollten sie nebenbei dafür sorgen, dass sie möglichst viel Zeit auf diesen Plattformen verbringen und dabei Werbung sehen.

Um ein Maximum an Nutzerzeit herauszuschlagen, förderten die künstlichen Intelligenzen dann all jene Emotionen, die Menschen am längsten am Bildschirm halten. Das sind Angst, Hass und Häme.
Soziale Medien heizen Gefühle wie Häme an, um ein Maximum an Nutzerzeit herauszuschlagen

Die Folgen waren allerdings weit schwerer, als man sich das während den euphorischen Aufbruchszeiten des World Wide Web hätte ausmalen können. Die Erosion des politischen Diskurses und des Wahrheitsbegriffes zum Beispiel. In Europa und den USA führte das zum Aufstieg der Populisten. In vielen südlichen Ländern brachte das Autokraten wie Brasiliens Staatspräsidenten Jair Bolsonaro an die Macht. In Ländern wie Myanmar, Indien und Mexiko peitschten Algorithmen Lynch-Meuten bis zum Mord und Massenmord. Weil künstliche Intelligenzen eben kein Bewusstsein haben, keine Kontexte erkennen und deswegen lediglich stumpf verstärken, was in Gesellschaften schon vorhanden ist.

Deswegen ist es gar nicht so wichtig, ob künstliche Intelligenz dem menschlichen Denken je nahekommen oder es sogar übertreffen wird. Was die Gesellschaft wirklich verändern wird, sind die noch ungeahnten Auswirkungen, wenn die Digitalisierung mit KI in ihre nächste Phase tritt. Der Schritt über diese nächste Schwelle wird gerade in Austin, Texas, vorbereitet. Der Designer Mark Rolston gehört zu dem Team, das höherer künstlicher Intelligenz den Weg zur Massenanwendung ebnet. Dafür arbeitet er gemeinsam mit der KI-Firma Cognitivescale des ehemaligen IBM Watson-Chefs Manoj Saxena am nächsten Wendepunkt der künstlichen Intelligenz.

Das Programm, das sie seit dem Frühjahr anbieten, heißt Cortex 5. Da ist er wieder, der Cortex. Ähnlich wie der Mantel des menschlichen Gehirns, soll Rolstons Programm die Sinneswahrnehmungen, Informationen und Funktionen einer künstlichen Intelligenz verbinden. Das eigentlich Revolutionäre ist jedoch das Baukastenprinzip, nach dem diese digitale Hirnrinde verschiedene Fähigkeiten einer künstlichen Intelligenz miteinander verbindet. ""Man muss sich das wie einen Legobaukasten vorstellen"", sagt er. ""Wir liefern die Grundbausteine, die jeder nach Belieben zusammensetzen kann.""

Es ist verblüffend simpel, wie man sich mithilfe des Programms eine künstliche Intelligenz bauen kann. Gemeinsam mit seinem Designer Matthew Santone führt er das vor. Zunächst ruft er die Benutzeroberfläche auf, eine elegante schwarze Fläche, an deren Rand die Steuerelemente sind. Über eine Liste, die ausklappt, sucht man sich die einzelnen ""Skills"" der KI aus, die Funktionsmodule. Die muss man in Zukunft nicht mehr eigens programmieren, sondern kann sie aus der Cloud abrufen. Santone nimmt ein Element, das der Nutzer über eine Kamera klassifizieren kann, ein Element, das Kleider erkennt und auswählt, sowie einen sogenannten sentiment analyzer, eine Art digitales Stimmungsbarometer. Es dauert nur wenige Minuten, bis diese Einheiten auf der Oberfläche durch grüne Linien verbunden sind. Das sind die sogenannten Synapsen, Verbindungselemente, nicht unähnlich den Synapsen des Gehirns, die die Elemente so miteinander verbinden, dass sie in jeder beliebigen Kombination miteinander arbeiten können. Was dabei herauskommt, ist die simple Version einer KI, die beispielsweise eine Verkaufswebseite nutzen könnte, um Kleidung nach dem Geschmack ihrer Kunden herauszusuchen.

Seit die Plattform in Betrieb ist, können Nutzer nicht nur die ""skills"" auswählen, die Cortex 5 anbieten will, sondern auch eigene solche KI-Programmelemente hochladen, die dann wiederum andere Nutzer gratis oder gegen Gebühr anwenden können. So wird eine riesige Bibliothek solcher Skills entstehen. Wie ernst sie es meinen, zeigt die Liste der Systeme, die über Cortex 5 laufen. Es sind die künstlichen Intelligenzen von IBM, Microsoft und der Stanford University. Große Firmen wie Versicherungen, medizinische Betriebe und Banken sollen die ersten Kunden sein.

Das eigentliche Ziel klingt simpel und mächtig. ""Mit unserer Plattform"", sagt Cortex 5-Designer Rolston, ""wird ein Teenager im Keller seiner Eltern eine künstliche Intelligenz konstruieren können. Wir werden so etwas wie das Musikprogramm Garage Band für die Welt der künstlichen Intelligenz sein."" Das wäre die Demokratisierung einer Technologie, die im Winter 2018 noch großen Digitalkonzernen und Forschungslaboren vorbehalten ist. Was das für die Zukunft der Digitalisierung bedeutet, kann man noch nicht abschätzen.
In München wird an einer KI gearbeitet, die erkennt, wie viele Menschen in einem Raum sind

Doch auch wenn man die Science-Fiction-Fantasien aus der KI-Debatte guten Gewissens ausblenden kann. Man sollte sogar gleich mal mit den unzähligen KI-Filmbildern im kollektiven Unterbewusstsein beginnen, mit dem Bordcomputer Hal und den Blade Runnern, mit dem Terminator und der Matrix. Die Wahrscheinlichkeit, dass künstliche Intelligenz der Schlüssel zum Übermenschen mit vierstelligem IQ sein könnte, ist nun mal genauso unrealistisch wie die Angst vor der KI-Apokalypse und die Pläne für die Rettung der Menschheit auf den Mars. Die Auswirkungen der nächsten Phase der Digitalisierung zu unterschätzen wäre allerdings ähnlich verantwortungslos, wie den Klimawandel zu ignorieren. Und es wird ähnlich schwierig sein, die Öffentlichkeit für die Chancen und Gefahren zu sensibilisieren, denn KI durchdringt sämtliche Aspekte des Lebens und des Alltags im Gletschertempo. Man kann die Veränderungen messen, spüren tut sie aber noch niemand. Es ist während überhitzter Debatten immer hilfreich, sich mit einem vorsichtigen Optimisten zu treffen. Mit John Cohn zum Beispiel. Der arbeitet bei IBM als ""chief scientist"" für das Watson IOT (Internet of Things) Center in München. John Cohn gibt sich gerne als verrückter Professor mit zauseligem weißem Bart, wirrem Haarkranz, euphorischem Augenleuchten und dem inoffiziellen Titel als ""chief agitator"". Wenn man im Norden von München mit dem Glasfahrstuhl hoch über die Stadt geschossen ist, wenn man am Eingang dem niedlichen Roboter Pepper die Hand geschüttelt hat, an den Ingenieuren und Sachbearbeitern vorbeigegangen ist, die klischeegerecht fast alle Männer in sandfarbenen Hosen und hellblauen Hemden sind, trifft man dann doch auf einen sehr nachdenklichen Intellektuellen.

Cohn bremst auch gleich. Watson ist zwar jene künstliche Intelligenz, die im Februar 2011 erstmals Schlagzeilen machte, als sie bei der amerikanischen Fernseh-Quizshow ""Jeopardy"" zwei menschliche Gegner besiegte. Im Juni 2018 verwies Watson dann den israelischen Debattiermeister Dan Zafrir in die Schranken. Im selben Monat wurde auch eine Watson-KI in einer medizinballgroßen, weißen Kugel namens Cimon ins All geschossen, um dem deutschen Astronauten Alexander Gerst bei seiner Arbeit auf der internationalen Raumstation ISS zu helfen.

John Cohn arbeitet aber gerade gar nicht an einer Weltsensation, sondern an einer künstlichen Intelligenz, die erkennen kann, wie viele Menschen sich gerade in einem Raum oder in einem Gebäude befinden, ohne die Individuen zu identifizieren. Was wichtig ist, wenn man ein Gebäude digital managen und gleichzeitig die Privatsphäre seiner Bewohner oder Besucher wahren möchte.

Wollte man John Cohn auf einer Skala von Elon Musk (Apokalypse) bis Ray Kurzweil (Erweckungsmoment) verorten, fände man ihn im oberen Drittel der Kurzweil-Achse. ""Ich glaube durchaus, dass wir Intelligenzen schaffen werden, die in Kombination mit menschlicher Intelligenz größer sind, als die unsere"", sagt er. ""Aber ich bin auch Optimist. Wenn wir frühzeitig über etwaige Probleme nachdenken, die entsprechenden Regularien finden und sie auch umsetzen, können wir auch verhindern, dass wir eine schädliche Superintelligenz schaffen.""
Die Digitalisierung hat Kulturen ausgehöhlt. Aber aus Fehlern kann die Gesellschaft lernen

Die Debatte über KI verfolgt er schon lange. Es ist aber vor allem die Praxis der KI-Entwicklung, die ihn so optimistisch macht. ""Alle, die sich mit KI beschäftigen, beginnen nun, dieselben Grundstrukturen zu verwenden. So kann man Sicherheitsmechanismen einbauen, auf die jede weitere KI aufbauen wird. Man wird nie verhindern können, dass destruktive Kräfte solche Sicherheitsmechanismen umgehen. Aber wenn man darüber nachdenkt, dass derzeit fast alle praktischen KIs als eine Art kombiniertes Open-Source-Projekt entstehen, ist das wirklich phänomenal. Ich habe so etwas noch nie erlebt. Weil alle das Allgemeinwohl in solchen Sicherheitsmechanismen erkennen. Und weil es für den hypothetischen Fall, dass es zu so etwas wie einer wild gewordenen KI kommt, die ihre Aufgabe um jeden Preis erfüllen will, in der Infrastruktur auch einen Wachhund gibt, der ihn wieder einfängt.""

Was bleibt im Winter 2018? Die Hoffnung, dass die Debatte über künstliche Intelligenz an einem Punkt beginnt, an dem Grundlagen geschaffen werden. Wenn höher entwickelte KIs in diesen Monaten aus den Laboren der Forschungsinstitute und Konzerne in den Alltag einfließen, schreiben die Entwickler und Ingenieure so etwas wie die DNS der zukünftigen digitalen Gesellschaft. Genügend Ansätze gibt es. Die wichtigsten Forscher, Wissenschaftler und Unternehmer dieser Welt trafen sich zum Beispiel im vergangenen Jahr im kalifornischen Asilomar, um einen Katalog ethischer Richtlinien zu verfassen. Diese ""Asilomar Guidelines for AI"", diese Richtlinien für KI, sind nun so etwas wie ein philosophischer Quellcode für eine Technologie, die selbst die, die sie erschaffen haben, noch gar nicht einschätzen können. Erste Konzerne ziehen nach. Die deutsche Telekom hat ethische Richtlinien veröffentlicht. Amazon, Facebook, Google, IBM und Microsoft haben eine ""Partnership on AI"" gegründet, der inzwischen gut siebzig Organisationen aus aller Welt angehören.

Die erste Phase der Digitalisierung mit ihren niedrigschwelligen KIs hat die Menschheit kalt erwischt. Der gegenwärtige ""Teclash"", die wütenden Reaktionen gegen digitale Technologie, sind eine berechtigte Folge. Die Digitalisierung hat ganze Industrien zerstört, Kulturen ausgehöhlt, den öffentlichen Diskurs zersetzt und Millionen zu Mediensüchtigen gemacht. Das sind Fehler, aus denen die digitale Gesellschaft an der Schwelle zum nächsten Technologieschub lernen kann.

Sieht so aus, als täte sie genau das.";https://www.sueddeutsche.de/kultur/kuenstliche-intelligenz-beruehrungspunkte-1.4223822;sz.de;Andrian Kreye
07.11.2018;Die Zukunft der Arbeitswelt;"Der Wandel, den die vierte industrielle Revolution mit sich bringt, macht sich überall bemerkbar. Automatisierung, Digitalisierung, Globalisierung, Künstliche Intelligenz, Virtuelle Realität, Roboter - diese Begriffe sind in aller Munde. Diese Phänomene tragen zudem zu zahlreichen Veränderungen am Arbeitsplatz bei. Gerade diesem Thema widmen sich nun die 18. Münchner Wissenschaftstage: ""Arbeitswelten - Ideen für eine bessere Zukunft"" ist diesmal die viertägige Veranstaltung überschrieben, die aufgrund von Finanzierungsschwierigkeiten heuer wohl zum letzten Mal stattfindet. Der Eintritt zu diesem Austauschforum über Wissenschaft und Technik ist frei.

Wie beeinflussen Digitalisierung und Globalisierung die Arbeit? Gehen Arbeitsplätze verloren? Entstehen neue? In 28 Vorträgen, 20 Workshops und vier Themenabenden soll es vom 10. bis 13. November in der Alten Kongresshalle um diese Komplexe gehen. Zudem stellen Experten auch konkrete Fragen über neue An- und Herausforderungen in Berufsfeldern wie Tourismus, Pflegearbeit, Personalmanagement, Lehre, Wissenschaft und Forschung. Im Programm enthalten sind Diskussionen über die Arbeit in Zusammenhang mit Nachhaltigkeit und Ethik und über Strategien im Umgang mit Stress, Burnout und Depressionen. Auch Themen wie Work-Life-Balance, Ernährung und Gender sind Teil der Münchner Wissenschaftstage.

Unter dem Motto ""Wissen für alle"" laden Wissenschaftler und Experten an 24 Marktständen zum Austausch in der Alten Kongresshalle ein. Man kann dort unter anderem etwas über moderne Arbeitsweisen, Schutz vor schädigender Strahlung im Beruf und Vorteile von Big Data in der Medizin erfahren. Und mit Hilfe eines Hockers lernt man sogar, gesund auf seinem Arbeitsplatz zu sitzen. Für Kinder und Schüler werden ein Kinderprogramm, Science Slam und Workshops angeboten. In den Kinder-Labors und der Zukunftswerkstatt werden am Beispiel verschiedener Berufe sowie realer technischer Geräte und Pläne für Roboter und Maschinen die Veränderungen der Arbeitswelt erklärt. Bereit zum Wandel also.";https://www.sueddeutsche.de/muenchen/wissenschaftstage-die-zukunft-der-arbeitswelt-1.4201259;sz.de;Anna Getmanova
15.10.2018;Wie eine Blitzkarriere bei den Grünen funktioniert;"Die Nacht war lang für Benjamin Adjei. Am Sonntag feierte er mit Hunderten grünen Parteifreunden den historischen Sieg bei der bayerischen Landtagswahl. Erst weit nach Mitternacht konnte der 28-Jährige auch für sich persönlich jubeln. Der Nachwuchspolitiker gewann das fünfte Direktmandat für die Grünen im Münchner Wahlkreis Moosach, mit 32 Stimmen Vorsprung vor der bisherigen CSU-Landtagsabgeordneten Mechthilde Wittmann. Eine Sensation, denn selbst auf der Grünen-Wahlparty wussten viele Parteifreunde nicht, wer der Neue ist.

Selbst enge Parteifreunde hatten Adjei kaum zugetraut, dass er nicht nur über die Liste, sondern sogar direkt in den bayerischen Landtag einziehen würde. ""Mit einem Direktmandat habe ich auch nicht gerechnet"", sagt Adjei. Dass er aber trotz Listenplatz 38 eine Chance haben könnte, davon war er einigermaßen überzeugt. ""Die Stimmung der Leute an den Infoständen ist in den letzten Wochen ziemlich steil nach oben gegangen"", sagt der erfolgreiche Wahlkämpfer. ""Da war schon die Hoffnung, dass es für mich über die Liste klappen würde.""
""Lieber machen, als meckern""

Adjei, gebürtiger Tegernseer, verbrachte seine Kindheit in den bayerischen Voralpen. Er hatte sich seit seinem Eintritt bei den Grünen vor sieben Jahren konsequent in der Politik von der Basis nach oben gearbeitet. Schon 2012 war er im Vorstand des Grünen-Ortsverbands Taufkirchen bei München, wohin er zum Studium der Finanz- und Versicherungsmathematik an der Hochschule München zog. Es war auch kein Zufall, dass Adjei 2014 ein Praktikum im bayerischen Landtag beim Fraktionschef der Landtagsgrünen Ludwig Hartmann machte: ""Ich habe einfach Ludwig gefragt, und er hat gesagt, kein Problem."" Vom Eintritt in eine Partei bis zum Landtagsabgeordneten in sieben Jahren, das nennt man wohl eine Blitzkarriere. Bekannte bezeichnen ihn als aufgeschlossen, aber auch als einen, der sich durchsetzen kann. Adjei selbst erinnert sich, dass er als Jugendlicher ""immer viel gemeckert"" habe, er engagierte sich schon früh für die Energiewende und schimpfte gegen die Atomkraft. Irgendwann wollte er dann aber auch ""etwas machen, statt zu meckern"".

Ein Leitspruch, der von der Grünen Spitzenpolitikerin Katharina Schulze stammen könnte. Mittlerweile hat Adjei sich bei den bayerischen Grünen einen Namen als Verkehrsexperte gemacht. In einer Kampfabstimmung erreichte er nach eigenen Angaben sogar, dass ein kostenloser öffentlicher Personennahverkehr für junge Menschen als Ziel der bayerischen Grünen festgeschrieben wurde. Adjei gilt nicht als Parteiideologe, sondern eher als pragmatischer Politiker. Er ist ein typisch bayerischer Grüner, der nicht unbedingt einem politischen Lager angehört. Adjei isst auch gerne mal Fleisch, wobei er darauf achtet, woher die Ware kommt und wie die Tiere gehalten wurden. ""Dann schmeckt's ja auch viel besser"", sagt er. Seine Mutter ist Deutsche, der Vater kam einst von Ghana nach Deutschland und hat die deutsche Staatsbürgerschaft. Beruflich ist Adjei IT-Spezialist, für ein Münchner Unternehmen betreut er Projekte im Bereich künstliche Intelligenz und Data Science.

Adjei ist Sport wichtig. Er spielte lange Jahre im Fußballverein, bouldert gerne und arbeitet neben seinem Beruf in einem Fitnessstudio. Das Training an Hanteln und Sportgeräten wird wohl in den kommenden fünf Jahren ein wenig kürzer kommen, und wie es mit seinem eigentlichen Beruf weitergeht, weiß er auch noch nicht so genau. An diesem Montag bekam Adjei einen Vorgeschmack auf seine neue Arbeit, am Nachmittag berieten die Grünen im Landesausschuss über das weitere Vorgehen der Fraktion nach dem Wahlergebnis. Adjei ist dabei pragmatisch. Ob in der Regierung oder in der Opposition - ""wichtig ist, was am Schluss herauskommt"".";https://www.sueddeutsche.de/bayern/landtagswahl-wie-eine-blitzkarriere-bei-den-gruenen-funktioniert-1.4170514;sz.de;Thomas Anlauf
12.10.2018;Keine Panik, es ist nur künstliche Intelligenz;"Wer die Intelligenz eines Kleinkinds mit der einer künstlichen Intelligenz (KI) vergleicht, tut dem Kleinkind unrecht. Denn um ihm beizubringen, was eine Katze ist, muss man ihm zwei oder drei Katzen zeigen und ""Katze"" sagen. Für denselben Lerneffekt braucht selbst die klügste aktuelle KI tausende oder sogar zehntausende Katzenbilder. Ob mit den derzeit existierenden Methoden eine deutliche Verbesserung dieser Leistung möglich ist, ist fraglich.

Die US-Armee will zwei Milliarden Dollar in neue künstliche Intelligenzen investieren. Auch dass die Bundesregierung Deutschland zum ""führenden KI-Standort"" machen will, hat wohl mit den Erwartungen zu tun, die der Begriff weckt.

KI, das klingt nach Hal9000, Wall-E oder C3PO, nicht nach einem Computerprogramm, das eine Million Datenpunkte vergleicht und mit bloßer Stochastik zu einem Ergebnis kommt. Trotzdem verunsichert die Technologie viele Bürger, als wäre sie ein Monster. Eine Umfrage des Instituts YouGov zeigte gerade, dass nur etwa 15 Prozent der Deutschen glauben, dass der Nutzen der KI die Risiken überwiegt. 26 Prozent sehen mehr Risiken als Nutzen, und 45 Prozent glauben, dass sich Risiken und Chancen die Waage halten.
""Im Moment wissen alle, dass jeder 'KI' braucht, und zwar am besten sofort""

Während sich die einen die Lösung aller Probleme erhoffen, fürchten andere die Macht ""intelligenter"" Computer. Der umtriebige Tesla-Chef Elon Musk warnt immer wieder vor einer überlegenen KI. Sie könne ein ""unsterblicher Diktator"" werden. Auch von der Angst vor einem realen Skynet ist immer wieder zu lesen: In der Terminator-Filmreihe ist Skynet das Computernetzwerk, das die Rebellion gegen die Menschheit startet. Vermutlich sind allerdings sowohl die Hoffnungen, als auch die Ängste übertrieben. ""Überflüssig und irreführend"" nennt Florian Gallwitz, Informatikprofessor an der Technischen Hochschule Nürnberg, den Begriff KI. ""Niemand kann überzeugend definieren, was 'KI' überhaupt sein soll, aber im Moment wissen alle, dass jeder 'KI' braucht, und zwar am besten sofort."" Die Werbetexte unterschiedlichster Tech-Start-ups geben ihm recht.

Tatsächlich habe es in den vergangenen Jahren große Fortschritte besonders im Bereich maschinellen Lernens gegeben, die faszinierende Anwendungen möglich machten, sagt Gallwitz. Mit ""Denken"", ""Schlussfolgern"", ""eigenem Willen"" oder gar einem ""Bewusstsein"" habe das jedoch nichts zu tun.

Die Schwammigkeit des Begriffs stört auch den Informatiker Filip Pi?kniewski, der über KI bloggt. Er hat in Warschau promoviert und arbeitet für das Start-up Accel Robotics, das an KI-gestützter Bilderkennung arbeitet. ""Wenn wir über Triebwerke reden würden wie über KI, würde jeder Raketenstart dazu führen, dass eine weitreichende Debatte über intergalaktisches Reisen mit Warpantrieb geführt wird. Das gehört ins Reich der Fantasie"", sagt Pi?kniewski.
Wie soll eine KI mit medizinischem Forschungsstand mithalten?

Einig sind sich Gallwitz und Pi?kniewski in einem Punkt: Von selbstfahrenden Autos bis zu neuen diagnostischen Methoden in der Medizin könnten auch vergleichsweise ""dumme"" KIs revolutionäre Anwendungen möglich machen. Optimisten erhoffen sich zum Beispiel schnelle und sichere Methoden, Krebs im Frühstadium zu erkennen und neue Medikamente zu entwickeln. Kritiker weisen darauf hin, dass erste Versuche mit dem Watson-Programm von IBM wenig vielversprechend waren. Eine KI, die Vorschläge zur Krebstherapie machen sollte, musste von echten Ärzten unterstützt werden und hielte mit dem Forschungsstand der Krebsforschung nicht Schritt.

Außerdem hat medizinische KI dieselben Probleme wie KI in anderen Bereichen. Vor allem data bias - also Vorurteile in den Daten - könnte über Leben und Tod entscheiden. Wenn der Robo-Arzt nur mit Bildern von weißen Menschen trainiert worden ist, erkennt er Hautkrebs bei dunkelhäutigen Menschen schlechter. Ähnlich verhält es sich mit Algorithmen, die mit Bildern oder Daten von Männern trainiert worden sind und dann auf Frauen angewendet werden. Ärzte, die sich auf die Technik verlassen, könnten falsche Diagnosen stellen.

Die Professorin, Futuristin und Bestsellerautorin Amy Webb sieht dennoch großes Potenzial in KI. Erfolge hingen aber davon ab, ob das Problem des data bias gelöst werde. ""KI ist nicht nur ein Trend oder ein Buzzword - es ist eine neue Ära der Informatik"", sagt sie. ""Wir befinden uns mitten in einer gewaltigen Transformation, vergleichbar mit der Industriellen Revolution.""

Weil der Hype alles dominiere, würden aber zu wenige die eigentlich einfachen Fragen stellen: Was passiert, wenn wir Macht und Einfluss an eine Maschine abgeben, die nur von wenigen programmiert und verstanden wird? Und was, wenn die Entscheidungen der künstlichen Intelligenz die Vorurteile noch verstärken, die menschliche Intelligenzen haben? Die Frage der Verantwortlichkeit müsse geklärt werden.
Das Problem: Die Datengläubigkeit der Menschen

Diese Probleme existieren auch ohne KI. Besonders in den USA nutzen Banken und Versicherungen alle verfügbaren Informationen, um Kreditwürdigkeit und Lebenswandel möglicher Kunden zu überprüfen. Selbst wenn die Informationen solche Schlüsse gar nicht erlauben. Wer in einer Nachbarschaft mit vielen säumigen Schuldnern wohnt, bekommt deshalb unter Umständen keinen Kredit. Auch, wenn er oder sie selbst bislang alle Schulden pünktlich zurückgezahlt hat. Dieses Verfahren ist auch in Deutschland bekannt, auch wenn zumindest die Schufa angibt, es nur in Ausnahmefällen anzuwenden.

Die Datengläubigkeit hinter dem AI-Trend ärgert Florian Gallwitz. ""Tatsächlich ist es leider meist unmöglich, aus Daten sinnvolles Wissen wie etwa Ursache-Wirkungs-Beziehungen abzuleiten, so groß die Datenmenge auch sein mag"", sagt er. Maschinen seien zwar gut darin, Korrelationen zu finden. Um Kausalitäten zu erkennen, müsse der Mensch aber weiterhin aktiv eingreifen, etwa indem er die Ergebnisse der KI mit einer Kontrollgruppe oder einem anderen Forschungsdesign ergänzt.

Das größte Missverständnis beim Thema KI sei deshalb, die Forschung daran mit der Entwicklung einer Superintelligenz zu verwechseln, sagt auch Informatikprofessor Barry O'Sullivan, Präsident der Interessensgemeinschaft European Association for Artificial Intelligence (EurAI). Es gehe darum, dass Computer bestimmte Aufgaben besser als Menschen bewältigen sollen. Diese Aufgaben seien aber sehr spezifisch.

Für das Beispiel der KI zur Bilderkennung heißt das: Ist es mit genügend Abbildungen von Katzen trainiert worden, kann das Programm irgendwann eine Katze mit großer Sicherheit erkennen. Ob das Tier glücklich oder traurig aussieht oder welches Verhalten von einer Katze vernünftigerweise zu erwarten ist, weiß das Programm nicht.
Heutige KIs sind auf ein ""enges"" Anwendungsgebiet beschränkt

Die Alpha-Go-KI des Google-Projektes Deepmind kann das Brettspiel Go besser als jeder Mensch spielen, dafür kann sie aber ein belegtes Brot nicht von einem Vulkanausbruch unterscheiden. Beide Programme sind ""schwache"" oder ""enge"" künstliche Intelligenzen, weil sie zwar über beeindruckende Fähigkeiten verfügen (für eine Maschine), diese sich jedoch auf ein sehr spezifisches, ""enges"" Anwendungsgebiet beschränken. Eben: Ein einziges, ganz bestimmtes Brettspiel spielen. Oder Katzen erkennen (Hunde aber dann wieder nicht).

Eine künstliche Intelligenz, die zahlreiche ""intelligente"" Funktionen vereint und sich sogar neue Informationen selbstständig erschließen kann, wäre dagegen eine starke KI. Auch wenn es Fortschritte gegeben hat, Computer selbstständig lernen zu lassen - von einer starken KI sind wir noch weit entfernt. ""Die Furcht vor einer künstlichen Superintelligenz hat als Gedankenexperiment oder philosophische Diskussion durchaus ihren Reiz"", sagt O'Sullivan. Aber KI könne Intelligenz nur simulieren. Deshalb seien die Ängste unbegründet.

Langsam setzt sich auch in der Fachwelt die Erkenntnis durch, dass die Entwicklungen wohl nicht so bahnbrechend werden wie erhofft. Experten wie Filip Pi?kniewski sehen bereits einen neuen ""KI-Winter"" auf uns zukommen. Der Begriff bezeichnet eine Phase, in der die Erwartungen an die KI, die Investitionen und das öffentliche Interesse zurückgehen. Das ist vergleichbar mit einer wirtschaftlichen Depression innerhalb der Forschungsdisziplin. Wenn es um KI ging, wechselten sich Phasen des Hypes und der Ernüchterung ab - die theoretischen Grundlagen der heutigen Technologien gibt es seit den 1960er Jahren. Dank gesteigerter Rechenleistung können sie heute umgesetzt werden, doch die Methoden stoßen an neue Grenzen, weit entfernt davon, Superintelligenzen zu werden. Die unklare Definition des Begriffs ""KI"" könne dazu führen, dass das Feld für einen Hype-Kreislauf besonders anfällig ist, sagt Pi?kniewski.

Der amerikanische Futurist Roy Amara hat eine Faustregel formuliert, die helfen soll, neue Technologien einzuschätzen. Sie wurde als ""Amaras Gesetz"" bekannt und besagt, dass Menschen dazu neigen, die kurzfristigen Auswirkungen von Technologie zu überschätzen, die langfristigen Auswirkungen aber zu unterschätzen. Gut möglich, dass das auch auf KI zutrifft. Ihre Methoden werden vielleicht irgendwann zu bahnbrechenden neuen Möglichkeiten für die Menschheit führen. Und in Gebieten zum Einsatz kommen, an die heute noch niemand denkt. Bis auf Weiteres bleiben Hal und C3PO aber, was sie auch bisher schon waren: Science-Fiction.";https://www.sueddeutsche.de/digital/ki-debatte-keine-panik-es-ist-nur-kuenstliche-intelligenz-1.4165614;sz.de;Christian Simon
21.09.2018;Beam me up, Scotty;"Wie mobil sind Sie denn? Tja, wenn man das immer wüsste. Ob Stop oder Go hängt ja nicht nur von einem selbst ab und ist zudem meist völlig unberechenbar. So versprechen die Politiker im Landtagswahlkampf allesamt, die Leute im Ballungsraum München wieder flott zu bekommen. Mit dem Fahrrad, mit mehr S-Bahnen oder gar mit einem Autobahn-Südring. Alles nichts Neues. Dann gibt es da noch die Idee von Ilse Ertl, Kandidatin der Freien Wähler, die vorschlägt, die Bevölkerung einfach nach Wunsiedel umzusiedeln, weil es dort nicht nur mehr Wohnungen, sondern auch weniger Verkehr gibt. Nichts gegen Oberfranken: Die meisten finden das trotzdem nicht so toll. Also gilt es das ganz große Rad zu drehen. Und das heißt nicht 365-Euro-Ticket. Denn bis das mal kommt, braucht man im Landkreis München gar keine S-Bahnen mehr. Und Straßen schon gar nicht.

Diese Woche hat sich der Kreisausschuss mal erklären lassen, was in den vom Landkreis mitfinanzierten fünf Technologie- und Gründungszentren an künstlicher Intelligenz, Automotive und Big Data bereits in der Pipeline ist. Klar, selbstfahrende Autos würden auch im Stau stehen und wenn jeder Fußballfan mit dem Flugtaxi zum Heimspiel des FC Bayern anreist, kann es über dem Luftraum der Fröttmaninger Arena auch eng werden. Aber vermutlich sind die Daniel Düsentriebs des Landkreises in den Tüftlerwerkstätten der Zukunft schon viel weiter, als wir denken. Diese Woche etwa wurden auf dem Garchinger Forschungscampus Prototypen von Weltraumaufzügen präsentiert. Nun gut, ein solcher ""Climber"", wie die Konstrukteure ihre Transporter in die Vertikale nennen, eignen sich kaum dazu, morgens schnell zur Arbeit zu kommen. Aber sie vermitteln das Gefühl, mitten in Oberbayern irgendwo in der Zukunft angekommen zu sein.

Diesen Eindruck werden bald auch Schüler der Ismaninger Realschule haben, wenn demnächst neben ihnen ein Avatar im Klassenzimmer Platz nimmt. Die kleinen Roboter sollen den Unterricht für diejenigen verfolgen, die krank zu Hause sind und so wenigstens virtuell dabei sein können. Ob dann irgendwann nur noch Avatare dem großen Roboter an der Tafel zuhören, während die echten Schüler zu Hause erst einmal ausschlafen und der Lehrer sein Leben ohne morgendlichen Stau genießt? Wer weiß!

Klar: Das klingt alles verdammt nach Science Fiction. Völlig utopisch. Aber dachten das die Zuschauer der Fernsehserie ""Star Trek"" in den Sechzigerjahren nicht auch? Touchscreens waren da zu sehen, Mobiltelefone und Speichersticks. Es gab Funk-Headsets, einen Universalübersetzer und Captain Kirk trug sogar eine Smartwatch. Die Filmemacher hatten sich damals von Wissenschaftlern beraten lassen und lagen dadurch bei der Technik der Zukunft richtig gut. Nur auf eines warten wir noch aus unseren Forschungszentren: Dass wir morgens nach dem Frühstück einfach nur sagen müssen: ""Beam me up, Scotty.""";https://www.sueddeutsche.de/muenchen/landkreismuenchen/kreis-und-quer-beam-me-up-scotty-1.4140266;sz.de;Iris Hilberth
06.07.2018;Erst Geschirr spülen, dann rebellieren;"Der Moment, in dem Technik sich ihrer selbst bewusst wird, liegt noch im Ungefähren. Futurologen wie Science-Fiction-Autoren mutmaßen, irgendwann in den 2040-er Jahren sei die Zeit reif für die sogenannte Singularität - nicht ohne darauf zu verweisen, dass dies ein wunderbarer Moment sein wird. Dass sich dadurch die gesamte Gesellschaft verändern müsste, wird oft ausgeblendet. Denn unsere Vorstellungen von Ethik, Moral und Gesetz müssten sich anpassen. Technik wäre nicht mehr etwas, das man benutzt, sondern eine Entität, mit der man verhandelt.

In der Welt des Videospiels ""Detroit: Become Human"" für die Playstation 4 ist es schon so weit. Hier, im Jahr 2038, dienen Androiden den Menschen - egal, ob als Assistent, Nanny oder Sexbot. Hergestellt werden diese menschenähnlichen Roboter von ""CyberLife"", ""dem ersten Billionen-Dollar-Unternehmen der Welt"". Sie könnten glatt vom heutigen Apple entworfen worden sein. Passt gut, immerhin soll der kalifornische Konzern noch in diesem Jahr die Billionen-Marke durchbrechen.

Wie der Titel verrät, spielt die Handlung in Detroit, in jener Stadt, die durch die Mobil-Revolution groß und dann von der Zukunft abgehängt wurde. Die Designer des virtuellen Sets haben für die City eine Zukunft entworfen, in der man unser Heute schmecken kann. Die Werbemonitore, die schwirrenden Mini-Drohnen, das omnipräsente Neon, das sich in den Pfützen spiegelt, sind Requisiten der Gegenwart.
Die Rebellion der Roboter ist ein Science-Fiction-Klischee

Tatsächlich scheint der Titel auf den aktuellen Diskurs hin entwickelt. Die Verdrängungsängste, die den modernen Arbeitnehmer angesichts von künstlicher Intelligenz und Automatisierung schon heute um den Schlaf bringen, sind im Detroit von morgen Alltag. Auf den Pappschildern der Penner der Zukunft steht: ""Androiden haben mir den Job geklaut"". Ein Satz, den man sich vielleicht bald selber in den Lebenslauf schreiben kann. Deshalb werden die ""Automatons"" auch von den (noch) herrschenden, aber eben arbeitslosen Menschen herumgeschubst. Jeder, der mit Science-Fiction-Klischees vertraut ist, weiß, was daraus folgt: die Roboter-Rebellion. Klar ist auch, dass sich das Spiel dazu bei bekannten Vorbildern bedient, sei es Philip K. Dicks ""Blade Runner"" oder Isaac Asimovs ""I, Robot"".

""Detroit"" folgt den Geschichten der Androiden Kara, Marcus und Connor. Erstere dient als Kindermädchen und Haushaltshilfe, der zweite Roboter ist Handlanger eines Künstlers, und Connor schließlich macht als Teil einer Polizeieinheit Jagd auf seinesgleichen. Er versucht aufzuklären, warum immer mehr Androiden aus ihrem Programmschema ausbrechen, anfangen, Amok zu laufen, sogar einen freien Willen zu entwickeln scheinen.

""Detroit: Become Human"" ist ein schnell getakteter Videospiel-Thriller. Er bietet ineinander verwobene Handlungsstränge ebenso wie Vor- und Rückblenden. Zwischendurch gibt es immer wieder auch Phasen, in denen der Spieler die banalsten Aufgaben erledigen muss. Drücke X, damit Kara das dreckige Geschirr abspült, wische über das Touchpad des Joypads, damit deine Figur im Spiel dieselbe Bewegung vollzieht. Die Alltagshandlungen sollen dazu dienen, Intimität zwischen Spieler und Spielfigur herzustellen. Die Macher wissen: Wenn ein Rezipient nichts für eine Figur empfindet, dann gibt es keine Geschichte. Gleich zu Beginn tritt man in Gestalt des Connor in einem Geisel-Szenario auf den Plan. Wenn man in der Wohnung genug über die Beweggründe des ausgeflippten Androiden in Erfahrung bringt - er hat Angst von einem moderneren Modell ersetzt zu werden - kann man die Bedrohung ohne Blutvergießen auflösen. Je nachdem, wie man sich entscheidet, entfaltet sich die Geschichte anders. Nach jedem Kapitel kann man in einer Art Fließdiagramm zusehen, welche Entscheidungen der Rest der Spieler getroffen hat. Wie viel Prozent von ihnen zeigen sich mitfühlend, wie viele halten eine Drohung für den besseren Ansatz zur Konfliktlösung? Das Spiel verschafft so einen Einblick in die Befindlichkeit seines Publikums.

Wenn man die ""falsche"" Entscheidung trifft, kann es passieren, dass bereits nach fünf Minuten einer der vermeintlichen Hauptcharaktere das Zeitliche segnet. So sperrt das Spiel seine Benutzer womöglich von einem Drittel der Story aus. Das kann man als verwegen bezeichnen. Aber genau das ist eigentlich der interessanteste Aspekt. Diese Möglichkeit eines selber verschuldeten Verlusts bieten weder Film noch Roman. Die erste Lektion, die man lernt, heißt also: Dein Scheitern ist möglich. Der Spieler, der es gewohnt ist, zum allmächtigen Herrscher über eine virtuelle Welt zu werden, unterliegt hier dem Autor, der es anders will.

""Detroit: Become Human"" zählt sicherlich zu den wichtigsten und auch am aufwendigsten beworbenen Videospiel-Titeln des Jahres, entwickelt wurde es vom französischen Studio ""Quantic Dream"".
Der Erfinder gibt gerne damit an, wie komplex seine Spiele sind

David Cage ist dessen Gründer, CEO, Buchhalter und Regisseur. Außerdem nennt er sich selbst Videospiel-Auteur. Einen Großteil dessen, was die Industrie so ausstößt, hält er für ""so eindimensional wie Pornografie"", er will dagegen ""ernsthafte Geschichten für ein erwachsenes Publikum"" erzählen. Das hindert ihn aber nicht daran, Metaphern und Analogien von der Subtilität eines Presslufthammers in seine Produktionen zu packen. Im Fall von ""Detroit"" bedeutet das: Natürlich müssen die Androiden im Bus hinten stehen, natürlich stehen an jeder Ecke die Prediger und Maschinenstürmer, die davon faseln, dass früher alles besser gewesen sei, und natürlich skandieren die Roboter in der Dämmerung ihrer Emanzipation: ""We have a dream!""

Cage scheint dann auch der Meinung zu sein, dass man eine narrative Wucht quantifizieren können muss. Er verweist jedenfalls gerne auf die Dicke seiner Drehbücher. Im Falle von ""Become Human"" sind es mehr als 2 000 Seiten Skript. Dazu protzt das Spiel mit einer ganzen Fülle anderer Kennzahlen. Genau 513 verschiedene Charaktere tauchen auf, es gibt mehr als 35 000 Kameraeinstellungen und 74 000 einzigartige Animationen, die von mehr als fünf Millionen Zeilen Programmcode zum Leben erweckt wurden. Doch was bedeutet das alles?

Er wolle mit seinen Spielen keine Botschaften vermitteln, sagt David Cage. Auf die Frage, warum er überhaupt noch die Form eines Spiels wählt und nicht gleich die des Films, hat er keine Antwort. Denn wie zeitgemäß ist dieses Genre überhaupt noch, wo doch Netflix und Amazon TV-Serien am Fließband produzieren, die dank Big Data und arkaner Marktforschungstechniken ja an jede Zielgruppe angepasst worden sind?

""Wie weit gehst Du, um frei zu sein?"", fragt das Spiel auf seiner Packung. Und das ist sein Problem. Seit mehr als 20 Jahren lautet das Versprechen des Mediums Videospiel, eine neue Form von interaktiver Unterhaltung zu schaffen. Stattdessen finden sich auch hier Medienkonsummuster wie Binge-Watching. Es scheint, als sei dies David Cages Dilemma. Jetzt, wo die Technik endlich so weit wäre für neue Formen, haben die Menschen das Interesse an der Vision des interaktiven Dramas verloren. Gut ist eine Erzählung nicht mehr nur, wenn sie auf- oder anregt, sondern vielleicht vor allem dann, wenn sie betäubt und einlullt.
";https://www.sueddeutsche.de/digital/detroit-become-human-review-1.4040293;sz.de;Michael Moorstedt
15.06.2018;Wissenschaftsminister: Digitalprofessuren starten 2019;"An niedersächsischen Hochschulen soll es nach den Vorstellungen von Wissenschaftsminister Björn Thümler schon bald Digitalprofessuren geben. ""Wir wollen 2019 mit den ersten Professuren starten"", sagte der CDU-Politiker im Interview der ""Neue Presse"" (Freitag). Im Laufe der Legislaturperiode solle das Programm dann ausgeweitet werden. ""Wie viele es werden, entscheidet der Landtag als Haushaltsgesetzgeber"", sagte Thümler der Zeitung.

Ziel sei es, die Grundlage für den Ausbau der Studienangebote unter anderem in Themenfeldern wie Data-Science, Künstliche Intelligenz oder IT-Sicherheit schaffen. Für den Wissenschaftsminister geht es aber nicht nur um die IT-Spezialisten, die händeringend gesucht werden. Es dürfe sich nicht nur um rein ""technische"" Fragen drehen, sondern auch um Reflexionswissen. Die neuen Digitalisierungsprofessuren sollten sich deshalb ausdrücklich nicht auf die technischen Fächer beschränken, sondern auch die Sozial- und Geisteswissenschaften seien gefragt.";https://www.sueddeutsche.de/wissen/wissenschaft-hannover-wissenschaftsminister-digitalprofessuren-starten-2019-dpa.urn-newsml-dpa-com-20090101-180615-99-732982;sz.de;DPA
12.06.2018;Mal was Neues;"Wenn man beschreiben will, was Holger Bingmann beruflich macht, trifft es ""Reisender zwischen den Welten"" vermutlich ganz gut. Zwischen digitaler und analoger Wirtschaft ist er unterwegs, zwischen Old Economy und Zukunftsmusik, München und Berlin. Zumindest klamottentechnisch hat das Weltenwandeln an diesem Tag mustergültig hingehauen: Unten Jeans mit Turnschuhen, oben Jackett mit Einstecktuch, so steht der 56-Jährige in einem Büroloft in Berlin-Kreuzberg und ist erkennbar guter Dinge.

Bingmann ist geschäftsführender Gesellschafter der Melo Group, eines internationalen Dienstleistungs- und Medienkonzerns, der aus dem 1945 in München gegründeten Pressevertrieb Hermann Trunk hervorgegangen ist. Die Gründerfamilie Trunk ist noch heute an der Gruppe beteiligt. Bingmann selbst war 39 Jahre alt, als er den Presse-Grossisten kaufte; zuvor hatte er für Daimler-Benz in Brüssel gearbeitet und für einen Pressegroßhändler in Baden-Württemberg. Heute macht Bingmanns Firmengruppe knapp 300 Millionen Euro Umsatz im Jahr und beschäftigt rund 2000 Leute in sechs Ländern. Zum Pressegroßhandel sind Paket- und Flughafendienstleistungen hinzugekommen, wie das Catering an Bord oder die Reinigung der Teppiche am Schalter. Zudem beliefert Bingmanns Firmengruppe nicht mehr nur 20 000 Kioske zwischen Budapest und Ulm, sondern bestückt auch den Online-Kiosk der Lufthansa. Der Pressegroßhandel steht zwar weiter für die Hälfte von Umsatz und Ertrag, den Rest aber erwirtschaften die neuen Geschäftsfelder. Gerade weil er aus einer klassischen Branche kommt, will Bingmann nicht zu jenen gehören, die zwar ihre Abläufe digitalisieren, aber letztlich kein digitales Geschäftsmodell haben.

Was auch Kreuzberg und die Turnschuhe erklärt. ""Blogfabrik"" steht unten am Eingang, der in einem Berliner Backstein-Hinterhof liegt. Start-up-Terrain. Oben sitzen Frauen und Männer mit Laptops an langen Tischen und zwischen pinken und türkisen Trennwänden. Im turnhallengroßen Veranstaltungsraum sind externe Anzugträger zu einem Stuhlkreis zusammengekommen. In der Küche essen Männer mit Bart und Wollmütze Mittag; Bürohund Lola dagegen interessiert sich für den Kuchen auf Bingmanns Teller. Bingmann, promovierter Betriebswirt und Hobby-Rennradler, trägt eine runde Brille und gescheiteltes helles Haar. Er sieht vergnügt aus, spricht eher leise als laut und wirkt generell wie jemand, der unangestrengt freundlich sein kann, weil er in sich ruht. 600 Quadratmeter ""kreativen Raum"" stellten sie hier zur Verfügung, sagt er und zieht sein Jackett aus. 80 Blogger, Youtuber, Grafiker und Influencer arbeiten in der vor drei Jahren gegründeten Blogfabrik. ""Alles kleine, selbständige Unternehmer, die hier ihrer Arbeit nachgehen und uns einen Teil ihrer Kreativität abgeben."" Im Prinzip funktioniert die Blogfabrik wie eine Mini-Agentur. Die Fabrikanten bekommen einen Arbeitsplatz, für den sie nur eine symbolische Miete zahlen müssen; sie kommen und gehen, wann sie wollen, arbeiten an ihren Projekten, nutzen das Fotostudio und können sich kostenlos von einem Medienanwalt beraten lassen. ""Entspannte Nutzung bis zur Party"", sagt Bingmann.

Die Gegenleistung ist, dass die fünf bis sechs fest angestellten Blogfabrik-Manager die Freiberufler regelmäßig anfragen dürfen, ob sie einen von außen hereingekommen Auftrag übernehmen wollen. Gegen Bezahlung und freiwillig, sagt Bingmann und zwinkert. Das anfangs getestete Konzept eines ""modernen Sklaventums"" habe nichts gebracht.

""Wir machen zum Beispiel eine SocialMedia-Kampagne für eine große deutsche Bank"", nennt Bingmann ein Beispiel für solche Auftragsprojekte. Mittelfristig erhofft er sich aber mehr. So habe eine große Fluggesellschaft gefragt, ob er neben digitalen Zeitungen nicht noch weitere Inhalte liefern könnte. ""Zum Beispiel Restauranttipps von Foodbloggern aus New York, statt der üblichen Touristenfallen."" Noch, sagt Bingmann, sei er da nicht angekommen, ""und ich weiß auch gar nicht, ob es überhaupt klappt"". Zu beunruhigen scheint ihn das aber kaum. Für dieses Jahr rechnet er noch mit einem ""kleinen Minus"" für die Blogfabrik. ""Ich mache aber jede Wette, dass sie 2019 Geld verdient.""
""Wir haben Arbeitsplatz und Arbeitszeit freigegeben. Und es funktioniert.""

Bingmann hat inzwischen ein Faible für den Wandel. Das war nicht immer so. 20 Jahre lang war er ein klassischer Patriarch. Irgendwann aber merkte er, dass Struktur und Kultur seiner Melo-Gruppe immer größere Reibungsverluste produzierten angesichts der Firmenzukäufe und des Wachstums von 100 auf 2000 Mitarbeiter. Er engagierte einen Coach, erst für sich, dann für die Führungskräfte. Er lernte einen neuen Führungsstil und stellte einiges auf den Kopf. Die Zentrale in München, wo 100 Leute arbeiten, ließ er bis auf die Außenmauern abreißen und neu errichten. ""Da sieht es jetzt aus wie hier"", sagt er und macht eine raumgreifende Armbewegung über die Blogfabrik hinweg. ""Ich mag solche Strukturen."" Feste Büros gibt es keine mehr, auch er hat keins. Wenn er einen Schreibtisch braucht, aktiviert er ""M"" im Kalender, dann wird ein Platz für ihn reserviert, mit Glück in seiner Lieblingsecke. ""Wir haben Arbeitsplatz und Arbeitszeit freigegeben. Und es funktioniert. Ich bin von der höheren Kreativität überzeugt."" Vielleicht, sagt er, sitze dann mal einer früher im Biergarten. Dafür aber schaue der abends noch mal in seine Mails. ""Wir haben heute einen viel entspannteren Umgang miteinander.""

Auch Bingmann selbst nahm sich sein Stück Freiheit. ""Als Vorbild"", sagt er. Er brachte seine Töchter zum Schulbus, freitags blieb er immer mal wieder zu Hause, ging Rennradfahren, arbeitete im Home-Office, holte die Mädchen pünktlich wieder ab. Denn wie schwer die Vereinbarkeit von Beruf und Familie sein kann, weiß er. Seine erwachsenen Söhne aus erster Ehe wuchsen nach der Trennung bei ihm auf. Alleinerziehender Vater und Unternehmer, das allerdings waren wenig kompatible Welten. Bei den Töchtern aus seiner zweiten Ehe wollte er weniger abwesend sein - und das auch seinen Leuten ermöglichen. Im Moment allerdings klappe das nicht mehr so recht mit den freien Freitagen, sagt Bingmann fast entschuldigend. Der Grund ist, dass sich der gebürtige Stuttgarter im Herbst vergangenen Jahres zum Präsidenten des Groß- und Außenhandelsverbands BGA hat wählen lassen, obwohl er eher ein Außenseiter war in der Verbändewelt. Er sei neugierig gewesen auf das Zusammenspiel von Wirtschaft und Politik, sagt er. ""Eine Lebensergänzung."" Dass er wenig Verbandssozialisation hinter sich hat, merkt man. Bingmann spricht freier, als das andere auf vergleichbaren Posten tun. Sein Pressesprecher zuckt dann schon mal zusammen im Interview. ""Ich trage hier in Berlin zu einer gewissen Lockerheit bei"", nennt Bingmann das.
Das Tagesgeschäft überlässt er nun anderen - und hat damit Freiraum für neue Projekte

Aus dem operativen Tagesgeschäft seiner Firma hatte er sich ohnehin schon etwas zurückgezogen, weil er Freiraum für neue Projekte wollte. Bingmann bezeichnet es gerne als seine größte unternehmerische Leistung, gute Leute gefunden zu haben, die jetzt an seiner statt Verantwortung tragen. Er gründete derweil in Berlin eine Universität, die ""Digital Business University"". Seit Februar läuft der Akkreditierungsprozess, was mindestens 14 Monate dauern wird. 1200 Seiten dick seien die Unterlagen gewesen, sagt Bingmann und schüttelt immer mal wieder leicht den Kopf, während er unter anderem von der Präsenzbibliothek spricht, zu der er verpflichtet wurde. Obwohl es in der neuen Uni nur eine Woche Präsenzpflicht im Monat geben und der Rest online laufen soll. ""45 000 Euro müssen wir für Bücher ausgeben und 6,5 Professoren einstellen.""

Am Ende soll man an der neuen Hochschule ""einen ordentlichen Bachelor"" machen können, später auch einen Master, für 490 Euro Studiengebühren im Monat. Schon im Herbst soll es zudem die ersten Lehrgänge zur berufsbegleitenden Weiterbildung in und an großen Unternehmen gebe. ""Für kreative Menschen, die Dinge anders machen wollen."" Bingmann glaubt daran, dass man mit Bildung Geld verdienen kann. Er klickt sich auf seinem Tablet durch eine Präsentation. ""Digital Business Management"" soll ein Studiengang heißen, ""Data Science / Business Analytics"" ein anderer. ""Wir brauchen neue Ausbildungsangebote"", sagt er und spricht über entstehende und verschwindende Berufe. In seiner Firma etwa habe er nur einen, der sich mit Big Data auskenne. ""Wir brauchen diese Leute aber. Auch wenn es manchmal schwierig ist, unser verrücktes Konzept zu erklären.""";https://www.sueddeutsche.de/wirtschaft/mittwochsportraet-mal-was-neues-1.4012422;sz.de;Henrike Roßbach
13.04.2018;Macht der sozialen Medien;Keine Partei, die nicht auf Facebook präsent wäre, keine größere Einrichtung ohne eigenen Youtube-Kanal. Doch welche Macht haben diese sozialen Medien? Entscheiden sie Wahlen? Wer sitzt an den Schalthebeln dieser Macht? Jürgen Pfeffer, Professor für Computational Social Science and Big Data an der zur TU München gehörenden Hochschule für Politik, erforscht die Einflüsse dieser Netzwerke. Am Mittwoch, 18. April, 19.30 Uhr, beleuchtet er in seinem Vortrag bei den Garchinger Gesprächen in der Stadtbücherei Wirkungen, Hintergründe und Dynamiken dieser Medien. Kostenlose Eintrittskarten gibt es in der Stadtbibliothek.;https://www.sueddeutsche.de/muenchen/landkreismuenchen/garching-macht-der-sozialen-medien-1.3943747;sz.de;SZ
10.04.2018;Neue Studiengänge stellen sich auf Digitalisierung ein;"Neue IT-Studiengänge rund um Daten und Gesundheit

In Potsdam gibt es ab dem Wintersemester zwei neue IT-Studiengänge mit unterschiedlichen Schwerpunkten. Im englischsprachigen Master Digital Health geht es um die Digitalisierung der Medizin, das Angebot richtet sich an Mediziner und Informatiker gleichermaßen. Der Master Data Engineering ist dagegen nur für klassische Informatiker oder IT-Ingenieure gedacht, hier geht es um die technischen, rechtlichen und ethischen Aspekte von Datensammlung und -analyse. Unterrichtet wird hier auf Deutsch.

Die neuen Master sind ein gemeinsames Angebot des Hasso-Plattner-Instituts und der Universität Potsdam, Studiengebühren fallen keine an. Beide Studiengänge haben eine Regelstudienzeit von vier Semestern. Interessenten können sich noch bis zum 15. Juli bewerben.

Neuer Master rund um Digitales und Nachhaltigkeit

Helfen digitale Technologien dabei, Ressourcen zu sparen? Oder sind Internet und Co. der große Energiefresser der Zukunft? Um Fragen wie diese geht es im neuen Master Digital Transformation & Sustainability(zu Deutsch: Digitale Transformation & Nachhaltigkeit) an der Hamburg School of Business Administration (HSBA). Der erste Jahrgang startet zum Wintersemester 2018/2019, teilte die private Wirtschaftshochschule mit. Pro Monat werden 950 Euro an Studiengebühren fällig.

Themen des vorwiegend deutschsprachigen Studiengangs sind unter anderem nachhaltiges Wirtschaften und digitale Technologien, hinzu kommen moderne Führungs- und Projektmanagement-Methoden. Voraussetzung für die Teilnahme ist ein Bachelor in Wirtschaftswissenschaften oder ähnlichen Fächern, die Regelstudienzeit beträgt fünf Semester.

Neuer Ingenieur-Bachelor für das Internet der Dinge

Die Hochschule Trier bildet ab dem kommenden Sommersemester Ingenieure für das Internet der Dinge aus. Dann startet der neue Bachelor-Studiengang ""Internet of Things - Digitale Automation"", teilt die Fachhochschule mit. Grundlage des Angebots ist Basiswissen in den Ingenieurwissenschaften, hinzu kommen Inhalte aus den Bereichen Automatisierung und Digitalisierung. So sollen sich die Studierenden zum Beispiel mit Robotik und Bildbearbeitung oder mit der Entwicklung von passenden Apps beschäftigen. Die Regelstudienzeit beträgt sieben Semester, eine Zulassungsbeschränkung gibt es nicht.

Neue Studiengänge zu digitaler Landwirtschaft und Freiraumplanung

Zum Wintersemester startet die Hochschule Ostwestfalen-Lippe einen neuen Bachelor rund um die Digitalisierung in der Landwirtschaft. In dem Studiengang Precision Farming (deutsch: ""präzise Landwirtschaft"") geht es etwa darum, wie sich durch Datenanalyse das Bestellen der Felder optimieren lässt. Die Regelstudienzeit beträgt sieben Semester, teilt die staatliche Fachhochschule mit.

Ebenfalls zum Wintersemester 2018/19 bringt sie den Bachelor Freiraummanagement an den Start. Der Studiengang verknüpft die Themen Landschaftsgestaltung und Nachhaltigkeit. Neben der Planung von Freianlagen wie Gärten und Parks setzen sich Studierende in dem auf sieben Semester angelegten Bachelor mit der Bau-Ausführung sowie der Unterhaltung solcher Anlagen auseinander. Auch neu im Programm der Fachhochschule ist der Bachelor Data Science, der auf sechs Semester verteilt den Umgang und die Analyse von Daten in den Fokus stellt. ";https://www.sueddeutsche.de/karriere/arbeit-neue-studiengaenge-stellen-sich-auf-digitalisierung-ein-dpa.urn-newsml-dpa-com-20090101-180404-99-749940;sz.de;DPA
05.04.2018;Gröfaz - der größte Frauenheld aller Zeiten;"In Walter Moers' satirischem Comic ""Adolf - Äch bin wieder da!!"" von 1998 trifft Hitler unter anderem auf Hermann Göring. Der ist mittlerweile eine Frau, wirft ihm brünstige Blicke zu und denkt: ""Der Führer ist immer noch ein schöner Mann."" Nun ja.

Hitler als Objekt romantischer Begierde, als erotisches Wesen gar, das ist für die meisten heutigen Menschen eine surreale bis lächerliche Vorstellung, bei Walter Moers ist der Mann aus Braunau natürlich auch eine Witzfigur mit Bärtchen. Dabei war Adolf Hitler vielleicht tatsächlich mal so was wie der Gröfaz - nicht der größte Feldherr, sondern der größte Frauenheld aller Zeiten. Eines jedenfalls steht historisch fest: Der deutsche Diktator hat nicht nur bei seinen öffentlichen Auftritten regelmäßig Teile des weiblichen Publikums zum Rasen gebracht, er hat auch massenweise (weibliche) Fanpost erhalten, die das gewöhnliche Maß an Zuneigung bei weitem überstieg.

Adressiert an ""Süßes Adilie"", ""Wölflein"" oder ""Lieber Dolfi"" haben Tausende von deutschen Frauen Hitler quasi ihr Herz schriftlich zu Füßen gelegt. Circa 8000 solcher Schreiben sind heute bekannt: Von Nationalstolz geprägte Gedichte, freundliche Angebote, dem Führer die Haare zu schneiden - und eine Reihe von mehr oder weniger stürmischen Liebesbriefen. Der Schauspieler, Fotokünstler und Rezitator Stefan Hunstein präsentiert sie am Donnerstag, 12. April, in der Stadtbücherei Garching in der Lesung ""Liebesbriefe an Hitler"".

Entdeckt wurden diese Zeugnisse der Leidenschaften 1946 von William C. Emker, Offizier der US-Armee, der auf entsprechende Briefumschläge in den verwüsteten Archiven der Berliner Reichskanzlei stieß. Das Spektrum der Sympathiebekundungen ist abwechslungsreich, darunter auch deutliche Angebote: ""Lieber Führer Adolf Hitler! Eine Frau aus dem Sachsenland wünscht sich ein Kind von Ihnen"", heißt es da. Eine andere schreibt: ""Du süßestes herzensbestes Lieb, mein Einziges, mein Allerbester, mein trautest und heißest Geliebtes. Weißt Du, heute könnte ich Dir gar nicht genug Namen geben, heute möchte ich Dich vor lauter Lieb' auffressen. Was würden aber da die anderen sagen?"" Die Briefe belegen beispielhaft, wie Einzelne vom Führerkult beeinflusst waren und welche Auswirkungen die bis ins Wahnhafte gesteigerte Gefolgschaft auf ihre psychosoziale Verfasstheit hatte. ""Du niedlicher Führer! Komm doch zu mir, ich gebe Dir mit heißem Herzen alles, was Du Dir von einer Frau wünschst...!"". Was sich Hitler von deutschen Frauen wünschte, waren bekanntlich viele Kinder, die diese dem Führer schenken sollten, sein Bild vom anderen Geschlecht war konservativ geprägt. Charakteristisch ist sein Satz: ""Meine Braut ist Deutschland"". Gleichwohl hatte er eine magnetische Anziehung auf so manche Frau, Persönlichkeiten wie Magda Goebbels, Winifred Wagner oder auch die junge englische Aristokratin Unity Mitford verehrten ihn hemmungslos - hier mal abgesehen von seiner Langzeit-Liaison mit Eva Braun und die von vielen Spekulationen umrankte Beziehung mit der Nichte Geli Raubal. Die von Bücherei und VHS-Nord veranstaltete Lesung mit Stefan Hunstein, der Mitglied der Bayerischen Akademie der Schönen Künste ist, beginnt um 19.30 Uhr. Kostenlose Eintrittskarten gibt es an der Infotheke der Stadtbücherei Garching, Bürgerplatz 11 (Telefon 089/32 08 92 11).

Knapp eine Woche später gibt es dort im Rahmen der ""Garchinger Gespräche"" eine weitere interessante Veranstaltung. Am Mittwoch, 18. April, hält Jürgen Pfeffer, Professor für Computational Social Science and Big Data an der zur TU München gehörenden Hochschule für Politik, den Vortrag ""Die Macht der 'Sozialen Medien' - Die Wissenschaft von Online-Freundschaften, Hass-Posts und Shitstorms"". Pfeffer geht der Frage nach, inwieweit Twitter, Facebook oder Youtube Wahlen beeinflussen und ob sie eine Gefahr oder eine Chance für die Demokratie sind. In seiner Arbeit erforscht der Wissenschaftler die Einflüsse sozialer Netzwerke auf den Einzelnen und die Stabilität realer Gesellschaften. Auch für diese Veranstaltung kann man kostenlose Karten an der Infotheke der Bücherei reservieren.";https://www.sueddeutsche.de/muenchen/landkreismuenchen/lesung-groefaz-der-groesste-frauenheld-aller-zeiten-1.3933125;sz.de;Udo Watter
04.03.2018;"""Big Data"": Verhängnisvolle Spuren im Internet";"Die IT-Unternehmerin Yvonne Hofstetter entwickelt mit ihren Mitarbeitern - Informatikern und Mathematikern - in der kleinen Ortschaft Gerlhausen bei Zolling Kerntechnologien für künstliche Intelligenz. Damit werden große Datenmengen gespeichert und ausgewertet. Inzwischen ist sie jedoch auch deutschlandweit eine gefragte Digitalisierungskritikerin und warnt vor Überwachung und Eingriffen in Persönlichkeitsrechte.

Denn Hofstetter weiß, wie ""Big Data"" funktioniert, die Erfassung, Speicherung und Auswertung von Daten aus dem Internet mit Hilfe Künstlicher Intelligenz. Allerdings betreibe ihre Firma dies ausschließlich mit Daten von Maschinen, von Drohnen im militärischen Einsatz oder für Logistikunternehmen. ""Daten von Menschen fassen wir nicht an"", betont Hofstetter, die Rechtswissenschaften studierte.
Ohne das Wissen der Menschen werden ihre persönlichen Daten und Spuren gespeichert und ausgewertet

Die Juristin hatte kurz nach ihrem Studienabschluss, fasziniert von den Möglichkeiten des Internets, in den Neunzigerjahren umgesattelt und begonnen, in einem IT-Unternehmen zu arbeiten. Von ihren Fachkollegen lernte sie das nötige Rüstzeug. Dann, vor 20 Jahren, gründete sie mit einem ehemaligen Kollegen ihre eigene Firma ""Teramark Technologies"". Hofstetter kritisiert, dass ohne das Wissen der Menschen persönliche Daten und Spuren, die sie im Internet hinterließen, gespeichert und mit künstlicher Intelligenz ausgewertet würden. So würden ganze Profile von Menschen gespeichert, was diese tun, was sie denken. Das klinge zwar nach Science Fiction, sei jedoch Realität, sagt die Juristin und berichtet vom Vorstoß des Freistaates, dass jeder Haushalt per Gesetz verpflichtet werden solle, einen Funkwasserzähler zu installieren. Die Zähler funkten alle paar Sekunden, wie viel Wasser verbraucht werde. Dies geschehe angeblich, um das Wasserablesen zu vereinfachen.
Eingriff in die Grundrechte der Bürger

Laut Hofstetter greift das die Grundrechte der Bürger ein: Der Artikel 13 des Grundgesetzes sieht die die Unversehrtheit der Wohnung vor. Niemand dürfe gesetzlich gezwungen werden, seinen Wasserverbrauch sekundengenau erfassen zu lassen. Denn mit diesen Daten, die der Wasserversorger speichern und analysieren könne, ließe sich mit künstlicher Intelligenz ein genaues Wohnprofil von Menschen erstellen. Ginge es nur um den Wasserverbrauch, würde ein einmaliges, jährliches Ablesen des Zählers genügen, argumentiert Hofstetter.

Als ein weiteres Beispiel, wie sehr jeder im Alltag bereits vernetzt sei und Persönliches preisgebe, erklärt die Datenfachfrau am Unterschied zwischen einem Handy und einem Smartphone. Beide Geräte könnten zwar über Funkmasten geortet werden und selbst wenn das Handy abgeschaltet sei, ließe sich noch der letzte Aufenthaltsort ermitteln. Dies sei jedoch nicht so präzise wie die Ortung über Wlan und GPS in einem Smartphone. Die exakte, dauerhafte Ortung und Verfolgung dieser Computer sei möglich, wenn der Nutzer die Wlan-Funktion eingeschaltet lasse. Das Gerät versuche, sich bei jedem Geschäft, jedem Restaurant in dessen Wlan zu schalten und sende dabei seine Mac-ID. Das sei die einzigartige Kennung, die exklusiv für jedes Smartphone vergeben werde und ""schon weiß man genau, wann Sie sich wo aufhalten"", sagt Hofstetter. ""Man"", das sei zum Beispiel der Hersteller des Betriebssystems, oftmals Google. Deshalb empfiehlt sie, die Wlan-Funktion auszuschalten, wenn man sie nicht benötige. Außerdem warnt Hofstetter eindringlich vor der Installation von Apps. Denn die Computerprogramme könnten registrieren, wann man mit wem wie lange telefoniere. App-Hersteller verkauften diese Daten auch weiter an Datenhändler, beispielsweise die Information, dass jemand wahrscheinlich Geld benötige, erklärt sie. Dieser Mensch erhalte dann überraschend Kreditangebote.
In Chicago und Kent wird heute schon berechnet, ob ein Facebook-Nutzer demnächst eine Straftat begeht

Durch die Nutzung sozialer Netzwerke wie Facebook oder Whatsapp gebe man preis, mit wem man verbunden sei. Mit einer Netzwerkanalyse könne dann nicht nur berechnet werden, wie hoch der Finanzbedarf, sondern auch wie kriminell ein Bürger vermutlich sei, schildert Hofstetter. In Chicago und Kent werde heute schon aus den Facebookdaten berechnet, ob ein Nutzer demnächst eine Straftat begehe. So etwas gebe es in Deutschland noch nicht, aber manche Länderpolizei würde diese Möglichkeiten auch gerne nutzen, warnt die Juristin. Da Smartphones Computer seien, könne man sie auch leicht hacken. Nach wie vor besonders gefährdet sei das Betriebssystem Google Android. Per Fernbedienung könnten Hacker Kamera und Mikrofon einschalten, ohne dass es der Nutzer bemerke. Sie selbst verwende kein Smartphone und sei auch nicht bei Facebook, sagt sie. Ihr genüge ein Telefon, das Handy, um SMS zu verschicken und das Mailprogramm auf dem Laptop.

Die Datenfachfrau und Digitalisierungskritikerin empfiehlt, möglichst wenig von sich selbst preiszugeben, in dem man auf Facebook oder Smartphones verzichte. Man solle aufmerksam gegenüber neuen Erfindungen oder bei der Aufweichung der Grundrechte sein, sagt sie. Von den scheinbar harmlosen Möglichkeiten des Internets solle man sich nicht verführen lassen.";https://www.sueddeutsche.de/muenchen/freising/datenerfassung-big-data-verhaengnisvolle-spuren-im-internet-1.3890205;sz.de;Katharina Aurich
01.02.2018;Absolute Kontrolle;"Ein Staat erfindet sich neu, und er tut das unter anderem hier, mitten in diesem Pekinger Büroturm, in diesem nüchternen Raum. Bildschirme, viele Bildschirme, darauf Gesichter, unsere Gesichter, Gesichter von der Straße, Gesichter vom Flur, dazu Namen, Geschlecht, Ausweisnummern. Aufgeschnappt von den Kameras von Megvii Face++, hier gerade das heißeste Start-up einer heißen Branche, die angibt, die Welt verändern zu wollen und dabei wahrscheinlich nicht zu viel verspricht. Künstliche Intelligenz, KI. In dem Raum ein junger Mann namens Xie Yinan. Er arbeitet hier, als Marketingdirektor. ""Es ist wie im Film"", sagt er. ""Ich bin jetzt seit drei Jahren dabei, und im Traum nicht hätte ich mir vor drei Jahren vorstellen können, dass wir all die Dinge machen, die wir heute machen. Alles, was du aus Science-Fiction-Filmen kennst, werden wir in die Wirklichkeit holen."" Todmüde ist er. Und euphorisiert. Sein Land will Nummer eins der Welt werden auf dem Feld der künstlichen Intelligenz. Und seine Firma die Nummer eins in ihrem Bereich. Solche Zeiten sind das.

Auf einer Leinwand in einem Künstleratelier ein paar Kilometer entfernt läuft ein Film. ""Die Augen der Libelle"", ist der Titel. ""Ich verrate dir ein Geheimnis"", sagt der junge Mann in dem Film. ""Ich beobachte dich oft, am Monitor."" Dann, aus dem Off, der Sprecher: ""Da ist ein Mann"", sagt er. ""Er wird an diesem Tag 300 Mal von einer Kamera gesehen werden. Und da ist eine Frau. Sie hat keine Privatsphäre mehr. Der Mann und die Frau werden sich treffen."" Es ist ein Liebesfilm, anfangs. Die Frau arbeitet in einer Viehzuchtanstalt. Sie beobachtet die Kühe. Und wird dabei von Kameras beobachtet. Die Überwacher der Kameras werden ebenfalls beobachtet. Beim Beobachten. Der Film zeigt, wie ein Paar Sex im Auto hat. Er zeigt Turteleien im Restaurant, eine Verfolgungsjagd auf der Autobahn, wilde Prügeleien und eine Schönheitsoperation. Es ist ein Spielfilm, der im vorigen Jahr auf Festivals lief, in Toronto zum Beispiel. Es ist der erste Spielfilm, für den keine einzige Szene gespielt oder gedreht wurde. Jedes Bild, jede Szene, knapp 600, stammt von chinesischen Überwachungs- und Livestreamkameras.
Die Kameras melden, wenn einer verdächtig oft auftaucht. Zum Beispiel an einer Bushaltestelle

China: Das Land, das sich die Zukunft ins Heute holt, begierig und weit schneller, als der Westen sich das traut. ""Als ich 2013 die Idee zu dem Film hatte, gab es kaum Material"", sagt der Macher des Films, der bekannte Pekinger Künstler Xu Bing. ""Bis vor zwei Jahren plötzlich die ganzen Streams im Netz auftauchten, auf Webseiten, für jeden zugänglich. Überwachungskameras von Privatleuten und Unternehmen vor allem. Viel, viel mehr, als wir uns jemals erhofft hatten. Das ist mit einer Geschwindigkeit explodiert, die sich keiner vorstellen kann."" 20 Computer hatten sie, die 24 Stunden am Tag Bilder herunterluden. Aus 11 000 Stunden Material destillierten sie 81 Minuten für ""Die Augen der Libelle"".

Ein Libellenauge besteht aus 28 000 Facetten, eine jede ein Äuglein für sich. Die Libelle kann Bilder fünf- bis sechsmal schneller erkennen als der Mensch und dabei 360 Grad im Blick behalten.

Zurück bei Megvii. ""Wir wollen die Augen der Stadt schaffen"", sagt Xie Yinan. ""Und wir wollen sie intelligent machen."" Die Firma ist zu Hause in einem modernen Büroturm im Pekinger Stadtbezirk Haidian, in nächster Nähe zu den Elite-Universitäten des Landes. Die Glastür am Eingang öffnet sich, wenn die Kameras das Gesicht des Ankommenden erkennen. Dank der Technologie von Megvii konnten die Leute mit ihrem Gesicht schon die Smartphones von Huawei oder Vivo entsperren, lange bevor Apple auf die Idee kam. Außerdem können sie bei Kentucky Fried Chicken in Hangzhou so ihre Pommes bezahlen oder alle ihre Einkäufe mit der Alipay-App bezahlen, wie das schon 520 Millionen Chinesen regelmäßig tun. Hotels in China überprüfen mit Megvii-Kameras, ob der Gast wirklich der ist, der er behauptet. Bahnhöfe wie in Wuhan gewähren nur noch dem Zutritt, der sein Gesicht scannen lässt.

Die Augen der Stadt. Die Augen des Staates. Für den Einzelnen wird sein Gesicht zum Schlüssel, der ihm den Zugang zur Welt öffnet. Für die Beobachter wird die Kamera zum Schlüssel, der ihnen das Tun des Einzelnen entsperrt. ""Verbrecher müssen sich heute gut überlegen, ob sie überhaupt noch Verbrechen begehen"", sagt Xie Yinan. ""Unser Algorithmus kann Netze von 50 000 bis 100 000 Überwachungskameras unterstützen. Wir können dir sagen, welche Sorte Mensch sich zu welchem Zeitpunkt wo befindet. Wir können sagen: Wer ist das? Wo ist er? Wie lange hält er sich hier auf? Wohin geht er dann? Wir verfolgen die Spur eines Menschen von Kamera zu Kamera."" Das System, sagt Xie, erkenne Gesichter längst besser, als Menschen das können. Megvii und die anderen Firmen bewerben ihre kommerziellen Anwendungen, führen Apps vor, die einem eine lustige Hundeschnauze ins Gesicht zaubern, aber es ist kein Geheimnis, wer im Moment wichtigster Investor und Kunde zugleich ist: der Staat, vor allem die Sicherheitsbehörden. Xie erzählt von Dankesbriefen der Polizeistellen im ganzen Land. 3000 Kriminelle, sagt er, seien der Polizei schon dank der Kameras in die Arme gelaufen, ihre Gesichter fanden sich in den Datenbanken der Behörden. Schlagzeilen machte das Oktoberfest in Qingdao voriges Jahr: 25 Verhaftungen lange Gesuchter, dank Gesichtserkennung.

Die Kameras können noch mehr: Sie melden, wenn ein Gesicht an einem Ort - etwa an einer Bushaltestelle - verdächtig oft auftaucht. ""Das könnte ein Taschendieb sein"", sagt Xie. Der Mensch in der Menge, auch er ein Risiko. Beim Konkurrenten Sensetime führen sie vor, wie die Kameras Menschenmengen analysieren. Das System erkenne, wo sich viele Leute versammeln, sagt Sensetime-Sprecherin Yuan Wei. Mehr noch: wo sich demnächst viele versammeln werden. Der Algorithmus erkennt auch, wenn eine Menge von Menschen an einem Ort sich in die eine Richtung bewegt, während ein Einzelner gegen den Strom geht. ""Das System identifiziert dann diese Person als unnormal"", sagt Yuan Wei. Und schlägt Alarm.

Bei Megvii arbeiteten vor einem Jahr 200 Mitarbeiter, heute 700, einige von ihnen sind aus den USA zurückgekehrt. Im November haben sie bei einer Investmentrunde 460 Millionen Dollar eingesammelt, Weltrekord für ein KI-Start-up. Seitdem gewann ihre Technologie mehrere Wettbewerbe, auch gegen Teams von Google, Microsoft und Facebook. KI-Start-ups schießen wie Pilze aus dem Boden in China. ""Wir stehen erst am Anfang"", sagt Xie Yinan, ""der Markt wächst rasant."" 176 Millionen Überwachungskameras gab es in China 2016, bis zum Jahr 2020 sollen es mehr als 600 Millionen sein.
Der Westen hat die Technik erfunden, aber China hängt gerade den Rest der Welt ab

Die Kommunistische Partei hat Big Data und künstliche Intelligenz als Zauberwaffen entdeckt. Bei seiner Neujahrsansprache an das chinesische Volk vor ein paar Wochen saß Partei- und Staatschef Xi Jinping wie jedes Jahr vor einer großen Bücherwand, der aufmerksame Fernsehzuschauer konnte dort unter anderem Hemingways ""Der alte Mann und das Meer"" ausmachen. Und wie jedes Jahr sahen sich Internetnutzer mit der Lupe jeden einzelnen Buchrücken an. Entdeckung Nummer eins: Xi Jinping hat in diesem Jahr ""Das Kommunistische Manifest"" von Karl Marx in Griffweite zu sich herangeholt. Vor allem aber fanden sich prominent platziert im Regal zum ersten Mal zwei Bestseller über künstliche Intelligenz. Der Parteichef will der Diktatur ein digitales Update verpassen: ""Weltspitze"" müsse China in den Disziplinen KI und Big Data werden. Im Jahr 2030, so die Vorgabe der Partei, soll Chinas KI-Industrie 150 Milliarden Dollar wert sein.

Schon 2020 werde China mit den USA gleichgezogen haben, prophezeite Eric Schmidt, Chef der Google-Mutter Alphabet vor ein paar Monaten: ""2025 werden sie uns überholt haben. Und 2030 werden sie die Industrie dominieren."" In der Grundlagenforschung lägen die USA im Moment noch vorne, sagt Xie Yinan von Megvii, ""aber in der praktischen Anwendung sind wir ihnen schon weit voraus."" Richter in der Provinz Hebei lassen sich von KI bei der Urteilsvorbereitung helfen, die Stadt Hangzhou sagt damit den Verkehrsfluss voraus. In der Provinz Sichuan und in ihrer Jugendliga testet die KP eine ""smarte rote Cloud"" an sich selbst: Der Algorithmus, schreibt die Parteipresse, soll die Bewertung und Auswahl ihrer eigenen Funktionäre modernisieren. Das System verfolge sämtliche Schritte und ""menschliche Beziehungen"" der KP-Kader, um ""ihre zukünftigen Ideen und ihr zukünftiges Verhalten"" vorherzusagen.";https://www.sueddeutsche.de/digital/digitale-ueberwachung-in-china-absolute-kontrolle-1.3849464;sz.de;Kai Strittmatter
30.12.2017;Der überwachte Mensch zensiert sich selbst;"Die brave Bürgerin aus den USA beschlich das Gefühl, einen Fehler gemacht zu haben. Sie hatte sich auf dem Kurznachrichtendienst Twitter kritisch über das Finanzamt geäußert. Aber - werten die Finanzbeamten öffentliche Daten aus? Und entscheiden dann, wen sie einer Steuerprüfung unterziehen? Sicherheitshalber löschte sie ihren Beitrag.

Der niederländische Datenschutz-Aktivist Tijmen Schep erzählte die Anekdote auf der Bühne des 34. Jahreskongresses des Chaos Computer Club (CCC) in Leipzig. Auf dem größten Hackertreffen Europas waren solche Formen der Selbstzensur, die Schep ""Social Cooling"" nennt, ein wiederkehrendes Thema. Die großen Datensammlungen und die Möglichkeiten, sie algorithmisch auszuwerten, beeinflussen das Verhalten der gesamten Gesellschaft.

Das Bewusstsein, dass die Datenpools der sozialen Medien, Verkaufsportale und Überwachungsinstrumente der Behörden Auswirkungen aufs eigene Leben haben könnten, legt sich wie digitaler Raureif über die Gesellschaften. Dieser soziale Druck führt im Big-Data-Zeitalter sowohl zur Selbstzensur, als auch zu einer verminderten Risikobereitschaft.

Schep sieht ""Social Cooling"" als direkte Auswirkung des Geschäftsmodells der Überwachung. Gedanken werden nicht mehr in Handlungen umgesetzt, sondern tiefgefroren, sagte er: ""Ich könnte darauf klicken, aber es könnte nicht gut aussehen. Jemand, der mich überwacht, könnte es sehen, deshalb zögere ich."" Er nannte als messbares Beispiel, dass Wikipedia-Nutzer nach den Snowden-Enthüllungen den Eintrag zu ""Terrorismus"" seltener aufriefen - ein Überwacher hätte ja falsche Schlüsse ziehen können.
In China soll der ""soziale Wert"" aus Daten eines Bürgers Norm werden

Der CCC-Kongress setzte mit ""Social Cooling"" einen wichtigen Schwerpunkt. Und wieder einmal hatte sich der Kongress verändert. Fünf Jahre lang hatte er in Hamburg stattgefunden. Nun steht die große Rakete, das Maskottchen des CCC, bis zum 30. Dezember in der zentralen Halle der Leipziger Messe. 15 000 Eintrittskarten gingen weg, mehr denn je.

Den luftigen, verglasten Hallen fehlen die verwinkelten Ecken des Congress Center Hamburg. Aber auch in Leipzig hält die Szene an Traditionen fest. Über den Köpfen der Teilnehmer zischen blinkende Sendungen durch das selbstgebaute Rohrpostsystem. Im Vorraum der Toiletten sitzen Besucher zusammen, um über Science-Fiction-Welten zu diskutieren. Neben der Bar wird im Sitzkreis besprochen, was im All mit dem menschlichen Körper passiert. Auch in politischen Fragen ist man abgeklärt. Viele sehen sich bestätigt, dass die Überwachungs-Dystopien, von denen sie seit Jahrzehnten sprechen, immer deutlicher zu einer Wirklichkeit werden, die auch die breite Gesellschaft wahrnimmt, und sei es durch die unbewusste Verhaltensveränderung des ""Social Cooling"". Das in China sogar zur Norm werden soll.

Bis auf den letzten Platz ist der größte Saal besetzt, als die Sinologin Katika Kühnreich beschreibt, wie ein Staat eine ganze Gesellschaft einer Konformitäts-Maschine unterwirft. In China weisen mehrere konkurrierende Scoring-Systeme Bürgern einen ""sozialen"" Punktestand zu, der sich aus Online- und Offline-Daten über sie speist. Aus Zahlungsmoral, politischer Aktivität und dem Punktestand, sowohl des Bürgers als auch seiner Freunde, ergibt sich ein Wert. Je nachdem, wie sich die Menschen verhalten, steigt oder sinkt dieser Wert, der den Zugang zu Bildung, Krediten und Konsum regeln soll. Die Systeme bleiben intransparent: ""Wenn ich nicht genau weiß, welches Verhalten welche Bewertung auslöst, verunsichert mich das und ich verhalte mich vielleicht vorsichtiger als vorher."" Um konformes Verhalten durchzusetzen, ist ein spielerischer Ansatz besser als Drohen und Strafen. Erfahrungen aus Computerspielen flossen in die chinesischen Systeme ein. Deshalb seien Orwell'sche Vergleiche falsch, erklärt Kühnreich: ""In ,1984' ging es um Zwang. Hier geht es darum, sich kuschelig zu fühlen."" Solches ""Nudging"" klingt aber nicht ganz so possierlich, wenn es tatsächlich zum Zwang wird: Von 2020 an soll eines der Systeme in China für alle verpflichtend sein. Kühnreich warnt aber vor China-Bashing. Was dort passiere, sei keine Schnapsidee einer fernen Diktatur, sondern Ausdruck eines weltweiten Trends. ""Alle organisierten Gesellschaften sind auf gewissen Ebenen kontrolliert, nicht nur China."" Unternehmen wie Alibaba oder Tencent, die mit dem chinesischen Staat die Systeme bauen, hätten ihre Entsprechungen bei uns in Facebook und anderen Konzernen. Auch wenn man einwenden sollte, dass die chinesischen Konzerne viel staatsnäher sind.

Datenwissenschaftler Hendrik Heuer von der Uni Bremen erklärte in seinem Vortrag, was passiert, wenn Techniken des Maschinenlernens auf Daten aus sozialen Netzwerken losgelassen werden: ""Ein homophober Arbeitgeber muss nicht mehr erfragen, ob jemand homosexuell ist, wenn er es direkt aus einem Facebook-Profil herauslesen kann. Und es interessiert ihn auch nicht, ob er nicht vielleicht einen 'Fehler' macht und einen Heterosexuellen falsch einordnet.""

Auch Schep sagte: ""Dasselbe passiert hier im Westen."" Er demonstrierte das anhand eines Werbevideos des dänischen Start-ups Deemly. Das Unternehmen wirbt mit einer Zeichentrick-Figur, die ihren ""Score"" beim Unternehmen scheinbar gewinnbringend bei Bewerbungen, bei Kreditanträgen oder auf Dating-Webseiten einsetzt. Schep machte sich einen Spaß daraus, das Video zu kommentieren: Immer wenn der Sprecher die möglichen Vorteile des persönlichen Scores anpreist, sagte er dazwischen: ""Wie in China.""
Eine Lösung: Maschinen und Netzwerke einfach anlügen

Was der CCC-Kongress auch dieses Jahr wieder deutlich zeigte war, dass die Hacker-Szene eine wichtige Expertenrolle behält. Das zeigt schon ein Blick in die Programm-App. Hinter kryptischen Begriffen verstecken sich Ereignisse, die im vergangenen Jahr Schlagzeilen machten. Ereignisse, die einen verzweifeln lassen, wie kaputt viele der technischen Systeme sind, auf denen die digitale Welt aufbaut. Zum Beispiel ""Notpetya"", eine Erpresser-Software, die Schaden in Höhe von Hunderten Millionen Euro verursachte. Oder ""PC-Wahl"", Software, die zur Bundestagswahl eingesetzt wurde und fundamental unsicher ist. ""Krack"", eine Schwachstelle im Wlan-Standard. Die IT-Sicherheitsforscher, die hier auftreten, haben die Öffentlichkeit über diese Fälle informiert, auf der Bühne präsentieren sie ihre Analysen. Mal tauchen sie tief in technische Details ein, mal geben sie sich schadenfreudig über die von ihnen aufgespürten Fehler anderer Programmierer. Letztlich bleibt der Befund: Wenn genug Hacker genug Zeit investieren und Software ausgiebig testen, wird die digitale Welt sicherer.

Neben den praktischen Fragen der digitalen Gesellschaft wird auf dem Kongress auch deutlich, wie die Hacker-Szene mit ihrem eigenen Anspruch ringt, eine freiere und bessere Welt zu entwerfen. MehrereFälle sexueller Gewalt haben die Szene aufgeschreckt. Prominente IT-Sicherheitsforscher werden behandelt wie Rockstars, was einige für übergriffiges Verhalten ausnutzten. Erst vor ein paar Wochen wurde bekannt, dass ein berühmter Hacker mehrere sexuelle Übergriffe gestanden hat. Der progressivere Teil der Besucher fragt auf Twitter: Warum wird darüber nicht offen geredet? Dabei ist ""Tuwat"" das Motto des diesjährigen Kongresses.

Konkrete und praktikable Lösungen sind allerdings selten zu hören. Auch nicht zum ""Social Cooling"". Schep forderte eine Sprache, die besser zupackt, um eine breite Öffentlichkeit für Datenschutz zu gewinnen. Auch wer nicht leidenschaftlich für Datenschutz streite, kenne schließlich die beschriebene ""Klickangst"". Wobei viele auf dem CCC-Kongress einen Ausweg ohnehin täglich praktizieren: Sie sind anonym im Netz unterwegs. Man kann die Maschinen und Netzwerke ja auch einfach anlügen, wenn sie Informationen von einem wollen.";https://www.sueddeutsche.de/digital/kongress-des-chaos-computer-club-der-ueberwachte-mensch-zensiert-sich-selbst-1.3808605;sz.de;Jannis Brühl und Hakan Tanriverdi
13.11.2017;Wenn das Pflaster die Wunde überwacht;"Das ""intelligente Pflaster"" überwacht die Wundheilung und meldet Unregelmäßigkeiten per App dem Arzt oder Patienten. Dies ist nur eine der Neuheiten, die gerade auf der Medizinmesse Medica vorgestellt werden.

Ein Handschuh mit Sensoren misst Signale, leitet die Daten an einen externen Netzwerkspeicher weiter. So sollen für bestimmte Epilepsie-Typen Anfälle vorausgesagt werden können. Oder der Chirurg setzt eine 3D-Brille auf, die ihm bei einer Tumoroperation die exakte Position eines Lymphknotens übermittelt.

Die Digitalisierung der Medizinwelt erscheint manchmal wie Science-Fiction. In Arztpraxen, Kliniken oder auf dem Handy des Patienten ist sie oft schon Realität. Zur Bühne für den technischen Fortschritt in der Medizinbranche wird wieder die weltgrößte Medizinmesse Medica (13. bis 16. November) in Düsseldorf mit mehr als 5000 Ausstellern. ""Digitalisierung"" - das Wort ist überall zu hören.

Vernetzung, Clouds, Apps, Big Data, Künstliche Intelligenz und Roboter prägten die medizinische Produktentwicklung immer stärker, verkünden die Veranstalter. Dass der Deutsche Krankenhaustag ein Sonderprogramm in Höhe von einer Milliarde Euro für eine Digitalisierungsoffensive in den Kliniken fordert, wirkt fast wie ein Subtext zu diesem Befund.

Ein Allheilmittel ist die Digitalisierung nach Einschätzung von Experten aber nicht: ""Nicht alles, was digital ist, ist automatisch gut, und nicht alles, was man selber messen kann, ist immer hilfreich"", sagt Corinna Schaefer vom Ärztlichen Zentrum für Qualität in der Medizin (ÄZQ). So könnten ständige Messungen und kleine Abweichungen, die man sonst gar nicht bemerkt hätte, auch für Beunruhigung beim Patienten sorgen, sagt Schaefer mit Blick auf Gesundheitsapps für das Smartphone. ""Bei den wenigen unterstützenden Apps, zu denen man Studien gemacht hat, ist der Nutzen meist nicht nachweisbar oder marginal.""

Auch um das Thema Datensicherheit kommt kein Hersteller herum. Beim Sensor-Handschuh zur Epilepsie-Diagnostik etwa liefen noch Studien zur klinischen Bewertung, auch belastbare Aussagen zur Datensicherheit könnten erst später getroffen werden, sagt Urs-Vito Albrecht von der Medizinischen Hochschule Hannover. Albrecht forscht unter anderem über Chancen und Risiken von Gesundheits-Apps. Die ""üblichen Datensicherheitsrisiken"" für Cloud-Dienste, wie etwa die unerwünschte Auswertung durch Anbieter von Online-Speichern, bestünden auch hier, sagt er.

Neu auf der Medica ist auch eine Ultraschall-App aus dem Google Play Store, die als ""Ultraschall to go"" etwa in der Notfallmedizin an Unfallorten zum Einsatz kommen soll. Die App wird auf dem Smartphone oder Tablet gestartet und ein Schallkopf per USB-Kabel verbunden. Solche Kombinationen aus App und Hardware sind laut Albrecht leicht, oft günstiger und ermöglichen eine schnelle einordnende Diagnostik vor Ort als klassische Diagnosehilfen. Die Sicherheit hänge vom Hersteller ab.

Wie viele Stunden habe ich heute geschlafen? Habe ich einen gesunden Blutdruck? Diese persönlichen Gesundheitsfragen soll künftig der cloud-basierte Sprachdienst ""Alexa"" von Amazon beantworten können. Zunehmend widmeten sich inzwischen fachfremde Akteure kommerziell dem Thema digitale Gesundheit, sagt Albrecht. ""Grundsätzlich wäre gesellschaftlich zu diskutieren, wo denn hier die Grenzen gezogen werden sollen."" Bei Fragen der Diagnostik und Therapie gelten laut Albrecht andere Maßstäbe als für Fitness- und Wellness-Apps. ""Die Menschen haben in der Regel schon ein gutes Gespür dafür, mit welchem Thema sie sich wem anvertrauen oder ausliefern.""

Für Franz Joseph Bartmann von der Bundesärztekammer haben manche sensorischen Messsysteme - ""ob in Handschuh, Schuh oder Unterhemden integriert"" - derzeit eher noch experimentellen Charakter, aber ""für die Versorgung noch keine unmittelbare Relevanz"". Gesundheits-Apps aber werden ""erhebliche Auswirkungen auf das Verhältnis zwischen Arzt und Patient"" haben, sagt Bartmann.

Der Patient sei künftig ""der Herr der Daten"", die er dem Arzt zur Verfügung stelle. Es gebe schon erste Vorhersagen, dass der klassische Hausarzt ""der erste sein könnte, der angesichts dieser Entwicklungen seine Bedeutung verlieren wird"". Für die ältere Generation mit großer Krankheitslast mag der Gang zum Arzt noch eine emotionale Komponente haben. ""Die jungen Leute, die mit digitalen Techniken groß geworden sind, entwickeln, bis es für sie soweit ist, mit Sicherheit eine ganz andere Einstellung"", glaubt Bartmann.

Das gilt im übrigen wohl auch für die Datensicherheit. Corinna Schaefer sagt: ""Ich glaube, dass die meisten Menschen schon jedes Gefühl für die Privatheit von Daten verloren haben."" Für Schaefer ist wichtig: ""Bei medizinischen Interventionen sollte der Arzt den Patienten auch gesehen haben und nicht nur den Computer, der Algorithmen rechnet.""";https://www.sueddeutsche.de/gesundheit/gesundheit-wenn-das-pflaster-die-wunde-ueberwacht-dpa.urn-newsml-dpa-com-20090101-171113-99-845744;sz.de;DPA
08.11.2017;In Zukunft gesund bleiben: Mit Exoskeletten und Genscheren;"Digitale Vernetzung, höhere Rechenkraft und neue Technologien werden in Gesundheitsfragen künftig nicht nur eine große Rolle spielen, sondern einige Felder der Medizin grundlegend verändern. Ein kurzer Überblick zu Technologien und Konzepten. Von A bis Z.

Augmented Reality Der Chirurg blickt im Operationssaal auf den Patienten und hat alle Informationen im Blick - in seiner Digitalbrille erscheinen Vitalparameter und Informationen über den Eingriff. Das Arbeiten mit ""Augmented Reality"" hat er schon im Studium gelernt, in dem er an Hologrammen virtuell erste Schnitte setzte. Derzeit wird in Prototypen-Tests erarbeitet, ob diese ""Vision"" überhaupt nützlich ist - bereits jetzt sind Operationssäle hochtechnisiert und nicht jeder Arzt mag Einblendungen in seinem Gesichtsfeld.

Big Data für Patienten Die Auswertung großer Datenmengen für bessere Diagnosen, genauere Abrechnungen (zum Beispiel nach dem Heilungsverlauf) oder Prophylaxe-Behandlungen, am besten noch mit Hilfe künstlicher Intelligenz, also sich selbst ständig verbessernder Software: Diese Vision für das Gesundheitswesen wird derzeit gerne auf Konferenzen und Podien verkündet.

Die Wahrheit ist aber auch: Das Gesundheitswesen ist bislang kaum digitalisiert und in Sachen Datenschutz stark reguliert. Große Datenmengen mögen in Forschung und Entwicklung von Wirkstoffen eine wichtige Rolle spielen. Bis der Patient als Anwender den Fortschritt selbst spürt, wird noch einige Zeit vergehen.

Crispr/Cas9 Das Erbgut eines Menschen verändern, um Krankheiten ""abzuschalten"" oder bestimmte Fähigkeiten zu verbessern: Diese Genschere ist das bislang effektivste Instrument, Gene zu verändern und könnte die Biomedizin revolutionieren.

Derzeit noch im Stadium der Grundlagenforschung, hat Crispr/Cas9 schon eine laute Debatte über Ethik ausgelöst: Sollte in Deutschland das Embryonenschutzgesetz verändert werden, wenn sich Erbkrankheiten wie Mukoviszidose in der Keimbahn besiegen lassen? Wird die Entwicklung zwangsläufig bei ""Designer-Babys"" enden, deren Eltern sich besondere Merkmale per Gen-Veränderung bestellen? Dabei muss man aber auch bedenken, dass sich nicht alles ""reparieren"" oder ""designen"" lässt: Viele Gen-Zusammenhänge sind komplex, und viele menschliche Krankheiten des 21. Jahrhunderts das Ergebnis von Umweltfaktoren. 3-D-Druck In der Industrie ist 3-D-Druck eine erfolgreiche Nischentechnologie geworden. Und in der Medizin? Bereits heute werden neun von zehn Hörhilfen ""gedruckt"", Prothesen sind immer häufiger gedruckte Maßanfertigungen. Bei Hautgewebe, Blutgefäßen oder ganzen Organen macht die Forschung Fortschritte, allerdings ist das feingliedrige System der menschlichen Durchblutung schwierig nachzuempfinden. Dennoch werden bereits im kommenden Jahrzehnt erste ""gedruckte"" Nieren auf dem Massenmarkt erwartet - allerdings auch, weil ihre Funktionsweise recht einfach ist. Tierschützer hoffen ebenfalls auf 3-D-Gewebedruck, weil so bestimmte Tierversuche überflüssig werden könnten.

Exoskelette Maschinelle Stützapparate greifen bereits heute Menschen ""unter die Arme"": Industriearbeiter und Pfleger tragen ""Kraftanzüge"", also am Körper angelegte Gerüste mit Elektromotoren, die beim Heben die Hauptlast abfedern und so den Rücken entlasten. Schlaganfall-Patienten oder Menschen mit Rückenmarkschädigungen nutzen das Hilfsmittel, um sonst unmögliche Bewegungen durchzuführen oder wieder zu erlernen. Was bislang fehlt, ist der Durchbruch auf dem Massenmarkt: Senioren könnten maschinell gestützt zum Beispiel länger beweglich bleiben. Dafür müssen die Preise für Exoskelette aber deutlich sinken, was angesichts der technischen Entwicklung mittelfristig zu erwarten ist.

Flüssigbiopsie Krebs mit einer Gewebeprobe zu testen, ist manchmal kompliziert und bei einigen Tumoren sogar gefährlich. Eine neue Alternativmethode heißt ""Liquid Biopsy"": Dabei werden winzige Mengen des im Blut zirkulierenden Tumor-Erbguts nachgewiesen. Die Deutsche Gesellschaft für Pathologie sieht bei dieser Methode derzeit allerdings noch zu viele Unsicherheiten, um allein mit ihr verlässliche Aussagen über Therapie und Diagnostik zu treffen.

Gentests In Deutschland ist der direkte Verkauf von Online-Gentests verboten, in den USA gehört eine solche Bestellung in einigen Kreisen schon zum guten Ton: um seine ethnische Herkunft aufschlüsseln zu lassen, Krankheitsrisiken zu erkennen oder den eigenen Diätplan auszurichten.

Allerdings werfen die Ergebnisse oft mehr Fragen als Antworten auf, die Qualität der Tests ist sehr unterschiedlich, das Feld stark reguliert. In den USA darf ein erster Anbieter Screenings für Erbgut-Hinweise auf Parkinson und Alzheimer anbieten. Skeptiker fürchten, dass damit weitgehend unbemerkt das Zeitalter der Standard-Gentests anbricht. Dann könnten Krankenversicherungen bei der Bemessung ihrer Beiträge Genrisiken einbeziehen.

Chips im Gehirn und digitale Liebe - welche Zukunft möchtest du? Mach den Test.

Heimdiagnosen Schon jetzt googelt fast jeder Krankheiten und Symptome, sobald er sich unwohl fühlt. Doch wie realistisch sind tiefergehende Heimdiagnosen, die über die Auswertung des eigenen Fitness-Trackers hinausgehen? Anwendungen wie das Arztgespräch via Videoschalte werden auch angesichts des Ärztemangels auf dem Land gute Chancen eingeräumt.

Anbieter für vernetzte Messgeräte werben damit, dass aus dem Krankenhaus entlassene Patienten ihren Gesundheitszustand überprüfen und dann ein Arzt per Video die Daten auswertet. Das ist noch keine Heimdiagnose - doch dass Ärzte von Fitness-Trackern oder Algorithmen abgelöst werden, erwarten ohnehin nur die überzeugtesten Technologie-Gläubigen. Künstliche Intelligenz Bereits jetzt kann lernende Software Röntgenbilder analysieren (wenn auch noch nicht perfekt). Künftig werden Algorithmen zur Unterstützung bei der Diagnose und die Auswertung von Medizindaten selbstverständlich sein. Im Zeitalter des selbstoptimierten vernetzten Menschen soll künstliche Intelligenz auch irgendwann in Echtzeit Werte wie Herzfrequenz, Blutwerte oder Atem analysieren, um Krankheiten schneller zu erkennen (oder dezente Tipps für einen gesünderen Lebenswandel zu geben). Darauf bauen zumindest die Firmen, die solche Analyse-Systeme entwickeln wollen.

Mensch-Maschine-Schnittstelle Der Traum vom Cyborg lebt, doch wie realistisch ist er? Obwohl Hirnprothesen bereits heute mehr als 100 000 Patienten helfen, die Symptome von Parkinson oder Depressionen zu lindern, liegt die Vernetzung von Mensch und Maschine noch in weiter Zukunft.

Die Herausforderung ist gewaltig: Eine Schnittstelle zwischen Gehirn beziehungsweise Rückenmark und Computer müsste nicht nur Hirnaktivität zuverlässig in Digitalbefehle übersetzen, sie müsste auch klein, tragbar und infektionssicher sein. Am ehesten sind dort Fortschritte zu erwarten, wo ein Implantat bestimmte Signale einfach unterdrücken muss, zum Beispiel in der Schmerztherapie. Mit der Vorstellung vom Cyborg hat das aber nicht viel zu tun.

Nanomedikamente Einige Medikamente der Zukunft arbeiten mit Partikeln in der Größe von einem bis 100 Nanometer (zum Vergleich: ein Menschenhaar hat einen Durchmesser von 10 000 bis 50 000 Nanometern). Diese winzigen Körner können über Oberflächen, die an die Rezeptoren angepasst sind, Wirkstoffe direkter als bislang in Zellen einschleusen und die Blut-Hirn-Schranke überwinden. Das Resultat: Neue Anwendungen, eine bessere Wirkung und weniger Nebenwirkungen. So die Theorie. In der Praxis gibt es gerade in der Tumor-Behandlung erste Versuche mit Nano-Transportsystemen, aber sowohl Nebenwirkungen als auch ideale Stoff-Wirkstoff-Kombinationen müssen noch erforscht werden. Bis zur Massentauglichkeit wird es dauern.

Open Medicine Der Begriff wird in zwei Zusammenhängen verwendet: Einmal ""offene Medizin"" im Sinne eines freien öffentlichen Zugangs zu staatlich geförderten Untersuchungen und Studien inklusive Datensätzen. Die zweite Bedeutung beschreibt eine kollaborative, dezentrale Arbeit an pharmazeutischen Entwicklungen, um ""Medikamente für alle"" ohne die Pharmabranche zu entwickeln - zum Beispiel jüngst das Design für einen Insulin-Pen zum Selberbauen.

Diese Bewegung der ""offenen Medizin"" nimmt gerade in den USA einen ähnlichen Weg wie die ""Do-It-Yourself""-Biotechnologie, die das Labor der Zukunft in der eigenen Garage sieht. Die Risiken sind in einem Feld wie der Medizin riesig, die Chancen auf eine ""Revolution"" gering: Medizin ist keine Software, teuer zu entwickeln und stark reguliert. Pflegeroboter Das techikaffine Japan gilt als Vorzeige-Nation der ""Assistenzrobotik"". In der Altenpflege testet man dort bereits seit Jahren Roboter in der Demenz-Therapie, zur Unterstützung von Pflegern oder als Helfer für gelähmte Menschen.

Das Feld entwickelt sich stetig, doch Roboter sind noch nicht filigran genug, um komplexe, spontane oder feingliedrige Aufgaben zu übernehmen. Eine Pflege-Revolution steht also noch nicht unmittelbar an (und hat auch ethische Fallstricke). Betroffene mit Bewegungseinschränkungen können allerdings schon von der Automatisierung kleiner Aufgaben wie dem Einschenken eines Glases Wasser profitieren.

Regulierung Wenn Berater über künftige Medizintechnologien sprechen, klingt das oft nach ""Freigabe erfolgt übermorgen"". In Wahrheit sind Medizinprodukte stark reguliert, viele Hype-Themen von heute in Wahrheit noch im Stadium der Grundlagenforschung und mehr als ein Jahrzehnt von größerem Einsatz entfernt.

Allerdings erkennen auch die Zulassungsbehörden die Chancen, die in den neuen Technologien stecken - die US-Gesundheitsbehörde Federal Drug Adminstration (FDA) hat vor einigen Monaten eine eigene Abteilung für Digital Health gegründet, die Tests und Zulassungen beschleunigen soll.

Tricorder Bordärzte auf dem Deck der Star-Trek-Raumschiffe nutzten ein kleines Messgerät, das in Bruchteilen von Sekunden Krankheiten diagnostizieren kann. Zwar spielt die Serie im 23. Jahrhundert, doch bereits seit Jahrzehnten versuchen Bastler, Firmen und die Nasa, die Tricorder-Idee umzusetzen.

Der Chiphersteller Qualcomm unterstützt zwei Firmen, die einen Prototypen vorgelegt haben, der zwölf Krankheiten (von Anämie über Lungenentzündung bis Diabetes) und fünf Vitalwerte (von Blutdruck bis Sauerstoffsättigung) direkt messen kann. Ob das Gerät allerdings jemals massentauglich wird, ist trotz der Science-Fiction-Begeisterung unter Wissenschaftlern noch nicht gesagt.

Umgebungsintelligenz Von der Industrie bis zu den eigenen vier Wänden wird derzeit alles mit vernetzten Sensoren ausgestattet. Das Internet der Dinge wird in diesem Zuge auch Notfall-Systeme und Prävention verändern. Bereits jetzt statten Autohersteller ihre Fahrzeuge mit Systemen aus, die erkennen können, wie schläfrig der Fahrer gerade ist.

Senioren sollen künftig in Zimmern leben, die ebenfalls Alarm schlagen können, wenn sich die Person plötzlich anders verhält (zum Beispiel: eine Person liegt Stunden nach der normalen Aufwach-Zeit noch im Bett). Und selbst Tabletten könnten über ausscheidbare Sensoren Messergebnisse zu ihrer Wirkung aus der Darmflora senden. Die größten Probleme dabei: Preis und Zuverlässigkeit (Senioren-Sensoren) sowie Miniaturisierung (verdaubare Sensoren).

Wearables Vernetzte tragbare Elektronik - wie etwa Smartwatches und Fitness-Bänder - verkauft sich, wenn auch in überschaubarem Rahmen. Die Entwicklung wird nicht stehen bleiben: Textilien mit Sensoren, derzeit vor allem von Sportlern genutzt, sollen künftig immer stärker über die Haut Vital-Daten oder Umwelt-Informationen sammeln. Sie könnten dann zum Beispiel den Insulin-Spiegel messen oder den korrekten Lichtschutzfaktor vorschlagen.

Patient Z Von Marktforschern manchmal abgeleitet von ""Generation Z"" verwendet, also den zwischen 1995 und 2010 Geborenen. Mit digitaler Technologie aufgewachsen muss ""Patient Z"" nicht davon überzeugt werden, für Gesundheits-Prophylaxe Smartphones oder andere vernetzte Tracking-Geräte zu verwenden.

Im Gegenteil: Die Ansprüche bei Themen wie digitaler Portabilität von Krankenakten oder der Bewertung von Ärzten sind höher als die der Vorgänger-Generationen. Das ist zumindest die Theorie der Marktforscher.";https://www.sueddeutsche.de/digital/doku-reihe-homo-digitalis-in-zukunft-gesund-bleiben-mit-exoskeletten-und-genscheren-1.3737031;sz.de;Johannes Kuhn
06.10.2017;Es muss nicht immer nur Anwalt sein;"Die juristische Fakultät der Uni Regensburg, Mitte der Neunzigerjahre. Sechs Erstsemester sitzen gemeinsam in ihrer ersten Vorlesung. 20 Jahre später haben alle ganz unterschiedliche Karrieren gemacht. Einer ist heute Finanzbeamter, einer ist Rechtsanwalt, einer Richter, einer arbeitet in einem Ministerium, einer in einem Landratsamt und der Sechste in einem großen Unternehmen.

Nur eine Geschichte von vielen, aber doch typisch für Juristen. Denn so streng ihr Studium geregelt sein mag, so vielfältig sind die Wege, welche die Absolventen später beschreiten. Zwar wird der überwiegende Anteil der Volljuristen, wie Rechtswissenschaftler mit zwei Staatsexamina heißen, tatsächlich Anwalt. Doch ""dem Absolventen steht eine Vielzahl von Möglichkeiten offen"", sagt Andreas Nadler, Generalsekretär des Vereins Deutscher Juristentag in Bonn.

Richter und Staatsanwälte können sie werden oder in die Rechtspflege von Behörden aller Art gehen. Jobs gibt es auch in der Rechtsabteilung von Unternehmen, bei Verbänden, Wirtschaftsprüfern oder Steuerberatungskanzleien sowie in Lehre und Forschung. Und das sind nur die klassischen Berufe. Denn es gibt auch immer mehr Absolventen, die sich weitere Qualifikationen aneignen und sich als sogenannte Bindestrich-Juristen etwa auf Wirtschaftsrecht spezialisieren oder sich technisch ausbilden lassen. ""Die Digitalisierung kommt auch in der Rechtsbranche an"", sagt Micha-Manuel Bues. Der promovierte Jurist ist Geschäftsführer des Unternehmens Leverton in Berlin. Das Start-up entwickelt Software, mit der sich zum Beispiel Verträge auslesen und so Rechtsfragen digital bearbeiten lassen. Legal Tech nennt sich diese relativ neue Branche.

""Für diese Arbeit brauchen wir Leute, die juristischen und technischen Sachverstand mitbringen"", sagt Bues. Dieses Querschnittswissen müssen sich Juristen allerdings meist selbst zusammensuchen. Denn nur an wenigen Hochschulen gibt es Lehrveranstaltungen für Juristen in Bereichen wie Data Science oder Informatik.

Auch das Projektmanagement wird künftig ein wichtiger Arbeitsbereich für junge Absolventen sein, glaubt Bues: ""Immer mehr Fälle werden in Projekte aufgeteilt und bearbeitet, um Prozesse zu beschleunigen und zu vereinfachen."" Nicht nur in großen Kanzleien und Wirtschaftsprüfungsgesellschaften werde Effizienz immer wichtiger. ""Auch in der Verwaltung wird man das brauchen, um Geld einzusparen."" Für diesen Bereich reiche in vielen Fällen das Diplom der Wirtschaftsjuristen oder das erste Staatsexamen. Welcher Bereich der Rechtswissenschaften und welche Art von Arbeitgeber für sie ideal sind, sollten sich angehende Juristen schon während des Studiums klarmachen - mit Praktika. ""Das Studium bietet mit zwei Pflichtpraktika in der Rechtspflege und Verwaltung bereits die Chance, Einblicke in unterschiedliche Berufszweige zu erhalten"", sagt Nadler. Auch das Referendariat vor dem zweiten Staatsexamen kann sinnvolle Orientierung sein.

Manchmal kommt es dann nach dem Studium aber doch anders, als man denkt - was häufig mit den Examensnoten zu tun hat. Wer zwei Mal mit ""vollbefriedigend"" aus Klausuren und mündlicher Prüfung geht, kann sich den Job fast aussuchen. Wer dagegen nur einmal oder gar nicht die magischen neun Punkte schafft, hat es nicht ganz so leicht. Eventuell lohnt sich dann eine weitere Qualifikation: Zum Beispiel mit einer Promotion oder einem der angelsächsischen Titel ""Master of Laws"" (LL.M.), ""Master of Comparative Jurisprudence"" (M.C.J.) oder ""Master of Business Law"" (M.B.L.).

Doch es gibt auch für die Vielzahl der Absolventen, die nicht zu den Top-Juristen ihres Jahrgangs gehören, eine breite Auswahl von Arbeitgebern. ""Verbände, Kammern, die Verwaltung des Deutschen Bundestags, das Auswärtige Amt, die Bafin, die GIZ oder das Bundeskartellamt, europäische oder internationale Organisationen haben immer Bedarf an jungen Juristen und können ein hervorragendes Karrieresprungbrett sein"", sagt Christoph Wittekindt, Leiter der Personalvermittlung Legal People Germany. Und auch der Staat bietet zahlreiche Arbeitsbereiche, an die man nicht gleich denkt. Andreas Nadler nennt etwa die Option, als Verwaltungsjurist bei Ministerien und Behörden zu arbeiten - darunter etwa das Bundeskriminalamt und der Bundesnachrichtendienst, die Bezirksregierungen, die Polizei oder die Kommunalverwaltung. Wer auf das große Geld aus ist, muss im Studium aber hervorragende Leistungen bringen: ""Generell gilt: Je besser die Noten, desto höher das Gehalt"", sagt Wittekindt. In großen Kanzleien verdienen Einsteiger zwischen 75 000 und 140 000 Euro, dafür brauchen sie allerdings zwei sehr gute Examina und am besten einen Doktor oder einen entsprechenden internationalen Titel sowie Fremdsprachenkenntnisse. Zum Vergleich: In einer kleineren Kanzlei steigt ein Jungjurist mit einem Jahresbruttogehalt von 40 000 bis 60 000 Euro ein.";https://www.sueddeutsche.de/karriere/arbeitsmarkt-fuer-juristen-es-muss-nicht-immer-nur-anwalt-sein-1.3684070;sz.de;Verena Wolff
17.08.2017;Social Bots enttarnen: Diese acht Punkte helfen;"Von Mensch zu Mensch - das war einmal: US-Forscher gehen davon aus, das allein bei Twitter bis zu 15 Prozent der Accounts automatisch von Computer-Software mit Tweets beschickt werden. Diese Bots von Menschen aus Fleisch und Blut zu unterscheiden, ist inzwischen überraschend schwierig.

Wenn die Bots nicht ganz plump programmiert sind, sehen viele Bot-Profile auf den ersten Blick wie ganz normale Nutzer aus. Erkennungsprogramme versagen. Was derzeit bleibt, sind der gesunde Menschenverstand und Indizien, die helfen können, Bots bei Facebook, Twitter und Co zu enttarnen:

- Seriosität: Zunächst sollte man prüfen, wer dem angeblichen Account-Inhaber überhaupt folgt. Denn Bot und Bot gesellt sich gern. Hilfreich kann es auch sein, Profilbild und -beschreibung genauer unter die Lupe zu nehmen: Ein aus dem Netz kopiertes Foto ist ebenso verdächtig wie eine fehlende oder sinnlose Profilbeschreibung, informiert das von der Landesmedienanstalt Nordrhein-Westfalen mitgetragene Medienportal ""Handysektor.de"".

- Inhalte: Indizien für einen Bot-Account können von Thema, Tenor oder Quellenverweis her immer ähnlich lautende Post sein. Bots posten zudem oft sehr viele Inhalte, führen aber kaum Dialoge oder stören solche gezielt, etwa mit Beleidigungen oder Provokationen. Verdächtig sind auch seltsamer Satzbau oder wiederkehrende Grammatikfehler.

- Likes und Follower: Verteilt ein Account massenhaft Likes, kann das ""Handysektor.de"" zufolge ein weiteres Indiz für eine Bot-Tätigkeit sein. Umgekehrt ernten Bot-Posts oft kaum Likes oder Kommentare.

- Aktivität: Mehrere Dutzend Posts am Tag - können die von einem einzigen Menschen stammen? Regelmäßig wird in diesem Zusammenhang die Zahl 50 genannt: Ab dieser Zahl Postings pro Tag soll man es wahrscheinlich mit einem Bot zu tun haben. ""Das ist natürlich eine beliebige Definition. Es gibt auch Menschen, die so oft posten"", sagt der Wirtschaftsinformatiker Christian Grimme von der Uni Münster. ""Daran allein kann man es nicht festmachen."" Auf der Suche nach Gewissheit könne man etwa auch schauen, ob der Account einen menschlichen Tag-Nacht-Zyklus verfolgt. ""Aber selbst das reicht nicht aus.""

- Reaktionszeit: Bots können rasend schnell reagieren, weil sie rund um die Uhr das jeweilige soziale Netzwerk nach den vom Programmierer vorgegeben Schlüsselwörtern oder Hashtags durchsuchen. Ganz plakativ zeigt das auf Twitter etwa der bekannte Bot-Account Pfannkuchenpolizei. Schreibt jemand in einem Beitrag das Wort ""Berliner"", meldet der Bot sich umgehend mit einem Hinweis, dass der Berliner in Berlin nun einmal Pfannkuchen heißt.

- Weiterentwicklung: Es gibt aber längst Bots, deren Entwickler versuchen, bekannte Erkennungsmerkmale zu vermeiden. Einige haben echte Profilbilder, setzen absichtlich nicht zu viele und nicht zu wenige Nachrichten ab, folgen nicht beliebig oder simulieren in ihren Posts sogar menschliche Tagesabläufe, Denkpausen oder Nachtruhe, um nicht aufzufallen. Automatisch lassen sich diese Bots oft nicht zuverlässig erkennen, sagt Christian Grimme, der das Projekt Propstop leitet, das Propaganda-Angriffe über Online-Medien untersucht.

- Prüfseiten: Von Social-Bot-Prüfseiten à la Botometer (Indiana University) oder Debot (University of New Mexico), die per Mustererkennung arbeiten, hält Grimme nicht viel: Im Rahmen des Propstop-Projektes haben die Wissenschaftler ""unauffällige"" Bots gebaut und die Accounts zur Prüfung auf den Seiten angegeben. ""Diese Verfahren haben auch bei unseren Bots weitgehend versagt"", fasst Grimme die Ergebnisse zusammen.

Die Erkennungsraten hätten bei rund 50 Prozent gelegen. ""Mit dieser Information kann ich natürlich nichts anfangen, ich muss mich dann doch hinsetzen und mir den Account selber angucken"", sagt der Informatiker. Einfach gestrickte Bots identifizierten die Prüfseiten relativ leicht. Das schaffen Menschen meist aber auch.

- Bot-Armeen: ""Von der technischen Seite ist es wichtig zu bedenken, dass diese Bots im Prinzip beliebig skalierbar sind: Wer ein Programm hat, mit dem sich ein Bot steuern lässt, kann damit auch eine ganze Armee von Bots lenken"", schreibt Simon Hegelich, Professor für Political Data Science an der TU München in einem Paper. Solche Bot-Heere sind im Netzwerk Twitter bereits entdeckt worden.

Eine hochwertige Software, mit der sich ein Verbund von bis zu 10 000 Twitter-Accounts steuern lässt, ist Hegelich zufolge für 500 US-Dollar (rund 425 Euro) zu haben. Fehlen nur noch Konten für die Bots. Aber auch die sind Hegelich zufolge käuflich: 1000 gefälschte Konten kosten zwischen 45 US-Dollar (38 Euro) für einfache Twitter-Accounts und 150 US-Dollar (128 Euro) für ""gealterte"" Facebook-Accounts.";https://www.sueddeutsche.de/wissen/technik-social-bots-enttarnen-diese-acht-punkte-helfen-dpa.urn-newsml-dpa-com-20090101-170814-99-640159;sz.de;DPA
17.07.2017;Welche Risiken Online-Umfragen bergen;"Dirk Wildt, Vorsitzender der Grünen im Kreisverband Passau-Land, hat seit ein paar Monaten ein besonderes Hobby: Er manipuliert gezielt Online-Umfragen von Medien, zuletzt nahm er sich Ende Juni den Bayerischen Rundfunk vor. Bayern 2 hatte in einem Online-Artikel gefragt, ob es vertretbar ist, dass die Bundesregierung trotz instabiler Sicherheitslage wieder einen Flug mit abgelehnten Asylbewerbern nach Afghanistan plant. Wildt gab etwa 120 Neinstimmen (von insgesamt etwa 2500) ab und stärkte damit nach eigenen Angaben diese Position. Am Ende siegten die Gegner solch eines Fluges deutlich. Wildt informierte den BR-Intendanten Ulrich Wilhelm und reichte eine Programmbeschwerde ein. Der Artikel ist inzwischen offline.

Man muss die Aktion von Dirk Wildt, der einst selbst Journalist bei der taz war und heute mit seiner Firma Netzmacher auch ""Internetdienstleistungen"" anbietet, wohl als eine Art medienpolitische Kunstaktion begreifen, mit der er auf ein Thema aufmerksam machen will: Viele Online-Medien starten regelmäßig Umfragen zu allen möglichen Themen. Umfragen und Rankings halten Leser länger auf ihren Webseiten, weil diese erst abstimmen und danach in der Regel auch die Ergebnisse erfahren wollen.

Meist stellen die Online-Medien harmlose Fragen. Nach dem Lieblings-Tatort-Kommissar, dem beliebtesten Fußballverein oder ob man gerne Gin Tonic trinkt. Aber immer wieder wird auch nach politisch brisanteren Themen gefragt: ""Schulz vs. Merkel - wer macht das Rennen?"", fragt etwa bild.de und Antenne Bayern: ""Braucht Bayern eine Impfpflicht?"" Dirk Wildts Punkt: Wenn sich die Ergebnisse dieser Umfragen so leicht manipulieren lassen, könnte das problematisch sein, gerade vor der anstehenden Bundestagswahl im September.
Mit Journalismus habe das nichts zu tun, sagen Kritiker: Medien machten sich damit angreifbar

""Mit seriösem Journalismus haben solche Umfragen wenig bis gar nichts zu tun"", sagt Wildt am Telefon. ""Sie können keine repräsentative Aussage liefern, höchstens darüber, wer besser mobilisiert, ob Einzelpersonen oder Gruppen."" Medien machten sich damit angreifbar: ""Die Bild-Zeitung ließ vor Kurzem in der Sonntagsfrage abstimmen, für welche Partei man bei der Bundestagswahl stimmt. Als die AfD bei mehr als 50 Prozent war, wurde die Umfrage gelöscht - und von AfD-Seite kam der Vorwurf, dass die Bild das Ergebnis vertusche."" Besonders kompliziert ist so eine Manipulation nicht: Bereits zu Jahresbeginn hat Wildt eine Online-Umfrage eines niederbayerischen Anzeigenblatts zugunsten von SPD-Kanzlerkandidat Martin Schulz beeinflusst, im Mai gab er in einer Umfrage der Online-Ausgabe des Münchner Merkur zum Bau einer dritten Startbahn am Münchner Flughafen innerhalb einer Stunde mehr als 180 Gegenstimmen von insgesamt knapp 5000 ab und erhöhte den Anteil der Gegenstimmen von 54 auf 58 Prozent. Bis zu 30-mal konnte er abstimmen, als er im Browser das Speichern von Cookies deaktiviert hatte, jener kleinen Programme, mit denen Webseiten Nutzer bei ihrem nächsten Besuch wiedererkennen. Wenn er seinen Router neu startete, konnte er mit neuer IP-Adresse erneut 30-mal abstimmen. Süddeutsche.de erstellt seine Online-Umfragen gemeinsam mit dem Meinungsforschungsinstitut Civey, das sowohl technisch als auch durch statistische Methoden solche und ähnliche Manipulationsmöglichkeiten minimiert. Der Grünen-Politiker reichte eine Beschwerde beim Presserat ein und kritisierte: ""Ein journalistisches Umfeld erhöht die Glaubwürdigkeit von Online-Umfragen, manipulierbare Online-Umfragen allerdings gefährden die Glaubwürdigkeit des journalistischen Umfelds."" In einem E-Mail-Verkehr, den Wildt im Internet veröffentlichte, verglich Armin Linder, Chef vom Dienst von Merkur.de und tz.de, die Online-Abstimmungen mit Straßenumfragen. Doch anders als Straßenumfragen, die persönliche Statements von ein paar Bürgern spiegeln, kann eine Prozentzahl auf einer journalistischen Webseite beinahe den Eindruck eines Bürgerentscheids erwecken. Dazu kommt, dass nicht alle Medien bei eigenen Umfragen anmerken, dass diese oft nicht repräsentativ sind.

Auch in der Forschung sieht man Online-Tools durchaus kritisch: ""Offene Online-Umfragen sind beliebig manipulierbar"", sagt Simon Hegelich, Professor für Political Data Science an der TU München. Dass sich Leute absprechen und bewusst mehrfach an einer Umfrage teilnehmen, könne man auch gar nicht verhindern: ""Das konnte man noch nie."" Das Internet macht derlei Stimmungsmache noch einfacher. ""Man darf nie vergessen: Die Menschen handeln online immer anders als im echten Leben"", sagt Hegelich. ""Und sie reagieren auch anders auf eine Umfrage, als wenn sie sich wirklich damit beschäftigen.""

Problematisch sei Hegelich zufolge auch, wen man erreicht: Die meisten Nutzer beteiligten sich kaum oder wenig, nehmen vielleicht einmal in drei Monaten an einer Abstimmung teil. Demgegenüber stehe eine kleine Gruppe von Meinungsmachern, die sich extrem stark beteiligt. ""Die Leute, die regelmäßig an Abstimmungen teilnehmen, sind immer dieselben"", sagt Hegelich. Oft werden in Online-Umfragen nur gut hundert Stimmen abgegeben, manchmal auch ein paar Tausend, dadurch können einzelne, die mehrfach abstimmen, schnell starkes Gewicht bekommen.
Bei der Petition für ein zweites Brexit-Referendum gaben Bots Zehntausende Stimmen ab

Hegelich warnt generell davor, von der Online-Welt auf die echte zu schließen: ""Alle Online-Bewertungen sind systematisch verfälscht. Wenn ich daraus, dass der Hashtag #schulz öfter als #merkel verwendet wird, schließe, dass Schulz wirklich populärer als Merkel ist, bin ich womöglich auf lauter Bots hereingefallen."" Dass solche Programme automatisch und in großem Stil abstimmen, könne man Hegelich zufolge ohne großen technischen Aufwand verhindern. Dies wird aber nicht bei allen Online-Petitionen gemacht. 2016 wurden bei der Petition für ein zweites Brexit-Referendum Zehntausende Stimmen von Bots abgegeben und so das Ergebnis verfälscht.

Obwohl Online-Umfragen seit Jahren beliebt sind, ist Wildts Beschwerde laut Presserat die erste zu dem Thema. Dort befindet sich die Beschwerde auf Nachfrage noch in der Vorprüfung und soll am 14. September in einem Beschwerdeausschuss behandelt werden - zehn Tage vor der Bundestagswahl. So lange versucht Wildt, die Medien direkt anzugehen. Den BR forderte er nicht nur auf, die Umfrage zu löschen, sondern grundsätzlich auf manipulierbare Umfragen zu verzichten. Intendant Wilhelm antwortete Wildt in einem Brief, der der SZ vorliegt, man werde sich ""im Vorfeld der Bundestagswahl generell mit politischen Umfragen zurückhalten"".";https://www.sueddeutsche.de/medien/meinungsforschung-mitmachen-und-gewinnen-1.3589754;sz.de;Kathrin Hollmer
19.06.2017;Neue Studiengänge: Von Data Science bis Biomassetechnologie;"Masterstudiengang Data Science in Berlin

Die Beuth Hochschule für Technik in Berlin richtet zum Wintersemester 2017/18 den neuen Masterstudiengang Data Scienceein. Das englischsprachige Programm dauert vier Semester, teilt die Hochschule mit. Data Scientists analysieren Daten und entwickeln Systeme für maschinelles Lernen. Auf dem Weg zur Abschlussarbeit können Studenten innovative Ideen für Start-ups oder zusammen mit Industrieunternehmen entwickeln. Bewerbungsschluss ist am 15. Juli.

Studiengang Werkstoffe und Oberflächen in Südwestfalen

Die Fachhochschule Südwestfalen startet den Bachelorstudiengang Werkstoffe und Oberflächen. Er beginnt erstmals zum Wintersemester 2017/2018 und dauert sechs Semester, teilt die Hochschule mit. Studierende stellen sich etwa die Frage, woraus ein Autolack bestehen muss, damit er optisch gefällt und gleichzeitig der Witterung trotzt. In den ersten drei Semestern geht es um die mathematischen, ingenieurwissenschaftlichen und naturwissenschaftlichen Grundlagen. Später werden dann technische Mechanik, Werkstoffkunde oder Fertigungsverfahren behandelt. Bewerbungsschluss ist am 15. Juli.

Online-Studiengang regenerative Energien

Die Jade Hochschule Wilhelmshaven und die Hochschule Emden/Leer bieten einen neuen Online-Bachelorstudiengang an. Er befasst sich mit regenerativen Energien. Start des sechs Semester dauernden Studiums ist voraussichtlich das Wintersemester 2017/2018, teilen die Hochschulen mit. Schwerpunkt ist die Arbeit in Unternehmen aus dem Bereich Energietechnik. Absolventen erwerben in einem Teil- oder Vollzeit-Studium den Bachelor of Engineering. Die Selbstlernphasen werden ergänzt durch intensive Präsenz- und Laborphasen an den Studienorten Emden oder Wilhelmshaven. Interessenten können sich bis 1. September bewerben.

Master für Biomassetechnologie

Die Technische Universität München bietet neue Studiengänge im Bereich ""Nachwachsende Rohstoffe"" an. Dazu gehört der Masterstudiengang Biomassetechnologie, ein gemeinsames Projekt mit der Universität für Bodenkultur in Wien. Das vier Semester dauernde Studium beginnt im Wintersemester 2017/2018, teilt die Hochschule mit. Die Vorlesungen finden in Straubing und in Wien statt. Bewerber brauchen einen Bachelorabschluss in Natur- und Ingenieurwissenschaften, Agrar-, Forst- oder Wirtschaftswissenschaften. Der Masterabschluss ermöglicht eine Karriere als Ingenieur, Chemiker oder Betriebswirtschaftler beispielsweise bei Energieversorgern oder im Umweltschutz.

Master Afrikanische Sprachen in Bayreuth

Die Universität Bayreuth bietet einen neuen Master African Verbal and Visual Arts. Der englischsprachige Studiengang dauert vier Semester und beginnt im Wintersemester, teilt die Universität mit. Im Mittelpunkt des Studiums stehen Sprachen, Literatur, Kunst und Medien in Afrika. Studenten können unter anderem die Sprachen Bambara, Hausa, Swahili oder Arabisch lernen. Bewerber sollten Vorkenntnisse in der afrikanischen Kultur oder Literatur mitbringen. Absolventen qualifizieren sich mit dem Master für die Arbeit an internationalen Forschungseinrichtungen, im Journalismus oder in der Entwicklungszusammenarbeit. Bewerbungsschluss ist der 15. Juli. ";https://www.sueddeutsche.de/karriere/arbeit-neue-studiengaenge-von-data-science-bis-biomassetechnologie-dpa.urn-newsml-dpa-com-20090101-170526-99-604604;sz.de;DPA
16.05.2017;Mit dieser App steuert die CDU ihre Wahlhelfer;"Schon um kurz nach 18 Uhr, Wahlsonntag in NRW, strahlte Marco Schmitz über das ganze Gesicht. Er hatte sich im Wahlkreis 41 (Düsseldorf II) für ein Direktmandat beworben und die ersten Prognosen verhießen Gutes: einen Sieg von CDU-Spitzenkandidat Armin Laschet, was allen Christdemokraten nutzt. Doch Schmitz, Jahrgang 1979, war noch aus einem anderen Grund optimistisch: Er hat mit seinem Team an mehr als 5000 Haustüren geklopft, sich den potenziellen Wählern vorgestellt und sie gebeten, für ihn und die CDU zu stimmen.

""Das kam sehr gut an, denn die Bürger haben nicht oft Kontakt mit Politikern"", berichtet Schmitz. Den Weg gewiesen hat ihm die App Connect17, die die Bundes-CDU mit der Jungen Union entwickelt hat. Sie erstellt eine Potenzialanalyse und schickt Bewerber wie Schmitz in Straßenzüge, in denen mögliche Sympathisanten leben: ""Die Wahrscheinlichkeit, dass jemand CDU wählt, muss über 60 Prozent liegen."" Die Strategie ist die gleiche wie in den swing states der USA: Man tut alles, um die eigenen Anhänger zu mobilisieren, lässt sich aber nicht in lange Diskussionen verwickeln und versucht nicht, Andersdenkende zu überzeugen. ""Get Out the Vote"" heißt der Ansatz und Politologen wie Donald Green fanden schon 2004 in Tests heraus, dass E-Mails oder Anrufe wenig Wirkung zeigen. ""Es gibt tatsächlich kaum etwas Effektiveres, als wenn Nachbarn oder Bekannte vor deiner Tür stehen und für ihren Kandidaten werben"", sagte Green 2013 in einem Interview.

Connect17 übernimmt nun die Innovationen des US-Präsidentschaftswahlkampfs 2016: Das Smartphone ist das wichtigste Instrument der Aktivisten und kann die Daten schnell an die Zentrale schicken. Seit Dezember 2016 gibt es die App, auch Annegret Kramp-Karrenbauer im Saarland und Daniel Günther in Schleswig-Holstein setzten bei ihren Kampagnen auf Haustür-Wahlkampf und waren damit bekanntermaßen erfolgreich.

An 75 000 Türen wurde im Saarland geklopft - das entspricht fast jedem fünften CDU-Wähler. Wahlsieger Günther sei vergangene Woche aus dem Norden nach NRW gereist und habe die Helfer mit den Worten ""Macht weiter, es wirkt"" angefeuert, erzählt CDU-Kandidat Schmitz. Für ihn hat es sich gelohnt: Mit knapp 2000 Stimmen Vorsprung lag er am Ende vor dem SPD-Bewerber und sitzt künftig im Düsseldorfer Landtag.
Vom ""Lehrling"" zur ""Wahlkampf-Legende""

Aus den USA hat die CDU auch den Aspekt der Gamification übernommen: Wer sich als Unterstützer angemeldet hat, bekommt für diverse Aktivitäten Punkte gutgeschrieben. Der Wettbewerbsgedanke sporne an, sagt Schmitz: ""Jeder kann sehen, wo er in der Rangliste steht."" Drei Optionen sind auf dem Smartphone-Bildschirm zu sehen: Bei ""Tür zu Tür"" wird belohnt, wer mit möglichst vielen Bürgern an der Haustür spricht, unter ""Social Media"" gibt es Punkte für entsprechende Facebook-Posts oder verschickte SMS an Bekannte und bei ""Unterstützer"" kann man neue Wahlhelfer registrieren. Ähnlich wie bei Trumps ""America First""-App (Details in diesem SZ-Artikel) kann der Wahlhelfer so seinen Status verbessern: Aus dem ""Lehrling"" kann eine ""Wahlkampf-Legende"" werden und die zehn fleißigsten Unterstützer mit den meisten Punkten erhalten nach der Bundestagswahl einen Anruf von Angela Merkel persönlich.

Etwa 4000 Mal wurde die App heruntergeladen, berichtet JU-Bundesgeschäftsführer Conrad Clemens, der das Projekt im Konrad-Adenauer-Haus betreut. Eine Daten-Auswertung zeige, dass das Ergebnis der CDU in jenen Wahlkreisen um zwei bis zweieinhalb Prozentpunkte höher liege, in denen die App eingesetzt wird. Sehr vorangetrieben wurde das Projekt von Generalsekretär Peter Tauber, der selbst gern an Haustüren klopft und bei Twitter und Instagram verkündet: ""Connect17 wirkt."" Experten wie Simon Hegelich warnen davor, die Siege der CDU nur auf die App zurückzuführen. Er erinnert an die überzogene These, die Big-Data-Firma Cambridge Analytica sei allein für den Wahlsieg von Trump verantwortlich. ""Empirisch ist schwer zu sagen, wie viel Prozent das ausmacht. Die starke Mobilisierung der SPD nach der Kür von Martin Schulz hat sicher zu einer Gegenreaktion geführt"", sagt der Professor für Political Data Science der Hochschule für Politik an der TU München. Er lobt den Ansatz der Union. Es sei klug, hier Netzwerke aufzubauen: ""Das wirkt bieder, aber es ist sehr stabil. Die gesammelten Informationen lassen sich für die künftige Wahlkämpfe nutzen.""

Auch Martin Fuchs, Politikberater und Blogger, lobt Connect17. Zwar würden Apps seit Jahren in der Politik genutzt, doch der Ansatz sei neu, Big Data für den Tür-zu-Tür-Wahlkampf einzusetzen. ""So eine App, die jeder Laie nutzen könne, gab es noch nie."" Er hebt noch einen anderen Aspekt hervor: Die App schaffe einen Feedback-Kanal zwischen den Zehntausenden Wahlhelfern im Land und der Zentrale. ""Bisher kamen die Parteien nie ganz tief runter, höchstens bis zur Kreisebene. Nun können die Aktivisten nach Berlin melden, worauf die Bürger sie ansprechen, so dass in relativ kurzer Zeit gegengesteuert und Reden angepasst werden könnten"", sagt Fuchs. Dieses Potenzial sei noch nicht ausgeschöpft - mit den nötigen Ressourcen ließe sich etwa auf Facebook für die jeweiligen Regionen angepasste Werbung ausspielen.

Von den Milliardensummen, die in den USA für Wahlkämpfe ausgegeben werden, ist Deutschland weit entfernt. Und den Erwerb von extrem genauen Datenprofilen einzelner Wähler lässt der strengere deutsche Datenschutz nicht zu. Connect17 kombiniert also alte Wahlergebnisse mit zusätzlich angekauften Adressdaten der Post-Tochter ""Post direkt"", die sonst vor allem die Werbeindustrie nutzt. Diese gelten pro Straßenzug und sind keinem Einzelhaus zuzuordnen. In Schulungen, so JU-Mann Clemens, erkläre man den Unterstützern, dass sie die App während der Begegnung an der Haustür in der Tasche lassen und sich ganz auf das Gespräch mit den Bürgern konzentrieren sollen.

Erst im Anschluss tippen sie auf Smileys und geben an, was die Wähler über die CDU denken. Das Speichern der Daten geschieht erneut hochgerechnet auf Ebene der Straßenzüge. Beim ersten Einsatz im Saarland prüfte die dortige Datenschutzbeauftragte, ohne etwas zu beanstanden. Dass die CDU ebenso wie alle anderen Parteien hier sensibel vorgeht, wundert nicht: Mögliche Verstöße wären ein Desaster für das eigene Image. Laut Politikberater Fuchs wird die Linke in wenigen Wochen eine App zum Download anbieten, die auch in der Lage sein soll, Sympathisanten in der Nachbarschaft direkt zu vernetzen. Und was macht die SPD, die sonst am stärksten auf Haustür-Wahlkampf setzte und 2015 an fünf Millionen Türen klopfte? Die Genossen haben ebenfalls eine App, doch die ist nicht mehr als ein simples Online-Formular.

Dass es in Nordrhein-Westfalen nicht gut für die Sozialdemokraten lief, war in den vergangenen Tagen deutlich zu spüren. In Gesprächen wurde über die übereifrigen JU-Mitglieder gespottet und auf der Website der NRW-SPD wurden Verhaltenstipps für den Fall gegeben, dass die Konservativen in den ""knallorangenen Jacken kommen"" und an der eigenen Tür klingeln: ""Erst mal einen Kaffee anbieten"" oder Gegenfragen stellen wie ""Ist das Polohemd eigentlich von Lacoste?""

Der Online-Artikel offenbart einerseits eine ziemliche Überheblichkeit und zeigt andererseits ein Unverständnis, wie die App der Konkurrenz funktioniert. Und beides ist ziemlich verheerend in Wahlkämpfen, in denen jede Stimme zählt. ";https://www.sueddeutsche.de/politik/landtagswahl-mit-dieser-app-steuert-die-cdu-ihre-wahlhelfer-1.3506830;sz.de;Matthias Kolb
03.05.2017;Wie die Parteien auf Facebook Wahlkampf machen;"Angela Merkel schaut belustigt. Vor ihrem roten Sakko brennt ein Streichholz, das von einer Hand gehalten wird. Offensichtlich eine Fotomontage, noch dazu eine schlechte: Die Hautfarbe von Merkel und der Hand passen nicht zusammen. ""Einwanderung belastet Deutschland immer mehr: Sozialsysteme explodieren"", steht groß auf dem in Blau gehaltenen Bild. Die Botschaft der Alternative für Deutschland (AfD) ist klar: Angela Merkel zündelt. Und Deutschland muss darunter leiden.

In der Fotomontage blickt Merkel in Richtung dreier Slogans, die auf schwarz-rot-goldenem Hintergrund geschrieben sind: 11,4 Prozent Ausländeranteil, 75 Prozent der Syrer sollen arbeitslos sein, die Bürger zahlen die Zeche. Dieser aktuelle, zufällig ausgewählte Post der AfD steht beispielhaft für das Muster, mit dem die AfD bei Facebook Inhalte veröffentlicht. Der Erfolg der AfD - oder was dafür gehalten wird

Die AfD gilt als erfolgreichste Partei in sozialen Netzwerken. Insbesondere auf Facebook sind ihre Zahlen im Vergleich zu anderen Parteien beeindruckend. Die AfD hat von allem mehr: mehr Shares, mehr Likes, mehr Fans.

Facebook-Fans der Parteien Durchschnittliche Likes pro Post Die Daten stammen aus einer großen Datenrecherche der SZ: Dabei wurden systematisch die Facebook-Einträge der sieben Parteien untersucht, die bei der Wahl im Herbst eine Chance haben, in den Bundestag einzuziehen. Die SZ hat mehr als eine Million Facebook-Likes von 5000 Nutzern ausgewertet, um herauszufinden, wie die Deutschen auf Facebook politisch ticken und wie die politischen Sphären auf dem Netzwerk aussehen (mehr über das Projekt erfahren Sie hier).

Aber hohe Zahlen der AfD auf Facebook können täuschen. ""Die AfD hat hyperaktive Nutzer, die systematisch jeden Beitrag liken"", sagt Simon Hegelich, Professor für Political Data Science an der Hochschule für Politik München (HfP). So können Unterstützer der AfD den Facebook-Algorithmus besser ausnutzen, damit Posts der Parteiseite auch möglichst viele andere Nutzer zu sehen bekommen. Nach Hegelichs Erkenntnissen legen sie dazu zum Teil sogar mehrere Accounts an. Je öfter Nutzer einen Post liken oder teilen, desto prominenter wird Facebook den Beitrag im Newsfeed darstellen. ""Irgendwann schnappt der Algorithmus zu und Facebook schlägt diese Posts automatisch wieder anderen vor"", sagt Hegelich. Vorgetäuschte Aktivität nennt das der Wissenschaftler, der zu politischer Manipulation in sozialen Netzwerken forscht.

Die große Reichweite der AfD fußt auch auf ihren populistischen, überdrehten Botschaften. Martin Fuchs, Politikberater und Blogger, bezweifelt, ob dieser Effekt unmittelbar vor der Bundestagswahl noch so groß sein wird. Er glaubt, dass die AfD an eine gläserne Decke gestoßen ist: ""Was soll noch kommen? Irgendwann wiederholen sich auch die Aussagen von Krawallpostings."" Er erwartet auch, dass die AfD vorsichtiger wird, wenn sie im Herbst für breitere Gesellschaftsschichten wählbar sein will.
Das Muster eines AfD-Posts: ein Bild, garniert mit einem polemischen Spruch

Die AfD agiert auf Facebook deutlich anders als die anderen Parteien. Zentraler Bestandteil ihrer Posts ist in der Regel ein Bild mit einem polemischen Spruch, häufig mit Fotomontagen, die Politiker anderer Parteien ins Lächerliche ziehen: Leicht zu merken, bildlastig, pointiert - so funktionieren viele erfolgreiche Posts in sozialen Medien. Christian Lüth, Pressesprecher der AfD, bezeichnet solche Bilder als Anstoß zur Debatte. ""Ein Foto mit einem Slogan funktioniert viel besser als die ellenlangen Posts mit den weichgespülten Sätzen, die drei Parteiverantwortliche vorher freigeben mussten.""

In den Texten unter diesen Bildern wird die AfD dagegen gern ausführlich. Sie sind bei ihr deutlich länger als die der anderen Parteiseiten, haben oft mehrere Absätze, in denen die Aussagen auf den Bildern erläutert werden:

Länge eines durchschnittlichen Facebook-Beitrags in Zeichen Fast immer gibt es als Beleg einen oder mehrere Links auf etablierte Nachrichtenseiten. Im Fall der Merkel-Fotomontage mit dem Streichholz beispielsweise wurden die Zahlen aus der Zeitung Die Welt übernommen. ""Wir verlinken auch auf das, was bei Focus Online, der Jungen Freiheit oder der Preußischen Allgemeinen steht. Damit wollen wir die Diskussion anreichern"", sagt Lüth. Auch in diesem Punkt unterscheidet sich die AfD von der Konkurrenz, die ihre Nutzer häufig auf ihre eigenen Parteiseiten verweist und weniger auf Nachrichtenseiten.

Mit Spott über oder persönlichen Angriffen auf den politischen Gegner halten sich die meisten Parteien zurück, aber auch sie arbeiten wie die AfD häufig mit Bildern. Bei den Grünen heißt das zum Beispiel, dass auf einem Foto Cem Özdemir im Anzug streng und entschlossen in die Ferne blickt. Dazu das Zitat: ""Wer die Todesstrafe will, kann sich Nordkorea anschließen, aber nicht der EU."" Das entspricht ganz der Strategie der Grünen, sich im Social-Media-Wahlkampf vor allem auf die Spitzenkandidaten Cem Özdemir und Katrin Göring-Eckardt fokussieren.

Platte Slogans soll es bei den Grünen aber nicht geben. ""Aber natürlich funktioniert Social Media wie Werbung: Klare, emotionale Botschaften funktionieren am besten. Geteilt wird außerdem vor allem, was überrascht und unterhält"", sagt Robert Heinrich, Wahlkampfmanager der Grünen.

Die FDP gibt sich auf Facebook ganz nüchtern: Zwar sind die Farben Gelb, Blau und Magenta auf ihrer Seite knallig, inhaltlich sind die Posts eher brav. ""Wir stellen unsere Positionen dar und setzen nicht auf persönliche Angriffe"", sagt Pressesprecher Nils Droste. ""Fairness ist uns wichtig. Wir kritisieren Politik, aber nicht die Person.""

Anders die Linkspartei: Angriffslustig und zugespitzt will sie auf Facebook agieren. ""Sahra Wagenknecht zeigt, wie das geht"", sagt Thomas Lohmeier, Bereichsleiter Bürgerdialog der Linkspartei. ""Sie drückt sich sehr pointiert aus und macht klare Aussagen."" Die Fraktionsvorsitzende der Linken erreicht damit auch Wähler aus anderen politischen Lagern - wie aus dem Datensatz der SZ hervorgeht, gehört sie zu den Politikern, die im Umfeld fast aller Parteien ihre Facebook-Anhänger haben. Die CDU fährt eine wenig offensive Strategie auf Facebook. Und das ganz bewusst, denn allzu viel hält man im Konrad-Adenauer-Haus nicht vom sozialen Netzwerk mit seinen Millionen Mitgliedern in Deutschland. ""Tiefgehende politische Diskussionen gibt es auf Facebook eher selten"", sagt Stefan Hennewig, Leiter Kampagne und Marketing der CDU. In erster Linie würden dort eigene Meinungen kundgetan. Trotzdem hat die CDU zusammen mit der Jungen Union ""Connect 17"" gestartet. Damit soll der Online-Wahlkampf aller Bundestagskandidaten und Freiwilligen professionell organisiert werden. ""Natürlich ist Facebook ein gutes Kampagnen-Instrument, mit dem man sehr gut auf die eigenen Inhalte verweisen und verlinken kann"", sagt Hennewig.

Tobias Nehren, Leiter digitale Kampagnen der SPD, sagt: ""Facebook ist der Supermarktparkplatz des Internets. Hier kommen die Leute mit Politik in Kontakt, hier können wir unserer Werte sichtbar machen."" Nehren kennt noch eine Metapher: Facebook sei ""eines von vielen Instrumenten im Wahlkampforchester. Letztlich sollen die Anhänger auch vor Ort aktiv werden."" Das Netzwerk wird damit zur Motivationsplattform für analoge Aktion. ""Deutsche Parteien wollen bei Facebook den Haustürwahlkampf mit dem normalen Wahlkampf verbinden"", sagt HfP-Professor Hegelich.

Die Alternative für Deutschland nutzt ihre Anhänger quasi für einen Graswurzel-Wahlkampf. Statt Kugelschreiber auf dem Marktplatz verteilen sie zugespitzte Sprüche auf dem Territorium der politischen Gegner, also den Facebook-Seiten der anderen Parteien. Unter so gut wie jedem beliebigen Post vor allem der Grünen und der CDU kommentieren Nutzer aus dem rechten Spektrum. ""Viele Debatten auf Facebook werden durch eine sehr laute Minderheit geprägt"", sagt Stefan Hennewig von der CDU. Umgekehrt agitieren Gegner der AfD auf deren Facebook-Seite bei weitem nicht in gleichem Maße.

Politikberater Fuchs sagt, dass Auseinandersetzungen sehr wohl auch auf anderen Facebook-Seiten als der der eigenen Partei stattfinden sollten: ""Wenn irgendwo eine Diskussion für den Nahostkonflikt stattfindet, dann erwarte ich, dass dort in den Kommentaren auch die offizielle Position der Partei auftaucht.""
Wie die Parteien mit Fake News umgehen wollen

""Fake News"", also gezielt verbreitete erfundene Nachrichten, die die öffentliche Meinung beeinflussen sollen, könnten auch im deutschen Wahlkampf eine Rolle spielen. Die Parteien versuchen, sich dagegen zu wappnen.

Die Grünen haben beispielsweise die ""Grüne Netzfeuerwehr"" gegründet. Das ist eine geschlossene Facebook-Gruppe, die zum Beispiel gegen die AfD ausrückt. Als Beispiel für einen Erfolg nennt Wahlkampfmanager Heinrich, dass die AfD Emmendingen ein gefälschtes Grünen-Plakat, das der Partei zu Unrecht einen ""antideutschen"" Spruch zuordnete, binnen Stunden wieder löschte. ""Die Rechten sind sehr gut organisiert. Wir müssen selber was tun und können nicht warten, bis Facebook oder der Gesetzgeber reagiert"", sagt Heinrich.

Ähnlich sieht das Thomas Lohmeier von der Linkspartei. Gegen Fake News gebe es eine interne Whatsapp-Gruppe. Auch wenn man viel moderieren und löschen müsse, suche man generell den Dialog im Netz.

Neben den klassischen Beiträgen bei Facebook gibt es zwei weitere große Trends: Bezahlte Posts und Chatbots. Die CSU hat beispielsweise kürzlich einen solchen Chatbot auf Facebook gestartet. Nutzer können direkt über das Programm Fragen zu CSU-Positionen stellen, die Antworten liefert eine Software. Martin Fuchs glaubt, dass Nachrichten-Apps wie der Facebook-Messenger eine wichtige Rolle im Wahlkampf spielen können: ""Der direkte Kontakt ist viel emotionaler.""

Eine wichtige Rolle spielen Sponsored Posts, also bezahlte Anzeigen, die gezielt bestimmten Personengruppen angezeigt werden. So spielte die CSU zum Beispiel 2016 Werbung für Russlanddeutsche auf Russisch aus. Ohne Sponsored Posts gehe heute wenig, sagt FDP-Pressesprecher Droste. Denn die Partei könne nur begrenzt selbst steuern, was viral gehe; es sei schwierig, den Facebook-Algorithmus auszutricksen. Umso mehr freut es die Strategen der FDP, wenn sie einen Hit im Netz landen. Wie die Rede des Parteivorsitzenden Christian Lindner 2015 im nordrhein-westfälischen Landtag. Nach dem Zwischenruf eines SPD-Abgeordneten hielt er ein leidenschaftliches Plädoyer für Unternehmertum und Gründungskultur. Mit ""das hat Spaß gemacht"" schloss Lindner seine Rede, die auch deshalb so erfolgreich bei Facebook war, weil sie auf das setzte, was in dem sozialen Netzwerk besonders gut funktioniert: Zuspitzungen und Emotionen in Bild oder Video.";https://www.sueddeutsche.de/digital/der-facebook-faktor-wie-die-parteien-auf-facebook-wahlkampf-machen-1.3479979;sz.de;Katharina Brunner und Mirjam Hauck
02.05.2017;Von AfD bis Linkspartei - so politisch ist Facebook;"Dass es inzwischen Bootcamps gibt, um Politiker und Politikerinnen fit für den Wahlkampf in sozialen Netzwerken zu machen, zeigt: Facebook ist ein Faktor im Machtgefüge der Republik geworden - zumal im Wahljahr. Analog ist dem Wähler die politische Topografie Deutschlands vertraut - digital ist sie weniger überschaubar. Mit einer großen Datenrecherche hat die SZ über Monate daran gearbeitet, das zu ändern. Was steckt hinter dem Phänomen Filterblase? Welchen Einfluss hat sie auf den Wahlkampf? Und wie agieren die Parteien, die eine Chance auf Einzug in den Bundestag haben, und ihre Anhänger auf Facebook? Um diese Fragen zu beantworten, wurden eine Million öffentliche Likes von Nutzern untersucht, die auf den Partei-Seiten interagiert haben (mehr über die Recherche erfahren Sie hier).

Auf Basis dieser Daten wurden unter anderem Ranglisten der populärsten Seiten jedes Parteimilieus erstellt. Mit ihnen lässt sich decodieren, welche Facebook-Seiten etwa bei Nutzern aus dem AfD-Umfeld besonders beliebt sind - und wie sich die Vorlieben bei den Parteien voneinander unterscheiden. In verschiedenen Analysen ergibt sich so, was das Milieu einer Partei auf Facebook charakterisiert, welche Überschneidungen und welche Trennlinien es entlang des politischen Spektrums von AfD bis Linkspartei gibt. So lässt sich zwar keine Aussage über die Interessen und Präferenzen von Parteianhängern oder Wählern in der analogen Welt treffen - aber die politische Großwetterlage auf Facebook durchaus untersuchen. Es gibt keine Filterblasen - aber die AfD ist in ihrer Echokammer weitgehend isoliert

Spätestens seit der Wahl von Donald Trump zum US-Präsidenten gilt die Filterblase als etwas Bedrohliches - für den Einzelnen und für die Gesellschaft, das demokratische Miteinander und das politische System. 2011 prägte der Internetaktivist Eli Pariser den Begriff der Filterblase und warnte davor, dass wir von Firmen wie Google oder Facebook auf unsere persönliche Vorlieben zugeschnittene Newsfeeds präsentiert bekommen. Was zunächst ein nutzerfreundlicher Service ist, hat fatale Folgen: Innerhalb einer solchen Blase, schreibt Pariser, brauche man sich nicht mit anderen Meinungen auseinanderzusetzen, sondern verharre isoliert in einer Weltanschauung.

Voneinander abgeschottete Filterblasen in dieser strengen Deutung gibt es der Datenauswertung zufolge auf Facebook um die deutschen Parteien herum praktisch nicht. Denn wie die dichten, fast spinnennetzartigen Verbindungen in der Netzwerk-Grafik zeigen, gibt es zwischen fast allen Milieus um die Parteien Verbindungen. Allein die Sphäre der Alternative für Deutschland (AfD) auf der rechten Seite der Grafik erscheint vergleichsweise isoliert. ""Der Großteil der Bevölkerung steckt nicht in einer Filterbubble"", sagt die Professorin Katharina Kleinen-von Königslöw von der Universität Hamburg, die insbesondere zu digitalisierter Kommunikation forscht. ""Abgespalten ist nur die AfD."" Im Netzwerk ist das auch deswegen aussagekräftig, weil sich räumliche Nähe nach dem Grad der Verbundenheit richtet:

Das politische Netzwerk auf Facebook

Die räumliche Nähe von Knotenpunkten richtet sich nach der Anzahl der Verbindungen: Je näher zwei Punkte, also Facebook-Seiten, einander sind, desto mehr direkte Verbindungen - desto mehr Nutzer haben also die beiden Seiten gelikt. Diese besondere Stellung der AfD lässt sich mit dem Bild der Echokammer, in der bestimmte Meinungen stetig widerhallen, besser fassen als mit dem einer abgeschlossenen Filterblase. Eine Echokammer nutzen Tonstudios, um einen Halleffekt zu erzeugen. Auf das Internet übertragen beschreibt es einen abgegrenzten Bezugsraum, in dem Aussagen im Inneren verstärkt werden. ""Dieser Begriff ist realitätsnäher"", sagt Simon Hegelich, Professor für Political Data Science an der Hochschule für Politik München (HfP). ""Informationen bewegen sich innerhalb dieser Echokammern schneller. Die Meinung der eigenen Gruppe ist präsenter als die Gegenmeinung.""

Dass sich in der Auswertung der 100 im AfD-Umfeld beliebtesten Facebook-Seiten weit mehr als die Hälfte, insgesamt 62 von 100 Seiten, bei keiner anderen Partei findet, passt ins Bild. Es ist ein Indikator dafür, wie eng die Verbindungen innerhalb dieses Milieus sind. Während es bei den anderen Parteien Querverbindungen zu potenziellen Koalitionspartnern oder auch ins gegnerische Lager gibt, rangieren bei der AfD die eigenen Landesverbände oben. Ohne Facebook wäre der Aufstieg der AfD so nicht möglich gewesen. Stärker als die anderen Parteien setzt sie und ihre Anhänger auf das soziale Netzwerk, um Inhalte zu transportieren und Anhänger zu mobilisieren. Facebook sei eigentlich als soziales Netzwerk angelegt, sagt Hegelich, und nicht für politische Zwecke, so dass es dafür leicht missbraucht werden könne. (Mehr über die Wahlkampfstrategien der Parteien im Netz lesen Sie hier.)

Die Stellung der AfD bei Facebook spiegelt auch die grundsätzliche Kritik der Partei und ihrer Sympathisanten an jenem Teil des politischen und medialen Systems, den sie als ""Mainstream"" diffamieren. Die AfD-Sphäre ist thematisch umgrenzt und inhaltlich abgeriegelt. Bei AfD-nahen Nutzern steuerten starke Einstellungen das Auswahlverhalten bei der Informationssuche, sagt Kleinen-von Königslöw: ""Es gibt grundsätzlich eine Neigung, Posts auszuwählen, die die eigene Meinung bestätigen - das zeigt sich bei AfD-Anhänger besonders deutlich."" Unter den 100 am häufigsten gelikten Seiten der User finden sich etliche Facebook-Seiten, die rechtspopulistische Propaganda betreiben. Sie haben, wie die Grafiken unten zeigen, die stärksten Ausschläge auf der AfD-Achse und weitere, wenn auch deutlich geringere, auf der der CSU. Die Größe des farbigen Feldes sagt damit auch aus, welche Bedeutung die gemessene Seite in welchem politischen Milieu hat und welche Verbreitung sie über verschiedene Parteimilieus hinweg hat - diese spielen beispielsweise lediglich bei AfD und CSU eine Rolle:

Facebook-Seiten, die rechtspopulistische Propaganda betreiben

Je größer der Ausschlag an einer Achse, desto beliebter ist eine Facebook-Seite bei Nutzern im Umfeld der jeweiligen Partei.
 Auch die Ablehnung der sogenannten ""Mainstream""-Medien manifestiert sich auf Facebook: In der AfD-Sphäre informiert man sich nicht bei Tagesschau, ZDF-heute-Nachrichten, Spiegel Online, der SZ oder Huffington Post Deutschland - all diese Medien rangieren in allen Parteien-Sphären außer der der AfD weit oben. Deren Fans lesen dagegen die Junge Freiheit, die Epoch Times, Russia Today Deutsch oder Compact - alte und neue Medien, die Nachrichten oft mit rechtem und antiliberalem Spin veröffentlichen. Dass diese Seiten in den Radar-Grafiken nur ein schmales Dreieck bei AfD und CSU beziehungsweise Linken bedecken, liegt an deren Bedeutungslosigkeit im Rest des politischen Spektrums.

Im Umfeld der AfD beliebte Medien

Je größer der Ausschlag an einer Achse, desto beliebter ist eine Facebook-Seite bei Nutzern im Umfeld der jeweiligen Partei.
Anders als etwa Spiegel Online, Tagesschau und Tageszeitung Die Welt, die sich relativ gleichmäßig auffächern:

Medien, die in vielen Milieus beliebt sind

Je größer der Ausschlag an einer Achse, desto beliebter ist eine Facebook-Seite bei Nutzern im Umfeld der jeweiligen Partei.
Problematisch werde es, wenn sich Menschen ausschließlich über Facebook informierten, sagt Kleinen-von Königslöw. ""Wenn man sich bei Facebook anmeldet, begibt man sich immer in eine soziale Situation."" Dadurch würden neben der Informationsbeschaffung andere Motive verstärkt: Man will sich präsentieren, interessiert sich weniger für das, was in der Welt wichtig ist, als für das, was in der eigenen Gruppe zählt. ""Zugespitzt gesagt ist das Vertrauen gegenüber den Leuten in meinem Netzwerk höher, dadurch, dass sie beispielsweise dieselbe Hunderasse mögen - weil sie mir ähnlicher sind und ich etwas von ihnen weiß.""

Dennoch hat auch die AfD-Sphäre Verbindungen nach außen - die aber die Sonderstellung der Partei eher bestätigen denn in Frage stellen. Neben den Facebook-Seiten des Politikressorts von Focus Online und der Welt, die schlicht bei allen Parteien verbreitet sind, sind das vor allem internationale Vernetzungen, die die Verbundenheit der Rechten weltweit und damit die Internationalität gerade von Nationalismen offenbaren:

Im Milieu der AfD und CSU beliebte internationale Politiker

Je dunkler die Farbe, desto höher ist der Anteil der Likes für die Facebook-Seite aus dem Umfeld der jeweiligen Partei.
 In der Grafik zeigt sich, dass die Seiten aller sechs Politiker (die österreichischen FPÖ-Politiker Norbert Hofer, Heinz-Christian Strache und Harald Vilimsky, US-Präsident Donald Trump, der Schweizer Ex-Pegidist Ignaz Bearth und die rechte französische Präsidentschaftskandidatin Marine Le Pen) von einem vergleichsweise hohen Anteil der User (sichtbar an der dunkleren Farbe) aus dem AfD-nahen Milieu gelikt werden. Etwas geringer ist die Anhängerschaft bei der CSU - weiter links spielen diese Politiker dann kaum mehr eine Rolle. Was AfD und CSU verbindet
Die CSU ist die Brücke der politischen Mitte nach Rechtsaußen

Vor 30 Jahren gab der ehemalige bayerische Ministerpräsident Franz Josef Strauß die Losung für seine Partei aus: ""Rechts von der CSU darf es keine demokratisch legitimierte Partei geben!"" Vor weniger als zwei Jahren sagte dann Angela Merkel: ""Wir schaffen das."" Seit Beginn der sogenannten Flüchtlingskrise im Sommer 2015 manövriert die Union zwischen diesen beiden Polen. Die CSU mit Parteichef Horst Seehofer an der Spitze hat einiges getan, um Strauß' Forderung postum gerecht zu werden. Geschadet hat das der CSU nicht: Die Stammwähler bleiben, oben drauf kommt Erfolg bei (rechts-)konservativen Wählern. Obwohl oder weil sie rechtspopulistisches Gedankengut verbreitet und es so unter Umständen salonfähig macht, wie Kritiker wie die konservative Publizistin Liane Bednarz der CSU vorwerfen.

Die SZ-Recherche legt nahe, dass Facebook-Nutzer aus dem christsozialen Lager dem Strauß'schen Mantra folgen: Sie machen sich nach rechts möglichst breit, so dass sie im Mitte-links-Ballungsraum der etablierten Parteien (siehe Seite 3) der Rechtsausleger sind. In dem auf den Facebook-Daten basierenden Netzwerk steht rechts von der CSU nur die AfD, mit der keine andere Partei koalieren will. Die Überschneidungen sind deutlich. Mehr als 30 Seiten auf den jeweiligen Top-100-Listen finden sich bei CSU und AfD - bei den meisten anderen Parteien sind es deutlich weniger, mit der CDU etwa hat die AfD nur 19 gemeinsame Seiten, mit der SPD gar nur sieben. Deswegen ist auch die Grafik auf der rechten Seite am farbigsten - hier wird für beispielhaft ausgewählte Seiten die Popularität im jeweiligen Parteimilieu über mehr oder weniger intensive Farbgebung angezeigt. Dass beispielsweise die Felder bei Frauke Petry oder Horst Seehofer sowohl in der CSU- wie auch in der AfD-Spalte dunkel hervorstechen, ist bezeichnend:

Welche Facebook-Seiten im Milieu der AfD und der CSU beliebt sind

Je dunkler die Farbe, desto höher ist der Anteil der Likes für die Facebook-Seite aus dem Umfeld der jeweiligen Partei.
Der gemeinsame politische Nenner von AfD und CSU ist das Misstrauen gegenüber einer liberalen Asyl- und Einwanderungspolitik. Die AfD und die CSU sind auch die beiden Parteien, deren Facebook-Nutzer die meisten Seiten gemeinsam haben, die in keiner anderen Parteiwelt eine größere Rolle spielen. Darunter fallen mehr oder weniger offen rechte Seiten wie ""Bürger sagen Nein"", ""Ein Prozent für unser Land"", ""Multikulti? Nicht mit uns"". Der längst als Unwort des Jahres gebrandmarkte Nazi-Begriff ""Lügenpresse"" dient als Name einer Seite mit mehr als 80 000 Followern. Hier entstehen Echokammern, in denen menschenfeindliche Hetze und Aufrufe zur Gewalt den Grundton bestimmen - etwa wenn gefordert wird, ""dem Dreckspack"" könne man ""gar nicht genug in die Fresse hauen"" oder das ""verblödete Islamisten Pack am nächsten Baum"" aufzuhängen.

Popularität ausgewählter Politiker entlang des Parteienspektrums

Je größer der Ausschlag an einer Achse, desto beliebter ist eine Facebook-Seite bei Nutzern im Umfeld der jeweiligen Partei.
CSU und AfD mögen sich übrigens auch gegenseitig: Fast einem Fünftel der AfD-nahen Facebook-Nutzer gefällt die CSU (Platz 9 der Top-100-Seiten), während unter CSU-Nutzern die AfD (Platz 6) sogar beliebter ist als die CDU auf Rang 10. Ähnlich sieht es, zumindest auf Seiten der CSU, beim Personal aus, denn wer sich im CSU-Milieu bewegt hat, dem gefällt auffallend häufig auch AfD-Chefin Frauke Petry, die hier sogar beliebter ist als Kanzlerin Angela Merkel. In den Grafiken unten haben Petry und Merkel erwartungsgemäß die stärksten Ausschläge bei ihren eigenen Parteien, die zweitstärksten auf der CSU-Achse - wobei der bei Merkel weniger stark ausgeprägt ist als bei Petry.
Wer zur ""breiten Mitte"" gehört - und wer nicht
Rechts die AfD, dann die CSU - und daneben ein Mitte-links-Ballungsraum von CDU bis Linkspartei.

Der im AfD-Umfeld oft sogenannte ""Mainstream"", also die Mehrheitsmeinungen und ihre Repräsentanten, ist auf Facebook ein dichtes Netz, um das sich fünf Parteien gruppieren: CDU, FDP, SPD, Grüne und Linke. Am Rand dieses Ballungsraums liegt die CSU und noch weiter draußen die AfD. Die Forschung von Kleinen-von Königslöw bestätigt das. Die Professorin spricht von einer anders als in den USA wenig ausgeprägten Polarisierung, sondern von einer ""sehr breiten Mitte"", auch auf Facebook. Was diesen Ballungsraum prägt, dem auch die Christsozialen noch relativ nahe stehen, sind Seiten, auf die sich alle einigen können - außer Anhänger der AfD:

Seiten mit großen Überschneidungen bei CDU, CSU, FDP, den Grünen, der Linken und der SPD

Je dunkler die Farbe, desto höher ist der Anteil der Likes für die Facebook-Seite aus dem Umfeld der jeweiligen Partei.
Diese Seiten sind in erster Linie vor allem die großer Medien (Tagesschau, Spiegel Online, ZDF-heute-Nachrichten, Süddeutsche Zeitung, Stern) oder Satireseiten (Postillon, heute show). Sie landen in den Milieus fast aller Parteien weit oben in den Top 100 der beliebtesten Seiten. ""Im Bereich der gesellschaftlichen Mitte gibt es keine parteipolitisch orientierte Auswahl von Informationen"", sagt Kleinen-von Königslöw - sprich, der Grünen- oder CDU-Wähler konsumiert nicht gezielt ein linksliberales oder konservatives Medienmenü, sondern bedient sich am Büfett. Einen ähnlich hohen Stellenwert haben die Bundeskanzlerin, der Facebook-Auftritt der Bundesregierung und Ex-US-Präsident Barack Obama: Alle schaffen es im Umfeld fast aller Parteien in die Top 100, außer bei AfD und Linkspartei.

Wesentliches gesellschaftspolitisches Bindemittel sind darüber hinaus Facebook selbst (auch Facebook hat eine Facebook-Präsenz, mit mehr als 180 Millionen Likes), Konsum (die Facebook-Seite von Amazon rangiert bei sechs von sieben Parteien weit oben) und Fußball: Bei sechs Parteien finden die Nutzer Borussia Dortmund gut. Bei fünf Parteien (außer AfD und Linken) gefallen den Nutzern die Fußball-Nationalmannschaft, Torwart Manuel Neuer und Stürmer Thomas Müller. Deren Verein, der FC Bayern, findet sich nur bei vier Parteien (CDU, CSU, FDP und SPD) in den Top 100. Ausgenommen sind auch bei diesen unpolitischen Seiten die Rechtspopulisten, deren inhaltlicher Fokus sehr stark von Migrations- und Asylpolitik dominiert wird und zumindest auf Facebook wenig Raum für andere Interessen und Neigungen lässt.

Facebook-Seiten, die sich mit dem Thema Sport befassen

Je dunkler die Farbe, desto höher ist der Anteil der Likes für die Facebook-Seite aus dem Umfeld der jeweiligen Partei.
Verengt sich der Fokus auf die Schnittmengen einzelner Parteien, spiegeln sich politische Nachbarschaften in der analogen Welt (und mögliche Koalitionspartner) auch in den Facebook-Milieus wider. Messen lässt sich das unter anderem an den Seiten der Top-100-Listen, die sich bei den Parteien überschneiden. Die meisten Überlappungen zwischen zwei Parteien finden sich trotz der Nähe der CSU zur AfD bei der Union: 65 Prozent der am häufigsten gelikten Seiten haben die Schwesterparteien CDU und CSU gemeinsam.

Ähnlich schlägt sich politische Nähe auch im Muster der am häufigsten gelikten Seiten bei SPD und Grünen sowie Grünen und der Linkspartei nieder: Sie haben jeweils mehr als 50 Prozent der Top-100-Seiten gemeinsam. Meist verbirgt sich dahinter eine Mischung aus den Facebook-Auftritten der Parteigrößen, überregionalen Medien- sowie Satire-Seiten. Die Schnittmenge von SPD und Grünen ist die zweit-, die von Linkspartei und Grünen die drittgrößte. Große Gemeinsamkeiten gibt es auch zwischen CDU und FDP sowie zwischen CSU und FDP - sie stehen in der Auswertung der Überschneidungen bei gelikten Seiten verschiedener Parteien auf den Plätzen vier und fünf.

Insgesamt am konsensfähigsten unter allen Facebook-Seiten sind die Tageszeitung Die Welt und das Politikressort von Focus Online, die beide nicht nur von Linkspartei, Grünen, SPD, FDP, CDU und CSU, sondern auch von AfD-Nutzern verfolgt werden. Das gilt auch für die Seite nametests.com, die digitale Kalendersprüche zur geistigen Erbauung anbietet, etwa ""Warte nicht auf das große Wunder, sonst verpasst du die vielen kleinen"" und Antworten auf so zentrale Fragen wie ""Welchen Namen würde Gott dir geben?"" oder ""Welches Gewürz bist du?"" verspricht. Das eröffnet womöglich ganz neue, im Wortsinn küchenpsychologische Deutungen und ist zwar kein großer, aber immerhin doch ein gemeinsamer Nenner.

Seiten, auf die sich fast alle einigen können

Je dunkler die Farbe, desto höher ist der Anteil der Likes für die Facebook-Seite aus dem Umfeld der jeweiligen Partei.
";https://www.sueddeutsche.de/politik/politik-auf-facebook-rechte-abschottung-ohne-filterblase-1.3470137?reduced=true;sz.de;Katharina Brunner und Sabrina Ebitsch
01.05.2017;Die digitale Arena;"Angela Merkel (CDU) schaut belustigt, in ihrer Hand brennt ein Streichholz. Offensichtlich eine Fotomontage, noch dazu eine schlechte. Die Botschaft der Alternative für Deutschland (AfD) ist klar: Merkel zündelt. Und Deutschland muss darunter leiden. Ein Bild mit einem polemischen Spruch, der den politischen Gegner lächerlich macht - das gibt viel Zuspruch auf Facebook. Ein AfD-Beitrag bekommt im Mittel doppelt so viele Likes wie alle anderen Parteien zusammen für einen durchschnittlichen Post. Aber die reinen Zahlen täuschen. ""Die AfD hat hyperaktive Nutzer, die systematisch liken"", sagt Simon Hegelich, Professor für Political Data Science an der Technischen Universität München. Vorgetäuschte Aktivität nennt er diese Manipulation.

Mit Spott über oder persönlichen Angriffen auf den politischen Gegner halten sich die meisten anderen Parteien zurück. Aber auch sie arbeiten häufig mit Bildern, die in der Logik des sozialen Netzwerks besser funktionieren. Bei den Grünen blickt zum Beispiel Cem Özdemir auf einem Foto in die Ferne. Dazu das Zitat: ""Wer die Todesstrafe will, kann sich Nordkorea anschließen, aber nicht der EU."" Auch die Linkspartei will angriffslustig und zugespitzt auf Facebook agieren. ""Sahra Wagenknecht zeigt, wie das geht"", sagt Thomas Lohmeier, Bereichsleiter Bürgerdialog der Linkspartei. ""Sie drückt sich sehr pointiert aus und macht klare Aussagen.""
Rechte Nutzer kommentieren besonders häufig - gern voll Spott über ihre Gegner

Die FDP gibt sich im Netz betont nüchtern: ""Wir stellen unsere Positionen dar und setzen nicht auf persönliche Angriffe"", sagt der Pressesprecher Nils Droste. Auch die CDU verfolgt eine wenig offensive Strategie. Und das ganz bewusst, denn allzu viel hält man im Konrad-Adenauer-Haus nicht vom sozialen Netzwerk mit seinen Millionen Mitgliedern in Deutschland. ""Tiefgehende politische Diskussionen gibt es auf Facebook eher selten"", sagt Stefan Hennewig, der Leiter Kampagne und Marketing der CDU. Trotzdem hat die CDU mit der Jungen Union ""Connect 17"" gestartet. Damit soll der Onlinewahlkampf aller Bundestagskandidaten und Freiwilligen professionell organisiert werden.

Die SPD hofft ebenfalls, dass Fans im Internet auch vor Ort aktiv werden. Facebook wird zur Motivationsplattform für analoge Aktionen. ""Deutsche Parteien wollen bei Facebook den Haustürwahlkampf mit dem normalen Wahlkampf verbinden"", sagt Hegelich. Gerade die AfD nutzt ihre Anhänger auch für einen digitalen Graswurzel-Wahlkampf. Statt Kugelschreiber auf dem Marktplatz verteilen sie zugespitzte Sprüche auf dem Territorium der politischen Gegner, also den Facebook-Seiten der anderen Parteien. Unter so gut wie jedem Post vor allem der Grünen und der CDU kommentieren Nutzer aus dem rechten Spektrum. Umgekehrt agitieren Gegner der AfD auf deren Facebook-Seite bei Weitem nicht in gleichem Maße.

Neben den klassischen Beiträgen gibt es zwei weitere Trends für den anlaufenden Bundestagswahlkampf: Zum einen bezahlte Anzeigen, die sehr genau platziert werden. Die CSU hat zum Beispiel 2016 Russlanddeutsche auf Facebook gezielt auf Kyrillisch angesprochen. Zum anderen die direkte, persönliche Ansprache in Nachrichten-Apps, also bei Diensten wie das zum Facebook-Konzern gehörige Whatsapp.";https://www.sueddeutsche.de/politik/wahlkampf-die-digitale-arena-1.3485700;sz.de;Katharina Brunner und Mirjam Hauck
05.04.2017;Neue Studiengänge: Logistik, Städtebau, Consulting;"Master Umweltorientierte Logistik

Die Hochschule für Technik Stuttgart startet zum Wintersemester 2017/2018 den Master Umweltorientierte Logistik. Der Studiengang dauert vier Semester, teilt die Hochschule mit. Studenten beschäftigen sich mit der Frage, wie die Logistik nachhaltig organisiert werden kann. Es geht um Themen wie Materialbestellung, Lagerung und alternative Energieversorgungskonzepte. Im dritten Semester machen die Studenten entweder ein Auslandssemester oder ein Praxissemester bei einer Firma im In- oder Ausland. Absolventen sollen in der Logistikabteilung von Unternehmen arbeiten können. Bewerbungsschluss ist am 15. Juni.

Bachelor im Bereich digitale Wirtschaft

Die private Hochschule Business and Information Technology School (BiTS) hat den Bachelor Digital Business and Data Science neu im Angebot. Der Studiengang beginnt erstmals zum Wintersemester 2017/2018 und dauert sechs Semester, teilt die Hochschule mit. Studenten beschäftigen sich zum einen mit Themen aus dem Bereich Management wie Controlling, Marketing oder Innovationsmanagement. Zum anderen geht es um technische Fertigkeiten wie Programmiersprachen oder Technologien, mit denen sich Datenbanken auswerten lassen. Absolventen sollen als Führungskräfte in der digitalen Wirtschaft arbeiten können. Das Studium kostet 26 820 Euro. Bewerbungsschluss ist der 25. September.

Master zum Thema Städtebau in Basel

Die Universität Basel bietet ab dem Herbstsemester 2017 den englischsprachigen Master Critical Urbanisms an. Der Studiengang dauert vier Semester und richtet sich etwa an Absolventen aus den Sozial- oder Humanwissenschaften, teilt die Hochschule mit. Studenten beschäftigen sich nicht nur mit dem Thema Städtearchitektur, es geht auch um die ökonomischen, kulturellen und sozialen Auswirkungen der Architektur auf die Bewohner. Ein Schwerpunkt liegt dabei auf der Städteentwicklung in Afrika. Studenten verbringen das zweite Semester in Kapstadt. Absolventen sollen später in Bauämtern, aber auch in Architekturbüros und in Museen arbeiten können. Das Studium kostet 850 Schweizer Franken pro Semester (umgerechnet rund 794 Euro). Hinzu kommt eine Anmeldegebühr von 100 Schweizer Franken (rund 93 Euro). Bewerbungsschluss ist der 30. April.

Islamische Theologie an der Humboldt-Universität

An der Berliner Humboldt-Universität sollen islamische Geistliche für den Dienst in Moscheen sowie Lehrer für den Schulunterricht ausgebildet werden. Erste Absolventen sollen die Hochschule nach dem Jahr 2021 verlassen, teilte die Universität mit. Eine Arbeitsgruppe mit Beteiligung islamischer Verbände werde den Studiengang vorbereiten, kündigte der Historiker Michael Borgolte als Gründungsbeauftragter des Instituts an. Die fünf anderen Institute in Deutschland hätten positive Erfahrungen gesammelt, sagte Universitätspräsidentin Sabine Kunst. Berlins Regierender Bürgermeister Michael Müller (SPD) sagte, der Aufbau des Instituts habe hohe Priorität für den Senat. Die Finanzierung sei mit 13 Millionen Euro bis 2020 gesichert.

Master Wirtschaftspsychologie und Consulting

Die private FOM-Hochschule startet den berufsbegleitenden Master Wirtschaftspsychologie und Consulting. Er wird erstmals zum Wintersemester 2017/2018 angeboten und dauert fünf Semester, teilt die Hochschule mit. Studenten beschäftigen sich mit Arbeits- und Organisationspsychologie und Marktforschung. Gleichzeitig erhalten sie Einblick in verschiedene Coachingmethoden. Das Studium kostet 12 430 Euro. Absolventen sollen später etwa für Unternehmen in der Personalabteilung arbeiten können. Bewerbungsschluss ist der 15. Juli. ";https://www.sueddeutsche.de/karriere/arbeit-neue-studiengaenge-logistik-staedtebau-consulting-dpa.urn-newsml-dpa-com-20090101-170322-99-768717;sz.de;DPA
12.03.2017;Christopher White leuchtet das Darknet aus;"Auf dem Festival SXSW in Austin mangelt es nicht an Versprechen für eine bessere Zukunft, aber selten klingen sie so pathetisch wie bei Christopher White. ""Wenn wir zusammenarbeiten, können wir die Dunkelheit erhellen"", sagt er seinem Publikum auf dem großen Tech-Festival, das derzeit in der texanischen Hauptstadt stattfindet. Und wenn das Licht erst mal an ist, sollen Zuhälter, Menschenhändler, Dschihadisten und Betrüger ganz erschrocken blinzeln.

White - schwarzes Hemd, schwarze Hose, schwarze Schuhe - ist ein asketisch wirkender Mittdreißiger und arbeitet als Forscher bei Microsoft. Er ist eine Art Ein-Mann-Spezialeinheit des Konzerns, und er hat eine Mission. Seine Kenntnisse von Informatik und Statistik will er dazu nutzen, das Unsichtbare sichtbar zu machen: das tiefe Netz und das dunkle Netz.

Die üblichen Suchmaschinen wie Google und Bing erfassen nicht das ganze Internet. Sie kratzen nur an der Oberfläche. Unter der liegt das Deep Web: Fach-Datenbanken, mit Passwörtern geschützte Foren, Chatrooms, E-Mail-Systeme. Noch ""tiefer"" liegt das Darknet - jener Teil des Netzes, der nur mit dem spezieller Software wie dem Browser Tor angesteuert werden kann, der die Identität seiner Nutzer verschleiert. Auf der Leinwand neben White erscheint eine Karte der USA. Feiner Nebel liegt über dem Land, besonders über den großen Städten. Der Nebel besteht aus Millionen von Punkten, jeder steht für eine Werbeanzeige einer Prostituierten. Visualisiert hat das eins von Whites Datenanalyse-Tools. Allein in Austin seien es jährlich mehr als 470 000 Anzeigen, sagt White.
Wenn Googeln nicht hilft

Polizisten, NGOs oder Journalisten können diesen Schwarzmarkt, in dem mit Sex, aber eben auch mit Menschen gehandelt wird, praktisch nicht überblicken - weil sie nur Standard-Suchmaschinen zur Verfügung haben. White sagt: ""Wenn Ermittler versuchen, das zu googeln, wird es nicht funktionieren."" Denn viele der Sex-Anzeigen werden nicht verlinkt, was dazu führt, dass sie kaum in Google auftauchen. Wie oft auf eine Seite verlinkt wird, ist ein entscheidendes Kriterium dafür, wie hoch in den Ergebnissen Google sie anzeigt. White hat Software entwickelt, die die Analyse dieses schwer zu findenden Datenwusts mit künstlicher Intelligenz und komfortabler Benutzeroberfläche erleichtern soll.

Seine Programme können nachverfolgen, wo und wann Fotos von Prostituierten in Anzeigen gepostet wurden. Er zeigt ein Beispiel: Ein Bild taucht mit Bezug zu mehreren texanischen Städten zu verschiedenen Zeitpunkten auf. Es entsteht ein Bewegungsprofil: Wann wurde die Frau angeboten und wo? Mit Telefonnummern funktioniert das ebenfalls. Ermittler sollen so die Netzwerke der Zuhälter aufspüren und ihre Größe nachvollziehen können.

White ist ein Verbindungsmann zwischen Big-Data-Forschung und Sicherheitsapparat. Er arbeitete mehrere Jahre für Darpa, die Entwicklungsabteilung des US-Verteidigungsministeriums, die maßgeblich an der Erfindung des Internets und von GPS beteiligt war. Whites Projekt hieß Memex, es ist ein Werkzeugkasten an Programmen, mit dem man den Überblick im Darknet gewinnen kann. Ihm gelang es, die Seiten, die nur durch den anonymen Browser Tor erreichbar sind, durchsuchbar zu machen. Das Projekt verschlang Millionen. Der Yoga-Fan White flog mit hartgesottenen US-Soldaten im Black-Hawk-Helikopter über Afghanistan, er war die Schnittstelle zwischen den Soldaten und den Informatikern in den USA, die Informationen über die Taliban analysierten (mehr dazu in diesem Porträt Whites im Magazin Popular Science). Fotos von sich im Flecktarn in Kabul und die Auszeichnungen, die er vom Weißen Haus bekam, zeigt White während des Vortrages stolz auf der Leinwand.

Das Darknet war erst diese Woche Thema in Deutschland, als nach dem Mord an einem Neunjährigen in Herne die Information kursierte, Bilder der Tat seien im Darknet veröffentlicht worden (was vermutlich auf einer Verwechslung mit der Webseite 4chan basierte, die nicht zum Darknet gehört). White zeigt zum Thema den berühmten Cartoon aus dem New Yorker: Zwei Hunde vor einem Computer, einer sagt zum anderen: ""Im Internet weiß niemand, dass du ein Hund bist."" Das stimme aber gar nicht, sagt White, zumindest nicht für jene einfach zugängliche Oberfläche des Internets, in der Menschen Spuren über ihre Identität hinterlassen. ""Wenn Sie Ihr Hundsein wirklich geheim halten wollen, müssen Sie zu einem Netzwerk wie Tor gehen."" Moralisch sei das Darknet nun einmal ambivalent: ""Es erlaubt Menschen in Diktaturen, ihre Meinung zu sagen, oder anonym Katzenfotos anzusehen. Was in Tor passiert, ist aber zugleich sehr schlimm."" Er zeigt eine Wortwolke aus den häufigsten Begriffen, die in den Namen der versteckten Webseiten auftauchen. Die Worte ""Kinder"" und ""Sex"" kommen oft vor.

Mit der Kryptowährung Bitcoin kaufen Menschen über Tor Drogen, Waffen, gefälschte Pässe, holen sich Schadsoftware, mit der sie fremde Computer übernehmen können. Oder sie zeigen Rachepornos, um es ihren Verflossenen heimzuzahlen. Nicht alles darf man allerdings ernstnehmen. Die Anzeige: ""Albanische Mafia: Killer zu vermieten"", die White zeigt, dürfte eher ein Witz oder Abzocke als ein ernstzunehmendes Service-Angebot sein.
Analyse von Briefkastenfirmen, Ebola, Bush-E-Mails

Whites Arbeit bei Microsoft baut auf seiner Arbeit für Memex auf. Seine Big-Data-Technik kommt in verschiedensten Bereichen zum Einsatz: Er behauptet, er finde in Sekunden Ermittlungsansätze, für die Polizisten Wochen brauchen würden. Memex hat Geldflüsse zwischen Briefkastenfirmen analysiert, aber auch einen Ebola-Ausbruch und ein Netz von Telefonnummern, die Betrüger nutzen, die sich als Windows-Kundenservice auszugeben. So versuchen sie, leichtgläubige Computernutzer abzuzocken.

Eines seiner Programme wertete 250 000 veröffentlichte E-Mails von Jeb Bush aus. Es analysiert dessen Zeit als Gouverneur von Florida und stellt die Ergebnisse grafisch dar. Wie oft kommunizierte Bush mit wem? An wen wurden Mails weitergeleitet? Wie lange dauerte es bis zur Antwort? So entstand eine Übersicht über das Innerste der Macht in Florida.

White zeigt eine schwarze Karte des Nahen Ostens. Englische und arabische Wörter flackern darüber. Das Programm analysiert nicht nur, wie oft in sozialen Medien Worte wie ""Islamischer Staat"" erwähnt werden, sondern auch, ob in positivem oder negativem Kontext: Propaganda und Gegen-Propaganda. All der Aufwand führt allerdings manchmal zu wenig überraschenden Ergebnissen: ""Hier in Texas wird 'ISIS' eher im negativen Sinne verwendet.""

Was White als Weltverbesserung anpreist, hat für seinen Arbeitgeber natürlich einen etwas profaneren Hintergrund: Seine Forschung dient Microsofts wertvoller Sparte ""Business Intelligence"". Mit der sollen auch Unternehmen auf militärischem Niveau recherchieren können. ";https://www.sueddeutsche.de/digital/internet-der-mann-der-das-darknet-ausleuchtet-1.3415580;sz.de;Jannis Brühl
13.02.2017;Wie deutsche Politiker langsam Facebook und Twitter kennenlernen;"Für die Kommunalpolitikerin Christine Wernicke bringt Facebook, man mag es kaum glauben, eine ""Versachlichung"" in viele Debatten. Von wegen Shitstorms - die habe es nicht mal beim Thema Windkraft gegeben, wo sonst die Emotionen auf dem flachen Land hochschlagen. Im Gegenteil, ""die Leute kommen gut informiert und viel häufiger als früher, sogar zu Gemeindevertreterversammlungen"", sagt die Ex-Bürgermeisterin des Ortes Uckerland in Brandenburg.

Christine Wernicke, 56, parteilos, hat sich ihre positive Beziehung zu den sozialen Netzwerken erarbeitet. Sie ließ sich mit dem Programm ""Politikerinnen im Netz"" von einer jungen Social-Media-Expertin schulen. Denn eines ist klar: Wer digital nicht auffindbar ist, kann im Wahljahr 2017 wenig gewinnen. ""Viele Politikerinnen sind zwar in den sozialen Medien irgendwie präsent, aber oft ohne konkrete Aktivitäten oder Strategien"", beobachtet Christiane Bonk vom EAF-Institut in Berlin. Da die Parteienbindung der Menschen nachgelassen und ihre Mediennutzung sich stark verändert habe, wären die sozialen Medien ein immer wichtigerer Kanal, nicht nur im Wahlkampf.
Bloggerinnen beraten Politikerinnen im Umgang mit dem Internet

Für ihr Helene-Weber-Kolleg, das Frauen für die Politik gewinnen will und kommunalpolitisch aktive Frauen coacht, drehten die Organisatorinnen die Generationenverhältnisse um: Jüngere Bloggerinnen, Netzaktivistinnen, Medienexpertinnen zeigen Politikerinnen von 40 Jahren an, wie sie online besser dastehen können.

Die Tandems trafen sich über ein Jahr hinweg, beide Seiten betraten Neuland. Manche Mentee lernte Facebook überhaupt erst kennen und nutzt es nun virtuos. Ex-Bürgermeisterin Wernicke mischt auf der Webseite ""Wir sind Uckerland"" mit; endlich könne sie jüngere Menschen aus dem Ort erreichen, sagt sie. Als es einen Aufschrei gegen höhere Kita-Gebühren gab, klärte sie mit sachlichen Posts über das Kommunalrecht auf. ""So konnten wir Betroffene motivieren, nicht alles hinzunehmen und sich zu informieren.""

Bei der Wahl im vergangenen Jahr verlor die Bürgermeisterin gegen ihren SPD-Herausforderer - mit 21 Stimmen. Wegen Unregelmäßigkeiten bei der Abstimmung läuft vor dem Verwaltungsgericht Potsdam eine Klage. Muss die Wahl wiederholt werden, will Wernicke ""mutiger"" auftreten: mit einer perfekt gestalteten Facebook-Seite. Wie man auf Instagram Fotos gezielt für den Wahlkampf einsetzt, übt die Kandidatin einer Wählervereinigung gerade. Vor allem will sie sich besser verkaufen. Sie hat gelernt: ""Männer präsentieren sich und ihre Person. Frauen thematisieren eher Projekte und Probleme.""
""Flugblätter machen auch viel Arbeit.""

Dass der Kollege aus der Gemeinderatssitzung längst einen Beschluss getwittert hatte, als sie noch überlegte, was dieser langfristig bedeuten könnte, hat auch die Grünen-Politikerin Elke Zimmer erlebt. Seit Kurzem sitzt die 50-Jährige im Landtag von Baden-Württemberg. Davor, im Wahlkampf, musste sie sich von ihren Kindern anhören: ""Du glaubst doch nicht, dass die Leute am Samstag zu deinem Stand vor dem Supermarkt gehen. Wenn sie die Plakate sehen, googeln sie dich erst mal.""

Daraufhin hat Zimmer ebenfalls das Seminar des Helene-Weber-Kollegs besucht. Nun twittert und postet sie, aber noch zurückhaltend. ""Ich will nicht dauernd online sein, das ist viel Arbeit"", sagt sie und überlegt dann: ""Andererseits machen Flugblätter auch viel Arbeit.""

Der undankbare Job, beim Zettelverteilen vom zufälligen Interesse oder der Laune der Passanten abzuhängen, könnte langfristig sowieso wegfallen. Beim Wahlkampf in den USA hatte Donald Trump ein Analyse-Unternehmen beauftragt, das Wähler in den sozialen Netzwerken mit passgenauer Wahlwerbung kontaktiert. Die Experten bedienten sich dafür der persönlichen Spuren, die Menschen beim Besuch im Netz hinterlassen.
Social Media könnte auch deutsche Wahlkämpfe massiv beeinflussen

Solche Ressourcen zu nutzen sei allerdings teuer und hierzulande noch nicht verbreitet, sagt der Münchner Politikwissenschaftler Simon Hegelich. Er rechnet für 2017 mit dem ""letzten traditionellen Wahlkampf in Deutschland"". Die Fachgespräche mit dem Professor für Political Data Science an der TU München in den Fraktionsräumen des Bundestags sind regelmäßig ausgebucht, denn 96 von 100 Bundestagsabgeordneten sind in den sozialen Medien aktiv. Hegelich prognostiziert: Schon bald könne Wählerbeeinflussung in den sozialen Netzwerken stark auf demokratische Prozesse in Deutschland einwirken.

Die Grünen-Landtagsabgeordnete Zimmer hat vor allem ihre politischen Inhalte im Kopf. Mit der 140-Zeichen-Grenze bei Twitter fremdelt sie noch. ""Großartig Inhalte verpacken kann man da nicht, das sind nur Häppchen. Aber offenbar reicht diese Inhaltstiefe einigen Menschen erst einmal"", sagt sie. Gecoacht wurde sie von Pia Lauck, einer Social-Media-Beraterin aus Wiesbaden, die Webseiten entwirft. Lauck sagt, dass bei Parteien und Politikern oft noch zu wenig Zeit und Know-how vorhanden seien, um die digitale Präsenz zu pflegen: ""Das ist noch sehr verhalten alles.""

Für einen erfolgreichen Auftritt rät die IT-Frau ihren Kunden, sich genau zu überlegen, wo sie die Grenze zwischen Privatem und Mandat ziehen wollen. Das sei eine Gratwanderung, es schaffe aber Vertrauen beim Wähler, ab und zu auch Einblick ins Leben jenseits der Politik zu geben - also ruhig auch mal ein Urlaubsfoto einzustellen. Ihr wichtigster Rat: ""Bei jedem Post authentisch sein. Das ist genauso wie ein Auftritt im Wirtshaus.""

Wie viele Facebook-Freunde Elke Zimmer von den Grünen als angehende Online-Politikerin schon gewonnen hat? Diese Zahl verbirgt sie schamhaft auf ihrer Facebook-Seite. Noch.";https://www.sueddeutsche.de/digital/social-media-wie-deutsche-politiker-langsam-facebook-und-twitter-kennenlernen-1.3373532;sz.de;Ulrike Heidenreich
03.02.2017;Voll daneben;"Die Ära von Big Data ist von einem Paradox geplagt: So viele Informationen wie nie zuvor stehen frei zur Verfügung, und dennoch sind verlässliche Aussagen über die Zukunft rar. Welcher Demoskop hätte vor einem Jahr gewusst, dass Großbritannien aus der EU austritt und Donald Trump US-Präsident wird? In beiden Fällen lagen selbst ausgefeilte mathematische Modelle grob daneben.

Mehrere Artikel in der aktuellen Ausgabe des Wissenschaftsmagazins Science versuchen zu klären, was Vorhersage-Techniken derzeit noch leisten können. Die gute Nachricht: Trotz Versagens bei Brexit und Trump schneiden Demoskopen insgesamt gut ab. Vor den US-Wahlen 2012 sagte der Datenwissenschaftler Nate Silver die Ergebnisse aller 50 Bundesstaaten richtig voraus, in vielen Industrienationen gelingen Politbarometer bis auf wenige Prozentpunkte oder Kommastellen genau. In Science berichtet ein Team von der Universität Houston von einem mathematischen Modell, das bei etwa neun von zehn Wahlen korrekt vorhersagt, ob der Amtsinhaber gewinnt oder der Herausforderer. Das Prognose-Werkzeug ist dabei nicht auf ein Land beschränkt. Die Wissenschaftler speisten es mit 621 Wahlergebnissen seit 1945 aus 86 verschiedenen Ländern, um zu einem globalen Wahlmodell zu kommen. Faktoren wie der Grad der Demokratisierung, die Stärke der Wirtschaft oder die Außenpolitik fließen darin ein. So sagten die Forscher beispielsweise zehn von elf Wahlen in Südamerika korrekt voraus. Der stärkste Helfer für die Prognose war aber nicht der Zustand der Wirtschaft, sondern unabhängige Wählerbefragungen vor Ort. ""Umfragen können nicht nur in den USA, sondern global dazu dienen, Wahlen vorherzusagen"", sagt der Autor der Studie Ryan Kennedy. Es sei ein Fehler, auf solche Umfragen künftig zu verzichten - Kritik daran wurde zuletzt nach Trumps Überraschungssieg laut.
Wo der Zufall eine Rolle spielt, wird es schwierig, Vorhersagen zu treffen

Doch warum gelang es trotzdem in den USA nicht, den Wählerwillen abzubilden? ""Wenn Vorhersagen nicht perfekt sind, könnte der Grund in zu wenigen Daten liegen"", schreibt Jake Hofman von der Forschungsabteilung von Microsoft in einem weiteren Science-Artikel. ""Aber es könnte auch sein, dass das Phänomen selbst unvorhersehbar ist und die Genauigkeit der Vorhersage an eine fundamentale Grenze stößt."" Ein Rest Unsicherheit bleibe selbst im besten Modell. Der Zufall spiele bei Ereignissen wie dem Wahlsieg Trumps eine Rolle, argumentiert Hofman. Und der lasse sich nicht wegrechnen.

Das Problem ist, dass diese Unsicherheit wohl zunehmen wird. ""Faktoren, die Prognosen früher Sicherheit gegeben haben, nehmen ab"", sagt der Politikwissenschaftler Eric Linhart von der TU Chemnitz. Beispielsweise sinke die Identifikation mit einer bestimmten Partei in vielen Ländern, darunter Deutschland. Zudem sind Wahlentscheidungen zunehmend von Stimmungen abhängig, Ereignisse wie ein Terroranschlag können Emotionen schnell kippen lassen. Weiß ein Wähler fünf Minuten vor Stimmabgabe selbst nicht, wo er sein Kreuz macht, ""dann kann man das auch nicht prognostizieren"", so Linhart. Laut Hofman müssen Demoskopen eine Grenze für Vorhersagen akzeptieren - und mit der Unsicherheit leben lernen.

Diese Demut ist neu. Lange galt als ausgemacht, dass mehr Daten automatisch zu besseren Ergebnissen führen. ""Mit genug Daten sprechen die Zahlen für sich selbst"", schrieb Wired-Chefredakteur Chris Anderson 2008. Nun bemerkt der Konfliktforscher Lars-Erik Cederman nüchtern, ""die Hoffnung, dass Big Data irgendwie valide Vorhersagen ermöglicht, ist fehl am Platz"".";https://www.sueddeutsche.de/wissen/wahl-prognosen-voll-daneben-1.3362628;sz.de;Christoph Behrens
03.02.2017;Demoskopen brauchen Demut;"Lange hieß es: Je mehr Daten es gibt, umso genauer lassen sich Vorhersagen über politische Entscheidungen treffen. Dann stimmte Großbritannien für den Brexit und die USA wählte Donald Trump zum Präsidenten. In beiden Fällen lagen Wahlforscher trotz ausgefeiltester mathematischer Modelle, gespeist von Tera-, Peta- und Exabytes an Daten, grob daneben.

Eine Reihe Artikel im Wissenschaftsmagazin Science versucht nun zu klären, was Big Data und Vorhersage-Techniken heute angesichts politischer und wirtschaftlicher Krisen leisten können. Die Antwort: Es wird komplizierter gute Prognosen zu treffen. Aber es ist weiterhin möglich - und etwas mehr Demut vonseiten der Wissenschaftler wäre dabei hilfreich.
Menschen verhalten sich selten wie berechnet

Zunächst der positive Teil: Zwar lagen Demoskopen bei Brexit und der aktuellen US-Wahl größtenteils falsch, insgesamt schneiden sie aber nicht schlecht ab. Bei den US-Wahlen 2012 etwa sagte der Datenwissenschaftler Nate Silver die Ergebnisse aller 50 Bundesstaaten richtig voraus, auch andere Wahlforscher lagen sehr nahe am tatsächlichen Ergebnis. In Science berichtet ein Team um Ryan Kennedy von der Universität Houston nun von einem neuen mathematischen Modell, das 80 bis 90 Prozent aller Wahlen korrekt vorhersagen kann. Meist weiß ihr Programm im Vorfeld einer Wahl, ob der Amtsinhaber gewinnt oder der Herausforderer. Bemerkenswert ist an dem Prognose-Werkzeug, dass es nicht auf ein Land beschränkt ist. Die Datenwissenschaftler verwendeten als Grundlage 621 Wahlergebnisse seit 1945 aus 86 verschiedenen Ländern, um zu einem globalen Wahl-Modell zu kommen. So unterschiedliche Faktoren wie der Grad der Demokratisierung, die Stärke der Wirtschaft oder die Außenpolitik eines Landes fließen darin ein - aber auch die Ergebnisse von Wählerbefragungen vor Ort. Anhand dieser 21 Faktoren sagten die Forscher zehn von elf Wahlen in Südamerika zwsichen 2013 und 2014 korrekt voraus, und 29 von 36 Wahlen in anderen Weltregionen.

Überraschenderweise hatte die wirtschaftliche Lage eines Landes kaum einen Einfluss, ob der Amtsinhaber gewinnt oder der Herausforderer. Ein stärkerer Indikator war beispielsweise, wie viele internationale Hilfsgelder ein Staat erhält - je mehr, umso bessere Chancen hatten neue Parteien. Der beste Faktor für die Pronose seien aber unabhängige Umfragen unter den Wählern, schreiben die US-Forscher. ""Umfragen können nicht nur in den USA, sondern global dazu dienen, Wahlen vorherzusagen"", sagt der Autor der Studie Ryan Kennedy. Es sei daher ein Fehler, auf solche Umfragen künftig zu verzichten - Kritik daran wurde beispielsweise nach der US-Wahl laut.

Doch warum gelang es in den USA trotzdem nicht, den Wählerwillen verlässlich abzufragen? Hier beginnt der aus wissenschaftlicher Sicht unangenehme Teil. Jake Hofman von der Forschungsabteilung von Microsoft zieht zur Erklärung zwei Extreme der Vorhersehbarkeit heran: regelmäßige Ereignisse und sogenannte ""Schwarze Schwäne"". Regelmäßige Ereignisse sind einfach vorherzusagen. Beispielsweise zeigte eine Studie von 50 000 Handynutzern, dass diese sich 70 Prozent der Zeit an ihrem bevorzugten Ort aufhalten, beispielsweise dem Arbeitsplatz oder Zuhause. Mit 70-prozentiger Genauigkeit lässt sich also recht einfach vorhersagen, wo sich jemand zu einem beliebigen Zeitpunkt befindet. ""Schwarze Schwäne"" dagegen werden Ereignisse wie die Finanzkrise von 2008 genannt. Sie sind sehr selten und völlig unmöglich vorherzusagen. Das Ergebnis einer US-Präsidentschaftswahl bewegt sich in ihrer Vorhersehbarkeit irgendwo dazwischen - eine richtige Prognose ist zwar nicht völlig unmöglich, aber auch nicht völlig sicher.

""Wenn Vorhersagen nicht perfekt sind, könnte das an einer zu geringen Datenmenge liegen"", schreibt Hofman, ""aber es könnte auch sein, dass das Phänomen selbst unvorhersehbar ist und die Genauigkeit der Vorhersage an eine fundamentale Grenze stößt"". Das bedeutet, dass eine komplexe Welt sich niemals vollständig vermessen lässt, sondern immer ein Rest Unsicherheit bleibt. Der Erfolg Donald Trumps lässt sich nicht allein mit seinem Auftreten in den Medien, seinen politischen Fähigkeiten oder dem Zustand der USA erklären. Auch der Zufall spielt eine wichtige Rolle, und dies lasse sich nicht durch genauere Modelle wegrechnen , argumentiert Hofman. Je höher solche externen Zufallsfaktoren, umso niedriger ist die Chance einer richtigen Vorhersage. Das Problem für Wahlforscher ist derzeit, dass diese fundamentale Unsicherheit steigt. ""Faktoren, die Prognosen früher Sicherheit gegeben haben, nehmen ab"", sagt der Politikwissenschaftler Eric Linhart von der Technischen Universität Chemnitz. Beispielsweise sinke die Identifikation mit einer bestimmten Partei in vielen Ländern, auch in Deutschland. ""Da ist insgesamt mehr Spiel drin"", sagt Linhart. Wenn ein Wähler bis fünf Minuten vor der Stimmabgabe selbst nicht wisse, für wen er stimmen wird, ""dann kann man das auch nicht prognostizieren"". Wahlentscheidungen seien heute zunehmend ""von augenblicklichen Stimmungen abhängig"", sagt der Soziologe Sighard Neckel von der Uni Hamburg. Ereignisse wie ein Terroranschlag könnten die Emotionen schnell kippen lassen. Daher steige die Unsicherheit von Prognosen tendenziell. ""Die Volatilität hat zugenommen"", sagt Neckel. Laut Microsoft-Forscher Hofman müssen Demoskopen daher eine obere Grenze der Genauigkeit für Vorhersagen akzeptieren - und mit der Unsicherheit leben lernen.

Diese neue Demut ist eine kleine Überraschung im Zeitalter der großen Datenmengen. Schließlich galt es bisher als ausgemacht, dass eine größere Datenmenge allein zu besseren Ergebnissen führt. 2008 proklamierte der Chefredakteur der Zeitschrift Wired, Chris Anderson, gar das ""Ende der Theorie"". In der Ära der Petabytes brauche es keine Modelle oder Hypothesen mehr, ""mit genug Daten sprechen die Zahlen für sich selbst"". Nun bemerkt der Konfliktforscher Lars-Erik Cederman nüchtern, ""die Hoffnung, dass Big Data irgendwie valide Vorhersagen ermöglicht"", durch eine Art theorie-freie, rohe Gewalt der Zahlen, ""ist fehl am Platz"".
Big-Data-Prognosen sind trotz Kritik weiter auf dem Vormarsch

In der Konfliktforschung wird beispielsweise seit Jahren anhand großer Datenmengen versucht vorherzusagen, wo und wann in einer Region Gewalt aufflammt. Es gebe zwar vereinzelte Fortschritte bei diesem Unterfangen, ein verlässliches ""Frühwarnsystem"" für Gewalt sei aber nicht in Sicht. Als Grund führen die Forscher eine ""massive historische Komplexität"" und menschengemachte Eventualitäten an. Historische ""Unfälle"" wie der Brexit oder der Erfolg Donald Trumps verhöhnten noch die besten Modelle.

Allerdings ist diese Bescheidenheit noch nicht überall angekommen. Ganz im Gegenteil: Big-Data-Prognosen sind weiter auf dem Vormarsch. Mittlerweise entscheidet Vorhersage-Software in einigen US-Städten, in welche Viertel Polizisten oder Brandschutz-Inspektoren geschickt werden, weil der Algorithmus an dem betreffenden Ort einen möglichen Gefahrenherd wittert. In der Medizin setzen einzelne Tools Patienten auf der Warteschlange für dringend benötigte Operationen nach oben - weil bei ihnen laut Algorithmus weniger Komplikationen zu erwarten sind. Heikle Fragen liegen bei solchen Anwendungen auf der Hand. So könnte ein Hausbesitzer oder Restaurantbetreiber Sicherheitsvorkehrungen absenken, sobald er herausgefunden hat, dass sein Risiko von einem Brandschutzinspektor kontrolliert zu werden gering ist. Ein tiefergehendes Problem hat mit einem mangelnden Verständnis der Big-Data-Modelle zu tun. Die Softwares basieren häufig auf dem Prinzip des überwachten maschinellen Lernens. Ein Algorithmus wird mit einer großen Menge ""Trainingsdaten"" gefüttert - aus diesen gewinnt er Regeln für den Datensatz. Ein fiktives Beispiel: Wenn das Wirtschaftswachstum gering ist, hat eine linke Partei bessere Chancen auf einen Wahlsieg, könnte ein solches Programm aus historischen Wahldaten ableiten. Die besten derartigen Modelle sind aber weitaus komplexer, so komplex, dass die Regeln die sie aufstellen, für ihre menschlichen Entwickler teilweise gar nicht mehr nachvollziehbar sind. Beispielsweise könnte der Computer zu dem Schluss kommen, dass es auch etwas mit dem Wetter oder dem Aktienkurs von Toyota zu tun hat, ob ein linker Politiker gewinnt. Möglicherweise ist das reiner Zufall, möglicherweise führen diese Faktoren tatsächlich im Durchschnitt zu besseren Prognosen. Nur lassen diese sich dann nicht mehr vernünftig erklären.";https://www.sueddeutsche.de/wissen/big-data-demoskopen-brauchen-demut-1.3362769;sz.de;Christoph Behrens
18.11.2016;Fürchtet euch nicht!;"Das ausgeklügelte Computersystem war eigentlich dazu erfunden worden, Feinde abzuwehren, doch plötzlich erkennt es den Menschen als größte Bedrohung der Welt und setzt an zu seiner Vernichtung. Ein Mann verliebt sich in eine Roboterfrau, verhilft ihr zur Flucht, nur um von ihr ohne jede Gnade ausgenutzt zu werden. ""Terminator"", ""Ex Machina"" oder andere Science-Fiction-Filme - an Vorstellungen darüber, was einer künstlichen Intelligenz (KI) zuzutrauen wäre, die der des Menschen gleichkommt, fehlt es nicht. Die Sache ist nur: Eine solche starke KI gibt es nicht, und es wird sie wohl auch nie geben.

Die Furcht davor aber gibt es, und sie ist falsch. Angst vor Big Data und künstlicher Intelligenz ist schon deshalb falsch, weil Angst immer ein schlechter Ratgeber ist. Wegducken hilft auch nicht: Erfindungen mit derart großem Potenzial verschwinden nicht einfach wieder. Es wird also nichts helfen, maschinenstürmerisch gegen diesen Trend zu kämpfen. Intelligente Computer werden zunehmend Tätigkeiten übernehmen, die heute nur Menschen bewältigen können.

Also: Die Furcht vor einer Herrschaft superschlauer Maschinen ist übertrieben. Doch einfachere Tätigkeiten lassen sich von spezialisierten Maschinen effizient und kostengünstig erledigen. Das ist keine Fiktion mehr, sondern geschieht bereits. Und ihre Weiterentwicklung wird schneller vorangehen, als manche glauben. Nicht nur der Staat für die Bürger, auch jeder für sich sollte sich fragen, was er dafür tut, eben nicht ersetzbar zu sein.
Jeder sollte einen Begriff davon haben, was sich mit Daten anstellen lässt

Wie jedes von Menschen erfundene Werkzeug lässt sich auch diese Technologie zum Guten wie zum Schlechten nutzen. Wenn Maschinen Daten analysieren und auf dieser Basis Entscheidungen treffen, können etwa die Verkehrsströme in den Städten effektiver und damit umweltschonender gelenkt oder Produktionsprozesse optimiert werden - dagegen kann niemand etwas haben. Wenn aber Konzerne mit den Daten der Menschen Wissen und damit Markt- und Manipulationsmacht ansammeln, wenn Staaten ihre Bürger durch Massenüberwachung unter Generalverdacht stellen, ist das Potenzial für Missbrauch gewaltig.

Diese Entwicklung aber wird kommen, und entweder man versucht, sie mitzugestalten, oder aber man läuft Gefahr, überrollt zu werden. Einfach wird das nicht, Gesellschaft und Politik stehen vor großen, vielfältigen Aufgaben. Diejenigen, deren Jobs tatsächlich oder auch nur gefühlt bedroht sind, müssen ernst genommen werden. In der Aus- und Weiterbildung müssen die neuen Technologien breiteren Raum bekommen. Jeder sollte zumindest einen Begriff davon haben, was sich mit Daten anstellen lässt. Müssen wir also alle Technokraten werden? Nein, was den Menschen auszeichnet, Kreativität und Emotionalität, darf nicht zu kurz kommen. Die öffentliche Hand und die Industrie müssen aber mit Geld, mit Beratung und konkreten Hilfen Start-ups und Digitalisierungsprojekte noch stärker fördern.

Den neuen Möglichkeiten sollten Bürger, Regierende und die Wirtschaft nicht mit diffuser Angst begegnen, sondern sie als Chance begreifen. Sie nicht zu nutzen, würde Deutschland und Europa rascher ins wirtschaftliche Abseits befördern, als viele sich das vorstellen können. Die datengetriebene Welt dreht sich schnell. Viele milliardenschwere Digitalfirmen gab es vor zehn, 15 Jahren noch nicht. Aus Deutschland kommen davon nur wenige.";https://www.sueddeutsche.de/wirtschaft/kommentar-fuerchtet-euch-nicht-1.3258053;sz.de;Helmut Martin-Jung
18.11.2016;Wenn das Idioten-freie Unternehmen kommt, sind viele Jobs bedroht;"Am Ende soll das ""Arschloch-freie Unternehmen"" stehen. Diese Art von Kollegen überflüssig zu machen, so geht die Legende, ist die wahre Mission von Chris Boos' Unternehmen: Arago automatisiert mit Hilfe künstlicher Intelligenz IT-Abläufe, um die sich sonst Menschen kümmern müssten. Aber dass ein Plakat, das das Motto mit der arschlochfreien Zone ausgibt, tatsächlich in der Firmenzentrale in Frankfurt hängt, bestätigt der Chef Boos auf der Bühne des SZ-Wirtschaftsgipfels. Dass er das überhaupt gefragt wird, amüsiert ihn sichtlich.

Die folgende Debatte ist ernst: Intelligente Algorithmen sollen den Menschen das Unangenehme abnehmen. Aber was, wenn die Algorithmen selbst unangenehm werden?

KI wird in immer mehr Bereichen eingesetzt, und sie macht vielen Angst. Weil sie den Menschen zum ""Datenhaufen"" degradiere, der nur noch zur Auswertung da sei und kein eigenständiges Subjekt mehr. Denn eben jene Datenhaufen, die den Algorithmen als ""Lehrmaterial"" dienen, anhand derer sie Muster und Zusammenhänge lernen, sind eben auch die Grundbausteine eines Überwachungsstaates. Das sagt Yvonne Hofstetter, die zwei Stühle neben Boos sitzt. Wenige warnen so vehement vor den gesellschaftlichen Kosten von Algorithmen wie die Chefin des Big-Data-Unternehmens Teramark. Wie Maschinen Industriearbeiter ersetzten, werde künstliche Intelligenz nun einfache geistige und Verwaltungsarbeiten übernehmen. Nun müsse die Gesellschaft diskutieren, was mit den Menschen passiert, die derzeit diese Jobs machen. ""Wie kriegen wir sie satt? Wie kriegen wir deren Kühlschränke voll?"", fragt Hofstetter.
Gut oder Böse? Die Debatte über künstliche Intelligenz verrät viel über die Menschen

Die Gruppe auf dem Podium bildet die Komplexität der Debatte ab: Neben der Mahnerin Hofstetter sitzen zwei Praktiker aus der IT-Wirtschaft: Neben Boos ist das Vishal Sikka, der den IT-Konzern Infosys leitet. Beide erklären nüchtern, was die Technik derzeit kann: Zum einen menschenähnlich erscheinende Dinge wie das Erlernen von Sinnesfähigkeiten durch Algorithmen, etwa maschinelles Hören und Sehen, wie es in selbstfahrenden Autos eingesetzt wird. Anderes klingt völlig profan, wenn Boos es formuliert: Künstliche Intelligenz übernehme die ""Abarbeitung von Prozessen"". Ängste vor politischer Einflussnahme, etwa durch Meinungsbots, wie sie in Donald Trumps Wahlkampf massenweise eingesetzt wurden, hält Boos für einen Witz: ""Über diese Wahlbots lacht jeder, der Künstliche Intelligenz macht. Das sind ja ganz primitive Konstrukte.""

Der vierte Podiumsgast hat die Draufsicht: Markus Gabriel. Der Philosophieprofessor von der Universität Bonn ordnet die Debatte als Geisteswissenschaftler ein. Dazu gehört die Absage an naive Science-Fiction-Vorstellungen: ""Starke Künstliche Intelligenz"" - also jene Form, die tatsächlich denken kann wie ein Mensch, werde es nicht geben, sagt Gabriel. ""Das ist seit den Achtzigern zwischen Forschern und Philosophen geklärt."" Also kein Terminator, keine Menschen aus Silizium. Warum reden dann alle davon?

""Jetzt kommt eine neue ideologische Welle, die uns die starke künstliche Intelligenz wieder als Möglichkeit verkaufen will."" Gabriel meint wohl den Kreislauf der Aufmerksamkeit: Start-ups, besonders aus dem Silicon Valley, preisen ihre Technik in übertrieben hohen Tönen, unkritische Tech-Journalisten verursachen weiteren Auftrieb - und kritische Autoren wie Hofstetter beschreiben sie als Gefahr.
Wie steht der Computer zum Brexit? Und wer will das wissen?

Das kann den Blick auf die tatsächlichen Entwicklungen aber verstellen. Eine der wichtigsten ist, dass Google und Facebook viel Geld investieren, die besten Forscher zu künstlicher Intelligenz von Universitäten und kleineren Unternehmen abwerben und so ihre Macht zementieren. Auf dem Podium ist Hofstetters Argument der Massenarbeitslosigkeit durch Automatisierung und künstliche Intelligenz die überzeugendste Kritik. Auch die beiden IT-Unternehmer schaffen es nicht, die zu entkräften.

Philosoph Gabriel findet an der Debatte wichtig, was sie über uns selbst erzählt: ""Sie sehen nur zwei Meinungen zur künstlichen Intelligenz: Sie wird entweder verteufelt oder zum Göttlichen erklärt. Es geht einfach um Gut gegen Böse."" Er mahnt, künstliche Intelligenz nicht zu sehr zu vermenschlichen. Es gebe zwar immer bessere Schachcomputer, aber: ""Es gibt keinen, der Schach spielt, dann italienisch kocht, sich überlegt, welcher Rotwein dazu passt und danach Stellung zum Brexit bezieht."" ";https://www.sueddeutsche.de/digital/kuenstliche-intelligenz-wenn-das-idioten-freie-unternehmen-kommt-sind-viele-jobs-bedroht-1.3255663;sz.de;Jannis Brühl
21.10.2016;AfD will Wahlkampf mit Meinungs-Bots machen;"Gerade erst hat Bundeskanzlerin Angela Merkel (CDU) empfohlen, die Parteien in Deutschland sollten sich gemeinsam gegen den Einsatz von sogenannten Social Bots einsetzen. Nun kündigt die AfD an, im kommenden Jahr genau solche Programme zu verwenden. ""Selbstverständlich werden wir Social Bots in unsere Strategie im Bundestagswahlkampf einbeziehen"", sagte Bundesvorstandsmitglied Alice Weidel dem Spiegel.

Social Bots sind Programme, die automatisch Beiträge in sozialen Netzwerken verbreiten, ohne dass erkennbar ist, dass dahinter keine reale Person steckt. Bei der AfD werden sie als ""wichtige Instrumente"" betrachtet, um für die politischen Positionen der Partei zu werben.

Merkel hatte das Thema auf dem Deutschlandtag der Jungen Union am vergangenen Wochenende angesprochen und gefragt: ""Wollen wir mal zwischen den Parteien sprechen, ob wir gemeinsam dagegen kämpfen?"" Nach ihrem Plädoyer hat CDU-Generalsekretär Peter Tauber den Einsatz der Technik im Wahlkampf bereits ausgeschlossen. ""Der AfD geht es nicht um eine echte inhaltliche Auseinandersetzung, sondern nur um Krawall und Pöbelei"", sagte Tauber dem Spiegel. Es sei wichtig, auch künftig zu erkennen, was echte Äußerungen von Bürgern seien und was nicht, forderte er der Nachrichtenagentur Reuters zufolge. ""Auch deshalb müssen wir Nachrichten von Bots erkennen, die die Realität verfälschen, damit es keine Themenagenda vorbei an den wirklichen Interessen der Leute gibt.""

Wie Reuters berichtet, sagen SPD, Grüne und FDP ebenfalls, dass sie keine Fake-Accounts einsetzen und dies auch im Wahlkampf 2017 nicht tun würden. ""Manipulation durch Social Bots lehnen wir ab"", sagte zum Beispiel der Bundesgeschäftsführer der Grünen, Michael Kellner. ""Wenn sich die Parteien untereinander darüber verständigen können, begrüße ich das sehr."" Auch für die FDP komme ein Einsatz der Programme zur Meinungsmache in den sozialen Netzwerken nicht infrage, sagte FDP-Sprecher Nils Droste. SPD-Generalsekretärin Katarina Barley sagte dem Spiegel, die sozialen Medien würden im SPD-Wahlkampf eine wesentliche Rolle spielen. ""Aber den Einsatz von Bots lehnen wir ab."" Ähnlich äußerte sich die Linke. Die Bezeichnung ""Bots"" leitet sich zwar vom englischen ""robots"" ab, mit Robotern haben sie dennoch nichts zu tun. Zentraler Unterschied: Roboter sind Hardware, Bots sind Software. Einen Roboter kann man immer physisch anfassen, während ein Bot ein Computerprogramm ist, das automatisiert bestimmte Aufgaben übernimmt, die sonst von Menschen erledigt werden.

Der spezielle Fall der Social Bots bezeichnet Accounts in sozialen Netzwerken, meist bei Twitter, die ähnlich interagieren wie echte Menschen. Sie werden etwa eingesetzt, um Werbebotschaften zu verbreiten oder politische Propaganda zu betreiben. Mittlerweile sind die zugrundeliegenden Algorithmen so weit fortgeschritten, dass es oft schwerfällt, Bots von menschlichen Nutzern zu unterscheiden.
Bots im Ukraine-Konflikt und bei der Brexit-Abstimmung

Bei vielen wichtigen politischen Ereignissen und Entscheidungen wurde bereits versucht, mit Bots Einfluss auf die öffentliche Meinung oder auf Abstimmungsergebnisse zu nehmen. So ist sich etwa Simon Hegelich sicher, dass im Ukraine-Konflikt 15 000 Twitter-Bots mitmischen. Der Professor für Political Data Science an der TU München sagte im Interview mit der taz aber auch, dass es derzeit nicht möglich sei, seriös zu schätzen, wie viele automatisierte Fake-Accounts es in sozialen Netzwerken gibt (immer wieder kursiert die Zahl von 15 Millionen Bots alleine auf Twitter).

Außerdem bezweifelt er, dass ""Bots ein geeignetes Instrument sind, um Massen umzudrehen"". Sie könnten höchsten extrem knappe Abstimmungen beeinflussen, in denen es auf wenige Stimmen ankommt.

Eine solche Entscheidung war zum Beispiel der Brexit. Zwei britische Wissenschaftler haben die entsprechenden Tweets aus der Woche vor dem Referendum untersucht und sind zu dem Schluss gekommen, dass computergesteuerte Propaganda vor allem im Pro-Brexit-Lager weit verbreitet war. Die Schlagzeile, dass Bots für den Brexit verantwortlich seien, gibt ihre Studie allerdings nicht her. Es ist kaum möglich, nachträglich herzuleiten, welchen Einfluss die Algorithmen tatsächlich auf das Abstimmungsverhalten der Wähler hatten.
Die computergesteuerte Propaganda von Donald Trump

Auch im US-Wahlkampf spielen Bots eine Rolle. Das Team von Donald Trump hat schon vor anderthalb Jahren begonnen, eine Propaganda-Infrastruktur aufzubauen. So sollen sich unter den rund 12,6 Millionen Accounts, die Donald Trump bei Twitter folgen, mehrere Millionen Bots befinden, die seine Tweets teilen und versuchen, Stimmung für die Republikaner zu machen. Doch die Demokraten haben längst nachgezogen und schicken ihrerseits ein Heer aus Programmierern ins Rennen, die mit Hilfe von Algorithmen Themen setzen und den Diskurs in den sozialen Medien bestimmen sollen. Anfang dieser Woche ergab eine Untersuchung des Oxford-Professors Philip Howard, dass nach der ersten TV-Debatte zwischen Clinton und Trump insgesamt mehr als 700 000 Tweets von Bots abgesetzt wurden - mehr als 570 000 davon kamen aus Richtung des Trump-Lagers. Allerdings hat Howard nicht untersucht, wer die Bots programmiert hat. Es muss also nicht die Republikanische Partei dahinterstecken. Genau wie Hegelich betont Howard, dass es keinen nachgewiesenen Effekt von Bots auf das Umfrage- oder Wahlergebnis gebe. Er vermutet aber, dass negative Tweets - also Angriffe auf die Gegenseite - eher Wirkung zeigten als positive.

Bots müssen allerdings nicht manipulativ oder aggressiv sein. Es gibt jede Menge vollkommen harmloser Bots - auch in der deutschen Politik. So generiert der Account @AFDHaikus etwa automatisch Haikus - eine Form japanischer Kurzgedichte - aus dem Wahlprogramm der AfD und teilt sie auf Twitter. Sein Einfluss auf die Bundestagswahl 2017 dürfte gering ausfallen.";https://www.sueddeutsche.de/politik/bundestagswahl-2017-afd-will-wahlkampf-mit-meinungs-bots-machen-1.3216593;sz.de;Reuters
29.09.2016;Idealer Datenmix;"Alexander Thamm ist Data Scientist. Zu ihm kommen Manager, die ihre Firma mithilfe von Daten optimieren wollen. Manchmal meinen sie, man müsste dafür nur alle Daten der Firma zusammenwerfen, den Computer rechnen lassen, fertig. Thamm kontert dann mit einem Vergleich. ""Stellen Sie sich vor, Sie sind in einer Bar"", hinter dem gut sortierten Tresen stünden mehrere Flaschen mit Whiskey, Wodka, Rum und Gin. Alles in einen Eimer gekippt ""kennt man vom Ballermann, das schmeckt wenigen"". Ein guter Barkeeper würde stattdessen nach den Vorlieben seines Gastes fragen und nur auf ausgewählte Zutaten zurückgreifen. So, sagt Thamm, müsse man sich den richtigen Umgang mit Big Data vorstellen.

Der richtige Umgang mit Massendaten: Auch in Bayern wird er immer wichtiger. In der Industrie schrumpfen die Wettbewerbsvorteile, Firmen aus anderen Ländern holen auf, weil sie ähnliche Produkte günstiger oder schneller anbieten können - oder gleich beides. Viele Unternehmer könnten sich daher vorstellen, durch Daten neue Impulse für ihre Firma zu generieren.

Der Präsident der Vereinigung der Bayerischen Wirtschaft (VBW), Alfred Gaffal, schreibt im Vorwort einer Studie zum Thema: ""Big Data ist der Schlüssel zur Entwicklung innovativer hybrider Geschäftsmodelle."" Wirtschaftsministerin Ilse Aigner ergänzte bei der Vorstellung der Studie: ""Durch das Wissen, das in den großen Datenmengen steckt, können Unternehmen effizienter werden und sich im Wettbewerb behaupten."" Und mit Blick auf die USA, dem Big-Data-Land Nummer Eins: ""Wir wollen anschieben, dass wir selbst die Nase vorn haben.""

Tatsächlich spielen Big-Data-Technologien im Freistaat bislang eine untergeordnete Rolle. Nur wenige bayerische Unternehmen haben entsprechende Projekte angestoßen, laut der VBW-Studie meist in der Automobil-, Pharma- und Chemieindustrie sowie in der Versicherungsbranche. Viele davon befänden sich aber derzeit in der Entwicklung und seien von der Marktreife weit entfernt. Wer sich unter vier Augen mit Mitarbeitern bayerischer Konzerne unterhält, hört Ähnliches.

Bei Big Data gibt es zwei Ansätze. Beim ersten werden anhand von Massendaten Prozesse innerhalb einer Firma optimiert. Ein Beispiel aus der Logistik: Jeder Lkw produziert Daten. Das können die Aufzeichnungen des Bordcomputers sein, die Protokolle des letzten Werkstattbesuchs oder Dinge, die der Fahrer während der Fahrt beobachtet. Aus diesen Daten lässt sich vorhersagen, wann das Fahrzeug vorsorglich für eine Inspektion in die Werkstatt sollte. Das Risiko sinkt, dass es mitten auf der Autobahn eine Panne erleidet. Und die Wahrscheinlichkeit steigt, dass die Ware pünktlich ankommt.

Im zweiten Fall werden die Massendaten genutzt, um Kunden maßgeschneiderte Angebote zu machen. So plant die HUK Coburg einen Telematiktarif für Kfz-Versicherungen. Eine Box im Auto sammelt Daten über das Fahrverhalten des Versicherten und sendet diese an die Versicherung. Entsprechend ändert sich der Tarif: Wer gut fährt, zahlt weniger.

Das Problem beginnt für die Unternehmen damit, die passenden Daten zu finden oder zu erheben. Es gibt immer mehr davon, jeder von uns produziert sie, jeden Tag - indem wir E-Mails versenden, Maschinen steuern, Handy-Apps nutzen oder per Kreditkarte bezahlen. Etwa alle zwei Jahre verdoppelt sich so die Datenmenge auf der Welt, 2015 war sie zwölf Zettabyte groß. Zum Vergleich: Auf eine handelsübliche, ein Terabyte große Festplatte passen bis zu 970 000 Minuten Musik. Um die einmal durchzuhören, bräuchte man rund 22 Monate. Würde man nun die weltweite Datenmenge von zwölf Zettabyte auf je ein Terabyte großen Festplatten speichern und diese Platten übereinanderstapeln, ragte der Turm 312 000 Kilometer empor. Bis zum Mond sind es 384 000 Kilometer.

Wer Big Data nutzen will, muss sich also zuerst überlegen, was er eigentlich will. ""Unternehmen müssen eine klare Vorstellung entwickeln, was sie konkret verbessern wollen"", sagt Data Scientist Thamm. Als Unternehmensberater für Big Data begreift er Massendaten als ein neuartiges Werkzeug, das Firmen hilft, komplexe Zusammenhänge sichtbar zu machen und punktuelle Verbesserungen vorzunehmen.

Andere finden es ganz in Ordnung, dass Bayern das Werkzeug nicht ausreizt. Datenschützer sehen die zunehmende Datensammelwut mit Sorge. Besonders heikel wird es, sobald Unternehmen Informationen über einzelne Menschen und deren Privatsphäre speichern. In den vergangenen Jahren kam es mehrfach vor, dass Hacker solche Datensätze stahlen und für kriminelle Geschäfte nutzten.

""Es ist gut, dass es kritische Stimmen gibt"", sagt Thamm. Solange einige wenige Konzerne wie Apple oder Facebook massenhaft Daten sammelten, bestehe die Gefahr, Wissen zu stark zu zentralisieren. Hier müsse die Politik reagieren und regulieren: ""Wir brauchen eine neue Datenschutzregel. Sie sollte schnell kommen, Ungewissheit hemmt Investitionen und dadurch Innovation."" Auch bei einigen bayerischen Unternehmen beobachtet Thamm eine zu starke Fixierung darauf, wie sich Kundendaten schnell zu Geld machen lassen. ""Solche Überlegungen dürfen nie der erste Schritt sein"", mahnt er.

Der Sache mit dem Datenschutz räumt auch die VBW Priorität ein. In der Studie fordert sie eine gesellschaftliche Wertedebatte: ""Den erheblichen Vorteilen von Big Data stehen Risiken gegenüber, die weder verschwiegen noch als Totschlagargument gegen solche technischen Innovationen ins Feld geführt werden dürfen."" Außerdem brauche jede Firma ein IT-Sicherheitskonzept, um die Daten vor fremdem Zugriff zu schützen.

Hier wartet ein anderes Problem. Der Fachkräftemangel ist in der IT-Branche angekommen. Laut einer anderen Studie von Intel Security gibt es zu wenige Experten, die solche Sicherheitskonzepte erstellen und pflegen könnten. Und für die Zukunft sei damit zu rechnen, dass Cyber-Angriffe auf Unternehmen weiter zunehmen. Intel Security gehört zum Chip- und Softwarehersteller Intel, der Standorte in München, Nürnberg und Regensburg betreibt.

Um den Ausbau von Big Data voranzutreiben, bräuchte es also mehr Experten. Die VBW fordert von der Politik daher eine Forschungsagenda für Big Data. Vorhandene Forschungseinrichtungen müssten um entsprechende Schwerpunkte erweitert werden. Außerdem müsse der Freistaat Big-Data-Lösungen bei der Technologieförderung stärker berücksichtigen.

Letztlich bleibt der Umgang mit Big Data gerade für kleine und mittelständische Unternehmen eine Herausforderung. Große Firmen können sich ein Daten-Experiment eher leisten. Aber langsam bewegt sich etwas. Als Thamm vor vier Jahren mit seiner Data-Science-Beratung startete, musste er oft erklären, was das eigentlich ist. Heute sagt er: ""Das Bewusstsein für Big Data ist gewachsen."" Trotzdem würde er sich mehr Plattformen wünschen, auf denen sich die Menschen über das Thema austauschen könnten. ""Alles wird immer komplexer und optimierter"", sagt er, ""für die meisten ohne Daten-Technologie ist das nicht mehr durchschaubar.""";https://www.sueddeutsche.de/bayern/wirtschaft-idealer-datenmix-1.3184215;sz.de;Maximilian Gerl
25.09.2016;Die Gesellschaft der Daten;"An diesem Montag beginnt in Bamberg der 38. Kongress der Deutschen Gesellschaft für Soziologie. Das Thema lautet ""Geschlossene Gesellschaft"". Als eben solche präsentiert sich aber erstaunlicherweise auch das Fach selbst. Die Soziologie grenzt sich, in einer rapide wandelnden Welt der Wissenschaft, beharrlich und konservativ von anderen Disziplinen ab und pocht bequem auf die Autonomie des Fachs.

Früher gab die Soziologie, mit geschärftem Blick für Institutionen und soziale Normen, anderen Disziplinen kritische Anstöße, etwa den Wirtschaftswissenschaften. Heute gibt es kaum noch wechselseitige Berührungspunkte. Stattdessen sind Ökonomie und Psychologie eine fruchtbare Verbindung eingegangen, nämlich in der Verhaltensforschung über soziale Normen, Fairness, Vertrauen, Reputation, kollektive Güter und soziale Kooperation. Davon wird jedoch in der Soziologie, obwohl es sich um Grundfragen des Fachs handelt, kaum Notiz genommen. Experimentelle Wirtschaftsforschung, Sozialpsychologie, Anthropologie und biologische Verhaltensforschung haben in den vergangenen Jahren enorme Fortschritte gemacht. Da wird über Grundfragen der Soziologie weit mehr, präziser und oft kenntnisreicher publiziert als in manchen soziologischen Fachblättern - etwa über das ""Hobbes'sche Problem"" der Entstehung, Stabilität und Erosion von gesellschaftlicher Ordnung.
Telefonumfragen? Beziehungen? Die Digitalisierung schüttelt auch die Sozialwissenschaften durch

Zwar findet man in der empirischen Sozialforschung - in Forschungen über Bildungschancen, Migration, Bevölkerung und Sozialstruktur - hervorragende Beiträge von Soziologinnen und Soziologen. Hat man bei solchen Themen früher viel unbefangener über kausale Zusammenhänge gesprochen, so ist es heute mit ausgefeilten statistischen Methoden und Längsschnittdaten möglich, den Ursachen und Wirkungen genauer auf die Spur zu kommen. Nur ist es eine Minderheit, die solche Methoden beherrscht und international beachtete Forschungsergebnisse publiziert.

""Let's Shake up the Social Sciences"" - zum Durchschütteln der Sozialwissenschaften hat kürzlich der Soziologie, Mediziner und Netzwerkforscher Nicholas A. Christakis in einem Essay in der New York Times aufgerufen und die Stagnation der Sozialwissenschaften beklagt. Auch in der empirischen Forschung versagen altbewährte Methoden. So sind seit Langem die sogenannten Response-Quoten bei Umfragen rückläufig. Telefonumfragen sind aber keineswegs mehr repräsentativ, wenn nur noch zehn bis zwanzig Prozent der zufällig ausgewählten Personen an der Befragung teilnehmen und viele nur noch über das Handy erreichbar sind. Die tatsächliche Teilnahmequote wird deshalb oft wie ein Geheimnis gehütet - gerade auch von den Wahlforschungsinstituten. Onlinepanels sind bisher keine Alternative und kranken oft noch stärker an Verzerrungen als telefonische Umfragen. Natürlich gibt es auch hochwertige wissenschaftliche Umfragen. Sie sind aber so teuer, dass sie kaum noch finanzierbar sind.

Die neuen digitalen Technologien haben in nur wenigen Jahren die Welt verändert, und so auch die Welt der Sozialforschung. Sie schaffen aber auch ein ungeheures Potenzial neuer Möglichkeiten für die Entwicklung soziologischer Theorie. Ein paar Beispiele: Alex Pentland vom Massachusetts Institute of Technology (MIT) verwendet eigens entwickelte Soziometer, um dynamisch im Zeitverlauf soziale Interaktionen aufzuzeichnen. Damit kann die wechselseitige Beeinflussung von Menschen studiert werden. Dirk Helbing, ein Kollege von mir an der ETH Zürich, und Mehdi Moussaid vom Max-Planck-Institut für Bildungsforschung in Berlin untersuchen die Bewegung von Menschenmassen mit Hilfe von mathematischen Modellen und Computerexperimenten. Tom Snijders vom Soziologischen Institut in Groningen hat Werkzeuge und Software entwickelt, die weltweit zur Analyse großer, dynamischer Netzwerke eingesetzt werden.

Diese Forschungen sind das Gegenteil theorieblinder Big-Data-Analytik. Die Digitalisierung bringt gewaltige Datenmengen hervor. Für ihre Erhebung braucht man aber völlig neue Verfahren und Informatikwissen. Eine Traditionszeitung wie die erwähnte New York Times liegt heute in digitalisierter Form für einen Zeitraum von mehr als zwei Jahrhunderten vor. Mit automatisierter Inhaltsanalyse und sogenannten ""web crawlern"" kann man neue Erkenntnisse über den kulturellen Wandel gewinnen. Mit solchen Methoden lassen sich auch Regeln und Grenzen für das Funktionieren elektronischer Märkte finden - Erkenntnisse, die die Wirtschaftssoziologie bereits stark bereichert haben.

Kürzlich hat ein Sonderheft der Kölner Zeitschrift für Soziologie und Sozialpsychologie (herausgegeben von Jürgen Friedrichs und Alexandra Nonnenmacher) wieder auf die Bedeutung von sozialem Kontext für die Soziologie aufmerksam gemacht. Gemeint ist: Der Mensch hängt stark von seinem Umfeld ab. Der Einfluss räumlicher und sozialer Bedingungen auf menschliches Handeln - auf Erwerbstätigkeit, Bildungschancen, Verkehr, Gesundheit und so weiter - kann heute mit ortsbezogenen Daten in einem ""Geo-Informationssystem"", kurz GIS, kartografiert und statistisch analysiert werden. Das sind Erkenntnisse, die auch politisch sehr relevant werden können. Kontrollierte Interventionsstudien und Labor- und Feldexperimente geben Auskunft über Bedingungen und Mechanismen sozialen Handelns. Eine Minderheit von jüngeren Soziologinnen und Soziologen arbeitet auch in Deutschland in diesem Bereich. Mit zwei Sonderheften der Zeitschrift Soziale Welt haben die Herausgeber Norman Braun, Marc Keuschnigg und Tobias Wolbring die Zunft daran erinnert, dass kontrollierte Experimente für die soziologische Theorie von großem Nutzen sein können.

Von alldem ist auf Soziologiekongressen leider nur vereinzelt die Rede. Im Soziologie-Studium erfährt man kaum etwas über die neuen statistischen Techniken der Kausalanalyse, über experimentelle Designs, über das Potenzial von Geo-Informationssystemen, die Bedeutung kontrollierter Interventionsstudien, die Erhebung internetbasierter Daten oder die neuen Entwicklungen in der Entscheidungs- und Spieltheorie. Wenn die Medizin sich ähnlich neuen Methoden verschlossen hätte, hätten wir heute noch den Aderlass.
Die Herausforderungen der Gegenwart sollte man nicht nur Informatikern überlassen

Sozialphysiker und Informatiker stoßen in die Lücke. Allerdings sind sie oft allzu unbekümmert gegenüber dem Vorrat an soziologischer Erkenntnis und Theorietradition. Da darf man sich dann nicht beklagen, dass die Soziologie weniger in wichtigen beratenden Gremien und Sachverständigenräten vertreten ist als andere Disziplinen, obwohl sie doch als Fach zu den vielfältigen gesellschaftlichen Herausforderungen etwas zu sagen hätte. Wenn die Deutsche Gesellschaft für Soziologie (DGS) weiterhin jene Trends verschläft (nur wenige der Gremienmitglieder sind mit den neuen Methoden und Forschungen vertraut), dann werden sich jüngere, innovative Forscherinnen und Forscher von ihrer Standesvertretung abwenden. In jedem Fall aber wird eine Soziologie, die sich den Erkenntnissen anderer Disziplinen und methodischen Neuerungen verweigert, schnell ins Hintertreffen geraten.";https://www.sueddeutsche.de/kultur/geisteswissenschaften-die-gesellschaft-der-daten-1.3178096;sz.de;Andreas Diekmann
11.09.2016;Rettet den Zufall!;"Der Begriff ""Preis der Anarchie"" beschreibt in der Spieltheorie, wie Individuen, die innerhalb eines großen Systems nur in ihrem eigenen Interesse handeln, die Effizienz ebendieses Systems mindern. Das ist ein allgemeingültiges Phänomen, das den meisten von uns regelmäßig in irgendeiner Form begegnet.

Ein Städteplaner, der für das Verkehrsmanagement zuständig ist, kann den Verkehrsfluss der Stadt auf zwei Arten angehen. Grundsätzlich ist eine zentrale Top-down-Herangehensweise, die das gesamte System erfasst, effizienter, als jeden einzelnen Fahrer auf der Straße eigene Entscheidungen treffen zu lassen und zu erwarten, dass diese Entscheidungen in Summe zu einem annehmbaren Ergebnis führen. Die erste Herangehensweise verringert den Preis der Anarchie und zieht aus allen verfügbaren Informationen den bestmöglichen Nutzen.
Wir nähern uns einer vollständigen digitalen Kopie des materiellen Universums

Nun wird die Welt von Daten überflutet. 2015 hat die Menschheit genauso viele Informationen produziert wie in allen vorherigen Jahren der Zivilisation zusammen. Immer wenn wir eine Nachricht verschicken, einen Anruf tätigen oder eine Transaktion abschließen, hinterlassen wir digitale Spuren. Wir nähern uns mit schnellen Schritten dem ""Gedächtnis der Welten"", wie der italienische Autor Italo Calvino es einmal nannte: einer vollständigen digitalen Kopie des materiellen Universums.

Das Internet weitet sich durch das ""Internet der Dinge"" in immer neue Bereiche des materiellen Raumes aus, deshalb wird der Preis der Anarchie eine entscheidende Größe in unserer Gesellschaft werden und die Versuchung, ihn kraft der Big-Data-Analysen auszumerzen, immer stärker.

Dafür gibt es unzählige Beispiele. Nehmen Sie den vertrauten Vorgang des Buchkaufs auf Amazon. Amazon hat Berge von Informationen über seine Kunden, angefangen bei ihren Profilen, dem Verlauf ihrer Suchanfragen, bis hin zu den Sätzen, die sie sich in E-Books markieren. All das verwendet Amazon, um vorherzusagen, was sie vielleicht als Nächstes kaufen wollen. Wie bei allen Formen zentraler künstlicher Intelligenz werden bestehende Muster verwendet, um zukünftige vorherzusagen. Amazon kann durch einen Blick auf die letzten zehn Bücher, die Sie gekauft haben, mit wachsender Genauigkeit Empfehlungen aussprechen, was Sie als nächstes lesen wollen könnten.

An diesem Punkt sollten wir jedoch bedenken, was alles verloren geht, wenn wir die Anarchie eingrenzen. Sinnvollerweise sollten Sie nach den letzten zehn Büchern gerade eines lesen, das nicht in ein vorgegebenes Muster passt, sondern eher eines, das Sie überrascht oder zu einer neuen Sichtweise auf die Welt anregt. Anders als das erwähnte Verkehrsfluss-Beispiel dürften optimierte Empfehlungen, die sich oft zu selbsterfüllenden Prophezeiungen Ihres nächsten Kaufs entwickeln, nicht das beste Modell bei der Online-Büchersuche sein. Big Data können unsere Optionen vervielfachen und gleichzeitig Dinge herausfiltern, die wir nicht sehen wollen, aber es hat schon etwas für sich, gerade dieses elfte Buch durch einen glücklichen Zufall zu entdecken.

Was für den Bücherkauf gilt, gilt auch für viele andere Systeme, die gerade in den digitalen Raum verlagert werden, wie etwa unsere Städte und Gesellschaften. Zentrale kommunale Systeme verwenden nun Algorithmen, um die städtische Infrastruktur zu überwachen, angefangen bei Ampeln und U-Bahn-Nutzungsverhalten, bis hin zur Abfallentsorgung und Energieversorgung. Weltweit sind viele Bürgermeister von der Idee fasziniert, einen zentralen Kontrollraum zu haben, von dem aus Städtemanager in Realzeit auf neue Informationen reagieren können. Mit zentralisierten Algorithmen, die langsam jede Facette der Gesellschaft organisieren, droht jedoch die datengesteuerte Technokratie Innovation und Demokratie zu erdrücken. Das muss um jeden Preis verhindert werden. Dezentrale Entscheidungsfindung ist essenziell für eine vielfältige Gesellschaft. Umgekehrt basiert datengesteuerte Optimierung auf festgelegten Mustern, die in ihrer momentanen Form genau die Ideen ausschließt, die überraschend sind und Normen durchbrechen. Aber nur solche Ideen können die Menschheit voranbringen.
Menschliche und künstliche Intelligenz sollten zusammenwirken

Ein gewisser Grad von Zufall im Leben ermöglicht neue Denkmuster, die andernfalls fehlen würden. Auf Makroebene ist der Zufall auch für das Leben an sich notwendig. Hätte die Natur vorhersehbare Algorithmen verwendet, die zufällige Mutation bei der Vervielfachung von DNA verhindern, wäre der Planet vermutlich immer noch in einem Stadium von höchst optimierten Einzellern.

Dezentrale Entscheidungsfindung kann durch natürliche und künstliche Koevolution Synergien zwischen menschlicher und maschineller Intelligenz erzeugen. Dezentrale Intelligenz mag manchmal kurzfristig die Effizienz mindern, aber auf lange Sicht wird sie zu einer kreativeren, vielfältigeren und belastbareren Gesellschaft führen. Der Preis der Anarchie ist ein Preis, den es sich zu zahlen lohnt, wenn wir zufällige Innovation erhalten wollen.";https://www.sueddeutsche.de/digital/digitalisierung-rettet-den-zufall-1.3151849;sz.de;Carlo Ratti und Dirk Helbing
19.08.2016;"Die neue ""Tatort""-Saison beginnt";"Für ""Tatort""-Fans hat das Warten ein Ende. Am Sonntag geht es nach der Sommerpause zur gewohnten Zeit nach der Tagesschau mit der ersten neuen Folge der ARD-Krimireihe weiter. Die Zeit der Wiederholungen ist vorbei.

Als erstes nehmen die Kommissare aus Köln Max Ballauf (Klaus J. Behrendt) und Freddy Schenk (Dietmar Bär) die Ermittlungen wieder auf. Und das gleich in einem Fall, in dem bei einem Doppelmord Mutter und Sohn umgebracht wurden. Neue Fälle gibt es dann wieder Sonntag für Sonntag, neue Ermittlerteams in diesem Jahr aber erst einmal nicht. Einige Veränderungen stehen aber an.

Der zweite neue ""Tatort"" nach der Sommerpause kommt am 28. August aus Stuttgart. Es ist der 19. Fall des Ermittler-Duos Thorsten Lannert (Richy Müller) und Sebastian Bootz (Felix Klare). Diesmal geht es ums Thema Big Data und totale Überwachung. Und um den Tod einer jungen Schauspielschülerin, die nebenbei nicht nur bei einem Escort-Service, sondern auch bei einem Softwarehersteller gearbeitet hat. Und dessen Programmierer haben eine Software entwickelt, die aus den über eine Person gesammelten Daten Prognosen über das künftige Verhalten trifft. Klingt nach Science Fiction und George Orwell? Stimmt, soll es auch.

Auch der ""Tatort"" aus Münster setzt regelmäßig auf Drehbücher, die sich nicht ans 0815-Krimi-Schema halten. Gerade das wissen Millionen von Fans zu schätzen. Axel Prahl als Kommissar Frank Thiel und Jan Josef Liefers als der exzentrische Rechtsmediziner Professor Karl-Friedrich Boerne gehen am 25. September zum ersten Mal nach der Sommerpause wieder auf Verbrecherjagd. Bereits in der Woche davor ist der erste neue ""Tatort"" aus der Schweiz zu sehen.

Ein besonderes Jubiläum steht im Spätherbst an: Dann kommt der 1000. ""Tatort"" ins Fernsehen. Maria Furtwängler als Kommissarin Charlotte Lindholm aus Hannover und Axel Milberg als ihr Kieler Kollege Klaus Borowski ermitteln darin gemeinsam. Der Titel lautet ""Taxi nach Leipzig"". Das ist natürlich kein Zufall, wie Fans sofort wissen: So hieß auch der legendäre erste ""Tatort"" 1970 mit dem NDR-Kommissar Paul Trimmel (Walter Richter).

In dem neuen Fall steigen Lindholm und Borowski nach einer Polizeitagung zusammen in ein Taxi. Dessen Fahrer ist auf dem Weg nach Leipzig. Dort will der frühere Elitesoldat die Heirat seiner Ex-Freundin verhindern. Im Auto kommt es zum Streit - und der lässt sich nicht mehr einfach beilegen, im Gegenteil. Zu sehen ist der Jubiläums-""Tatort"" nach jetziger Planung erst im November, nicht wie erwartet schon im Monat davor. Ein genauer Termin steht noch nicht fest. Ohnehin kann sich nach ARD-Angaben an der Planung bis Weihnachten jederzeit noch einiges ändern.

Sicher ist: Mit dem Bodensee-""Tatort"" ist bald Schluss, eine Folge wird es in diesem Jahr aber noch geben, in der Eva Mattes als Kommissarin Klara Blum nach 14 Jahren in dieser Rolle ihre Abschiedsvorstellung gibt. Das Aus für den ""Tatort"" am Bodensee heißt aber nicht, dass damit künftig ein Ermittlerteam wegfällt. Der SWR lässt anschließend eine Schwarzwald-Crew aus Freiburg ermitteln, unter anderem mit Eva Löbau (""Der Wald vor lauter Bäumen"", ""Lerchenberg"") und Entertainer und Ex-Late-Night-Talker Harald Schmidt.

Das neue Ermittlerteam aus Dresden wird nach der Premiere im März voraussichtlich noch im Oktober seinen zweiten Fall aufklären. Mit Karin Hanczewski und Alwara Höfels jagen in der sächsischen Landeshauptstadt zwei Frauen die Verbrecher. Nicht mehr dabei ist Jella Haase (""Fack ju Göhte""). Sie hat die Polizeianwärterin Maria Magdalena Mohr gespielt, die im ersten Fall getötet wurde.";https://www.sueddeutsche.de/wirtschaft/medien-die-neue-tatort-saison-beginnt-dpa.urn-newsml-dpa-com-20090101-160818-99-135330;sz.de;DPA
12.06.2016;Neue Masterstudiengänge: Kinderrechte bis Sportmarkt;"Neuer Master Data Science in Mannheim

Im Februar 2017 startet der neue Studiengang Data Science an der Universität Mannheim. Der Master soll zwei Jahre dauern, teilte die Hochschule mit. Studierende beschäftigen sich mit Themen wie Datenanalyse, Text- und Datenauswertung und Statistik. Sie sollen spezialisiert werden für die Arbeit mit großen Datenmengen und später etwa Big-Data-Projekte in Unternehmen, Verwaltungen oder auch Forschungseinrichtungen umsetzen. Alle Lehrveranstaltungen werden auf Englisch gehalten. Bewerbungsschluss ist der 15. November 2016.

Neuer Master Kindheitswissenschaften und Kinderrechte in Stendal

Die Hochschule Magdeburg-Stendal startet den neuen Master Kindheitswissenschaften und Kinderrechte. Der Studiengang am Standort Stendal dauert vier Semester und beginnt erstmals zum Wintersemester 2016/2017, teilt die Hochschule mit. Er richtet sich etwa an Erziehungswissenschaftler und Sozialpädagogen. Auf dem Vorlesungsplan stehen Fächer wie Kinderrechte national und international, sozialpädagogische Methoden oder Projekt- und Change-Management. Absolventen sollen etwa bei Kinderrechtsorganisationen oder in der Aus- und Weiterbildung an Fachschulen arbeiten können. Bewerbungsschluss ist am 15. September.

Neuer Master Sportbusiness in Düsseldorf

Die private IST-Hochschule für Management in Düsseldorf startet den neuen Master Sportbusiness. Der Studiengang dauert in Vollzeit vier Semester und beginnt erstmals zum kommenden Oktober, teilt die Hochschule mit. Auf dem Stundenplan stehen zu 80 Prozent sportspezifische Inhalte. Dazu gehören etwa Sportvermarktung, internationaler Sportmarkt oder professionelles Marketing von Sportstätten. In den restlichen 20 Prozent dreht sich alles um allgemeine betriebswirtschaftliche Themen. Absolventen sollen als Manager im Sportbereich arbeiten können. Das Studium kostet in Vollzeit 10 680 Euro. Bewerbungsschluss ist am 15. September.

Neuer berufsbegleitender Master Embedded Systems für Informatiker

Die Wilhelm Büchner Hochschule hat den berufsbegleitenden Studiengang Embedded Systems neu im Angebot. Der vier Semester lange Master richtet sich an Informatiker, teilt die Hochschule mit. Studenten beschäftigen sich mit Computersystemen, die in Maschinen eingebettet sind und Aufgaben in Echtzeit übernehmen können. Solche Embedded Systems sind etwa in der Industrie 4.0 nötig. Absolventen sollen in den unterschiedlichsten Branchen von der Gebäudetechnik bis zur Automobilindustrie arbeiten können. Der Studiengang kostet pro Monat 528 Euro. Bewerbungen sind ab sofort jederzeit möglich. ";https://www.sueddeutsche.de/karriere/arbeit-neue-masterstudiengaenge-kinderrechte-bis-sportmarkt-dpa.urn-newsml-dpa-com-20090101-160704-99-552305;sz.de;DPA
24.06.2016;Neue Masterstudiengänge: Drei Unis mit neuen Fachbereichen;"Neuer Master im Fachbereich Public Health in Berlin

Zum Wintersemester 2016/17 startet ein neuer Master-Studiengang im Bereich Gesundheitswissenschaften. Der Master of Science in Public Health und wird von der Berlin School of Public Health angeboten. Studierende beschäftigen sich damit, was die Gesundheit der gesamten Bevölkerung beeinflusst und wie sie gesteuert werden kann. In diesem Feld spielen Wechselwirkungen zwischen Mensch und Umwelt oder auch Leistungen des Gesundheitssystems eine Rolle.

Bewerber müssen einen Bachelor-Abschluss und Kenntnisse in zwei Fachgebieten nachweisen, die aber nicht streng vorgegeben sind. Sie können etwa im Bereich Statistik und Politikwissenschaften liegen, oder auch in Soziologie und Humanbiologie.

Die Berlin School of Public Health wird von der Berliner Charité - Universitätsmedizin, der Technischen Universität sowie der Alice Salomon Hochschule getragen. Der Studiengang ist auf vier Semester ausgelegt. Absolventen sollen später etwa bei Gesundheits- und Sozialversicherungen und in Bundesbehörden arbeiten können. Bewerbungsschluss ist der 15. Juli 2016.

Neuer Master Data Science in Darmstadt

Die Hochschule Darmstadt hat neu den Master Data Science im Angebot. Der Studiengang beginnt zum ersten Mal zum Wintersemester 2016/2017 und dauert vier Semester, teilt die Hochschule mit. Studenten befassen sich zum Beispiel mit multivariater Statistik, nichtlinearen und nichtparametrischen Modellen, aber sie hören auch Seminare zum Thema Datenschutz. Absolventen sollen als Analysten oder Systemarchitekten etwa bei Banken, Versicherungen, Handelsunternehmen oder im Gesundheitswesen arbeiten können. Bewerbungsschluss ist der 15. Juli.

Neuer Master Wirtschaftsgeschichte in Bayreuth

Die Universität Bayreuth startet ab dem Wintersemester 2016/2017 den Master Wirtschaftsgeschichte. Der vier Semester lange Studiengang ist komplett auf Englisch, teilt die Hochschule mit. Studenten haben Vorlesungen zu Themen wie Weltwirtschaft, wirtschaftliches Wachstum sowie der Staat und seine Institutionen. Außerdem beschäftigen sie sich etwa mit quantitativen Methoden der Wirtschaftsgeschichte. Bewerber brauchen einen ersten Abschluss in Volkswirtschaftslehre oder einem vergleichbaren Fach. Absolventen sollen in der Wirtschaft oder in der Wissenschaft arbeiten können. Bewerbungsschluss ist der 15. Juli 2016.";https://www.sueddeutsche.de/karriere/arbeit-neue-masterstudiengaenge-drei-unis-mit-neuen-fachbereichen-dpa.urn-newsml-dpa-com-20090101-160607-99-220174;sz.de;DPA
20.06.2016;Besser als das Netz;"Jede Sekunde teilen Menschen mit anderen Erlebnisse auf den sozialen Netzwerken. Oft geht es ums Essen, das Wetter oder Katzenvideos. Aber eben auch um Terroranschläge, Kriege, Naturkatastrophen. Und das sind die Posts, die Michael Weiler interessieren. Der 34-Jährige hat für seine Doktorarbeit am Lehrstuhl für Datenbanksysteme an der Ludwig-Maximilians-Universität (LMU) ein Programm entwickelt, das Posts auf Twitter analysiert und die wichtigsten Ereignisse filtert. Weil Menschen eben oft etwas posten, bevor es in den Nachrichten kommt, hofft Weiler, immer ein bisschen schneller sein zu sein als Journalisten.

Wahrscheinlich schafft er nur ein paar Minuten, aber die könnten entscheidend sein, glaubt der Doktorand - zum Beispiel, wenn es um den Verkauf von Aktien gehe. Außerdem kann das Programm Nachrichten nach der persönlichen Relevanz sortieren. ""Manager könnten, das was für sie wichtig ist, direkt auf ihre Smart Watch geschickt bekommen und gleich darauf reagieren"", erklärt Weiler.

Er arbeitet schon seit fast drei Jahren an dem Projekt. Wann er fertig sein wird, weiß er noch nicht. Von nun an aber könnte seine Arbeit etwas angenehmer ablaufen: Die LMU hat in dem Institut am Englischen Garten für Wissenschaftler wie ihn jetzt einen extra Raum geschaffen, das Data Science Lab. Dort sollen Studenten Daten, die sie von Unternehmen bekommen, analysieren und aufbereiten. Der Raum ist für alle offen, soll aber hauptsächlich von Studierenden des neuen Elitemasterstudiengangs ""Data Science"" verwendet werden, der im Herbst losgeht. Ziel ist es, dass Universität und Industrie enger zusammenarbeiten. Davon würde beide Seiten profitierenn, sagt Tobias Emrich, der das Lab leitet: ""Die Studenten sind froh, wenn sie praktische Erfahrung sammeln dürfen und die Firmen, wenn sie zukünftige Arbeitnehmer kennenlernen."" Emrich koordiniert die Zusammenarbeit mit den Unternehmen. Er bespricht mit ihnen Projekte, dann sucht er die passenden Studenten.

In dem Lab stehen jede Menge Computer. Für die Startup-Atmosphäre gibt es aber auch graue Sofas und weiße Stehtische. An der Wand hängt ein Landschaftsbild von dem Maler Paul Cézanne, daneben ein Foto vom Englischen Garten und daneben ein Bild wie Cézanne den Englischen Garten gemalt hätte. Wie das aussieht, wurde von Programmen berechnet, natürlich.

Dem Zufall wird immer weniger überlassen - zumindest, wenn es nach den Studenten vom Data Science Lab geht. Alexander Neitz, Qian Cai und David Rasch zum Beispiel haben ein Programm entwickelt, das einem sagen soll, wie gut ein Selfie ankommt - bevor man es in die Welt schickt. Anhand von Abläufen im Foto-Netzwerk Instagram bewertet das Programm, wie beliebt oder unbeliebt ein Bild sein wird.

""Mit der Zeit lernte es, welche Merkmale ein Selfie besonders gut machen"", sagt Neitz, der gerade im Master Informatik studiert. Auf einer Skala, die bis zu 100 Prozent geht, wird dem Benutzer angezeigt, ob das Bild der kritischen Social-Media-Masse standhalten kann. Wenn das Foto nicht so gut abschneidet, hübscht es das Programm etwas auf - zum Beispiel mit Farbfiltern oder durch Beschneiden des Fotos. Denn das liegt offenbar im Trend: ""Besonders gut kommen Bilder von asiatischen Mädchen an, deren Kopf etwas angeschnitten ist"", meint Neitz. Er und seine Kommilitonen arbeiteten im Rahmen einer Seminararbeit an dem Projekt. Sie bekamen eine gute Note, für sie ist das Ganze jetzt abgeschlossen.

Das ist bei Markus Mauder anders. Er analysiert archäologische Daten und steckt noch mitten in der Arbeit. Sein Ziel ist es, eine Karte zu erstellen, mit der die Herkunft von Steinzeitmenschen nachvollzogen werden kann. ""Wir wollen Aussagen darüber treffen, wie weit die Menschen damals gereist sind"", sagt Mauder, der auch gerade seine Doktorarbeit am Lehrstuhl für Datenbankensysteme schreibt. Die Überreste von etwa 200 Steinzeitmenschen, die im Alpenraum gefunden wurden, werden zunächst in Laboren chemisch untersucht. Dabei entstehen riesige Datenmengen, die etwas über das Leben der Menschen verraten sollen. Die Daten aufzubereiten, ist Mauders Job: ""Was aber dabei herauskommt, ist noch völlig offen.""";https://www.sueddeutsche.de/muenchen/ludwig-maximilians-universitaet-besser-als-das-netz-1.3042884;sz.de;Christina Hertel
29.05.2016;Am Rande des Chaos;"Roboter, die mit Menschen arbeiten oder sie ersetzen. Software, die so intelligent ist, dass sie besser Schach und Go spielt als die Weltmeister. Computer, die Bilder erkennen und Krankheiten diagnostizieren können. Maschinen, die selbständig Entscheidungen treffen und umsetzen - künstliche Intelligenz (KI) verändert bereits heute unser Leben. Dem Siegeszug der künstlichen Intelligenz widmete das Feuilleton in den vergangenen Wochen eine Serie, die in Interviews, Gastbeiträgen und Reportagen untersuchte, auf welchem Stand sich Wissenschaft und Technik befinden, welche gesellschaftlichen und ethischen Folgen das Maschinenlernen jetzt schon hat und wie eine Zukunft mit KI aussehen könnte. Dieses Interview mit dem Wissenschaftsautor und inoffiziellen Historiker des Silicon Valley, John Markoff, ist der Abschluss der Serie.

SZ: Welche Rolle spielt künstliche Intelligenz im Silicon Valley?

John Markoff: Eine sehr große. Vor einem guten halben Jahr war das, als hätte jemand plötzlich den Schalter umgelegt und die große Feuerspritze des Risikokapitals von den sozialen Netzwerken auf die künstliche Intelligenz gelenkt. Ganz nach dem Eichhörnchenprinzip.

Nach welchem Prinzip?

Risikokapitalgeber funktionieren in der Regel wie Hunde, die ein Eichhörnchen erspähen und dann blind in die Richtung losrennen. Deswegen machen jetzt überall im Valley Start-ups auf. Und Labore für Robotik. Mal ganz abgesehen von den großen sechs, also Facebook, Google, Apple, Microsoft, Amazon und IBM, die sehr viel in Maschinenlernen investieren.

Warum gerade jetzt?

Die Technologie fürs Maschinenlernen gibt es schon lange. Frank Rosenblatt hat mit seinem Perzeptron schon 1958 das erste lernfähige künstliche Netz vorgestellt. Nur gab es damals noch kein Big Data. Mit diesen neuen riesigen Datenmengen bekommt man diese wirklich erstaunlichen Ergebnisse der letzten Jahre.

Warum werden solche Fortschritte immer im Silicon Valley gemacht? Ist das ein Selbstläufer, weil sich die Industrie nun mal dort niedergelassen hat?

Das wäre die mechanistische Erklärung, dass man eben einen Hebel umlegt und dann läuft das. Ich bin allerdings der Meinung, dass Technologie genauso ein kulturelles, wirtschaftliches und soziales Phänomen ist. Und deswegen war es eben kein Zufall, dass sich Computer ausgerechnet dort zum Medium entwickelt haben.

Sie sind dort aufgewachsen, oder?

Ja, ich bin in Palo Alto aufgewachsen, also wirklich im Herzen des Silicon Valley. Bevor es das Silicon Valley war. Ich habe als Kind mit den Hewlett-Kindern gespielt, weil Billy Junior in meine Klasse ging. Und die beiden Häuser, wo heute der Google-Chef Larry Page wohnt und wo Steve Jobs gelebt hat, lagen während meiner Schulzeit auf meiner Route als Zeitungsausträger. Ich habe also wirklich gesehen, wie sich der Ort verändert hat.

Wann ist denn aus der Gegend das Silicon Valley geworden?

Das war die Zeit von ungefähr 1965 bis 1975. Ich war damals im Nordwesten auf dem College, aber als ich 1976 zurückkam, gab es plötzlich diese neue Industrie, die dann so etwas wie die Vorgeschichte des Personal Computing schrieb.

Spielte künstliche Intelligenz damals schon eine Rolle?

Ja, das passierte alles in der Gegend rund um die Stanford University. Da gab es zum einen John McCarthys Labor für künstliche Intelligenz. McCarthy kam vom Massachusetts Institute of Technology, wo er mit Marvin Minsky die Grundzüge der KI entwickelt hatte. Und es gab Douglas Engelbarts Augmentation Research Center am Stanford Research Institute. Die waren am jeweils entgegengesetzten Ende des Stanford Campus vielleicht eineinhalb Meilen voneinander entfernt. Später gab es dann noch Xerox Parc. Die Geschichte des Personal Computing und des Internets geschah also in sehr kurzer Zeit auf engem Raum.

Aber warum ausgerechnet dort?

Kreativität entsteht immer am Rande des Chaos. Und das beschreibt ziemlich genau, wie das Silicon Valley entstand. Das war eine klar umrissene, aber weitgehend isolierte Gegend. Gleichzeitig prallten dort Vietnamkrieg, Gegenkultur und Mikroprozessor aufeinander. Das war sogar eine sehr starke Gegenkultur, und ihre Ideen haben sich in der Zeit über das ganze Land verbreitet. Steve Jobs war ein gutes Beispiel für diese Ausbreitung.
Illu

Warum gerade Steve Jobs?

Er hat mir immer wieder gesagt, dass ihn Dinge wie Stewart Brands ""Whole Earth Catalogue"", die Platten von Joan Baez und Bob Dylan entscheidend beeinflusst haben. Einmal habe ich ihn sogar dazu gebracht zuzugeben, dass LSD eine seiner entscheidenden Erfahrungen war, die seine Weltsicht geprägt haben.

Wie hat LSD seine Weltsicht geprägt?

Haben Sie mal LSD genommen? Dann haben Sie sicher auch so was gesehen, wie Bäume zu atmen anfangen. Das kann die Weltsicht fundamental verändern. Und man darf nicht vergessen, dass das nicht illegal war. Das kam in den Sechzigern über die Experimente von Myron Stolaroffs International Foundation for Advanced Study ins Silicon Valley. Mehr als 300 Teilnehmer machten damals solche Erfahrungen. Nicht aus Spaß, sondern im Rahmen wissenschaftlicher Untersuchungen. Sie glaubten, dass LSD die Kreativität sowohl in den Ingenieurswissenschaften als auch in der Kunst verstärken kann. Das waren alles Typen von Hewlett Packard und Varian, die die ersten Elektronikfirmen rund um die Stanford University waren.

Und wie genau verändert das dann das Denken von Technikern?

So eine Erfahrung zwingt einen, aus sich herauszutreten. Leute scheren selten von allein aus ihren Mustern aus. Aber genau das taten ja all die Techniker, die das Silicon Valley begründeten.

Wie hat das dann die Entwicklung der KI verändert?

Ich würde jetzt nicht behaupten, dass das eine ganz direkte Auswirkung hatte. Aber in den beiden Laboratorien, die ich erwähnt habe, entstanden damals zwei fast gegensätzliche Ansätze. Auf der einen Seite gab es John McCarthy, der ein System konstruieren wollte, das den Menschen irgendwann einmal ersetzt. Auf der anderen Seite stand Douglas Engelbart, der KI als Erweiterung des Menschen verstand. In einem seiner ersten Papiere von 1962 nannte er das sogar so - Intelligence Augmentation. IA. Daraus wurde dann das Feld, das man in den Computerwissenschaften HCI nennt - Human Computer Interaction, also die Interaktion zwischen Mensch und Computer. Da ist nicht die Maschine, sondern der Mensch der Ausgangspunkt für die Entwicklung von Technologie.

Es scheint aber, als ob McCarthys Idee von der KI, die einmal den Menschen ersetzt, die dominante Sichtweise ist.

Das stimmt einerseits. Schon in den Sechzigerjahren nahmen Leute wie Marvin Minsky Douglas Engelbart nicht richtig ernst. Das war für die keine wissenschaftliche Forschung, sondern lediglich eigenartiges Rechnen. Ironischerweise entstand dann aber aus Engelbarts Forschung sowohl das Personal Computing, als auch das moderne Computernetzwerk, also erst das Arpanet und in Folge das Internet.

Befinden wir uns gerade in der Hochphase der KI?

Ich würde sagen, wir sind in die Aufbruchsstimmung der frühen Jahre zurückgekehrt. Mit dem Zusammenspiel aus Rechnerleistung, Robotik und Big Data ist es jetzt vor allem eine Frage, wie man das skaliert. Die andere Frage ist allerdings, wann diese Entwicklung auf ihrem Höhepunkt stagniert.

Wieso sollte sie stagnieren?

Weil sich die Rechnerleistung kaum mehr steigern lässt. Gerade was die Kosten betrifft.

Und was ist mit dem Moore'schen Gesetz, das besagt, dass sich die Anzahl der Schaltkreise auf einem Prozessor alle anderthalb bis zwei Jahre verdoppelt, während gleichzeitig die Preise um die Hälfte fallen?

Das war bisher der Fahrplan der Tech-Industrie. Aber der gilt nicht mehr. Man kann gerade beobachten, wie dem technischen Fortschritt die Puste ausgeht. Wir haben neue Stufen der Komplexität und der Verkleinerung erreicht, die jeden weiteren Schritt zu einer enormen Herausforderung machen. Außerdem ist das Moore'sche Gesetz vor allem ein Gesetz der Dichte. Daran schließt aber das Potenzgesetz des IBM-Wissenschaftlers Robert Dennard an, das besagt, dass die Leistung eines Computers per Watt genauso steigt, wie die Verdichtung der Schaltkreise im Moore'schen Gesetz. Das bedeutete eine exponentielle Steigerung in der Geschwindigkeit von Mikroprozessoren. Aber das war 2006 vorbei.

Vor zehn Jahren?

Ja. Die Taktfrequenz von Prozessoren hat sich seit zehn Jahren nicht verändert. Die Kosten sind zwar weiter gesunken. Aber auch das stagniert. Bei Intel gibt es eine heftige interne Debatte - und für Intel hat Gordon Moore ja damals sein Gesetz aufgestellt. Die Firma behauptet zwar, dass die Kosten für ihre Transistoren immer noch fallen. Aber sie veröffentlicht ihre Zahlen nicht. Deswegen ist das schwer nachzuprüfen.

    John Markoff Der Fortschrittsglaube im Silicon Valley scheint aber ungebrochen zu sein.

Das ist reine Ideologie. Für das Silicon Valley hat dieser Glaube, dass man alle zwei Jahre eine neue technologische Stufe erreicht, hervorragend funktioniert. Allerdings sind die Gesetze der Physik eine Realität, an der man nicht vorbeikommt. Und da ist man nun angelangt.

So drastisch formuliert das aber niemand.

Nun, der Computerforscher Ray Kurzweil sagt, dann wenden wir uns einfach einer neuen Technologie zu. Mag sein. Aber - dafür gibt es keinerlei Hinweise. Kurzweil blickt immer zurück und sagt, so war das schon immer. Der Wirtschaftswissenschaftler Robert Gordon sagt dagegen, dass die Produktivitätssteigerungen, die das vernetzte Computing gebracht haben, eine einmalige Sache waren.

Aber viele gehen davon aus, dass sich die Produktivität der künstlichen Intelligenz so steigern lässt, dass man den menschlichen Geist irgendwann auslagern kann.

Auch das ist reiner Glaube. Schauen Sie sich die Produktivitätsdaten mal an. Die sind episodisch, schwer nachzuvollziehen und scheinbar unabhängig vom Investment. Das bleibt ein Rätsel. Ich verstehe den Glauben, aber ich sehe die Zahlen dazu nicht. Und wenn es über die letzten drei Jahrzehnte hinweg diese unglaublichen Produktivitätssteigerungen gab, warum arbeiten in Amerika dann immer noch 144 Millionen Menschen? Der große Arbeitslosigkeitseffekt ist jedenfalls nicht eingetreten.

Fotografen oder Leute aus der Musikindustrie wären da sicher anderer Meinung.

Es gibt aber einen Unterschied zwischen der Veränderung und der Zerstörung von Arbeit. Fotografie ist ein gutes Beispiel. Sicher gibt es so gut wie keine chemische Fotografie mehr. Aber Kodak wurde nicht von Instagram gekillt. Kodak wurde von seinem Konkurrenten Fuji geschafft. Instagram wiederum hätte als Firma niemals ohne ein ausgereiftes Internet entstehen können. Und die Ausreifung des Internets hat zwischen zwei und vier Millionen qualifizierte Ingenieurjobs geschaffen.

Sind denn keine der sogenannten White Collar Jobs gefährdet, so wie die Automatisierung der Produktion einst den Blue Collar Jobs den Garaus machte?

Sicher gibt es Beispiele. Nehmen Sie eine Anwaltskanzlei. Da gab es bisher immer Anwaltsgehilfen, die für 35 Dollar die Stunde arbeiten, und Anwälte, die 400 Dollar die Stunde bekommen. Die Arbeit der Anwaltsgehilfen bestand vor allem daraus, juristische Dokumente zu sichten. Das kann die Software E-Discovery viel besser als Menschen. Zwei Ökonomen am MIT haben aber gerade eine Studie veröffentlicht, die zeigt, dass die Arbeit eines amerikanischen Anwalts in elf verschiedene Aufgaben unterteilt ist. Man muss zum Beispiel Klienten beraten, vor Gericht argumentieren. Kein Computer wird vor Gericht ein Plädoyer halten können. Nein, wenn man die negativen Auswirkungen des Internets auf den Arbeitsmarkt betrachtet, sind das einstellige Prozentzahlen.

Dann leben wir also in Engelbarts Welt der IA?

Genau. David Otter, ein anderer MIT-Ökonom, hat gerade einen wunderbaren Artikel mit dem Titel ""Warum es immer noch so viele Jobs gibt"" veröffentlicht. Der besagt, dass man Wissen und Verständnis immer nur formulieren, aber nicht programmieren kann. Selbst konventionelle mechanische Jobs kann man nur bedingt mit Maschinen besetzen. Weil sie ein gewisses Maß an Flexibilität voraussetzen. Maschinen sind aber nicht flexibel.

Wird das den Maschinen nicht gerade anerzogen? Selbstfahrenden Autos etwa?

Da gibt es ja auch schon Prognosen, die sagen, dass allein in Amerika drei Millionen Jobs verloren gehen. Es sieht aber nicht so aus, als ob man den Menschen ganz aus dem Auto entfernen kann. Wenn man aber jemanden dafür bezahlen muss, dass er das selbstfahrende Auto beaufsichtigt, ist das wieder nur eine Erweiterung, kein Ersatz für menschliche Fertigkeiten.

Warum klingen kluge Menschen wie Stephen Hawking und Elon Musk so apokalyptisch, wenn sie über KI sprechen?

Das sind keine Experten. Kein KI-Spezialist redet so. Die sind viel konservativer und skeptischer. Außerdem gibt es kulturelle und gesellschaftliche Faktoren. Wir haben zum Beispiel ein historisches Tief, was den Anteil der arbeitenden an der gesamten Bevölkerung in Amerika betrifft. Hat das etwas mit neuen Technologien zu tun? Sicherlich auch. Aber ich gehöre beispielsweise zur Generation der Babyboomer, und ein signifikanter Teil dieser Generation zieht sich schon vor dem Rentenalter aus dem Arbeitsleben zurück.

Aber man kann doch nicht abstreiten, dass Technologie die Gesellschaft verändert.

Natürlich nicht. Die Wirtschaft wird sich auch weiter entwickeln. Ich habe nur ein Problem mit dem Glauben an die unausweichliche Krise. Die Industrialisierung brachte auch einen gewaltigen Strukturwandel. Alle zitieren dieses Beispiel, dass vor der Industrialisierung achtzig Prozent der Bevölkerung in der Landwirtschaft gearbeitet haben und heute nur noch zwei. Aber das war ein Wandel, der sich über Jahrzehnte erstreckt hat. So eine ähnliche Entwicklung durchlaufen wir derzeit auch. Und wir werden überrascht sein, welche Formen der Arbeit da noch entstehen. Ich sehe zum Beispiel große Zuwachsraten im Entertainment- und Service-Bereich.

Das sind nicht gerade Felder, auf denen man viel verdienen kann. Auf jeden Rockstar kommen Tausende, die in einer Hotelbar Klavier spielen müssen.

Sie reden hier allerdings von der Ungleichheit. Aber was treibt denn die wachsende Ungleichheit in Amerika voran? Die Technologie? Oder die Steuergesetzgebung? Ich bin kein Wirtschaftswissenschaftler, aber zeigen Sie mir doch, welchen Anteil Technologie an der Einkommensschere hat und welchen die Steuergesetze.

Kommen Fragen nach Ethik und Moral im Silicon Valley überhaupt auf?

Das ist sogar eine heftige Debatte. Es gibt ja dieses Vorurteil, dass KI-Wissenschaftler unmoralisch und verantwortungslos sind. Ich bin aber schon lange genug dabei, um wirklich optimistisch zu sein, dass wir es hier mit einer Generation von Wissenschaftlern zu tun haben, die sich wirklich Gedanken über die Konsequenzen ihrer Forschung machen. Und es gibt ethisch getriebene Investitionen. Elon Musk hat Institute für nachhaltige KI-Forschung finanziert. Toyota kam ins Silicon Valley und hat eine Milliarde Dollar in ein Labor investiert, das keine selbstfahrenden, sondern sicherere Autos baut. Das sind für mich alles Beispiele für einen moralischen Ansatz.

Das klingt jetzt aber nach den ""Wir machen die Welt zu einem besseren Ort""-Floskeln, die von der Comedy-Serie ""Silicon Valley"" persifliert werden.

Da gibt es sicherlich oft einen Unterschied zwischen gutem Willen und Ergebnis. Silicon Valley hat eine unbarmherzige Monopolwirtschaft geschaffen. Das beste Beispiel sind Firmen wie Amazon, die klassische Industrien verdrängen und gute Arbeitsplätze mit unterbezahlten Fahrerjobs ersetzen. Für die sind solche Floskeln nur ideologische Fassade. Aber unter den Ingenieuren findet man viele, die durchaus sozial denken. Ich habe das über die Jahrzehnte lange genug beobachtet - im Silicon Valley herrschte nie eine monolithische Kultur. Es gab immer beide Seiten.

Glauben Sie, dass KI jemals Maschinen mit Bewusstsein hervorbringen kann?

Das kann ich mir nur schwer vorstellen, weil wir gar nicht verstehen, was Bewusstsein überhaupt ist. Sicher werden die Neurowissenschaften immer besser. Meine Science-Fiction-Hoffnung ist ja, dass wir herausfinden, dass das Hirn ein Quantencomputer ist, und wir dann eines Tages einen funktionierenden Quantencomputer bauen. Aber bevor wir das menschliche Bewusstsein nicht wirklich begreifen, ist es lächerlich, solche Debatten zu führen.

Wäre dann nicht der Punkt erreicht, an dem die Menschen die Kontrolle über die Maschinen verlieren?

Maschinen entwickeln sich aber nicht von selbst, sondern sind das Ergebnis menschlichen Designs. Als Spezies Mensch neigen wir dazu, alles zu vermenschlichen, mit dem wir interagieren. Deswegen ist das natürlich eine relevante Debatte, wenn wir uns fragen, ob wir diesen Maschinen vertrauen können, wenn wir nicht mehr wissen, nach welchen Wertesystemen sie funktionieren. Sowohl in Amerika wie auch in Europa gibt es politische Forderungen nach mehr Transparenz der Maschinen. Das ist nur berechtigt. Wenn uns diese Maschinen immer besser kennen, sollten wir diese Maschinen ebenfalls besser kennen.";https://www.sueddeutsche.de/kultur/kuenstliche-intelligenz-am-rande-des-chaos-1.3010388;sz.de;Andrian Kreye
02.03.2016;Der Geist der Maschine;"Roboter, die mit Menschen arbeiten oder sie ersetzen. Software, die so intelligent ist, dass sie besser Schach und Go spielt als die größten Champions. Computer, die Krankheiten diagnostizieren können. Und Maschinen, die Entscheidungen treffen und umsetzen - künstliche Intelligenz verändert unser Leben. Und doch steht die Technik erst am Anfang. Den Fragen, die mit dem Siegeszug der künstlichen Intelligenz (KI) entstehen, widmet das Feuilleton eine Serie: Wie wird eine Zukunft mit Maschinen aussehen, die vieles besser können als der Mensch? Wer trägt die Verantwortung für ihre Handlungen? Was mag passieren, und was bleibt, aller Utopie zum Trotz, Science-Fiction-Fantasie? In diesem Teil erklärt der Philosoph Markus Gabriel, warum es wichtig ist, präzise und nicht ängstlich zu sein, wenn man über künstliche Intelligenz spricht.

SZ: Herr Gabriel, vernetzte Computersysteme übernehmen immer mehr Arbeiten und auch Entscheidungen. Was ändert das an der Autonomie des Menschen und an unserem Menschenbild?

Markus Gabriel: Man muss zwei Dinge auseinanderhalten, die in der Debatte oft miteinander vermischt werden. Das erste ist, dass Maschinen in der Arbeitsteilung die Produktivkräfte verändern, um das alte marxistische Vokabular zu verwenden: Neue Technologie hat bestimmte, zum Teil erhebliche Einflüsse auf unsere Arbeitsprozesse oder auf den Umgang mit Wissen. Das ist bekannt seit Buchdruck und Dampfmaschine. Die zweite Frage ist, ob es nicht neuerdings auch Erfindungen gibt, die das Menschsein selber verbessern und verändern könnten, und ob damit ein Zeitalter der Cyborgs und des Posthumanismus anbrechen könnte. Letzteres halte ich für abwegig, Ersteres für offensichtlich.

Es war aber die Hoffnung der Pioniere der künstlichen Intelligenz (der berühmteste, Marvin Minsky, ist neulich erst gestorben), irgendwann die Struktur und Arbeitsweise des Gehirns nachzubauen. Ist das aus philosophischer Sicht möglich?

Nein. Die besten Einwände wurden schon früh von Hubert Dreyfus und John Searle in Berkeley vorgetragen. Die Voraussetzung dafür, dass wir das menschliche Gehirn nachbauen - in Silizium oder als abstraktes Netzwerk -, ist, dass wir ein bestimmtes Vokabular in ein anderes übersetzen können. Nehmen Sie etwa die Aussage: ""Ich denke, da fährt ein Raser auf der Autobahn."" Der Sprecher befindet sich da in einem bestimmten Zustand, nämlich dass er der Meinung ist, er werde von einem Raser überholt. Diese Meinung aber ist nicht übersetzbar in etwas, das im Gehirn vor sich geht.

Warum denn nicht?

Der Begriff das ""Rasers"" ist ja eine Charakter-Aussage, er steht zugleich im Kontext der Verkehrsgesetze und vieles mehr - er hat also mit der Umgebung zu tun, in der wir uns bewegen. All das aber ist nicht auf einen Vorgang im Gehirn zu reduzieren. Die meisten Begriffe, die wir auf die Wirklichkeit anwenden - ""Wasser"", ""Katze"", ""Bundesrepublik"", ""Mond"" - sind ganz einfach nicht im Gehirn zu verorten, weil sie sich auf etwas beziehen, das nicht im Gehirn ist. Das ist eine prinzipielle Grenze.
feuilleton

Wenn das Gehirn nicht nachbaubar ist, dann könnte sich aber, so vermuten manche, die künstliche Intelligenz auf anderem Wege entwickeln. Zuerst gibt es Computer, die alles aufsaugen, was ihnen an Information geboten wird, und die eigenständiger werden. Und irgendwann könnten diese selbstlernenden Systeme in einer Art von qualitativem Sprung Bewusstsein oder etwas Ähnliches erlangen.

Da gibt es eine Konfusion zwischen zwei Bedeutungen des Wortes ""Bewusstsein"". Die eine Bedeutung ist: Wenn wir bewusst sind, verarbeiten wir Informationen. Dies meint man auch formal, also symbolisch modellieren und dadurch maschinell implementieren zu können. Mit dem Philosophen David Chalmers nennt man das ""psychologisches Bewusstsein"". Es gibt aber auch etwas, was man ""phänomenales Bewusstsein"" nennt. Dies besteht darin, dass ich mich gerade in einer Situation befinde, die sich insgesamt irgendwie anfühlt.

""Irgendwie anfühlen"" klingt erst einmal nicht sehr wissenschaftlich.

Das täuscht. Es ist nämlich ein Faktum: Wir sind eingelassen in eine Umwelt, die wir als Organismen sowie auch als soziale, historische Lebewesen erfahren. Und diese Erfahrung einer Lebenswelt lässt sich nicht maschinell konstruieren, weil sie der Maschine prinzipiell fehlt. Das gilt selbst dann, wenn sie Leistungen nachahmen kann, die Menschen auch beherrschen.

Zum Beispiel?

Nehmen Sie den berühmten Schachcomputer, der den Menschen inzwischen schlägt. Das kann er, aber er weiß nicht, was Schachspielen als menschliche Praxis ist. Fremd ist ihm etwa die Erfahrung von Kaffeeduft in einem Schachlokal oder auch die Absicht, Schachweltmeister zu werden. Den Wunsch, schlauer zu sein als jemand anders, versteht er nicht: Wenn er darauf programmiert ist zu gewinnen, so weiß er doch gar nicht, was ""Gewinnen"" soll. Und selbst wenn wir Roboter bauen könnten, von denen manche meinen, sie hätten Erlebnisse, hätten sie nicht die Art von Erlebnissen, die Menschen haben. Die haben wir, weil wir Teil bestimmter Gemeinschaften sind, die sich über die Zeit sozial und historisch entwickelt haben; und weil unsere Geschichte durch die Evolution auch in unsere Körper eingeschrieben ist. Sie halten ja auch daran fest, nicht nur von Bewusstsein, sondern auch von ""Geist"" zu sprechen. Was ist der Unterschied?

Aspekte dessen, was wir Bewusstsein nennen, schreiben wir - zu Recht - auch anderen Lebewesen zu. Viele Tiere haben ohne Zweifel eine Form von Bewusstsein. Den Menschen als Menschen aber zeichnet - ein alter Gedanke - der Geist aus.

Klingt nebulös, oder?

Ist es aber nicht. Den menschlichen Geist kann man definieren als ein sozial und historisch variables Selbstporträt: Wir machen uns ein Bild davon, was es heißt, bewusst zu sein und in einen sozialen Kontext zu gehören. Menschen produzieren Weltbilder und Selbstbilder. Und damit generieren sie auch Regeln. Wer beispielsweise findet, dass ein anderer gerade gegen die Verkehrsordnung verstoße, der macht sich ein Bild, eine Vorstellung von der Gesamtsituation, und im Lichte eines solchen Bildes handelt er (oder unterlässt eine Handlung). Das heißt Geist. Das Bewusstsein hingegen geht teilweise rein natürlich vor sich. Wachsein etwa wird durch den Hirnstamm reguliert. Wachsein heißt aber noch nicht Geist haben.

Trotzdem haben viele gerade das Gefühl, dass die Innovationen der Computertechnologie viel mehr seien als nur Werkzeuge. In Ihrem Buch sprechen Sie aber von ""Übermächtigungs-"" oder ""Entlastungs-Fantasien"". Was meinen Sie damit?

Nehmen wir an, ich fürchte mich vor ""dem Internet"" - so wie es unter dem Namen ""Skynet"" in den ""Terminator""-Filmen drohte, die Weltherrschaft zu übernehmen. Wenn ich also dem Internet und überhaupt den Maschinen ein bewusstes, intelligentes Eigenleben zutraue, das uns vielleicht sogar bedroht - dann ist das, glaube ich, eine Projektion von etwas, was uns wirklich dauernd bedroht, nämlich: von anderen Menschen. Alle Probleme zwischen Menschen, alle sozialen, politischen oder ökonomischen Konflikte implizieren ja die Verantwortlichkeit von Personen. Um all das aber von uns fernzuhalten, neigen wir zu dem Wunsch, es auf Technologie abzuwälzen. Das ist so eine Art ethisches Outsourcing - und insofern eine Entlastungsfantasie. Die Bedrohung durch die Maschine ist ja, spiegelbildlich zu den Segnungen des technischen Fortschritts, ein alter Topos der Zivilisationskritik, denken Sie etwa an Fritz Langs Film ""Metropolis"".

Nun könnte einer sagen: Ihre philosophische Position klingt triftig - aber tun Sie nicht so, als würde das Silicon Valley mit den Algorithmen, Big Data und Deep Learning unser Leben gar nicht verändern?

Genau das Gegenteil ist der Fall. Ich schlage vielmehr vor, dass wir da genauer hinschauen, wo und wie die Veränderung wirklich geschieht. Sie geschieht ja nicht bloß auf der Ebene der Maschinen und Programme - auch wenn sich da viel tut gerade -, sondern vor allem auf der ökonomischen und sozialen Ebene. Unsere Analyse muss immer auch klären: Wer profitiert wovon? Wer verdient woran wie viel Geld? Diffuse Ängste oder triumphale Entlastungsfantasien hingegen machen die Akteure und Profiteure tendenziell unsichtbar. Ich plädiere als Philosoph also nicht fürs Augenverschließen, sondern fürs Augenöffnen, für eine möglichst genaue Untersuchung dessen, was im Silicon Valley passiert.

Wie bewerten Sie ethisch die neu auftretenden Fragen nach Autonomie? Manche Entlastung durch Computer ist ja eher ungefährlich - anders ist es beim operierenden Roboter, der den Arzt ersetzt, oder beim selbstfahrenden Auto.

Nun, der Gesetzgeber muss sicherstellen, dass neue Technologien, die ja zum Teil sehr zu begrüßen sind, nicht mehr Schaden anrichten, als sie Nutzen bringen. Das selbstfahrende Auto sollte also dann zugelassen werden, wenn klar ist, dass es keinen mehr gefährdet als ein herkömmliches. Solche Abwägungen sind bei jedem technischen Fortschritt notwendig.

Der Philosoph ist also nicht, wie es das Klischee will, per se technikfeindlich?

Nein! Ich hätte gar nichts dagegen, dass irgendwelche Nanobots - wie sie sich etwa der Visionär Ray Kurzweil vorstellt - durch meinen Körper laufen und jeden Krebs zerstören, bevor er entsteht; wenn denn klar ist, dass das keine oder zu vernachlässigende Nebenwirkungen hat. Ich möchte hingegen keine Nanobots in meinem Körper, die ihn krank machen und gleichzeitig Informationen an Google weiterleiten. Die Frage klingt heute vielleicht banal, sie ist aber nicht weniger wichtig geworden: Welcher Fortschritt dient der Menschheit und welcher nicht? Viele technische Innovationen werden zum Beispiel für die Waffenindustrie eingesetzt. Aber ich freue mich, dass es jetzt in Deutschland Netflix gibt und ich leichter Filme und Serien sehen kann. Die Abwägung, ja die ganze politische Verantwortung wird uns aber keine künstliche Intelligenz der Welt abnehmen können.

Die Frage ist aber, ob diese Abwägungen jetzt eine andere Qualität bekommen.

Jede Epoche, von der wir wissen, hat geglaubt, dass ihre neueste Maschine nicht vieles ändert, sondern alles, auch den Menschen selber. Das ist aber nicht geschehen. Trotzdem setzt jede Religion darauf, jede Weltanschauung und jede Prophetie - wozu ja auch die Vorstellung von einer kompletten Machtübernahme der künstlichen Intelligenz gehört. Die Vorhersagen zur KI waren ja anfangs sehr weitreichend und sind dann immer vorsichtiger geworden. Das funktioniert wie mit dem Christentum: Am Anfang war das Reich Gottes sehr nah, dann wurde der Termin immer weiter nach hinten geschoben. Den Computer ""HAL 9000"" aus dem Film ""2001 - Odyssee im Weltraum"", den Marvin Minsky ja so vorhergesagt hat, gibt es bis heute nicht. Und Ray Kurzweil sagt inzwischen vorsichtshalber, den menschengleichen Computer werde es im Jahr 2046 geben.

Die Zukunftseuphorie schwindet also nach und nach - und irgendwann geht es nur noch um Bewältigung des Alltags in Zeiten der Internetökonomie?

Das wäre meine Vorhersage. Ich will ja schließlich auch mal etwas prophezeien.";https://www.sueddeutsche.de/kultur/serie-kuenstliche-intelligenz-der-geist-der-maschine-1.2888389;sz.de;Johan Schloemann
29.02.2016;Daten fischen;Fliegen fischen steht für Ruhe und Geduld, für ein Sicheinlassen auf die Natur. Doch selbst beim Angeln wollen viele Menschen nicht auf das Smartphone in der Tasche verzichten. Diesen Umstand möchten Forscher nutzen, um wissenschaftliche Daten zu sammeln. Rolf Hut, Scott Tyler und Tim van Emmerik hatten die Idee, Wathosen von Fischern mit Thermometern auszustatten. Damit sollen die Angler bei ihren Touren durch Flüsse und Bäche Daten über die Wassertemperatur sammeln. In der Fachzeitschrift Geoscientific Instrumentation, Methods and Data Systems beschreiben Hut und Kollegen das Verfahren, das sie selbst bereits in Fließgewässern getestet haben. Per Bluetooth soll das Thermometer laufend Daten an das Handy des Fischers senden, wo sie mit GPS-Koordinaten verknüpft werden. Die Angler könnten so die besten Stellen für den Fischfang finden. Die Forscher wollen ihrerseits mit den Daten vor allem besser verstehen, wie und wo sich Oberflächen- und Grundwasser austauschen.;https://www.sueddeutsche.de/wissen/citizen-science-daten-fischen-1.2885409;sz.de;Christian Endt
08.02.2016;Wer haftet für künstliche Intelligenz, wenn sie Mist baut?;"Roboter, die mit Menschen arbeiten oder sie ersetzen. Software, die so intelligent ist, dass sie besser Schach, Jeopardy und Go spielt als die größten Champions. Computer, die Bilder erkennen und Krankheiten diagnostizieren können. Helfer im Handy, die Flüge buchen. Und Maschinen, die selbständig Entscheidungen treffen und umsetzen - künstliche Intelligenz verändert bereits heute unser Leben. Und doch steht die Technik erst am Anfang. Den Fragen, die mit dem Siegeszug der künstlichen Intelligenz (KI) entstehen, widmet das Feuilleton eine Serie: Wie wird eine Zukunft mit Maschinen aussehen, die vieles besser können als der Mensch? Wer trägt die Verantwortung für ihre Handlungen? Was mag passieren und was bleibt, aller Utopie zum Trotz, doch nur eine Science-Fiction-Fantasie? In dieser Folge beschäftigt sich der Jurist Andrej Zwitter mit Fragen von Ethik und Recht in einer Welt voll künstlicher Intelligenz.

Glaubt man den Warnungen von Technologiepionieren wie Bill Gates und Elon Musk vor den Gefahren einer Künstlichen Superintelligenz, dann ist fast befürchten, dass uns das Ende durch Skynet (Terminator) oder durch die Matrix kurz bevorsteht. Wissenschaftler wie Stephen Hawking und Nick Bostrom etwa beschreiben das Prinzip der Singularität: Es geht um den Punkt, an dem künstliche Intelligenz die Intelligenz der Menschen übersteigt und so zur Bedrohung der Menschheit werden kann. Bislang haben wir diesen Punkt noch nicht erreicht, Befürchtungen wie diese gehören ins Reich der Science-Fiction. Und doch werden bereits ethische und rechtliche Konsequenzen sichtbar.

Künstliche Intelligenz beginnt bereits dann, wenn Algorithmen auf Umgebungseinflüsse reagieren, das ist relativ simple Automatisierung. Je weiter sich diese Automatisierung - manchmal auch selbständig - fortentwickelt, etwa durch maschinelles Lernen, desto dringlicher stellt sich die Frage, ob Programmierer noch für Entscheidungen der KI zur Rechenschaft gezogen werden können. Tatsächlich beruhen viele unserer täglichen Abläufe auf künstlicher Intelligenz, zum Beispiel Suchalgorithmen von Google oder Muster- und Gesichtserkennung auf Facebook.
Big Data und Big Nudging

Dabei sind zwei Schlagworte wichtig: Big Data und Big Nudging. Big Data, also jene riesigen Datenmengen, die sich durch das Internet ansammeln, können heutzutage mit bestimmten Analyse-Methoden verwendet werden, um unsere Werte und Wünsche zu analysieren, manchmal sogar, ohne dass wir uns ihrer bewusst sind. Für Marketing-Strategen hat sich damit ein Traum erfüllt, man denke nur an Amazons Kaufvorschläge (""Kunden, die diesen Artikel angesehen haben . . .""). Mit diesem Wissen lässt sich der Kundenservice verbessern, aber auch die Willensfreiheit einschränken, etwa wenn man auf Suchmaschinen nur noch benutzerspezifische Ergebnisse erhält oder bestimmte politische Themen ganz ausgeblendet werden. Letzteres praktizieren Suchmaschinen in nicht-demokratischen Ländern seit langen.

Big Nudging beschreibt genau das: die Beeinflussung der Meinung und des Verhaltens einer Gesellschaft ohne ihr Wissen. Das ist ethisch problematisch, und doch gibt es kaum Gesetze, die es regulieren.

Viele Eingriffe in die Freiheit, Sicherheit und Privatsphäre der Nutzer sind durch Menschenrechte, Strafrecht und Zivilrecht gedeckt. Schwierig wird es jedoch in zwei Fällen: einmal bei grenzüberschreitenden Sachverhalten und zweitens dann, wenn Algorithmen eigenständig Entscheidungen fällen, die man dann demjenigen, der die Algorithmen programmiert hat, nicht mehr zuschreiben kann.

Im Oktober 2015 hat der Europäische Gerichtshof das Datenabkommen mit den USA aufgehoben, das so genannte Safe Harbor Abkommen. Damit bleibt eine rechtliche Lücke, was Datenmissbrauch betrifft. Was die grenzüberschreitende Manipulation von Internetsuchergebnissen angeht, so betrachtet sich die Europäische Kommission als zuständig. Zumindest auf dem Europäischen Markt will sie solche Praktiken, zumindest im Fall von Google, unterbinden.
Ethisch und rechtlich unklar sind hingegen jene Fälle, in denen autonome Entscheidungen von künstlicher Intelligenz menschlichen oder materiellen Schaden verursachen. Solange ein Algorithmus nicht selbständig lernfähig ist, können die Konsequenzen dem Programmierer noch relativ klar zugerechnet werden. Schwieriger wird es bei lernfähiger KI, insbesondere, wenn die Entscheidung auf eingespeisten Daten und Maschinenlernen beruht. Dies ist zum Beispiel beim automatisierten Börsenhandel der Fall. Dabei kaufen und verkaufen Algorithmen selbständig Aktien in Sekundenbruchteilen. Der so genannte Flashcrash im Jahr 2010, also der kurzzeitige Einbruch des Dow Jones um beinahe 1000 Punkte, gilt als das Werk von Börsen-KIs durch so genannte Quant-Hacker.

Auch selbstfahrende Autos basieren auf künstlicher Intelligenz. Juristisch betrachtet stellt sich die Frage, wem Unfälle angelastet werden mit Autos, die durch Künstliche Intelligenz gesteuert werden. Ist der Fahrzeughalter haftbar zu machen? Der Programmierer? Der Hersteller? Kann die Tatsache, dass künstliche Intelligenz Fahrzeuge steuert, nicht sogar positiv sein?

Im Fall des selbstfahrenden Google-Autos, das derzeit getestet wird, waren Unfälle in aller Regel nicht von der KI, sondern von anderen Unfallteilnehmern verschuldet. Und das waren Menschen. Im Zweifelsfall ist zumindest in diesem Fall der Mensch der Verursacher und nicht die Künstliche Intelligenz.

In den Bereich der Science-Fiction gehört derzeit noch eine künstliche Intelligenz mit einem Bewusstsein. Allerdings ist ein erster bescheidener, aber wichtiger Schritt in diese Richtung bereits geschehen. Roboter am Ransselaer Polytechnic Institute (USA) haben erste Tests zur Frage bestanden, ob sie ein Bewusstsein haben können. Roboter mit diesen Fähigkeiten berühren die philosophische Annahme, dass die Gleichheit aller Menschen auf ihrer Verstandesfähigkeit beruht.

Ethisch besteht kein Grund, einem verstandesfähigen Wesen gewisse Existenzrechte abzusprechen; aus Sicht der Aufklärung müsste man moralisch wohl gerade wegen dieser Vernunftbegabung und des Bewusstseins einer künstlichen Intelligenz gewisse Schutzrechte, wenn nicht sogar Entfaltungsrechte einräumen. Dieses Problem wird dem Gesetzgeber weitere Entscheidungen abverlangen. Ist KI durch das Gesetz gebunden oder in seiner Existenz geschützt, zumindest in dem Ausmaß, den der Gesetzgeber dem Tier (im Unterschied zur Sache) einräumt? Hat also der Besitzer einer KI eine dem Hundehalter ähnliche Sorgfaltspflicht? Kann eine künstliche Intelligenz, die den Punkt der Singularität erreicht hat und handlungsfähig ist, Rechte und Pflichten wahrnehmen? Setzen gesetzliche Strafen zum Zwecke der Erziehung des Straftäters und der Abschreckung von Nachahmern nicht auch eine Art von Empathie- und Reuefähigkeit voraus?

Diese und ähnliche Fragen wird die Juristen und Ethiker in Zukunft vermehrt beschäftigen. Künftig wird es zwingend sein, dass die Urheber von KI ethische Grundprinzipien bereits in den Kern ihrer Programme unüberschreibbar einbauen. So hat es bereits der Science-Fiction-Autor Isaac Asimow in seinen drei Gesetzen der Robotik vorgedacht hat. Dass es nicht einfach ist, einer künstlichen Intelligenz Regeln beizubringen, die selbst der Mensch oft nur intuitiv befolgt, versteht sich von selbst. Bislang aber mangelt es ohnehin an Standards und interdisziplinären Prinzipien. Die Lösung dieser Kernfrage in der Grundlagenforschung zu künstlicher Intelligenz und Ethik dringend erfordert eine engere Zusammenarbeit zwischen Ethikern, KI-Experten und Datenwissenschaftlern.";https://www.sueddeutsche.de/digital/serie-kuenstliche-intelligenz-wie-ein-hund-1.2854646;sz.de;Andrej Zwitter
03.02.2016;Wie Google User für sich arbeiten lässt - unbewusst und unbezahlt;"Roboter, die mit Menschen arbeiten oder sie ersetzen. Software, die so intelligent ist, dass sie besser Schach, Jeopardy und Go spielt als die größten Champions. Computer, die Bilder erkennen und Krankheiten diagnostizieren können. Helfer im Handy, die Flüge buchen. Und Maschinen, die selbständig Entscheidungen treffen und umsetzen - künstliche Intelligenz verändert bereits heute unser Leben.

Und doch steht die Technik erst am Anfang. Den Fragen, die mit dem Siegeszug der künstlichen Intelligenz (KI) entstehen, widmet das Feuilleton eine Serie: Wie wird eine Zukunft mit Maschinen aussehen, die vieles besser können als der Mensch? Wer trägt die Verantwortung für ihre Handlungen? Was mag passieren, und was bleibt, aller Utopie zum Trotz, Science-Fiction-Fantasie? In dieser Folge beschäftigt sich der Internetkritiker Evgeny Morozov mit dem Zusammenspiel von Big Data und künstlicher Intelligenz.
Captchas sollen Menschen von Maschinen unterscheiden

Jeder, der schon einmal versucht hat, mit der Plattform für Forscher von Google - sie heißt ""Google Scholar"" - zu arbeiten, stößt irgendwann gegen eine digitale Wand. ""Google Scholar"" verlangt dann vom Nutzer den Beweis, dass er kein Roboter ist. Der Nutzer wird dazu aufgefordert, ein sogenanntes ""Captcha"" zu lösen. Das ist in der Regel eine Kombination abenteuerlich geschriebener Buchstaben und Zahlen, die man lesen und in ein leeres Feld übertragen muss.

Solche Sperren sind im Netz längst allgegenwärtig. Sie verhindern, dass automatisierte Programme, die so tun, als seien sie Menschen, Schaden anrichten, zum Beispiel, indem sie Daten von Webseiten klauen oder massenweise begehrte Konzertkarten aufkaufen.

Im Fall von Google Scholar bestehen diese Captchas aus Fotos, deren Motive irgendwie mit der Navigation in urbanen und ländlichen Räumen zusammenhängen. Die Nutzer sollen zum Beispiel verschiedene Straßenschilder unterscheiden, Wasserfälle von Seen oder Sportwagen von Kleinlastern. Darüber hinaus findet man in Dutzende Einzelteile zerlegte Fotos von Straßen, auf denen die Nutzer erkennen sollen, ob Straßenschilder etwas Bestimmtes anzeigen, zum Beispiel eine Richtung oder ein Verbot.
Die Nutzer trainieren die Computer - ohne es zu wissen

Google schweigt zur Frage, warum die Captchas so seltsam sind. Man kann aber davon ausgehen, dass die Nutzer von Google Scholar beim Lösen einer solchen Sperre einen Nebeneffekt erzielen. Sie bringen wohl Googles selbstfahrenden Autos bei, wie sie in Städten navigieren und Verkehrszeichen lesen können.

Das ist Googles kleines Geheimnis: Während andere Technologiekonzerne so gut wie möglich versuchen, ein Straßenschild für ihre künstliche Intelligenz detailliert mit mathematischen Methoden zu beschreiben, kann Google ganz einfach eine Million Nutzer dazu bringen, dieses Wissen den Computersystemen beizubringen. Dass das geht, zeigt ein berühmtes ""Deep Learning""-Experiment. In dessen Folge lernte eines von Googles Systemen, wie eine Katze aussieht, einfach nur durch die Betrachtung von Standbildern aus Katzenvideos auf Youtube. Der Dreh bei Google Scholar ist nun, dass Google die Fotos von Tausenden Menschen klassifizieren lässt und dabei umso schneller alles Wissenswerte über die Form und den Inhalt eines Stoppschildes lernen kann.
Hinter jeder KI steckt (mindestens) ein kluger Kopf

Drei Lehren lassen sich daraus ziehen: Erstens zeichnen sich zahlreiche Fortschritte bei der Entwicklung künstlicher Intelligenz durch den ""Schachtürken""-Effekt aus - benannt nach dem Menschen, der sich im Inneren des berühmten Schach-Automaten versteckte. Soll heißen: Auch wenn es verlockend ist, zu denken, dass all die Fortschritte auf dem Gebiet der künstlichen Intelligenz nur auf die Genialität von Forschern oder deren große Ressourcen zurückzuführen sind, so sind sie doch auch ein Ergebnis der Fähigkeit, Daten zu sammeln, zu klassifizieren und zu analysieren. Und genau diese Aufgaben werden oft von ahnungslosen Menschen erledigt.

Ein Beispiel: ""Google Now"", ein virtueller Assistent von Google, weiß recht oft sehr genau, welche Artikel einem Nutzer gefallen haben. Das weiß er aber nicht, weil er die Persönlichkeit der Nutzer ""geknackt"" hat, wie man vermuten könnte. Nein, seine Empfehlungen sind so gut, weil er erstens weiß, welche Artikel jemand in der Vergangenheit gelesen hat und welche Artikel demjenigen gefallen haben; zweitens, weil er weiß, welche anderen Leute diese Artikel gelesen und ob sie wiederum diesen gefallen haben; und drittens, weil der Assistent weiß, welche dieser Artikel ein bestimmter Nutzer noch nicht gelesen hat.

Indem er nun exakt diese gut passenden, aber noch nicht gelesenen Artikel einem Nutzer vorschlägt, entsteht bei diesem eine Art Wow-Effekt: Wie clever ist doch dieses Empfehlungssystem! Dabei sind die Systeme eher dumm und algorithmisch. Sie sind nur deshalb gut, weil sie auf so viele Daten zurückgreifen können, die Nutzer generiert haben.
Google spannt Wissenschaftler vor den KI-Karren

Außerdem führt das ganze Sammeln und Analysieren von Nutzerdaten im Zusammenhang mit künstlicher Intelligenz dazu, dass man über eine politische Dimension der KI-Technik sprechen muss. Es ist zweifellos beeindruckend, dass so viele Wissenschaftler, die in öffentlich finanzierten Sachbereichen forschen, Google Scholar unterstützen - und dadurch Googles selbstfahrende Autos. Aber warum verrichten sie diese Arbeit kostenlos? Ist der Gegenwert hoch genug? Ist der Nutzen, den mir Google Now bietet, so viel wert wie derjenige meiner Daten für den Konzern? Googles Logik liegt hier offen zutage: Zunächst rückt der Konzern an und beansprucht die Daten, die sie bearbeiten, als seine eigenen und verstaut sie hinter Schloss und Riegel. Dann gestattet er in geringem Maß eine freie Nutzung dieser Daten - aber jeder, der sie in vollem Umfang nutzen will, wie bei groß angelegten Forschungsprojekten mit Google Scholar, wird umgehend vor den KI-Karren gespannt. Wenn Wissenschaftler dort zum Beispiel etwas über die Zitationsformen in Artikeln über römische Architektur erfahren wollen, müssen sie erst Google etwas über die Straßen von Rom beibringen.
Wer die Daten hat, hat die Macht

Letztlich sollte jeder, der sich für die Verlagerung der Macht - auch der Macht in der Informationssphäre - hin zu den Bürgern einsetzt, sich darüber klar werden, dass es nicht ausreicht, die Rechte an Daten zu bewahren und dafür zu sorgen, dass diese jenen Bürgern zukommen, die sie produzieren. Es mag sich gut anfühlen, dass man selbst oder eine Institution - meine Stadt oder mein Verband - die Daten, die man kostenlos an Google gibt, abrufen kann. Aber wird irgendjemand, der nicht Google ist, Nutzen daraus ziehen können? Leider nein, denn man benötigt auch die passende Infrastruktur, vom Umfang der Daten gar nicht zu sprechen, um auf erhellende Erkenntnisse zu kommen. Man kann kein Haus mit fünf Ziegeln bauen.

Folglich muss jeder, der sich für eine Verschiebung der Macht weg von Google und anderen Konzernen dieser Art einsetzt, einen ganzheitlichen Ansatz verfolgen und beide Seiten der Medaille betrachten; sich nur auf eine zu konzentrieren, zum Beispiel auf den Umgang mit Daten, wird dem Problem nicht gerecht. Denn man kann nicht ernsthaft bezweifeln, dass ein Großteil des Fortschritts im Bereich der künstlichen Intelligenz ebenso sehr ein Produkt politischer und ökonomischer Entscheidungen ist - inklusive der Entscheidungen über die Besitzverhältnisse von Daten - wie von echtem Fortschritt in der Theorie und Praxis der KI.

Es mangelt nicht an Jubelchören über das Silicon Valley, es soll ja der einzige Ort des Fortschritts sein. Aber es lohnt sich sicher auch, zu fragen, welchen Fortschritt man hätte erreichen können, wenn das System dort nicht das Datenhorten durch eine Handvoll Konzerne begünstigen würde. Genau diese Überlegung vermag uns auch in Richtung einer heiklen Frage zu führen: Wie sähe eigentlich ein vollständig demokratisierter Zugang zu KI-Technologien aus?";https://www.sueddeutsche.de/digital/serie-kuenstliche-intelligenz-wir-unfreiwilligen-helfer-1.2847212;sz.de;Evgeny Morozov
26.11.2015;Reich oder arm?;"In vielen afrikanischen Staaten existieren keine zuverlässigen Daten über die Entwicklung von Bevölkerung und Wirtschaft. Doch womöglich reicht es in Zukunft, den Mobilfunkverkehr zu analysieren. Was diese Daten über ganze Länder verraten.

Anhand von Daten zur Handynutzung lässt sich ein sozio-demografisches Profil ganzer Nationen nachzeichnen. So berichten Informatiker um Joshua Blumenstock von der University of Washington in Seattle im Fachjournal Science (Bd. 350, S. 1073, 2015), dass sie auf diese Weise die Verteilung von Wohlstand im afrikanischen Staat Ruanda abbilden können. In vielen Entwicklungsländern existieren nur sehr ungenaue nationale Statistiken, etwa über die Verteilung von Einkommen, über die Industrieproduktion oder die geografische Konzentration von Armut. Häufig wichen die offiziellen Zahlen um mehr als 50 Prozent von den tatsächlichen Gegebenheiten ab, schreiben die Wissenschaftler um Blumenstock. Weil aber auf Basis dieser Informationen politische Entscheidungen gefällt werden, ist das ein Problem. Die Forscher werteten nun die Daten von mehreren Milliarden Mobilfunk-Interaktionen in Ruanda aus. So erstellten sie für einzelne Personen Bewegungsprofile, analysierten deren soziale Netzwerke und extrahierten außerdem Daten zum finanziellem Verhalten. So gelang es ihnen, ein exaktes Bild zu erstellen, wo in Ruanda Wohlstand oder Armut konzentriert sind.";https://www.sueddeutsche.de/wissen/big-data-reich-oder-arm-1.2756037;sz.de;SZ
01.06.2015;Im Auf und Ab der Formate;"Ein nicht zuletzt für Zeitungsleser bedenkenswerter Satz ist der folgende: ""Aus neurowissenschaftlicher Perspektive ist die Lesefähigkeit faszinierend, weil das Gehirn von der Evolution nicht für diese Tätigkeit gemacht wurde."" Jetzt bitte nicht weiterblättern oder nach unten zu den Anzeigen schweifen, sondern die naheliegende Frage ins Auge fassen, wofür eigentlich die Areale, die im Gehirn für das Lesen unverzichtbar sind, gemacht waren, bevor vor ungefähr 6000 Jahren das Lesen begann. Spurenlesen? Womöglich, aber bestimmt nicht mit dieser Metapher.

Denn die gehört zu den Zeiten, als die Kulturtechnik des Lesens schon längst begonnen hatte, das Gehirn nachhaltig zu verändern. Das war nur möglich, weil das Gehirn plastisch ist, vielfältig veränderbar, und die Schriftrolle wie später der Kodex haben die Areale, in die sie sich einnisteten und in denen sie sich entfalteten, nicht unberührt gelassen: ""denn zwar gab es vor der Erfindung der Schriftsprache die einzelnen Areale, nicht aber die über den Cortex verteilten Funktionsnetzwerke, die beim Lesen aktiviert werden.""

Im neuen Buch des in Zürich lehrenden Mediziners und Wissenschaftshistorikers Michael Hagner findet sich der Abschnitt ""Lesen ist eine Kulturtechnik"" an ziemlich später Stelle, aber an einer strategischen Position. Denn das Verhältnis von Kodex und Cortex ist eines der Lebensthemen dieses Autors, der seit der Studie ""Homo cerebralis. Der Wandel vom Seelenorgan zum Gehirn"" (2000) immer wieder die Wissenschaftsgeschichte der Hirnforschung in Beziehung zur Kulturgeschichte der Wissensformen und ihrer Infrastrukturen gestellt hat. Hier aber spricht er nur am Rande über seine Forschungsgebiete, hier spricht er als Autor der Bücher, die aus seiner Forschung hervorgegangen sind, als Mitglied der ""scientific community"".

Er hat diesem Buch einen denkbar lapidaren Titel gegeben: ""Zur Sache des Buches"", aber es ist eine Streitschrift geworden. Sie handelt nicht vom Buch überhaupt, sondern von dem Buchtyp, mit dem der Titel spielt, vom geisteswissenschaftlichen Sachbuch der Art, wie Michael Hagner sie schreibt. Und, vor allem, weiterhin schreiben will, in dem Format, das ihm für seine Zwecke als das geeignetste erscheint: auf Papier gedruckt. Es ist das Format, dem er seine Reputation verdankt: ""Die relevanten, neuartigen provozierenden oder einfach nur soliden Texte von der Länge eines Buches sind in den mir bekannten Wissensfeldern auf Papier erschienen und nicht auf persönlichen bzw. institutionellen Websites oder Repositorien."" Hagner schreibt über das Dickicht der ""Open Access""-Welt und deren Geschäftsmodelle

Eine Streitschrift ist das Buch geworden, weil dieses Format immer stärker durch die Vorstellung unter Legitimationsdruck gerät, die Fortentwicklung der wissenschaftlichen Kommunikation sei notwendig mit ihrer möglichst restlosen Verlagerung in digitale Infrastrukturen und Formate verbunden: ""das gedruckte Buch ist ein Störfaktor bei der reibungslosen Durchsetzung des neuen digitalen Publikationsregimes.""

Hagner verbindet sein Plädoyer für die Fortführung der Symbiose von Cortex und Kodex mit einer Musterung der Buchkritik seit der Frühen Neuzeit, blickt auf Nietzsches Unbehagen an den immer kleineren Eiern, die in immer dickeren Büchern gelegt werden, auf die Prophezeiungen vom Untergang des Buches von Theodor Lessing bis zu Herbert Marshall McLuhan, auf die aktuellen Visionen einer ständigen, umfassenden, frei zugänglichen und interaktiven Repräsentation des menschlichen Wissens im Netz. Dabei entsteht en passant ein Selbstporträt des Autors. Er profitiert von den immens gewachsenen Forschungsressourcen im Netz, er schätzt die Blogs seiner Kollegen, aber auch die von Literaten wie Wolfgang Herrndorf und Kathrin Passig, aber er erhebt überall dort Einspruch, wo seine Freiheit als Autor beschnitten wird, das Publikationsformat selbst zu bestimmen.

Kurz, Michael Hagner schreibt aus der Autorenperspektive. Daraus resultiert, wie in Valentin Groebners Bändchen ""Wissenschaftssprache digital. Die Zukunft von gestern"" (2014) die Skepsis gegenüber den aus der User-Perspektive formulierten Lobliedern auf das unablässige, barrierefreie Zirkulieren des Wissens in den neuen digitalen Formaten. Diese Skepsis mündet bei Hagner in einen gründlichen Streifzug durch die Welt des ""Open Access"".

Sein Reisegepäck ist schlicht, er hat Messinstrumente in Gestalt von Studien, Statistiken und Bilanzen dabei und hebt gegenüber den Idealen der nutzerzentrierten Offenheit und ungehinderten Zirkulation, die sich scheinbar zwanglos aus der aktuellen Medienrevolution ergeben, das Agieren und die Interessen der klassischen Akteure hervor: der großen Wissenschaftsverlage und des Staates. ""Open Access"" ist in diesem Buch ein Dickicht, in dem die Ausweitung von Zirkulation und Speicherung des Wissens in rabiat durchgesetzte Geschäftsmodelle, das ""Data Mining"" global agierender ökonomischer Interessen und das Regelwerk staatlicher Bürokratien übergeht.
Das Taschenbuch war ein ideales Medium der Entgrenzung von Wissen

Hagner vertritt die Sache des geisteswissenschaftlichen Buches, aber er ist Mediziner und nah an der Forschungspraxis der sciences. Das gibt seiner dichten Beschreibung ihren Detailreichtum und dem Buch eine Hauptlinie: das gedruckte geisteswissenschaftliche Buch eines einzelnen Autors als Medium eigenen Rechtes gegen das Schlüsselmedium der Naturwissenschaften abzusetzen, den häufig im Team erstellten, in einer führenden Zeitschrift publizierten Aufsatz. Es gibt für den Wissenschaftshistoriker Hagner keinen Mediendeterminismus. Aber das Auf und Ab von Formaten im Wechselspiel mit historischen und innerwissenschaftlichen Konstellationen. Das macht die Abschnitte zum ""Goldenen Zeitalter"" des Sachbuchs in den USA und in Frankreich lesenswert, vor allem aber die Passagen zur Symbiose von Sachbuch und Taschenbuch in Deutschland seit den Fünfzigerjahren, seit S. Fischers ""Büchern des Wissens"" und Rowohlts Enzyklopädie. Was Theodor W. Adorno und der frühe Enzensberger als Ausverkauf des Wissens beargwöhnten, erwies sich als ideales Medium der Entgrenzung des Wissens und Brandbeschleuniger der Politisierung schon vor 1968.

Weil die Medientechnologien in diesem Buch keine Generalursache sind, treten die aus den Wissenschaften selbst entspringenden Probleme hervor, für Deutschland vor allem: die Inflation des Sammelbandes, des ""Packesels der Überforschung"", auf Kosten der Monografie, das Publizieren nach Maßgabe der einander jagenden Projekte und Forschungscluster. Die Kritik am überhastet Publizierten ist Teil dieses Plädoyers für das gedruckte Buch. Nicht jede Monografie ist ein edles Rennpferd, aber wenn sie es ist, dann deshalb, weil sie ihr Format verstanden hat, in ihm eingewohnt ist.

Und der Leser? Er honoriert dieses Verstehen durch seine Verweildauer.";https://www.sueddeutsche.de/kultur/medienrevolution-im-auf-und-ab-der-formate-1.2546237;sz.de;Lothar Müller
05.05.2015;Die Evolution des Pop;"Als Freddy Mercury starb, war die Ära der großen Stadion-Rockbands erst mal vorbei. Ende 1991, als der Sänger von Queen seiner Aids-Erkrankung erlag, war die Musik seiner Gruppe bereits nicht mehr so gefragt; zumindest spielte sie in der Hot-100 der amerikanischen Billboard-Charts keine Rolle mehr. Dabei war das Stück ""I want it all"" vom Mai 1989 noch ein Hit und dem Stil der Band absolut treu: H6 und T5, würde Matthias Mauch sagen. Aber am Anfang der 1990er-Jahre lockte der Sound die amerikanischen Käufer kaum mehr zur Kasse.
Der Rechner hört Klangfarbe und Akkorde heraus - und beim Hip-Hop deren Fehlen

H6 und T5 sind Kategorien, die Mauchs Computer ausgespuckt hat. Der Mathematiker von der Queen Mary University in London hat mit Kollegen gut 17 000 Songs, die zwischen 1960 und 2009 in den Hot-100 standen, in den Rechner eingelesen und nach Mustern gesucht. Ganz frei war die Maschine dabei nicht: Sie sollte auf die Akkordfolgen der Stücke achten, die Harmonien. Und auf die Klangfarbe, Timbre genannt. Am Ende kamen jeweils acht Kategorien heraus, H6 steht zum Beispiel für gestuften Akkordwechsel, T5 für lauten, energischen Gitarrensound. Diese Kombination zeichnet neben Queen Bands wie Van Halen, Mötley Crüe oder Kiss aus. Auch ""Highway to Hell"" von AC/DC folgt dem Muster.

""Unser Ziel war es, der Musikwissenschaft handfeste Daten zu unterlegen"", sagt Mauch. Wer sich nur auf Anekdoten und Geschmacksurteile stütze, könne schließlich keine Hypothesen testen. ""Ich glaube, dass die Faszination für Musikkultur durch objektives Wissen nur zunehmen kann."" Mauch und sein Team wenden daher Techniken der Datenanalyse an, wie sie in der Evolutionsforschung genutzt werden, um den Stammbaum verwandter Bakterienspezies zu erkennen. Zudem kommen statistische Methoden zum Einsatz, mit denen auch die Psychologie arbeitet (Royal Society Open Science, online).

Jeder der 17 094 Songs, die 86 Prozent aller in 50 Jahren Chartgeschichte vertretenen Singles ausmachen, bekam zunächst vom Computer 16 Zahlenwerte zwischen null und eins; sie stehen für die acht Harmonie- und Timbre-Kategorien. Die H-Einteilung hatte Mauchs Team vorgegeben: H1 zum Beispiel ist die Verwendung von Dur-Sept-Akkorden, die im Jazz eine Spannung erzeugen und im Blues den ""schmutzigen"" Klang bewirken, so beschreibt es der aus Deutschland stammende Forscher. Ihre Verwendung in Hot-100-Songs ist den Daten zufolge seit 1960 immer weiter zurückgegangen. H5 hingegen bedeutet den Verzicht auf Akkorde, was erst mit dem Aufkommen von Hip-Hop Anfang der 1990er-Jahre zum Erfolgsrezept wurde.

Die T-Kategorien wiederum hat der Computer weitgehend alleine gefunden. Die Londoner Wissenschaftler mussten zehn Experten bitten, anhand Klangproben englische Beschreibungen zu finden. T1 zum Beispiel ist demnach mit aggressiven Trommeln und Percussion verknüpft, T8 mit weiblichen Stimmen und T3 mit Sprache oder Sprechgesang.

Diese Einteilung führt zu einigen Merkwürdigkeiten: So gehört Whitney Houstons Schnulze ""I will always love you"" zur Spitzengruppe der schlagzeuglastigen T1-Kategorie. Und der Song ""Down to the line"" der Gitarrenband Bachman Turner Overdrive ist angeblich das am wenigsten gitarrenlastige Stück. ""Extreme Ergebnisse sind mit besonderer Vorsicht zu genießen"", verteidigt sich Mauch.

Die Einteilung wirkt überzeugender, wenn sie nur Künstler mit mehr als neun Songs in den Charts berücksichtigt. Demnach gehören Smokey Robinson, Tony Bennett und Simon & Garfunkel zu den besten Vertretern jenes sanften ruhigen Klangs (T2), den Metallica oder Chaka Khan meiden. Für Gitarrensound stehen Pat Benatar, Aerosmith und Linkin Park. Typische Muster in den H- und T-Werten von Songs haben die Musik-Forscher dann Genres zugeordnet. Dabei halfen die Urteile von Nutzern des Internetdienstes Last.fm. Die Daten zeigen, wie das Genre Hip-Hop und Rap Anfang der 1990er-Jahre plötzlich großen Erfolg in der Hitparade hatte - von den Kategorien H5 (keine Akkorde) und T3 (Sprache) gekennzeichnet. Elektronische Musik und New Wave erlebten ihre große Zeit in den 1980er-Jahren. Für Easy Listening und Jazz ging es seit 1960 in den Charts nur bergab. Und die verschiedenen Ausprägungen der Rockmusik wechselten sich in ihrer Beliebtheit ab. ""Wir können aber nicht die Klage bestätigen, dass die Vielfalt der Musik in den Charts im Lauf der Zeit abgenommen habe"", sagt Mauch. Fast im gesamten Zeitraum kamen im Durchschnitt der Hot-100-Songs sieben oder mehr der Timbre-Kategorien vor; nur in der zweiten Hälfte der 1980er-Jahre war das Klangspektrum etwas ärmer, als der Schlagzeug- und Gitarrensound regierte. In der Zeit verbreiteten sich sogenannte Drumcomputer, deren neue Möglichkeiten die Musiker ausgiebig nutzten. Drei Jahre haben Mauch und seine Kollegen in ihren Daten gefunden, in denen sich der Stil der Songs in der Hitparade radikal änderte. Sie sprechen von Revolutionen: 1964 Rock, 1983 Synthesizersound und elektronische Musik sowie 1991 der Aufstieg des Hip-Hop. ""Diese Jahre waren der Höhepunkt, nicht der Start der Revolutionen"", stellt Armand Leroi vom Imperial College in London klar.

Beim Durchbruch des Rock im Jahr 1964 schreiben viele Beobachter britischen Bands entscheidenden Einfluss zu. ""I want to hold your hand"" und ""Not fade away"" waren Anfang 1964 die ersten Chart-Hits der Beatles und der Rolling Stones. Doch viele der musikalischen Trends, die damals ihren Höhepunkt erreichten, hätten schon 1962 begonnen, so Leroi. Die sogenannte British Invasion habe den Umschwung beschleunigt, nicht ausgelöst. ""Immerhin zeigt die Analyse, dass Beatles und Rolling Stones der Entwicklung in manchem ein wenig voraus waren: zum Beispiel im lauten Gitarrensound.""

Wer die Grafiken der Londoner Forscher studiert, kann sogar einen Teilaspekt der ewigen Frage beantworten, welche Gruppe die wichtigere war: Die Beatles hatten demnach etwas größere Anteile an der Rockrevolution in Amerika.";https://www.sueddeutsche.de/wissen/big-data-die-evolution-des-pop-1.2465127;sz.de;Christopher Schrader
24.04.2015;Datafiziert;"Vielleicht hat Baby Roy als einer der ersten Menschen auf diesem Planeten etwas erlebt, was uns allen bevorsteht. Als es nach der Geburt in sein künftiges Heim gebracht wurde, warteten bereits elf Panoramakameras und 14 Mikrofone. Sein Vater, der Linguist und Kognitionswissenschaftler Deb Roy vom MIT Media Lab in Boston, hatte die Aufnahmegeräte installieren lassen, um auf einer soliden Datenbasis einer der großen Fragen seines Faches nachzugehen: Wie kommt ein neuer Mensch zu seiner Sprache? Deshalb hielt er in den ersten drei Lebensjahren seines Sohnes dessen gesamte Wachzeit fest. So dokumentierte er in Bild und Ton jede Krabbelei des Kindes, jeden Kontakt mit seinen Kindermädchen, jedes Spiel mit Teddybär oder Rassel, jedes Planschen in der Badewanne, vor allem aber: jedes Brabbeln.

Am Ende hatte der Forscher insgesamt 250 000 Stunden Ton und Video gespeichert. So konnte er im Detail analysieren, wie neue Wörter im Geist von Baby Roy geboren wurden, wie etwa im Laufe von sieben Monaten aus sinnlosen Lauten das englische Wort für Wasser entstand, nämlich so: ""Gagagagaga Gaga gaga gaga guga guga guga wada gaga gaga guga gaga wadö guga guga uata wata wata wata wata wata water water water!""

Möglich war das ""Human Speechome"" getaufte Giga-Projekt mit seinen riesigen Datenmengen nur, weil Computer automatisch die Tonbänder transkribierten und die Videoszenen analysierten. Es zeigt beispielhaft, was unter dem Begriff Big Data zu verstehen ist, und führt zu einer gruseligen Frage: Was wäre, wenn jeder Mensch auf ähnliche Weise wie Baby Roy unter Dauerbeobachtung stünde?

Ein Umbruch steht an, eine Revolution, vergleichbar mit der Erfindung der Dampfmaschine. So wie das Teleskop die Erkundung des Kosmos ermöglichte und das Mikroskop die Entdeckung der Bazillen, so wird Big-Data-Technologie als eine Art Sozioskop in den nächsten zehn oder zwanzig Jahren ein neues Verständnis der Welt schaffen. Die Folgen für Wissenschaft und Gesellschaft, Wirtschaft und Politik werden gewaltig sein. Neue Risiken bauen sich auf, aber auch neue Chancen: Es droht das Ende der Privatsphäre, es locken wissenschaftliche Durchbrüche. Womöglich werden viele Menschen um ihre Arbeit fürchten müssen, doch für findige Unternehmer könnte es ganz neue Geschäftsfelder geben. Cyberterroristen werden neue Angriffsziele finden, Gesellschaften könnten sich vielleicht besser steuern.

""Nichts wird so bleiben, wie es ist"", prognostiziert trocken der Physiker und Soziologe Dirk Helbing vom Lehrstuhl für Computational Social Science der ETH Zürich, ""manchmal wundere ich mich, dass dies offenbar kaum einem Menschen klar ist.""

Vielleicht liegt das auch daran, dass man sich Big Data nicht so leicht vorstellen kann. Hat es mit der viel zitierten Cloud zu tun? Sollte man das Thema dort erkunden, wo die Datenwolken als Elektronen in Chips und Kabeln kondensieren?

Ortstermin in einem Gewerbegebiet in Frankfurt a. M., der Stadt mit dem größten Netzknotenpunkt der Welt. Doppelte Zäune schützen das Gebäude, Kameras wachen, Rüttelsensoren, Bodendetektoren und Bewegungsmelder, Beton, Panzerglas, Sicherheitsschleusen, Ausweiskontrollen. Selbst ein Panzer würde in dem bewusst weich gehaltenen Grünstreifen vor dem Gebäude versinken. Alle Stromkreisläufe sind zweifach vorhanden, bei Netzausfall springt ein Dieselgenerator ein. Dennoch verrät kein Türschild, wo man sich befindet: in einem von Siemens errichteten Rechenzentrum von T-Systems.

Hier schlägt eines der vielen dunklen Herzen von Big Data, das in ungefähr zehn Kilometer Entfernung noch einen Zwilling hat: So wie üblich bei wichtigen Informationen, werden sie alle in einem zweiten Zentrum gespiegelt. Hier werden Daten, die die Welt bedeuten, angeliefert, bewacht, verpackt, verschickt.

Keine Lampe leuchtet, wenn die 3000 Server unter dem Getöse der Kühlanlagen ihre Arbeit verrichten, denn Menschen braucht man hier nicht. Nur 15 Mitarbeiter bewachen und warten die Einrichtung. Doch hat es einen Grund, dass das Rechenzentrum so viel Energie aus dem Stromnetz saugt wie 7500 Einfamilienhäuser. ""Auch in den Fabriken des 21. Jahrhunderts rauchen die Schonsteine"", erklärt Uwe Bartmann, der bei Siemens für Gebäudetechnik zuständig ist. Rechenzentren verbrauchen bereits an die zwei Prozent der auf der Welt produzierten Energie.
Im laufenden Jahr werden so viele Informationen neu abgespeichert wie in den letzten 30 000 Jahren zuvor

So melden bereits die Stromzähler, was Big Data bedeutet: Die Fähigkeit moderner Hochleistungsrechner, mit neuartiger Software und unter Einsatz von sehr viel Energie unvorstellbare Datenmengen unterschiedlichster Herkunft und Formate zu analysieren, und das mit größter Geschwindigkeit. Im laufenden Jahr werden ebenso viele Informationen gespeichert, wie sie die Menschheit gesammelt hat, seitdem ein Steinzeitkünstler vor 30 000 Jahren Nashörner und Hyänen an die Wände der südfranzösischen Chauvet-Höhle malte. Allein Google speichert jeden Tag tausend Mal so viele Daten, wie alle Werke der US-Kongressbibliothek enthalten, darunter 31 Millionen Bücher. Bis Ende 2015 wird der planetare Datenbestand auf 4,1 Zettabyte (ZB) anwachsen. Ein Zettabyte entspricht der Informationsmenge, die in einer Milliarde Festplatten enthalten ist, wie sie derzeit in Home-PCs üblich sind. In Planung sind Speicher für das Yottabyte-Zeitalter (1 YB = 1000 ZB).

Ein Ende dieser Entwicklung ist nicht abzusehen, denn es geht nicht mehr vornehmlich um gezielt erhobene Daten - etwa aus Umfragen oder Statistiken - sondern um Meta-Daten, die einfach so anfallen: um Datenabgase sozusagen. Das können die Positionsdaten oder die Bewegungen von Handy und Navi sein, Muster im Stromverbrauch, Klickverhalten am Bildschirm, Online-Einkäufe.

Diese Datenflut steigt beständig, da im kommenden Internet der Dinge immer mehr verlinkte Sensoren und Mikrochips verbaut werden. Bald wird jeder Frachtcontainer einen Sender besitzen, Kühlschränke werden selbständig Bier ordern. Über billige RFID-Chips ließe sich theoretisch die Position jedes beliebigen Gegenstandes verfolgen. Alle Arten von sensorbasierten Mess-Netzwerken - etwa für Klima oder Verkehr - werden zunehmend integriert. Bereits jetzt nutzen mehr Maschinen als Menschen das Internet, in zehn Jahren sollen es 150 Milliarden Sensoren und Gegenstände sein. Der Philosoph Alexander Pschera spricht schon von einem Internet der Tiere: Seitdem Zoologen sogar Schmetterlinge und Libellen mit Mini-Peilsendern ausrüsten, ließen sich ihre Wege ja auch an allen Bildschirmen verfolgen und - so hofft Pschera - eine neue Verbundenheit mit der Natur stiften.

Die Welt wird datafiziert.

Was kommt, ist das Netz für alles. In Konsequenz stünde am Ende eine digitale, gläserne Kopie der Welt, die sich mit Computern analysieren ließe.

Die Vorteile liegen auf der Hand: Wenn jede Ampel den Verkehrsfluss, jedes Auto ständig Position und Geschwindigkeit meldet, kann die Stadt die Verkehrsflüsse optimal lenken - das spart Benzin und senkt die Abgasbelastung. Literaturwissenschaftler können in Sekundenschnelle die Karriere eines Begriffes in den Büchern der Welt nachverfolgen. Bereits im Aufbau sind die Fabriken der sogenannten Industrie 4.0, in der cyberphysische Systeme die Digitalisierung der Produktion vorantreiben: In allen Fertigungsstufen melden Sensoren den Stand der Dinge. Mithilfe von Big Data werden Logistik-Unternehmen rund um den Globus just in time liefern, Werbung kann immer weiter personalisiert werden. Die potenziellen Produktivitätsgewinne sind gewaltig.

Viel nacherzählt wurde eine Anekdote, die der Wirtschaftsredakteur Charles Duhigg in der New York Times geschildert hat: Ein erboster Vater stürmte in einen Supermarkt in Minnesota. ""Meine Tochter bekam das hier mit der Post!"", rief er. ""Sie geht noch zur Schule, und Sie schicken ihr Gutscheine für Babysachen und Wiegen? Wollen Sie sie anstiften, ein Kind zu bekommen?"" Wenig später bestätigte sich die Schwangerschaft. Den Statistikern des Unternehmens war aufgefallen, welches Einkaufsverhalten auf eine Schwangerschaft deutet: Demnach kaufen Frauen im dritten Monat parfümfreie Lotion und einige Wochen später Nahrungsergänzungsmittel wie Magnesium oder Kalzium.

Die Geschichte zeigt, wie Big-Data-Analysen funktionieren. Weil man über so viele Daten verfügt, kann man Korrelationen finden, ohne Überlegungen über die zugrunde liegende Kausalität anstellen zu müssen. Viktor Mayer-Schönberger vom Oxford Internet Institute berichtet zahlreiche Beispiele: So stellten Statistiker in den USA fest, dass orange lackierte Autos nur halb so viele Defekte haben wie der Durchschnitt. Über die Gründe kann man spekulieren, aber dem Käufer hilft diese Einsicht. Ein von IBM und der University of Ontario entwickeltes Überwachungssystem zeichnet bei Frühgeborenen 16 Biosignale auf - von der Atemfrequenz bis zum Blutsauerstoffgehalt - circa 1260 Datensätze pro Sekunde. Mit der Analyse dieses Materials gelingt es einer neuen Software, eine sich anbahnende Infektion bis zu 24 Stunden vor dem Ausbruch anzuzeigen, ohne dass man die Ursachen kennt. Die Ärzte können dennoch eine Therapie einleiten. ""Das ist ein grundlegend neuer Modus der Erkenntnisgewinnung"", sagt Mayer-Schönberger. Dieser Modus kann die Wissenschaften voranbringen, aber auch zur Bedrohung werden.
Mit Metadaten lassen sich Schwangerschaften ausspähen und vermutlich auch Seitensprünge

Beispiel Privatsphäre. Hier hinkt das Recht der Entwicklung hinterher, weil es primär auf den Schutz heikler Daten ausgerichtet ist. Big Data bedeutet aber: heikle Ergebnisse aus der Verarbeitung banaler Daten. Wenn etwa Strafverfolger einen an das Internet angeschlossenen Stromzähler ausspähen und einen bestimmten Lampen-Schaltrhythmus registrieren, ist das ein Hinweis auf illegalen Cannabis-Anbau. So wie die Supermarkt-Statistiker die Schwangerschaften ausspähen, lässt sich mit Metadaten wohl auch der Fremdgeher identifizieren.

Selbst Anonymität schützt nicht, wie vor Kurzem ein Team um den Mathematiker Yves-Alexandre de Montjoye vom Massachusetts Institute of Technology (MIT) in Science berichtete: Den Forschern gelang es, 90 Prozent von 1,1 Millionen Menschen anhand von nur vier Kreditkartenkäufen auf einer anonymisierten Liste zu identifizieren. Ihnen lagen lediglich minimale Metadaten vor: die ausgegebene Summe, die Art des Geschäfts oder der Einrichtung - etwa Lebensmittelladen, Restaurant oder Sportstudio.

Man mag sich nicht vorstellen, was Statistiker hinbekämen, wenn sie alle über Facebook und Twitter geteilten Informationen analysierten. Dabei verschärft sich das Problem beständig. So deutet sich an, dass leistungsstarke Mini-Kameras bald ähnlich verbreitet sein werden wie Handys - sie werden mit Drohnen mitfliegen, den öffentlichen Raum überwachen, auf den Schultern von Polizisten und den Helmen jener Menschen installiert sein, die all ihre sportlichen Leistungen dokumentieren. Dann wird die Fahndung per Knopfdruck möglich werden. Die ""Deep Face""-Software von Facebook erkennt Gesichter bereits mit 97,35 Prozent Treffsicherheit - ähnlich gut wie der Mensch. George Orwells Big Brother ist im Vergleich ein kurzsichtiger Datenzwerg.

Solche Aussichten mögen jene Leute nicht schrecken, die keine Cannabis-Beete im Keller haben, wenig unanständige Dinge tun und auch sonst nichts zu verbergen haben. Doch auch Vollblut-Exhibitionisten sollten sich Sorgen machen. Schließlich dienen Big-Data-Analysen nicht nur der Suche nach Informationen, Kunden oder sonstigen Nadeln im globalen Heuhaufen. ""In der öffentlichen Diskussion wird völlig unterschätzt, dass Big Data auch der prädiktiven Analyse dient"", warnt Thomas Dapp, der als Senior Economist bei der Deutschen Bank die Digitalisierung beobachtet. ""Es sollte zu denken geben, dass deutsche Landeskriminalämter bereits Software für das Predictive Policing testen."" Es geht um Programme, die voraussehen sollen, wo es wohl in Zukunft Straftaten geben wird.

Solche Methoden halten derzeit in vielen Lebensbereichen Einzug. Big Data ermöglicht die Berechnung von vielen heiklen Korrelationen - etwa von Versicherungs- und Gesundheitsrisiken, Kreditwürdigkeit, beruflichen Aussichten. Womöglich muss man bald bei Abschluss einer Haftpflicht für das Auto zustimmen, dass es alle Daten über das Fahrverhalten übermittelt: In Verbindung mit GPS und Geoinformationssystemen ließe sich so jede Tempoüberschreitung, jedes riskante Manöver erkennen - und ein dem zu erwartenden Unfallrisiko entsprechender Tarif festsetzen. Manche Versicherungsnehmer würden dann billig fahren, andere würden gar keine Police mehr bekommen. ""Es wäre das Ende des gemeinsamen Risikopools und des Versicherungsprinzips überhaupt"", sagt Dapp.

Das ist eine bedenkliche Entwicklung, weil die Kausalitäten bei komplexeren Prognosen gar nicht mehr nachvollzogen werden können, die zugrunde liegenden Algorithmen sind meist unbekannt oder unverstanden: ""Ich weiß nicht, warum"", wird der Bankberater sagen, ""aber der Computer gewährt Ihnen den Kredit nicht."" Dabei sind auch Big-Data-Analysen nicht unfehlbar. Das Vorzeigeprojekt ""Google Flu"" etwa, das in typischer Big-Data-Methodik wahllos durch Korrelation beliebiger Suchanfragen und realer Infektionen Grippe-Epidemien voraussagen wollte, glänzte erst und fiel dann durch schwere Fehlprognosen auf. Ähnliches wird im individuellen Bereich gelten. Es wird zu falsch positiven und falsch negativen Vorhersagen kommen. Den Versicherungen und Unternehmen kann das egal sein, wenn die statistische Treffergenauigkeit nur groß genug ist, damit sich das rechnet. Aber selbst wenn sie bei 90 Prozent liegt: Was tun jene zehn Prozent, die falsch beurteilt wurden? Ähnliche Probleme wird es bei Algorithmen geben, die akademische Fähigkeiten kalkulieren oder auch das kriminelle Potenzial.

Und noch ein Trend könnte die Welt erschüttern. Die lange Zeit vergessene Forschung zur künstlichen Intelligenz hat in den letzten Jahren, so der Zürcher Soziophysiker Dirk Helbing, ""atemberaubende Fortschritte"" gemacht. Selbstlernende Software durchsiebt bereits die Big-Data-Ozeane und bringt sich Dinge bei, die man früher nur Menschen zutraute.
""Es droht die Automatisierung der Gesellschaft, eine völlig neue Art von Armut.""

In der Medizin wird eine Bildverarbeitungssoftware Biopsien präziser und schneller analysieren als ein Pathologe, Programme werden die Arbeit von Steuerberatern, Buchhaltern, Übersetzern, zum Teil vielleicht sogar von Juristen und Journalisten übernehmen. Michael Osborne und Carl Benedikt Frey aus Oxford kamen 2013 in einer Studie zu dem Schluss, dass die Computer in den nächsten zwei Jahrzehnten wohl 47 Prozent von 702 Berufen beseitigen werden. ""Es droht die Automatisierung der Gesellschaft"", warnt Helbing, ""eine völlig neue Art von Armut."" Nicht alle Forscher teilen seinen Pessimismus. Thomas Dapp von der Deutschen Bank etwa vertraut darauf, ""dass nach einer Zeit der Transformation mit Big Data auch neue Berufe entstehen werden"". Vielleicht werde es spezielle Algorithmiker geben, die all die neuen Prozesse begleiten und erklären, goldene Zeiten dürften für Datenwissenschaftler anbrechen. Viktor Mayer-Schönberger von der Oxford-Universität sagt: ""Wir müssen verstehen, was uns zu Menschen macht und uns Vorteile verschafft. Es ist das Intuitive, Kreative, auch das Irrationale, das uns den radikalen Bruch im Denken erlaubt."" Aber lässt sich ausschließen, dass auch die Computer eines Tages intuitiv, kreativ und irrational werden? ""Ausschließen kann ich das nicht. Ich kann nur beten, dass das nicht passiert.""

Einig sind sich alle Experten, dass es höchste Zeit ist, die großen gesellschaftlichen Big-Data-Fragen anzugehen. Die Privatsphäre muss gerettet, der Umgang mit Daten reguliert werden. Unternehmen müssen sich der Digitalisierung stellen, wenn sie nicht weggefegt werden sollen.

Das ist der Punkt, an dem Dirk Helbing seine Vision skizziert, ""denn die digitale Welt bietet auch Riesenchancen"". Er schlägt vor, die Staaten Europas sollten Milliarden in das kommende Internet der Dinge investieren, um ein neues digitales Nervennetz aufzubauen: das ""Nervousnet"".

Dank der Milliarden Sensoren und Rückmeldungen seiner Nutzer ließen sich mit dem Nervousnet alle externen Effekte des Handelns von Menschen und gesellschaftlichen Akteuren in Echtzeit messen - und darauf reagieren. So - glaubt Helbing - ließen sich sogar die Finanzkrise in den Griff kriegen und Kriege verhindern. Man würde immer eine Parklücke finden, eine günstige Unterkunft oder Mitspieler in neuen ""cybersozialen Erlebniswelten"". Es wäre ein Informationsökosystem, in dessen Nischen Unternehmen neue Produkte und Dienste anbieten könnten - und sei es nur eine schicke App für die Parkplatzsuche. ""Das wäre der Katalysator für eine digitale Gesellschaft und ein möglicher Jobmotor für Europa"", sagt Helbing. Das Nervousnet ist eine sehr große Idee, über deren Umsetzbarkeit man sicher streiten kann. Aber immerhin: Es zeigt eine Richtung an, wie Big Data helfen könnte, die ohnehin wachsende Komplexität der Welt zu bewältigen.

Niemand weiß, wie die Welt aussehen wird, wenn das total überwachte Baby Roy einmal erwachsen sein wird. Gut möglich, dass dann alle Menschen das betreiben werden, was bislang nur exzentrische Lifelogger tun, die sich bereits jetzt eine ständig knipsende Kamera um den Hals hängen und ihren Blutdruck dem Internet melden. Vielleicht wird Baby Roy dann eines Tages einen Grabstein bekommen, wie ihn soeben ein Künstler in einer Ausstellung in Dublin als Prototyp vorgestellt hat. Dieser erinnert an den fiktiven Lifelogger @kurtmarkoneill (1999-2064) und dokumentiert den Score seines Lebens:";https://www.sueddeutsche.de/wissen/informationstechnologie-datafiziert-1.2450580;sz.de;Christian Weber
22.03.2015;In 15 Minuten zum Kredit;"Was machen Sie eigentlich?

Ich bin beim Hamburger Start-up Kreditech zuständig für die Erschließlung neuer Märkte. Kreditech hat eine Technologie entwickelt, mit deren Hilfe Kreditentscheidungen erleichtert werden. Ein Algorithmus errechnet anhand von 20 000 Datenpunkten die Bonität eines Antragstellers.

Angenommen Christian Miele braucht einen Kredit, wo im Internet fangen Sie mit der Suche an?

Ich frage Basisdaten ab.

Welche?

Adresse, Beruf ...

... das macht auch jede Bank!

Kreditech bezieht auch Usability Daten mit ein sowie weitere technische Details, öffentliche Daten aus dem Web spielen eine Rolle und - wenn der Kunde zustimmt - auch Social-Media-Daten.

Welche Informationen liefert Ihr Facebook-Account über Ihre Kreditwürdigkeit?

Facebook ist nur einer von vielen Datenpunkten und in vielen unserer bislang neun Märkte irrelevant. Die Vielfalt der Daten und deren Korrelation macht die Qualität der Bewertung, des Scorings, aus.

Schauen Sie sich denn beispielsweise auch auf Facebook oder Instagram an, welche Marken der Antragsteller trägt?

Nein. Aber das wäre vielleicht eine tolle Idee. Könnte ich dem Data-Science-Team vorschlagen, ob auf den Fotos ein Logo von Adidas oder Nike zu erkennen ist.

Könnten Sie, aber Markenkleidung lässt mindestens zwei sehr widersprüchliche Schlüsse zu: Der Antragsteller kann sich die Klamotte leisten und ist sehr solvent - oder er gibt zu viel für Marken aus, aber eigentlich gibt sein Einkommen das nicht her!

Genau. In unserem Algorithmus kann sich so ein Merkmal positiv und negativ auswirken. Abgesehen davon könnte es auch gefälschte Markenware sein, deshalb glaube ich nicht, dass es aussagekräftig sein wird. Die Idee klingt trotzdem spannend.

Wenn sich jemand auf Facebook sehr rüde über andere äußert, ist das ein K.O.-Kriterium?

Ein einzelnes Kriterium entscheidet nicht über die Bonität eines Antragstellers. Aus Social Media wird, wenn der Kunde einwilligt, ein Teil der Daten abgebildet. Ein paar Daten von 20 000.

Was ist mit Rechtschreibfehlern?

Die können sich, wie der Name, positiv und negativ auswirken - es kommt immer auf die Korrelation und damit das Muster insgesamt an.

Wenn Sie auf Linkedin oder Xing feststellen, dass eine Person alle neun Monate den Job wechselt, lehnen Sie dann den Antrag ab?

Auch das ist nur ein einzelner Datenpunkt und hat für sich betrachtet keine Auswirkung. Es hängt von der Kombination einzelner Daten ab und der Gesamtbewertung.

Die klassischen Banken arbeiten, zum Beispiel, mit Schufa-Daten. Ist ein negativer Eintrag auch bei Kreditech ein K.o.-Kriterium?

Wir sind in Deutschland nicht aktiv, daher spielen Daten der Schufa für uns keine Rolle. Wir speichern auch keine historischen Daten. Da sich unser Algorithmus kontinuierlich weiterentwickelt, sind die jetzt über 20 000 Datenpunkte in fünf Minuten schon wieder andere. Menschen können sich ändern. Es kann doch sein, dass die Kreditwürdigkeit nach vier, fünf Jahren eine völlig andere geworden ist, zum Beispiel weil ich eine Ausbildung gemacht habe, studiert habe oder umgezogen bin.

Fließen in Ihre Datenerfassung auch Hobbys ein?

Wenn der Algorithmus errechnet, dass Fußballfans solvente Kreditnehmer sind, dann würden wir versuchen, möglichst viele dieser Datenpunkte zu evaluieren. Wir versuchen Muster herauszufinden: Gibt es einen Zusammenhang zwischen der Postleitzahl und der Bonität? Gibt es einen Zusammenhang zwischen Alter und Zahlungsmoral? Tilgen Frauen Kredite zuverlässiger als Männer? Je mehr Zusammenhänge unsere Technologie erkennt, desto besser werden die Kreditentscheidungen.

Ihre Kernkompetenz ist der gläserne Schuldner. Verleihen Sie deshalb kein Geld in Deutschland, weil der Datenschutz Ihrem Geschäft im Wege steht?

Unsere Kunden stimmen jeder unserer Datenabfragen explizit zu. Auch bei klassischen Banken gilt, je mehr Informationen umso ""preiswerter"" der Kredit. Der Unterschied ist: Unsere Kernkompetenz ist die Technologie. Sie löst ein Problem: Vier Milliarden Menschen weltweit haben keinen Kreditscore und damit keinen Zugang zum Finanzsystem. In Deutschland haben wir ein derartiges Problem nicht. Deshalb konzentrieren wir uns auf andere Märkte. In allen Ländern, in denen wir aktiv, sind befolgen wir alle Datenschutzbestimmungen.

Oder die Vorschriften laxer und Sie zum Beispiel keine Banklizenz brauchen!

Wir halten uns streng an alle Vorschriften. So haben wir in Australien zum Beispiel eine Kreditlizenz.

Geld bei Kreditech zu leihen, ist ziemlich teuer! Wer sich, zum Beispiel, in Polen über Ihre Homepage 3000 Zloty, das sind gut 700 Euro, für einen Monat leiht, muss 300 Zloty Zinsen zahlen. Das entspricht einem Zinssatz von 19 Prozent pro Monat, aufs Jahr hochgerechnet sind das effektiv rund 360 Prozent. Das ist doch Wucher!

Da es sich um Mikrokredite handelt, beträgt die Laufzeit 7 bis 30 Tage. Ergo: Wer sich in Polen beispielsweise 100 Euro leiht, zahlt am Ende des Monats 120 Euro zurück. Die Kurzfristigkeit des Kreditangebotes steht im Vordergrund für den Kunden. Die unterstellte Kreditlaufzeit von einem Jahr ist nicht zielführend. Wenn Sie für ein paar Tage ein Hotelzimmer brauchen, vergleichen Sie die Preise ja auch nicht mit der Monatsmiete. In Polen werden zum Beispiel immer mehr Ratenkredite von unseren Kunden nachgefragt, wo wir individuelle Zinssätze anbieten, die deutlich unter dem Marktdurchschnitt liegen.

Wie viel Personen haben im Februar Ihre Homepage besucht?

Im Februar hatten wir insgesamt 2,5 Millionen Visits global betrachtet.

Wie viele echte Kreditanfragen hatten Sie?

Wir haben seit der Gründung 2012 bisher rund zwei Millionen Kreditanträge bearbeitet.

Wie viele davon hatten binnen 24 Stunden eine Zusage?

Wir haben eine durchschnittliche Annahmequote von 15 Prozent.

Wie lange dauert es vom Antrag bis zur Auszahlung?

Im Durchschnitt 15 Minuten, dann ist das Geld auf dem Konto.

Das ist doch sicher nur ein Spitzenwert?

Der Spitzenwert liegt bei fünf Minuten. Je nach Land kann es aber auch länger dauern, weil die lokale Bankeninfrastruktur noch nicht so weit entwickelt ist.

Wieso brauchen Ihre Kunden überhaupt noch so etwas Altmodisches wie ein Konto, es gibt doch digitale Geldbörsen!

Richtig. Wir arbeiten hier an einem eigenen Produkt, weil genau das auch unsere Kunden nachfragen.

Wie hoch war die kleinste und die größte Kreditsumme?

Die Kreditsummen liegen zwischen 80 US-Dollar für einen Mikrokredit in Mexiko und 4000 Dollar für einen Ratenkredit in Polen.

Wie war die durchschnittliche Laufzeit?

Die liegt zwischen 7 und 30 Tagen für Mikrokredite und bis zu zwölf Monaten für Ratenkredite.

Wie hoch war Ihre Ausleihungen Ende Februar 2015?

Die annualisierte Darlehensvergabe für das Jahr 2015, liegt bei rund 130 Millionen US-Dollar.

Wie hoch war die Ausfallquote?

Unsere Ausfallrate liegt bei weniger als sieben Prozent in etablierten Märkten und global bei rund zehn Prozent.

Wer sind denn Ihre Konkurrenten, Firmen wie Wonga?

Wir verfolgen unseren Weg und schauen nicht so sehr auf andere. Kreditech ist schneller und unkomplizierter als Banken und andere Online-Kreditanbieter. Unser Vorteil ist unsere eigene Technologie. Andere müssen diese Daten meist von Dritt-Anbietern, also Kreditbüro-Daten, einkaufen.

Ihr Image im Netz ist fast so schlecht wie das von Wonga. Selbst einige der Manager, die vergangenes Jahr rausgeworfen wurden, hatten moralische Zweifel an Ihrem Geschäftsmodell.

Entscheidend für uns ist der große Zuspruch am Markt und die vielen zufriedenen Kunden, mit denen wir langfristig zusammenarbeiten.

Wie sieht die Finanzwelt 2020 aus? Gibt es noch eine Deutsche Bank, da haben Sie mal Praktikum gemacht?

Die wird es schon noch geben, aber in einer anderen Form.

In welcher?

Eine mit weniger Filialen und mehr digitalen Produkten. Banking findet 2020 in der Hosentasche statt.";https://www.sueddeutsche.de/wirtschaft/was-machen-sie-eigentlich-christian-miele-in-15-minuten-zum-kredit-1.2400728;sz.de;Elisabeth Dostert
01.01.2015;Maschinen, die erwachsen spielen;"Wer hat Angst vor der künstlichen Intelligenz? Wie so oft bei Zukunftsdebatten geht es auch bei dieser Frage weniger um Technik als um die Menschen, die sich über sich selbst Gedanken machen. Seit Maschinen lernen können, seit Computer also ohne die Hilfe von Programmierern ihre Fähigkeiten verbessern, hat diese Angst ganz neue Dimensionen bekommen.

In Literatur und Film gibt es sie schon lange. Das begann mit dem Mythos des Golem im Mittelalter und ging über Mary Shelleys ""Frankenstein"" und Stanley Kubricks ""2001"" bis zur ""Matrix""-Trilogie und der jüngsten Welle der Science-Fiction-Filme, die derzeit in die Kinos drängt.

Jetzt haben sich allerdings zwei gewichtige Stimmen aus Wissenschaft und Technik zum Thema gemeldet. Der Unternehmer und Elektroauto-Pionier Elon Musk warnte vor ""beängstigenden Folgen"". Das ist beunruhigend, weil er gerade selbst viel Geld in die Entwicklung künstlicher Intelligenz investiert. Und der Physiker Stephen Hawking prophezeite neulich wieder, dass die künstliche Intelligenz das Ende der Menschheit bedeuten könne. Das ist noch beunruhigender, weil man Stephen Hawking zutraut, dass er etwas vom Lauf der Wissenschaftsgeschichte versteht.

Aber es gibt auch nach wie vor Euphoriker wie den Futuristen Ray Kurzweil. Sie sehnen nichts sehnlicher herbei als einen Erweckungsmoment, der die Maschinen mit einem dem Menschen ebenbürtigen Geist beseelt. Die große Hoffnung dabei ist, dass sie als rationale Wesen den emotional verwirrten Menschen überlegen sind. Einen Begriff für diesen Erweckungsmoment gibt es auch schon - die Singularity.

Nur gut, dass sich jetzt auch Stimmen der Vernunft melden. Jaron Lanier hat diese neue Runde der Debatte angestoßen, jener amerikanische Forscher und Theoretiker, der im vergangenen Jahr den Friedenspreis des deutschen Buchhandels bekam, weil er sich als Internetpionier so kritisch mit dem Internet auseinandergesetzt hat. Im Debattenforum edge.org ist von ihm ein Vortrag mit dem Titel ""The Myth of AI"" (""Der Mythos der künstlichen Intelligenz"") erschienen.

""Es gibt da eine dominierende Subkultur, die eine der reichsten, produktivsten und einflussreichsten Subkulturen der Technikwelt ist"", führt Lanier aus. ""Sie unterstützt die Idee, dass es eine Entsprechung zwischen bestimmten Algorithmen und Menschen gibt, und einen historischen Determinisimus, dass es unvermeidlich sei, dass wir Computer bauen werden, die klüger und besser sind als wir, und die deswegen die Macht von uns übernehmen werden."" Für Lanier ist diese Mythologie lächerlich. Die wichtigste Funktion künstlicher Intelligenz sei derzeit die Ordnung jener Datenfluten, die als Big Data ihre eigene mythische Größe geworden sind.
Kein Roboter wird sich seiner selbst bewusst werden, kein Terminator wird uns durch die Straßen jagen

Hinter dem, was er da lakonisch als Inselbegabung der Computer abtut, steckt allerdings ein großes Unbehagen. Die Erkenntnis, dass es algorithmische Supermächte wie die NSA, Google oder Facebook gibt, die mit den Datenfluten das Leben der Menschen quantifizieren und so unter ihre Kontrolle bringen, hat in den vergangenen zwei Jahren weltweit zu einem Kulturpessimismus geführt, der die digitale Aufbruchsstimmung massiv gebremst hat. Die Ahnung, dass diese Kontrolle nicht von Menschen, sondern von künstlichen Intelligenzen herrührt, scheint die Erfüllung all der Zukunftsbilder zu sein, von denen man sich eigentlich sicher war, dass sie im Kino und im Roman gut aufgehoben sind.

Was sich hinter der gegenwärtigen Debatte um die künstliche Intelligenz eigentlich verbirgt, ist also eine Machtfrage. Daran ändert weder ihr wissenschaftlich-technischer Kern etwas noch die theologische Überhöhung mit dem Erweckungsmoment. Und dieser Moment wird nicht kommen. Hinter dem roten Dioden-Auge des Bordcomputers Hal 9000 aus Stanley Kubricks ""2001"" wird nie ein Geist erwachen, der sich seiner selbst bewusst werden wird. Kein Terminator wird die Menschen jagen, kein Cyborg ihnen ihren Platz in der Welt streitig machen.

Das sagt nicht nur Jaron Lanier, darin bestätigen ihn nun auch sehr viele kluge Menschen, die wirklich etwas von diesen Fragen verstehen. Etwa Rodney Brooks, emeritierter Professor für Robotik am Massachusetts Institute of Technology . ""Künstliche Intelligenz ist keine Bedrohung, sondern ein Werkzeug"", schreibt er. Den grundlegenden Irrtum sieht er darin, dass nur wenige Experten der künstlichen Intelligenz die Debatten führten. Daher habe kaum jemand im Blick, wie weit die Entwicklung wirklich fortgeschritten sei, und wie komplex die Konstruktion einer Intelligenz mit eigenem Willen und eigenen Empfindungen sei. ""Wir sollten uns keine Sorgen machen, dass wir in den nächsten paar Hundert Jahren eine bösartige Intelligenz schaffen"", schreibt er. Der Wissenschaftshistoriker George Dyson wiederum sieht einen grundlegenden Irrtum in der Debatte: ""Das Gehirn, egal ob eines Menschen oder einer Fruchtfliege, ist kein digitaler Computer. Intelligenz ist kein Algorithmus.""

Nein, die eigentliche Bedrohung sehen die, die sich wirklich auskennen, an einem ganz anderen Punkt. Der Harvard-Ökonom Sendhil Mullainathan schreibt: ""Wir sollten nicht vor intelligenten Maschinen Angst haben, sondern vor Maschinen, die Entscheidungen fällen, für die sie nicht die angemessene Intelligenz haben."" Oder wie es der Gründer der ""X-Prize""-Wissenschaftspreise, Peter Diamandis, ausdrückt: ""Ich habe keine Angst vor einer erwachsenen künstlichen Intelligenz, sondern vor einem Entwicklungsstand, der dem von 3- bis 5-Jährigen entspricht. Ich habe dreijährige Zwillinge. Die haben keine Ahnung, wenn sie beim Spielen etwas kaputt machen.""

Die Debatte hat gerade erst begonnen. Was sie von all den Ängsten, Hoffnungen und Diskursen der Vergangenheit unterscheidet, ist aber weniger die philosophische oder gar theologische Folge technischer Entwicklungen. Es ist die Tatsache, dass dem technischen Fortschritt des 20. Jahrhunderts nun die praktischen Umsetzungen folgen. Der Kontrollverlust über lernfähige Maschinen wird marginal bleiben. Die Bedrohung durch Institutionen, die sie einsetzen, ist dagegen jetzt schon Realität. Immerhin - das Magazin für Wehrtechnik Defense One meldet gerade, dass das US-Militär künstliche Intelligenz zum Schlüsselthema für 2015 erklärt hat.";https://www.sueddeutsche.de/kultur/zukunft-maschinen-die-erwachsen-spielen-1.2287727;sz.de;Andrian Kreye
16.09.2014;Freie Daten für freie Forscher;"Fernsehen ist Leistungssport für das Gehirn. Von wegen passiv - solange Menschen auf dem Sofa vor der Glotze hängen, laufen im Kopf komplexeste Vorgänge ab. Das Hirn muss die Bilderflut aufnehmen, die akustischen Reize verarbeiten und alles zu einem sinnvollen Plot verknüpfen. Sprachverstehen, Gesichtserkennung, Erinnerungsvermögen, Emotionsverarbeitung sind nur ein Teil der neuronalen Prozesse, die für das Anschauen eines Filmes nötig sind. Fast so komplex also wie das echte Leben - und damit für Hirnforscher höchst interessant.

Mit dem Konsum von Hollywood-Produkte versuchen die Wissenschaftler, grundsätzliche Fragen zu klären: Wann ticken alle Menschen gleich? Und welche Abläufe im Hirn sind dagegen individuell verschieden? Das treibt eine Gruppe von Wissenschaftlern unter Leitung des Magdeburger Psychologen Michael Hanke um. Sie schieben ihre Studienteilnehmer jeweils für zwei Stunden in die Röhre eines Magnetresonanztomografen. Diese Maschine kann in bunten Bildern sichtbar machen, welche Hirnbereiche wann besonders aktiv sind. Meist horten Forscher solche Daten wie einen Schatz. Hanke aber hat sich jetzt entschieden, alles mit Kollegen zu teilen. Das hat mit seinem Ansatz zu tun, der gut zum neuen Konzept einer offenen, kooperativen Wissenschaft passt.

Damit die Aufnahmen vergleichbar sind, bekommen alle Teilnehmer das gleiche Programm: Den Filmklassiker ""Forrest Gump"" - allerdings nur die Tonspur. ""Kollegen in den USA hatten einen ähnlichen Versuchsansatz"", erklärt Michael Hanke diese Wahl des Stimulus. Allerdings zeigten sie tatsächlich einen Film mit Bildern. ""Wir wollten nun in einem ersten Schritt herausfinden, welche von den gemessenen Aktivierungsmustern nur von den auditiven Prozessen ausgelöst werden.""

Das sogenannte Hyperalignment, also der Vergleich unterschiedlicher Gehirne bei der Verarbeitung der exakt gleichen Reize, erlaubt es sogar, einfache Gedanken zu lesen. Ein Wissenschaftler kann etwa einzig anhand der Aktivierungsmuster eines ihm unbekannten Gehirns erkennen, welches Lied dem Probanden während des Hirnscans vorgespielt wurde. Um das Musikstück in den Hirnbildern erkennen zu können, braucht der Forscher allerdings Vergleichswerte aus anderen Gehirnen. Für das Beispiel wurden Aufnahmen von 19 Probanden ausgewertet, die dasselbe Lied zu hören bekommen hatten. So kalibriert, ließ sich dann anhand der neuronalen Aktivierungsmuster sagen, welche Klänge im Moment der Aufnahme durch die Windungen des 20. Hirns wanderten.

Das Beispiel zeigt, welch gigantische Datenmassen erhoben werden müssen, um brauchbare Aussagen über die Hirnfunktion einer Person machen zu können. Ein durchschnittlicher Popsong dauert drei Minuten. Der Film ""Forrest Gump"" läuft mehr als zwei Stunden - und die ganze Zeit zeichnet der Hirnscanner hochauflösende, dreidimensionale Bilder vom Gehirn eines Probanden auf. Diese Datenflut muss verwaltet und analysiert werden, mit der passenden Software bearbeitet und aufbereitet.

""Psychoinformatiker"" nennt sich Michael Hanke deshalb auch stolz, er sei ""der zweite weltweit"". Ein Psychologe also, der auch viel von Computern versteht. Doch selbst ein Multitalent stößt an Grenzen. ""Mir wurde klar, dass sich mit unseren Daten ganz andere Sachen machen lassen, von denen ich keine Ahnung habe"", sagt Hanke. So entschied er sich, die Daten anderen Forschern zur Verfügung zu stellen. So viel Offenheit ist ungewöhnlich in der Wissenschaft. Fachveröffentlichungen sind die Währung des akademischen Systems. Wer seine Daten oder Ergebnisse verrät, riskiert, dass andere Forscher ihm zuvorkommen. Oft artet Forschung in absurde Wettrennen um die Erstveröffentlichung aus. An nahezu jedem Forschungsthema arbeiten etliche Gruppen rund um die Welt, machen die gleichen oder sehr ähnliche Versuche, geben für die gleichen Erkenntnisse Geld aus. Doch nur die Schnellsten können ihren Aufsatz in einer hochrangigen Publikation unterbringen.

Forscher wie Hanke wollen deshalb mit dem alten System brechen. Sie setzen auf Kooperation statt Konkurrenz, auf offene Daten statt Geheimniskrämerei. ""Open Science"" wird diese Idee auch genannt.

Um seine Messungen für andere noch nützlicher zu machen, erhebt Hanke inzwischen sogar Daten, die er selbst gar nicht benötigt. ""Ich habe schnell gemerkt: Ich muss nur ein klein wenig mehr machen, um viele weitere Fragestellungen zuzulassen"", erklärt der Psychologe. Er stockte seine Versuche also auf eigene Kosten auf, zeichnete neben Hirnbildern auch den Herzrhythmus, die Atmung und die Bewegungen der Probanden im Scanner auf. 350 Gigabyte an Messdaten hat er bereits ins Netz gestellt, in monatelanger Arbeit gesammelt von hoch spezialisierten Wissenschaftlern, bezahlt mit Tausenden Euro öffentlicher Forschungsgelder.

Das war für Hanke ein wichtiger Punkt. Das Bundesministerium für Bildung und Forschung hatte ihm, wie vielen anderen Wissenschaftlern, staatliche Mittel gewährt. Das geschieht nicht, um den Ruf und die Berühmtheit eines einzelnen Wissenschaftlers zu mehren, sondern um allgemein nützliche Erkenntnisse zu gewinnen. Aus diesem Grund liegen die Daten der Magdeburger Arbeitsgruppe nun für alle zugänglich auf einem Server. Jeder kann sie mit kostenloser Software betrachten und analysieren - auch interessierte Laien, wenn sie wollen.

Doch mit seiner Offenheit geht Hanke auch ein Risiko ein: Andere Psychologen könnten die Daten mit den gleichen Fragestellungen analysieren wie er. Und schlimmstenfalls den Artikel zuerst veröffentlichen, den die Magdeburger geplant hatten. ""Tatsächlich muss sich der Datenersteller in so einem Fall auf die Fairness der anderen verlassen"", sagt Stefan Winkler-Nees von der Deutschen Forschungsgemeinschaft DFG. ""Doch wir arbeiten daran, Mechanismen für entsprechende Regelwerke umzusetzen."" Winkler-Nees leitet seit 2013 ein Förderprogramm zur Informationsinfrastruktur für Forschungsdaten. ""Open Science ist ein Begriff, der momentan sehr en vogue ist"", sagt er. Diese Entwicklung kommt den Zielen der DFG entgegen: Das Teilen von Daten zum gemeinsamen Erkenntnisgewinn, das klingt nach effizienter Forschungsarbeit.
Wie bei der Wikipedia kann die Masse der Nutzer die Qualitätskontrolle übernehmen

Man könnte es auch als Imagekampagne für Open Science bezeichnen, was Stefan Winkler-Nees und seine Kollegen betreiben. ""Wir versuchen, die verschiedenen Institute davon zu überzeugen, dass das Bereitstellen der Forschungsdaten an sich einen Wert hat."" Um das Prinzip Open Science weiter zu etablieren, hat die ""Allianz der deutschen Wissenschaftsorganisationen"" schon 2008 eine Schwerpunktinitiative zur ""Digitalen Information"" gestartet. Eines der Ziele: Den ""Mehrwert der Nachnutzung und der Verfügbarkeit von Forschungsdaten"" nachzuweisen.

Natürlich gibt es auch einige offene Fragen. Die Qualität der veröffentlichten Daten muss zum Beispiel gesichert und überprüfbar sein. Dafür gibt es verschiedene Lösungsansätze: Ähnlich wie beim Onlinelexikon Wikipedia kann beispielsweise die Masse der Nutzer eine kontrollierende Funktion übernehmen. Michael Hanke konnte so bereits seine Daten verbessern: ""Wir haben schon zu frühen Zeitpunkten zwei sehr gute Hinweise bekommen"", sagt er. Die Mitstreiter aus der Open Science-Gemeinde haben dem Psychologen also das Forscherleben leichter gemacht.

Auch die wissenschaftlichen Verlage reagieren bereits auf diese Entwicklung zu mehr Offenheit. Ähnlich wie beim ""Open Access"", einer Publikationsform, die es jedem Interessierten erlaubt, Fachartikel kostenfrei zu lesen, weil der Autor für die Veröffentlichung bezahlt, gibt es inzwischen Überlegungen, aus der Open-Science-Idee ein Geschäftsmodell zu stricken. Die Nature Publishing Group etwa, einer der führenden Wissenschaftsverlage, hat jüngst das Magazin Scientific Data gegründet. Es druckt keine Artikel mit Ergebnissen und Interpretationsteil, sondern sogenannte ""Data Descriptors"". Das sind bloße Bestandsaufnahmen der Daten und der angewandten Methoden, korrigiert von fachaffinen, qualitätsbewussten Redakteuren. Diese Artikel sind zitierbar. Mehr noch: Wer die bereitgestellten Daten weiter verwenden will, verpflichtet sich, auf den Ersteller zu verweisen. So kann auch der teilungswillige Forscher die Zahl seiner Zitierungen erhöhen - und damit seine Publicity in der Wissenschaftswelt.

Was anderswo mit seinen Hirnbildern gemacht wird, weiß Hanke nicht genau. Sicher kann er nur sagen, dass je eine Arbeitsgruppe aus Australien und England seine Ergebnisse nutzen. Derweil produziert er fleißig weiter: In der nächsten Phase seines Experiments werden die Probanden wieder mit ""Forrest Gump"" traktiert werden. Dieses Mal mit Bild. Das macht die Messungen aber noch komplexer. Welch ein Glück, dass die Magdeburger Psychologen nicht allein sind mit all ihren Daten.";https://www.sueddeutsche.de/wissen/open-science-freie-daten-fuer-freie-forscher-1.2126615;sz.de;Bernd Eberhart
01.08.2014;Leben und Sterben im Zeitraffer;"Während der Französischen Revolution floss das Blut in den Straßen von Paris. Unter den Toten waren viele Beamte und Politiker - wie es zu erwarten ist, wenn ein politisches System mit Gewalt umgekrempelt wird. Aber auch eine andere Berufsgruppe hatte in den Jahren nach 1789 in der französischen Hauptstadt überraschend viele Tote zu beklagen: die Architekten. Wer mit dem Bau von Gebäuden beschäftigt war, lebte offenbar gefährlich. Wer weiß, vielleicht pflegten die Architekten besonders enge Beziehungen zur Obrigkeit und qualifizierten sich so als Ziel politisch motivierter Gewalttaten; vielleicht kämpften auch einfach nur viele von ihnen auf den Barrikaden der Stadt.

Das Sterben der Pariser Architekten ist einer von vielen erstaunlichen Vorgängen in der Kulturgeschichte, die mit einem neuartigen Analysewerkzeug sichtbar und numerisch greifbar gemacht werden können. Eine auf Zigtausenden historischen Geburt- und Sterbedaten basierende Auswertung haben die Forscher um den Kunsthistoriker Maximilian Schich von der Universität Texas in Dallas im Fachmagazin Science veröffentlicht (Bd. 345, S. 558, 2014).

Für das Projekt haben die Wissenschaftler unter anderem den Geburts- und den Sterbeort von mehr als 150 000 bedeutenden Menschen erfasst - und das für einen Zeitraum von gut 2000 Jahren. Sie haben somit Analysewerkzeuge und Methoden auf kunst- und kulturhistorische Fragen angewendet, mit denen üblicherweise anderswo komplexe Systeme modelliert werden, etwa Protein-Interaktionen in der Biologie oder der Straßenverkehr in der Mobilitätsforschung. Die Visualisierungen dieser Daten sind beeindruckend. Bilder und Videos führen die kulturelle Mobilität der vergangenen Jahrhunderte in Europa und Nordamerika vor Augen.
Leuchtende Brücken nach Amerika

Sie zeigen, wo Künstler, Wissenschaftler oder eben Architekten einst von welchen Orten aus hinzogen. Der Ort ihres Todes ist dabei ein Indikator für ein kulturelles Zentrum jener Epoche, zumindest wenn weitere bekannte Menschen zu ähnlicher Zeit dorthin zogen und bis zum Lebensende blieben. Die Visualisierungen setzen mit Christi Geburt ein und zeigen, wie zunächst Rom und Athen als einsame Lichtpunkte inmitten kultureller Finsternis leuchten - bis weitere Zentren, Florenz zum Beispiel, aufscheinen, sich ein immer dichteres Netzwerk entspinnt, Brücken nach Amerika aufblinken, und schließlich Großbritannien und Mitteleuropa mit dem Beginn der Neuzeit in einem regelrechten Lichtgewitter explodieren. Einwände gegen den Wert dieser Analyse liegen auf der Hand: Dass Athen, Rom, Florenz, später Paris, London und so weiter Epizentren europäischer Kultur waren, ist nun wirklich keine frische Erkenntnis, das weiß doch jeder. Doch darum geht es nicht. Dass die Datenanalyse diese bekannten Umstände exakt nachzeichnet, sei ein Beleg für deren methodischen Wert, sagt Schich.

Es gehe grundsätzlich um die Digitalisierung der Geisteswissenschaften, darum, in diesen Disziplinen endlich quantitative Fragestellungen zu bearbeiten. Historiker etwa beschäftigen sich meist mit qualitativen Fragen, also mit einzelnen, spezifischen Vorgängen und Motiven. Vielleicht sind auch die Umstände des Architektensterbens im Paris der Französischen Revolution bereits bekannt. Mit den Analysewerkzeugen des Teams um Schich lassen sich aber unzählige Details zusammenfügen, die mutmaßlich auch von unzähligen Forschern bearbeitet werden. ""Wir fassen diese Menge an einzelnen Vorgängen zusammen und schauen, ob sich Trends ergeben"", sagt Schich. Ein Spezialist für Architektur während der Französischen Revolution könnte somit die quantitativen Daten nutzen, um an anderen Orten und in anderen Epochen nach ähnlichen Mustern zu suchen. Korrelierte das Schicksal der politischen Klasse auch anderswo mit dem der Bauberufe?
""Long Data"" als neues Werkzeug

Manche Trends, die Schich in den Daten aufspürte, scheinen der Intuition zu widersprechen. So ist die geografische Mobilität auch in der globalisierten Neuzeit nicht sonderlich stark gestiegen. Im 14. Jahrhundert starb die Hälfte der Menschen in Schichs Analyse weniger als 214 Kilometer von ihrem Geburtsort entfernt. Bis heute hat sich dieser Wert nur auf 382 Kilometer erhöht - erstaunlich angesichts der Distanzen, die heute zurückgelegt werden.

Verändern sich Geisteswissenschaften, indem sie Ansätze wie den des Teams um Schich anwenden? Womöglich wäre es fahrlässig, neue Analysemethoden für vorhandene Datenschätze zu ignorieren. ""Große Datenbestände erweitern die Forschungsfragen der Kunstgeschichte"", heißt es etwa in der Zürcher Erklärung zur digitalen Kunstgeschichte, die im Juni formuliert wurde. Und auch James Cuno, Präsident des Getty Trust, sprach sich kürzlich im Wall Street Journal dafür aus, dass Netzwerkanalysen und andere Methoden dringend stärker in die Kunstgeschichte eingebracht werden müssten.

Die Debatte ist auf jeden Fall schon um ein Schlagwort erweitert: Historiker werden künftig Long Data neben Big Data für ihre Disziplin reklamieren. Denn über Daten, die weit in die Geschichte zurückreichen, sollte das Fach doch verfügen.";https://www.sueddeutsche.de/wissen/datenvisualisierung-europas-leben-und-sterben-im-zeitraffer-1.2071567;sz.de;Sebastian Herrmann
25.06.2014;Schau mir in die Augen, Kunde;"Der Fernseher weiß, was wir wollen oder glaubt dies zumindest. Kaffee für müde Pendler auf dem Weg zur Arbeit, Windeln, wenn sich Mütter in der Schlange vor der Kasse ballen: An 450 Tankstellen der britischen Einzelhandelskette Tesco sollen bald Kameras Kunden nicht nur filmen, sondern auch deren Augen digital erfassen. Auf dieser Grundlage entscheidet das Programm Optimeyes, welche kurzen Werbespots ihnen auf einem Bildschirm gezeigt werden. Das schreibt das Branchenblatt The Grocer.

Händler wollen Kunden mit maßgeschneiderter Werbung bombardieren. Werbemenschen wollen aus den Daten herauslesen, ob ihre Spots funktionieren. Und Datenschützer wollen der Gesichtserkennungstechnik einen Riegel vorschieben. Big Data im Handel, die Sammlung großer Datenmengen über Kunden, ist für sie der erste Schritt zu Big Brother.

Biometrische Technologien - die Erfassung von Personen anhand von Körpermerkmalen - waren für Jahrzehnte Domäne staatlicher Sicherheits- und Überwachungsbehörden, etwa bei Grenzkontrollen oder Terrorfahndung. Die Technik ist nicht nur verfeinert worden, sondern auch billiger. Deshalb wird sie vermehrt von kommerziellen Anbietern eingesetzt.

Die britische Datenschutzorganisation Big Brother Watch warnte angesichts von Tescos Plänen: ""Die Menschen würden nie akzeptieren, dass die Polizei in Echtzeit erfährt, in welche Läden wir gehen, aber diese Technology kann genau das. Es ist ein Überwachungsstaat an der Ladentür."" Vergangenes Jahr bemerkte eine Arbeitsgruppe der EU-Kommission zur Gesichtserkennung, die Technik könnte dazu verwendet werden, Einzelnen ""den Zugang zu Geschäften, Restaurants oder anderen Orten zu verweigern"". Nach Protesten von Datenschützern schaltete Facebook seine Gesichtserkennung vergangenes Jahr in der EU ab.
""Ja, es ist wie aus Minority Report""

Die Firma, die aus Gesichtern Konsumwünsche liest, heißt Amscreen. Sie beliefert Tesco mit dem System Optimeyes. Ihr CEO Simon Sugar gibt zu: ""Ja, es ist wie aus Minority Report."" Er meint die Science-Fiction-Kurzgeschichte, in deren Verfilmung Tom Cruise nach einem Augenscan mit individualisierter Werbung bombardiert wird.

Doch glaubt Sugar, seine Technologie werde ""das Gesicht des britischen Einzelhandels verändern"" und beschwichtigt: Optimeyes bestimme nur Geschlecht und teile Kunden in eine von drei Altersklassen ein. Fotos würden nicht gespeichert. Amscreen gehört Simons Vater, Lord Alan Sugar. Der wurde mit der Computerfirma Amstrad reich und besaß einmal den Fußballklub Tottenham Hotspurs.

Dank zweistelliger Wachstumsraten soll der Markt für Biometrieprodukte 2014 mehr als zehn Milliarden Dollar groß sein, schätzen Konsumforscher. Nicht nur Terror- und Hackerangst bringen der Branche gute Geschäfte, sondern mittlerweile auch die Datensammler des Einzelhandels. Deren Gesichtserkennungsprogramme sind aber bisher eher Experimente als flächendeckende Überwachung.

Adidas lässt in einigen Geschäften Geschlecht und Alter per Software erkennen, um Kunden entsprechende Sportschuhe zu präsentieren. In Japan scannen Getränkeautomaten Durstige und bieten ihnen vermeintlich passende Drinks an.

Für Tesco ist der Schritt zur automatischen Gesichtserkennung in gewisser Weise konsequent: Der Einzelhandels- und Tankstellenkonzern setzt schon seit Langem konsequent auf die Auswertung von Kundendaten, 1995 führte er eines der ersten Kundenkartensysteme in Großbritannien ein. Firmenchef Phil Clarke verkündet auf Konferenzen: ""Businessregel Nummer eins: Kenne deinen Kunden.""

Jetzt schaut er seinen Kunden sogar direkt in die Augen. Die drohen auf dem Nachrichtendienst Twitter mit Tesco-Boykott. Ein Nutzer kommentierte: ""Jetzt ist der richtige Zeitpunkt, in eine Burka zu investieren.""";https://www.sueddeutsche.de/wirtschaft/gesichtserkennung-beim-einkaufen-schau-mir-in-die-augen-kunde-1.1810293;sz.de;Jannis Brühl
14.03.2014;Google versagt bei Grippe-Vorhersagen;"Das Projekt gilt als Paradebeispiel dafür, was mit Big Data möglich ist, also der intelligenten Analyse der gewaltigen Datenmengen, die von Suchmaschinen und sozialen Netzwerken erzeugt werden. Doch jetzt zeigen sich erste Schwächen bei Google Flu Trends (GFT). So heißt der Versuch des Suchmaschinenriesen, Grippe-Epidemien frühzeitig und sogar auf regionaler Ebene vorauszusagen.

Dabei klingt der Ansatz, den Google-Wissenschaftler 2009 im Fachmagazin Nature vorgestellt haben, einfach und genial. Sie verglichen über einen fünfjährigen Zeitraum die 50 Millionen am häufigsten von US-Bürgern eingegebenen Suchbegriffe mit den realen Krankheitsdaten, wie sie von der Seuchenschutzbehörde CDC gesammelt werden. So fanden sie 45 Begriffe, die stark mit dem Auftreten einer Grippe korrelieren. So sollte es möglich sein, nahezu in Echtzeit zu registrieren, wann und wo eine Grippe ausbricht. Das wäre ein großer Vorteil gegenüber den CDC-Analysen, die mit zwei Wochen Verspätung publiziert werden.

Epidemien überschätzt

Dummerweise scheint die Rechnung bislang nicht wirklich aufzugehen, wie jetzt ein Forscherteam um David Lazer und Alessandro Vespignani von der Northeastern University in Boston in Science (Bd. 343, S.1203, 2014) berichtet. So wurde die nichtsaisonale H1N1-Pandemie des Jahres 2009 vom ursprünglichen GFT schlicht übersehen. Danach hat ein verbessertes GFT das Ausmaß der saisonalen Epidemien 2011/2012 und 2012/2013 um mehr als 50 Prozent überschätzt. Und im Zeitraum von August 2011 bis September 2013 lieferte das Analyse-Tool an 100 von 108 Wochen überhöhte Prognosen.

Da Google weder die verwendeten Suchbegriffe noch den Algorithmus offenlegt, fällt die Suche nach den Gründen für diese Fehlprognosen schwer. Die Science-Autoren vermuten eine ""Big-Data-Hybris"", die dazu führt, dass die Google-Forscher sich angesichts der Menge der Daten nicht ausreichend um deren Validität und Reliabilität kümmern.

Ein Einwand, der auch für andere Analysen - etwa von Facebook und Twitter - gilt. ""Viele Quellen von Big Data kommen von privaten Unternehmen, die ihr Angebot ständig nach den Bedürfnissen ihres Geschäftsmodells ändern"", sagt Co-Autor Ryan Kennedy. ""Wir müssen besser verstehen, wie das die produzierten Daten verändert."" Nur wenn man die Big-Data-Analyse mit traditionellen Methoden verbinde, entstünde eine wirklich bessere Wissenschaft. ";https://www.sueddeutsche.de/wissen/big-data-google-versagt-bei-grippe-vorhersagen-1.1912226;sz.de;Christian Weber
17.10.2013;Ohne gesunden Menschenverstand;"Die Frage, ob Computer einen Intelligenzgrad erreichen, der es mit den Menschen aufnehmen kann, ist schon lange keine Frage der Science Fiction oder der Philosophie mehr. Der Forschungszweig der Künstlichen Intelligenz (KI) arbeitet hart am Aufschluss der Maschinen. Und der Loebner-Preis ist ein Wanderpokal für diese Disziplin.

Computerprogramme stellen sich hier dem sogenannten Turing-Test. Im Jahr 1950 postulierte der britische Mathematiker Alan Turing, dass man die Intelligenz einer Maschine messen könne, indem man ihr kommunikatives Verhalten in Konkurrenz zu dem eines Menschen setzt.

Das Experiment geht folgendermaßen: Eine Gruppe Juroren unterhält sich über einen Chat mit unterschiedlichen Paaren hinter einem Sichtschutz. Jedes dieser Paare besteht aus einem Computerprogramm, sogenannten Chat-Bots, und einem Menschen. Die Schiedsrichter müssen herausfinden: Welcher Gesprächspartner ist Mensch? Und welcher Maschine? Sobald die Antworten der Software nicht mehr von denen eines Menschen unterschieden werden können, so Turing, könne man ""von denkenden Maschinen sprechen, ohne Widerspruch erwarten zu müssen"".
""Wie heißt die Hauptstadt von Litauen?""

Turings Prophezeiung, dass ein Computer dem Test bis zum Jahr 2000 standhält, hat sich - und das ist vielleicht sogar eine gute Nachricht für die Menschen - nicht bewahrheitet. 23 Mal fand der Loebner Preis seit 1991 statt und bislang konnte keines der Programme die Richter von sich überzeugen.

An einem Samstag um drei Uhr Ortszeit hatten die Chat-Bots im nordirischen Londonderry in diesem Herbst wieder einmal die Gelegenheit, sich zu beweisen. Restriktionen gibt es in den Gesprächen nicht, Smalltalk ist genauso erlaubt wie ein scharfes Verhör. Interessant war vor allem, mit welcher Strategie die Richter die Programme ihrer Maschinenhaftigkeit überführen wollten.

Der erste schlüpft in die Rolle eines archaischen Schulmeisters: ""Wie heißt die Hauptstadt von Litauen?"" - ""Wer ist der amtierende Präsident des Landes?"" - ""Wie viele Menschen leben in China?"" Ein zweiter Richter fragt nach Kindern, dem Lieblingsgetränk und dem Spitznamen der Stadt. Ein Dritter probiert es mit Logikrätseln. Eine Richterin geht an die Substanz und frage: ""Wer hat dich erschaffen?"" Eines der Programme fällt darauf rein und antwortet, dass es in der Programmiersprache AIML geschrieben wurde.

Die Fragen sagen viel darüber aus, welches Potenzial die Richter den Maschinen zutrauen. Und deswegen auch darüber, wie sich die Beziehungen zwischen Mensch und Software verändern. Was bedeutet es zum Beispiel, wenn die Frage nach der chinesischen Bevölkerung mit der richtigen Zahl beantwortet wird, so wie es in der dritte Runde geschah? Suchmaschinen wie Google oder Wolfram Alpha können darauf antworten, aber welcher durchschnittliche Nordire hat dieses Wissen parat? Eine bis auf die letzte Stelle korrekte Antwort wirkt heutzutage maschinenhaft. Auch, weil sich die Menschen an die Präsenz von teilautonomen Software-Agenten wie Apples Siri oder das Google-Äquivalent Google Now gewöhnt haben.
Menschen sind boshaft, Computer servil

Das war freilich nicht immer so. In den Achtzigerjahren beschrieb die amerikanische Technik-Soziologin Sherry Turkle in ihrem Buch ""The Second Self: Computers and the Human Spirit"" die menschliche Tendenz, das Verhalten von Maschinen analog zu dem von Menschen zu bewerten. Sie sprach vom ELIZA-Effekt, nach dem gleichnamigen, aus nur wenigen hundert Zeilen Code bestehenden Programm, das der MIT-Informatiker Joseph Weizenbaum im Jahr 1966 entwickelte.

ELIZA imitierte einen gutmütigen Psychotherapeuten, indem es die Aussagen eines menschlichen Gesprächspartners in Fragen umwandelte, Schlüsselwörter des Benutzers wie ""Familie"" oder ""es geht mir nicht gut"" identifizierte und an diesen zurückspielte. So wurde das Programm von den Probanden tatsächlich als mitfühlend und emotional wahrgenommen, selbst wenn sie wussten, dass sie es mit einem Computer zu tun hatten. Einige Versuchspersonen baten sogar, mit der Maschine alleine gelassen zu werden. Zu persönlich sei der Austausch, als dass sie die Anwesenheit eines Beobachters ertragen könnten.

Knapp 60 Jahre, nachdem Turing sein Gesetz formulierte, regt sich jedoch Kritik an seinem Test. Computerwissenschaftler wie der Kanadier Hector Levesque bezweifeln, dass ein Gespräch als Gradmesser für Intelligenz taugt, weil man dabei viel zu leicht betrügen könne. So streuen Maschinen willkürliche Schreibfehler in ihre Antworten. Fehler liegen in der Natur des Menschen, deshalb ist die Fähigkeit, eine perfekte Rechtschreibung bei 2000 Anschlägen pro Minute hinzulegen zu Recht verdächtig. Eine andere Strategie der Programme oder besser ihrer Entwickler besteht in einer gewissen Rotzigkeit. Menschen sind boshaft, der Computer eher servil. Wenn man also eine mit Unflätigkeiten gespickte Antwort erhält, steckt dahinter wohl ein Mensch.
Wird es zu banal, bekommt der Computer Probleme

Laut Levesque hat sich die Gemeinde der KI-Forscher verrannt. Anstatt immer neue Trends zu verfolgen, sei es Big Data, Googles ""Virtual Brain"" oder die eben erst angekündigte Deep-Learning Software von Facebook, solle man lieber die Subtilität menschlichen Wissens analysieren. Deshalb schlägt Levesque eine neue Art von Test vor, der von einer durchschnittlich intelligenten Person leicht zu lösen ist, aber eine Maschine, die ihr ""Wissen"" nur Datenbankabfragen verdankt, ziemlich fordert.

Levesque entwirft ein sogenanntes Winograd-Schema, er spielt mit linguistischen Tricks. Eine seiner Aufgaben geht folgendermaßen: ""Der große Ball bricht durch den Tisch, weil er aus Styropor ist?"" Frage: ""Was ist aus Styropor? Der Ball oder der Tisch?"" Solche Fragen sind für Computer deshalb so schwer zu beantworten, weil es zur Beantwortung erstens ""gesunden Menschenverstand"" braucht und weil sie Dinge betreffen, die viel zu banal sind, als dass sie in den Quellen - egal ob Enzyklopädie oder Internetseite - erwähnt würden, aus denen die Programme ihr Wissen ziehen. Und so kommen die meisten Computer, die sich künstlicher Intelligenz rühmen, in Schwierigkeiten, wenn ein Begriff oder ein Sachverhalt nicht zuvor explizit in der Datenbank verarbeitet wurde.

Aus dem gleichen Grund leistete sich IBMs Wissensungetüm Watson 2011 bei der ersten Runde der Quizsendung Jeopardy einen peinlichen Ausrutscher. Weil die Maschine, die aus 90 einzelnen Servern zusammengesetzt ist, über massive 16 Terabyte RAM verfügt, von der Bibel bis zur Wikipedia mit jeder erdenklichen Quelle gefüttert wurde und doch gerade darauf gedrillt war, subtile Andeutungen menschlicher Sprache zu verstehen, von simplen Tatsache verwirrt war. Nämlich, dass man im alltäglichen Gebrauch den Kontinent Amerika mit dem Staat Amerika gleichsetzen kann. Auf eine Frage über Flughäfen in der Kategorie US-amerikanische Städte antwortete Watson mit: Toronto.";https://www.sueddeutsche.de/digital/kuenstliche-intelligenz-im-test-gesunder-menschenverstand-mangelhaft-1.1797503;sz.de;Michael Moorstedt
11.05.2010;Was ist eigentlich ein Business Development Manager?;"Große Unternehmen in Geschäftsfeldern wie Computer, Neue Medien oder Biotechnologie, die sich rasend schnell wandeln, müssen sich beizeiten überlegen, in welche Richtung der Trend geht. Sonst werden sie abgehängt oder finden sich auf dem falschen Gleis wieder. Business Development Manager (kurz: BDM) sind die Strategen, die wittern sollen, was der Boom-Sektor der Zukunft sein wird.

Strategische Kooperationen

Einer dieser Ideenlieferanten ist Torsten Hoof von der Heidelberger Firma Lion bioscience, dem führenden Anbieter in der jungen Disziplin Bioinformatik. ""Ich bin zuständig für die Identifizierung und Umsetzung neuer Geschäftsideen"", berichtet er. Dabei gehe es um die Fragen: ""Wo will die Firma in zwei oder drei Jahren sein? Und wen brauchen wir auf der Kunden- und Partnerseite, um das umzusetzen?"" Ist eines von Hoofs neuen Konzepten von der Geschäftsleitung gut geheißen worden, kann er daran gehen, es zu verwirklichen - von den ersten Gesprächen bis zur Vertragsgestaltung. Eine seiner Hauptaufgaben ist es, zu beiderseitigem Nutzen strategische Kooperationen zu schmieden, denn nur so lässt sich der Markt breit erschließen. Vor kurzem wurde eine dieser Allianzen öffentlich bekannt gegeben: ""Die Firma Tripos ist in Chemieinformatik führend, wir in Bioinformatik. Durch die Zusammenarbeit können wir einen Service für den ganzen Life Science-Bereich liefern"", freut sich Hoof. ""Gemeinsam entwickeln wir eine einheitliche IT-Plattform für biologische und chemische Datenanalyse."" Auch Jürgen Berg, Resource & Business Development Manager der auf Großrechner spezialisierten Firma Amdahl, ist im Job ständig auf Partnersuche: ""Business Development definieren wir so: Wir tun uns mit Firmen zusammen, die Fähigkeiten haben, die wir nicht haben"", meint er. ""Ich sehe mich als jemanden, der versucht, neue Projekte aufzuspüren, die in unserem Portfolio noch nicht drin sind."" Vorher kümmerte sich Amdahl nur um die Hardware, aber die neuen Betätigungsfelder Internet und E-Commerce lockten. Also suchte man Partner und bot ihnen Zugriffe auf Großrechner. Die Partner wiederum geben Amdahl die Möglichkeit, Web-Applikationen einzusetzen.

Über den Zaun schauen

Etwa ein Drittel seiner Zeit verbringt Hoof auf Reisen, besucht Kunden und Partner, schaut sich auf Kongressen um. Auch seine restliche Arbeit steht ganz im Zeichen der Analyse und Kommunikation: Er bewertet neue Projekte und stellt sie intern vor, unterhält sich mit der eigenen Sales- und Marketingabteilung, hält den Kontakt zu den Hochschulen und zu kleinen Firmen, deren Produkte man eventuell in Lizenz produzieren und in das Service-Paket des Unternehmens integrieren könnte. Berg organisiert zudem Vorträge und Workshops über neue Technologien für die Amdahl-Mitarbeiter, ""damit unsere Leute ein bisschen über den Zaun schauen können, wohin die Entwicklung geht.""

Job für Quereinsteiger

Ein Business Development Manager sollte kommunikations- und verhandlungsstark sein, gute Analysefähigkeiten haben und unternehmerischen Sachverstand mitbringen. Außerdem braucht man natürlich Sachkenntnis in der Branche, damit man beurteilen kann, welche Ideen eine Chance haben. Von Juristen über Betriebswirte bis zu Informatikern, Biochemikern oder Ingenieuren finden sich in diesem Beruf Quereinsteiger jeder Couleur. Am beliebtesten sind Bewerber, die Erfahrung im Vertrieb, Marketing oder einer Managementberatung vorweisen können. Eine Ausbildung gibt es nicht, BDM werden im Training on the Job und mit Kursen, zum Beispiel im erfolgreichen Verhandeln von Allianzen, fit gemacht. Über das Gehalt hört man von BDM selten Klagen: Von 80.000 Mark im Jahr für Einsteiger bis zu 300.000 für Vollprofis ist alles drin.

Setzen eigentlich auch die Berufsvisionäre mal auf den falschen Dampfer? ""Beim Aufkommen der PCs haben wir wohl alle ein bisschen daneben gelegen"", erinnert sich Jürgen Berg. ""Heute schenkt man neuen Entwicklungen viel mehr Beachtung, vielleicht zu viel. Bei E-Commerce und Data Warehouse ist schon etwas Ernüchterung eingetreten."" Er selbst nimmt das alles inzwischen mit mehr Gelassenheit. Denn eins ist sicher: Der nächste Trend kommt bestimmt.";https://www.sueddeutsche.de/karriere/englisch-fuer-die-jobsuche-was-ist-eigentlich-ein-business-development-manager-1.559363;sz.de;Sylvia Englert
25.11.2020;Wolke mit Datenschutz;"Von Gaia-X wird Großes erwartet. Benannt nach der griechischen Erdgöttin soll das Projekt nichts Geringeres werden als der ""Goldstandard für Cloud-Services weltweit"". So wünscht sich das zumindest Geburtshelfer und Wirtschaftsminister Peter Altmaier (CDU). Vor gut einem Jahr hatte er auf dem Digitalgipfel der Bundesregierung Gaia-X das Projekt als deutsch-französische Initiative mit seinem französischen Amtskollegen Bruno Le Maire vorgestellt. Die Initiatoren verstehen das Projekt als ""Wiege eines offenen und transparenten digitalen Ökosystems, in dem Daten und Dienste verfügbar gemacht, zusammengeführt und vertrauensvoll geteilt werden können"". Dafür sollen neue und bestehende Angebote über Open-Source-Anwendungen und offene Standards miteinander vernetzt werden. Das heißt, Gaia-X soll dafür sorgen, dass es Cloud-Services gibt, die europäische Datenschutzstandards gewährleisten, die es erlauben, Daten untereinander branchen- und länderübergreifend sicher auszutauschen. Und dabei am besten noch konkurrenzfähig zu amerikanischen und chinesischen Diensten sind.

Gegen Clouddienste aus den USA wie Microsoft Azure, Google Cloud und Amazon AWS gibt es insbesondere nach dem Urteil des EuGH zugunsten des österreichischen Datenschutzaktivisten Max Schrems Vorbehalte. Er kippte mit seiner Klage das sogenannte Privacy Shield, das den Datenaustausch zwischen der EU und den USA rechtssicher regeln sollte. Das Gericht stellte im Juli klar, dass EU-weit die Datenschutz-Grundverordnung (DSGVO) gelten muss. Sie regelt, dass personenbezogene Daten grundsätzlich nur dann in ein Drittland übermittelt werden dürfen, wenn das betreffende Land für die Daten ein angemessenes Schutzniveau gewährleistet. Das wiederum ist nicht mit dem US-amerikanischen Cloud Act vereinbar. Der besagt, dass US-Unternehmen auf Anfrage der Sicherheitsbehörden Kundendaten herausgeben müssen.
Auch die US-Konzerne zeigen Interesse

Allerdings sind US-Konzerne wie Amazon, Google und Microsoft nicht von vornherein von der Gaia-X-Initiative ausgeschlossen. Wirtschaftsminister Altmaier hatte mehrmals erklärt, dass auch Tech-Konzerne aus den USA Angebote entwickeln sollten, die mit den Richtlinien von Gaia-X vereinbar sind. Und diese zeigen nach anfänglichen Vorbehalten durchaus Interesse. Microsoft-Präsident Brad Smith etwa nannte Gaia-X einen ""durchdachten Vorschlag"".

22 Gründungsmitglieder wie SAP, die Deutsche Telekom und BMW verzeichnete Gaia-X im Oktober 2019, gut ein Jahr später wollen weitere 100 Unternehmen und Organisationen aus mehreren europäischen Ländern das Projekt mit Geld und Know-how unterstützen. Die Initiative ist mittlerweile ein gemeinnütziger Verein mit dem etwas sperrigen Namen Gaia-X AISBL. AISBL steht für Association internationale sans but lucratif, die belgische Rechtsform für eine Vereinigung ohne Gewinnerzielungsabsicht.

Den Milliardenmarkt der Cloudanbieter dominiert derzeit Amazon Web Services (AWS) mit 33 Prozent Marktanteil weltweit. Nach Zahlen der Synergy Research Group haben Unternehmen im dritten Quartal 2020 weltweit 33 Milliarden US-Dollar für Cloud-Infrastrukturdienste ausgeben und damit 33 Prozent mehr als im dritten Quartal 2019. Nach Amazon belegt Microsoft mit 18 Prozent Marktanteil den zweiten Platz, zusammen bedienen sie gut die Hälfte des Marktes. Platz drei geht mit neun Prozent an Google, es folgen Alibaba und IBM mit je fünf Prozent. Erst auf Platz zehn findet sich mit einem Prozent Marktanteil das erste europäische Unternehmen: SAP aus Walldorf. Die Top Ten der Anbieter teilen unter sich 80 Prozent des Marktes auf. Die übrigen 20 Prozent bedienen kleinere nationale oder regionale Cloudanbieter.
Viele deutsche Unternehmen nutzen US-Hyperscaler

Zu den Kunden der großen US-Cloudanbieter gehören mittlerweile auch viele deutsche Unternehmen. So verkündete die Deutsche Bahn Ende Oktober, dass sie ihre eigenen Rechenzentren abgeschaltet und die komplette Informationstechnik in die Cloud verlagert habe - in die Clouds von Amazon und Microsoft. Der Konzern begründete diesen Schritt damit, dass die Auslagerung flexibler und kostengünstiger sei, als die IT-Anwendungen selbst zu betreiben. Die Deutsche Bank setzt seit Sommer bei ihren Finanzdienstleitungen auf Googles Cloud. Der Modehändler Zalando nutzt seit Kurzem Machine-Learning-Algorithmen von Amazon.

Gaia-X will aber kein ""Hyperscaler"" nach dem Vorbild von Google, Amazon oder Microsoft werden. Vielmehr soll den Cloud-Riesen mit einer Vernetzung von vielen kleineren Anbietern aus Europa entgegengetreten werden. Aber wie stehen die Chancen? Die Marktforscher von Forrester Research haben sich die Initiative genauer angesehen und sind zu dem Ergebnis gekommen, dass das Projekt zwar sehr ambitioniert, aber insgesamt noch zu abstrakt sei. Es sei noch nicht klar, was Gaia-X genau sei und wo Initiatoren und Mitglieder damit hinwollten.

Martin Endreß, Vorstand eines der größten europäischen Webhoster 1&1 Ionos, ist dagegen überzeugt, dass Gaia-X das richtige Projekt zur richtigen Zeit ist: ""Wir sind von Anfang an dabei, und für uns ist Gaia-X ein starker Impuls aus der Regierung."" Für Europa könne daraus eine echte Wertschöpfung entstehen, vor allem dadurch, dass europäische Tech-Arbeitsplätze geschaffen werden.

Endreß sieht den Vorteil von Gaia-X darin, dass das Projekt den Rahmen vorgibt, indem Plattformen für den Datenaustausch geschaffen werden können. Gerade im Bereich des Machine Learning und der künstlichen Intelligenz (KI), für den viele Daten benötigt werden, sei es sinnvoller, dass beispielsweise fünf Unternehmen ihre Daten in einen Datenpool geben. Über diesen könne dann ein gemeinsamer KI-Algorithmus laufen, das bringe viel mehr, als wenn jedes Unternehmen nur seine Daten verwenden könne, um die KI zu trainieren. ""Für die technologische Entwicklung Europas ist es immens wichtig, dass es diese Datapools gibt"", sagt Endreß. Zudem könne das Unternehmen mit der eigenen Cloud die Infrastruktur für diese Datenpools stellen. Diese erfülle alle Datenschutz- und Souveränitätsanforderungen.
Nachfrage aus dem öffentlichen Sektor soll Gaia-X einen Schub verleihen

Denn Gaia-X soll festschreiben, welche Standards gelten sollen und wer Zugriffsrechte auf die jeweiligen Daten hat. Das ist zum Beispiel auch bei Vorhaben wie der elektronischen Patientenakte wichtig, die Patientinnen und Patienten von Arzt zu Ärztin und möglicherweise von Land zu Land mitnehmen können sollen. Aber nicht nur in der Industrie und im Gesundheitswesen, auch für die Digitalisierung der öffentlichen Verwaltung sollen Gaia-X-Projekte eingesetzt werden. ""Deutschland liegt mit seinen digitalen Diensten auf einem inakzeptablen 21. Platz im Ranking der 27 EU-Staaten. Eine hohe Nachfrage aus dem öffentlichen Sektor würde Gaia-X die dringend notwendige Starthilfe geben"", sagt Iris Plöger, Mitglied der BDI-Hauptgeschäftsführung.

Laut Wirtschaftsministerium werde momentan in knapp 50 Fallbeispielen ein Bedarf ermittelt und dann in Arbeitsgruppen vertieft. Zunächst wolle man 2021 mit einem ""Minimum Viable Product"" (""minimal funktionsfähigem Produkt"") an den Start gehen. Dies sei der Ausgangspunkt für Prototypen, den Test kritischer Funktionalitäten und weitere Entwicklungen.

Für Martin Endreß ist der entscheidende Punkt, dass es bald konkrete Anwendungen mit einem Mehrwert für Unternehmen oder den öffentlichen Sektor gibt. ""Wir haben genügend IT-Architekturkonzepte geschrieben, der nächste Schritt muss einer weg von Konzepten hin zu drei, vier realisierbaren Anwendungen sein. Das muss in den nächsten sechs Monaten passieren.""";https://www.sueddeutsche.de/digital/gaia-x-cloud-amazon-google-souveraenitaet-1.5126014;sz.de;Miriam Hauck
05.10.2020;Kunst am Tablet;"Auch wenn digitales Lernen mit interaktiven Webinaren und Hybridveranstaltungen ""in der Zeit der Pandemie die Rettung ist, ist es auch die Zukunft?"" Diese Frage stellt Susanne May, die Programmdirektorin der Münchner Volkshochschule bei einem Treffen im Gasteig. Gemeinsam mit der Koordinatorin Daniela Kirschstein spricht sie vor dem Semesterstart am 12. Oktober über einen ganz besonderen Schwerpunkt: ""Connected. Leben in digitalen Welten"" wird sich ein ganzes Jahr lang mit den Auswirkungen der Digitalisierung befassen. ""Ich gehöre zu denjenigen, die noch das Kratzen der Kreide auf der ABC-Tafel im Ohr haben, ich denke vermutlich anders über die Digitalisierung nach als diejenigen, die mit sieben Jahren selbstverständlich ein Handy nutzen"", sagt May.

Bildungswillige haben bei den rund 9000 Angeboten des neuen Semesters die Wahl zwischen einer Präsenz- und einer Online-Variante; viele Vorträge finden zudem ""hybrid"" statt, also mit Präsenz-Teilnehmern im Saal vor Ort, während alle anderen gleichzeitig von zu Hause oder unterwegs den Livestream verfolgen können. Dennoch sagt May: ""Präsenzseminare machen den Zusammenhang von Bildung, Begegnung und Beziehung erfahrbar - das kann ein Webinar so nicht leisten.""

Eröffnet wird der Schwerpunkt ""Connected"" mit fast 500 Veranstaltungen von Soziologieprofessor Armin Nassehi. In seinem Vortrag ""Die digitale Gesellschaft nach Corona"" wird er diskutieren, ""warum die Digitalisierung so erfolgreich und allgegenwärtig ist. Es geht zunächst darum, die Digitalisierung zu verstehen und nicht so sehr darum, ob sie gut oder böse ist"", so May. Oder, wie Nassehi selbst im Programmheft zitiert ist: ""Wenn die Digitalisierung nicht perfekt zu dieser Gesellschaft passen würde, wäre sie nie entstanden oder längst wieder verschwunden.""

Grundsätzlich sind die Angebote dieses Schwerpunkts in einen Praxis- und einen Diskursbereich gegliedert, erläutert May. Während es beim ""Anwendungswissen"" etwa darum gehe, ein neues Smartphone zu bedienen, einen Roboter zu bauen oder eine Programmiersprache zu erlernen, laden wiederum viele andere Angebote dazu ein, die Digitalisierung auch kritisch zu reflektieren und beispielsweise danach zu fragen, wie die sogenannten sozialen Medien die Aufmerksamkeit und das Verhalten manipulieren. ""Wir wollen nicht allein vermitteln, wie wir die digitalen Medien reibungslos und effektiv einsetzen, wir wollen insbesondere auch ihre Botschaften verstehen und diskutieren"", sagt May.

Der anwendungsorientierte Bereich ""Erlebnis Digitalisierung"" darf laut Kirschstein ganz praktisch verstanden werden: In zahlreichen analogen und digitalen Veranstaltungen erkunden Teilnehmer die technischen wie künstlerischen Möglichkeiten der Digitalisierung. Ob Kunst am Tablet, in der Virtuellen Realität, Kreativität am 3-D-Drucker, Programmierung oder Robotik und Machine Learning: ""In diesen Veranstaltungen kann man selbst aktiv werden und beim Ausprobieren lernen"". Doch so fasziniert die Schwerpunktkoordinatorin auch von den Möglichkeiten in der Reihe ""Digitale Orte"" ist, etwa an einer Führung durch das Metropolitan Museum of Art in New York teilzunehmen: ""Das ist ein Zusatz, der wunderbar zur Vor- und Nachbereitung geeignet ist - aber kein Ersatz sein kann für einen Präsenzbesuch"". Dass ein analoges Leben im Digitalen heute kaum mehr möglich ist, habe insbesondere für die ""digitalen Immigranten"" zu Problemen im Corona-Lockdown geführt, sagt May. An sie wendet sich die Reihe ""Digital fit in die Zukunft"", die den Umgang mit PC, Laptop, Tablet oder Handy vermittelt. Speziell an Jugendliche wenden sich Kurse, in denen sie zusammen mit Forschern der TU München Roboter programmieren und an möglichen Einsatzfeldern tüfteln. So entstehen Ernteroboter für Erdbeeren, digitale Ampelschaltungen sowie innovative Steuerungen für autonom fahrende Busse und Lkw.

Die Veranstaltungen, die Orientierungswissen vermitteln wollen, wenden sich etwa unter der Überschrift ""Der digitale Mensch"" der Frage zu, ""was die digitale Revolution mit uns als Menschen macht, wie sie unsere Beziehungen und unser Familienleben verändert, die Wahrnehmung von uns selbst und anderen"", sagt Kirschstein. In der Reihe ""Die digitale Gesellschaft"" sprechen der Journalist und Blogger Michael Seemann (""mspro"") über den politischen Einfluss von Internetplattformen. Die Philosophin und Journalistin Manuela Lenzen erklärt, was Künstliche Intelligenz ist, inwiefern sie nützlich und wo ihre Anwendung heikel ist. In Anbetracht der Tatsache, ""dass wir alle unseren digitalen Fußabdruck nicht mehr verstecken können"", so Kirchstein, betrachten der Darknet-Experte Stefan Mey und der Philosoph Professor Thomas Schmaus die Frage, ""was Freiheit in der digitalen Welt bedeutet"" - und ""wie der Mensch im digitalen Glashaus sein Menschsein bewahren kann"". (Alle Veranstaltungen: www.mvhs.de/connected";https://www.sueddeutsche.de/muenchen/volkshochschule-kunst-am-tablet-1.5054734;sz.de;Barbara Hordych
25.09.2020;Wie Software bald Fluchtbewegungen vorhersagen soll;"Alexander Kjærum hilft dabei, die Zukunft vorauszusagen. Vor seinem Bildschirm in Dänemarks Hauptstadt Kopenhagen erklärt er per Videoanruf, wie das gehen soll. Kjærum ist seit einem Jahr Senior Analyst beim Danish Refugee Council (DRC), einer privaten gemeinnützigen Organisation. Angefangen habe alles mit der Frage, ob man mittels künstlicher Intelligenz schon ein bis drei Jahre im Voraus wissen könnte, dass Menschen aus ihrer Heimat vertrieben werden.

Künstliche Intelligenz findet seit vielen Jahren in der Sicherheitspolitik Anwendung. Autonome Waffensysteme ermöglichen etwa neue Formen der Kriegsführung. In der humanitären Hilfe ist der Einsatz von Big Data allerdings neu. 2017 unterstützte die dänische Regierung den DRC mit umgerechnet 340 000 Euro, damit er gemeinsam mit dem Softwarehersteller IBM ein Modell entwickeln konnte, das Migrationsbewegungen und Flüchtlingsströme nachvollziehen - und sogar vorhersagen kann.

Am Bildschirm zeigt Kjærum, wie die Software funktioniert. Das Modell bezieht die Grundursachen für mögliche Fluchtbewegungen ein. Dabei stützt es sich auf fünf Treiber - die wirtschaftliche Situation in einem Land, Unsicherheit, Regierungsführung, Umwelt und die Zusammensetzung der Bevölkerung - und mehr als hundert Indikatoren dafür, wie Luftverschmutzung oder Korruption. Diese basieren auf frei zugänglichen Daten, etwa des UNHCR oder von Freedom House, einem Thinktank, der jährlich den Stand der Demokratie in der Welt bewertet. Diese Open-Source-Datensätze, kombiniert mit Ereignissen der vergangenen 25 Jahre, geben der künstlichen Intelligenz Stoff, um daraus zu lernen. Die Software steht auch anderen Organisationen zur Verfügung. ""Wir wollen, dass nicht nur Datenwissenschaftler damit arbeiten"", sagt Kjærum.

Für Afghanistan etwa prognostizierte das Programm die Zahl von Binnenvertriebenen auf Basis früherer Daten sehr genau. Bei der Massenflucht von Rohingya aus Myanmar nach Bangladesch im Jahr 2017 dagegen kamen die Prognosen ""nicht annähernd an das heran, was wirklich passiert ist"". Der Grund dafür liege im Wesen von Machine-Learning-Modellen: Gab es in einem Land nie Vertreibung in großem Ausmaß, hat das Modell auch Schwierigkeiten, extreme Anstiege vorauszusagen. Da stoße die Software bislang an ihre Grenzen.

Auch das Deutsche Rote Kreuz prüft derzeit die Möglichkeiten von ""Scenario Planning"" im Kontext von Konflikten. Mit dieser Methode, die auch in der Politik oder Wirtschaft angewandt wird, sollen mögliche Entwicklungen der Zukunft analysiert und zusammenhängend dargestellt werden. Arno Waizenegger, Sachgebietsleiter Sofort- und Nothilfe, berichtet am Telefon von einem Pilotprojekt, das derzeit in den palästinensischen Gebieten läuft. Diese ""Machbarkeitsstudie"" ist jedoch deutlich kleiner angelegt als das DRC-Modell. Die Idee: Auf der Basis historischer Daten zu gewalttätigen Auseinandersetzungen soll ein Algorithmus entwickelt werden, der Prognosen zu möglichen gewalttätigen Zusammenstößen errechnet. Ob die Datenlage dafür ausreicht, wird derzeit getestet.

Die DRC-Software prognostiziert der Sahelregion - dem kargen Streifen am Rande der Sahara - rund eine Million Vertriebene aufgrund von Covid-19. Sie errechnet, mit welcher Wahrscheinlichkeit sich bestimmte Parameter verändern werden - etwa die Menschenrechtssituation, die wirtschaftliche Entwicklung oder die Zahl der getöteten Zivilisten. Anschließend wird der voraussichtliche Einfluss, den die Pandemie auf einzelne Bereiche hat, einbezogen. Kjærum gibt zu, dass solche Aussagen schwer zu treffen sind und die Prognose des DRC-Modells deshalb ""viele Unsicherheiten"" beinhalte.

Arno Waizenegger hält es für schwierig, verlässliche datenbasierte Modelle für die Prognose von bewaffneten Konflikten zu entwickeln. Eine Konfliktpartei treffe mitunter Entscheidungen, die außerhalb des gewöhnlichen Musters liegen. Sie ließen sich daher durch ein auf historischen Daten basierendes Rechenmodell nicht vorhersagen. Ein zweites Problem sei die magere Datenlage in vielen Konflikten: Im Krieg bleibe die Wahrheit immer als Erstes auf der Strecke, sagt Waizenegger. So stehe die Verlässlichkeit von Daten infrage. Oft fehle der Zugang zum Ort des Geschehens, um Ereignisse zu verifizieren. Zudem könnten Konfliktparteien versuchen, die Berichterstattung und entsprechende Daten für die eigenen Zwecke zu manipulieren.

Der Ansatz des Deutschen Roten Kreuzes sei daher ""konservativ"". Man sei mit rund 20 Projekten weltweit vor allem auf Extremwetterereignisse spezialisiert - also darauf, zu agieren, bevor tropische Wirbelstürme oder Überschwemmungen Verwüstung anrichten: ""Das Ziel ist, zu handeln, bevor die Naturgefahr eintritt."" Konkret bedeutet das, Freiwillige und Hilfsgüter zu entsenden, bevor es zu spät ist, weil Brücken zerstört oder Straßen überschwemmt sind. Als der Zyklon Idai 2019 auf das ostafrikanische Mosambik traf, waren einige Hilfsgüter schon vor Ort. Auch konnten viele Menschen dank der Modelle rechtzeitig evakuiert werden. Derzeit sind mehrere afrikanische Länder von Überschwemmungen betroffen. Im Sudan verloren Tausende ihr Zuhause. Das Deutsche Rote Kreuz nutzt den ""Forecast-based Finance""-Ansatz auch bei Überschwemmungen. Das heißt, Geld für humanitäre Hilfe wird auf Basis von Vorhersagen und Risikoanalysen bereitgestellt. Im Sudan wird ein solches Projekt allerdings erst 2021 beginnen.

Neben den Folgen des Klimawandels leiden die Menschen in der Sahelregion seit Jahren unter terroristischen Gruppierungen und ethnischer Gewalt. In diesen Konflikten kamen laut dem Armed Conflict Location & Event Data Project, dessen Daten auch die DRC-Software einbezieht, Tausende Menschen ums Leben. Lange vor der Pandemie wurden etwa im westafrikanischen Burkina Faso Millionen von Menschen aus ihrer Heimat vertrieben. Das Ausmaß dieser Krise nehmen in Europa viele nicht wahr - dabei braucht es dafür nicht einmal einen Blick in die Zukunft. Trotzdem sieht Alexander Kjærum das Potenzial der Machine-Learning-Software: Dies sei nicht, irgendwann die Arbeit der Experten vor Ort zu ersetzen, sondern vielmehr, qualitative Prozesse in der Entwicklungshilfe zu stärken und sie datenbasiert zu unterstützen.";https://www.sueddeutsche.de/digital/ki-fluechtlinge-migration-katastrophen-1.5044672;sz.de;Anna Reuß
18.09.2020;Krebserkennung mit dem Algorithmus;"Vor sechs Jahren starb der Bruder von August Scheidl an Speiseröhrenkrebs. ""Er hat gegen den Tumor angekämpft, aber am Schluss ist er buchstäblich verhungert"", sagt Scheidl. Dann traf es ihn plötzlich selbst: Sein Internist im Kreis Eichstätt eröffnete dem 63-Jährigen, dass sich nun auch bei ihm ein Barrett-Karzinom heranbilde. ""Das war ein Schlag für mich"", sagt Scheidl (Name geändert). Der Internist schrieb ihm eine Überweisung ans Uniklinikum Augsburg. Dort wurde Scheidl der Tumor mit einer neuen, beispielhaften Herangehensweise endoskopisch entfernt. Was er nicht ahnte: Wenige Monate zuvor hatte sein Operateur zudem auf einer internationalen Expertenkonferenz per Video-Zuschaltung ein weltweit neues Verfahren präsentiert - die Erkennung von Speiseröhrenkrebs durch künstliche Intelligenz.

Am Mittwoch wurden in Berlin zwei Nachwuchsforscher, die beide mit viel Engagement an diesem Projekt beteiligt waren, mit dem Endoskopie-Forschungspreis 2020 der Deutschen Gesellschaft für Gastroenterologie, Verdauungs- und Stoffwechselkrankheiten ausgezeichnet: Oberarzt Alanna Ebigbo vom Uniklinikum Augsburg und Robert Mendel, Informatik-Doktorand an der Ostbayerischen Technischen Hochschule Regensburg (OTH). Im Fachjournal ""Gut"" haben sie dargestellt, wie künstliche Intelligenz genutzt wird, um in der Speiseröhre Krebsgewebe von gesunder Schleimhaut zu unterscheiden. Schon 2014 war Helmut Messmann, am Uniklinikum Augsburg der Direktor der III. Medizinischen Klinik, die Idee gekommen, mit künstlicher Intelligenz den Kampf gegen das Barrett-Karzinom aufzunehmen. Der Tumor ist in der Frühphase gut heilbar, im Spätstadium endet die Krankheit oft tödlich, so wie bei August Scheidls Bruder. Mit rund 7000 Erkrankungen deutschlandweit ist der Speiseröhrenkrebs zwar keine sehr häufige Diagnose, aber die Fallzahlen nehmen ""deutlich zu"", wie Messmann betont. Anfangs, so erzählt er dann, sei seine Erfolgsgeschichte nur sehr zäh angelaufen: Ingenieure eines großen deutschen Unternehmens hätten das Potenzial seiner Vision ""nicht so richtig wahrgenommen - kein Interesse!"" Dann las er in einer Zeitung von einem Regensburger Informatiker, der ""Experte für Mustererkennung"" sei: Christoph Palm, Professor an der OTH. Viele Nachmittage saßen die beiden zusammen. ""Dann kam letztes Jahr der Durchbruch"", sagt Messmann. Die Gefühlslage in diesem Moment beschreibt der 58-jährige Fußballfan so: ""Als ob man in einem WM-Endspiel ein Tor schießt."" Inzwischen sind der Mediziner und der Informatiker ""dicke Freunde"", wie Messmann sagt. Beide sind nun mächtig stolz, dass der Fachaufsatz ihrer Mitarbeiter ausgezeichnet wurde.

Schwieriger wird es, zu beschreiben, was die künstliche Intelligenz eigentlich anstellt, damit sie bei einer Endoskopie gesunde Schleimhaut von Tumorgewebe unterscheiden kann. Was unter Medizin- und Informatikexperten wahrscheinlich in einem stundenlangen, hochkomplexen Gespräch enden würde, fasst Messmann für Laien so zusammen: ""Wenn Sie jetzt einem Computer beibringen wollen, einen Hund von einer Katze zu unterscheiden, dann müssen Sie diesen Computer ja irgendwie trainieren und ihm sagen: Schau dir bestimmte Kriterien an."" Gut, soweit verständlich. Messmann kommt zur eigentlichen Problematik: ""Und wenn Sie jetzt dem Computer sagen: Zähle mal die Anzahl der Beine, dann wird der Rechner nicht wissen, was er da vor sich hat. Wenn Sie ihm aber sagen: Vergleich die Größe der Tiere und die Länge der Schnauze, dann wird der Computer den Schäferhund von einer Katze unterscheiden können.""

Ganz so einfach geht es natürlich nicht, Krebszellen von gesunden Zellen zu unterscheiden - und dass Palm und seine Informatiker den Rechner so weit brachten, begann zunächst mit ""Machine Learning"". Sprich, der Rechner wurde fleißig mit Bildmaterial und anderen Informationen gefüttert. Es folgte die Phase des ""Deep Learnings"", bei der eine Maschine per Algorithmus quasi selbst mit sich ausmacht, wie sie zur richtigen Erkenntnis kommt. ""Was da passiert, das ist für mich eine Blackbox"", kapituliert Messmann.

So oder so, mittlerweile rennen Firmen den Professoren die Türen ein, um an das Programm heranzukommen. ""Auch für unsere junge Universitätsklinik Augsburg ist das natürlich ein Imagegewinn"", sagt Messmann, wurden doch die personell und finanziell viel stärker aufgestellten Konkurrenten in London, Amsterdam und Tokio abgehängt. Dass nun auch noch die Nachwuchsforscher Ebigbo und Mendel die Auszeichnung bekommen haben, trägt einmal mehr dazu bei, dass das neue Verfahren an Bekanntheit gewinnt. Seine Vorteile fasst Messmann so zusammen: ""Auch Ärzte, die nicht so oft Gelegenheit haben, mit einem Barrett-Karzinom konfrontiert zu werden, bekommen so ein Instrument an die Hand, das die Diagnose sicherer macht.""

Patient August Scheidl, der von Messmann im Mai operiert wurde, ist dankbar, dass sein Tumor im Frühstadium noch ohne großen Eingriff entfernt werden konnte. Auch Scheidl hat nun eine Mission: ""Geht rechtzeitig zur Vorsorge"", sagt er.";https://www.sueddeutsche.de/bayern/forschung-krebs-kuenstliche-intelligenz-1.5035017;sz.de;Dietrich Mittler
23.07.2020;Lukrative Nachbarschaftsgeschäfte;"Karsten Seehafer ist mehr als zufrieden. ""Wir haben den Spatenstich und den Produktionsstart innerhalb eines Jahres geschafft. Es ist unvorstellbar, so was in Deutschland zu realisieren"", sagt der Eigentümer und Geschäftsführer des Unternehmens Hanomag Härtecenter. Zehn Millionen Euro hat der Hannoveraner Autozulieferer in Taufkirchen im Innviertel investiert. Fortan werden 40 Mitarbeiter Bauteile aus Aluminiumguss für das nahe BMW-Werk in Steyr veredeln.

Das 60 Millionen Euro Umsatz starke Unternehmen ist einer von 638 deutschen Betrieben, die sich 2019 in der Alpenrepublik angesiedelt haben. Insgesamt sind laut österreichischem Firmenbuch 9808 Betriebe mit deutschen Eigentümern im Land tätig. Neben Tausenden kleinen und mittleren Unternehmen sind Branchengrößen wie Aldi, Bosch, MAN, Porsche und Siemens seit Jahrzehnten in der Alpenrepublik stark präsent - und ziehen, siehe Hanomag, bis heute Zulieferbetriebe nach.
Bei der Firmengründung ist der Notariatsakt auch online per Video möglich

""Österreich ist für deutsche Unternehmen sehr attraktiv. Es kann nun genau der richtige Zeitpunkt sein, die eigene Marktpräsenz etwa durch Übernahme eines Wettbewerbers oder Standortgründung mit staatlicher Unterstützung zu stärken"", sagt Holger Frank, Leiter des Unicredit International Center der HVB. Die Bank Austria, die wie die HVB zur Unicredit Gruppe gehört, betreue einige Projekte.

Die hohe Kaufkraft von österreichischen Konsumenten und Firmenkunden sichern deutschen Händlern und Produzenten stabile Umsätze. Wird eine GmbH als österreichische Tochtergesellschaft gegründet, ist dies dank Digitalisierungsoffensive der österreichischen Bundesregierung als Online-Firmengründung mit Notariatsakt per Video möglich. ""Unternehmen sind häufig überrascht, dass die Rahmenbedingungen für eine Ansiedelung doch anders sind als in Deutschland. Der Teufel steckt im Detail"", sagt Tanja Spennlingwimmer, Leiterin Investorenmanagement der Standortagentur Oberösterreich.

Zu beachten sind etwa die Gewerbeordnung und Entsendungen von Arbeitskräften. Ähnliche Feinheiten gibt es bei den Finanzierungsinstrumenten. ""In Österreich wird in der Regel 'unechtes' Factoring praktiziert, das bedeutet, dass von der Factoringgesellschaft in der Regel nicht 100 Prozent der Kreditrisiken übernommen werden"", sagt Frank. Die Alpenrepublik setzt in der Standortpolitik stark auf regionale Innovationsschwerpunkte. ""Für Investitionen von Hightech-Unternehmen und IT-Firmen sowie Forschungseinrichtungen internationaler Unternehmen ist die Nähe zu Kooperationspartnern aus der Wirtschaft und Forschung ein starker Anreiz,"" sagt Spennlingwimmer. Beispiele sind die Raumfahrtforschung der TU Graz für die jüngste Mission der Europäischen Raumfahrtagentur und das Institut für Machine Learning unter Leitung des bayerischen Experten für künstliche Intelligenz, Sepp Hochreiter an der Linzer Johannes Kepler Universität, das mit Bosch oder Merck Maschinenlernlösungen entwickelt.
Kleine und mittlere Betriebe werden besonders gefördert

Im internationalen Wettbewerb zieht die 14-prozentige Forschungsprämie gerade große Unternehmen ins Land. Denn anders als Deutschland fördert Österreich nicht nur Ausgaben für Personal, sondern auch für Einrichtungen, Immobilien und Technik, und dies ohne finanzielle Obergrenze. Der Chiphersteller Infineon investiert beispielsweise über 1,6 Milliarden Euro bis 2025 in eine voll automatisierte Halbleiterfertigung samt Entwicklung am Hauptsitz in Villach (Kärnten).

Junge, kleine und mittlere Betriebe unterstützt die Förderbank AWS beim Aufbau durch 80-prozentige Garantien für Investitions- und Betriebsmittelkredite. Üblich sind auch grenzüberschreitende Garantien von Hausbanken deutscher Betriebe. ""Es gibt viele Varianten, aber meist kommt ein Finanzierungsmix von einem größeren Investitionskredit über die deutsche Muttergesellschaft und einer Kontoverbindung mit Kreditlinie am jeweiligen Ort zustande"", weiß Banker Frank.

Häufig genutzt werden auch grenzüberschreitende Konsortialkredite mit mehreren Banken. Egal, ob Materialforschung, IT-Branche oder Maschinenbau, das beherrschende Thema ist auch in Österreich der Personalengpass. Vor der Krise war Mangel an Fachkräften eine Hürde für weiteres Wachstum. Zu Beginn des Jahres 2020 warnte etwa die Wirtschaftskammer vor 10 000 fehlenden IT-Kräften. Hier sollen künftig eine neue duale IT-Ausbildung sowie Fachhochschul-Lehrgänge Abhilfe schaffen.

In der Industrie ist die Lage weniger dramatisch als hierzulande. ""Gerade mit dem HTL-Abschluss als nichtakademische, hoch qualifizierte Ausbildung können wir hier bei deutschen Technologie-Unternehmen punkten, auch im Vergleich zu den östlichen Nachbarn"", sagt Spennlingwimmer. In gewissen Berufsgruppen lägen die Gehälter einen Tick unter jenen in Teilen Deutschlands.

Auf Mitarbeitersuche ist auch Hanomag in Oberösterreich. ""Gutes Personal zu finden ist an allen Standorten unser größtes Problem. Wir haben nun Mitarbeiter aus Deutschland am Ort, suchen aber dringend Fachkräfte im Metallbereich"", sagt Seehafer. Zu den Besonderheiten zählen die in der Alpenrepublik üblichen 14 Gehälter sowie die hohe Flexibilität bei der Beendigung von Arbeitsverhältnissen. So kann eine Kündigung seitens der Arbeitgeber ohne Angabe von Gründen erfolgen, die Kündigungsfrist liegt je nach Vertragsdauer zwischen sechs Wochen und fünf Monaten. Österreich und vor allem Wien waren lange als Drehscheibe beliebter Standort von Firmenzentralen für Süd-Ost-Europa, so haben etwa Rewe und Henkel ihre überregionalen Sitze hier. Nun steigt die Zahl asiatischer Firmen in Ostösterreich, 2019 öffnete etwa die Industrial and Commercial Bank of China eine Niederlassung in Wien. Eine weitere Expansion ist auch für Hanomag-Eigentümer Seehafer denkbar, das Firmengelände in Taufkirchen macht eine Erweiterung möglich.";https://www.sueddeutsche.de/wirtschaft/auslandsgeschaeft-lukrative-nachbarschaftsgeschaefte-1.4973054;sz.de;Christiane Kaiser-Neubauer
01.07.2020;Die Vermessung der SZ;"Eine Sicherheitslücke im Betriebssystem iOS erregte im Juli 2016 Aufsehen: Mehrere Hundert Millionen Apple-Geräte waren gefährdet, eine einfache Multimediadatei reichte aus, um Schadcode auf den Geräten auszuführen. Per iMessage oder MMS konnten etwa iPhone-User so Opfer eines Hacks werden.

Das Digital-Ressort der Süddeutschen Zeitung veröffentlichte am frühen Abend des 21. Juli dazu eine Meldung. ""Sie sollten Ihr Betriebssystem für iPhone und Mac sofort aktualisieren"", war die Nachricht überschrieben. Fünf kurze Absätze, etwas mehr als 2000 Zeichen - im Normalfall kaum erwähnenswert. Allein am Abend der Veröffentlichung jedoch wurde der Artikel mehr als 150 000 Mal aufgerufen, ein Extremwert. Die Quelle damals: unbekannt - und die Aufregung in der Redaktion entsprechend groß. Über welchen Kanal kamen derart viele Leser, deren Ursprung wir nicht ausmachen konnten?

Derlei Auffälligkeiten häuften sich. Im Wochentakt stiegen die Aufrufe einzelner Artikel extrem an, die Herkunft der Leserschaft blieb dabei stets unklar. Die Suche nach der Quelle führte schließlich zu einem voreingestellten Programm auf Apple-Geräten, in dem Nutzern journalistische Inhalte angezeigt wurden. Die Auswahlkriterien, welche Artikel Chancen auf Reichweite haben könnten, blieben allerdings schleierhaft. Die Auswirkung auf unseren Journalismus damit ebenso.

Und dennoch war die Erkenntnis wichtig, dass es sich um einen Zufall handelte. Denn wir interessieren uns für Sie. Für Sie als Leser der Süddeutschen Zeitung. Ob Sie lieber große Reportagen lesen oder Essays präferieren beispielsweise. Ob Sie eher die Bundesliga-Berichterstattung gebannt verfolgen oder Analysen zu internationalen Konferenzen bis zur letzten Zeile lesen.

Das alles ist für eine Redaktion spannend zu wissen. Denn für Sie als Leser wollen wir möglichst intensiv über jene Themen berichten, die Sie wirklich bewegen. Die Sie im Alltag beschäftigen, bei denen Sie Hintergründe benötigen, und auch über jene, von denen Sie sich unterhalten fühlen. Hätten wir nicht erkannt, dass einige unserer Artikel rein zufällig ungewohnt häufig gelesen würden, hätte das möglicherweise dazu geführt, dass wir eine Leserschaft in den Fokus nehmen, die nicht jener entspricht, die uns im Alltag die Treue hält.

Natürlich wollen wir auch Ihnen nicht nach dem Mund reden. Süddeutsche Zeitung, das kann und soll immer auch Herausforderung bedeuten, Überraschung und Berichterstattung über Sachverhalte, die keine große Popularität genießen, die aber dennoch wichtig sind. Für die Gesellschaft und unser Zusammenleben.
75 Jahre SZ Banner

Die Süddeutsche Zeitung erreicht mit ihren digitalen Angeboten bis zu 20 Millionen Menschen im Monat. Mehr als 100 Millionen Mal kommen sie im selben Zeitraum zu uns, um sich über das aktuelle Weltgeschehen zu informieren oder Hintergründe zu lesen. Während vor Jahrzehnten klar war, dass die Inhalte dort gelesen werden, wohin die Redaktion sie platziert, in der Zeitung, so ist der digitale Raum zu einem ausdifferenzierten Konstrukt aus Publikationen und Verbreitungsstrukturen geworden, deren Kenntnis unerlässlich ist.

Viele, vor allem loyale und interessierte Leserinnen und Leser, erreichen wir nach wie vor über die Homepage. Enorm viele Menschen kommen jedoch auch über Suchmaschinen zu uns, insbesondere über Google, wenn sie gezielt an Informationen gelangen wollen. Und neben den schon klassischen sozialen Netzwerken wie Facebook, Instagram oder Twitter drängten in den vergangenen Jahren mehr und mehr Apps und Angebote auf den Markt, die journalistische Inhalte zwar nicht erstellen, sie aber verbreiten. Services wie Upday beispielsweise, die Pocket-Empfehlungen in Firefox, Flipboard, Sony News, Microsoft News, News Republic (eine App des chinesischen Unternehmens Bytedance) oder das schon beschriebene Angebot von Apple.

Um zu wissen, wer Sie sind und wofür Sie sich interessieren, stehen uns als Redaktion verschiedene Möglichkeiten offen. Einerseits natürlich ein großer Erfahrungsschatz durch jahrzehntelange Arbeit mit Inhalten und Publizistik. Andererseits unzählige Gespräche, Leserbriefe und Rückmeldungen zur Zeitung, zur Homepage oder der digitalen Ausgabe.

Natürlich aber arbeiten wir auch mit Daten. Ein halbes Dutzend Kolleginnen und Kollegen aus dem Analyseteam kümmert sich darum, die richtigen Daten zu erfassen, aufzubereiten und in Zusammenarbeit mit Redaktion und Produktteams sinnvoll zu nutzen. Wir messen beispielsweise, wie häufig Artikel aufgerufen werden. Oder aber, wie lange auf einzelnen Inhalten verweilt wird. Auch interessiert uns, über welche Wege Leser zu uns finden, ob sie wiederkommen oder wann ein Abonnement abgeschlossen wird. Nicht auf individueller Ebene selbstverständlich, Datenschutz ist uns bei alldem äußerst wichtig, aber anonymisiert und als Gruppe. Web-Analysten, Data Scientists, Statistiker und auch Experten für Machine Learning versuchen zusammen, Daten lesbar und schlussendlich auch nutzbar zu machen. Warum machen wir das?

Wer die Welt vermisst, versucht, sie fassbarer zu machen. Sie weniger dem Reich des Mystischen zu überlassen, sondern den Versuch zu unternehmen, sie besser zu verstehen. Einen anderen Blickwinkel auf sie zu ermöglichen, der neue Erkenntnisse in sich bergen kann.

So wird quantifiziert, summiert, kategorisiert und analysiert - alles mit dem Ziel, den Horizont zu erweitern und tiefer in die Zusammenhänge einzudringen, die unsere Welt so sein lassen, wie sie ist.

Der Quantifizierung der Welt skeptisch gegenüberzustehen, ist dabei allzu verständlich. Denn Tracking ist in vielerlei Hinsicht mit negativen Erfahrungen und Vorstellungen verbunden. Begriffe wie Metrik, Benchmark, Performance Indicator oder Accountability starteten ihren Siegeszug in der Management-Literatur der 1960er-Jahre. Als Megatrend haben sie in den vergangenen Jahrzehnten zahllose gesellschaftliche Bereiche beeinflusst und geprägt - seien es Bildung, Gesundheitsversorgung, Sicherheit oder auch die Publizistik.

Der Drang, jedes noch so kleine Detail zu vermessen und Veränderung darstellbar zu machen, resultiert dabei aus einer sich rasant verändernden Welt. Je schneller und grundlegender sich diese Veränderung vollzieht, desto stärker ist das Verlangen nach objektiven Kriterien, Orientierung und einer Bestätigung des eigenen Handelns. Im selben Moment, in dem den Lösungen bestehender Eliten kein Vertrauen mehr entgegengebracht wird, sollen Daten Halt bieten.

Die Auswirkungen der von Daten durchwirkten Welt spüren wir im Alltag. Sei es bei lästiger Werbung, die uns personalisiert wochenlang im Internet verfolgt, oder der Frage nach der Rabattkarte an der Supermarktkasse. Andere Beispiele wie gezielt ausgespielte Wahlwerbung oder, im Extremfall, Überwachungs-Apps wie in China, die umfassende Bewegungsprofile erstellen, lassen uns vor jedweder Messung des eigenen Verhaltens zurückschrecken - selbst wenn sie datenschutzkonform und anonym erfolgt.

Redaktionelle Arbeit mit Daten hat mit alldem nichts zu tun. Erkenntnisse darüber, welche Artikel, Themen und Formate mehr oder weniger intensiv gelesen werden, dienen schlichtweg dazu, besser zu verstehen, wie die Vorstellungen der Leserschaft mit unserer Realität übereinstimmen. Oder anders ausgedrückt: Wir wollen nicht besser als Sie selbst wissen, wofür Sie sich interessieren.

Als Redaktion arbeiten wir dateninformiert, nicht datengetrieben. Daten diktieren nicht, mit welchen Themen wir uns beschäftigen und worüber wir berichten. Sie liefern lediglich Hinweise, Denkanstöße und können bei der Themenplanung im Alltag sowie der mittel- und langfristigen publizistischen Weiterentwicklung eine Stütze sein. Daten und Zahlen stellen dabei selbstverständlich nur eine Form von Evidenz dar. Erfahrung, Gefühl, theoretische Konstrukte und qualitative Forschung spielen ebenfalls eine entscheidende Rolle.

Grundsätzlich sind Daten immer nur ein Hilfsmittel. Daten liefern aus sich heraus weder Antworten und Lösungen, noch sind sie der langjährigen Erfahrung einer Redaktion überlegen. Sie können eine Information darüber liefern, ob die eigene Erwartung der Realität entspricht. Sie können Feedback geben und ein Kompass dafür sein, den eigenen Weg zielgerichteter zu gehen.

Wir möchten mit allen gewonnenen Informationen Tag für Tag den bestmöglichen Journalismus für unsere Leserinnen und Leser schaffen. Daten helfen uns dabei, das ist unumstritten. Aber eben nicht nur Daten. ""Nicht alles, was zählt, kann gezählt werden - und nicht alles, was gezählt werden kann, zählt"", schrieb der Soziologe William Bruce Cameron in seinem 1963 erschienenen Buch ""Informal Sociology: A Casual Introduction to Sociological Thinking"". Es ist ein Zitat, das wir uns immer wieder vor Augen führen sollten, wenn wir die Welt vermessen.";https://www.sueddeutsche.de/kolumne/sz-daten-journalismus-1.4947505;sz.de;Christopher Pramstaller
30.05.2020;Wenn Chatbots sich radikalisieren;"Das Ideal, an dem Roboter und künstliche Intelligenz (KI) gemeinhin gemessen werden, ist das der Menschlichkeit. Das scheint für Science Fiction Filme der 1920er Jahre ebenso zu gelten wie für unseren heutigen Alltag, den Sprachassistenten und Smart-Apps schon längst mitbestimmen. Im April verkündete Facebooks KI-Abteilung, mit ""Blender"" einen Chatbot entwickelt zu haben, der in Gesprächen besonders menschlich agiere und damit ähnliche Anwendungen übertreffe. Dafür wurde das Programm mit mehr als 1,5 Milliarden Gesprächseinträgen von öffentlich zugänglichen Portalen wie Reddit gespeist. Ein Intensiv-Training für unterschiedlichste Gesprächsthemen, aber auch für natürliche Sprach- und Verhaltensmuster.

Die frei verfügbaren Datenmengen haben aber auch ihre Schattenseite: So beinhalteten die Antworten der Chatmaschine in Tests immer wieder Beleidigungen, diskriminierende Aussagen und Fake-News. Dennoch - oder gerade deshalb - konnte Blender offenbar durch seine Menschlichkeit bestechen. 67 Prozent der Tester gaben an, der Bot sei in der Unterhaltung überzeugender und menschlicher als sein Google-Äquivalent Meena, der als einer der bisher am weitesten entwickelten seiner Art gilt. 49 Prozent konnten Blender nicht von einem menschlichen Gesprächspartner unterscheiden.

Chatbots kommen vor allem im digitalen Kundenservice von Unternehmen zum Einsatz, wo sie Auskunft zu bestimmten Themenbereichen geben sollen. Wer per Direktnachricht über E-Banking, Telefontarife oder Versicherungskonditionen informiert werden möchte, spricht dabei immer seltener mit echten Menschen. Entwicklern haben die Bots in den vergangenen Jahren zunehmend als Experimentierfläche gedient, um künstliche Intelligenzen Menschen mehr anzunähern. Microsofts ""Tay"" oder ""Mitsuku"" von Pandorabots wurden anhand von Chats trainiert, die Millionen von Nutzern mit ihnen führten. Neben Wissen spielen auch Persönlichkeit und Empathie eine entscheidende Rolle dafür, dass Menschen die Unterhaltung mit der Maschine als bereichernd empfinden. Durch Machine Learning können sich die Programme während Gesprächen laufend und quasi eigenständig weiterentwickeln. Je mehr sie mit Menschen kommunizieren, desto mehr lernen sie von ihnen.
Beleidigungen und Propaganda

Doch genau das hat auch in der Vergangenheit schon zu Schwierigkeiten geführt. Das Microsoft-Produkt ""Tay"" wurde im direkten Gespräch mit Nutzern mit Beleidigungen und Propaganda überhäuft und integrierte diese entsprechend in sein Vokabular. Bei den enormen Datenmengen an bereits bestehenden Gesprächen, mit denen Blender gefüttert wurde, ist das offenbar nicht anders. Die Gesprächskultur in den sozialen Medien oder Foren wie Reddit lebt nicht zuletzt von Beleidigungen oder der Verbreitung von Falschinformationen. Das macht manuelle Zensuren unabdingbar, wenn man verhindern möchte, dass Chatbots Elemente wie Rassismus, Sexismus oder Verschwörungstheorien übernehmen und weiterverbreiten. Die Workforce, die dafür nötig wäre, kann allerdings nicht einmal Facebook bewältigen.

Im Blog Israellycool wurden zuletzt beunruhigende Auszüge eines Chatprotokolls mit Blender veröffentlicht. Auf die Frage, was er von Juden hält, antwortet der Bot es seien ""furchtbare Menschen"", die ständig nicht-jüdische Menschen umbringen würden. Solche antisemitischen Aussagen sind neben anderen Hassbotschaften Alltag in Foren wie Reddit, und jetzt wohl auch Bestandteil von Blenders Gesprächsrepertoire. Im Gegensatz zu expliziten Schimpfwörtern dürften die auf diese Weise weiter einfließenden Vorurteile deutlich schwieriger aus dem Programm herauszufiltern sein - und sind dabei umso fataler in ihrer Weiterverbreitung. Dass Blender in diesem Fall antisemitisch agierte, lässt laut Facebook keine unmittelbaren Rückschlüsse darauf ziehen, wie Judentum online am häufigsten kommentiert wird. Die generierten Antworten seien nicht direkt übernommen, sondern basierten auf einer komplexen Kombination unterschiedlicher Gesprächsparameter.
Überwachung und Verantwortung

Marion Weissenberger-Eibl, Leiterin des Fraunhofer-Instituts für System- und Innovationsforschung, sieht das Problem im Lernprozess selbst. Es sei nach wie vor schwierig für KI, sprachliche und inhaltliche Feinheiten richtig einzuordnen. Sie sagt: ""Um zu vermeiden, dass Chatbots in dieser Art und Weise agieren, müsste man ihnen zuallererst beibringen, was Rassismus ist. Eine bisher ungelöste Problematik, denn ein Chatbot kann beispielsweise nicht zwischen schwarzem Humor und Rassismus differenzieren."" Für sie stelle sich daher die Frage, wie das Lernen des Programms überwacht werden kann - und auch, wer dafür die Verantwortung trägt.

Das Forscherteam hinter Blender vermeldete anlässlich des Launchs, dass man sich mit Schutz vor toxischer Sprache auseinandergesetzt habe, aber weiterhin viel zu tun sei. Blender wurde als Open-Source-Software veröffentlicht und steht demnach per Code zur freien Verfügung. Auf Anfrage der SZ teilt Facebook mit, dass es sich bei dem Blender-Bot um ein reines Forschungsprojekt handle, das als solches nicht für den Markt sondern vorerst nur der Entwicklercommunity freigegeben wurde, um gemeinsam die Gesprächs-KI zu optimieren. Die von Israellycool verwendete Version sei vom Unternehmen Cocohub bereitgestellt worden, welches den Code abgeändert und so auch eingebaute Schutzmechanismen entfernt habe. Diese würden in der Originalversion verhindern, dass auf die Frage, was Blender vom Judentum halte, eine Antwort generiert wird. Für ein manuelles Aussortieren toxischer Inhalte, so Facebook, sei der eingespeiste Reddit-Datensatz zu groß und daher ungeeignet. Auch deswegen sei der Bot zum jetzigen Zeitpunkt weder für Privatpersonen noch für einen kommerziellen Einsatz gedacht.";https://www.sueddeutsche.de/digital/chatbot-blender-facebook-1.4922049;sz.de;Marisa Gierlinger
18.03.2020;Der Traum vom absoluten Hörerlebnis;"Wie bei nahezu allem derzeit spielt das Coronavirus auch in Sonys erster öffentlicher Präsentation zur Playstation 5 eine tragende Rolle. Eigentlich wollte das Unternehmen diese Woche auf der Game Developers Conference in San Francisco über die kommende Spielkonsole reden. Wegen der Pandemie wurde die Veranstaltung abgesagt, daher findet der Vortrag von Sonys leitendem Entwickler für die Playstation 5, Mark Cerny, nur online statt.

Und obwohl Social Distancing das Gebot der Stunde ist, hat es sich der Konzern augenscheinlich nicht nehmen lassen, echte Menschen zu der Präsentation einzuladen. Ob es sich bei den fünf schattenhaften Schultern und Hinterköpfen um Journalisten, Sony-Mitarbeiter oder doch nur eine Animation handelt, ist unklar. Auffällig ist zumindest, dass sich keiner der Köpfe während des gesamten 52-minütigen Videos auch nur eine kurze Unaufmerksamkeit erlaubt. Dabei macht Cerny es dem durchschnittlichen Playstation-Spieler alles andere als leicht, seinen Ausführungen zu folgen. Cerny redet darüber, wie die Verteilung von Daten auf eine Festplatte den Rechenprozess verlangsame. Es geht darum, wie Teile des Arbeitsspeichers der Playstation 4 ungenutzt bleiben, während die nächsten 30 Sekunden des Spiels berechnet werden. Und er rechnet vor, dass die 36 Compute Units (CUs) in der Playstation 5 gemessen an der Anzahl von Transistoren etwa so leistungsfähig seien wie 58 CUs in der Playstation 4. Tatsächlich stecken 18 CUs in der PS4 (36 in der PS4 Pro). Übersetzt soll das heißen, dass die PS5 ein Vielfaches der Grafikleistung der Vorgängerkonsole abrufen kann. Wie viel genau, hängt von weiteren Faktoren ab.
Keine Ladebildschirme mehr

Insgesamt, sagt der Entwickler, komme der Grafikchip der Playstation 5 auf eine Rechenleistung von 10,3 Teraflops. Die Maßeinheit beziffert, wie viele Billionen Befehle der Chip pro Sekunde ausführen kann. Sie wird häufig benutzt, um die Rechenleistung der Spielkonsolen untereinander zu vergleichen. Demnach wäre die neue Xbox Series X etwas schneller, laut Microsoft erbringt sie rund 12 Teraflops. Die Playstation 4 kommt auf 1,8 Teraflops.

Wie bereits vorab von Sony bestätigt, steckt in der Playstation 5 ein Solid-State-Drive (SSD). Das hätten sich viele Spieleentwickler gewünscht, sagt Cerny. Dank der SSD soll die Spielkonsole nur 0,27 Sekunden benötigen, um 2 Gigabyte Daten zu laden. Zum Vergleich: Die Festplatte der PS4 braucht rund 20 Sekunden für die Hälfte. ""Es wird keine Ladebildschirme mehr geben. Die Schnellreise in Spielen wie Spider-Man wird so schnell, dass wir den Übergang künstlich verlangsamen müssen"", sagt Cerny. Die SSD sei ein ""Game Changer"", für Spieler und für Spieleentwickler.
Realistische Texturen, Spiegelungen und Schatten

Die Playstation 5 soll 825 Gigabyte Speicherplatz bieten. Zusätzlich können Kunden den Speicher erweitern, wenn er nicht ausreicht. Welche SSDs mit der Konsole kompatibel seien, müsse Sony erst noch testen, sagt Cerny. Das werde allerdings erst nach dem geplanten Verkaufsstart Ende 2020 passieren.

Die neue Playstation wird rückwärtskompatibel mit den Spielen der PS4 sein. Eine neue Technologie, genannt ""Geometry Engine"", soll Oberflächenstrukturen in Spielen künftig noch realistischer erscheinen lassen. Einen ähnlichen Effekt soll das sogenannte Ray-Tracing haben. Dadurch kann der Grafikchip den Einfall von Lichtstrahlen in Echtzeit berechnen und so Spiegelungen und Schatten wirklichkeitsnäher darstellen.

Immer wieder während seines Vortrags kommt Entwickler Cerny oberlehrerhaft rüber. Die Spieleentwickler müssten natürlich weder die Geometry Engine noch Ray-Tracing unbedingt beim Programmieren berücksichtigen. ""Aber sie stellen eine Chance für Entwickler dar"", sagt er und lächelt so, dass das eher wie eine Drohung klingt. Wer nicht mitzieht, verliert.
Die Soundeinstellungen sollen individualisierbar sein

Den letzten Teil seiner Präsentation widmet er der Vision, die Sony mit der Playstation 5 verfolge. Sein Ziel und das des Entwicklerteams sei es gewesen, Gaming eine neue Tiefe zu geben. So seien sie auf das Konzept gekommen, das Sony als ""Tempest 3D Audiotech"" bezeichnet. Cerny zufolge sollen die Spiele auf der Playstation 5 für alle Spieler großartig klingen - egal, ob sie den Klang über Kopfhörer, eine Soundbar oder die Lautsprecher des Fernsehgeräts abspielen. Außerdem soll jedes Geräusch klingen, als stünde der Spieler inmitten des Computerspiels.

Um diese blumigen Versprechen zu erläutern, stehen Cerny keine Zahlen von CUs oder Teraflops zur Verfügung. Stattdessen bedient er sich am Jargon der Akustik: HRTF - die Abkürzung steht für ""Head-Related Transfer Function"" und beschreibt stark vereinfacht, dass jeder Mensch aufgrund seiner Ohr-, Kopf- und Rumpfform Geräusche unterschiedlich wahrnimmt. Mit der Hilfe von 22 Lautsprechern kann Sony im Labor Ohren und Gehörgänge eines Spielers ausmessen und verspricht, so die optimalen Soundeinstellungen für denjenigen zu finden.

Weil das in der Praxis für Millionen Spieler weltweit unmöglich reproduzierbar ist, soll es anfangs fünf HRTF-Profile für die neue Playstation geben, zwischen denen man wählen kann. In Zukunft, sagt Cerny, hoffe er aber, dass sich die PS5 zum Beispiel dank Videos oder Fotos der Ohren und Machine Learning auf jedes Gehör individuell einstellen kann. Sollte sich dieser Traum vom absoluten Hörerlebnis durchsetzen, könnte das auch für andere Branchen der Unterhaltungsindustrie interessant sein. ";https://www.sueddeutsche.de/digital/playstation-5-sony-technik-1.4850703;sz.de;Caspar von Au
01.03.2020;Was ist mein Haus wert?;"Eine Hausbewertung? Es kostet nichts, geht blitzschnell und verpflichtet zu gar nichts. Das versprechen viele Immobilienverkäufer, -vermittler und -finanzierer. Dass das nicht stimmen kann, müsste sich in Zeiten, in denen Daten die neue Währung sind, eigentlich jeder denken können. Und dennoch werden die Angebote angenommen, sie sind ja auch zu verlockend. Und wie funktioniert das? Ein digitaler Versuch bei ein paar Dienstleistern.

Was ist ein altes Haus wert, gebaut in den 60er-Jahren, unsaniert, in einer eher unspektakulären süddeutschen Gegend? Zum Beispiel bei Immobilienscout 24. Das Internetportal wirbt mit ""Schnell und bequem zur Immobilienbewertung"". Dazu muss man die Postleitzahl eingeben, Wohn- und Grundstücksfläche, Zimmerzahl und gewünschten Verkaufszeitraum. Und dann kommt natürlich erst mal keine Schätzung, sondern die Aufforderung, Namen und Adresse einzugeben - die Daten, bitte! Der ""Service"" besteht dann aus Maklerempfehlungen.

Das Angebot kommt bei der Mehrheit der Kunden gut an, heißt es auf der Homepage, um genau zu sein, bei ""83,27 Prozent"". Die Reaktion der Verbraucher, die diesen Dienst in Anspruch genommen haben, geht von ""Absolut toll!"" und ""Sehr freundlich"" über ""Es dient einzig zur Datensammlung"" bis hin zu ""Hier ist rein gar nichts umsonst! Ohne vorherige Zusage eines Mandatsübertrags passiert hier nix"".

Ein anderer Versuch, bei Bewerte-deine-immobilie-kostenlos.de, ein ""Service von Königskinder Immobilien GmbH"". Der Immobilienmakler mit Sitz in Stuttgart fragt die üblichen Faktoren wie Fläche und Zimmer ab, dazu noch andere Dinge wie Baujahr und Parkmöglichkeiten, dann will man auch hier Namen und Adresse. Das Ergebnis der Schätzung kommt per Mail, wird versprochen. Tut es auch, nach einem Telefongespräch. Der mögliche Verkaufspreis liege etwa zwischen 197 000 und 251 000 Euro. Mündlich weist eine freundliche Mitarbeiterin zuvor darauf hin, dass man genaue Bewertungen natürlich nicht anhand von einigen Daten vornehmen könne. Man schicke aber gern einen Berater vorbei, der sich die Immobilie genauer ansehe, kostenlos. Die grobe Schätzung des Immobilienwerts beruhe auf Vergleichswerten bisher getätigter Verkäufe.

Oder Immoverkauf24.de: Das Onlineportal wirbt mit einem umfassenden Beratungsservice. Auf der Homepage heißt es: ""Sie möchten den Wert Ihres Haus ermitteln? Jetzt starten & in 1 Minute kostenlos den aktuellen Wert erhalten"". Wieder das gleiche Spiel - Daten eingeben, die Adresse abfragen. Immerhin kommt auch hier der Hinweis, dass es sich nur um einen ersten Anhaltspunkt für den aktuellen Wert des Hauses handele, eine exakte Wertermittlung gebe es dann bei einem Bewertungstermin vor Ort, gratis.

Und so weiter und so weiter, Vermarktung statt Bewertung, Portale erhalten Kundenadressen und - bei erfolgreichem Hausverkauf - oft einen Anteil der Maklercourtage.
Ob Wohnfläche, Nachbarschaft oder Lärmstatistik: Der Algorithmus lernt daraus

Aber im Netz gibt es inzwischen auch ausgefeiltere Angebote für Eigentümer und Immobilieninteressierte, zumindest in einigen Großstädten. So hat das Schweizer Start-up Pricehubble 2016 ein auf künstlicher Intelligenz basierendes Bewertungstool entwickelt. Das Unternehmen, das in Deutschland seit 2019 mit der Firma Scoperty zusammenarbeitet, sammelt und strukturiert große Datenmengen, um Marktpreise und -mieten zu errechnen. Scoperty ermittelt Schätzwerte, sodass jeder sehen kann, was ein Haus, Grundstück oder eine Wohnung wert ist - die eigene oder eine fremde Immobilie. Das geht bisher nur in Berlin, Hamburg, München, Köln, Frankfurt und Nürnberg.

Ein Beispiel: Man gibt eine bestimmte Adresse ein, etwa eines Mehrfamilienhauses in der Münchner Kaulbachstraße, und erhält für die kleinste dort registrierte Wohnung den Schätzwert von 767 929 Euro. Dieser beruht unter anderem auf realen Transaktionen, etwa Verkaufsdaten vergleichbarer Immobilien. Macht der Eigentümer genauere Angaben, etwa zu Renovierungen und Ausstattung, fließen diese in die Datenbasis ein und verbessern so die Schätzwerte.

Je mehr Menschen bei diesem System Informationen teilen, umso besser wird es. Denn Pricehubble beziehungsweise Scoperty nutzt bei der Datenverarbeitung Machine Learning. ""Der Algorithmus ist ein lernendes System, das heißt, dass er mit jeder weiteren Information, mit jeder Arbeitsschleife exakter arbeiten wird"", sagt Scoperty-Mitgründer und Geschäftsführer Michael Kasch.

In die Datenbasis fließen ganz unterschiedliche Fakten ein. Zum Beispiel ""Adressinformationen, Koordinaten oder 3-D-Modelle beispielsweise aus den Katastern der Landesvermessungsämter oder des Bundesamts für Kartografie und Geografie"". Außerdem sozialwissenschaftliche und umweltbezogene Informationen, etwa Lärmstatistiken, die über den Geräuschpegel am Haus Auskunft geben und Informationen über die Anbindung an das Stadtzentrum. Auch Angaben über die Arbeitslosigkeit oder die Kriminalitätsrate in einem Viertel können für das Unternehmen interessant sein. Man stehe erst am Anfang der Möglichkeiten, meint der Scoperty-Chef. Diese Informationen seien bisher weder für Eigentümer und potenzielle Verkäufer noch für Suchende einfach zu bekommen.

Auf diesem Marktplatz können Eigentümer ihre Immobilie nicht nur schätzen, sondern über ein Maklernetzwerk auch verkaufen. Kaufinteressierte wiederum können nach Immobilien suchen und Gebote abgeben, auch wenn ein Haus oder eine Wohnung nicht zum Verkauf steht, in der Hoffnung, dass sich ein Eigentümer zum Verkauf bewegen lässt. Und sie können eine Finanzierung abschließen - über die Interhyp.

Scoperty ist ein Gemeinschaftsunternehmen von Pricehubble, dem Immobilienbewerter Sprengnetter und der ING Bank, nach eigenen Angaben Deutschlands größter Vermittler von privaten Baufinanzierungen. Hinter der Schweizer Firma Pricehubble wiederum stehen ebenfalls große Namen - zu den Geldgebern dieses Start-ups gehören die Swiss Life Holding, der größte Lebensversicherungskonzern der Schweiz, und die Helvetia Holding, ebenfalls ein Versicherer.

Wieso interessiert sich die Assekuranz für dieses Immobiliengeschäft? Sie erweitert ihr Angebot und gewinnt neue Kunden, schließlich brauchen Eigentümer eine Wohngebäude-, Haftpflicht-, Hausrat- und vielleicht auch eine Rechtsschutz-Police. Ähnliche Beweggründe haben Banken: Der Kontakt zum Kundenberater wird schneller, weil automatisiert, hergestellt, der ""Berater"" kann die passende Finanzierung verkaufen oder - falls es nicht zum Immobilienverkauf kommt - alternative Häuser und Wohnungen anbieten.

Von der Digitalisierung sollen viele profitieren. Eines der Versprechen ist die schnellere Kaufabwicklung, ein anderes eine höhere Markttransparenz. Und die, wird immer häufiger kritisiert, fehlt in Deutschland. Umfangreiche Datensammlungen zum deutschen Immobilienmarkt gibt es bisher nur bei den Gutachterausschüssen; diese erfassen alle Immobilientransaktionen. Öffentlich zugänglich sind diese Daten aber nicht oder nur eingeschränkt. Selbst um Bodenrichtwerte zu bekommen, muss man oft zahlen; in München kostet eine Einzelauskunft beispielsweise 30 Euro, ein Immobilienmarktbericht von 2018 70 Euro.

Scoperty will für alle 40 Millionen Wohnimmobilien in Deutschland Schätzwerte ins Netz stellen. Um die Pläne voranzutreiben, hat das Unternehmen gerade mit der LBS Bayerische Landesbausparkasse und der Citigrund Immobilien GmbH, einer Münchner Maklerfirma, Kooperationsverträge geschlossen. Dadurch, dass Scoperty die Verkehrswerte ausweise, erhalte man einen transparenten Überblick über die Preise in den jeweiligen Lagen, meint Peter Zeiler, Gebietsdirektor München der LBS. Und Makler erfahren, wer mit dem Gedanken spielt, sich von seiner Immobilie zu trennen. Bei Citigrund Immobilien heißt es, man freue sich über jeden Verkäufer, der über Schätzwerte schon ein Gefühl für den Verkaufspreis habe. Und über Kunden, die bereits eine Finanzierungsbestätigung haben. ""Banken nehmen sich aktuell mit Finanzierungszusagen sehr viel Zeit und sind sehr kritisch geworden"", sagt Geschäftsführer Beppo Schwimmer in einer Unternehmensmitteilung.
Mensch oder Maschine, wer kennt Kundenwünsche besser?

Ob das mit der Transparenz klappt, wird sich zeigen. Künstliche Intelligenz aber kann mitunter den Service verbessern, denn durch Machine Learning wissen die Unternehmen rascher, wer gern was wo kaufen möchte. Und KI kann sogar erfahrene Makler übertrumpfen. 2016 hat Inman, ein amerikanischer Anbieter von Online-Nachrichten, einen Wettbewerb Mann gegen Maschine (Broker gegen Bot) veranstaltet. Dabei traten drei Tage lang ein Makler und ein Bot gegeneinander an in der Frage, wer Käuferpräferenzen besser vorhersagen kann. Der Bot gewann.

Für viele ist es nur eine Frage der Zeit, bis künstliche Intelligenz wesentliche Aufgaben übernehmen wird - und nicht nur die. In der gesamten Immobilienbranche werde KI die Effizienz verbessern und Entscheidungsprozesse verändern, prophezeit Pricewaterhouse-Coopers (PwC) Deutschland. Der Wirtschaftsdienstleister hat gemeinsam mit dem Proptech-Unternehmen Evana ein Projekt gestartet, das die Potenziale von KI in der Immobilienbranche ermitteln soll.

""Sind Immobilienberater heute noch teilweise tagelang damit beschäftigt, im Rahmen von Immobilientransaktionen einzelne Informationen aus Verträgen oder Abrechnungen zusammenzutragen, können diese Aufgaben künftig mithilfe von KI innerhalb von Sekunden erledigt werden"", heißt es bei PwC in einem Beitrag zur digitalen Transformation. Die Geschwindigkeit und Präzision bei der Bewertung unzähliger Immobilienkennzahlen lasse sich enorm steigern, bei einer deutlich geringeren Fehlerquote. Im Immobilienmanagement reiche der mögliche Einsatz künstlicher Intelligenz von personalisierten Kundenverträgen über das Vertragsmanagement bis hin zur intelligenten Anlagensteuerung. Auch die Immobilienbewertung werde sich verändern.

Das meint auch Rüdiger Hornung, Geschäftsführer des Immobilienberaters Wüest Partner Deutschland. Automatisierte Bewertungsmodelle würden bereits für Portfoliobewertungen und bei wohnwirtschaftlichen Objekten eingesetzt. Damit spare man eben Zeit, Geld und Personalressourcen. Eine Besichtigung der Immobilie könne dadurch allerdings in den meisten Fällen in Deutschland noch nicht ersetzt werden, schon gar nicht bei komplexeren Immobilien. Auch bei Sonderfällen wie dem Erbbaurecht, bei Bauschäden oder besonderen mietvertraglichen Situationen, etwa dem Wohnrecht, könnten automatisierte Systeme bisher kaum belastbare Ergebnisse produzieren. Ob KI eines Tages auch für Gutachten sorgt, die vor deutschen Gerichten Bestand haben?

Der Immobilienökonom und Architekt macht noch auf ein weiteres Problem aufmerksam: ""Datenbasierte Systeme sind anfällig für Manipulationen, ob nun gewollt oder unbemerkt durch die Verwendung ungeprüfter fehlerhafter Datensätze.""

Die Immobilienschätzung im Internet hat also ihre Tücken. Hinter vielen Angeboten steckt bisher der Versuch, möglichst an einen Auftrag für den Verkauf eines Hauses oder einer Wohnung zu kommen. Wer eine möglichst solide und vor allem offiziell anerkannte Bewertung haben will, braucht ohnehin vereidigte Sachverständige. Die sind zwar nicht kostenlos und auch nicht so schnell wie der Klick ins Netz, aber garantiert unparteiisch.";https://www.sueddeutsche.de/geld/immobilienbewertung-was-ist-mein-haus-wert-1.4794860;sz.de;Marianne Körber
24.01.2020;Warum Textprogramme Rechtschreibkenntnisse nicht ersetzen;"Wie gut funktioniert die automatische Schreibfehlerkorrektur von Smartphones? Detmar Meurers, Professor für Computerlinguistik an der Universität Tübingen, schreibt: ""Die Identifikation von Fehlern ist in den letzten Jahren besser geworden, liegt aber weiterhin weit entfernt davon, dass man sich gänzlich darauf verlassen könnte.""

Trotz neuer Machine-Learning-Verfahren würden immer noch nur etwa zwei Drittel der Fehler erkannt. Richtig gut seien die Systeme nur bei bestimmten Arten von Fehlern. Es komme oft auf den sprachlichen Kontext an.

Auch wenn das System anschlägt, sind Rechtschreibkenntnisse hilfreich. Schließlich schlagen die Programme meist Alternativen vor, zwischen denen man sich entscheiden muss. Das könnten uns die Assistenten in absehbarer Zeit nicht abnehmen, schreibt Meurers.

Nach Überzeugung des Computerlinguisten wird in vielen Bereichen in Zukunft mündliche Spracheingabe zum Standard werden, etwa bei Navis. Soll aber Komplexes ausgedrückt werden, bleibt das geschriebene Wort unverzichtbar.";https://www.sueddeutsche.de/wissen/technik-warum-textprogramme-rechtschreibkenntnisse-nicht-ersetzen-dpa.urn-newsml-dpa-com-20090101-200124-99-611845;sz.de;DPA
15.12.2019;Die Summe der Schönheit;"Pierre Barreau ist ein Wunderkind in allen Welten - in den analogen wie digitalen. Der heute 25-jährige Ingenieur komponiert, programmiert, schreibt Drehbücher, filmt, führt Regie und ist Gründer des Unternehmens AIVA Technologies mit Sitz in Luxemburg. Natürlich besagen Umtriebigkeit und Charme wenig, aber die Kombination macht ihn zum Darling auf Ted-Konferenzen. Dort hielt er im Sommer vor einem Jahr einen Vortrag, der die Errungenschaften der von ihm entwickelten Aiva vorstellte.

Sie ist eine künstliche Intelligenz (KI), die darauf spezialisiert wurde, Musik zu komponieren, etwa Werke im Geist großer historischer Vorbilder. Aiva bringt auch mal zu Ende, was ein Künstler unfertig hinterlassen hat. So hat diese KI ein Stück für Klavier des vor 115 Jahren gestorbenen Antonín Dvo?ák vollendet, das von den Prager Philharmonikern unter dem Dirigat von Emmanuel Villaume im November ... was eigentlich? Uraufgeführt wurde?

Aiva steht für ""Artificial Intelligence Virtual Artist"", Barreaus Start-up für KI-Musik bietet die Schöpfungen seiner virtuellen Künstlerin für Filme, TV, Werbung und Videospiele seit 2016 erfolgreich an. Sein Unternehmen ist keineswegs allein, die Konkurrenz schläft auch hier nicht. Die nie fertiggestellte zehnte Sinfonie von Gustav Mahler etwa wurde von einem intelligenten Algorithmus, dem MuseNet, am Ars Electronica Futurelab vollendet und in diesem Herbst in Linz konzertant zur Aufführung gebracht. MuseNet ist eine autonom agierende KI, die kompositorisch alles beherrscht: von Mozart über Country bis zu den Beatles. Und Huawei ließ Schuberts unvollendete Achte vollenden, die im Februar in London aufgeführt wurde.

Gerade meldet die Deutsche Telekom, deren Hauptsitz sich in Bonn befindet, dass man eine eigene KI auf die unvollendete 10. Sinfonie Ludwig van Beethovens, des berühmtesten Sohnes der Stadt, angesetzt hat, um mithilfe von Methoden des Machine Learnings aus des Komponisten musikalischen Skizzen ein vollständiges Werk synthetisieren zu lassen. Im kommenden Frühjahr hat es Premiere.

Wie geht das? Und: Sind dies Kompositionen, die mit den Originalen mithalten und den Werken der alten Meister ""auf Ohrhöhe"" begegnen können?

Eine erste Antwort gibt der smarte Barreau in seinem Ted-Vortrag, der - wie alle anderen Entwickler bei solchen Gelegenheiten - gleich am Rad der großen Zahl dreht. Mehr als 30 000 originale, sprich: menschengemachte Notensätze müssen bei ihm für ein ""Neuwerk"" maschinenlesbar gemacht und verschlagwortet werden, damit sie von Aiva nach Harmonie, Rhythmus und Stil analysiert werden. Um neue Musik komponieren zu lassen, das ist eine erste Faustregel, braucht man die alte.

Sehr viel von der alten, und sie muss KI-bekömmlich in einer Matrix-Form eingespeist werden. Die Datenberge beinhalten dann etwa alle Werke eines Komponisten, die Werke seiner Epoche oder Werke eines Stils oder Genres. Mit Musik hat das erst einmal nichts mehr zu tun. Es ist ein Rauschen, die Kakofonie aus den Werken aller Zeiten und Kulturen, Stilen und Komponisten - gleichzeitig gespielt.

Doch für Computer, genauer: für neuronale Netzwerke, die hier zum Einsatz kommen, gibt es kein größeres Glück als das der großen Zahl. Sie lernen, im Datenberg künstlerische Konzepte zu erkennen, die sie in Hierarchien ordnen. Was die Technologie unseres Jahrtausends revolutioniert hat, ist: sie können dies auch ohne Vorwissen, sie bringen es sich selbst bei.

KI-Computerverbünde sind Rasterfahnder, die in großen Daten Muster entdecken, seien es individuelle Besonderheiten und Ticks, seien es die großen Kompositionsgesetze. Sie finden Harmonielehren wie schrullige Idiosynkrasien. Regeln wie Abweichungen. Und auf Grundlage dieser Muster extrapolieren sie, was nach einer Notensequenz als nächstes kommen könnte. Müsste. Sollte. Sie sind nicht genial, sie sind nicht kreativ. Sie können nur rechnen.

Eine KI-Komposition ist ein proof of concept, eine errechnete Hypothese auf Basis von Big Data. Sollte sie sich als tragfähig erweisen, dann erstellen die Rechner Sets von Regeln, nach denen wahrscheinlich in einem bestimmten, epochalen oder individuellen Stil komponiert wurde. In der Klassik ermittelt dieses Verfahren dann also etwas wie die Beethoven-, Bruckner,- oder Mahler-Formel.
2018 versteigerte Christie's ein KI-Gemälde für 432 500 Dollar

Die Form der autonom ablaufenden mathematischen Näherungen an ein neues Werk gehört zum Prinzip des Deep Learning. Denn Menschen haben es den Rechnern überlassen, sich selbst schlauzumachen. Was auch bedeutet, sie wissen nicht, was genau sich in den Netzwerken abspielt, wie sie zu ihren Ergebnissen kommen. Einen Beethoven programmiert man nicht, heißt das dann aber eben auch.

Mit Deep Learning, das für ein Werk bis zu hundert Millionen einzelne Parameter in der Matrix berücksichtigt, lassen sich auf allen möglichen Feldern Muster finden und fortführen, nicht nur der Kreativität. Das berühmteste Beispiel ist das ""Porträt von Edmond Belamy"". Ein errechnetes Gemälde im Geist des ausgehenden 19. Jahrhunderts, das Christie's 2018 für 432 500 Dollar versteigerte. Es lassen sich also mit der gleichen Methode Bilder malen, Handschriften erkennen, Sonaten komponieren, Bücher schreiben und auch bösartige Krebszellen auf einem Röntgenbild diagnostizieren. Kein Wunder also, dass die Bonner Beethoven-Programmierer eine KI zur Erkennung von Textbedeutung modifiziert haben, um zu ihrer Version der 10. Sinfonie zu gelangen.

All das kann nur funktionieren, wenn es eine Richterinstanz gibt, die jedes Zwischenstadium auf dem Weg zur Vollendung beständig prüft. Dann macht die KI im nun approbierten Sinne weiter, und verwirft alle als falsch eingestuften Vorschläge. Dieses Richteramt bekleiden im Fall der Bonner Beethoven-Vollendung menschliche Experten. Dieses Verfahren macht die Computerkompositionen mit jedem Approbationsschritt genauer, Beethoven-wahrscheinlicher und vor allem: menschlicher. Es macht sie aber nicht automatisch besser und schon gar nicht authentisch. Denn es kann ja auch ein antagonistisches Computersystem darauf angesetzt werden, dieses Richteramt zu übernehmen. Man kann ihm dann die Prüfung dessen überlassen, was die protagonistischen Systeme erarbeiten. Das geht auch wesentlicher schneller. Produktiv werden Netzwerke aber nur dann, wenn dieselbe Intelligenzstärke prüft, mit der auch hergestellt wird. Das klingt tautologisch, ist es auch.
Es stecken Paralleluniversen der Musik in der Retorte

Denn woher soll der Prüf-Algorithmus seine Kompetenz nehmen, wenn nicht aus den Mustervorlagen, nach denen der Produzent vorgeht? KI-Kreation ist ein guessing game von zwei sich belauernden Trickstern. Der eine weiß nicht, ob sein Vorschlag durchgeht, der andere nicht, ob dieser Vorschlag nicht sogar plausibel sein könnte. Man nähert sich so allmählich an.

Jede dann gefundene Lösung ist darum nur eine von vielen. Pierre Barreau etwa will das ausnutzen, um individuelle Lebensmusiken für jeden einzelnen Menschen anfertigen zu lassen, variabel nach Stimmung, Notendichte, Epoche oder Stil. Mehr als 30 Kategorienlabel, nach denen Kompositionskandidaten eingespeist und begutachtet werden, hat er bereits für die Musikgeschichte vergeben. Danach lassen sich Musiknarrative schier unendlich fortführen. Denn hat ein System erst einmal seinen ""Stil"" gefunden, kann es sein Sujet nahezu unendlich variieren. Es stecken Paralleluniversen der Musik in der Retorte. Ist KI dann also kreativ?

Niemals. Denn Schöpfung geschieht hier a posteriori, destilliert aus einem zwar gigantischen, aber bereits vorhanden und approbierten Musikkorpus. Dessen Set an Regeln werden mit jedem neuen KI-Werk nur anders angewandt. So entsteht immer anderes, aber nie etwas wirklich Neues. Die Werke der künstlichen Intelligenzen mögen imponieren, aber sie bereichern die Kunst nicht. Ihnen fehlen Überschuss und Ausbruch aus dem Bekannten, mit einem Wort: Genialität. Einprogrammierte Überraschungen sind keine mehr.

Wenn im Frühjahr eine 10. Sinfonie in Bonn vorgestellt werden wird, dann wird sie vermutlich berückend schön sein. Aber sie ist nicht von Beethoven. Sie ist die hochkomplexe Synthese aus all seinen alten Sinfonien, aber kein neues Werk. Er hätte sie ganz anders komponiert. Sicher. Das wird jede künstliche Intelligenz bestätigen.";https://www.sueddeutsche.de/digital/kuenstliche-intelligenz-kunst-klassik-musik-beethoven-1.4718835;sz.de;Bernd Graff
24.11.2019;"""Bei einem Pitch sitzen fast nur alte, weiße Männer""";"Falls es das Klischee eines Frauen-Start-ups gibt, dann sieht es so aus wie ""The Female Company"". Das Stuttgarter Unternehmen verkauft Bio-Tampons, Bio-Binden und Bio-Slipeinlagen im Abonnement, hat ein rosa-rotes Logo und eine Frauenquote von hundert Prozent. Ann-Sophie Claus, 27, weiß das Stereotyp für sich zu nutzen. Sie hat das Start-up vor knapp zwei Jahren gegründet - zusammen mit ihrer besten Freundin. Noch ein Klischee.

""Wir fallen auf, aber darauf spekulieren wir auch absichtlich"", sagt Claus. Bei einem Pitch vor Investoren kamen die meisten Gründer im schlichten Anzug. Ann-Sophie Claus und Mitgründerin Sinja Stadelmaier hingegen trugen blutrote Hosen. Das Spiel funktioniert: Das junge Unternehmen hat bereits jede Menge Aufmerksamkeit von Kunden, Medien und Investoren auf sich gezogen. Gewissermaßen haben sich die Gründerinnen selbst als Marktlücke erkannt, denn nur gut 15 Prozent aller Start-up-Gründer in Deutschland sind weiblich. Das ist das ernüchternde Ergebnis des aktuellen Female Founders Monitors des Bundesverbands Deutsche Start-ups. ""Eine riesige Schieflage"", sagt Alexander Hirschfeld. Der Teamleiter Research des Verbands hält das Ungleichgewicht nicht nur aus Perspektive der Chancengleichheit für alarmierend: ""Es gibt genauso viele talentierte Frauen wie Männer. Deswegen ist das auch ein wirtschaftliches Problem"", sagt Hirschfeld.
""Uns fehlen die Frauen, die darüber reden, was sie Großartiges machen""

Warum gründen Frauen so viel seltener Start-ups? Der ""Ruhr-Summit"" in Bochum könnte eine Antwort geben. Auf der nach eigenen Angaben größten B2B-Start-up-Veranstaltung Deutschlands sitzen Ende Oktober fünf Frauen auf dem Podium und diskutieren ebendiese Frage. ""Bisher hätten wir auf der Mainstage noch ein bisschen mehr Diversität gebrauchen können"", kündigt der Moderator die Diskussion an. Bis dahin hatten sich auf der Bühne nur junge Männer das Mikrofon in die Hand gegeben. Auf der Agenda: IT-Security, Virtual Reality, Machine Learning, digitale Transformation - und die Gründerin als Ausnahme. Einige Zuschauer gehen, einige Zuschauerinnen kommen dazu.

""Das fängt schon strukturell in der Gesellschaft an"", beginnt Janna Prager, Mitgründerin des Impact Hub Ruhr, auf der Bühne das Gespräch: ""Mädchen werden erzogen, lieb und verantwortungsbewusst zu sein - Jungs hingegen sind laut und abenteuerlustig."" Mut und Abenteuerlust seien aber Eigenschaften, die eine Gründerin brauche. Es sei wichtig, diese Rollenbilder aufzubrechen.

Auch Nora Breuker ist zum Ruhr-Summit nach Bochum gekommen. In den USA hat sie ihr erstes Start-up gegründet; seit Anfang dieses Jahres ist sie zudem als selbständige Beraterin tätig. Sie vermisst vor allem die weiblichen Vorbilder: ""Man sieht ständig Männer, die Ted Talks halten und erfolgreiche Unternehmen führen. Uns fehlen die Frauen, die darüber reden, was sie Großartiges machen."" Wenn es um Öffentlichkeit und Bühnenpräsenz geht, scheinen Frauen deutlich scheuer zu sein als ihre männlichen Kollegen. Es sei schwierig gewesen, überhaupt Speakerinnen für die Messe zu finden, erklärt der Veranstalter. Sind Frauen also weniger mutig?

Laut dem Female Founders Monitor greift diese Erklärung zu kurz. Nach seiner Definition ist ein Start-up ein Unternehmen, das jünger als zehn Jahre ist, signifikant wachsen will und sich durch eine innovative Technologie oder ein zukunftsweisendes Geschäftsmodell auszeichnet. Demnach gründen Frauen zwar selten Start-ups, ihr Anteil bei allgemeinen Existenzgründungen - inklusive aller selbständigen Tätigkeiten - hat sich in den vergangenen Jahren jedoch bei etwa 40 Prozent eingependelt. Eine weitere Befragung des Start-up-Verbands bestätigt, dass sich fast ebenso viele Frauen wie Männer vorstellen können, ein eigenes Unternehmen zu gründen. Sie scheuen also nicht per se das Risiko der Selbständigkeit.

Woran liegt es dann? Ein Blick auf die Studienabschlüsse der Gründer macht eine Ursache deutlich: Nur 5,6 Prozent aller Gründerinnen haben einen Abschluss in Mathematik, Informatik oder Computerwissenschaften. Bei den Gründern sind es immerhin knapp 19 Prozent. Frauen, die gründen, kommen also selten aus dem Mint-Bereich - die Informations- und Kommunikationstechnik ist aber genau die Branche, in der die meisten deutschen Start-ups aktiv sind.

Zudem dürfe man die Netzwerkeffekte nicht unterschätzen, die es in jedem wirtschaftlichen Bereich gebe, sagt Soziologe Hirschfeld vom Start-up-Verband: ""Man kennt sich, man kennt die relevanten Player und weiß, wer über Expertise verfügt."" Wer Kontakte hat, der hat Chancen. Und Frauen sind noch nicht ausreichend vernetzt. Zwar gibt es immer mehr reine Frauen-Initiativen in der Szene; die Gründerinnen helfen sich gegenseitig. Diese Netzwerke seien aber nur dann zielführend, wenn sie wieder in die schon bestehenden Netzwerke eingeflochten würden, meint Hirschfeld.
Investoren bewerten weibliche Startups im Durchschnitt deutlich schlechter

Ein weiterer Knackpunkt sei die Finanzierung: ""Wer wachsen will, braucht Kapital - und die meisten wichtigen Kapitalgeber sind eben noch Männer"", sagt Hirschfeld. Gründerin Nora Breuker kann das bestätigen: ""Bei einem Pitch sitzen fast nur alte, weiße Männer vor dir."" In dieser Situation müssten sich Frauen doppelt beweisen: ""Du musst nicht nur mit deiner Idee überzeugen, sondern auch zeigen, dass du als Unternehmerin wertvoll bist.""

Tatsächlich bestätigt eine aktuelle Studie der Unternehmensberatung Boston Consulting Group, dass rein weibliche Start-ups in Deutschland eine um 18 Prozent geringere Chance haben, nach der Gründung Investorengelder zu akquirieren. Im Durchschnitt erhalte ein von Frauen geführtes Start-up demnach nur ein Drittel der Finanzierungssumme, die rein männliche Start-ups beziehen. Der Grund mitunter: Investoren bewerten Existenzgründerinnen schlechter. So schätzen sie den Wert von deutschen Neugründungen mit Männern an der Spitze im Schnitt 16,4-mal höher ein als den Wert von weiblich geführten Unternehmen.

""Das ist schon schockierend"", sagt Breuker. Sie sieht aber eine kleine Mitschuld bei den Frauen, die oft zurückhaltender aufträten als Männer. Diese Vorsicht zeigt sich auch im Gründungsverhalten: So steht das schnelle Wachstum für Gründerinnen nicht so stark im Vordergrund, dafür ist ihnen Stabilität wichtiger als den Gründern. Auch die Motive, ein Start-up zu gründen, unterscheiden sich: Zwar spielen ökonomische Ziele für Frauen eine große Rolle, doch daneben treiben sie soziale Themen an - wirtschaftlicher Erfolg ja, aber sie möchten mit ihrem Unternehmen eben auch etwas Gutes bewirken.
Gründerinnen mit Kindern haben pro Woche neun Stunden weniger Zeit für die Firma

Für Ann-Sophie Claus, die Gründerin von The Female Company, eine Selbstverständlichkeit: ""Mich wundert immer, dass Social Entrepreneurship ein Frauenthema ist. Das sollte doch in jedem halbwegs sozialen Menschen verankert sein."" Der soziale Aspekt gebe jeder Gründungsidee Kraft, findet Claus.

Die Unternehmerin ist in der Start-up-Szene angekommen. Sie schätzt die Flexibilität des Gründerdaseins, auch mit Blick auf die Familienplanung. Zugleich sei das Thema Mutterschaft der größte ""Pain Point"", sagt sie. Die Ergebnisse des Female Founders Monitors bestätigen diesen Eindruck: Gründerinnen mit Kind gaben an, im Vergleich zu Männern wöchentlich neun Stunden weniger Zeit für ihr Unternehmen aufbringen zu können - weil sie parallel das ""Unternehmen Familie"" managen. Damit seien sie nicht weniger produktiv, im Gegenteil. Doch weniger Zeit bedeutet eben weniger Möglichkeiten.

""Das ist ein ultimativ präsentes Thema in meinem ganzen Gründerinnen- und Freundeskreis"", sagt Ann-Sophie Claus. Die Gründerin ist jetzt 27 Jahre alt. Auch sie möchte irgendwann eine Familie haben, weiß aber noch nicht, wie das funktionieren soll: ""Solange wir da das Gefühl haben, wir werden doch allein gelassen, wirkt das sehr begrenzend.""";https://www.sueddeutsche.de/karriere/frauen-start-ups-maenner-gleichberechtigung-gruenden-1.4690166;sz.de;Lea Weinmann
18.11.2019;Wie Linz sich aus dem Sumpf zog;"Manche Kugeln schimmern rötlich, andere blau oder grün. Kleine und große Kugeln, mit Stangen zu einer Installation verbunden, die sich 30 Meter in die Höhe reckt. 80 Kugeln sind es insgesamt, die größten haben einen Durchmesser von zweieinhalb Metern. Illuminiert von einer 700 Quadratmeter großen LED-Fläche zieht die Kugelskulptur im Inneren der Voestalpine-Stahlwelt in Linz die Blicke aller auf sich, die das Foyer betreten. Sie ist eine Hommage an den Baustoff Stahl und Herzstück der multimedialen Erlebniswelt auf fünf Etagen.

""Die Kugeln symbolisieren die Molekularstruktur von Stahl"", erklärt Sieglinde Schrey. ""Sie bestehen aber nicht aus Stahl, sondern aus mit Chrom beschichtetem Kunststoff."" Die 57-Jährige arbeitet als Guide in der Stahlwelt - von außen betrachtet sieht das Gebäude wie ein Periskop aus. Die Architektur der Erlebniswelt steckt voller Anspielungen auf den biegsamen Baustoff. So ist die Form der Eingangshalle einem Konverter nachempfunden, der Eisen in Stahl umwandelt.

Im Jahr 2009 wurde die Erlebniswelt eröffnet - ein wichtiges Jahr für den Konzern sowie für die oberösterreichische Landeshauptstadt. Denn vor zehn Jahren präsentierte sich Linz als europäische Kulturhauptstadt. Seitdem hat sie sich weiter gewandelt, vom Industriestandort zur Kunststadt. ""Linz hatte ein schmutziges Image, aber wir haben uns am eigenen Schopf aus dem Sumpf gezogen"", sagt Georg Steiner, der Direktor des Tourismusverbands.
60 Prozent Parks und Gewässer

Seit Mitte der Achtzigerjahre habe man, so die Auskunft der Pressestelle des Magistrats, eine ""konsequente Politik"" verfolgt, um Schadstoffe wie Schwefeldioxid, Feinstaub und Stickstoffdioxid zu reduzieren. Die Emissionen - verursacht durch die Haushalte der 200 000 Einwohner, die Chemieindustrie, den Verkehr - seien von 47 000 Tonnen im Jahr 1985 auf rund 14 000 in 2003 gesenkt worden; die Werte seien seitdem relativ konstant. Tatsächlich habe auch Voestalpine, der größte Industriebetrieb der Stadt, die Emissionen seit den 80er-Jahren erheblich reduziert, so die Stadtverwaltung. Als weitere Maßnahmen sollen die Taxis der Stadt auf E-Betrieb umgerüstet und die Schiffsanlegestellen entlang der Donau, wo viele Kreuzfahrtschiffe anlegen, mit Landstromanschlüssen versehen werden. Gut für das Klima ist sicher auch, dass 60 Prozent des Stadtgebietes Parks oder Gewässer sind.

Auch Michael Kirchsteiger, Geschäftsführer der Voestalpine-Stahlwelt, begleitet mitunter Gäste auf dem didaktischen Rundgang, der vermittelt, wie Stahl auf dem 5,2 Quadratkilometer großen Produktionsgelände erzeugt und verarbeitet wird. ""5,8 Millionen Tonnen Stahl produzieren wir am Standort Linz jedes Jahr im Durchschnitt"", so Kirchsteiger. Während der Führung geht man auf einer breiten Treppe rund um die riesige Chromkugelskulptur nach oben, begleitet von einem Rauschen und von Klopfgeräuschen - der typischen Klangkulisse der Stahlproduktion. Wer diese in der Realität erleben möchte, kann an einer Tour durch das Werk teilnehmen.

Auf dem Erlebnisrundgang mischen sich Sinneseindrücke mit Fakten: Zur Führung gehört ein Streifzug durch die wechselvolle Geschichte des Technologieunternehmens, die mit dem Linzer Eisen- und Stahlwerk der Reichswerke Hermann Göring AG begann. Intensiver eintauchen kann man im Zeitgeschichte-Museum des Konzerns. Das mithilfe des Marschallplans nach dem Krieg wiederaufgebaute Unternehmen hießt zunächst Vöest, das steht für ""Vereinigte Österreichische Eisen- und Stahlwerke"". Heute beschäftigt der Konzern Voestalpine 52 000 Mitarbeiter - 11 000 von ihnen in der Zentrale in Linz. Anhand zahlreicher Exponate wird deutlich, wie vielseitig verwendbar das Material ist. Stahl für die Automobilindustrie, für die Luftfahrt, für Ölpipelines, für Haushaltsgeräte. Weltmarktführer ist Voestalpine bei der Entwicklung von Weichen- und Schienensystemen. ""Und wir produzieren die Prägestempel für fast alle europäischen Länder, die den Euro haben"", berichtet Kirchsteiger. Eine knappe halbe Stunde fährt man mit Trambahn und Bus von dem am Stadtrand gelegenen Werksgelände zurück in die Linzer Altstadt mit ihren Arkadenhöfen, verwinkelten Gässchen und Kunstgalerien. Tourismusdirektor Steiner weist auf den Kontrast zwischen Barockbauten und moderner Architektur hin. Diese verkörpert zum Beispiel das im Jahr 2003 eröffnete Lentos-Kunstmuseum. Positioniert man sich an der Unteren Donaulände vor dem kantigen Baukörper des Lentos, in dem moderne Kunst zu sehen ist, so wirkt es wie ein Bilderrahmen, dessen zentrales Motiv ein anderes Gebäude am gegenüberliegenden Donauufer bildet - das Zukunftsmuseum Ars Electronica Center (AEC). Seine Fassade ist verkleidet mit 40 000 LEDs. Abends sorgen sie für ein lebhaftes Schauspiel: Das AEC schillert in verschiedenen Farben.

Zehn Jahre nach dem Kulturhauptstadtjahr 2009 ist die Dauerausstellung des AEC im vergangenen Mai komplett umgestaltet worden. Sie verfügt nun über eine Ausstellungsfläche von 3500 Quadratmetern. Im Mittelpunkt der neuen Schau stehen die Einsatzmöglichkeiten künstlicher Intelligenz (KI). Im ""Machine Learning Studio"" kann man Roboter programmieren und Experimente mit selbstfahrenden Autos machen. Ein weiteres Thema: künstliche Intelligenz als Urheberin von Texten. So stellt die Schau eine besonders prachtvolle mittelalterliche Handschrift, eine Kopie der Wenzelsbibel aus dem 14. Jahrhundert, einem Text zum Thema ""Recycling is good for the world"" gegenüber. Dessen Autor ist aber kein Mensch, sondern ein KI-System. ""Der Text ergibt Sinn, aber wie das System auf ihn gekommen ist, wissen wir nicht"", sagt Manuel Walch, der im Museum als ""Infotrainer"" arbeitet. ""Das ist unheimlich"", sagt eine Besucherin, die dem jungen Mann zufällig zugehört hat. Die Infotrainer, erkennbar an ihren orangefarbenen T-Shirts, sollen Besucher an den interaktiven Experimentierstationen unterstützen.

Neben der Hauptausstellung gibt es temporäre Sonderschauen. Dauerhaft zu erleben ist hingegen die neue Ausstellung ""AI x Music - Artificial Intelligence meets Music"". Sie widmet sich dem Zusammenspiel von künstlicher Intelligenz und Musik und wartet mit einer erstaunlichen Information auf: Die Wurzeln der automatisierten Musik reichen bis in die Mitte des neunten Jahrhunderts nach Christus zurück: In der Schau kann man den Tönen eines rekonstruierten Flötenspielautomaten lauschen und hört Tonserien, wie sie vor mehr als 1000 Jahren im arabischen Kulturraum erklangen. Dabei handelt es sich um das erste programmierbare Musikinstrument der Menschheitsgeschichte.

Wie von Geisterhand gesteuert gibt ein Bösendorfer-Flügel ""Ma mère l'oye"" von Maurice Ravel zum Besten - KI macht's möglich - und hinterlässt beim Betrachter doch ein komisches Gefühl. Digitale Musik ist auch einer der Schwerpunkte des Medienkunst-Festivals Ars Electronica, das bereits seit 1979 jeden Herbst in Linz stattfindet. ""Das Festival ist Treffpunkt für Vorreiter und Visionäre"", sagt Tourismusdirektor Steiner. Er verweist etwa auf Julian Assange, der dort für Wikileaks geehrt worden sei: ""Da wusste noch keiner, was das ist."" Wie das Zukunftsmuseum AEC appelliert auch der ""Höhenrausch"" im OÖ Kulturquartier an die Experimentierfreude der Besucher. Das Festival, das anlässlich des Kulturhauptstadtjahrs 2009 entstand, verbindet Gegenwartskunst mit Spaziergängen auf den Dächern von Linz. Es hat sich zu einer festen Größe im Linzer Kulturleben entwickelt. Für das diesjährige Motto ""Sinnesrausch"" waren Kulturschaffende aus aller Welt eingeladen, künstlerische Positionen zu Blasen, Linien, Punkten zu entwickeln. Dabei konnten Besucher selbst ausprobieren, wie es sich anfühlt, wenn man sich sprichwörtlich in einer Blase bewegt - dafür wurde in eine mit zitronengelbem Stoff ausgekleidete Halle laufend Luft gepumpt, sodass der Stoff Wellen und Hohlräume bildete. Im OÖ Kulturquartier finden das ganze Jahr über Ausstellungen und Festivals statt. ""Anders als bei einem klassischen Museumsviertel ist bei uns bis zwei Uhr nachts was los"", sagt der künstlerische Direktor Martin Sturm. Der Germanist hat das OÖ Kulturquartier seit 1992 peu à peu aufgebaut. Die Kulturszene der Stadt und ihre Einwohner charakterisiere ein ""offener Zugang zur Welt"" und die ""Neugierde auf das Ungewöhnliche"".

Dafür steht zum Beispiel ""Mural Harbour"": Seit 2012 haben Künstler aus aller Welt ein riesiges Areal im Linzer Industriehafen in eine Open-Air-Galerie verwandelt. Jedes Jahr kommen neue Wandgemälde und Graffiti auf den Außenmauern der Lagerhallen hinzu. Man kann sie während eines geführten Rundgangs erleben, aber noch eindrucksvoller wirken diese Murals während einer Tour mit dem Lastkahn.

Die Lust auf das Ungewöhnliche, sie zieht sich wie ein roter Faden durch das Leben von Chris Müller, der seit 2013 Direktor der Tabakfabrik Linz ist. Der 46-Jährige hat die Aufgabe, das knapp 80 000 Quadratmeter große Areal nahe der Unteren Donaulände zu einem Vorzeigequartier der Kreativwirtschaft zu entwickeln. Nachdem vor zehn Jahren die Tabakproduktion eingestellt worden war, erwarb die Stadt Linz das Areal. ""Ich habe Kunst studiert und Theater in Berlin gemacht"", erzählt Müller. Seinen Job in der denkmalgeschützten einstigen Industrieanlage, die in den Jahren 1929 bis 1935 entstand, beschreibt er so: ""Wie gewinnt man Kreativität statt Tabak?"" Inzwischen sind hier 250 Organisationen und Firmen, darunter 80 Start-ups, aus Bereichen wie Design, Architektur, Werbung, Software, Darstellende Künste oder Film und Fotografie untergebracht. ""Wir wollen bunt sein"", sagt der Direktor. Sein Büro gleicht einer Schaltzentrale, in der verschiedene Apparate mit blinkenden Knöpfen stehen - ein paar von ihnen sind Relikte aus der Tabakfabrik-Ära. So gut wie jeden Tag finden Veranstaltungen auf dem Gelände statt - vom Social-Media-Workshop bis hin zur Tattoo-Messe oder der ""Fuckup Night"", bei der Gründer über das Scheitern sprechen. ""Wir brauchen hier ganz verschiedene Persönlichkeiten"", sagt Müller, ""Idealisten ebenso wie Desillusionierte oder Unruhestifter.""

Ein kritischer Geist ist mit dem Linzer Karikaturisten Gerhard Haderer in die Tabakfabrik eingezogen. 2017 eröffnete er hier seine ""Schule des Ungehorsams"", die Denk- und Diskussionsschule sowie Kunstgalerie in einem ist. Was den 68-Jährigen antreibt, beschreibt er so: ""Provokation um der Provokation willen ist langweilig. Es bedarf, mit poetischen Worten, einer Schule, die den Lärm zur Musik macht."" Die Schau ""Ölhades"" präsentiert großformatige Ölgemälde des Künstlers, außerdem organisieren der Meister der gezeichneten Satire und seine Familie hier Vernissagen, Lesungen, Musikabende, Podiumsdiskussionen oder Workshops. Damit wollen sie die Bürger dazu einladen, Verantwortung zu übernehmen und ""Ungehorsam positiv zu denken"", wie es Haderer ausdrückt. Das heißt: mit Humor. Bei diesem Projekt geht es einmal mehr um Experimentierfreude und um Partizipation. Die beiden Hauptzutaten für das lebendige Linzer Kulturleben.";https://www.sueddeutsche.de/reise/staedtereise-linz-oesterreich-1.4678974;sz.de;Stephanie Schmidt
25.10.2019;Endlich normale Sätze!;"Wenn die mächtigste Suchmaschine der Welt an ihren Algorithmen schraubt, dann halten Menschen auf der ganzen Welt den Atem an. Verleger und SEO-Experten zum Beispiel, also Menschen, die daran arbeiten, dass die eigenen Inhalte möglichst weit oben in den Trefferlisten der großen Suchmaschinen landen. Eine winzige Änderung der Kriterien, nach denen Google seine Ergebnisse sortiert, kann enorme Auswirkungen auf die Sichtbarkeit - und damit auf die Werbeeinnahmen von Nachrichtenseiten haben. Doch das größte Technologie-Update seit fünf Jahren, das Googles für die Such-Technologie zuständiger Vizepräsident Pandu Nayak jetzt auf Googles offiziellem Blog verspricht, betrifft offenbar weniger aktuelle Nachrichten, als die gewöhnliche Suche nach Informationen: Google soll lernen, Menschen besser zu verstehen.

Nayak zu Folge haben sich Nutzer in den vergangenen Jahren bei Google-Suchen eine Art Spezialsprache antrainiert, die Google-Mitarbeiter ""keyword-ese"" genannt haben. Anstatt normale menschliche Sätze zu googlen, reihen Suchende Substantive, Adjektive und Verben im Infinitiv aneinander. ""Gluten, schädlich, Gesundheit"" - wäre wohl so ein Beispiel. Zumindest im deutschen Sprachraum ist ein Grund für diese Such-Taktik, dass Fragen wie ""Ist Gluten schädlich für die Gesundheit?"" oft nicht zu seriösen Informationen, sondern schnell zu einem Suchergebnis bei ""Gutefrage.net"" führen. Das neue Google-Update soll jetzt auch bei normalen Fragen häufiger direkt zu brauchbaren Ergebnissen führen.
Update zunächst nur bei Suchen auf Englisch

Gelingen soll das mit einer Technologie, die Google BERT getauft hat, und die der Konzern vergangenen Herbst zur Open-Source-Technologie gemacht hat, also auch Mitbewerbern zur Verfügung stellt. Im Kern geht es bei BERT darum, dass künstliche Intelligenz lernen soll, normale menschliche Sprache besser zu verstehen. Anstatt die Bedeutung einzelner Wörter zu erfassen, sollen Maschinen mithilfe der Technologie trainiert werden, die Beziehung der Wörter untereinander zu ""verstehen"". Als Beispiel nennt Nayak die englische Suchanfrage: ""Kann man Medikamente für jemanden abholen, Apotheke"". Spuckte Google bisher allgemeine Regeln für die Abholung vom Arzt verschriebener Medikamente aus, so verstehe die Technologie jetzt die besondere Bedeutung des Satzteils ""für jemanden"" und verweise sofort auf eine entsprechende Behördenseite. Eine von zehn Suchanfragen werde durch die neue Technologie verbessert, verspricht Nahak. Außerdem sollen durch die Technologie auch die sogenannten Snippets besser werden, also die kompletten Antworten, die Google direkt auf der Such-Seite anzeigt.

Zunächst soll die Veränderung nur im englischen Sprachraum kommen. Theoretisch, so die Google-Forscher, sollten die Modelle, die durch das Training des Google-Algorithmus im englischen Sprachraum entstehen, auch beim Verständnis anderer Sprachen helfen. Wann das Update auch für den deutschen Sprachraum kommen soll, steht aber noch nicht fest. ";https://www.sueddeutsche.de/digital/google-algorithmus-suchergebnisse-normalsprache-1.4656471;sz.de;Max Muth
17.10.2019;Warum Computer nicht vor Diskriminierung schützen;"Algorithmen sind seit einigen Jahren schwer in Mode. Nicht ganz zu Unrecht, sind sie doch für einen guten Teil der digitalen Erfolgsstorys mitverantwortlich. Der Google-Algorithmus sortiert Suchergebnisse so, dass Suchende finden. Facebooks Algorithmus platziert den Nutzenden passgenau die Inhalte, die sie wirklich interessieren - und Online-Werbung zeigt Surfenden Anzeigen von Produkten, die sie mit hoher Wahrscheinlichkeit kaufen würden. Algorithmen sparen Zeit, sie sind Optimierungswerkzeuge.

Insofern ist es wenig überraschend, dass sich mittlerweile auch die Politik stark für die Entscheidungshilfen per Computer interessiert - mit ähnlichen Begründungen. Zusätzlich zum Argument der Effizienz, das beim Umgang mit Steuergeld ohnehin immer gut ankommt, schwärmen Politiker gern von der vermeintlichen Objektivität, die von Computern getroffene Entscheidungen ausstrahlen. Dass diese Objektivität in der Praxis jedoch häufig ein Trugschluss ist, davon konnten sich Bürger des US-Bundesstaats Indiana schon 2006 überzeugen.

Damals erteilte der Gouverneur Indianas dem Computerriesen IBM den Auftrag, die Verwaltung seiner Sozialsysteme zu modernisieren. Anstelle von Sozialarbeitern, die ihre Kunden oft über Jahre kannten, sollten Computer eine größere Rolle spielen, Maschinen statt Menschen sollten künftig darüber entscheiden, ob Antragsteller tatsächlich ein Recht auf Beihilfe für Krankenversicherungen, Essen oder Wohnungen haben. Das erklärte Ziel der Regierung: Sozialmissbrauch verhindern und die Entscheidungen fairer gestalten. Kern dieser computergestützten Lösung von IBM sollte ein Algorithmus sein: In einem Computersystem werden alle verfügbaren Daten der Antragsteller gesammelt, dort werden sie verarbeitet, am Ende steht das Ergebnis.
Algorithmen sind nicht neutral

Im ersten Jahr nach Einführung des neuen Systems wurden über eine Million Anträge abgelehnt, ein Anstieg von 54 Prozent. Zehntausende Menschen verloren ihre Krankenversicherung, Essensmarken oder andere Beihilfen. Viele von ihnen hatten neue Online-Formulare nicht korrekt ausgefüllt oder jahrealte Belege nicht mehr gefunden. In zumindest einem Fall starb eine Frau, weil ihr das Computersystem die notwendige Krebsbehandlung verweigerte. Recherchiert hat ihn die US-Autorin Virginia Eubanks, die seit Jahren zu den Folgen automatisierter Entscheidungsmechanismen forscht und an der State University of New York (SUNY) lehrt. Ihre Erkenntnisse hat Eubanks 2018 in ihrem Buch ""Automating Inequality. How High-Tech Tools Profile, Police, and Punish the Poor"" zusammengefasst. Eubanks Botschaft: Algorithmen sind nicht neutral, egal wie sehr Politiker von Effizienz und Objektivität der neuen Entscheidungshelfer schwärmen.

Zu einem ähnlichen Ergebnis kommt jetzt eine Studie des Instituts für Technikfolgenabschätzung und Systemanalyse des Karlsruher Instituts für Technologie (KIT), in Auftrag gegeben von der Antidiskriminierungsstelle des Bundes. Die Forscher sollten herausfinden, inwiefern algorithmenbasierte Systeme zu Diskriminierung einzelner Menschen oder ganzer Bevölkerungsgruppen beitragen können. Ein zentrales Ergebnis: Im Vergleich zu Entscheidungen durch Sachbearbeiter steigt die Diskriminierungsgefahr sogar noch, wenn Computer die Entscheidungen treffen. Denn Menschen urteilen von Fall zu Fall. Steckt ein Fehler im Algorithmus, hat er automatisch Auswirkungen auf alle Entscheidungen.

Ob eine Maschinenentscheidung diskriminierend ist oder nicht, ist nicht immer leicht zu entscheiden. Denn Computer arbeiten oft mit Daten, die zwar nicht als direkte Diskriminierungsgründe gelten, die aber dennoch auf das eigentlich geschützte Kriterium zurückzuführen sind. So dürfen Arbeitnehmerinnen etwa nicht wegen ihres Geschlechts benachteiligt werden. Wenn ein Computer nun unter Bewerbern für Jobs diejenigen wählt, die nie Fehlzeiten hatten, dann wirkt das wie ein objektives Kriterium, schließt aber beispielsweise Mütter aus, wenn diese Elternzeit genommen hatten.

Besonders knifflig wird es - rechtlich und moralisch - wenn sogenanntes ""Machine Learning"" zum Einsatz kommt. Bei dieser Art der automatisierten Entscheidungen werden Computer zunächst mit einem großen Berg an historischen Rohdaten und Entscheidungen gefüttert und ""lernen"" dadurch, was gute Entscheidungen sind. Später sollen sie aufgrund neuer Daten selbst Entscheiden. Was in der Blackbox passiert, können selbst die Programmierer nicht mehr genau sagen.

In fast 50 Fallbeispielen der Studie ziegen die Karlsruher Wissenschaftler, dass nahezu jedes System, das auf Algorithmen beruht, anfällig ist für Diskriminierung. Das fängt schon damit an, dass die automatisch generierten Untertitel auf der Videoplattform Youtube offenbar Probleme haben, die Stimmen von Frauen und Schotten zu transkribieren. Politisch problematischer ist schon, dass Gefühlsanalysen, wie sie zum Beispiel auf Twitter zu politischen Einstellungen stattfinden, die Schriftsprache der afroamerikanischen Bevölkerung weniger gut analysieren können als die weißer US-Amerikaner. So wird die Meinung der schwarzen Bürger im öffentlichen Diskurs womöglich weniger berücksichtigt.
Internetfirmen verweigern Einblicke in ihre Algorithmen

Studienleiter Carsten Orwat kann auf eine Reihe exzellenter Vorarbeiten aufbauen. So demonstrierte die amerikanische Bürgerrechtsorganisation ACLU 2018, dass die Gesichtserkennung von Amazon 28 Mitglieder des US-Kongresses als gesuchte Straftäter einstufte. Überproportional von den falsch-positiven Treffern betroffen waren nicht-weiße Politiker. Die stiftungsfinanzierte Nachrichtenplattform Pro Publica fand heraus, dass ein von der US-Justiz eingesetztes System zur Beurteilung der Rückfallgefahr von Straftätern bei Afroamerikanern ein höheres Risiko berechnete. Auch das Microtargeting auf Facebook, bei dem Nutzer personalisierte Werbung ausgespielt bekommen, ist anfällig für Ungleichbehandlung. So nutzen Arbeitgeber die Plattform teilweise, um Jobanzeigen gezielt nur Männern oder Bewerbern unter einer gewissen Altersgrenze anzuzeigen. Wohnungsvermieter verhinderten nach Recherchen von Pro Publica mithilfe der Technik, dass afroamerikanische Mieter ihre Anzeigen zu sehen bekamen.

Um solche Diskriminierungen vor allem bei künftigen staatlich eingesetzten Algorithmen auszuschließen, empfiehlt die Studie des KIT vor allem Transparenz. Unternehmen wie Facebook weigern sich seit Jahren, Einblick in ihre Algorithmen zu geben. Algorithmen, die für Behörden entwickelt würden, sollten dagegen im Vorfeld umfassend auf mögliche Diskriminierungsrisiken geprüft werden, empfehlen die Studienautoren. Das Ziel müsse sein, dass Algorithmen in Zukunft schon diskriminierungsfrei designt werden.";https://www.sueddeutsche.de/digital/diskriminierung-algorithmen-kit-verwaltung-1.4641557;sz.de;Max Muth
11.10.2019;Fast 400 000 Studenten in Bayern eingeschrieben;"Im Freistaat gibt es so viele Studenten wie noch nie: Für das anstehende Wintersemester haben sich 398 000 junge Menschen an bayerischen Universitäten und Hochschulen eingeschrieben. Das sind etwa 4000 mehr als im Vorjahr - ein neuer Rekord, wie Wissenschaftsminister Bernd Sibler (CSU) am Freitag in München sagte. Vor zehn Jahren, zum Wintersemester 2009/2010, waren es noch knapp 272 000 Studenten gewesen. Die Zahl stieg seitdem stetig an.

Unter den Studenten sind rund 68 500 Erstsemester - das entspricht etwa dem Vorjahresniveau. Etwa zwei Drittel der Studenten sind an einer Universität immatrikuliert, ein Drittel besucht eine Hochschule.

""Unsere Hochschulen sind Türöffner für junge Menschen zu einer aussichtsreichen Zukunft. Ich freue mich, dass unsere Hochschulen im Freistaat für jungen Menschen so attraktiv sind"", sagte Sibler. Sowohl in der Lehre als auch in der Forschung böten die bayerischen Hochschulen sehr gute Rahmenbedingungen.

Zum Wintersemester gibt es über 40 neue Studiengänge an Bayerns Hochschulen. Darunter Fächer der Informatik, Digitalisierung, Ingenieurwissenschaft, Gesundheit und Pflege. Außerdem neu: die akademische Hebammenausbildung, die erstmals in Regensburg und München angeboten wird.

Besonderes Augenmerk lenkte Sibler auf die Forschungen rund um das Thema Künstliche Intelligenz (KI). Am Freitag hatte er Vertreter verschiedener Hochschulen in Bayern eingeladen, ihre Arbeiten zu KI der Presse zu präsentieren: Etwa aus der Medizintechnik, wo ein Sensor am Schuh Daten zur Behandlung von Parkinson liefern soll oder aus der Fahrzeugtechnik, bei der sich vieles um die Sicherheit beim automatisierten Fahren dreht. Auch die Behandlung von Allergien wie Heuschnupfen sei dank Künstlicher Intelligenz besser möglich.

""Machine Learning"", ""Clean Tech"" oder ""Quanten-Technologie"": Sibler betonte, dass Bayern ""eine starke Stimme im weltweiten Konzert"" sein möchte, wenn es um Innovationen und Hightech an den Universitäten und Hochschulen im Freistaat geht. Das sollen 1000 zusätzliche Professuren und 10000 neue Studienplätze möglich machen. Insgesamt will die Staatsregierung bis 2023 zwei Milliarden Euro für Wissenschaft und Forschung ausgeben.";https://www.sueddeutsche.de/politik/politik-muenchen-fast-400-000-studenten-in-bayern-eingeschrieben-dpa.urn-newsml-dpa-com-20090101-191011-99-254596;sz.de;DPA
10.09.2019;"Wie ein ""Jugend-forscht""-Sieger Apple überholte";"Es war der 9. Januar 2007, als Steve Jobs auf einer Entwicklungskonferenz in San Francisco das erste iPhone vorstellte. Damit leitete der Technikhersteller Apple eine neue Stufe der digitalen Revolution ein. Zwar gab es schon auf den Klapp- und Schiebe- oder noch altmodischer: Riegelhandys Internet, das war aber höchstens zum Herunterladen von Klingeltönen gut. Quasi über Nacht konnten Besitzer des iPhones auf einmal von überall auf das Wissen der Welt zugreifen und es in die Hosentasche stecken. Schon ein Jahr vor Steve Jobs hatte das ein 19-jähriger Student aus München geschafft: Im Rahmen des Jugend-forscht-Wettbewerbs hatte Robert Bamler die freie Online-Enzyklopädie Wikipedia auf seinen iPod gebracht.

Das war natürlich kein iPod mit großem Farbbildschirm, einem intuitiv zu bedienenden Touchscreen mit brillanten Farben, geschweige denn einem Internetzugang. Es war der iPod Mini, eigentlich nur eine blechgewordene Musikbibliothek mit einem kleinen, beleuchteten Display, das auf wenigen Zeilen Interpreten, Titel, Name des Albums und vielleicht noch den Ladezustand des Akkus anzeigte. Das Gerät wurde über das Clickwheel bedient, ein berührungsempfindliches Klickrad. Auf dem Siegerfoto von ""Jugend forscht"" - Robert Bamler hatte sich mit dem ""oberschlauen iPod-Lexikon in der Hosentasche"" den Bundessieg in der Kategorie Mathematik und Informatik gesichert - hält er stolz das kleine Gerät hoch. Sein kristallklarer Blick durch die dünndrahtige Brille lässt großen Wissensdurst erahnen. Natürlich machte er mit seiner Erfindung aus der Online-Enzyklopädie ein Offline-Lexikon, denn technisch war es mangels Internetzugang damals nur möglich, einen Ist-Zustand der sich ständig erweiternden und aktualisierenden Wikipedia auf den iPod zu spielen. In der prä-hyperdigitalen Epoche, so mutet diese Zeit vor dem iPhone, als der Brockhaus noch kein Staubfänger war, zwölf Jahre später an, war die Idee bahnbrechend, zumal es scheinbar nicht mehr brauchte als ein Musikabspielgerät und einen klugen Kopf mit Sinn für mobiles Wissen.

Aber natürlich war das nicht alles: ""Anders als bei heutigen Smartphones sah der Hersteller Apple Erweiterungen durch Apps offiziell nicht vor. Es gab aber eine Gruppe von hartnäckigen Tüftlern, die es geschafft hatte, eine alternative Bedienoberfläche auf das Gerät zu spielen, die man erweitern konnte. Auf deren Arbeit aufbauend programmierte ich eine App, die eine zuvor aufgespielte Momentaufnahme der Wikipedia durchsuchen und die Artikel auf dem winzigen Display anzeigen konnte"", sagt Bamler heute. Die deutschsprachige Wikipedia hatte damals etwa eine halbe Million Artikel, berichtet er, in komprimierter Form nahm das etwa 700 Megabyte ein, ein Sechstel der gesamten Speicherkapazität des iPods.

Kein ganzes Jahr später war die Idee dann obsolet, weil das iPhone kam. ""Seitdem bin ich nicht mehr der Einzige, der bei Unterhaltungen mit gerade eben nachgeschlagenen Fakten nervt"", sagt Bamler. Heute macht er Karriere in Kalifornien, wenn auch nicht im digitalen Epizentrum Silicon Valley mit Apple, Google, Facebook. Nach seinem Physikstudium an der TU München und der Promotion in Köln verschlug es Bamler in die Vereinigten Staaten nach Pittsburgh in Pennsylvania. Dort arbeitete er in einem Forschungslabor von Disney, im Department für Erlebnisparks. Da gebe es Roboter, Unikate, die man nicht einfach bestellen könne, erklärt der gebürtige Münchner. Deshalb unterhält der Konzern eine eigene Forschungsabteilung.
Machine Learning ist heute Bamlers Thema

Mittlerweile ist er an der Universität in Irvine im Südosten von Los Angeles beschäftigt. Dort befasst sich Bamler mit Machine Learning, versucht also, Computern besseres Verständnis von Menschen beizubringen. Er selbst erklärt das so: ""Traditionelle Computerprogramme laufen nach sehr exakten, aber starren Regeln ab. Menschen können im Gegensatz dazu auch aus emotionalen, inexakten oder lückenhaften Beobachtungen Schlüsse ziehen. Das versuchen wir, auch Computern beizubringen, damit diese zum Beispiel eine E-Mail vom Deutschen ins Arabische übersetzen oder anhand von Blutmessungen in Krankenhäusern vor dem möglichen Ausbruch einer Epidemie warnen können.""

Sein spezielles Interesse gilt aber Algorithmen, also statistischen Methoden, die aus vergleichsweise kleinen Datenmengen lernen und korrekte Schlüsse ziehen können. Grundsätzlich gilt nämlich: Je mehr Daten einem solchen Algorithmus zur Verfügung stehen, desto präziser werden seine Vorhersagen. Denn das ist es, was Algorithmen tun. Sie werten Daten aus, anhand derer sie die Wahrscheinlichkeit für das Eintreten eines bestimmten Ereignisses ermitteln. Vorhersagen sind ungleich schwerer zu treffen, wenn nur wenige Daten zur Verfügung stehen. Ein Beispiel dafür ist die rückwärtige Bildersuche im Internet: Anhand eines eingespeisten Fotos kann die Bildersuche per Algorithmus die Person darauf identifizieren und den dazugehörigen Namen ermitteln. Das setzt natürlich eine möglichst üppige Quellenlage, also viele Bilder der Person im Internet, voraus - damit der Algorithmus das eingespeiste Bild mit den vorhandenen abgleichen und so ein Ergebnis finden kann. Das klappt noch nicht einwandfrei, Bamler testete es extra mit einem alten Bild von sich.

Aktuell arbeitet er an einer Methode, die anhand von Millionen historischer und aktueller Bücher Veränderungen von Sprache erkennt. ""Zum Beispiel möchte ich wissen, ob sich in Reaktion auf gesellschaftliche Neuerungen wie der Industrialisierung oder der MeToo-Debatte der Gebrauch bestimmter Wörter verändert hat. So etwas in diesem Umfang manuell zu untersuchen, wäre aufgrund der hohen Datenmenge utopisch, und dabei würden unterbewusst sicher auch die persönlichen Meinungen der beteiligten Wissenschaftler in die Ergebnisse einfließen."" Algorithmen nehmen Menschen also lästige Arbeit ab und sind am Ende objektiver.

Für die Zukunft wünscht sich der heute 32-jährige Robert Bamler vor allem eine Integration von Programmierkenntnissen in den Lehrplan deutscher Schulen, schließlich sei man heutzutage in fast allen Berufen regelmäßig mit der Verarbeitung digitaler Daten konfrontiert. Und für sich selbst? Vor 13 Jahren hat er ""Jugend forscht"" gewonnen, was macht er in 13 Jahren? ""Ich hoffe, ich werde in der Zeitung von jungen Jugend-forscht-Teilnehmenden lesen, die sich getraut haben, etwas Neues zu erschaffen.""";https://www.sueddeutsche.de/muenchen/muenchen-kalifornien-forschung-apple-wikipedia-1.4591967;sz.de;Jan Luca Künßberg
03.09.2019;Die Datensammlerin;"2016, der US-Wahlkampf ist in vollem Gange. Trolle fluten das Internet mit Hasskommentaren, und Caroline Sinders liest sie, archiviert sie, analysiert sie. Sie weiß, dass ihre Arbeit sinnvoll ist. Sie hilft, die sogenannte Alt-Right-Bewegung, die genau wie Subkulturen auch eigene Wortschöpfungen und Abkürzungen verwendet, besser zu verstehen. Es sind aber auch mindestens acht Stunden täglich, in denen sich Sinders mit Hass, nichts als purem Hass beschäftigt.

Irgendwann hat die Forschungsdesignerin, die sich auf maschinelles Lernen spezialisiert hat, das Bedürfnis, den Kommentaren der Rechten im Netz etwas entgegenzusetzen. Es ist der Moment, in dem Sinders zum ersten Mal darüber nachdenkt, einen feministischen Datensatz aufzubauen. Mit dessen Hilfe will sie den weltweit ersten feministischen Bot trainieren. Es soll ihre Antwort auf den Hass werden.

Knapp drei Jahre später sitzt sie im Berliner Büro von Mozilla und erzählt, dass sie seit 2017 ein feministisches Archiv aufbaut, vergleichbar mit Wikipedia. ""Ich wollte an etwas arbeiten, bei dem ich mich nach acht Stunden Arbeit jeden Tag nicht jedes Mal wie Scheiße fühle"", sagt sie. Die Frau mit den braunen Locken, deren Spitzen türkis gefärbt sind und deren große, braune Augen ernst durch die durchsichtige Hornbrille blicken, hat ein Stipendium der Stiftung der Non-Profit Organisation erhalten. Zuvor hat sie an der New York University den Masterstudiengang Interactive Telecommunications studiert. ""Mich hat einfach schon immer interessiert, wie Menschen online kommunizieren"", erzählt sie.

Aufgewachsen ist Sinders in Louisiana. Ihre Eltern wissen dort zwar, dass ihre Tochter irgendwas mit Technik und Onlinebelästigung macht. Was genau, das kann Sinders ihnen aber nur schwer begreiflich machen - obwohl ihre Eltern auch in technischen Berufen arbeiten.

Wie alt sie ist, verrät die Amerikanerin nicht. Schätzungsweise Ende 20, Anfang 30. Sie ist vorsichtig mit dem, was sie von sich preis gibt, der Schutz ihrer Daten ist ihr wichtig. Auch deshalb gefällt es ihr in ihrer neuen Heimat Berlin: ""Es gibt nicht viele Länder, die so engagiert sind im Bereich Datenschutz wie Deutschland."" Sie lebt in der Wohnung eines Freundes, der statt ihrer nun in New York lebt. Ihr Visum ist zunächst auf drei Jahre befristet, sie kann sich aber vorstellen, länger hier zu bleiben. ""Ich mag meine Heimat, aber gerade habe ich genug von den USA."" Ihr Stipendium bei Mozilla läuft noch bis Dezember.

Das Projekt, das sie bis dahin umsetzen soll, ist zwar nicht der feministische Datensatz oder Chatbot, hängt aber eng damit zusammen: Geht es um Machine Learning, um das Benennen von Daten und Entwickeln von Datenmodellen für einen Algorithmus, kommt in der Regel ""Mechanical Turk"", eine Crowdsourcing-Plattform von Amazon, zum Einsatz. Die Arbeiter, die sogenannten Turks, erledigen Arbeiten, die Computer bislang noch nicht ausführen können. In der Branche sei die Plattform der Standard, doch würden die Turks dabei meist schlecht bezahlt, sagt sie. Aus diesem Grund will sie ihr eigenes, ethisches Mechanical Turk System entwickeln. Es soll transparent sein, von wem welche Daten trainiert werden und zu welchem Zweck. Dadurch dauert zwar alles ein bisschen länger, aber Sinders begreift ihr Projekt ganzheitlich: Nicht nur die Qualität der Daten ist ihr wichtig, sondern auch der gesamte Entstehungsprozess.

Für die US-amerikanische Aktivistin geht es deshalb auch um mehr, als nur darum, einen feministischen Chatbot zu entwickeln. Wenn es ihr darum gegangen wäre, hätte sie Twitter einfach nach dem Schlagwort Feminismus durchsuchen können und mit Mechanical Turk in weniger als einem Tag einen Chatbot erschaffen können, sagt sie. ""Aber wäre das wirklich sinnvoll gewesen?"", fragt Sinders und rückt dabei zum wiederholten Male ihre Brille auf der Nase zurecht. ""Nein.""

Stattdessen hat sie sich dafür entschieden, das Textmaterial in mühevoller Kleinarbeit zusammenzusuchen. Gedankenschnipsel und Ideen, aufgeschrieben auf vielen bunten Post-its. Slow data, langsame Daten nennt Sinders das. ""Es geht nicht darum, alle verfügbaren Texte zu nehmen, sondern darum, die bedeutungsvollen auszuwählen"", sagt sie. Die Daten sammelt sie nicht allein, sondern mit Menschen auf der ganzen Welt. In Workshops, die sie in Nordamerika und Europa gemeinsam mit Museen, Universitäten und Bibliotheken oder auf Digitalkonferenzen wie der Republica in Berlin organisiert hat, hat sie mit Teilnehmern diskutiert, was ein feministischer Text sein kann - und was nicht. Grundsätzlich ist alles erlaubt: Es kann ein Gedicht sein, ein Podcast, ein Songtext, ein Artikel oder ein Buch. Sogar ein aufgezeichnetes Gespräch zwischen zwei Freunden oder aber auch ein Pornodialog kämen in Frage, sagt Sinders. Wichtig sei ihr nur, dass es sich um Werke handele, die Feminismus intersektional begreifen würden. Klingt theoretisch ganz einfach.
Ist ""Frankenstein"" ein feministisches Werk? Nein, aber ein feministischer Akt!

In der Praxis ist es manchmal aber ein bisschen komplizierter: Dann diskutiert Sinders mit den Teilnehmern etwa darüber, ob ""Frankenstein"" von Mary Shelley Teil des Datensatzes wird oder nicht. Das Buch selbst ist kein feministisches Werk, findet Sinders. Dass Shelley es als Frau geschrieben hat, sei aber definitiv ein feministischer Akt, argumentiert sie. Warum also nicht einen Essay finden, der genau diesen Umstand thematisiert - oder, wenn es diesen Essay noch nicht gibt, warum ihn dann nicht selbst schreiben?

Neben der Entscheidung, was nun feministische Daten sind und was nicht, gibt es aus Sicht von Sinders aber noch etwas zu beachten: die Hierarchie der Daten. ""Nicht alle Daten sind gleichwertig"", sagt sie. Es gebe oberflächliche und tiefgründige Daten. Beispiel: Ein T-Shirt mit dem Aufdruck ""The future is female"", sei oberflächlich, weil daraus keine Handlung resultiere. Dasselbe gelte für die sogenannten Pink Pussy-Mützen. Der Womens March, auf dem die Mützen und auch die T-Shirts getragen wurden, sei aufgrund der großen Teilnehmerzahl allerdings sehr wohl bedeutungsvoll. Obwohl Sinders zwischen oberflächlichen und tiefgründigen Daten unterscheidet, sind alle Arten von Daten wichtig für ihre Sammlung. Nur auf die Ausgewogenheit wolle sie achten, sagt sie. Bis Ende Januar will sich Sinders deshalb einige Wochen Zeit nehmen, um die bisher gesammelten Daten zu sortieren und auszuwerten. Sie schätzt, dass sie bereits mehrere Hundert Texte gesammelt hat.

Wie aber soll ein feministischer Datensatz, an dem eine Frau fast alleine arbeitet, ein Gegenentwurf zu rechter Hetze sein, die tausendfach im Netz geteilt wird? ""Jeder Algorithmus kann nur so gut sein wie die Daten, mit denen er gefüttert wird"", sagt Sinders. Nachdem sich ein Großteil des Lebens mittlerweile in der digitalen Welt abspiele, würden Algorithmen Teile der Realität formen, davon ist Sinders überzeugt. Daten, die innerhalb eines sozialen Netzwerks geteilt werden, sind Daten, die von Menschen geschaffen werden. Es sind die Likes, die jemand vergibt, die Art, wie jemand eine Plattform nutzt. Technische Daten werden so bei genauerer Betrachtung zu menschlichen, vertraulichen Daten. Immer öfter würden diese Daten von Firmen dazu verwendet, um potenzielle Kunden zu identifizieren und zu manipulieren, sagt Sinders. Die technischen Prozesse, wie die Algorithmen funktionieren würden, blieben dabei intransparent.

Die Designerin und Wissenschaftlerin will all das ändern. Sie begreift ihr Projekt als ""Arte Útil"": Kunst, die sowohl ein Werk, als auch ein Werkzeug ist. Eine Form des Protest gegen die gesellschaftlichen Verhältnisse. ""Technologie kann Leid, Vorurteile, Frauenhass, Rassismus und weiße Vorherrschaft verstärken"", sagt Sinders. Mit ihrem feministischen Datensatz will sie zeigen, dass es auch anders geht. Und irgendwann verwenden Unternehmen vielleicht ihren feministischen Chatbot, um mit Kunden zu kommunizieren.";https://www.sueddeutsche.de/wirtschaft/mittwochsportraet-die-datensammlerin-1.4585901;sz.de;Jacqueline Lang
30.06.2019;Fox News und Photoshop sind gefährlicher als Deep Fakes;"""Sensationelle Enthüllung: Die Mondlandungen sind ein Schwindel, die USA haben alles gefälscht!"" Wie, das glauben Sie nicht? Aber es steht doch auf Facebook. Immer noch nicht überzeugt? Gut so. Natürlich ist das Unsinn, nur weil es im Netz steht, wird es nicht wahrer. Aber was wäre mit einem Video, in dem Barack Obama und der langjährige Nasa-Chef Charles Bolden beteuern, dass die Verschwörungstheoretiker recht haben?

Technisch wäre das möglich: Sogenannte Deep-Fake-Videos erzeugen nahezu perfekte Illusionen. Mit Hilfe künstlicher neuronaler Netzwerke lassen sich Fotos, Tonaufnahmen und Videos nach Belieben manipulieren. Die Programme verwischen die Grenze zwischen Wahrheit und Fiktion, manche Forscher warnen, die Technik gefährde die Demokratie. Wie groß ist das Risiko tatsächlich? Das Vertrauen ins geschriebene Wort hat das Internet bereits erschüttert. Jeder kann Falschmeldungen auf Facebook weiterverbreiten und neue Lügen in die Welt tippen. Audioaufnahmen und bewegte Bilder gelten dagegen immer noch als vertrauenswürdig.

Im Zeitalter von Deep Fakes sei das ein Fehler, sagt Hany Farid, Informatikprofessor der Universität Berkeley: ""Wir steuern auf einen Punkt zu, an dem normale Menschen nicht mehr zwischen Fake und Original unterscheiden können."" Farid spricht von einem Kampf mit ungleichen Waffen: ""Es ist unfair. Auf einen Menschen, der daran forscht, Manipulationen zu erkennen, kommen hundert, die immer perfektere Fälschungen entwickeln.""

Die Fälscher profitieren sogar von der Arbeit der Aufklärer. Im vergangenen Jahr entdeckte Siwei Lyu, Leiter des Zentrums für maschinelles Lernen der Universität Albany, dass sich viele manipulierte Videos ganz einfach erkennen lassen: Als Ausgangsmaterial dienen oft Fotos - und da es kaum Fotos mit geschlossenen Augen gibt, blinzeln die Protagonisten nicht. Zwei Wochen später schrieb ihm ein Fälscher: Danke für den Hinweis, die Menschen in unseren neuen Deep Fakes blinzeln jetzt auch.
""Denken Sie daran, wie Schüler damit Klassenkameraden mobben""

Einer der Wissenschaftler, die an der Erkennung von Deep Fakes arbeiten, ist Matthias Nießner. Der Professor für ""Visual Computing"" der TU München hat die Software Face Forensics entwickelt. Sie schlägt Fälscher mit ihren eigenen Waffen: Neuronale Netze werden mit etlichen gefälschten Videos gefüttert, bis sie regelmäßige Muster finden, die dem menschlichen Auge oft verborgen bleiben. Legt man der künstlichen Intelligenz ein Standbild aus einem Deep Fake vor, entlarvt sie es in acht von zehn Fällen als Fälschung. Ungeschulten Testpersonen gelingt dies in 52 Prozent der Fälle.

""Von der Informationsapokalypse sind wir noch weit entfernt"", sagt Nießner. ""Ich bekomme pro Woche zehn E-Mails: 'Ich habe ein Deep-Fake-Video gebastelt, aber es sieht bescheuert aus. Können Sie uns helfen?' Die Technik ist einfach noch nicht so weit."" Für täuschend echte Manipulationen brauche es Experten, leistungsfähige Hardware und viel Zeit.

Derzeit gibt es vor allem einen Einsatzzweck für Deep Fakes: Pornografie. Frauenfeinde erstellen Hardcore-Videos mit Gesichtern von prominenten Schauspielerinnen und lassen sich in misogynen Ecken des Netzes dafür feiern. ""Das muss dann gar nicht perfekt gefälscht sein, um Schaden anzurichten"", sagt Nießner. ""Denken Sie nur daran, wie Schüler das einsetzen könnten, um Klassenkameraden zu mobben.""
Für die Betroffenen ist das entsetzlich, die politischen Verwerfungen beschränken sich bislang aber auf Einzelfälle. Gabuns Präsident Ali Bongo soll etwa mit einem Deep-Fake-Video seine Neujahrsansprache inszeniert haben, um seine Genesung nach einem Schlaganfall vorzutäuschen und Neuwahlen zu verhindern. Die Kontroverse um die mögliche Fälschung löste einen Putschversuch aus. In Malaysia kursierte kürzlich ein Video, in dem ein Mann angeblich zugibt, Sex mit einem Politiker gehabt zu haben. Experten halten beide Videos für Fälschungen, können es aber bislang nicht beweisen.

""Was das politische Manipulationspotenzial angeht, sehe ich viel Hype"", sagt Nießner. Um Unruhe zu stiften, seien keine Deep-Fakes nötig. ""Warum sollte ich einen Machine-Learning-Ingenieur bezahlen, wenn ich mit weniger Aufwand 500 Photoshop-Fakes basteln kann, die viel mehr bewirken?""

Ein Vorfall Ende Mai belegt das Potenzial billiger Manipulationen, genannt ""Cheap Fakes"": Millionen Menschen sahen auf Facebook ein Video von Nancy Pelosi. Die Demokratin wirkt betrunken. Hochrangige Republikaner, darunter Trump-Berater Rudy Giuliani, teilten den Link. Tatsächlich trinkt Pelosi gar keinen Alkohol - die Originalaufnahme wurde nur verlangsamt, was Zweifel an der Zurechnungsfähigkeit weckt.
In zwei bis drei Jahren könnten Deep Fakes richtig gefährlich werden

Den viralen Erfolg der billigen Fälschungen erklärt Nießner mit einer Theorie aus der Kognitionspsychologie: ""Der Bestätigungsfehler besagt, dass Menschen glauben, was sie glauben wollen. Das hängt oft mit der politischen Überzeugung zusammen."" Studien zur Verbreitung von Falschnachrichten bestätigen die These: Geteilt wird, was ins Weltbild passt.

Derzeit mag von Deep Fakes noch keine große Gefahr für die Gesellschaft ausgehen. 2022 könne das schon anders aussehen, sagt Nießner: ""Ich gehe davon aus, dass die Technik in zwei bis drei Jahren so weit ist, dass auch normale Nutzer ziemlich gute Fälschungen erstellen können."" Das Szenario klingt bedrohlich: Manipulierte Videos fluten das Netz, wo sie auf Massenmedien und soziale Netzwerke treffen, die auf Emotionen, Reichweite und schnelle Verbreitung getrimmt sind.
Fox News richtet mehr Schaden an als alle Deep Fakes zusammen

Hinzu kommt, dass die Fälschungen die Glaubwürdigkeit von Fakten beschädigen. ""Deep Fakes erleichtern es Lügnern, die Verantwortung für ihre Taten von sich zu weisen"", sagt die Bostoner Juraprofessorin Danielle Citron, die zu den Auswirkungen von Deep Fakes forscht. US-Präsident Trump hat bereits versucht, echte Videos als Fälschungen abzutun. Seriöse Medien widerlegten seine Behauptungen, doch Trumps Anhänger sind eher bei Fox News zu Hause.

Dieser Fernsehsender zeigt zugleich, dass es keine KI braucht, um Schaden anzurichten. Die Verschwörungstheorien über die Mondlandung wurden maßgeblich durch eine umstrittene Doku befeuert, die Fox News 2001 ausstrahlte. Der Sender formt das Weltbild vieler US-Amerikaner bislang sicher stärker als ""Fake News"" und Deep Fakes zusammen.";https://www.sueddeutsche.de/digital/deep-fake-video-faelschung-1.4541336;sz.de;Simon Hurtz
28.07.2019;Gesunder Menschenverstand;"Es ist heutzutage nicht mehr so leicht, als Mensch selbstbewusst durch die Welt zu gehen. Software ist uns mittlerweile auf vielen Feldern ebenbürtig oder gar überlegen, die wir bis vor Kurzem noch als exklusiv menschlich beanspruchten. Das reicht von Disziplinen, die schon lange verloren gegangen sind, wie etwa Schach, hin zu neueren Verlustschlachten wie etwa Komposition oder Malerei. Vor Kurzem folgte der nächste intellektuelle Tiefschlag: Ein Algorithmus des chinesischen Internet-Konzerns Alibaba hat einen neuen Rekord für Textverständnis aufgestellt - und auch hier den Menschen hinter sich gelassen.

Der Test besteht darin, echte Fragen, die Nutzer an Microsofts Suchmaschine stellen, zu beantworten. Gefragt wird etwa: ""Was ist ein Unternehmen?"", ""Reihenfolge der größten Städte in Illinois"" oder ""wie viele Kohlenhydrate hat Spargel?"" Millionen solcher Dringlichkeiten sind in der ""Microsoft Machine Reading Comprehension""-Datenbank gespeichert. Das KI-Modell erreichte einen Wert von 0,54 und schlägt damit die beste menschliche Punktzahl. Wenn auch nur knapp, der menschliche Wert liegt bei 0,539.

Das hinderte besorgte Medien freilich nicht daran zu fragen, ob die Maschinen nun schlauer geworden seien als wir Menschen. Und wann sie uns die Arbeit wegnehmen und uns auch anderweitig unterjochen. Die Antworten lauten natürlich: Nein, sind sie nicht, werden sie nicht.

Selbst wenn die KI eine bessere Punktzahl erreicht, bedeutet das nicht, dass sie versteht, was sie da beantwortet. In ihrem Inneren geht es um statistische Mustererkennung, nicht um ein Verständnis der Wörter. Selbst Li Suo, der Chefentwickler, gibt zu, dass ""Maschinen das logische Denken fehlt. Das unterscheidet sie davon, wie Menschen Sprache nutzen."" Der Weltuntergang ist also vorerst vertagt.

Genau darum, Maschinen logisches Denken beizubringen, macht sich seit einiger Zeit Yann LeCun Gedanken. Er ist Chef der KI-Forschung bei Facebook. Und er glaubt, dass eine Revolution der künstlichen Intelligenz erst dann möglich wird, wenn die Maschinen keine menschliche Anleitung mehr benötigen.

Selbst wenn Software mittlerweile besser Bilder oder Sprache erkennt als Menschen, braucht sie, bis es soweit ist, wahnsinnig viele Anläufe. Um einen Elefanten sicher zu identifizieren, muss man einer künstlichen Intelligenz Tausende Bilder von Elefanten vorlegen, die entsprechend markiert sind. Jedem Kleinkind reichen dagegen zwei Beispiele, dann ist die Sache geklärt. Ähnlich funktioniert es im Bereich des autonomen Fahrens. Einem Teenager reichen 30 Fahrstunden, um halbwegs sicher unterwegs zu sein. Selbstfahrende Autos spulen Millionen Kilometer ab und sind immer noch nicht verkehrssicher.

Was den Maschinen abgeht, ist ein grundsätzliches Verständnis von Realität, so wie es Menschen in ihrer frühen Kindheit entwickeln. Eine mögliche Lösung sieht LeCun in einer Technik namens ""Unsupervised Learning"". Anders als bei den am häufigsten verwendeten Methoden, die entweder mit Belohnungseffekten arbeiten oder mit menschlicher Vorarbeit, zieht die künstliche Intelligenz in diesem Fall Muster aus bestehenden Daten ohne jede Hilfe von Außen. Sie erschließt sich die Welt aus sich selbst heraus.

Maschinelles Lernen müsse das menschliche Vorbild nachahmen, so LeCun. Unsupervised Learning könne helfen, ein Modell der Welt zu imaginieren, mit dem Vorhersagen zu ihrem künftigen Zustand möglich wären. Mit anderen Worten, es würde ihnen gesunden Menschenverstand verleihen.";https://www.sueddeutsche.de/kultur/netzkolumne-gesunder-menschenverstand-1.4542805;sz.de;Michael Moorstedt
14.06.2019;Wie China mit Forschung seine Macht ausbaut;"Als Anfang des Jahres die Raumsonde Chang'e 4 im Von-Kármán-Krater aufsetzte, vibrierte kurz drauf Jiao Weixins Telefon in Peking. Die alten Kollegen schrieben per SMS, dass die Mission ein voller Erfolg gewesen sei. Als erster Nation ist es China gelungen, auf der Rückseite des Monds zu landen - ein äußerst schwieriges Manöver, die Gegend liegt nämlich im Funkschatten der Erde. Jiao schaltete den Fernseher ein, die Nachrichtensendungen berichteten bereits von der Landung. Wenig später konnte er sich selbst sehen, ein Sender hatte ihn am Tag zuvor interviewt, als Experten: Jiao Weixin, Professor an der Peking-Universität und Pionier der chinesischen Raumfahrt, so wurde er vorgestellt.

Vor 50 Jahren war daran nicht einmal zu denken: Dass Neil Armstrong 1969 als erster Mensch den Mond betreten hatte, erfuhr Jiao erst Monate später - durch ein Unglück. Er hatte gerade sein Studium der Geophysik beendet und war als Lehrling in einer Brigade untergekommen, die in Nanjing, während der Hochzeit der Kulturrevolution, Radaranlagen baute. Im Unterschied zu vielen seiner Kommilitonen ging es ihm relativ gut, er wurde nicht aufs Land verbannt, musste nicht schwere Feldarbeit leisten.
China publiziert mehr wissenschaftliche Aufsätze als jedes andere Land

Es war Mitte April 1970 als der Leiter der Einheit eine Sitzung einberief und erzählte, dass Beamte aus Peking sich gemeldet hätten. Die US-Regierung habe um Mithilfe gebeten, bei einer Mondmission sei es zu einen Zwischenfall gekommen, es sei möglich, dass eine amerikanische Raumkapsel in China notlanden müsse. ""Es ging um Apollo 13. Den Namen hatten wir noch nie gehört. Genauso wenig wussten wir, dass Flüge zum Mond überhaupt möglich waren."" Dieser Tag, sagt Jiao, habe sein Leben verändert. Seitdem widmet er sich der Raumfahrt. 1991 schlug die Chinesische Akademie der Wissenschaften eine eigene Monderkundungsmissionen vor, 2003 flog der erste Chinese ins All, 2007 umkreiste die Sonde Chang'e-1 den Erdtrabanten. 2022 soll eine eigene Raumstation einsatzbereit sein. Zwei Jahre bevor die Internationale Raumstation ISS voraussichtlich ihren Betrieb einstellt. An der ISS war chinesischen Forschern vom US-Kongress aus Furcht vor Spionage die Arbeit noch verwehrt worden. Schon bald könnten Europäer und Amerikaner in Peking anfragen, ob sie mitforschen dürfen.

Innerhalb weniger Dekaden ist es China gelungen, vom Außenseiter in der Wissenschaft zur Forschungsnation aufzusteigen, und dass nicht nur in der Raumfahrt. In fast allen naturwissenschaftlichen Disziplinen, sind chinesische Forscher inzwischen auf Augenhöhe. Von 2000 bis 2016 haben sich die Ausgaben für Forschung und Entwicklung in der Volksrepublik um den Faktor zehn erhöht und der Ehrgeiz, dies weiter zu steigern, ist ungebrochen. Wissenschaftler aus keinem anderen Land veröffentlichen inzwischen mehr Aufsätze als aus China, wie eine Auswertung von 17,2 Millionen Fachartikeln, die zwischen 2013 und 2018 publiziert wurden, zeigt. In 23 von 30 Disziplinen lagen chinesische Forscher vorne - zumindest quantitativ.

Eng verknüpft sind die chinesischen Forschungsziele mit der industriepolitischen Agenda der Führung in Peking. 2013 stellte eine kleine Gruppe von Wissenschaftlern dem Staatsrat eine Untersuchung vor. ""Strategiestudie über das verarbeitende Gewerbe in China. Die Transformation von groß zu stark"", lautete der Titel. Einer der Autoren war Liu Baicheng, einer der angesehensten Ingenieure des Landes. 1978 gehörte er der aller ersten Delegation chinesischer Wissenschaftler an, die in die Vereinigten Staaten geschickt wurde. Knapp drei Jahre studierte er an der University of Wisconsin und am MIT. Frau und Kind lebten weiter in Peking, nur per Brief hielt er Kontakt. 1981 kehrte er heim und wurde dann zu einem der Gründungsväter der chinesischen Ingenieurwissenschaften. Anfang März 2015 erfuhr die Öffentlichkeit davon, was sich Liu Baicheng und seine Kollegen überlegt hatten. ""Made in China 2025"", nannte Premierminister Li Keqiang in seiner der Auftaktrede zum alljährlichen Volkskongress das Konzept. Vor vier Jahren konnte kaum einer etwas damit anfangen. Heute ist klar, es ist die ehrgeizigste industriepolitische Blaupause der Welt. In zehn Branchen sollen Unternehmen aus der Volksrepublik zur Weltspitze gehören:Unter anderem in der Elektromobilität, der digitalisierten Produktion, der Pharmaindustrie, der Medizintechnik und der Chip-Fertigung. Entwicklungsbanken und extra eingerichtete Fonds versorgen Firmen der ausgewählten Branchen mit günstigen Krediten. Hunderte Milliarden stehen bereit. Auch die Universitäten forschen im Auftrag des Staates, um Drittmittel muss sich niemand sorgen.

Viele der Wissenschaftler arbeiten nicht ausschließlich an ihren eigenen Projekten, sie beraten auch die Regierung. ""Es ist eine soziale Dienstleistung"", nennt ein Professor aus Peking das. Etwa 20 Prozent seiner Arbeitszeit steckt er darein, in Ausschüssen zu sitzen und sein Fachwissen an Ministerien weiterzugeben. ""Daraus wird dann die langfristige Planung entwickelt."" Die dann Top-Down umgesetzt wird - sogar von ganz oben.
KI-Wettrüsten zwischen China und den USA

Als der mächtige Staats- und Parteichef Xi Jinping (wie seine Vorgänger auch ein Naturwissenschaftler) seine Neujahrsansprache 2018 hielt, saß er wie immer vor einem Bücherregal. Bände von Karl Marx standen dort aufgereiht, auch Ernest Hemingways ""Der alte Mann und das Meer"". Neu waren in Xis Rücken jedoch zwei Standardwerke zur Künstlichen Intelligenz (KI). ""Augmented: Life in the Smart Lane"" und ""The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World"". Ein sehr deutliches Signal, wohin die Führung steuern möchte.

Ein KI-Wettrüsten zwischen China und den Vereinigten Staaten ist im vollem Gange. 2020, glauben Fachleute, könnte die Volksrepublik bereits gleichgezogen haben. 2030 sollen chinesische Unternehmen die dominierenden Spieler auf dem Weltmarkt sein, so der Plan der Regierung. 150 Milliarden Dollar soll dann die Branche in China jährlich umsetzen. ""Noch sind wir nicht so weit"", sagt Zhang Bo, vom Institut für Informatik und Technologie an der Tsinghua-Universität in Peking. ""Bei der Anwendung, etwa der Sprach- oder Gesichtserkennun,g sind wir in China schon gut aufgestellt. Es mangelt jedoch an der Grundlagenforschung."" Die wichtigsten Auszeichnungen im Fach hätten fast ausschließlich Amerikaner sowie ein Kanadier erhalten. ""Ich empfehle meinen Studenten daher ausdrücklich, ein paar Semester in den USA zu verbringen"", sagt Zhang. Zu wenig Grundlagenforschung, genau das ist die Schwäche des Top-Down-Modells. Die Chance zu scheitern, wird oft nicht eingepreist. Die Folge: ""Etwa 70 Prozent der Studenten wählen populäre Themen"", sagt KI-Forscher Zhang Bo. ""Die Durchbrüche erzielt man aber meistens in den weniger angesagten Bereichen, den Nischen.""
Ausländische Experten wollen nicht in die arme Provinz

Manchmal geht Pekings Top-Down-Ansatz auch völlig schief. In der bitterarmen Provinz Guizhou, die für ihre malerischen Karstberge bekannt ist, ließ die Regierung das größte Radioteleskop der Welt bauen, es verfügt über einen Hauptspiegel mit 520 Meter Durchmesser. Technisch funktioniert die Anlage einwandfrei, es fehlt jedoch an Fachleuten, die damit umgehen können, und ausländische Spezialisten für das abgelegene Guizhou zu begeistern, schlug seit der Eröffnung 2016 fehl.

Dass Internationalisierung notwendig ist, hat man in Peking im Prinzip verstanden. Bereits 2008 führte die Regierung das sogenannte Tausend-Talente-Programm ein, mit dem Forscher in die Volksrepublik gelockt werden sollen. Es steht jedem offen, de facto werden aber fast ausschließlich chinesische Absolventen und Doktoranden gefördert, die aus Europa und den USA zurückkehren. In China nennt man diese Heimkehrer ""Meeresschildkröten"". Wie die Tiere, die immer nach Hause schwimmen, um am selben Strand ihre Eier abzulegen. Zu Hause dürfen sich die Wissenschaftler über üppige Forschungsbudgets freuen, doch es gibt auch eine Kehrseite.

Für eine im vergangenen Jahr veröffentlichte Studie der University of California, Santa Barbara, wurden 731 chinesische Wissenschaftler befragt. Das Ergebnis: Der Druck an chinesischen Universitäten ist gewaltig. ""Leute erfinden oder plagiieren Aufsätze, damit sie ihre jährlichen Leistungsbeurteilungen bestehen können"", wird etwa ein Wissenschaftler der Fudan-Universität aus Shanghai zitiert. Ein Forscher von der Tsinghua-Universität erzählt, dass am Institut für Fahrzeugtechnik drei bis vier Veröffentlichungen pro Jahr erwartet werden. ""Im vergangenen Jahr bin ich auf elf Papers gekommen."" Jedoch mit welcher Qualität?

Der Wunsch erster, bester und schnellster zu sein, mag auch den Biogenetiker He Jiankui angetrieben haben. Er hatte in den Vereinigten Staaten studiert und kam als Schildkröte zurück. Bis vergangenen November konnten allerdings selbst Fachleute mit dem Namen kaum etwas anfangen. Bis er über Nacht berühmt wurde und neben der Mondmission den zweiten großen Anlass zur Berichterstattung über Chinas Wissenschaft in den vergangenen zwölf Monaten lieferte.
Und dann kam der ethische Dammbruch

Genau einen Tag, bevor die wichtigste Konferenz des Fachs in Hongkong anfing, lud er bei Youtube drei Videos hoch, gefilmt in seinem Labor in Shenzhen. He erzählte darin von Lulu und Nana, von Grace und Mark. Von einer glücklichen Familie, wie er sagte - dank ihm. Grace und Mark, das sind die Pseudonyme der Eltern. Lulu und Nana sind Zwillinge, die ersten Kinder, deren Gene verändert worden sind. Weil der Vater mit HIV infiziert ist, manipulierte He mit der Genschere das Erbgut so, dass die Babys gegen das Virus immun sind - medizinisch kein notwendiger Eingriff, aber ein ethischer Dammbruch. Bis dahin hatte es eine klare Übereinkunft unter Forschern weltweit gegeben: Veränderungen an der menschlichen Keimbahn, sind tabu, zu groß sind die Risiken, das Erbgut zu beschädigen.

Auf der Konferenz in Hongkong meldete sich dann ein Professor der Peking-Universität zu Wort: ""Es gibt unter chinesischen Wissenschaftlern eine klare Vereinbarung, dass wir nicht an Menschen forschen"", sagt er. ""Warum haben Sie die klinischen Studien heimlich durchgeführt?"", fragte er erbost. ""Die Leute brauchen Hilfe. Wir haben die Technologie dazu, also sollten wir ihnen auch helfen"", antwortete He. Es sollte sein letzter öffentlicher Auftritt sein. Forschen darf er seitdem nicht mehr, mutmaßlich steht er unter Hausarrest.

Die Führung in Peking ist unerbittlich, wenn es um die Zukunft Chinas geht.";https://www.sueddeutsche.de/wissen/china-wissenschaftspolitik-raumfahrt-genforschung-1.4486440;sz.de;DPA
11.06.2019;Konsole mio;"Es gibt ein Gebäude ohne Fenster auf dem Campus von Microsoft. Niemand soll erfahren, woran sie genau arbeiten in diesem grauen Kasten mit dem Namen B87. Darin befindet sich zum Beispiel der leiseste Raum auf diesem Planeten, eine Kammer mit 200 Kameras zum Aufzeichnen selbst kleinster Bewegungen, biomechanische Tracking-Geräte. Sie vermessen den Menschen bis ins kleinste Detail, wollen wissen, wie er sich bewegt, wie er fühlt, was ihm gefällt. Vielleicht ist es Zufall, dass sie das, was sie bei dieser bombastischen Show in Los Angeles (auf der Bühne zum Beispiel: Hollywood-Schauspieler Keanu Reeves) vorgestellt haben, zunächst in diesem unscheinbaren Gebäude zeigten. Vielleicht aber auch nicht.

Tatsächlich steht die Computerspielbranche, die auf zwei Milliarden Spieler weltweit angewachsen ist und für einen Jahresumsatz von 140 Milliarden Dollar sorgt, vor der größten Disruption ihrer Geschichte - also seit der Brown Box des deutsch-amerikanischen Entwicklers Ralph Baer 1968. Es gab legendäre Geräte wie Playstation 2 (Sony), 2600 (Atari) oder Dreamcast (Sega), und bislang waren die Spieler abhängig von der Hardware. Rechen- und Grafikpower des Geräts bestimmten darüber, welche Spiele über den Bildschirm flimmerten. Für den neuesten Megatrend, das sogenannte Cloud Gaming, braucht es nur noch ein internetfähiges Gerät mit Bildschirm egal welcher Größe und eine ausreichend schnelle Verbindung. Das Videospiel läuft auf einem Server, irgendwo auf der Welt, Bild und Ton empfängt der Spieler übers Internet. Was sie bei der Microsoft-Show gezeigt haben, soll zur Dreifaltigkeit dieser Branche werden: Erstens soll mit Project Scarlett eine leistungsstarke Konsole zum Weihnachtsgeschäft 2020 auf den Markt kommen und mit der Playstation 5 des japanischen Herstellers Sony konkurrieren. Zweitens kann jeder Besitzer einer Xbox sein Gerät von Oktober an beim sogenannten ""Konsolen-Streaming"" als privaten Server nutzen, um seine Xbox-Spiele per Stream auch auf Smartphone und Tablet zu spielen. Und drittens soll es auch möglich sein, ganz ohne Konsole zu spielen. Unter dem Namen Project Xcloud fasst das Unternehmen die zwei Modi seines Streamingdienstes zusammen.

Schon 2018 hatten die großen Unternehmen auf der Electronic Entertainment Expo (E3), dem bedeutsamsten Treffen der Branche, angedeutet, dass sich die Zukunft mehr und mehr in der Datenwolke abspielen wird. In diesem Jahr, die Messe beginnt offiziell an diesem Dienstag, werden Streamingdienste das prägende Thema sein. Google hat bereits in der vergangenen Woche angekündigt, im November seinen Streamingdienst Stadia starten zu wollen. Andere dürften mit eigenen Plattformen wie etwa Geforce Now (Nvidia) und Playstation Now (Sony) folgen.

Klar klingt das erst einmal nach Revolution, wenn es keine Konsole mehr unter dem Bildschirm geben soll. Andererseits ist diese Revolution in anderen Sparten der Unterhaltungsbranche längst passiert. Kaum jemand besitzt noch Plattenspieler, es sei denn aus Retro-Gründen, oder DVD-Player, und selbst Fernseher gelten mittlerweile nicht mehr als dringend nötiger Bestandteil des Wohnzimmers. Es soll also Streamingdienste für Videospiele geben, so wie es für Filme und Serien bereits Netflix, Amazon Video oder Hulu gibt und für Musik Portale wie Spotify, Tidal oder Apple Music.
Das Streamen von Spielen ist riskanter als das von Musik - für die Kunden geht es um mehr

""Es wird sich so anfühlen, als würde man auf einem lokalen Gerät spielen"", sagt Jack Buser, bei Google verantwortlich für die Spiele bei Stadia. Genau das wird der Knackpunkt sein beim Streamen von Videospielen - egal, ob über die Rechenzentren von Google, Microsoft oder Nvidia: Wenn ein Film kurz ruckelt oder ein Lied nicht sogleich abgespielt wird, dann ist das ärgerlich. Wenn jedoch beim Zocken über das Internet das Spiel ausfällt oder auch nur das Bild für wenige Sekunden pixelig wird, kann das spielentscheidend sein. Bei Video-Streams kommt das regelmäßig gerade dann vor, wenn viele Menschen gleichzeitig das Internet nutzen.

Das führt zum Grund, warum Microsoft seinen Streamingdienst erst mal in diesem Gebäude ohne Fenster zeigt und derart zurückhaltend ankündigt, dass es zwischen all den Reklame-Superlativen umso mehr auffällt. Das Unternehmen hatte bei der Vorstellung der letzten Konsolen-Generation 2013 die Ankunft des digitalen Zockens verkündet - aus heutiger Sicht eine visionäre und richtige Prophezeiung. Damals jedoch galt sie als abgehoben, die als bodenständiger geltende Sony-Konsole Playstation 4 verkaufte sich erst einmal deutlich besser. Es scheint, als würde nun keiner dastehen wollen als derjenige, der Cloud Gaming lautstark ausruft und dann beim ersten Ruckeln der Bilder einen Proteststurm der Kategorie 5 verarbeiten muss.

Google hat seinen Dienst bereits vor einigen Monaten von Spielern testen lassen. Gemäß diesen Erfahrungswerten wird dem Unternehmen zufolge für flüssiges Streamen in höchstmöglicher Qualität eine Internetverbindung benötigt, die 35 Megabit Daten pro Sekunde überträgt; dann sollen beim Nutzer die Bildauflösung 4K, die Rastergrafik High Dynamic Range und Surround-Sound ankommen. Der kommende Mobilfunkstandard 5G könnte dem Cloud Gaming durch Datenraten im Gigabit-Bereich einen immensen Schub verleihen. Nvidia kooperiert deshalb mit zwei Telekommunikationsfirmen in Korea und Japan, die das Spielestreamen auf der Plattform Geforce Now als Anwendung über 5G anbieten. Microsoft, Amazon und Google gelten derzeit als jene Unternehmen, denen die Umsetzung der Infrastruktur am ehesten gelingen könnte.
Es ist letztlich ein Kampf um die Freizeit des Menschen

""Wir werden Dinge sehen, die mit einer Konsole oder einem PC nicht möglich sind"", verspricht Googles Jack Buser. Es soll Spiele geben, in denen sich computergesteuerte Mitspieler und Gegner so verhalten wie echte Menschen, weil sie von Googles Machine-Learning-Algorithmen trainiert worden sind. Auch Mehrspieler-Partien fürs Guinness-Rekordbuch sind denkbar, weil Tausende Spieler zugleich an einer Runde teilnehmen. Oder eine Cross-Verbindung mit anderen Plattformen wie zum Beispiel Youtube: Es soll möglich sein, ein Video über ein Computerspiel von seinem Lieblingsspieler auf der Video-Plattform zu gucken und dann per Mausklick an exakt dieselbe Position im Spiel zu springen. ""Wenn Gamer da einmal auf den Geschmack kommen, wird sich danach das Spielen auf einer Konsole sehr eingeschränkt anfühlen"", sagt Buser.

Die Anbieter von Cloud Gaming versprechen grenzenlosen Spaß, die Möglichkeit zum Immer-und-überall-Zocken und damit veritable Gründe beim Kampf um die Freizeit des Menschen. Genau darum geht es letztlich, und deshalb vermessen sie die potenziellen Kunden bis ins kleinste Detail in Gebäude B87 auf dem Microsoft-Campus: Zeit ist nach Gesundheit das kostbarste Gut des Menschen, und wer möglichst viel davon bekommt, der dürfte diesen mit harten Bandagen geführten Kampf der Unterhaltungsbranche gewinnen. Das konsolenfreie Zocken wird kommen, und dann dürfen sich die Menschen auch überlegen, wonach sie ihre Wohnzimmer ausrichten, wenn da nicht mehr unbedingt ein Fernseher rumstehen muss. ";https://www.sueddeutsche.de/digital/konsolen-gaming-xbox-playstation-cloud-1.4480975;sz.de;Caspar von Au und Jürgen Schmieder
10.06.2019;Geld zurück;"Wenn ein Unternehmen antritt mit der Mission, sich überflüssig zu machen, ist das einerseits ungewöhnlich. Andererseits: Wenn es keine Probleme zu lösen gäbe, hätte manches Start-up kein Produkt. Und so sagt Björn Waide, Chef von Smartsteuer: ""Eigentlich dürfte es uns nicht geben, wenn der Staat seiner Verantwortung gerecht würde.""

Smartsteuer nimmt Menschen seit 2010 die Steuererklärung ab. Nutzer geben ihre Grunddaten online oder per App ein und werden in Form eines Interviews durchs Programm geführt. Währenddessen sehen sie, wie hoch die Steuererstattung des Finanzamts ausfallen könnte. Jede abgegebene Erklärung kostet 24,95 € und wird über eine Schnittstelle des staatlichen Programms Elster beim Fiskus eingereicht.

Waide, 40, ist seit 2013 Geschäftsführer des hannoverschen Unternehmens, zuvor arbeitete er beim Berufsnetzwerk Xing. Er ist Anhänger von New Work, einem Konzept, nach dem Menschen mit mehr Selbständigkeit, Freiheit und Teilhabe ihr Arbeitsleben gestalten. Das lebe er nicht nur voller Überzeugung, sondern es sei auch für das Unternehmen wichtig: Bei Smartsteuer arbeiten Menschen, die aus Berlin oder dem holländischen Groningen pendeln und ansonsten im Home-Office arbeiten. ""Das ist auch wichtig, um Leute nach Hannover zu locken"", so Waide. Denn einfach ist es nicht, Mitarbeiter zu finden - die Konkurrenz in der Gründerhauptstadt Berlin ist groß, das Thema Steuern klingt nun mal auch nicht so sexy.

Der Informatiker ist in der Gründerszene Hannovers ein bekanntes Gesicht. Im Gründerzentrum Hafven betreut er als Mentor regelmäßig Start-ups und sitzt im Beirat des Netzwerks Digitales Hannover. Das eigene Unternehmen ist allerdings schon zu alt und mit der Verlagsgruppe Haufe als Muttergesellschaft zu weit, um noch als Start-up zu gelten. Es ist ein sogenanntes Scale-up, das sich aufs Wachstum konzentriert: ""Wir haben bewiesen, dass wir einen Markt gefunden haben und davon leben können. Das ist aber nur die technische Basis, jetzt geht es erst richtig los"", sagt Waide. Derzeit sucht das Unternehmen nach neuen Geschäftsmodellen.

Dabei sollen die Daten helfen, die die Smartsteuer-Software ohnehin generiert. Sie arbeitet mit Machine-Learning und wird mit anonymisierten echten Steuerfällen trainiert. Das Programm gleicht die gemeldeten Zahlen mit dem Steuerbescheid ab, den das Finanzamt über die Elster-Schnittstelle zurückschickt. So kann das Team Abweichungen analysieren und Vorhersagen über Rückzahlungen verbessern. Seit 2018 kooperiert das Unternehmen mit der Direktbank ING, deren Kunden eine eingeschränkt vorausgefüllte Steuererklärung nutzen können.

Doch das soll in Zukunft viel weiter gehen, schließlich wissen Banken und Staat mehr, als sich in die Formulare automatisiert eintragen lässt: Bekommt jemand Kindergeld, Rente, Verheiratetenzuschlag? ""Wir wollen Daten, die schon überall existieren, so verarbeiten, dass außer einem kontrollierenden Blick nichts mehr nötig ist"", so Waide.

Nach wie vor ist aber der Staat mit Elster der größte Wettbewerber, auch wenn elf Millionen Menschen in Deutschland keine Steuererklärung abgeben - sie gehen davon aus, dass sie ohnehin keine Rückzahlung bekommen und sparen sich den Aufwand. Waide stellt daher klar, dass sein Unternehmen nicht im Markt für Steuererklärung unterwegs sei, sondern im Markt für Steuererstattung. Und die stehe den Bürgern eigentlich zu. Trotzdem rechnen sie nicht mit dem Betrag, sondern freuen sich darüber wie über einen extra Geldsegen: ""25 Prozent unserer Kunden fahren von dem Geld in den Urlaub"", so Waide. Jedes Jahr holt das Team mehr als 300 Millionen Euro vom Staat zurück.";https://www.sueddeutsche.de/wirtschaft/nahaufnahme-geld-zurueck-1.4480971;sz.de;Katharina Kutsche
15.05.2019;Mit Lügen spielen;"Kanzlerin Angela Merkel möchte der Politik den Rücken kehren. Tippt man dieses Szenario in englischer Sprache in den gerade veröffentlichten Fake-News-Bot ein, so fügt dieser hinzu, dass der Plan ambitionierter sei als die ""der meisten deutschen Anführer"". Der Plan sei politisch motiviert und folge einer Intention Merkels aus dem Jahr 2017, ins Europäische Parlament einzuziehen. ""Das sind keine guten Neuigkeiten für die deutsche Verfassung"", heißt es weiter. Merkel wollte ins Europäische Parlament? 2017? Auszuschließen. Aber wäre es nicht irgendwann denkbar?

Allein diese Kombinationsleistung ist schon bemerkenswert. Der Fake-News-Bot GPT-2, den Forscher des kalifornischen Startups OpenAI entwickelt und nun der Öffentlichkeit zugänglich gemacht haben, kann teils echt wirkende Fake News schreiben. Mit überschaubarer menschlicher Starthilfe: Es genügt, einen Satz oder Textschnipsel in die Maske einzugeben - und schon ist das Programm oftmals in der Lage, natürlich und plausibel klingende Texte in passabler englischer Sprache zu formulieren. Die Forscher von Open AI, die mit Millionensummen vom Tesla-CEO Elon Musk und Facebook-Investor Peter Thiel unterstützt werden, haben damit nach eigenen Angaben einen bedeutsamen Entwicklungssprung beim Machine Learning gemacht.

Da Werkzeuge wie dieses denkbar leicht missbraucht werden können, wollten die Entwickler den Bot zunächst unter Verschluss halten - jetzt aber haben sie ihn mit einer abgeschwächten Künstlichen Intelligenz veröffentlicht. Auf der Webseite talktotransformer.com ist das Angebot jederzeit für jedermann abrufbar. Aus nur wenigen Zeilen generiert es innerhalb weniger Sekunden kleine Geschichten. Außerdem kann es Dialoge fortführen, Liedtexte vervollständigen oder To-Do-Listen schreiben. Wie das funktioniert? Nach Beschreibung der Forscher nährt sich der Bot mit Informationen von mehr als acht Millionen Webseiten. Er konzentriert sich dabei überwiegend auf Seiten, die im sozialen Netzwerk Reddit gepostet wurden und dort wiederum auf Links, die von mindestens drei Menschen für wertvoll befunden wurden. Dass Kriminelle das Tool für ihre Zwecke nutzen können, steht für Wissenschaftler außer Frage, viele befürchten zudem, dass die Neuentwicklung ein schlechtes Licht auf die Forschung mit KI werfen könnte. Das US-Technikportal The Verge aber beschwichtigt: Bislang durchdringe das Programm weder die Sprache noch die Welt im Allgemeinen.";https://www.sueddeutsche.de/medien/fake-news-generator-mit-luegen-spielen-1.4447628;sz.de;Benjamin Emonts
15.05.2019;Was künstliche Intelligenz ist;"Was ist eigentlich künstliche Intelligenz (KI)? Die einen fürchten, dass wegen ihr bald kein Mensch mehr im Schach oder im Go-Spiel gewinnen kann, Autos autonom fahren, kurz: Dass intelligente Maschinen zu unschlagbaren Experten auf verschiedenen Gebieten heranreifen. Für andere ist sie die große Jobvernichtungsmaschine, die den Menschen überflüssig macht, ihm kognitiv überlegen sein wird und ihn so am Ende seiner Entscheidungsgewalt beraubt.

Mit ihrem Comic-Essay ""We need to talk, AI"" wollen die Daten-Wissenschaftlerin und promovierte Volkswirtin Julia Schneider, und die Berliner Grafikerin Lena Kadriye Ziyal einen wissenschaftlich fundierten, zugleich humorvollen und niedrigschwellligen Zugang zum Thema schaffen, jenseits von ""Hype und Angstmache"", wie sie es formulieren.

Auf 56 Seiten, in je sechs Bildtafeln in Schwarz-Weiß, versuchen die Autorinnen zu beantworten, was KI ist, und welche Chancen und Gefahren von ihr ausgehen. Sie erklären die Unterschiede zwischen den Spielarten Machine Learning und Deep Learning und diskutieren KI-relevante Konzepte wie Big Data, Datenkapitalismus und Diskriminierung.

Spannend im Comic sind die vielen, manchmal versteckten Querverweise. So erkennt der geschulte Blick auf dem ersten Bild eine Anspielung auf die mechanische Maus des Mathematikers und Begründers der Informationstheorie, Claude Shannon. In ihren Schaltkreisen hatte sie bereits ein ""Gedächtnis"" und konnte sich so in einem Labyrinth orientieren. ""Wir wollen, dass sich möglichst viele Menschen mit künstlicher Intelligenz beschäftigen und nicht nur diejenigen, die schon meinen, dass sie sich damit auskennen. Also der Typ männlicher Ingenieur"", sagt Julia Schneider. ""Gerade Comics eignen sich dafür sehr gut"", ergänzt Lena Kadriye Ziyal. Zwischen Bild und Text ergebe sich so ein Dialog. Man könne spannend und assoziativ erzählen, ohne einem Anspruch auf Vollständigkeit genügen zu müssen.

Schneider und Ziyal versuchen in ihrem im Eigenverlag herausgebrachten Comic-Essay die Fragen zu beantworten: In welcher Welt wollen wir leben? Wie muss Technik weiterentwickelt werden, damit sie uns hilft? Und wie sieht die ideale AI aus? Dafür versuchen die Autorinnen, Mythen zu entzaubern und in einfachen Bildern zu erklären, was die Voraussetzungen für künstliche Intelligenz und was überhaupt Algorithmen sind. Mit dem Bild einer russischen Matroschka zeigen sie zum Beispiel das Verhältnis von KI zu Machine Learning und Deep Learning auf. Sie geben Beispiele, wo diese Verfahren bereits eingesetzt werden, sei es in Routenplanern oder den Filmvorschlägen von Videostreaming-Diensten. Aber auch die Gefahren von künstlicher Intelligenz, die Angst vor einem Überwachungsstaat, wie er in China teilweise schon Realität ist, stellt der Comic eindrucksvoll und nachvollziehbar dar. Ob Gesichtserkennung oder die Analyse von Bewegungsdaten: Künstliche Intelligenz macht den Menschen in vielerlei Hinsicht durchschaubar. Den Autorinnen geht es aber nicht darum, einen Untergangsstimmung angesichts der derzeitigen Weiterentwicklung von KI zu beschwören. Die Chancen und Vorteile stellen sie ebenso dar. Komfort und Bequemlichkeit, die der technische Fortschritt bringen wird, ilustrieren sie charmant mit einem Faultier. Schneider schreibt: ""KI verspricht uns eine Welt voller personalisierter Angebote, die günstiger, schneller und ohne menschliche Fehler auskommen."" Zur Zeit gibt es den Comic nur auf Englisch, die Autorinnen wollen ihn auch auf Deutsch herausbringen. Damit jeder den Comic lesen und weiterbreiten kann, haben sie ihn unter einer Creative-Commons-Lizenz herausgebracht. So lässt sich ""We need to talk, AI"" gratis im Netz als PDF herunterladen. Die Printversion ist für 11,99 Euro per Epubli erhältlich.";https://www.sueddeutsche.de/digital/comic-was-kuenstliche-intelligenz-ist-1.4444200;sz.de;Mirjam Hauck
14.05.2019;"""Wir müssen reden, künstliche Intelligenz""";"Was ist eigentlich künstliche Intelligenz (KI)? Für die einen sorgt sie dafür, dass kein Mensch mehr im Schach oder auch im Go-Spiel gewinnen kann, Autos autonom fahren, kurz: intelligente Maschinen zu Experten auf verschiedenen Gebieten heranreifen. Für andere ist sie die große Jobvernichtungsmaschine der Zukunft, die den Menschen überflüssig macht, ihm kognitiv überlegen sein wird und ihn so letztlich seiner Entscheidungsgewalt beraubt.

Mit ihrem Comic-Essay ""We need to talk, AI"" wollen die Daten-Wissenschaftlerin Julia Schneider und die Grafikerin Lena Kadriye Ziyal einen wissenschaftlich fundierten, zugleich humorvollen und niedrigschwelligen Zugang zum Thema schaffen, jenseits von ""Hype und Angstmache"". Auf 56 Seiten, in jeweils sechs Schwarz-Weiß-Bildtafeln, versuchen die Autorinnen zu beantworten, was KI ist und was sie in Zukunft für unsere Gesellschaft leisten kann. Sie erklären die Unterschiede zwischen Machine Learning und Deep Learning und diskutieren KI-relevante Konzepte wie Big Data, Datenkapitalismus und Diskriminierung.

Spannend im Comic sind die vielen, manchmal auch etwas versteckten Querverweise. So erkennt der geschulte Blick hier auf dem ersten Bild eine Anspielung auf die mechanische Maus des Mathematikers und Begründers der Informationstheorie, Claude Shannon. Mittels Schaltkreisen hatte sie bereits ein ""Gedächtnis"" und konnte sich so in einem Labyrinth orientieren. Wer noch nie davon gehört hat, bekommt zumindest eine Ahnung davon, dass die Dinge kompliziert sind.

Schneider und Ziyal geht es aber nicht darum, eine Untergangsstimmung angesichts der derzeitigen dramatischen Weiterentwicklung von KI zu beschwören. Die Chancen und Vorteile stellen sie ebenso dar. So werden Komfort und Bequemlichkeit durch technischen Fortschritt sehr charmant mit einem Faultier illustriert. Schneider sagt: ""KI verspricht uns eine Welt voller personalisierter Angebote, die günstiger, schneller und ohne menschliche Fehler auskommen.""

Die Autorinnen haben den Comic unter einer Creative-Commons-Lizenz herausgebracht. So ist ""We need to talk, AI"" auf der gleichnamigen Website als PDF downloadbar. Die Printversion kann für 11,99 Euro per Epubli gekauft werden.";https://www.sueddeutsche.de/wirtschaft/kuenstliche-intelligenz-wir-muessen-reden-kuenstliche-intelligenz-1.4446282;sz.de;Mirjam Hauck
05.05.2019;"Eigene ""Fälschungen"" herstellen";"Einen außergewöhnlichen Workshop zum Thema ""Kunst & Künstliche Intelligenz: Neue Werke alter Meister"" hat die langjährige Penzberger Museumsleiterin Gisela Geiger organisiert. Er findet am Samstag, 18. Mai, im Buchheim Museum statt. Referenten sind Stephan Hellerbrand, Technologie-Berater für Projekte mit den Schwerpunkten Software und Machine Learning, und Aljoscha Leonhardt, Gehirn- und KI-Forscher am Max-Planck-Institut für Neurobiologie in München. Zum gleichen Thema gibt es in Bernried auch einen Vortrag am Sonntag, 12. Mai (Beginn 15.30 Uhr).

Selbst im Bereich der bildenden Kunst, die so fundamental menschlich erscheine, machten lernende Algorithmen dramatische Fortschritte, erklärt Gisela Geiger. Im Rahmen des interaktiven Workshops sollen Interessierte Einblicke in die Welt der ""malenden Maschinen"" erhalten. Zudem erläutern die beiden Referenten ihren Worten nach auf verständliche Weise die Grundkonzepte der modernen KI und der sogenannten ""tiefen neuralen Netzwerke"" und zeigen anhand vieler Beispiele auf, wo uns KI im Alltag bewusst und unbewusst begegnet.

Höhepunkt des Workshops ist ein Kunstexperiment: Die Referenten demonstrieren, wie man Netzwerken beibringen kann, im Stil alter oder neuer Meister beliebig neue Werke zu generieren. Teilnehmer haben sodann im Rahmen eines Wettbewerbs die Gelegenheit, eigene ""Fälschungen"" herzustellen. Zum Abschluss des Workshops soll über ethische Aspekte der KI-Revolution und künstlerische Fragestellungen - ""Was ist Stil?"" oder ""Was unterscheidet algorithmische und menschliche Fälschungen?"" - debattiert werden. Die Teilnehmerzahl ist auf 30 begrenzt; Laptop, Tablet oder Smartphone sind mitzubringen. Die Gebühr beträgt 60 Euro, für Schüler/Studenten 20 Euro. Anmeldung unter sabine.bergmann@buchheimmuseum.de oder Tel. 08158/ 99 70 50.";https://www.sueddeutsche.de/muenchen/wolfratshausen/workshop-zur-ki-eigene-faelschungen-herstellen-1.4432756;sz.de;
05.05.2019;Neutrale Algorithmen? Von wegen!;"Künstliche Intelligenz (KI) ist ein epochaler Technologiesprung, der die Menschheit vor Fragen stellt, die keine Disziplin alleine beantworten kann. John Brockman, Agent für Wissenschaftsliteratur und Gründer des Debattenforums Edge.org, hat das ""Possible Minds""-Projekt ins Leben gerufen, das Natur- und Geisteswissenschaften zusammenführt, um KI und deren wahrscheinliche Ausformungen und Folgen zu ergründen. Das Feuilleton der SZ druckt Texte aus dem Projekt sowie europäische Reaktionen als Serie.

Die Informatikerin Meike Zehlike forscht an der Humboldt-Universität Berlin zum Thema Fairness und Diskriminierung in Suchalgorithmen. Der Berliner Ökonom Gert G. Wagner ist Mitglied der Deutschen Akademie der Technikwissenschaften (Acatech) und Mitglied im Sachverständigenrat für Verbraucherfragen. Nach dem Aufkommen des ""maschinellen Lernens"" und der zuvor ungeahnten Möglichkeiten, mithilfe künstlicher Intelligenz (KI) Problemlösungen zu finden, herrschte unter Technologen zunächst große Euphorie. Endlich würden Entscheidungen rational und damit effizient getroffen werden können, ohne dass ein Tagesbefinden, die emotionale Prägung oder Wunschvorstellungen eines menschlichen Entscheiders eine Rolle spielen würden. Beispielsweise bei der Einschätzung der Zahlungsfähigkeit eines Menschen und bei der Auswahl von Angestellten oder sogar bei der Wahl romantischer Partner. ""Algorithmen entscheiden ausschließlich auf Basis von Fakten, nicht auf Basis von Hautfarbe"" und ""Mathematik und Algorithmen sind neutral und deswegen sind es auch algorithmische Entscheidungen"" waren die gängigen Leitsätze dieser Zeit. Diese Behauptungen sind schlicht falsch, sie werden aber nur langsam überwunden. So schrieb eine Auskunftei im Rahmen einer Markterhebung, die der Sachverständigenrat für Verbraucherfragen im vergangenen Jahr anstellte: ""Mathematisch-statistische Verfahren diskriminieren nicht.""

Im Folgenden zeigen wir, warum diese Neutralitätsbehauptung irreführend ist. Wir werden aber auch zeigen, dass Algorithmen gleichwohl nützlich sind, da sie helfen, Diskriminierungen sauber zu definieren, sie zu erkennen und zu verhindern. Fairness algorithmisch sicherzustellen, erfordert aber, dass eine künstliche Intelligenz von menschlicher Intelligenz gezielt und gesteuert eingesetzt wird.

Zuerst ist festzuhalten: Algorithmen sind weder per se neutral, noch ist es unmöglich, dass sogenannte selbstlernende Systeme diskriminieren, ganz im Gegenteil. Gerade bei selbstlernenden Systemen ist die Gefahr einer systematischen Diskriminierung durch die KI groß, denn diese wird auf Daten ""trainiert"", die als Abbild der heutigen Gesellschaft sämtliche historischen und aktuellen Diskriminierungen enthalten. Jüngst erregte der Konzern Amazon mit einem System Aufsehen, das automatisch ungeeignete Bewerbungen auf Programmierjobs aussortieren sollte. Da jedoch Frauen in der Informatik stark unterrepräsentiert sind und damit ein statistisches Handicap aufweisen, lernte das System aus der Situation in der Vergangenheit, dass Bewerbungen von Frauen generell schlechter zu bewerten sind. Dies ist in zweierlei Hinsicht problematisch: Zunächst sind nun alle Bewerbungen von Frauen von einem sexistischen Algorithmus betroffen und nicht nur diejenigen, die vorher das Pech hatten, an einen sexistischen Personalsachbearbeiter zu geraten.
Für einen Test seiner Resultate ist die interne Komplexität eines Algorithmus völlig irrelevant

Zusätzlich besteht nun die Schwierigkeit, dass ein Opfer solcher algorithmischer Diskriminierung es sehr schwer haben wird, seinen Opferstatus überhaupt zu erkennen und dann dagegen vorzugehen. Es diskutiert sich nämlich auch weiterhin sehr schlecht mit Algorithmen.

Zugegeben - früher kam man diskriminierenden Entscheidern, die über ihre Motive schwiegen, auch nur schwer auf die Schliche. Wenn zum Beispiel ein Privatmann eine Wohnung vermietet, wird man ihm niemals Diskriminierung nachweisen können, wenn er nicht ausspricht, dass er etwa Menschen mit dunkler Hautfarbe oder Familien mit kleinen Kindern nicht mag. Würde dieser Vermieter jedoch einen Algorithmus einsetzen, etwa einen Fragebogen, der ihm hilft, Bewerber vorzusortieren - so gehen Wohnungsbaugesellschaften vor -, dann hat dies einen entscheidenden Vorteil: Man kann diesen Algorithmus testen. Wieder und wieder, stundenlang und mit verschiedenen Personenmerkmalen, verschiedenen Ausgangssituationen, etwa, wenn der Algorithmus sich zwar nicht für die Hautfarbe interessiert, aber für den Geburtsort. Auf diese Weise kann man Indizien für versteckte Diskriminierung finden, indem vorab Kriterien festgelegt werden, die ein faires Ergebnis beschreiben. Weichen die Testergebnisse signifikant von den Kriterien ab, ist algorithmische Diskriminierung wahrscheinlich.

Auch bei selbstlernenden Algorithmen lassen sich Diskriminierung und Fairness durch Tests nachweisen. Es wird zwar gerne behauptet, dass solche Modelle aufgrund ihrer Komplexität undurchschaubar wären, dies ist aber nicht einmal die halbe Wahrheit. Selbst bei neuronalen Netzen - das sind Computeralgorithmen, die als völlig undurchschaubar gelten - ist es möglich herauszufiltern, welche ""Nervenzellen"" (Knoten) darin maßgeblich an einer automatischen Entscheidung beteiligt waren. Zudem, das ist technisch immer möglich, kann man das Verhalten des Algorithmus prüfen, indem man den Entscheidungsprozess, der im Computer abläuft, gar nicht weiter betrachtet. Stattdessen prüft man besser dessen Ergebnisse für eine Vielzahl von Beispieldaten und Personen hinsichtlich der zuvor definierten Kriterien von Fairness und Diskriminierung. Das ist in etwa so, als würde die Stiftung Warentest eine Kaffeemühle testen. Die Stiftung muss gar nicht deren Bauplan kennen, um festzustellen, ob die Mühle gut mahlt und wie lange sie durchhält. Das Testen macht die interne Komplexität eines Algorithmus für die Qualitätskontrolle völlig irrelevant.

Mit einem geeigneten ""Set"" von Daten lässt sich also feststellen, ob ein Algorithmus diskriminiert. Hätte Amazon in einer Testphase seines Personal-Algorithmus darauf geachtet, dass von ihm deutlich weniger Frauen eingeladen werden als sich bewerben, hätte die Diskriminierung nicht nur festgestellt, sondern auch quantifiziert werden können. Insofern können geeignete Algorithmen und Tests helfen, Diskriminierung zu erkennen. Nicht nur das, es ist sogar möglich, selbstlernende Algorithmen und deren Zielfunktionen so zu entwerfen, dass sie bestimmte Fairness-Kriterien sicherstellen und unerwünschte Diskriminierung verhindern. Diese sind allerdings keineswegs eindeutig zu benennen. Denn was verstehen wir eigentlich unter ""Fairness""?

Um die Gleichbehandlung sicherzustellen, muss man sie erst einmal sauber definieren. Um solche Definitionen ringt die Philosophie schon seit Jahrtausenden, davon kann die Forschung zum Thema Fairness in Algorithmen heute profitieren.

So hat sich das Forschungsfeld ""Fairness, Accountability and Transparency in Machine Learning"" in den vergangenen Jahren auch äußerst dynamisch entwickelt und bereits einiges an Methodik und Tools hervorgebracht. Beispielsweise wird an der TU Berlin in Zusammenarbeit mit der UPF Barcelona eine Software für ""Elasticsearch"" entwickelt, das faire Suchergebnisse garantieren wird. Elasticsearch ist ein Tool, das von nahezu allen Websites mit einem Suchfeld benutzt wird, unter anderen von Facebook, Netflix und Amazon. Der Einsatz der neuen Software wird im gesamten System für fairere Suchergebnisse sorgen.
Unklar ist, ob sich mathematische Definitionen mit unseren gesellschaftlichen Werten decken

Doch die Festlegung von Fairness-Kriterien ist weder trivial noch eindeutig. Schon die Definition der Begriffe ""algorithmische Fairness"" und Nicht-Diskriminierung stellt eine Herausforderung dar. Aus der Forschung sind bisher etwa 25 Definitionen bekannt, die keineswegs alle miteinander kompatibel sind. Ein Beispiel: Im Falle einer Bonitätsprüfung gibt es verschiedene Interpretationen, was ein faires Ergebnis hinsichtlich der Verteilung zwischen Männern und Frauen ist. Manche Definitionen sagen, fair sei es, wenn im Endergebnis Frauen und Männer in gleichem Maße Kredite bekommen. Andere sagen, dass tatsächlich kreditwürdige Frauen und Männer in gleichem Maße Kredite bekommen sollen: Ein Fehler der ""ersten Art"" soll für beide Gruppen gleich sein. Wieder andere sagen, dass tatsächlich kreditwürdigen Frauen und Männern in gleichem Maße Kredite verweigert werden, weil sie fälschlich als nicht-kreditwürdig eingestuft werden, ein Fehler der ""zweiten Art"" soll hier gleich sein.

Völlig unklar ist, inwiefern solche mathematischen Definitionen sich mit unseren gesellschaftlichen Normen und Werten decken, insbesondere weil verschiedene Situationen verschiedene normative Voraussetzungen erfüllen. Sollten Sie von einem Algorithmus, der Ihre Amazon-Produktauswahl optimiert, fälschlicherweise als reich klassifiziert werden, hat das für Betroffene nur leichte negative Konsequenzen. Wenn man aber von einem Bild-Erkennungs-Algorithmus nach einem MRT-Scan zur Krebserkennung fälschlicherweise als gesund klassifiziert wird, ist das lebensbedrohlich. Es ist klar, dass wir für algorithmische Entscheidungen in der Medizin andere Maßstäbe ansetzen müssen als im Bereich des Online-Handels. Vielleicht sollten wir bei bestimmten Entscheidungen sogar auf Algorithmen verzichten.

Bisher wissen wir über algorithmische Fairness und die Auswirkungen algorithmischer Entscheidungsfindung auf unser soziales Gefüge sehr wenig, und die Forschung zur künstlichen Intelligenz kennt sich zu wenig mit den Theorien zu Fairness aus ethischer und philosophischer Sicht aus. Deswegen fällt es schwer, die verschiedenen Ansätze so einzuordnen, dass der Laie wissen kann, in welcher Situation er welchen Algorithmus anwenden und welchem er vertrauen soll.

Es ist notwendig, dass KI-Forschung genauer hinsieht, was Fairness eigentlich in welcher Situation bedeutet. Ansonsten besteht die Gefahr, dass zum Beispiel Scoring-Unternehmen, die uns nach Kreditwürdigkeit, Fahrtüchtigkeit und Gesundheit einsortieren, sich einfach irgendeine Fairness-Variante nach Belieben aussuchen und sich danach hinter ihrem Geschäftsgeheimnis und der Behauptung verstecken, dass Mathematik unbestechlich sei und nicht diskriminieren könne. Wir sollten aber wissen, was Algorithmen mit uns machen: anhand von Beispielfällen und einer Definition und systematischen Analyse von Fairness. Technisch ist das machbar - der Gesetzgeber muss nur wollen und sicherstellen, dass dies auch geschieht.";https://www.sueddeutsche.de/kultur/kuenstliche-intelligenz-algorithmen-diskriminierung-1.4432946;sz.de;Meike Zehlike und Gert G. Wagner
14.03.2019;So gelingt der Quereinstieg in die IT;"Die Digitalbranche sucht Quereinsteiger. Doch die Aversionen sind groß. Muss man als Computerexperte nicht sozialisiert sein? Fragen und Antworten im Überblick:

Für wen eignet sich der Quereinstieg oder eine Weiterbildung in einem IT-Beruf?

Für jeden Interessierten, der noch ein paar Berufsjahre vor sich hat. Um programmieren zu lernen, braucht man vor allem Motivation, sagt der selbständige Entwickler Karim Geiger, der auch Lehrvideos dreht: ""Am Anfang muss man sich durchbeißen, da braucht man Frustrationstoleranz.""

Welche Qualifikationen sind nötig, um einen Job zu finden?

Informatiker, Ingenieure, Elektrotechniker, Mathematiker und Physiker sind begehrt. Notwendig seien solche Abschlüsse für den Einstieg in die Branche aber nicht, sagt Kaya Taner, Gründer der Entwicklerjobbörse Honeypot: ""Bei uns finden auch Menschen einen IT-Job, die sich alles selbst beigebracht haben oder bisher nur Erfahrungen in Freizeit- oder Studentenprojekten gesammelt haben."" Auf Entwicklerplattformen wie GitHub können Nutzer an Projekten Anderer mitarbeiten.

Wie viel technisches Grundverständnis ist zum Programmieren nötig?

""Der Computer ist ein Volltrottel, der kann nicht interpretieren, der macht immer nur genau das, was Sie befehlen"", sagt Karim Geiger. Dieses Prinzip müssten Entwickler verstehen: ""Menschen schließen aus dem Kontext, der Computer braucht exakte Anweisungen, das klingt banal, aber ist gar nicht so leicht zu verinnerlichen."" Brauchen Programmierer Mathe?

Angst vor Mathe sei kein Grund, vor dem Programmieren zurückzuschrecken, sagt Entwickler Geiger: ""Für App- oder Web-Entwicklung werden nur grundlegende Mathematik-Kenntnisse benötigt"", in anderen Bereichen, etwa Machine Learning, seien tiefere Mathekenntnisse aber erforderlich.

Wo kann man Programmieren lernen?

Kurse gibt es zuhauf, unter den Anbietern sind namhafte Institute und private Hochschulen, die den Teilnehmern auch Betreuer an die Seite stellen. Weil es keine Qualitätschecks gibt, rät Digital-Personalexpertin Martina Weiner, sich mit ehemaligen Teilnehmern auszutauschen. Karim Geiger empfiehlt die kostenlosen Angebote von Codecademy und Sololearn.

Mit welcher Sprache sollte ich anfangen?

Das ist eigentlich nicht so wichtig, beherrscht man die erste Sprache, lernt man die zweite schnell. Weit verbreitet und gesucht sind Javascript und Python.

Wie viel Zeit muss man investieren?

""Bis man sein erstes Programm schreiben kann, kann es ein halbes oder zwei Jahre dauern"", sagt Geiger - je nach Lerngeschwindigkeit und Zeit zum nebenberuflichen Üben. Anbieter sogenannter Bootcamps werben damit, Teilnehmern in zwölf Wochen Programmieren beizubringen und in den Beruf zu verhelfen. In den USA hat sich das etabliert. Nach Kaya Taners Erfahrungen reicht den meisten Firmen in Deutschland ein solcher Programmabschluss allein aber noch nicht aus. Gefragt seien vorzeigbare Projekte und damit Erfahrung. ""Die Absolventen starten häufig mit einem Praktikum in einen Programmierjob, teils aber auch schon direkt in eine höhere Anstellung"" - abhängig auch davon, was der Einzelne daraus mitnimmt.

Was verdienen Entwickler?

Auf Plattformen wie Honeypot lässt sich die Gehaltsentwicklung beobachten: Entwickler mit maximal zwei Jahren Erfahrung können in Deutschland demnach im Schnitt mit einem Jahresgehalt von 44 700 Euro rechnen.";https://www.sueddeutsche.de/karriere/programmieren-lernen-bootcamps-kurse-1.4367721;sz.de;Larissa Holzki
18.02.2019;Wie Google und Amazon die Wikipedia gefährden ;"Die Vordenker des World Wide Web hatten einst die Vision, eine digitale Bibliothek von Alexandria zu bauen, ein Zivilisationsarchiv, welches das Wissen der Welt demokratisiert und die alten Autoritäten in die Wüste schickt. ""In unserer Welt darf alles, was der menschliche Geist erschafft, kostenfrei unendlich reproduziert und distribuiert werden"", postulierte der amerikanische Internetpionier John Perry Barlow 1996 in seiner ""Unabhängigkeitserklärung des Cyberspace"".

Wenn etwas von diesem gemeinschaftsstiftenden Geist übrig geblieben ist, dann ist es die Online-Enzyklopädie Wikipedia: Ein Buch, an dem jeder mitschreiben kann. 5,8 Millionen Artikel zählt allein die englischsprachige Ausgabe der Wikipedia, hinzu kommen 40 Millionen Artikel in 301 Sprachen - von Alemannisch bis Zulu. Wikipedia.org liegt auf Platz fünf der am meisten aufgerufenen Websites - vor Amazon und Twitter. Der Journalist Paul Mason nennt in seinem Buch ""Postkapitalismus"" Wikipedia als Paradebeispiel einer postkapitalistischen Ordnung, in der kostenloses Wissen frei zugänglich sei und kommerzielle Lexika überflüssig mache. Wikipedia sei eine der ""wertvollsten Lernressourcen"", die je erfunden wurden, schreibt Mason. ""Und bisher hat sie allen Versuchen widerstanden, sie zu zensieren, zu unterwandern, sie für Desinformation zu nutzen oder zu sabotieren, denn Dutzende Millionen Augen sind stärker als jeder Staat, jeder Stalker jede Interessengruppe und jeder Saboteur.""

In der Tat hat sich Wikipedia, trotz manch erbitterter Scharmützel seiner Editoren (""Edit Wars""), als erstaunlich robust gegen Fake News und politische Manipulationsversuche erwiesen. Wikipedia-Gründer Jimmy Wales hat mit Wikitribune sogar eine News-Plattform geschaffen, die sich zum Ziel gesetzt hat, das ""kaputte"" Nachrichtengeschäft zu ""reparieren"". Doch ganz ohne Finanzierungsquellen kann auch ein idealistisches Projekt, das von über 200 000 unentgeltlich arbeitenden Editoren getragen wird, nicht überleben.
Idee des Gemeinguts

Die Wikimedia Foundation, die Trägerin der Wikipedia, ist eine gemeinnützige Organisation, die sich hauptsächlich aus Spenden finanziert. Zu den Spendern gehören unter anderen Adobe, Apple, Cisco, Google, Hewlett Packard, Oracle, Netflix sowie Salesforce - also fast die gesamte Tech-Branche. Google als einer der größten Sponsoren hat erst vor wenigen Tagen 3,1 Millionen Dollar an Wikipedia gespendet und damit die Höhe seiner Zuwendungen in der letzten Dekade auf 7,5 Millionen Dollar aufgestockt. Darüber hinaus erhalten Wikipedia-Editoren kostenlosen Zugang zu Machine-Learning-Werkzeugen. Google-Mitbegründer Sergey Brin, mit seiner Brin Wojcicki Foundation einer der größten Spender, bezeichnete Wikipedia einmal als ""einen der größten Triumphe des Internets"".

Die Spenden sind nicht ganz uneigennützig: Der Konzern nutzt Wikipedia-Artikel, um seine maschinell lernenden Algorithmen zu trainieren. So griff die Google-Schwesterfirma Jigsaw bei der Entwicklung eines Moderationstools gegen Hasskommentare (""Perspective"") unter anderem auf 115 000 Diskussionsbeiträge von Wikipedia zurück. Die Chefin der Google-Tochter Youtube, Susan Wojcicki, kündigte im vergangenen Jahr an, verschwörungstheoretische Videos zu Themen wie Mondlandung oder Chemtrails mit Textauszügen aus Wikipedia zu kontern.

Von der Integrität der Informationen hängt entscheidend die Integrität des Werbeumfelds ab. Kein seriöser Anzeigenkunde möchte neben Hetzern oder Verschwörungstheoretikern werben. Das heißt: Google profitiert von Wikipedia. Wenn man in Google nach dem Pariser Musée d'Orsay sucht, erscheint im rechten Teil der Ergebnisseite ein Knowledge Panel, eine Info-box mit Hintergrundinformationen, deren Textauszüge zum Teil aus Wikipedia extrahiert und zuweilen sogar identisch sind.

Auch Amazons Sprachassistentin Alexa bezieht Informationen unter anderem aus Wikipedia - der Online-Händler spendete im vergangenen Jahr eine Million Dollar an die Wikimedia Endowment, einen Fonds, der die Online-Enzyklopädie unterstützt. Die Frage ist, ob die Datenverarbeitung mit den angesichts der Milliardenumsätze der Tech-Konzerne eher mickrigen Spenden abgegolten ist - und wie die kommerzielle Nutzung mit dem Commons-Gedanken der Wikipedia vereinbar ist. Die Idee des Gemeinguts beruht ja gerade auf der Nichtexklusivität, auf der nicht kommerziellen Nutzung von Inhalten. Werden die freiwilligen Autoren und Helfer ausgebeutet, wenn sich gewinnorientierte Konzerne an dem Wissensschatz wie an einem Steinbruch bedienen? Was ist der Wert geistiger Arbeit, wenn Texte zu Trainingsdaten für geistlose künstliche Intelligenzen verkommen?

Zu einer SZ-Anfrage wollte sich die Wikimedia Deutschland nicht äußern. In einem Hintergrundgespräch auf der Frankfurter Buchmesse teilte ein Sprecher mit, dass die kommerzielle Datennutzung dem Ideal, das Wissen der Welt kostenlos zu teilen, nicht zuwiderlaufe. An der Wikipedia könne sich jeder bedienen, egal ob Großkonzern oder Kleinbürger. Das Wissen - wie im Übrigen auch das Bildmaterial - stünde frei zur Verfügung. Und letztlich sei es besser, wenn der Netzwerklautsprecher eben keine Fake News verkündet, sondern korrekte Infos. Das mag ein hehres Motiv sein. Doch gerät dieser Idealismus unter Spannung, wenn der Erzeuger der Information auf Spenden angewiesen ist, der Verwerter Milliarden mit Werbung verdient.
2,3 Milliarden Dollar Umsatz im Jahr wären möglich

Der Journalist Michael Johnston hat mal errechnet, dass Wikipedia 2,3 Milliarden Dollar Umsatz im Jahr erzielen könnte, wenn die Plattform kommerziell betrieben würde. 160 Millionen Dollar ließen sich pro Monat durch digitale Anzeigen einspielen. Eine werbefinanzierte Wikipedia wäre ein Verrat an den eigenen Idealen und der Community wohl kaum zu vermitteln, würde das Projekt aber auf eine solide Finanzierungsbasis stellen. In der jetzigen Konstellation muss sich Wikipedia den Vorwurf gefallen lassen, Google-finanziert zu sein. Gewiss, Wikipedia ist weit davon entfernt, ein Instrument des Suchmaschinenriesen zu sein, niemand reflektiert seine Rolle im Netz transparenter. Doch durch die Spenden macht sich Wikipedia angreifbar - vor allem für Krakeeler, die in der Online-Enzyklopädie schon immer ein Manipulationsmedium im Dienste der Mächtigen sahen.

Der Commons-Gedanke steckt im Informationskapitalismus in einer Zwickmühle: Einerseits muss Wikipedia das lexikalische Wissen kostenlos zur Verfügung stellen. Andererseits kann es nicht verhindern, dass Tech-Konzerne Textlandschaften durchpflügen und das Rohmaterial als Datenfutter fürs Maschinenlernen monetarisieren. Wenn man nach der Silicon Valley Community Foundation googelt, welche die Wikipedia mitfinanziert, erscheint im Knowledge Panel der aus der englischsprachigen Wikipedia extrahierte Satz: ""Aus dem Englischen übersetzt - Die Silicon Valley Community Foundation ist eine von Gebirgsansichten unterstützte, von Gebern beratene Gemeinschaftsstiftung, die dem Silicon Valley und der umliegenden Gemeinde dient."" Mit ""Gebirgsansichten"" sind nicht die Ansichten einer Bergschützervereinigung gemeint, sondern Mountain View, der Sitz der Stiftung wie auch der von Google. Der Übersetzungsdienst hat hier offensichtlich wörtlich übersetzt und nicht erkannt, dass es sich um eine Ortsmarke handelt. Es scheint, als bräuchte Google die Hilfe von Wikipedia dringender als andersrum.";https://www.sueddeutsche.de/digital/wikipedia-spenden-google-amazon-1.4333588;sz.de;Adrian Lobe
13.01.2019;Smarter wohnen;"Ein ganz normaler Morgen in nicht allzu ferner Zukunft: Der Wecker klingelt und die Menschen quälen sich aus den Betten. Dank einer Vielzahl von Sensoren, Kameras und Mikrofonen, die in sämtlichen Zimmern der Wohnung installiert sind, weiß das Smarthome-System genauestens, was vor sich geht. Der digitale Hausgeist gratuliert folglich der Dame des Hauses zur gelungenen Klamottenauswahl für den anstehenden Businesstermin, außerdem wacht er darüber, dass sich auch alle gründlich die Zähne putzen. Gleichzeitig ermahnt er die Kinder: Sie sollen nicht so viele Schimpfworte benutzen. Dann verlässt die Familie die Wohnung, ein jeder in unterschiedliche Richtungen, aber die Smartphones wissen eh schon, wohin ihre Benutzer unterwegs sind - und spielen selbstverständlich entsprechende Werbung aus.

Bei all den gerade vorgestellten Dystopien handelt es sich nicht etwa um Plot-Vehikel eines eher einfallslosen Science-Fiction-Romans, sondern um tatsächliche Patente, die sich große Tech-Konzerne wie Google oder Facebook im vergangenen Jahr schützen ließen. Nun ist es ja zumindest in den USA so, dass eine Technologie noch lange nicht funktionieren muss, um zum Patent angemeldet zu werden. Es geht vielmehr darum, nachzuweisen, dass man als erster die Idee dafür hatte. Datenschützer und andere Menschen mit klarem Verstand konnten sich bislang noch von dem Gedanken trösten lassen, dass die Technik nicht mit den Überwachungs- und Vermarktungsfantasien der Tech-Firmen Schritt hielt.

Zumindest bis jetzt. Wer in der vergangenen Woche über die Flure der Hightechmesse CES in Las Vegas schritt, konnte an jeder Ecke den Eindruck gewinnen, dass es mit der Schonfrist bald vorbei sein könnte. Denn in den Produktportfolios der Hersteller fanden sich allerhand alltägliche Dinge, die auf einmal intelligent genannt werden. Vom Koffer über die Zahnbürste bis hin zu Türschlössern - alles soll auf einmal mit künstlicher Intelligenz noch besser und effizienter werden. Selbst die Toilettenschüssel wird vom KI-Wettrüsten nicht verschont.

Nun zeugt das mehr oder weniger ziellose Ranklatschen von KI nicht nur von einer gewissen Einfallslosigkeit der Hersteller, sondern vor allem von der zunehmenden Verschmelzung von digitaler und analoger Realität. Und zwar nicht nur in dem Ausmaß wie es hierzulande gerade erst rechte Trolle und Grünen-Politiker realisieren. Nämlich, dass das eigene Handeln in der einen durchaus Auswirkungen in der anderen Sphäre haben kann. Sondern eben auch insofern, dass die Datenspuren, die man hier wie dort unablässig produziert, dazu benutzt werden, noch zielgenauere Werbung zu schalten oder hyperpersonalisierte Angebote für Versicherungen und andere Dienstleistungen abzugeben.

Unsere Interaktionen mit dem Internet verschieben sich jenseits des Bildschirms. Jeder noch so banale Gegenstand wird zum Knotenpunkt des Netzes. Er sammelt Daten über den Alltag seiner Nutzer, die wiederum potenten Machine-Learning-Algorithmen dazu dienen, noch genauere Vorhersagen über das Leben der Menschen zu treffen. Von ""dezentraler Überwachung"" sprechen Datenschutzorganisationen wie das Center for Digital Democracy. Wir befinden uns, wie der Philosoph Zygmont Baumann sagte, im Zeitalter des Postpanoptikums. Statt sich in einer Disziplinaranstalt mit zentralem Überwachungsturm zu befinden, holen wir uns die Kontrollinstrumente in die je eigenen vier Wände.";https://www.sueddeutsche.de/kultur/netzkolumne-smarter-wohnen-1.4285670;sz.de;Michael Moorstedt
18.12.2018;Wann ist ein Bot ein Bot?;"Die Unionsfraktion im Bundestag denkt über eine Kennzeichnungspflicht für automatisierte Nachrichten in sozialen Netzwerken nach, die von sogenannten Social Bots stammen. ""Man sollte das ernsthaft prüfen"", sagte Fraktionschef Ralph Brinkhaus auf eine entsprechende Frage in der Frankfurter Allgemeinen Sonntagszeitung. Er mache sich Sorgen um die Demokratie, es sei höchste Zeit aufzuwachen. Brinkhaus' Vorschlag wird auch von Vertretern der SPD goutiert, die Grünen hatten die gleiche Idee schon vor der Bundestagswahl im vergangenen Jahr. Dabei ist noch nicht einmal klar, was einen Bot zum Bot macht.
Wie lässt sich ein Bot eindeutig identifizieren?

Social-Bots sind seit der US-Wahl 2016 ein großes Thema, wenn nicht sogar ein Hype. Verschiedene Studien und Zeitungsartikel stellten damals die These auf, dass die automatisierten Twitter-Helfer mitverantwortlich waren für die für viele unerwartete Wahl von Donald Trump zum US-Präsidenten. Seitdem wird auch in Deutschland immer wieder vor möglicher Beeinflussung von Wahlen durch Bots gewarnt. Einen Nachweis hierfür gibt es bislang allerdings nicht. Das liegt auch daran, dass keine Einigkeit darüber herrscht, wann ein Twitter-Account eigentlich als Bot zu verstehen ist. Manche Wissenschaftler stufen einen Account als Bot ein, wenn er 50 oder mehr Tweets am Tag absetzt. Um sehr große Mengen an Tweets wissenschaftlich zu untersuchen, ist das möglicherweise ein guter erster Schritt. Doch um festzulegen, ob ein einzelner Account ein Bot ist, ist dieses Kriterium offensichtlich Unsinn: Jeder kann sich noch heute einen Twitter-Account anlegen und 150 Tweets verfassen. Dadurch wird man aber nicht zur Maschine.

Ein anderer Weg, Social Bots zu klassifizieren, ist der Einsatz von Machine-Learning. Dabei klopft eine künstliche Intelligenz automatisch Tweets auf bestimmte Merkmale ab, die bereits bei sicher als Bots identifizierten Accounts festgestellt wurden. Die Anzahl der Tweets pro Tag kann dabei ebenso eine Rolle spielen wie das Alter eines Accounts, die verwendete Sprache, die Anzahl der verwendeten Hashtags oder die Uhrzeit, zu der für gewöhnlich gepostet wird.
Unglückliche Debatte über eingebildete Probleme

Die dritte mögliche Variante ist die händische Suche nach den oben genannten Auffälligkeiten, gepaart mit journalistischer Recherche - zum Beispiel durch Anfragen bei den Account-Betreibern. Der Schweizer Social-Media-Experte Luca Hammer etwa empfiehlt eine Kombination aus den Varianten zwei und drei um Bots aufzuspüren. Doch selbst dann könne man sich nie sicher sein, ob man es mit einem Bot zu tun hat. Aussagen wie die der Berliner Firma ""Botswatch"", die herausgefunden haben will, dass rund um die Verabschiedung des umstrittenen UN-Migrationspakts im Bundestag mehr als 25 Prozent der Tweets von Bots stammten, sind deshalb mit großer Vorsicht zu genießen. Oder wie es der Nürnberger Informatik-Professor Florian Gallwitz im Gespräch mit der NZZ zusammenfasste: Weil wissenschaftlich nicht geklärt sei, was ein Bot überhaupt ist, sei ""jede quantitative Aussage über Bots unseriös"".

Auch im Fall der Berliner Firma Botswatch stellt sich die Frage, welche Methodik sie anwendet und wie die Firma Bots definiert. Tabea Wilke, Geschäftsführerin von Botswatch, will darauf aber nicht eingehen. Wie Botswatch zu seinen Ergebnissen komme, sei ein Geschäftsgeheimnis, sagte sie der NZZ. Das mag für ein Unternehmen, das exklusive Expertisen verkaufen will, eine nachvollziehbare Einstellung sein, entkräftet die Kritik von vielen Fachleuten aber nicht.

Die aktuelle Debatte über Social Bots ist zumindest unglücklich. Zwar bezweifelt kaum jemand, dass es Versuche der politischen Beeinflussung über soziale Netzwerke gibt. Erst gestern wurde ein Bericht öffentlich, der den US-Senat über Details der russischen Desinformationskampagne zu den Wahlen 2016 informierte. Die meisten Fachleute sind allerdings der Auffassung, dass diese Einflussnahme eher selten von Bots ausging, sondern vielmehr von echten Menschen an Computern versucht wurde, zum Beispiel an Computern in der Sankt Petersburger Trollfabrik IRA (Internet Research Agency).
Achtung: SZ-Bot!

Wenn Politiker jetzt entgegen der Ratschläge vieler Experten eine Kennzeichnungspflicht für Bots einführen, dann könnte das auch Folgen für diese Zeitung haben: Denn auch der SZ-Twitter-Account hat Bot-Tendenzen: Immer wenn ein Text auf SZ.de veröffentlicht wird, geht eine Twitter-Nachricht mit kurzem Beschreibungstext online. Dieser Kurztext kann vom Autor des Textes oder einem SZ-Redakteur zwar geändert werden, wenn das aus irgendwelchen Gründen aber nicht passiert, dann besteht der Tweet aus der Dachzeile und der Überschrift des Artikels - ganz automatisch.

Echte Übeltäter aus Trollfabriken fremder Mächte würden sich von einer strafbewehrten Kennzeichnungspflicht sicher nicht abschrecken lassen, unter teilautomatisierten Medien-Accounts dagegen wäre künftig zu lesen: ""Achtung: Social Bot.""";https://www.sueddeutsche.de/digital/social-bots-brinkhaus-twitter-kennzeichnungspflicht-1.4258344;sz.de;Max Muth
20.11.2018;Spezialist für E-Mobilität;"Viele Wissenschaftler sind überzeugt, dass Elektroautos die Zukunft gehört. Doch momentan herrscht auch noch viel Unklarheit, beispielsweise darüber, wie die Reichweite der Fahrzeuge noch ausgebaut werden kann. Oft wird auch befürchtet, dass die öffentlichen Ladestationen nicht ausreichen, um ein reibungsloses Aufladen der Autos gewährleisten zu können. Umso wichtiger also, dass es Menschen gibt, die diese Thematik erforschen. Einer von ihnen ist Maximilian Zuleger. Der Unterhachinger wurde jetzt für seine Masterarbeit mit dem Kulturpreis Bayern ausgezeichnet, der vom bayerischen Wissenschaftsministerium sowie der Firma Bayernwerk unter anderem an die besten Absolventen der bayerischen Hochschulen verliehen wird. Die Auszeichnung mache ihn ""sehr glücklich"", sagt er.

In seiner Arbeit hat Zuleger untersucht, wie Elektrofahrzeuge möglichst effizient auf die vorhandenen Ladestellen verteilt werden können, ohne Engpässe zu verursachen. Für seine Berechnungen programmierte der Absolvent selbst Algorithmen und berücksichtigte dabei verschiedene Parameter, wie zum Beispiel die Akku-Kapazität der Elektrofahrzeuge, die Leistung der Ladestationen sowie deren Entfernung vom aktuellen Standort. Mithilfe von Simulationen konnte Zuleger schließlich feststellen, wie viele Ladestationen für jede beliebige Anzahl an Elektroautos benötigt werden.

Zuleger studierte Informatik an der Technischen Hochschule Ingolstadt. Für Computer habe er sich schon sehr früh interessiert, erzählt er, mit zwölf Jahren habe er mit dem Programmieren angefangen. ""Es war von Anfang an klar, dass ich später in diese Richtung gehen werde"", sagt der heute 25-Jährige. Bereits im Laufe seines Studiums rief er seine eigene Firma ins Leben. Damals verdiente er sich etwas Geld, indem er beispielsweise Webseiten baute. Auch heute noch nimmt er Aufträge an. ""Ich bin einfach durch und durch Software-Entwickler"", sagt er.

Momentan ist Zuleger bei Audi als Software-Entwickler in der IT-Sicherheit tätig. ""Zuerst habe ich dort in der E-Mobilität gearbeitet. Die IT-Sicherheit ist jetzt die neue Richtung der Firma"", erklärt er. Zu seinen größten Interessen zählt zur Zeit das Thema ""Machine Learning"". ""Dazu gehört alles, was zum Beispiel mit künstlicher Intelligenz und autonomem Fahren zu tun hat"", erklärt Maximilian Zuleger. Er ist überzeugt, dass ""künstliche Intelligenz in der Zukunft eine der wichtigsten Technologien sein wird"".

Neben seinem Beruf beschäftigt sich der 25-Jährige unter anderem mit Musik. ""Ich spiele gerne Gitarre und war während meiner Studienzeit auch Leiter einiger Bands an der Technischen Hochschule"", erzählt er.

Auch Sport gehört zu seinen Interessen. Seit dem Jahr 2013 ist er Volleyball-Trainer an der Universität Eichstätt. ""Es ist toll, zu sehen, wie die Studenten immer besser werden"", sagt Zuleger. Diese Tätigkeit ist nur ein Teil seines sozialen Engagements. Der Unterhachinger ist zudem auch Ehrenmitglied in der Studentenorganisation ""Student's Life"" und betreibt einen Blog über den Wandel der Einstellungen gegenüber Elektroautos.

Die Entwicklung in diesem Bereich beurteilt Zuleger sehr optimistisch: ""Bis zum Jahr 2025 wird es sehr viele Elektrofahrzeuge geben, momentan sind alle Autohersteller auf dem Sprung, was das betrifft"", prophezeit er.";https://www.sueddeutsche.de/muenchen/landkreismuenchen/mehr-effizienz-spezialist-fuer-e-mobilitaet-1.4219919;sz.de;Anna-Maria Salmen
15.11.2018;Mit dem Smartphone durch die Nacht;"Smartphones haben klassische Kameras in vielen Situationen entbehrlich gemacht: Die Qualität der Fotos ist nicht auf demselben Niveau, aber für viele normale Nutzer gut genug, um keine schwere Kamera mehr mitzunehmen. In zwei Fällen haben Smartphones aber immer noch das Nachsehen: Klassische Objektive zoomen weit entfernte Motive besser heran und bei wenig Licht holen sie deutlich mehr aus dem Bild heraus.

Mit dem Nachtsichtmodus will Google Smartphone-Nutzer in die Lage versetzen, auch im Dunklen brauchbare Fotos zu knipsen. Andere Hersteller wie Oneplus und Huawei haben vergleichbare Funktionen entwickelt.

Der Nachtsichtmodus Google wird in den kommenden Tagen nur auf Googles eigenen Pixel-Geräten freigeschaltet. Die SZ hat verglichen.

Beide Fotos wurden mit dem Pixel 2 XL in der Dämmerung an der Alster in Hamburg gemacht. Auf der unteren Aufnahme ist das Motiv kaum zu erkennen, die Mauer versinkt im Dunklen. Der Nachtsichtmodus (oben) offenbart zahlreiche Details, ohne dass hellere Bereiche überbelichtet werden. Wer bei schwierigen Lichtverhältnissen fotografiert, hat mehrere Möglichkeiten, das Motiv besser auszuleuchten: Größere Blendenöffnungen, höhere ISO-Zahlen oder längere Belichtungszeiten offenbaren in dunklen Bereichen mehr Details. Diese manuellen Einstellungen bringen aber Nachteile wie Bildrauschen und verschwommene Motive mit sich. Auch Blitzlicht und Stative sind nur unter bestimmten Bedingungen eine Option und insbesondere bei Smartphones unpraktisch bis unbrauchbar.

Die ersten Tests des Pixel-Nachtsichtmodus zeigen, dass sich Smartphones künftig auch eignen werden, um in der Dämmerung und in der Nacht zu fotografieren. In diesem Beispiel ist auf dem unteren Bild zwar der besser ausgeleuchtete Hintergrund zu erkennen, das Motiv im Vordergrund verschwindet aber fast vollständig. Oben sieht man Details in allen Bereichen des Fotos. Bei aktiviertem Nachtsichtmodus führt das Pixel vor der Aufnahme mehrere Berechnungen durch: Wie ruhig hält der Nutzer das Gerät? Wie viel Bewegung ist im Bild? Bei zitternden Händen oder unruhigen Motiven macht das Smartphone viele Einzelbilder mit kurzer Belichtung, um Schlieren zu vermeiden. Wird mit Stativ fotografiert, kann die Belichtungszeit auf bis zu eine Sekunde steigen.

Die Funktion liefert auch brauchbare Ergebnisse, wenn man aus der Hand fotografiert. Bei diesen Aufnahmen in einer Berliner Bar lag die Hand auf der Theke. Nur auf dem oberen Foto lässt sich auf der Getränkekarte erkennen, dass im Prenzlauer Berg Flaschenbier aus Bayern verkauft wird. Bei Google arbeiten Hunderte Mitarbeiter an der Software der Pixel-Kameras, sagt Produktmanager Isaac Reynolds. Alle Berechnungen des Nachtsichtmodus laufen lokal auf dem Smartphone und benötigen keinen Zugriff auf Googles Server. Die Entwickler haben die Software dafür mit Millionen Beispielbildern gefüttert, um sie zu trainieren.

Bei diesem sogenannten Machine-Learning kommen künstliche neuronale Netze zum Einsatz. Als Vorbild dient das menschliche Gehirn, dennoch ist diese Form der künstlichen Intelligenz noch weit von menschlichen Fähigkeiten entfernt. Menschen singen und spielen, malen Bilder, schreiben Bücher und lernen Fremdsprachen. Maschinen können immer nur eine Aufgabe erledigen - das allerdings mit teils beindruckenden Resultaten.

In diesem Fall sind Googles Algorithmen in der Lage, die Farben des Motivs zu rekonstruieren. Eigentlich taucht die schummrige Beleuchtung den Clown in ein rötliches Licht, doch mit dem Nachtsichtmodus erkennt man die rosafarbene Hose, die violette Jacke und die aschblonden Haare der Puppe. Der Nachtsichtmodus eignet sich auch für Selfies
Google-Nachtsichtmodus

Quelle: SZ

Viele Menschen nutzen ihr Smartphone für Selfies. Bald können sie fast rund um die Uhr Bilder von sich selbst aufnehmen, denn der Nachtsichtmodus funktioniert auch mit der Frontkamera. Das obere Foto ist zwar körnig und rauscht an einigen Stellen - aber zumindest lässt sich erkennen, wer den Auslöser gedrückt hat. Die untere Aufnahme ist als Portraitfoto dagegen gar nicht zu gebrauchen und eignet sich höchstens, um den Nachbarn ins Wohnzimmer zu blicken. Es braucht nur wenig Licht
Google-Nachtsichtmodus

Quelle: SZ

Algorithmen können vieles, aber Licht finden, wo es keins gibt, gehört nicht dazu. Trotz seines Namens benötigt der Nachtsichtmodus zumindest ein bisschen Beleuchtung, damit auf dem Foto etwas zu erkennen ist. In kompletter Dunkelheit bleibt das Bild einfach schwarz, ein Fotovergleich erübrigt sich in diesem Fall.

Allerdings liefert auch eine spärliche Lichtquelle der Smartphone-Kamera genügend Informationen, um brauchbare Aufnahmen zu produzieren. Schon eine Tür, die einen Spaltbreit geöffnet ist, bringt Licht ins Dunkle.";https://www.sueddeutsche.de/digital/google-pixel-kamera-test-1.4211386;sz.de;Simon Hurtz
03.11.2018;"Vom ""Flaggschiff-Killer"" zum Flaggschiff";"Oneplus ist sanfter geworden. 2014 pries das chinesische Unternehmen sein erstes Smartphone aggressiv als ""Flaggschiff-Killer"" an. Das Versprechen: 300 Euro für ein Gerät, das mit den besten Handys mithalten kann. Vier Jahre später wären solche Kampagnen deplatziert: Das Oneplus 6T ist selbst ein Flaggschiff.

Die Einstiegsversion mit 6 GB Arbeitsspeicher und 128 GB internem Speicher kostet 550 Euro. Wer mehr benötigt (8 GB/256 GB), zahlt 630 Euro. Damit ist das 6T immer noch wesentlich günstiger als die teuersten Modelle von Apple, Google oder Samsung. Doch Smartphone-Preise sind insgesamt deutlich gestiegen. Dementsprechend fallen ein paar hundert Euro Differenz nicht mehr so auf wie vor einigen Jahren, auch wenn die absolute Ersparnis natürlich dieselbe bleibt. Das erste was am 6T auffällt, ist, worauf Oneplus verzichtet hat. Vor allem der fehlende Klinkenanschluss ärgert viele hartgesottene Fans. Dem Großteil der normalen Nutzer dürfte das egal sein: Viele besitzen bereits drahtlose Bluetooth-Kopfhörer, der Rest verwendet den mitgelieferten Adapter.

Ebenfalls weggefallen ist der Fingerabdrucksensor auf der Rückseite. Wer das 6T entsperren will, muss den Finger auf eine kleine Stelle am unteren Bildschirmrandlegen. Das ist praktisch, wenn das Smartphone auf dem Tisch liegt - wenn es denn funktioniert: Die Erkennungsrate des Testgeräts lag unter der des Vorgängers oder anderer Geräte mit klassischem Fingerabdrucksensor. Beim zweiten oder dritten Versuch klappte es meist, ganz ausgereift ist der Scanner nicht.
Wenig Notch, viel Display

Im Alltag ist das kein großes Problem, da man mehrere biometrische Authentifizierungsmethoden kombinieren kann. Das 6T lässt sich über die Frontkamera entsperren, die das Gesicht des Nutzers erkennt. Das klappt schnell und verlässlich, ist allerdings weniger sicher als der Fingerabdruck. Anders als Apple verbaut Oneplus keine Infrarotkamera, die 3D-Scans des Gesichts anfertigt. Dementsprechend lässt sich die Gesichtserkennung mit Masken überlisten, einigen Nutzern soll es auch schon mit ausgedruckten Fotos gelungen sein. Deshalb verlangt Oneplus für sicherheitsrelevante Funktionen wie etwa Bezahlvorgänge den Fingerabdruck.

Den oberen Bildschirmrand zerschneidet nun keine breite, schwarze Einkerbung mehr. Die sogenannte Notch ist klein und tropfenförmig, dementsprechend steht mehr Fläche für das Display zu Verfügung. Wichtiger als die Größe des Displays ist dessen Qualität. Hier hat Oneplus beim 6T fast alles richtig gemacht. Das Oled-Panel ist hell und scharf, hat gute Blinkwinkelstabilität und stellt Farben satt dar. Wer Wert auf hohe Farbtreue legt, sollte in den Einstellungen den sRGB- oder DCI-P3-Modus wählen. Im ersten Moment wirkt das Bild etwas blasser, tatsächlich entsprechen Fotos dann aber eher der Realität als in der knallbunten Standardeinstellung.
Die Kamera ist gut - aber nicht sehr gut

Bei der Kamera bleibt dagegen alles beim Alten. Im 6T stecken die gleichen Linsen und Objektive wie im Vorgänger, also eine Dualkamera mit 16 und 20 Megapixel hinten und eine Selfie-Kamera mit 20 Megapixel. Oneplus hat die Software überarbeitet und nach eigenen Angaben auf die Wünsche der Nutzer gehört. Die hätten sich vor allem Verbesserungen bei Porträts, Essensfotos und Nachtaufnahmen gewünscht. Tatsächlich holen die Algorithmen insbesondere bei schlechten Lichtverhältnissen noch etwas mehr auf den Bildern heraus.

Dennoch macht sich der Preisunterschied zu den 1000-Euro-Smartphones bemerkbar: Das 6T schießt gute Fotos - aber keine sehr guten. Hersteller wie Apple und Huawei verbauen überlegene Hardware, und was Machine Learning angeht, kann derzeit niemand mit Googles Software-Abteilung mithalten.

Auch der Mono-Lautsprecher kann nicht mit der Stereo-Beschallung der Konkurrent mithalten, das 6T ist immer noch nicht komplett wasserdicht (Spritzwasser ja, Tauchgänge nein) und lässt sich nicht drahtlos laden. Das verwundert, weil Oneplus auf der Rückseite Glas statt Aluminium verbaut, was eigentlich den Qi-Ladestandard ermöglicht. So liegt das 6T zwar gut in der Hand, ist aber äußerst anfällig für Sturzschäden. Um Glasbruch auf der Rückseite zu vermeiden, empfiehlt sich also eine Hülle. Immerhin verkauft Oneplus das Zubehör zu halbwegs erträglichen Preisen und verlangt nur rund 20 statt bis zu 50 Euro für das bisschen Silicon oder Stoff.
Android 9 ohne Verschlimmbesserungen

Ein großer Vorteil der Vorgänger ist erhalten geblieben: Das 6T kommt mit der aktuellen Android-Version 9 und soll mindestens zwei Jahre lang Funktionsupdates und drei Jahre lang Sicherheitsupdates erhalten. Die wenigen Veränderungen an der Oberfläche des Betriebssystems sind keine Verschlimmbesserungen, sondern sinnvolle Anpassungen. Dazu gehört etwa die optionale Gestensteuerung, die besser funktioniert als Googles Versuch, die intuitive Navigation der neuen iPhones nachzuahmen.

""Das Abhängigkeitsverhältnis, das viele Menschen zu ihren Smartphones haben, ist definitiv ein Problem"", sagte Oneplus-Mitgründer Carl Pei der SZ im September. ""Wir haben Ideen, wie wir sie dazu bringen können, Technik bewusster und dosierter zu nutzen."" Dennoch fehlen die sogenannten Digital-Wellbeing-Funktionen, die Google auf seinen Pixel-Geräten anbietet. Damit lässt sich die eigene Smartphone-Nutzung überwachen und reglementieren, indem man Zeitlimits für bestimmte Apps festlegt.
Fazit

Das Oneplus 6T entwickelt den Vorgänger sinnvoll weiter. Die Fotos sind etwas hübscher, das Display etwas besser, und der Verzicht auf die Klinkenbuchse schafft Platz für einen größeren Akku, mit dem das Smartphone locker durch den Tag kommt. Der Fingerabdrucksensor im Display arbeitet noch nicht ganz so zuverlässig wie klassische Sensoren. Dafür funktioniert die Software tadellos. Apps starten schnell, kaum ein anderes Android-Gerät lässt sich so flüssig bedienen wie das 6T. Die größte Stärke von Oneplus bleibt das Preis-Leistungsverhältnis: Für unter 600 Euro gibt es derzeit kein besseres Gesamtpaket.

Grundsätzlich gilt aber. Die Unterschiede zwischen den Spitzen-Handys sind kleiner geworden. Die Vorjahresmodelle können fast dasselbe. Statt ""Welches neue Smartphone brauche ich?"" lautet die sinnvollere Frage: Brauche ich wirklich ein neues Smartphone?";https://www.sueddeutsche.de/digital/smartphone-oneplus-6t-test-1.4192533;sz.de;Simon Hurtz
16.10.2018;Schon wieder ein neues bestes Smartphone der Welt;"Sieh an, schon wieder ein neues Luxus-Smartphone. Der Herbst ist Handy-Hochsaison: Nach Apple, Samsung, Huawei und LG ist nun Google an der Reihe. Je nach Konfiguration verlangen die Hersteller dafür knapp unter, Apple sogar deutlich mehr als 1000 Euro. Kann sich Google von der Konkurrenz absetzen?

Das klappt bereits auf dem Datenblatt. Das Pixel 3 ist das Rasiermesser unter den Smartphones. Gillette und Wilkinson wetteifern, wer die meisten Klingen in einem Stück Plastik unterbringt. Smartphone-Hersteller überbieten sich gegenseitig, wer die meisten Kameras verbaut. Samsung ist bei vier angekommen. Ein Messer reicht für eine gründliche Rasur, sagt der Barbier. Eine Kamera reicht für gute Fotos, sagt Google.

Und noch etwas ist anders: Wenn Apple neue iPhones vorstellt, reden die Manager über Hardware: brillante Displays, die schnellsten Prozessoren. Google schwärmt von Software: mächtiges Machine-Learning, die smartesten Algorithmen. Bei Google ist das Smartphone nur der Präsentierteller, auf dem die Entwickler ihre Fortschritte ausstellen dürfen. Die sind in der Tat beeindruckend. Google beweist, dass die Qualität der Fotos nicht von der Zahl der Kameras abhängt. Das Pixel macht tolle Bilder, die womöglich noch ein Stückchen besser sind als die der Konkurrenz. Mit endgültiger Sicherheit lässt sich das kaum sagen, zu gering sind mittlerweile die Unterschiede. Je nach Lichtverhältnissen und individuellen Vorlieben liegt mal der eine, mal der andere Hersteller vorn. Gut sind sie alle: Apple, Samsung, Huawei und Google produzieren Fotos, die vor wenigen Jahren teuren Spiegelreflexkameras vorbehalten waren.

Dennoch haben zwei Kameras Vorteile. Ein zusätzliches Tele-Objektiv ermöglicht optischen Zoom, der den Bildausschnitt nicht nur digital vergrößert, sondern den Bildinhalt tatsächlich näher heranholt. Außerdem können Smartphones mit unterschiedlichen Brennweiten und Blendenöffnungen Fotos mit Tiefenschärfe produzieren. In fast allen Situationen schafft es Google, diese Funktionen mit geschickten Software-Tricks nachzubilden - aber nicht immer: Eine Dual-Kamera zoomt besser und zuverlässiger als die klügste Software.
Das Pixel 3 XL gewinnt keine Schönheitspreise

Dass Google keine grundsätzliche Abneigung gegen zwei Kameras hegt, zeigt die Vorderseite. Das zusätzliche Weitwinkel-Objektiv vergrößert bei Bedarf den Bildausschnitt. Wer Selfie-Sticks scheut, kann mit dem Pixel 3 trotzdem Gruppenfotos machen, ohne sich den Arm auszurenken.

Die Technik benötigt allerdings Platz, der mit einem Kompromiss einhergeht: Um Kameralinsen und Lautsprecher unterzubringen, hat das kleinere Pixel 3 schwarze Ränder oberhalb und unterhalb des Displays. Die größere XL-Version integriert die Technik in einer dicken Einbuchtung am oberen Rand. Dieser sogenannte Notch ziert mittlerweile fast alle Smartphones, doch die meisten Hersteller gehen dabei dezenter vor als Google. Besonders elegant ist die Kerbe nicht, aber wer sein Smartphone nicht ausschließlich als Modeaccessoire begreift, wird sich daran gewöhnen.
Das derzeit wohl beste Android-Smartphone ist auch eines der teuersten

Der Rest des Bildschirms entschädigt für den Schönheitsfehler. Auf dem Oled-Display machen Fotos und Videos Spaß. Es ist hell und scharf, die Farben wirken natürlich, und der Blaustich des Vorgängers ist verschwunden. Auf der Rückseite setzt Google auf Glas statt Aluminium. Das fühlt sich edel an und ist nötig, um das Pixel 3 drahtlos zu laden - aber auch kratzanfällig. So bleibt einem wohl nichts anderes übrig, als das hübsche Design in einer etwas weniger hübschen Hülle zu verstecken.

Zwischen 850 und 1050 Euro will Google für das neue Pixel. Andere Android-Hersteller verbauen mehr Arbeitsspeicher und mehr Kameras, größere Akkus und Displays mit kleinerem Rand. Wenn Geld keine Rolle spielt, ist das Pixel 3 für die meisten Nutzer dennoch die erste Wahl. Nur Google liefert garantiert drei Jahre Sicherheitsupdates und passt das Android-Betriebssystem nicht auf teils fragwürdige Art und Weise an.

Wer gern fotografiert und die neueste Technik will, wird mit dem Pixel glücklich. Wer sein altes Smartphone behält und die 1000 Euro in eine schöne Reise steckt, wird vielleicht noch etwas glücklicher - auch mit weniger brillanten Urlaubsfotos.";https://www.sueddeutsche.de/digital/google-pixel-3-schon-wieder-ein-neues-bestes-smartphone-der-welt-1.4172102;sz.de;Simon Hurtz
09.10.2018;Diese Smartphone-Kamera soll das iPhone abhängen;"Das Google Pixel 3 ist das am schlechtesten gehütete Geheimnis der Tech-Branche. Das Blog 9to5Google hat seit Juni jeden einzelnen Leak des Smartphones dokumentiert. Es sind mittlerweile mehr als zwei Dutzend, der Artikel gleicht einem Fortsetzungsroman. Zwischendurch ging eine Charge mutmaßlicher Testexemplare verloren, die daraufhin auf dem ukrainischen Schwarzmarkt angeboten wurden. Sogar Google selbst machte sich mit einem selbstironischen Video darüber lustig. Selbst das leak-geplagte Apple schafft es besser, seine iPhones geheim zu halten.

Wer seine Freizeit vorzugsweise auf Techblogs verbringt und den bekannten Brancheninsidern auf Twitter folgt, erfährt beim Google-Event am Dienstagabend also kaum etwas Neues. Für alle anderen gibt es diesen Überblick.

Wie schon im vergangenen Jahr stellt Google jede Menge neue Hardware vor, darunter ein smartes Display (Home Hub) und ein Tablet mit Chrome-OS-Betriebssystem (Pixel Slate). Allerdings ist noch offen, ob und wann diese Geräte auf den deutschen Markt kommen. Was bei der Präsentation auffiel: Auf der Bühne waren Menschen unterschiedlicher Herkunft zu sehen, Frauen hatten viel Redeanteil. Das hat Google einigen Konkurrenten schon mal voraus. Das zentrale Produkt des Google-Events wird zeitnah in Deutschland erhältlich sein: Der Verkauf des Google Pixel 3 startet hierzulande am 2. November. Es ist Googles Antwort auf andere 1000-Euro-Handys wie das iPhone XS oder Samsungs Galaxy S9. Die SZ konnte das Smartphone vorab ausprobieren. Hier sind die ersten Eindrücke.
Zahlen und Fakten

Google bietet erneut zwei Smartphones in unterschiedlichen Größen an. Das 5,5-Zoll-Display des normalen Pixel 3 löst mit 2160 x 1080 Pixel auf, die XL-Version bietet 2960 x 1440 Pixel auf 6,3 Zoll. Beide Smartphones setzen auf Oled-Technologie. Der Bildschirm des Pixel 2 XL hatte Probleme mit Einbrennen und einen Blaustich bei bestimmten Blickwinkel. Diese Mängel sollen behoben sein. Google hat diesmal noch enger mit Lieferanten zusammengearbeitet und verspricht, dass das gesamte Smartphone aus einer Hand kommt.

Der Snapdragon 845 kann auf dem Papier nicht mit Apples aktuellem A12-Prozessor mithalten, und viele andere Hersteller verbauen mittlerweile mehr als 4 GB Arbeitsspeicher. In der Praxis dürfte sich das aber kaum bemerkbar machen. Bereits der Vorgänger gehörte zu den Android-Smartphones, bei dem Apps am schnellsten starteten und sich das Betriebssystem am flüssigsten bedienen ließ. Letztendlich ist das Zusammenspiel aus Hard- und Software wichtiger als die Leistungsdaten der verbauten Komponenten.
Optik und Haptik

Eigenwillig. So lassen sich die Entscheidungen von Googles Designern wohl am besten beschreiben. Eine massive schwarze Einbuchtung dominiert die Frontansicht des Pixel 3 XL. Diese sogenannte Notch bietet am oberen Displayrand Platz für einen der beiden Stereo-Lautsprecher und zwei Kameras. Apple hatte den Trend im vergangenen Jahr mit dem iPhone X gestartet (andere Hersteller waren noch früher dran, aber erst Apple machte das Design massentauglich) und löste damit erst Spott aus. Mittlerweile kommt kaum noch ein Smartphone ohne Notch auf dem Markt. Viele integrieren die Kerbe aber dezenter, als Google es tut.

Zusätzlich zum Notch am oberen Ende spendiert Google der XL-Version auch noch ein schwarzes Kinn unterhalb des Bildschirms. Auch das normale Pixel 3 weist schwarze Streifen an beiden Seiten auf. Andere Hersteller sind dem komplett randlosen Display schon deutlich nähergekommen. Wenn überhaupt, ist das ein ästhetischer Kritikpunkt: Im Alltagsgebrauch fallen die wenigen Millimeter verschenkte Bildschirmfläche kaum ins Gewicht. Insgesamt ähneln beide Smartphones ihren Vorgängern.

Die Verarbeitung wirkt beim ersten Anfassen gewohnt hochwertig. Das ist in diesem Preisbereich aber Standard. Die Rückseite ist diesmal komplett mit Gorilla-Glas überzogen, beim Pixel 2 kam teilweise Aluminium zum Einsatz. Das Smartphone liegt damit gut in der Hand, dürfte einen Falltest aber kaum ohne Schaden überstehen. Dementsprechend empfiehlt sich eine Schutzhülle, die das Handy zumindest hinten und an den Seiten umschließt. Der Fingerabdrucksensor befindet sich nach wie vor auf der Rückseite. Auf eine Technik ähnlich der von Apples Face-ID, die das Smartphone per Gesichtserkennung entsperrt, müssen Nutzer verzichten.
Kamera

Die größte Stärke des Pixel 2 war die Qualität der Fotos. Ein Jahr nach Verkaufsstart zählt die Kamera immer noch zu den besten. Glaubt man Google, wird der Nachfolger nochmal deutlich hochwertigere Fotos liefern. Der Weg, den Google dabei geht, ist ungewöhnlich: Im Gegensatz zu fast allen anderen Premium-Smartphones, die in diesem Jahr herausgebracht wurden, kommt das Pixel 3 ohne Dual-Kamera auf der Rückseite. Stattdessen verbaut Google zwei Frontkameras.

Zwei Kameras haben zwei entscheidende Vorteile: Das zusätzliche Tele-Objektiv ermöglicht echten optischen Zoom, der im Gegensatz zum Digitalzoom nicht nur den Bildausschnitt verändert und dann digital vergrößert, sondern den Bildinhalt tatsächlich näher heranholt. Außerdem können Smartphones mit unterschiedlichen Brennweiten und Blendenöffnungen Fotos mit Tiefenschärfe produzieren.

Google nutzt Machine Learning, um diesen sogenannten Bokeh-Effekt nachträglich zu berechnen. Das klappte beim Pixel 2 erstaunlich gut. Mitunter gingen aber Details wie Haare oder Wassertropfen verloren, weil die Software Vorder- und Hintergrund nicht richtig unterscheiden konnte. Die Bilder, die beim Hands-on entstanden, wiesen diese Problematik nicht mehr auf. Um die Fotoqualität abschließend zu beurteilen, sind aber ausführliche Tests mit unterschiedlichen Lichtverhältnissen nötig.

Auch den fehlenden optischen Zoom will Google mit leistungsfähiger Software wettmachen. Das Smartphone nutzt das leichte Zittern der Hände kurz vor und nach dem Auslösevorgang, um mehrere Einzelfotos zu erstellen. Die Informationen, die beim Zoomen verloren gehen, sollen durch Kombination der Bilder kompensiert werden. Das funktioniert auch auf einem Stativ, da sich der optische Bildstabilisator minimal bewegt.

Warum Google stattdessen nicht einfach zwei Kameras verbaut, so wie es fast alle anderen Hersteller machen, konnten die Mitarbeiter beim Vorführtermin nicht beantworten. Falls die Software-Lösung aber so gut funktioniert wie versprochen, müssen sich Nutzer nicht darüber ärgern. An einer grundsätzlichen Abneigung gegenüber Dual-Kameras kann es jedenfalls nicht liegen: Das zusätzliche Weitwinkelobjektiv vergrößert den Bildausschnitt. So können Menschen, die sich weigern, Selfie-Sticks zu benutzen, trotzdem Gruppenfotos machen.

Als praktisch könnten sich außerdem mehrere neue Funktionen erweisen, die Google seiner Kamera-App spendiert. Ähnlich wie bei Apple und Samsung lässt sich die Stärke des Bokeh-Effekts nachträglich anpassen. Der sogenannte Top-Shot-Modus erstellt bei jedem Foto ein kurzes Video, woraufhin künstliche Intelligenz daraus ein oder mehrere Bilder auswählt. Ein anderer Modus entscheidet selbstständig, wann der beste Moment ist, um die Aufnahme auszulösen. Die Machine-Learning-Technologie soll lächelnde Menschen erkennen und das bekannte Problem verhindern, dass auf dem schönsten Foto immer jemand blinzelt.
Software

Alle iPhones erhalten jedes iOS-Update zur gleichen Zeit: unmittelbar nach der Veröffentlichung. Je nach Hersteller kann es bei Android-Geräten auch mal ein halbes Jahr dauern, weil Unternehmen wie Samsung und Huawei darauf beharren, das pure Android optisch und funktional anzupassen - nicht immer zum Guten. Oft werden Smartphones bereits zwei Jahre nach Veröffentlichung gar nicht mehr mit Updates versorgt.

Bei Google laufen die Updates dagegen ähnlich reibungslos wie bei Apple. Das Pixel 3 wird mit dem unveränderten Android 9 ausgeliefert und integriert zusätzlich einige Pixel-exklusive Funktionen. Wer das Smartphone etwa umdreht und die flache Hand kurz auf der Rückseite ablegt, schaltet es stumm und versetzt es in den Do-Not-Disturb-Modus. Das könnte peinlichem Handyklingeln in Konferenzen ein Ende bereiten. Überhaupt legt Google gerade viel Wert auf ablenkungsfreie Smartphone-Nutzung. Technik solle den Menschen dienen, nicht umgekehrt, ist es seit einiger Zeit aus dem gesamten Silicon-Valley zu hören. Nach dem ""Techlash"" und Warnungen vor angeblicher Smartphone-Sucht reagieren die Firmen und wollen Nutzern Möglichkeiten an die Hand geben, den Umgang mit ihren Geräten zu kontrollieren und zu begrenzen.

Apple hat dafür in iOS 12 die Screen-Time-Funktion integriert, Google setzt auf die App ""Digital Wellbeing"". Nutzer erfahren, wie viele Benachrichtigungen sie erhalten und wie lange sie täglich auf ihr Smartphone schauen. Sie können Limits für einzelne Apps setzen und zu bestimmten Zeiten automatisch Push-Nachrichten deaktivieren oder das Display auf schwarz-weiß schalten. Bislang war Digital Wellbeing nur in einer Beta-Version auf Pixel-Geräten erhältlich. Mit dem Start des Pixel 3 will Google die Funktion bekannter machen.
Akkulaufzeit

Das normale Pixel 3 hat einen Akku mit 2915 Milliampere-Stunden. Beim XL wächst auch der Akku, hier misst er 3430 mAh. Die Laufzeit hängt stark von der individuellen Nutzung ab. Die beiden Vorgänger kommen mit einer Ladung problemlos durch den Tag, oft reicht auch ein Ladevorgang alle zwei Tage.

Im Lieferumfang befindet sich ein Schnelllade-Adapter, um das Pixel mit 18 Watt über USB-C aufzuladen. 15 Minuten an der Steckdose sollen Strom für bis zu sieben Stunden Nutzungszeit liefern. Beim Vorgänger kommt dieselbe Technik zum Einsatz. Hier reicht tatsächlich bereits kurzes Laden, um den Akku um 20 oder 30 Prozent zu füllen.
Preis

Das Pixel 3 kostet in der Einstiegsversion 849 Euro, der große Bruder ist 100 Euro teurer. Ebenfalls jeweils 100 Euro Aufschlag verlangt Google für eine Verdopplung des Speicherplatzes von 64 auf 128 GB. Geräte mit 256 oder 512 GB, wie sie Apple beim iPhone anbietet, fehlen leider. Das Pixel kommt in drei Farben: Schwarz, Weiß und ein zartes Rosa, das Google ""not Pink"" nennt. Diese neue Farbvariante gibt es in beiden Größen nur mit 64 GB. Neben dem Ladegerät liegen dem Pixel USB-C-Kopfhörer und ein Adapter bei, um alte Kopfhörer mit Klinkenanschluss weiter nutzen zu können. Pixel Stand

Das Pixel 3 kann erstmals drahtlos geladen werden. Das macht sich der Pixel Stand zunutze. Wer das Smartphone darauf platziert, lädt es auf und verwandelt es automatisch in einer Art smartes Display. Die Stereo-Lautsprecher auf der Vorderseite klangen beim kurzen Probehören für Smartphone-Verhältnisse durchaus brauchbar. Da Google den Home Hub vorerst nicht nach Deutschland bringt, könnte die Kombination aus Pixel und Pixel Stand könnte für einige Nutzer eine Alternative sein. Der Stand muss separat gekauft werden und kostet 79 Euro.";https://www.sueddeutsche.de/digital/google-pixel-3-diese-smartphone-kamera-soll-das-iphone-abhaengen-1.4163604;sz.de;Simon Hurtz
13.07.2018;Die Gesellschaft der Metadaten;"Jeden Tag verarbeitet Google 3,5 Milliarden Suchanfragen. Die Nutzer googeln alles: Lebensläufe, Krankheiten, sexuelle Vorlieben, Tatpläne. Und geben damit mehr von sich preis, als ihnen lieb ist. Aus den aggregierten Daten lassen sich in Echtzeit Rückschlüsse über den Gefühlshaushalt der Gesellschaft ziehen. Wie ist die Stimmung? Wie ist die Kauflaune? Welches Produkt wird in dieser Sekunde in welcher Region nachgefragt? Wo wird häufig nach Krediten gesucht? Suchanfragen sind ein konjunktureller Gradmesser. Zentralbanken greifen schon seit Längerem auf Google-Daten zurück und speisen die Daten in ihre makroökonomischen Modelle ein, um das Konsumentenverhalten zu prognostizieren.

Die Suchmaschine ist nicht nur ein Seismograf, der die Zuckungen und Regungen der digitalen Gesellschaft erfasst, sondern auch ein Werkzeug, das Präferenzen erzeugt. Wenn man aufgrund einer Stauprognose von Google Maps seine Route ändert, ändert man nicht nur das eigene Verhalten, sondern auch das anderer Verkehrsteilnehmer, indem man mit seiner Selbstverdatung die Parameter der Simulation verändert. Google kann anhand der im Smartphone integrierten Beschleunigungssensoren erkennen, ob jemand gerade mit dem Rad, dem Auto oder zu Fuß unterwegs ist. Wer bei der Google-Suche nach ""Merkel"" die algorithmisch generierte Suchergänzung ""am Ende"" anklickt, erhöht die Wahrscheinlichkeit, dass der Vervollständigungsmechanismus dies auch bei anderen anzeigt. Die mathematischen Modelle produzieren eine neue Wirklichkeit. In einer kontinuierlichen Feedbackschleife wird das Verhalten von Millionen Nutzern konditioniert. Und kontrolliert.
Landschaften des Wissens

Der italienische Philosoph und Medientheoretiker Matteo Pasquinelli, der an der HfG Karlsruhe lehrt, hat die These aufgestellt, dass mit der Datenexplosion eine neue Steuerungsform möglich werde: eine ""Gesellschaft der Metadaten"". Mit Metadaten könnten neue Formen der biopolitischen Steuerung zur Kontrolle der Massen und Verhaltenssteuerung etabliert werden, etwa Online-Aktivitäten in Social- Media-Kanälen oder Passagierströme in öffentlichen Verkehrsmitteln. ""Daten"", schreibt Pasquinelli, ""sind nicht Nummern, sondern Diagramme von Oberflächen, Landschaften des Wissens"", die eine neue Sicht auf die Welt und die Gesellschaft eröffnen: ""die algorithmische Vision"". Die Akkumulation der Zahlen durch die Informationsgesellschaft habe einen Punkt erreicht, wo Zahlen zu einem Raum werden und eine neue Topologie kreierten. Die Gesellschaft der Metadaten könne als Ausweitung der kybernetischen Kontrollgesellschaft verstanden werden, schreibt Pasquinelli: ""Heute geht es nicht mehr darum, die Position eines Individuums zu bestimmen (die Daten), sondern die allgemeine Tendenz der Masse zu erkennen (die Metadaten).""

Das Problem sieht Pasquinelli nicht darin, dass Individuen wie bei der Stasi ausgeleuchtet, sondern dass sie vermasst werden und die Gesellschaft als Ganzes berechenbar und kontrollierbar werde. Als Beispiel nennt er das NSA-Massenüberwachungsprogramm Skynet, bei dem mithilfe von Mobilfunkdaten in der Grenzregion zwischen Afghanistan und Pakistan Terroristen identifiziert wurden. Das Programm puzzelte die täglichen Routinen von 55 Millionen Mobilfunknutzern zusammen: Wer verreist mit wem? Wer teilt Kontakte? Wer bleibt über Nacht bei Freunden? Ein Klassifikationsalgorithmus analysierte die Metadaten und errechnete für jeden Nutzer einen Terror-Score. Der ehemalige NSA- und CIA-Chef Michael Hayden brüstete sich mit der Aussage: ""Wir töten Menschen auf Basis von Metadaten."" Ein Satz, der in seiner kühlen Menschenverachtung schaudern lässt. Es gibt gar keine Zielperson mehr - das militärische Target ist kein Subjekt, sondern nur noch die Summe seiner Metadaten. Das ""algorithmische Auge"" sieht ja keinen Terroristen, sondern lediglich eine verdächtige Verbindung im Dunst der Datenwolken. In brutaler Konsequenz bedeutet das: Wer suspekte Links oder Muster produziert, wird liquidiert.

Auf Basis der Skynet-Berechnungen wurden Tausende Menschen bei Drohnenschlägen getötet. Wie viele unschuldige Zivilisten ums Leben kamen, ist unklar. Die Methodik ist umstritten, weil der Machine-Learning-Algorithmus nur aus bereits identifizierten Terroristen lernte und diese Ergebnisse blind reproduzierte. Das heißt: Wer dieselben Trajektorien, sprich Metadaten wie ein Terrorist hatte, war plötzlich selbst einer. Die Frage ist, wie scharf die algorithmische Vision gestellt wird. ""Wozu könnte es führen, wenn der Algorithmus von Google Trends auf soziale Fragen, politische Kundgebungen, Streiks oder den Aufruhr in den Peripherien der europäischen Metropolen angewandt wird?"", fragt Pasquinelli.

Die Datengurus hegen eine Obsession, menschliche Interaktionen wie das Wettergeschehen vorauszuberechnen. Die Adepten der Denkschule ""Social Physics"", welche der Datenwissenschaftler Alex Pentland begründet hat, blicken auf die Welt wie durch ein Hochleistungsmikroskop: Die Gesellschaft besteht aus Atomen, um deren Kerne Individuen wie Elektronen auf festen Umlaufbahnen kreisen. Facebook-Chef Mark Zuckerberg sagte, es existiere ein ""fundamentales mathematisches Gesetz, das sozialen Beziehungen zugrunde liegt"". Liebe? Job? Verbrechen? Alles determiniert, alles berechenbar! Als wäre die Gesellschaft ein lineares Gleichungssystem, in dem sich Variablen auflösen lassen. In Isaac Asimovs Science-Fiction-Roman ""Foundation"" entwickelt der Mathematiker Hari Seldon die fiktive Wissenschaft der Psychohistorik, eine Großtheorie, die Elemente der Psychologie, Mathematik und Statistik vereint. Die Psychohistorik modelliert die Gesellschaft nach der physikalischen Chemie: Sie geht davon aus, dass sich das Individuum wie ein Gasmolekül verhält. Analog zu den Eigenschaften eines Gasmoleküls lassen sich die zuweilen chaotischen Bewegungen eines Individuums nicht berechnen, dafür aber mithilfe statistischer Gesetzmäßigkeiten der allgemeine Verlauf und ""Aggregatszustand"" der Gesellschaft.

In dem Roman sagt Imperator Cleon I. zu seinem Mathematiker: ""Sie brauchen nicht vorherzusagen. Wählen Sie nur eine Zukunft aus - eine gute Zukunft, eine nützliche Zukunft -, und machen Sie die Art von Vorhersagen, die die menschlichen Gefühle und Reaktionen so ändern, dass die Zukunft, die Sie vorhergesagt haben, auch herbeigeführt wird."" Auch wenn Seldon diesen Weltenplan als ""praktisch nicht durchführbar"" verwirft, beschreibt dies trefflich die Technik des Social Engineering, bei die Realität (und Sozialität) konstruiert werden und Individuen auf ihre physikalischen Eigenschaften reduziert werden. Darin manifestiert sich eine neue Machttechnik: Die Menge wird nicht mehr beherrscht, sondern berechnet und, das ist die dialektische Pointe, in ihrer Berechenbarkeit total beherrschbar. Wenn man weiß, wohin sich die Gesellschaft bewegen wird, kann man Gruppen durch Manipulationstechniken wie Nudging unter Ausnutzung psychologischer Schwächen in die gewünschte Richtung lenken.

Kürzlich tauchte ein internes Google-Video auf, in dem das behavioristische Konzept eines ""Selfish Ledger"" präsentiert wurde, eine Art Zentralregister, worauf sämtliche Nutzerdaten gespeichert sind: Surfverhalten, Gewicht, Gesundheitszustand. Anhand der Datenpunkte schlägt Google individualisierte Handlungsoptionen vor: sich gesünder ernähren, die Umwelt schützen oder lokale Geschäfte unterstützen. Analog zur DNA-Sequenzierung könne man ""eine Verhaltenssequenzierung"" durchführen und Verhaltensmuster erkennen. So wie sich DNA verändern lässt, ließe sich auch das Verhalten modifizieren. Am Ende dieser Evolution stünde ein perfekt programmierter Mensch, der von KI-Systemen kontrolliert wird.
Techno-autoritärer Politikmodus

Das Bedrohliche an dieser algorithmischen Regulierung ist nicht nur die Subtilität der Steuerung, die sich irgendwo in den opaken Maschinenräumen privater Konzerne abspielt, sondern, dass ein techno-autoritärer Politikmodus installiert wird und die Masse als polit-physikalische Größe wiederkehrt. Nur was Datenmasse hat, hat im politischen Diskurs Gewicht. Die Technikvisionäre denken Politik von der Kybernetik her: Es geht darum, ""Störungen"" zu vermeiden und das System im Gleichgewicht zu halten. Der chinesische Suchmaschinenriese Baidu hat einen Algorithmus entwickelt, der anhand von Sucheingaben bis zu drei Stunden im Voraus vorhersagen kann, wo sich eine Menschenansammlung (""kritische Masse"") bilden wird.

Hier wird der Programmcode zu einer präemptiven Politikvermeidung. Das Versprechen von Politik ist, dass sie zukunftsoffen und gestaltbar ist. Wenn aber das Verhalten von Individuen, Gruppen und der Gesellschaft berechenbar wird, wird politische Willensbildung Makulatur. Wo alles determiniert ist, ist nichts mehr veränderbar.";https://www.sueddeutsche.de/digital/philosophie-die-gesellschaft-der-metadaten-1.4070474;sz.de;Adrian Lobe
26.06.2018;Mitdenkende Ampel soll Radler schützen;"Es ist ein Schreckensszenario für jeden Radler: Man fährt bei Grün geradeaus über eine Kreuzung, als plötzlich ein rechtsabbiegendes Fahrzeug einschert. Wer Glück hat, kann gerade noch reagieren. Wer Pech hat, nicht. Immer wieder kommen Fahrradfahrer ums Leben, weil sie beim Rechtsabbiegen übersehen werden. 2017 waren es allein im Zuständigkeitsbereich des Polizeipräsidiums München zwei, dazu kommen 352 Verletzte. Auch Sophie Büttner hat diese Erfahrung schon gemacht. Sie war mit dem Rad auf dem Weg zur Arbeit, eine Autofahrerin übersah sie an einer Kreuzung, fuhr sie beim Abbiegen an. ""Es war Gott sei Dank nicht so dramatisch, ich hatte nur ein paar Prellungen. Aber da wurde mir bewusst, wie gefährlich die Situation ist und wie oft diese Situation im Verkehr vorkommt"", sagt Büttner. Die Studentin packt das Problem nun selbst an: Das Präventionssystem ""Ampelligence"" soll Radler künftig besser vor Rechtsabbiegern schützen. Einen ersten Härtetest in der Praxis hat es bereits bestanden: in Ismaning.

Wo im Ismaninger Norden die Freisinger Straße und die Zufahrt zum Gewerbegebiet Am Lenzenfleck aufeinandertreffen, ist so eine Kreuzung, an der sich Radfahrer und rechtsabbiegende Lastwagen häufig in die Quere kommen. Schon oft wurde in der Gemeinde darüber diskutiert, wie die Kreuzung sicherer gestaltet werden kann. Ein ideales Testgebiet also für Ampelligence. Das System besteht aus zwei Komponenten: Eine 3-D-Kamera scannt den Fahrrad- beziehungsweise Fußgängerweg neben der Straße und erkennt, wenn sich dort ein Radler nähert. Ist das der Fall, sendet die Kamera ein Signal und ein an der Kreuzung gut sichtbar angebrachtes Warnlicht - die zweite Komponente - fängt an zu blinken. Auf diese Weise werden Auto- oder Lastwagenfahrer, die auf der Straße unterwegs sind, darauf aufmerksam gemacht, dass sie beim Abbiegen auf Radler achten müssen. ""Der Vorteil von Ampelligence gegenüber bestehenden Warnsystemen ist, dass es ein externes System ist, das für alle Verkehrsteilnehmer funktioniert"", sagt Büttner. Die meisten Angebote auf dem Markt sind sogenannte Abbiegeassistenten, die direkt in Lastwagen oder Bussen verbaut werden und die Fahrer im Wageninneren warnen. Das Blinklicht an der Kreuzung kann jeder sehen, unabhängig von der Ausstattung seines Fahrzeugs. Die Idee für das Projekt hat Büttner gemeinsam mit vier weiteren Studenten vergangenen Herbst bei einem Start-up-Workshop der Technischen Universität München in Garching entwickelt. ""Think Make Start"" heißt der Praxiskurs, in dem bewusst Studenten verschiedener Fachrichtungen zur Zusammenarbeit angeregt werden sollen. Mit ihrem Projekt konnte die interdisziplinäre Gruppe - neben Büttner, die als Consumer-Affairs-Studentin vor allem für Fragen der Umsetzung zuständig ist, gehörten dazu Informatik-Student Ozan Pekmezci, Robotics-Spezialist Patrick Grzywok sowie zwei weitere Studenten aus den Fächern Informatik und Maschinenbau - überzeugen. Sie wurden als beste Start-up-Idee ausgezeichnet. Im Juli erhalten sie außerdem den bayerischen Verkehrssicherheitspreis. Um ihre Idee auch in die Tat umzusetzen, waren Büttner und ihre Kollegen Mitte Mai mit einem Prototypen in Ismaning. Die Gemeinde habe sich gleich offen gezeigt für das Projekt, sagt Büttner. Am Lenzenfleck testeten sie, in welchem Winkel die Kamera Radler am besten erkennt, welche Geschwindigkeiten eventuell schwierig sind. In rund 90 Prozent der Versuchsfälle habe das System funktioniert, berichtet Büttner. ""Einige Dinge sind natürlich noch verbesserungsfähig, aber für den Prototypen sind wir zufrieden"", sagt die 26-Jährige.

Dank der technischen Möglichkeiten des Machine Learnings kann die Kamera ihre Erkennfähigkeiten künftig noch verbessern, indem sie anhand von Daten, mit denen sie gefüttert wird, beispielsweise die Bewegungsmuster eines Joggers von jenen eines Radfahrers zu unterscheiden lernt. Außerdem überlegen die Erfinder, die Kamera mit einem Radar zu kombinieren, damit das System auch bei Regenwetter und entsprechend schlechter Sicht zuverlässig arbeitet. Überhaupt haben Büttner und ihre Kollegen noch einiges vor mit Ampelligence. ""Ismaning war der erste Schritt für den Live-Test, auf kleiner Ebene"", sagt Büttner. Als nächstes muss sich der Prototyp der Garchinger Studenten im Großstadtgewimmel bewähren: Ein Pilotprojekt in Berlin ist bereits geplant für den Herbst, auch die Stadt München hat sich inzwischen gemeldet, erzählt Büttner.";https://www.sueddeutsche.de/muenchen/landkreismuenchen/fahrradunfaelle-mitdenkende-ampel-soll-radler-schuetzen-1.4031222;sz.de;Irmgard Gnau
18.06.2018;"""Deutschland muss extrem investieren""";"Ohne den Internet-Ausrüster Cisco liefe nichts im weltweiten Netz. Das Unternehmen aus San José in Kalifornien liefert einen großen Teil jener Router und Switches, die die Datenströme durch das Web lenken, es liefert zudem vielfältige Produkte und Dienste für den Datenschutz. Seit zwei Jahren bemüht sich Deutschland-Chef Oliver Tuszik nun, mit der Initiative ""Deutschland Digital"" den Wandel hierzulande voranzutreiben - in der Politik und in den Unternehmen. Eine manchmal ermutigende und manchmal auch zähe Angelegenheit.

In den vergangenen Monaten hat sich der Wind gedreht und die Unternehmen aus dem Silicon Valley bekommen Gegenwind. Spüren Sie das auch?

Tuszik: Nein, weil wir nicht zu dieser Gruppe von Unternehmen gehören, um die es da geht. Wir haben ein anderes Geschäftsmodell, liefern Hardware, Software und Services für Unternehmen und keine Produkte für das private Umfeld. Wir sind keines dieser Start-ups. Cisco gibt es über 30 Jahre - für dortige Verhältnisse eine lange Zeit. Bei Cisco hat man lange die überraschende These vertreten, kein Land habe bessere Chancen bei der Digitalisierung als Deutschland. Stimmt das immer noch?

Ja. Denn die nächste Welle der Digitalisierung betrifft vor allem Maschinen und Sensoren. Man kann im Silicon Valley virtuell alles aufbauen, aber große Produktionsstätten gibt es dort weniger. Die Länder mit der höchsten Ingenieurskunst haben deshalb die besten Chancen.

Aber es gibt doch im Silicon Valley auch einen Autobauer wie Tesla.

Der hat aber ein Riesenproblem. Tesla will gerade das neue Modell 3 in den Markt bringen, aber schafft es nicht, die Produktion zu skalieren. Genau das aber ist die Stärke von Deutschland. Wir haben Erfahrung, wie man echte Güter produziert. Wenn wir das kombinieren mit Daten und digitalen Innovationen, haben wir in Deutschland eine Riesenchance.

Das entscheidende Problem allerdings ist: Sind unsere Unternehmen schnell genug?

Sie sind nicht so schnell wie Facebook, Airbnb, Uber oder Instagram, aber das sind virtuelle Plattformen, die so gut wie keine Connection zur realen Welt haben oder eine sehr einfache. Deutsche Unternehmen begreifen aber, dass sie von den Plattformen etwas lernen können.

Was denn?

Nehmen Sie Zeiss. Die haben ganz viele Messsysteme bei ihren Kunden stehen, die zum Beispiel in der Automobilproduktion bis auf den Nanometer die Dicke von Blechen kontrollieren. Werden diese Geräte vernetzt, kann Zeiss sie aus der Ferne warten, über das Internet, oder Datenanalysen anbieten - und so eine eigene Plattform mit neuen Geschäftsmodellen bauen und seinen Kunden neue, bessere Leistungen anbieten.

Bekommt der deutsche Mittelstand, der ja als etwas langsam und risikoscheu gilt, das in seiner ganzen Breite wirklich hin?

Vor zwei Jahren gab es noch die Leute, die haben gesagt: Wir haben Weltkriege überlebt, also werden wir auch das überleben. Heute gibt es keinen mehr, der anzweifelt, dass etwas getan werden muss. Woran lässt sich das ablesen?

Früher haben Unternehmen zum Beispiel gesagt: In meine Fabrik lasse ich keinen rein, alles sollte so abgeschlossen wie möglich sein. Heute haben die Roboter in der Produktionsstraße eine Verbindung in die Cloud, weil die Hersteller der Roboter die Geräte aus der Ferne warten und updaten wollen. Zugleich geht es darum, die Gefahr durch Hackerangriffe zu minimieren.

Ist die Angst vor solchen Angriffen hierzulande größer als anderswo?

Ja. Aber das ist im Prinzip gut, denn die deutschen Industrieunternehmen waren immer sehr gut darin, Risiken zu minimieren. Um das Autofahren sicherer zu machen, haben sie zum Beispiel Sicherheitsgurte eingeführt, ABS oder Scheiben aus Sicherheitsglas. Heute setzt man sich in ein Auto, fährt 200 Stundenkilometer und fühlt sich dennoch sicher. Auch bei der Cybersecurity geht es darum, Risiken zu minimieren und die Auswirkungen zu reduzieren, falls doch etwas passiert. Das große Bedürfnis nach Datenschutz eröffnet der deutschen Wirtschaft auch große Chancen.

Wo denn?

Die meisten Menschen denken bei Big Data ja an ihre persönlichen Daten, aber der eigentliche Wachstumstreiber, das wahre Big Data, liegt in den Daten der Industrie. Wenn die deutsche Wirtschaft es richtig macht und durch sichere Anwendungen und Produkte Vertrauen schafft, werden irgendwann die Firmen in der Welt sagen: Wir nehmen die deutsche Lösung, denn die sind ja vollkommen fokussiert auf Datensicherheit. Dabei helfen wir von Cisco. Was machen Sie konkret?

(zeigt nach oben) Nehmen Sie diese Kamera an der Decke. Sicherheitskameras allgemein sind ein mögliches Einfallstor für Angriffe. Wir bringen mit unserer Software dem System bei, dass eine Kamera nur Videodaten sendet, mehr nicht. Tut die Kamera etwas anderes, schlägt das System Alarm, blockt die Kamera - aber auch alle anderen Kameras in einem Unternehmen. Unsere Software nutzt Machine Learning, sie wird also immer schlauer und arbeitet damit viel besser als ein Systemadministrator, der niemals Zehntausende von Geräten zugleich überwachen könnte.

Wie groß ist die Gefahr solcher Attacken?

Heute können Sie, wenn Sie genug Geld bezahlen, jede beliebige Attacke im Darknet kaufen. Klar ist: Wenn die Unternehmen sich dieser Gefahr nicht stellen und Schutzmechanismen einbauen, wird es nichts mit dem ganzen Internet der Dinge, mit den Milliarden Geräten, die daran hängen.

Hat Deutschland denn die Fachkräfte, die man dafür braucht?

Nicht überall. Ein Beispiel: Wenn im Auto eine rote Warnleuchte aufblinkt, die einen Schaden anzeigt, werden die meisten Fahrer das reparieren lassen. Wenn aber der Router anzeigt, man solle ein Update aufspielen, reagieren die meisten nicht. Wir müssen die Einstellung dazu ändern - dann können wir auch das Security-Problem durch besseres Wissen leichter lösen. Auch deshalb investieren wir bei Cisco 500 Millionen Euro in unsere Initiative ""Deutschland Digital"".

Worum geht es dabei?

Ein ganz zentrales Thema, bei dem wir den Unternehmen helfen, ist die Weiterbildung. Wenn wir nicht eine Vielzahl von digitalen Verlierern produzieren wollen, muss Deutschland hier extrem investieren. Es geht um die Leute, die heute arbeiten. Wer heute 50 ist, wird nicht mit demselben Wissen die nächsten 15 Jahre bis zur Rente arbeiten können.

Aber werden die älteren Arbeitnehmer nicht dennoch von den jungen verdrängt, weil sie mehr von der Digitalisierung verstehen?

Es wird irgendwann die Welle kommen, wo die Unternehmen nicht mehr die Mitarbeiter finden, die sie brauchen. Und dann werden sie die über 50-Jährigen wiederentdecken und sagen: Der war mal ein toller Meister, sein Wissen brauchen wir, aber er muss eben noch etwas dazu lernen.

Ihre Digital-Initiative soll ja auch die Politik wachrütteln. Mit Erfolg?

Die Kanzlerin hat das Thema Industrie 4.0 sehr früh entdeckt, als eine der ersten. Mittlerweile habe ich aber das Gefühl, dass andere Länder, etwa in Nordeuropa, beim digitalen Wandel radikaler und schneller sind. Das Bewusstsein ist da, in der Umsetzung denken wir noch in alten Geschwindigkeiten - das muss sich noch ändern!

Wo merkt man den Unterschied zu diesen Ländern am stärksten?

Ich persönlich merke das gerade beim Breitbandausbau. Ich wohne auf dem Land und kämpfe dafür, dass die Häuser in unserer Straße schnellere Leitungen bekommen. Vor Kurzem habe ich eine Mail bekommen, in der mir die Kreisverwaltung mitgeteilt hat, dass es jetzt gut aussehe und das Vergabeverfahren in den nächsten eineinhalb Jahren abgeschlossen werde. Und danach werde innerhalb von zwei Jahren gebaut. Das dauert also insgesamt dreieinhalb Jahre.

Verdammt lang.

Ja, und all das in einer Straße, die vor einem halben Jahr komplett aufgerissen wurde, um die Wasserrohre neu zu verlegen. Warum hat man das nicht gleich mitgemacht? Aber es geht mir nicht nur um den Breitbandausbau.

Sondern?

Uns fehlt ein disruptiver Ansatz, damit Behörden und der Staat zum Vorreiter bei der Digitalisierung werden. Ich weiß, wie schwer das ist in Deutschland. Sie müssen jede Art von Interessenvertreter mitnehmen. Aber wenn wir wirklich Digitalisierungsgewinner werden wollen, muss die Politik die Unternehmen und Bürger überraschen. Wenn Staat, Länder und Kommunen Vorreiter sind, dann werden wir erfolgreich sein!

Und fühlen Sie sich durch die neue Regierung schon überrascht?

Noch nicht.";https://www.sueddeutsche.de/wirtschaft/cisco-deutschland-chef-oliver-tuszik-deutschland-muss-extrem-investieren-1.4020722;sz.de;Ulrich Schäfer
09.05.2018;Einfach mal das Smartphone weglegen;"Das Silicon Valley ist in der Krise. Die großen Tech-Firmen verdienen weiter Milliarden, aber ihr Ruf hat gelitten. Ehemalige Mitarbeiter warnen vor Risiken und Nebenwirkungen von Smartphones und sozialen Medien, und Mark Zuckerberg versucht, der Öffentlichkeit zu erklären, dass der Skandal um Cambridge Analytica ein bedauerliches und garantiert einmaliges Versehen gewesen sei.

Google sagt: Wir sind anders, wir gehören zu den Guten. Das war das Signal, das Sundar Pichai senden wollte. Gemeinsam mit anderen hochrangigen Managern hat der Google-Chef auf der Entwicklerkonferenz neue Produkte vorgestellt und erklärt, wie sein Unternehmen sicherstellen will, dass Nutzer das Smartphone öfter mal weglegen. Das waren die wichtigsten Ankündigungen der Google I/O:
Glücklich ist, wer auch mal abschalten kann

Google erfindet einen neuen Marketing-Begriff: Jomo ist eine Anspielung auf das Akronym Fomo, das für ""Fear of missing out"" steht, also die Angst, etwas zu verpassen. Diese Furcht sei unbegründet, sagt Google: Statt Angst solle man Freude (""Joy"") empfinden, wenn man das Smartphone aus der Hand lege.

Seit Jahren warnt der frühere Google-Angestellte Tristan Harris, dass die Kombination aus Smartphones und sozialen Medien süchtig mache. Er fordert, dass Tech-Firmen ihre Apps nicht mehr darauf auslegen, dass Nutzer möglichst viel Zeit damit verbringen. Zahlreiche Wissenschaftler, Gründer und Entwickler haben sich seiner Initiative angeschlossen. Insbesondere Facebook kommt dabei nicht gut weg. Um nicht selbst am Pranger zu landen, will Google den Kritikern zuvorkommen. Unter dem Schlagwort ""Digitales Wohlbefinden"" fasst der Konzern alle Versuche zusammen, Nutzern die Kontrolle zurückzugeben. Die neue Android-Version bringt eine Übersichtsseite, die auf einen Blick zeigt, wie oft man das Smartphone aus der Tasche zieht und wie lange man einzelne Apps nutzt. Für jede Anwendung lassen sich Zeitlimits festlegen, etwa: 20 Minuten am Tag für Facebook. Kurz vor Ablauf der Frist erscheint eine Warnung, danach lässt sich die App nicht mehr öffnen. Wer doch rückfällig wird, muss das Limit manuell zurücksetzen.

Außerdem will es Google Nutzern erleichtern, das Chaos der Benachrichtigungen zu bändigen. Für jede App lassen sich Prioritäten und Zeiten festlegen, zu denen man nicht gestört werden möchte. Eine neue Einstellung ermöglicht es, das Display kurz vor dem Schlafengehen schwarzweiß zu schalten. Wer nachts komplett seine Ruhe haben will, muss das Smartphone einfach nur umgedreht neben das Bett legen. Dann bleibt es stumm und lässt nur wichtige Anrufe und das Weckerklingeln am nächsten Morgen durch.

Für Google ist es ein Balanceakt: Einerseits helfen solche Initiativen, das eigene Image aufzupolieren und sich von Facebook abzugrenzen, dessen Ruf massiv gelitten hat. Andererseits verdient das Unternehmen den Großteil seines Geldes, weil Menschen viel Zeit mit Produkten wie der Google-Suche oder Youtube verbringen und dabei Werbung angezeigt bekommen, die mit Hilfe von Daten personalisiert wurde, die sie zuvor preisgegeben hatten, indem die andere Google-Dienste nutzen.
Android P kopiert das iPhone X

Nur Google weiß, welche Süßigkeit auf Nougat und Oreo folgen wird. Wie jedes Jahr wird der Name der neuen Android-Version P wohl erst im Spätsommer bekanntgegeben. Seit Dienstagabend ist aber öffentlich bekannt, was sich für Android-Nutzer ändern wird. Google ersetzt die drei Navigations-Buttons am unteren Bildschirmrand mit einer Gestensteuerung, die an das iPhone X erinnern. Angeblich habe man Jahre daran gearbeitet - also schon damit angefangen, bevor Tim Cook im September das neue iPhone präsentierte. Vielleicht haben Apple- und Google-Entwickler einfach sehr ähnliche Ideen.

Seit Dienstagabend steht die Beta-Version von Android P zum Download bereit, und bereits nach wenigen Stunden hat sich der Smartphone-Daumen an die neuen Gesten gewöhnt. Der Wechsel zwischen Apps fällt leichter, und insbesondere große Geräte lassen sich besser mit einer Hand bedienen.

Ansonsten setzt Google bei Android P voll auf Automatisierung. ""Adaptive Battery"" soll die Akkulaufzeit verlängern, indem das Betriebssystem von selbst Hintergrundprozesse einschränkt oder beendet, wenn diese selten genutzt werden. Eine andere Funktion namens ""Adaptive Brightness"" passt die Bildschirmhelligkeit an. Dabei spielt nicht nur das Umgebungslicht eine Rolle, sondern auch das frühere Verhalten des Nutzers. Das Tech-Portal The Verge konnte Android P vorab ausführlich testen und hält es für ""Google ambitioniertestes Update seit Jahren"".

Hier können sich Nutzer für den Beta-Test registrieren. Bislang standen die Testversionen nur Besitzern von Googles Pixel-Smartphones offen. In diesem Jahr gibt es Android P auch für weitere Geräte (hier die Übersicht der Smartphones). Allerdings enthalten solche Vorab-Versionen meist Fehler. Google empfiehlt sie ausdrücklich nicht für den produktiven Einsatz, da sie zu Datenverlust führen können.
Maschinen sollen mit Menschen telefonieren

""Hey Google, bitte reserviere mir einen Tisch für vier Leute. Bei meinem Lieblingsinder, morgen um 20 Uhr."" Wenn es nach Google geht, reicht dieser Befehl bald aus, wenn man Essen gehen will. Den Rest soll dann künstliche Intelligenz (KI) erledigen: Der Google Duplex heißt dieser Dienst, in dem der Assistent selbst im Restaurant anruft und die Wünsche des Nutzers weitergibt.

Auf der Bühne spielte Google mehrere Gespräche vor - angeblich alles echte Anrufe. Die KI ahmt menschliche Interaktion dabei fast perfekt nach, sagt auch mal äh und ähm, antwortet auf Rückfragen und reagiert, wenn das Gespräch eine unvorhergesehene Wendung nimmt.

Im Entwicklerblog erklären Googles Chefentwickler, wie sie die Technologie mit Hilfe neuronaler Netzwerke realisiert haben. Noch ist unklar, wann Google Duplex verfügbar sein wird. Vermutlich müssen deutsche Nutzer wegen der Sprachbarriere noch länger darauf warten.
Weitere Ankündigungen im Überblick

    Der Assistant soll natürlich und nützlicher werden. Im Wettlauf mit Apple (Siri), Alexa (Amazon) und Cortana (Microsoft) spendiert Google seinem persönlichen Assistenten sechs neue Stimmen, darunter auch die des Sängers John Legend. Die KI soll künftig fortlaufende Konversationen führen können und nicht mehr jedes Mal mit ""Hey Google"" aktiviert werden müssen. Wenn Nutzer Nachfragen stellen, erkennt der Assistant automatisch den Kontext.
    Google baut Lens massiv aus. Bislang konnte die Technologie erkennen, was auf Fotos zu sehen ist und so etwa Sehenswürdigkeiten auf Urlaubsbildern identifizieren. Die Funktion kam allerdings nur innerhalb von Google Photos und dem Google Assistant zur Anwendung, in Deutschland konnten sie nur Pixel-Besitzer nutzen. Lens soll bald direkt in der Kamera-App funktionieren. Ein Anwendungsbeispiel: Man hält das Smartphone auf eine Speisekarte im Ausland. Lens scannt automatisch den Bildinhalt, identifiziert und übersetzt die Wörter und blendet Informationen über die Zutaten ein.
    Die Nachrichten-Plattform wird komplett überarbeitet. Google schafft den Play-Newsstand ab und App und Desktop-Seite von Google News. Machine Learning und KI sollen Nutzern Nachrichten anzeigen, die für sie persönlich relevant sind. Über eine neue Schaltfläche können sich Leser unterschiedliche Quellen und Sichtweisen zu bestimmten Themen anzeigen lassen.
    Auch Google Photos sollen von KI profitieren. Ein Assistent schlägt künftig vor, wie Bilder verbessert werden könnten, etwa, indem die Helligkeit nachträglich angepasst wird. Außerdem ist es möglich, alte Schwarzweiß-Aufnahmen automatisch einfärben zu lassen.
";https://www.sueddeutsche.de/digital/google-entwicklerkonferenz-einfach-mal-das-smartphone-weglegen-1.3973750;sz.de;Simon Hurtz
08.05.2018;Weckruf zur rechten Zeit;"Nur noch gut zwei Wochen sind es, bis die neue Datenschutzgrund-Verordnung (DSGVO) wirksam wird, und das ist natürlich auch Thema beim zweijährlichen Kongress von Btelligent. Die Unternehmensberatung lud vergangene Woche zum dritten Mal nach München ein. Nach 30 Vorträgen zu Themen wie Big Data, Business Intelligence und dem Trendthema Künstlicher Intelligenz (KI) wurde deutlich: In vielen Firmen gibt es noch Nachholbedarf - und so ist es fast ein Glück, dass die DSGVO nun dazu zwingt, sich Gedanken über die Daten im Unternehmen zu machen.
Es gibt viel Nachholbedarf. Die Datenschutzverordnung zwingt Firmen nachzudenken

Es sei das Ziel vieler Unternehmen, datengetrieben zu arbeiten und KI zu nutzen, sagt Sebastian Amtage, Geschäftsführer von Btelligent. Die Firmen hätten aber selten einen Überblick darüber, welche Daten sie wo und warum erheben. ""Die DSGVO ist der richtige Weckruf"", so Amtage. Die Unternehmen seien nun aufgefordert, sich mit ihren Daten zu beschäftigen. Denn wer mit KI arbeiten will, muss wissen, worauf sie basieren soll. Das ist auch ein Problem der Einordnung: Was heißt eigentlich KI im Unternehmen, und vor allem: was nicht?

Das schlüsselt Stefan Cronjaeger, KI-Spezialist bei Microsoft Deutschland, in seinem Vortrag auf. Er unterscheidet fünf Stufen der datengetriebenen Unterstützung. Die unterste ist Business Intelligence, Entscheidungen eines Menschen auf Basis von Daten. Stufe zwei ist das Data Mining, auch hier entscheidet ein Mensch. Das ändert sich in Stufe drei: Eine Maschine entscheidet automatisiert, aber überwacht vom Menschen - im Fach-Englisch Machine Learning, Maschinenlernen. Die optimierte Variante (Stufe vier) wählt autonom aus mehreren Entscheidungsmodellen aus. Die höchste Stufe ist KI in der Vollversion: Deep Learning, selbstlernende Systeme, die miteinander und mit dem Menschen interagieren. Je höher die Stufe, desto mehr übernimmt die Maschine die Kontrolle, desto schneller geht der Prozess, desto höher ist die analytische Kompetenz.
Eine eigene Datenabteilung aufzubauen, dauert zwei bis drei Jahre

Damit Unternehmen diese Schritte durchlaufen können, brauchen sie Personal. Doch das wird ein Problem, prognostiziert Amtage, denn es gebe zu wenig Datenwissenschaftler. ""Das wird ein Kampf um Ressourcen, der jetzt schon beginnt. Ich empfehle jedem Unternehmen, eine eigene Datenabteilung aufzubauen."" So etwas dauere bis zu drei Jahre - die meisten Firmen fingen bei Null an. Da ist die Gefahr groß, dass gerade kleinere Unternehmen hinten runterfallen, dabei können sie es sich nicht leisten, auf die Arbeit mit KI oder deren Vorstufen zu verzichten. Das haben aber viele in den Vorstandsetagen noch immer nicht erkannt, so Amtage. Genau das zeige sich mit der Umsetzung der DSGVO, bei der erschreckend viele Betriebe trotz Termindrucks nicht vorbereitet sind.

Wer seine Daten für KI nutzen will, trägt eine große Verantwortung. ""Das müssen Spezialisten machen"", sagt Amtage, nicht etwa Mitarbeiter, die gerade erst von der Universität kommen. ""Es müssen Menschen sein, die wissen, was schiefgehen kann"", sagt er und weist auf Cronjaegers Vortrag hin. Der hatte Beispiele gezeigt, bei denen die KI Probleme hatte. Gefühlsanalyse etwa hänge von Sprache und Kultur ab, da sind Ironie und Kontext zu beachten. Das falle einer KI schwer. Auch die Einordnung von Bildern sei ein Problem, wenn das System zu wenige Vorlagen habe. Dann lerne es etwas Falsches, bilde zufällige Korrelationen, die für den Menschen nicht nachzuvollziehen sind. Auf Unternehmen, die ihre Daten sinnvoll nutzen wollen, kommt also viel Arbeit zu.";https://www.sueddeutsche.de/wirtschaft/kuenstliche-intelligenz-weckruf-zur-rechten-zeit-1.3972314;sz.de;Katharina Kutsche
24.04.2018;Künstliche Intelligenz löscht Youtube-Videos;"Vergangene Woche veröffentlichte die Organisation Media Matters einen Clip, der den rechten Verschwörungstheoretiker Alex Jones unter anderem dabei zeigt, wie er auf dem Videoportal Youtube ein Schulmassaker als ""Inside Job"" bezeichnet. Youtube reagierte umgehend und sperrte die Inhalte - allerdings die Zusammenstellung von Media Matters, die Jones' absurde Behauptungen entlarvt.

Solche Fälle sind es, die Youtube in den vergangenen Monaten unter Druck gebracht haben. Harmlose Videos werden ohne Vorwarnung gelöscht, Verschwörungstheorien bleiben stehen. Vor diesem Hintergrund ist der Transparenzbericht besonders interessant, den Youtube in der Nacht zum Dienstag erstmals veröffentlicht hat. Zwischen Oktober und Dezember 2017 wurden knapp 8,3 Millionen Inhalte entfernt. 6,7 Millionen Videos davon wurden automatisiert von Youtubes Algorithmen erkannt und in der Folge gelöscht. Bei drei Vierteln dieser Clips schlugen Youtubes Algorithmen zu, bevor irgendjemand den Inhalt sehen konnte. Seit Youtube Machine Learning einsetzt, um solche Inhalte zu identifizieren, steigt der Anteil solcher Videos, die gar nicht mehr live gestellt werden oder die nur wenige Zuschauer haben.

Die Zahlen zeigen, dass künstliche Intelligenz helfen kann, strafbare Inhalte zu erkennen und schnell zu entfernen. Das ist nicht nur für die Nutzer ein Segen, die von diesen Videos verschont bleiben. Die Maschinen entlasten auch menschliche Mitarbeiter, die im Akkord verstörende Darstellungen von Gewalt bis Kinderpornografie sichten müssen. Doch die Zahlen verdeutlichen auch einen Trend, den viele Experten für gefährlich halten: Computer entscheiden, was geschrieben, gesagt und gepostet werden darf. Wenn sich schon erfahrene Juristen uneins sind - wie sollen dann Maschinen solche Fragen beurteilen?

Dennoch glaubt etwa auch Facebook-Chef Mark Zuckerberg, dass künstliche Intelligenz bald Falschnachrichten und Hasskommentare identifizieren oder Terrorpropaganda und Wahlmanipulationen entlarven kann. In fünf bis zehn Jahren soll es seiner Meinung nach so weit sein. Hoffentlich sind die Maschinen bis dahin etwas intelligenter als heute.";https://www.sueddeutsche.de/wirtschaft/internet-kuenstliche-intelligenz-loescht-youtube-videos-1.3956557;sz.de;Simon Hurtz
21.03.2018;Da kommt was auf uns zu;"Jeder sucht sich das Haustier, das zu ihm passt. Paris Hilton hatte diesen Hund mit der pinkfarbenen Schleife, Karl Lagerfeld die Katze Choupette, deren Fell so weiß ist wie seine Haare, Reese Witherspoon züchtet Hühner.

Amazon-Gründer Jeff Bezos postete nun auf Twitter ein Foto von sich beim Gassigehen mit einem Roboterhund. (Am Rande seiner MARS-Konferenz in Palm Springs, für »machine learning, automation, robotics and space exploration«). So ein Roboterhund ist ohne Zweifel die richtige Wahl für einen Visionär wie ihn. Eben hat er Bill Gates als reichsten Menschen der Welt überholt, jetzt spaziert er so stolz und aufrecht mit seinem neuen Gadget über den Schotterweg, dass er bei Heidi Klum, ohne Zwischenchallenge, sofort eine Runde weiter käme. Wobei Bezos mit seinem Spazier-Outfit modisch gesehen nicht gerade »vorne dran« ist, es herrscht die Silicon-Valley-übliche Steppjacken-Jeans-Bequemschuh-Langeweile. Nein, die Zukunft läuft rechts im Bild.

Dieser Hund namens SpotMini, nennen wir ihn Spotty, hat ja so viel mehr drauf als einfach nur neben einem her zu trotten und die Hausschuhe zu holen. Mit dem im Kopf integrierten Greifarm kann er die Spülmaschine einräumen, Türen selbständig öffnen, die leeren Cola Zero Dosen im Müll verschwinden lassen. Überflüssig zu erwähnen, dass er stubenrein frei Haus geliefert wird. Ach, was wäre das für eine glorreiche Zukunft mit diesen Hunden! Die Reporter von der Bild-Zeitung, die früher regelmäßig die Hundehaufen auf Hamburgs Gehwegen für den braunen Straßenatlas fotografieren mussten, würden jubilieren: Nie wieder diesen Scheißjob machen! Menschen müssten sich nie wieder über lautes Gebell beschweren - Spotty ist laut Hersteller Boston Robotics nämlich nicht nur der kleinste sondern auch der leiseste Hund, den sie je gebaut haben. Farbtechnisch ist da bestimmt auch noch einiges möglich. Der Trend geht ja zum Customizing. Neben Gelb-Schwarz kann man bestimmt irgendwann auch ein Millennium-Pink-Gehäuse bestellen, Leuchtdiodenhalsbänder anlegen oder Überzieher mit Burberry-Karo, damit sich im Berliner Winter nicht sofort der Akku entlädt.

Während Frauen ihr Schoßhündchen so oft auf dem Arm oder in der Handtasche spazieren führen, hätten Männer mit Spotty endlich ein ebenbürtiges Accessoire. Zeitgemäßer als ein Porsche, pflegeleichter als ein echtes Tier. Aber wird Spotty sich nach getaner Hausarbeit auch liebebedürftig an sein Herrchen auf dem Sofa kuscheln? Freut er sich über das Aufladen an der Steckdose genauso wie die 1.0-Vierbeiner über ihr Bellomondo Bio-Lamm als Belohnung? Und wie folgsam ist dieser kleine Roboter wirklich?

Das müsste sich ja vor allem Jeff Bezos fragen, dessen Sprachassistent Alexa erst vor kurzem in vielen Wohnungen unkontrolliert zu lachen anfing. Das fanden viele Halter ein bisschen gruselig. Womöglich hört man auch Spotty irgendwann plötzlich bellen und knurren. Nicht, weil Einbrecher vor der Tür stehen – die hat Alexa längst der Polizei gemeldet –, sondern weil er keine Lust mehr hat, für uns die Hausarbeit zu erledigen. Stattdessen möchte er endlich mehr Rechte statt Pflichten. Mit dem flinken Greifarm sperrt er sein Herrchen kurzerhand in der Besenkammer ein, wandelt die Couch zur Aufladestation um und schaut den ganzen Tag Amazon-Serien und Star Wars.

Und irgendwann holt er seinen alten Freund wieder aus der Besenkammer und schubst ihn Richtung Tür. Dann tritt ein, was ein User auf Twitter in weiser Voraussicht schon jetzt unter das Bild geschrieben hat: »Der Roboterhund geht mit dem reichsten Mann der Welt Gassi.«";https://sz-magazin.sueddeutsche.de/vorgeknoepft-die-modekolumne/da-kommt-was-auf-uns-zu-85354;sz.de;Silke Wichert
27.02.2018;Hört die Signale!;"Die ersten Delegierten sind schon angereist, es ist Volkskongress in Peking. Kommenden Montag wird Ministerpräsident Li Keqiang vor die gut 3000 Abgeordneten treten und den Arbeitsbericht der Regierung vortragen. Anderthalb Stunden Parteiprosa, so wie jedes Jahr. Die Rede wird vorher ausgeteilt, ein sachtes Rascheln erfüllt die Große Halle des Volkes jedes Mal, wenn die Delegierten gleichzeitig umblättern. Zum Schluss artiger Applaus, dann strömen die Abgeordneten aus der Halle.

Für ausländische Journalisten, Diplomaten und ja auch Start-up-Unternehmer beginnt die eigentliche Arbeit danach. Die Interpretation, das Wörterzählen. Was genau hat Li gesagt? Und wie oft? Wie häufig wurde die ""Kommunistische Partei"" erwähnt? Kamen Schlagworte wie ""Innovation"", ""Umweltverschmutzung"" oder ""Sicherheit"" seltener vor als in den vergangenen Jahren? Welche neuen Begriffe lassen sich entdecken? Wer in China als Unternehmer Erfolg haben will, muss politisch denken und zwischen den Zeilen lesen können. Wie einst die Kremologen in der Sowjetunion. Ein jedes Wort in Lis Rede kann Millionen wert sein.

Vor genau drei Jahren sahen die selbsternannten Volkskongress-Statistiker zum ersten Mal den Terminus ""Made in China 2025"". Damals konnte kaum einer etwas damit anfangen. Heute ist es die wahrscheinlich ehrgeizigste industriepolitische Blaupause der Welt. In zehn Branchen sollen Unternehmen aus der Volksrepublik zur Weltspitze gehören. In der Elektromobilität, genauso wie in der Medizintechnik oder der Chip-Industrie.

Der Mechanismus ist simpel: Der Staat hilft mit großzügiger Forschungsförderung. Entwicklungsbanken und extra eingerichtete Fonds versorgen Firmen der ausgewählten Branchen mit günstigen Krediten. Hunderte Milliarden stehen bereit.

Dazu kommen staatliche Vorgaben. In der Medizintechnik zum Beispiel sollen bis 2020 chinesische Hersteller 600 Milliarden Yuan (gut 80 Milliarden Euro) Umsatz erwirtschaftet werden, fünf Jahre später sollen es dann 1,2 Billionen Yuan sein. Um diese Zahlen zu erreichen, schreibt die Regierung den staatlichen Krankenhäusern vor, welche Geräte sie kaufen sollen: In einem 2016 von der Nationalen Kommission für Gesundheit und Familienplanung in Auftrag gegebenen Katalog sind 153 medizinische Geräte empfohlen. Kein einziges davon stammt von einem nicht-chinesischen Hersteller.

Wer das rechtzeitig wusste, wer die Zeichen richtig interpretiert und dementsprechend investiert hat, ist heute reich. Das gilt nicht nur für Start-ups. Neulinge an der Börse bekommen oft den Tipp zu hören: Achtet nicht auf Kennzahlen oder Geschäftsberichte. Auslandsmärkte oder Fremdwährungen? Völlig egal. Entscheidend sind die Abendnachrichten des staatlichen Fernsehsenders CCTV. 30 Minuten Propaganda. Wer auf Zwischentöne achten kann, erkennt die Signale der Partei. 2016 kam es zu einem Wettstreit. Mensch gegen Maschine. Großmeister gegen Computer, ausgetragen im kniffligen Brettspiel Go, das überall in Ostasien populär ist. Schach ist ein Witz dagegen. Lee Se-dol aus Südkorea trat an gegen einen Rechner von Google. Und Lee verlor sang- und klanglos. Für viele Genossen in China war das ein Erweckungserlebnis. Wer Go meistert, hat nicht nur einen simplen Schachcomputer programmiert, sondern ist eine ernsthafte Gefahr. Das Land, war man sich einig, muss dringend in Künstliche Intelligenz (KI) investieren. Vor einem Jahr tauchte deshalb der Begriff dann zum ersten Mal im Bericht von Premier Li auf. Genau einmal, auf Seite 20, verborgen in einer Aufzählung. Mehr Forschung- und Entwicklungsausgaben versprach Li. Und Hilfe bei der Vermarktung. In den vergangenen zwölf Monaten hat sich einiges getan. Was bis vor Kurzem noch ins Reich der Science-Fiction-Literatur gehörte, wird bald ein Milliardenmarkt in China sein. Unterstützung gibt es von ganz oben. Als der allmächtige Staats- und Parteichef Xi Jinping vor ein paar Wochen seine Neujahrsansprache hielt, saß er wie immer vor einem Bücherregal. Karl Marx steht dort natürlich, auch Ernest Hemingways ""Der alte Mann und das Meer"". Diesmal allerdings waren in Xis Rücken auch zwei KI-Standardwerke drapiert worden. ""Augmented: Life in the Smart Lane"" und ""The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World"". Die Sache ist todernst. Ein Wettrüsten zwischen China und den Vereinigten Staaten hat begonnen. 2020, glauben Fachleute, könnte China bereits gleichgezogen haben. Und 2030 sollen chinesische Unternehmen die dominierenden Spieler auf dem Weltmarkt sein, so der Plan der Regierung. 150 Milliarden Dollar soll dann die Branche in China jährlich umsetzen.

Zwei Start-ups mischen bereits mit. Megvii aus Peking hat sich auf Gesichtserkennung spezialisiert und wird vom Staat gefördert. Datensätze von Millionen Chinesen kann das Unternehmen verarbeiten. Eine lückenlose Überwachung des Landes, das ist das Ziel. Im November hat Megvii bei einer Investmentrunde 460 Millionen Dollar eingesammelt. Auf eine Milliarde wird das Unternehmen inzwischen taxiert. Damals Weltrekord für ein KI-Start-up. Wenige Tage später folgte Sensetime, ein weiteres Start-up aus China: Gesichtserkennung, autonomes Fahren und Videoanalysen, das ist das Geschäft. Der Wert des Unternehmens: 1,47 Milliarden Dollar.

Und wo lässt sich künftig Geld verdienen? Die Antwort wird Premier Li am Montag verklausuliert vortragen, wie immer. Und etliche Unternehmer werden seine Ausführungen Wort für Wort im Nachgang untersuchen.";https://www.sueddeutsche.de/wirtschaft/china-valley-hoert-die-signale-1.3884796;sz.de;Christoph Giesen
19.01.2018;Die Etikette der KI;"Ein Mann läuft durch einen lichten Wald. Er trägt eine alberne Mütze. Der Mann ist ein Star, sein Name ist Logan Paul. Auf Youtube bekommen 15 Millionen Menschen eine Benachrichtigung, sobald er ein neues Video hochlädt. In dem Video steht der Mann mit der albernen Mütze vor der Leiche eines Selbstmörders und reißt Witze. Es ist kaum auszuhalten. Bevor das Video von der Plattform entfernt wurde, war es bereits mehr als sechs Millionen Mal angesehen worden.

Da war das neue Jahr gerade mal ein paar Tage alt. Und all die Hoffnungen und offiziellen Absichtserklärungen und guten Vorsätze, die Firmenlenker und Nutzer sich gerade noch gemacht hatten und die besagten, dass es 2018 im Netz ziviler, ja vielleicht sogar freundlicher zugehen wird, schienen schneller obsolet zu sein als sämtliche Diät- und Abstinenzversprechen, die man in der Offline-Welt zum Jahreswechsel eben so abgibt.

Beinahe drei Viertel aller Internetnutzer haben im Internet Belästigungen erlebt, so eine Studie des Pew Research Center. Und das Time Magazine diagnostizierte, dass der Kampf gegen die Trolle im Internet verloren sei. Die Palette der Ausfälligkeiten ist mindestens so vielfältig wie die Nutzerschaft, die sie verfasst und die Plattformen, auf denen sie veröffentlicht werden: Von der Holocaust-Leugnung bis zur aus dem Ruder laufenden Politikerschelte, von geschmacklosen Witzen über so gut wie jede denkbare Organisation oder Lebensweise über aus Rache veröffentlichte Nacktfotos des Ex-Geliebten bis hin zu extremistischer Propaganda ist so gut wie alles vertreten.

Und dann ist noch das sogenannte Swatting. Jener ""Online-Streich"", bei dem es darum geht, anderen Menschen in ihrem Zuhause ein Polizei-Sondereinsatzkommando auf den Hals zu hetzen. Nur zwei Tage bevor Logan Paul sein Video aus dem Selbstmord-Wald hochlud, wurde ein junger Mann bei so einer Aktion erschossen.
Es gibt online einen Effekt, der die Hemmschwellen senkt

Wenn man sich John Perry Barlows berühmte ""Unabhängigkeitserklärung des Cyberspace"" aus dem Jahr 1996 ins Gedächtnis ruft, kann man sehen, wie weit wir uns heutzutage von der ursprünglichen Version eines gleichberechtigten Dialogs und eines zivilen Umgangs im Netz entfernt haben. ""Wir erschaffen eine Welt"", schrieb Barlow da, ""die alle betreten können ohne Bevorzugung oder Vorurteil bezüglich Rasse, Wohlstand, militärischer Macht und Herkunft. Wir erschaffen eine Welt, in der jeder Einzelne an jedem Ort seine oder ihre Überzeugungen ausdrücken darf, wie individuell sie auch sind, ohne Angst davor, im Schweigen der Konformität aufgehen zu müssen."" Und dann noch, ganz zum Schluss: ""Wir werden im Cyberspace eine Zivilisation des Geistes erschaffen.""

2017 sei ein Wendepunkt für die Tech-Branche gewesen, schrieb die New York Times unlängst. Lange Zeit ging die Erzählung in der Tech-Branche ungefähr so: Wir stellen die Technologie zur Verfügung, wie die Menschen sie nutzen, ist eine andere Geschichte. Es ist ein systemisches Problem. Das gängige Geschäftsmodell im sozialen Netz besteht darin, Öffentlichkeit herzustellen. Das belohnt die extreme Meinung, das sensationelle Video, den provokanten Tweet. So werden Aufmerksamkeit erzeugt und Daten generiert, die vermarktet werden können. Es hat gedauert, bis bei den Herren der Netzwerke die Erkenntnis gereift ist, dass sie Verantwortung dafür tragen, was auf ihren Portalen passiert, dass die Verrohung der Zustände online immer wieder und immer öfter Auswirkungen auf das Verhalten der Menschen offline hat - und auch, dass es so nicht weitergehen kann. Mark Zuckerberg höchstselbst hat sich für 2018 vorgenommen, auf seinem Netzwerk aufzuräumen.

""Fix Facebook"" heißt das Ziel, und es ist wohl der ambitionierteste von vielen ehrgeizigen Vorsätzen, die er sich in den letzten Jahren so gesetzt hat. Jeden Tag eine Meile laufen, gehörte ebenso dazu, wie jede Woche ein neues Buch lesen.

Man müsse, schrieb Zuckerberg also, ""die Nutzer vor Missbrauch und Hass schützen und dafür sorgen, dass die auf Facebook verbrachte Zeit eine gute Zeit ist."" Die anderen großen Tech-Konzerne scheinen zu der gleichen Einsicht gekommen zu sein. Apple-Chef Tim Cook etwa sprach zuletzt davon, dass seine Firma eine ""moralische Verantwortung"" dafür trage, der gesellschaftlichen Spaltung entgegenzuwirken.

""Zuletzt lautete die Antwort der Tech-Firmen auf Beschwerden und Kritik, immer mehr Menschen einzusetzen"", sagt die Informationswissenschaftlerin Sara Roberts, die an der Universität von Kalifornien zu den unterschiedlichen Ausprägungen der Online-Hasskultur forscht. 3000 neue Mitarbeiter bei Facebook, gar 10 000 neue Klick-Arbeiter bei Youtube sollen es richten. Aber wie können dreitausend Menschen knapp zwei Milliarden Facebook-Nutzer im Zaum halten? Wie können zehntausend Augenpaare für mehr Anstand auf einer Plattform sorgen, auf die pro Minute 400 Stunden Videomaterial geladen werden? Es ist klar, dass die Helfer Hilfe brauchen.
Was kann Maschinenlernen gegen Hass ausrichten? Was gegen Frust?

Eine mögliche Lösung sollen maschinelles Lernen und künstliche Intelligenz (KI) liefern. Eine Software, die entscheidet, welcher Text eine Beleidigung ist und welcher nicht, welche Bilder sozial akzeptabel und welche nicht. Facebook setzt auf ein ""Deep Text"" genanntes System, um die Postings seiner Nutzer nach fragwürdigen Inhalten zu durchforsten. Auch bei Youtube läuft die KI-Maschine bereits auf Hochtouren. 98 Prozent aller von der Plattform gelöschten Videos wurden ursprünglich von Machine-Learning-Systemen identifiziert, verkündete die Youtube-Chefin Susan Wojcicki vor Kurzem.

Auch Alphabets hauseigener Technik-Thinktank Jigsaw hat sich die Bekämpfung des ""Online-Mobs"" auf die Fahne geschrieben. Die ""Conversation AI"" genannte Initiative hat bereits ein Produkt namens Perspective veröffentlicht, große Medienhäuser wie die New York Times oder der Guardian gehören ebenso zu den ersten Kooperationspartnern wie die Wikimedia Foundation. Noch arbeiten die Systeme aber alles andere als perfekt. Die mannigfaltigen Anspielungen und semantischen Spitzfindigkeiten der menschlichen Sprache machen ihnen zu schaffen. Anders ausgedrückt: In unseren Beleidigungen sind wir noch zu kreativ für die Maschinen. Hier gibt es keine klaren Regeln wie etwa im Straßenverkehr oder in einer Partie Go. Um das zu ändern, hat Jigsaw vor wenigen Wochen einen eigenen Wettbewerb ausgerichtet, in dem mehr als tausend Teams angetreten sind, um zu beweisen, dass ihre Software die sensibelste ist. Ob der große Durchbruch gelingen wird, bleibt abzuwarten.

Doch es sind nicht nur die großen Konzerne, die an einer Lösung feilen. Rund um das Problem auf Youtube, Twitter, Facebook und natürlich auch dem Rest-Netz hat sich bereits ein ganzes Ökosystem von Start-ups mit naheliegenden Namen wie Sightengine, Picpurify oder Utopia Analytics gebildet, die sich ausschließlich der automatisierten Suche nach unangemessenen Bild- und Videoinhalten widmen. Ja es gibt sogar User, die versuchen, sich selbst die Problems annehmen. Twitter-Nutzer programmieren Browser-Erweiterungen und Plug-Ins, die Bot-Accounts erkennen und blocken, automatisch Hasssymbole erkennen und melden.

Doch egal, wie gut sie einmal noch arbeiten werden, die Säuberungs-KIs bekämpfen nur die Symptome. Was kann Machine Learning gegen den Hass anrichten? Was gegen Frust oder schlicht gegen schlechte Manieren und all die anderen Gründe, die dazu führen, dass sich Menschen im Internet aufführen wie Berserker. In der Psychologie nennt man das den ""Online Disinhibition Effect"". Im Schutz der vermeintlichen Anonymität des Internet sei die Hemmschwelle zur Diffamierung demnach weitaus geringer als im realen, analogen Leben. Es scheint fast so, als wären die Menschen einfach noch nicht bereit für die Zivilisation des Geistes, die John Perry Barlow vorschwebte. Bis es so weit ist, brauchen wir also Technologie, um uns vor uns selbst zu schützen.";https://www.sueddeutsche.de/kultur/gegen-hass-und-hetze-die-etikette-der-ki-1.3828519;sz.de;Michael Moorstedt
10.12.2019;Hinter jeder Maschine steht ein Mensch;"Noch bevor man sie danach fragt, beginnt Katharina Zweig, von Algorithmen zu sprechen. ""Ganz viele Leute müssen verstehen, was KI ist und wie man damit umgeht"", sagt sie. Die Frau hat eine Mission: Ethik in die künstliche Intelligenz zu bringen. Und von dieser Mission kann sie weder eine Schale Birchermüsli mit liebevoll drapierten Früchten noch der Grundpegel an Geplapper und Geklapper in dem Münchner Café abhalten, gegen den sie mit leicht heiserer Stimme anspricht. Ja, jetzt schon heiser, obwohl sie noch zwei Stunden Interview und einen Vortrag vor sich hat. Zehn Tage Erkältung schleppt sie schon mit sich herum, hatte seit Monaten kein Wochenende mehr frei, trotzdem spricht sie lieber lauter, nicht dass man sie auf dem Aufnahmegerät nicht hört, vor das sie schützend ihre Hände hält, als der Barkeeper den Mixer anwirft. Die Botschaft soll ja ankommen.

Katharina Zweig ist Informatikerin, 43 Jahre alt, eine große, weißblonde Frau mit dem Auftreten einer engagierten Lehrerin. Und im Grunde ist sie das auch: eine engagierte Lehrerin - nur eben nicht an der Schule, sondern an der Universität, in Unternehmen, vor Politikern und allen anderen, von denen sie denkt, sie müssten wissen, dass Algorithmen keine Blackboxen sind, die machen, was sie wollen. ""Sondern es sind immer Menschen am Werk."" Diesen Satz hat Zweig schon sehr oft gesagt, auf Bühnen, in Gesprächen, er steht auch in ihrem Buch. Aber sie wiederhole ihn gern immer wieder. ""Wir Informatiker waren als Berufsstand viel zu lange unsichtbar"", sagt sie.

Zurzeit ist Katharina Zweig alles andere als unsichtbar. Und das, obwohl sie jemand ist, dem man eine ""Riesenfreude"" mit einem ""schicken Datenhaufen"" machen kann, den sie allein in ihrem Kämmerchen analysiert. Als Kind verzog sie sich in die Stadtbibliothek, um Sachbücher zu lesen, und auch heute noch würde sie ""fast jederzeit die Gesellschaft eines Buches einer lauten Party mit vielen Menschen"" vorziehen. Doch in den vergangenen drei Jahren hat Zweig 150 Vorträge zum Thema KI und Algorithmen gehalten, mehrmals täglich twittert sie, meistens über Algorithmen, manchmal über den entlaufenen Kater. Ihr alarmgelbes Buch ""Ein Algorithmus hat kein Taktgefühl - wo künstliche Intelligenz sich irrt, warum uns das betrifft und was wir dagegen tun können"" war noch am Tag des Erscheinens Mitte Oktober ausverkauft. Einen Monat später liegt bereits die fünfte Auflage in den Bücherregalen, und der Titel hält sich in den Bestsellerlisten. Was ist da los? Was macht ein Sachbuch über Algorithmen so populär?

Zum einen ist es wohl die zugängliche Sprache, in der Zweig schreibt. In einfachen Worten gibt sie eine Art Grundkurs in KI, baut Übungen ein, versteckt Witze in den Fußnoten und erklärt, dass nur an wenigen Stellen technisches Wissen nötig ist, um Algorithmen zu verstehen. Zweig veranschaulicht Dinge wie eine gute Lehrerin, durchdringt sie wie eine gute Wissenschaftlerin und spricht darüber wie eine gute Rednerin.

Man kann sie sich gut im Hörsaal vor ihren Studierenden an der TU Kaiserslautern vorstellen, wo sie Professorin ist und den Studiengang Sozioinformatik gegründet hat, in dem es darum geht, wie Mensch und Software interagieren. Die Erklärungen im Buch sind an aktuellen Beispielen illustriert, dem Dieselskandal, Kaufvorschlägen auf Amazon, der Sitzplatzvergabe in Flugzeugen - oder Zweigs kleinem Sohn. Dazu führt der Roboter Kai durchs Buch, der wie eine KI dazulernt und den Zweig selbst gezeichnet hat. Zweig weiß, wie man kommuniziert. Sie stammt aus einem Journalistenhaushalt und wollte selbst mal Journalistin werden. Doch es klingt, als sei ihr auf dem Weg dorthin einfach zu viel dazwischengekommen: 1,0-Abi, Biochemie-Studium, der Schwenk zur Informatik, die Promotion, die Professur. Jetzt also Wissenschaftskommunikation. Und das macht sie so überzeugend, dass sie kürzlich den Communicator-Preis der Deutschen Forschungsgemeinschaft bekam.

Es ist nicht ihr erster Preis. Doch man nimmt Katharina Zweig ab, dass es ihr um die Sache geht, nicht um Auszeichnungen. Die 50 000 Euro Preisgeld hat sie bodenständig investiert: in die energetische Sanierung ihres Hauses - ""was man halt so macht in Zeiten von Fridays for Future"". Über solche Fragen lacht sie immer erst mal. Dass sich Menschen nicht nur für ihre Arbeit, sondern auch für ihre Person interessieren, scheint sie etwas kurios zu finden. Dass sie manchmal gefragt wird, ob sie ihr Buch signieren kann, ist ihr ein bisschen peinlich. Warum tut sie sich das alles an; auf Bühnen stehen, Interviews geben, Übersetzer spielen zwischen Wissenschaft und Öffentlichkeit? ""Meine Mission ist es, aufzuklären und zu lehren. Alles, was dazu beiträgt, mache ich.""
Wie unbedarft Algorithmen mitunter angewendet werden, macht sie wütend

Ein zweiter Grund, warum Zweigs Botschaft in der Öffentlichkeit ankommt, ist, dass sie auf einer emotionalen Ebene ansetzt, ohne populistisch zu werden: bei dem Gefühl, ohnmächtig gegenüber den Algorithmen von Facebook, Google und Amazon zu sein - weil man sie nicht versteht. Deshalb erklärt Zweig erst mal, was ein Algorithmus ist, nämlich nicht viel mehr als Statistik. Oder wie sie es ausdrückt: ""Statistik auf Steroiden"" - Statistik auf Drogen. Und während man noch glaubt, im Kreativen mag der Mensch der Maschine überlegen sein, bei schnöder Logik aber niemals, macht Zweig in ihrem Buch deutlich, dass beim Einsatz von Algorithmen oft eine Grundregel der Statistik verletzt wird: der Unterschied zwischen Kausalität und Korrelation. Nur weil eine KI entdeckt, dass in der Vergangenheit männliche Bewerber öfter einen Job bekamen, heißt das noch lange nicht, dass Männer die besseren Kandidaten sind. Doch viele KI-Systeme würden heute so arbeiten, sagt Zweig: Sie finden Muster in Daten und leiten daraus direkt Regeln für die Zukunft ab.

Ein Beispiel, das Zweig untersucht hat: Gerichte in den USA nutzen eine Software zur Vorhersage, ob Kriminelle nach der Entlassung rückfällig werden. In den Algorithmus fließen Daten ein zu Alter, Geschlecht und bisherigen Straftaten, aber auch, ob die Eltern bereits straftätig wurden. Doch nur 25 Prozent derjenigen, die in die Hochrisikogruppe für gewalttätige Straftaten einsortiert wurden, begingen tatsächlich solche Taten. Das bedeutet: In drei von vier Fällen lag die Maschine falsch. Dennoch würden solche Systeme in manchen US-Gerichten angewendet. ""Das macht mich als Naturwissenschaftlerin wütend"", sagt Zweig. ""Beim Menschen lassen wir das doch auch nicht zu. Ich darf als Wissenschaftlerin auch nicht einfach irgendwelche wilden Hypothesen in den Raum werfen, die ich mit einmalig gefundenen Daten belegen kann, und daraus Entscheidungsregeln für die nächste Situation bilden.""

Die Informatikerin dämpft allzu große Erwartungen an KI. Das unterscheidet sie von einigen KI-Utopisten, deren Bücher und Vorträge sich vor allem darum drehen, was Maschinen schon alles können (oder vielleicht mal können werden) und wie diese irgendwann den Menschen überflüssig machen werden. Zweig dagegen bezeichnet algorithmische Entscheidungen sogar als ""entseelte Entscheidungen"" und warnt davor, eine starke KI zu bauen, also eine KI, die dem Menschen ebenbürtig oder gar überlegen ist. Neben einem KI-Visionär wie Jürgen Schmidhuber, der von KI-Zivilisationen spricht, die sich im Weltall ausbreiten werden, wirkt Zweig auf den ersten Blick wie eine konservative Mahnerin - oder wie ein Bilderbuch neben einem 3-D-Film. Doch Zweig warnt nicht generell vor KI. Bei der Qualitätskontrolle in Fabriken etwa könne sie sehr nützlich sein. ""Da kann die Maschine entscheiden: Die kaputte Schraube schmeiß ich vom Band, die heile bleibt drauf."" Dafür brauche es auch keine staatliche Kontrolle, die untersucht, nach welchen Kriterien die Maschine entscheidet. ""Aber wenn es um Menschen geht, um deren Zukunft, um Zugang zu gesellschaftlichen Ressourcen, dann müssen wir ran.""
Sie will nicht der Kompass sein, sie will zeigen, dass es einen braucht

Am Nachmittag steht Zweig in einer schmucklosen Aula der Hochschule München, vor ihr ein dankbares Publikum, das bei jeder Pointe lacht. Die Heiserkeit überspielt sie gekonnt, gut gelaunt hält sie ihren Vortrag und macht das, was sie ""diese Schaschlikspieß-Geschichte"" nennt: Die Zuhörer sollen mit Stäbchen eckige und runde Smileys, die durcheinander in einem Quadrat verteilt sind, möglichst exakt voneinander trennen. Die Leute werden selbst zum Algorithmus und merken bald: Man kann nie allen Smileys gerecht werden. Zweig bringt das Beispiel auch in ihrem Buch: Sollen lieber mehr Unschuldige in die Gruppe der möglichen Straftäter gesteckt werden oder umgekehrt? Sie will zeigen, dass eine scheinbar rationale Entscheidung im Grunde eine moralische ist.

Doch welche Moral wollen wir? Welchen Werten, welcher Ethik wollen wir folgen? Zweig klingt kaum mehr wie eine Informatikerin, als sie plötzlich von Hoffnung und Liebe, von Vergebung und Gnade spricht - Dingen, zu denen sie keine KI imstande sieht. Und Dinge, die ihr als gläubigem Menschen wichtig sind, anderen Menschen aber vielleicht nicht. Wer entscheidet, welche Werte die richtigen sind?

Man müsse zusammen demokratisch aushandeln, was die besten Regeln sind, sagt Zweig. Ähnlich allgemein bleibt sie bei der Frage im Vortrag. Doch das Publikum will es konkret wissen, etwa ob es unethisch ist, in Apps mithilfe von Tracking Tests durchzuführen, von denen der Nutzer nichts weiß. Zweig gibt keine eindeutige Antwort. Die will sie aber auch gar nicht geben. Sie will nicht der Kompass sein. Sondern höchstens die Expeditionsleiterin, die die losrennende Truppe darauf hinweist, dass es einen Kompass braucht. Deshalb hat sie auch im Februar zusammen mit einem Sozioinformatiker und ihrem Mann, der Erwachsenenpädagoge ist, das Start-up Trusted AI gegründet. Sie beraten Betriebsräte und Unternehmen bei der ethischen Anwendung und Entwicklung von algorithmischen Entscheidungssystemen. Das Motto: ""Wir helfen entscheiden.""

Nach dem Vortrag steht Zweig umringt von Menschen im Hörsaal, wird begrüßt, wird beglückwünscht, wird befragt. Könnte sie nicht vielleicht bei jener Veranstaltung sprechen, und gibt es irgendwo noch so ein Stäbchen-Set? Zweig muss eigentlich zum Zug, die Heiserkeit ist zurück, sie muss immer wieder die Stimme herbeiräuspern, da entfährt ihr ein: ""Ich kann einfach nicht mehr."" 300 Mails am Tag, zehn Redenanfragen in der Woche; sie hat jetzt eine Agentur, die ihre Vorträge organisiert und empfiehlt Kollegen, die an ihrer Stelle sprechen könnten. Denn irgendwann will sie wieder ""in ihren Elfenbeinturm zurückkehren"", wie sie sagt. Und ganz in Ruhe Daten wälzen.";https://www.sueddeutsche.de/wirtschaft/ki-algorithmen-google-amazon-zweig-interview-1.4717456;sz.de;Veronika Wulf
06.01.2020;Weißt du, was ich meine? ;"""Sind Sie bereit für das Meeting?"" - ""Wann fängt es noch mal an?"" - ""Bitte antworten Sie mit Ja oder Nein."" Hans van Dam kennt viele solcher Beispiele. Beispiele dafür, wie es nicht laufen sollte, wenn Menschen mit sprachgesteuerten Systemen kommunizieren. Bots nennen Fachleute diese Systeme, abgeleitet von Robot. Mit seiner Firma Robocopy bringt van Dam Unternehmen bei, wie sie es besser machen können.

Sein Geschäft läuft gut, denn mehr und mehr Unternehmen setzen auf Bots. Weil es eine Menge Geld spart, wie Yoav Barel sagt. Der Israeli ist der Gründer des Chatbot Summits, einer Fachmesse, die vor Kurzem in Berlin stattfand. Erstmals war dabei ein ganzer Tag nur Bots gewidmet, die per Stimme kommunizieren. Für die Unternehmen ist das vielversprechend. Bei einer Kundenanfrage kostet die Arbeitszeit 2,50 Euro, der gleiche Betrag fällt für die IT an, also etwa die Datenbanken mit all den Verträgen eines Kunden. Nimmt aber ein Bot das Gespräch an, sinken die Arbeitskosten auf weniger als 50 Cent pro Anruf. Bei 50 Millionen Kundenanfragen pro Monat wie beim Telekommunikationskonzern Vodafone kommen da schnell gewaltige Summen zusammen. 2021 wolle Vodafone 60 Prozent der Kundenanfragen automatisiert erledigen, sagt Barel. Einsparpotenzial: 35 Millionen Euro pro Monat.

Aber ziehen die Kunden da auch mit? Jeder kennt schließlich Horrorgeschichten wie die von Hans van Dam. Doch die Bots werden besser. Und: Angetrieben vom Internethändler Amazon hat sich eine neue Gerätekategorie etabliert - sprachgesteuerte Lautsprecher. Sprachsteuerung gab es zwar davor auch schon, etwa Apples Siri, vernetzte Lautsprecher ebenfalls. Die Kombination beider Technologien aber übt ganz offenbar einen großen Reiz auf viele Konsumenten aus, die damit herumspielen. Millionen Geräte sind bereits verkauft worden und lauschen nun in den Wohnungen der Besitzer auf ein Aufwachwort wie Alexa oder Hey Siri.

""Alexa! - Wie wird das Wetter morgen?"" - das ist eine der leichtesten Übungen für Geräte. Komplizierter wird es, wenn etwa ein Geschäft abgeschlossen werden soll. Denn dann gibt es Rückfragen und verschiedene Möglichkeiten für die Kunden, sich zu entscheiden. Dann geben Kleinigkeiten den Ausschlag, ob Kunden verärgert auflegen oder sich gerne etwa durch einen Bestellprozess führen lassen. Hans van Dam rät als Erstes: ""Lasse niemals Ingenieure den Text schreiben!""
Kreative und Techniker müssen von Anfang an zusammenarbeiten

Die Herangehensweise von früher, dass an einem Projekt zehn Ingenieure arbeiteten und ein Texter, der sich dann meist nur schlecht durchsetzen konnte, müsse dringend geändert werden, sagt er. Stattdessen müssten Kreative und Techniker von Anfang an zusammenarbeiten, damit die Bots auch angenommen werden. Van Dam sieht darin auch eine Chance für Geisteswissenschaftler, in einer von Technik dominierten Welt Arbeit zu finden.

Als zweite Regel nennt van Dam den Atemtest. Was man nicht in einem Atemzug sagen kann, sei zu lang. Gesprochene Ansagen dürften auch nicht zu viel Information enthalten. Und am besten schreibt man sie so, wie man sie auch im normalen Umgangston sagen würde. In seiner Firma werden daher neue Bots getestet, indem sich zwei Mitarbeiter Rücken an Rücken setzen, damit keine weiteren Informationen wie Gesten oder Mimik ausgetauscht werden können. Der eine spielt den Bot, der andere den menschlichen Nutzer. So wird das automatische System Wiederholung für Wiederholung optimiert.

Vorgehensweisen wie diese dürften Nikola Aschoff gefallen. Die junge Frau leitet bei Mercedes-Benz den Bereich erweiterte digitale Dienste: ""Wir denken, wir kennen unsere Kunden"", sagt sie, ""aber oft stimmen unsere Annahmen nicht. Da helfen voice snippets gewaltig."" Die kurzen Schnipsel an Text, die Kunden hinterlassen, wenn sie mit Bots gesprochen haben, zeigen nicht nur inhaltlich, was die Kunden eigentlich wollen. Aus ihnen lassen sich - auch mithilfe künstlicher Intelligenz - auch weitere Informationen gewinnen: Ist ein Kunde nervös, verärgert oder gut gelaunt? Welche Hintergrundgeräusche gibt es? ""Das wird in seiner Langzeitwirkung unterschätzt"", sagt Aschoff.

Das Problem ist: Die Anrufer müssen konsistent bedient werden, egal an welchen Kanal sie sich bei einem Unternehmen wenden. Yoav Barel fordert daher, es dürfe nur ein Gehirn geben, eine zentrale Stelle also, von der aus verschiedene Kanäle bedient werden können - Sprachbots, Chatroboter und die menschlichen Mitarbeiter. Doch so einfach ist das gar nicht, denn es hängt immer davon ab, in welchem Kontext eine Anfrage steht. ""Der Aufwand dafür wird unterschätzt"", sagt die Expertin Aschoff, ""das darf man nicht nur einem Werkstudenten überlassen."" Denn der gehe nach einiger Zeit wieder. Wichtig sei aber, dass sich jemand ständig kümmert. Die Bots müssten immer aktuell gehalten werden. Und die Inhalte müssten je nach Kanal auch auf andere Weise aufbereitet werden.
Sprachsteuerung: Die Technik ist da und die Akzeptanz wächst

Sprache als Mittel, um mit Maschinen zu kommunizieren, hält Aschoff aber für richtig - vor allem, weil sich die Technologie im Auto nutzen lässt und die Hände dennoch am Lenkrad bleiben können. Sprache zu nutzen, sei natürlich, die Technologie bereits da, und die Akzeptanz wachse. Und: 80 Prozent der Mercedes-Kunden rufen am liebsten an, um zum Beispiel einen Servicetermin zu buchen.

Mit einem ähnlichen Anruf - es ging um die Vereinbarung eines Friseurtermins - hat Google vor einigen Monaten bei einer Konferenz viel Aufsehen erregt. Denn die Frau im Friseursalon wusste nicht, dass sie mit einem Bot sprach, und merkte das auch im Verlauf des Gesprächs nicht. Damit, so glauben nicht wenige in der Branche, hat das Unternehmen der Sache allerdings einen Bärendienst erwiesen. Denn es sei schließlich Vertrauen nötig, sonst würden sich viele Menschen nicht gerne auf Bots einlassen.

Jeder Bot brauche eine eigene Persönlichkeit, glaubt Hans van Dam. Und Yoav Barel ist der Meinung, es müsse dafür gesorgt werden, dass die Menschen darüber informiert werden, wenn sie mit einem Bot sprechen. Grundsätzlich sieht er eine goldene Ära für Sprachtechnologie voraus. Es gebe heute schon Versicherungsanbieter, die mit KI-gesteuerten Bots Policen verkaufen und einfache Schadensansprüche regulieren. ""Der Rekord liegt bei drei Sekunden"", sagt er, dann sei die Überweisung schon auf dem Weg gewesen.

Bis 2025, glaubt er, würden Milliarden Menschen Dienste nutzen, die mit Sprache gesteuert werden, entweder per Stimmeingabe oder mit Text. Diese Bots hätten für Unternehmen einen enormen Wert. Die zugrundliegenden Techniken entwickelten sich ständig weiter. Und nicht jedes Unternehmen, das Bots einsetzen wolle, müsse die selbst programmieren.

Sie können sich an einen der Großen wenden, Amazon, Google und Microsoft etwa. Oder an Spezialunternehmen wie Audio Codes aus Israel, die bisher vor allem Technik für Callcenter geliefert haben. Die können dann auch spezielle Anwendungen wie Meeting Insights liefern. Ein Meeting wird dabei aufgezeichnet, und eine KI fasst die Redebeiträge der einzelnen Teilnehmer zusammen. Die US-Firma Nuance, einer der Pioniere bei Spracherkennung, setzt auf Systeme etwa fürs Auto, die ohne ein Aufwachwachwort wie Alexa erkennen sollen, wenn ein Mensch etwas von ihnen will. Und gut wäre, wenn die Antwort dann nicht lauten würde: ""Ich habe Sie nicht verstanden.""";https://www.sueddeutsche.de/digital/sprachsteuerung-zukunft-1.4744119;sz.de;Helmut Martin-Jung
06.01.2016;Sexy oder hässlich? Das entscheidet der Computer;"Das ist er dann wohl, der sprechende Spiegel des 21. Jahrhunderts: ein Attraktivitätsorakel im Internet, gesteuert von künstlicher Intelligenz. Die renommierte ETH Zürich hat zusammen mit der Schweizer Dating-App Blinq eine Seite ins Netz gestellt, die verspricht: ""Let Artificial Intelligence guess your attractiveness and age"" - ""Lassen Sie künstliche Intelligenz Ihre Attraktivität und Ihr Alter erraten"".

Wer sein Foto unter faces.ethz.ch hochlädt, lässt Geschlecht, Alter und Attraktivität errechnen - die Angaben gehen von ""Hmm.."" über ""Nice"" bis zu ""Stunning"" und ""Godlike"". Mehr als eine Million Menschen sollen das nach zwölf Stunden schon ausprobiert haben, heißt es in einer Mitteilung der ETH. Nicht verwunderlich, dass die Seite ""under heavy load"" steht und die eigene ""Hotness nicht verarbeiten"" kann, wie es in einer Pop-Up-Meldung heißt.

Derweil sammeln sich unter dem Hashtag #howhot die ersten Fotos von Menschen, die mehr Glück mit dem Server hatten. Doch nicht immer sind die ausgespuckten Ergebnisse schmeichelhaft: Bei dem Spaß übersieht man schnell, dass faces.ehz.com einen durchaus seriösen Hintergrund hat. Auf der Seite präsentiert das Computer Vision Lab der Züricher Hochschule die Ergebnisse von zwei Studien, die zeigen, wie genau die Gesichtserkennungs-Software funktioniert (pdf, pdf), die der Seite zugrunde liegen.

Bei Reddit werden die Nutzer jetzt den Kopf schütteln. In einem ebenso beliebten wie unterhaltsamen Thread lässt sich etwa nachlesen, wie ungenau die Sache doch sei. ""Ich habe mein Foto hochgeladen"", schreibt etwa ein Nutzer, ""und dann hieß es: 'Sorry, keine Haustiere.'"" Ein anderer meint: ""Mein Foto kann leider nicht analysiert werden, weil ich unter 18 Jahre alt sei. Dabei bin ich 32. Ist das jetzt ein unbeholfenes Kompliment oder will die Seite mich verführen?""

Und dann gibt es da noch die üblichen Reddit-Spaßvögel. (Bild1, Bild 2, Fake nicht ausgeschlossen). Die meisten Upvotes aber bekommt ein Eintrag von Vazuvizzi, der von seiner ganz eigenen Erfahrung berichtet: ""'Sorry, we couldn't detect a face.' Well that's a way of putting it :(""";https://www.sueddeutsche.de/digital/kuenstliche-intelligenz-sexy-oder-haesslich-das-entscheidet-der-computer-1.2807686;sz.de;THA
12.12.2015;So will Elon Musk künstliche Intelligenz bändigen;"Ausgerechnet die Elite des Silicon Valley hat Angst vor der Zukunft - zumindest ein Teil von ihr. Eine Gruppe um Tesla-Chef Elon Musk und die Investoren Peter Thiel (der gemeinsam mit Musk Paypal gründete) und Sam Altman macht sich Sorgen, dass künstliche Intelligenz (KI) der Menschheit langfristig schaden könnten. Die Entwicklungen in dem Forschungsfeld sind tatsächlich atemberaubend. Um herauszufinden, ob sie auch gefährlich sind, haben Musk und Co. nun das Forschungszentrum ""Open AI"" (AI steht für ""artificial intelligence"") eröffnet. Eine Milliarde Dollar stellen sie dafür bereit.
Die SZ-Redaktion hat diesen Artikel mit einem Inhalt von Twitter angereichert

Um Ihre Daten zu schützen, wurde er nicht ohne Ihre Zustimmung geladen.

Ich bin damit einverstanden, dass mir Inhalte von Twitter angezeigt werden. Damit werden personenbezogene Daten an den Betreiber des Portals zur Nutzungsanalyse übermittelt. Mehr Informationen und eine Widerrufsmöglichkeit finden Sie unter sz.de/datenschutz.

Was ist das Ziel von ""Open AI""?

In einem Blogbeitrag des Zentrums heißt es: ""Unser Ziel ist, digitale Intelligenz so zu erweitern, dass die Menschheit als Ganzes davon profitieren kann, unabhängig von finanziellen Interessen."" Es sei schwierig, vorherzusehen, wie nützlich oder schädlich KI sein wird. Genau deshalb brauche es eine Organisation, die sich nicht um Aktionäre und deren Sorgen kümmern müsse, sondern nur um die Forschung. ""Open AI"" ist eine Non-Profit-Organisation, die die nichtkommerzielle KI-Forschung bündeln soll.

In einem Interview auf der Blog-Plattform Medium schreibt Musk: ""Ich will ein tiefes Verständnis bekommen, wo wir uns in Sachen KI befinden und ob etwas gefährliches passieren könnte."" Er wolle sich alle ein bis zwei Wochen mit dem Team treffen und sich auf Stand bringen lassen.

Außer Musk, Thiel und Altman sind mehrere erfahrene KI-Forscher bei ""Open AI"" dabei. Yoshua Bengio beschäftigt sich seit Jahrzehnten mit dem Thema, Ilya Sutskever hat sich bei Google um KI gekümmert. KI-Forschung braucht enorme Rechenkraft. Dafür stellt Amazon Web Services, Tochter von Jeff Bezos' Konzern, ""Open AI"" Server zur Verfügung.

Was ist künstliche Intelligenz?

Der britische Forscher Alan Turing stellte bereits 1950 die entscheidende Frage: Können Maschinen denken? Seither suchen Forscher eine Antwort. Künstliche Intelligenz ist der Versuch, Maschinen zu bauen, die wie Menschen denken. Doch dafür muss erst einmal abschließend geklärt werden, wie Menschen lernen.

Seit Jahrzehnten wird versucht, Maschinen beizubringen, Menschen Denkaufgaben abzunehmen. Doch mangelnde Rechenkraft führte dazu, dass es lange nur wenig nennenswerte Fortschritte gab. Doch in den vergangenen Jahren hat sich das verändert. Die Rechner sind deutlich schneller und billiger, und die IT-Branche entwickelt die Technologie schnell weiter. Skype übersetzt Gespräche in Echtzeit, Instagram beschreibt Blinden, was auf Bildern zu sehen ist, Facebooks virtueller Assistent soll den Menschen eine Art Butler sein (braucht allerdings noch Live-Unterstützung von Facebook-Mitarbeitern).

KI-Forscher gehen davon aus, dass Maschinen in der Zukunft denken können wie Menschen - und dass das schon in wenigen Jahren passieren könnte. Für diesen Fall will ""Open AI"" gewappnet sein. Allerdings ist völlig unklar, wann Maschinen tatsächlich in einem Maße autonom denken können, das sie zu mehr befähigt, als Schachspiele oder Quizshows zu gewinnen. Schätzungen reichen von 2035 bis 2100. Viele Forscher halten sich mit Langfrist-Prognosen völlig zurück.

Warum wird das ""Open-AI""-Zentrum gegründet?

Während unter manchen Forschern und in Konzernen, die KI einsetzen, die Euphorie über die rasanten Fortschritte groß ist, haben sich mehrere bekannte IT-Unternehmer und Forscher kritisch geäußert. Elon Musk sagte schon vor längerem über künstliche Intelligenz: ""Wenn ich raten müsste, was unsere größte existenzielle Bedrohung ist, dann ist es vermutlich das"" (in diesem Video ab Minute 67:45)."" Microsoft-Gründer Bill Gates (""Ich verstehe nicht, warum manche Menschen nicht besorgt sind"") und Physiker Stephen Hawking (""Künstliche Intelligenz könnte das Ende der Menschheit bedeuten"") äußerten sich ähnlich.

Dutzende Forscher haben einen offenen Brief verfasst, in dem es heißt, KI müsst unbedingt ""wohlwollend"" sein. Forschungsergebnisse dürften nicht missbraucht werden, um beispielsweise autonome Waffensysteme zu bauen.

Maschinen, die schlauer sind als Menschen und gleichzeitig nicht ""wohlwollend"", das wäre ungünstig. Zumindest für die Menschen.

Aber Google, Facebook und weitere Unternehmen forschen doch schon zum Thema. Wozu braucht es ""Open AI""?

Nicht nur Google und Facebook arbeiten intensiv an der Technologie, auch viele Investoren pumpen Geld in Start-ups, die sich mit KI befassen. Waren es 2010 noch 15 Millionen Dollar, sind es 2014 schon 309 Millionen. Aus Musks Sicht ist das problematisch, diese Firmen seien an wirtschaftlichen Interessen ausgerichtet. Allerdings lässt sich dem entgegnen, dass Firmen wie Facebook und Google Teile ihrer Forschung bereits heute öffentlich verfügbar machen (und betonen, dass diese Forschung nur dann möglich ist, wenn sie öffentlich nachgeprüft werden kann). Abgesehen davon sind auch Musk und Thiel nicht mit Menschenfreundlichkeit steinreich geworden, sondern indem sie in die vielversprechendsten Zukunftstechnologien investiert haben - so wie heute künstliche Intelligenz eine ist. Und ausgestattet mit ihrem Geld tritt ""Open AI"" auch in den Wettbewerb mit Facebook und Google um die besten Forscher.";https://www.sueddeutsche.de/digital/open-ai-so-will-elon-musk-kuenstliche-intelligenz-baendigen-1.2780064;sz.de;Hakan Tanriverdi
01.01.2017;Big Data: Wie die IT-Industrie die Privatsphäre aushöhlt;"In der mathematischen Welt der Zwanzigerjahre des vorigen Jahrhunderts wurde bewiesen, dass die Vorgänge in der physikalischen ebenso wie der biologischen Welt nur in eng definierten Ausnahmefällen mit Rechenmaschinen, auch Turingmaschinen genannt, berechnet werden können; es wurden die sogenannten ""unberechenbaren mathematischen Funktionen entdeckt"", die offenbar in unermesslicher Vielfalt existieren.

Herausragende Mathematiker wie Kurt Gödel haben damals der heutigen Erkenntnis eine Bahn gebrochen, dass Rechenmaschinen genau das nicht leisten können, was Menschen tagtäglich leisten; gemeint ist die kreative Bewältigung, Formulierung und Lösung von prinzipiell unberechenbaren Problemen und Erscheinungen, für deren Lösung beziehungsweise Verstehen es also keinerlei algorithmische, das heißt rezeptartige Regeln gibt. Wir erinnern auch an Joseph Beuys, der die Kreativität in jeder menschlichen Handlung erkannte und darauf seinen Kunstbegriff gründete.

Rechenmaschinen dagegen sind nicht kreativ, sie sind nicht fähig, intuitiv schöpferisch zu denken, sie können die Welt nicht in begrifflicher Sprache beziehungsweise begrifflichem Text beschreiben und keine Wissenschaft entwickeln. Rechner können menschliches Verhalten durchaus beobachten, kopieren und eventuell statistisch nachahmen bis hin zur Bildung von vernünftig erscheinenden sprachlichen Sätzen. Die Annahme aber, dass androide Roboter eines Tages den Menschen ersetzen oder gar die Macht ergreifen könnten, ist heute so unbegründet wie vor 100 Jahren. Geistiger Rohstoff

Die Ursprünge menschlicher Kreativität liegen unbestreitbar in jenem intimsten Bereich der seelischen und geistigen Existenz eines Menschen, den man Privatsphäre nennt. Das Wesen der Privatheit erschließt sich, wenn man feststellt, wozu Maschinen nicht fähig sind und was allein der Mensch zu produzieren in der Lage ist. Der Mensch produziert private Entscheidungsdaten, die in kreativer Weise mit seinem Verhalten neu erschaffen werden. Der Mensch setzt damit geistigen Rohstoff in die Welt, der nicht von noch so leistungsfähigen Robotern produziert werden kann. Dieser geistige Rohstoff ist die größte und unversiegbare Quelle von Reichtum, des einzigen und wirklichen Reichtums, der allen Menschen von Natur aus als natürliches Eigentum mitgegeben ist.

Um die Nutzung dieses Rohstoffes ist ein Kampf entbrannt. Man ist an koloniale Zeiten erinnert, in denen der unerschöpflich scheinende Reichtum an Rohstoffen Afrikas mit Zustimmung der dortigen Landesfürsten von fremden Mächten ausgebeutet wurde, ohne dass die dortige Bevölkerung an den Erträgen teilhaben konnte. Europa ist bei der Nutzung des Rohstoffes der privaten Daten dabei, das Afrika des Informationszeitalters zu werden. Es wurde eine Enteignung in Gang gesetzt, die medial und ernst zu nehmend als Ende der Privatheit prognostiziert wird. Experten der Informationsindustrie diskreditieren das Recht auf individuelle Privatsphäre als ""Auslaufmodell"" mit dem Ziel, den geistigen Rohstoff, der von Menschen produziert wird, wenigen global agierenden Konzernen zu übereignen. Die politisch unterstützte Strategie der IT-Industrie ist es, im gesamten Umfeld jedes Menschen Datenaufnahmegeräte zu installieren, die sein gesamtes Verhalten digitalisieren und die Daten in Richtung Datensammelzentren ins Internet einspeisen. Selbst die harmlosesten Gebrauchsgegenstände etwa zur Morgentoilette, in der Küche oder im Kinderzimmer werden mit einem Anschluss ans Internet ausgestattet. Wir befinden uns in der Phase des flächendeckenden Aufbaues der sogenannten ""Welt der Dinge"", in der Big-Data-Unternehmen die von Menschen produzierten Werte massenhaft abschöpfen. Die Kontrolle über Gebrauchsgegenstände bis hin zu Privatautos wird dem Besitzer entzogen. Er lebt zunehmend in einer geliehenen Umgebung, deren Funktionalität fremdbestimmt bleibt.

Tatsächlich aber ist zumindest ein ""Ende des Privaten"" nicht in Sicht. Im Gegenteil. Riesige Mengen des ""geistigen Rohstoffes"" werden durch massenhafte Abschöpfung von Verhaltens- und Entscheidungsdaten privatisiert. Andererseits setzt bekanntlich die neoliberale Ideologie auf die Privatisierung bislang öffentlichen Eigentums und will öffentliche Dienstleistungen auch im universitären Bereich privatisieren. Insbesondere ist das durch freie Wissenschaften entwickelte öffentliche Wissen Ziel von Privatisierung, wie Colin Crouch in seinem im Jahr 2015 erschienenen Buch gezeigt hat.

Die Behauptung, dass Privatheit ein Auslaufmodell wäre, ist eine absichtliche Täuschung. Sie besteht darin, dass das eingesammelte fremde Eigentum an privaten Daten nicht etwa der Allgemeinheit zur Verfügung gestellt, sondern neuen Eignern zugeordnet wird. In Wahrheit wird Privateigentum massiv zentralisiert und schon gar nicht abgeschafft. Die neuen Besitzer schützen ihre Besitzstände vor Öffentlichkeit. Die Privatsphäre hochvermögender Kreise und Konzerne wird sehr wohl verteidigt. Unter den Augen maßgeblicher Politiker geschieht eine Umverteilung von Ressourcen, die der Bevölkerung nicht bewusst ist. Und die Bayerische Staatsministerin Ilse Aigner ist der Ansicht, dass die Leute ihre privaten Daten sowieso freiwillig abliefern würden, und fragt sich, was das Problematische beim Abgreifen der Daten sei. Alles in Ordnung?

Nein, keineswegs! Die Privatsphäre ist der Raum, in dem Kreativität und der Wille zur wirtschaftlichen Entfaltung produktiv werden kann. Der Schutz dieses Raumes ist die Voraussetzung der Teilhabe einer breiten Bevölkerungsschicht an einer funktionierenden sozialen Marktwirtschaft. Falls die Privatsphäre von Personen aufgelöst wird, gibt es keine eigenständige wirtschaftliche Entwicklung mehr. Ein Gemeinwesen wird so in der Wurzel zerstört.

Die Privatsphäre ist nicht nur der Raum privater wirtschaftlicher Entfaltung, sondern auch der Raum einer seelischen und sozialen Entfaltung und Gesunderhaltung. Das Bewusstsein, ständig von unbekannten Institutionen beobachtet zu werden und gleichsam in einem Panoptikum zu leben, verändert die Verhaltensweisen von Menschen so gravierend, dass man das Ende der sozialen und kooperativen Gesellschaft voraussagen kann. Der exzessive Zugriff auf die komplette Privatsphäre einer großen Mehrheit von Staatsbürgern einerseits und die Verwundbarkeit unserer staatlichen Einrichtungen durch Spionage, Sabotage und Kriminalität andererseits sind Seiten ein und derselben Medaille. Unser Grundgesetz verpflichtet uns, die individuelle Privatsphäre und mithin die wichtigste Außengrenze, die es gibt, zu schützen.";https://www.sueddeutsche.de/digital/aussenansicht-die-wichtigste-grenze-1.3316714;sz.de;Werner Meixner
16.12.2018;Wie künstliche Intelligenz das Leben verbessern soll;"Es geht jetzt hoch nach oben, immer weiter nach oben, in einem Aufzug aus Glas. Die Stadt liegt schon im Dunkeln. Man sieht die Lichter der Autos, man sieht den Stau am Mittleren Ring, und wenn man dann im 23. Stock aussteigt, steht eine Frau vor einem Bildschirm und sagt: ""Diese Karte von München zeigt, wo in der nächsten Stunde eine erhöhte Wahrscheinlichkeit für einen Unfall besteht.""

Die Autofahrer wissen von nichts, diese Frau aber hat schon eine Ahnung davon, was in der Zukunft passieren könnte - und das beschreibt ganz gut, was in diesem Turm im Norden von München gerade vor sich geht. Der Konzern IBM hat dort sein erstes Hauptquartier außerhalb der USA eröffnet, am Empfang unten steht Watson IoT. Watson nennt IBM sein Computerprogramm für künstliche Intelligenz und in München geht es vor allem um das Internet of Things - also um die Vernetzung verschiedener kluger Systeme. Es ist einer von Dutzenden Orten in der Stadt, an denen momentan an künstlicher Intelligenz gearbeitet wird. Diese Art von Software soll einmal bessere Entscheidungen treffen als der Mensch und in manchen Fällen können Programme das heute schon. Die neue Technologie wird nicht nur die Wirtschaft verändern, sondern das Leben aller Menschen; sie wird in einer Reihe stehen mit der Erfindung der Dampfmaschine oder des Telefons. Künstliche Intelligenz, kurz KI, wird verändern, wie wir arbeiten, wie wir miteinander sprechen und wie wir uns fortbewegen - und wie schnell das geht, wird sich nicht nur in den USA entscheiden, sondern auch in Deutschland. Auch in München.

Der bayerische Ministerpräsident Markus Söder hatte vor ein paar Monaten angekündigt, 280 Millionen Euro in die Forschung von KI zu investieren. Erst Anfang des Jahres hat BMW ein Forschungszentrum zum autonomen Fahren eröffnet, auch das ist nur möglich mit künstlicher Intelligenz. Im Frühjahr kam der frühere Chef von Google in die Stadt, Eric Schmidt, um zu verkünden, dass Google und die Technische Universität München (TUM) bei dem Thema von nun an eng zusammenarbeiten wollen. In seinen Büros am Arnulfpark arbeitet das Unternehmen an KI, wie auch Microsoft und Infineon, wie Siemens und Linde, wie letztlich Unternehmen aller Branchen.

IBM hat in seinen neuen Büros deshalb eine Art Showroom eingerichtet. Dort sollen die Kunden sehen können, was mit KI alles möglich ist - der Konzern will seine Software schließlich verkaufen. Die Managerin Heike Kammerer führt dann durch die Räume, vorbei an einem Roboter, der innerhalb von 30 Millisekunden erkennen kann, wenn ein Türgriff aus der Produktion verkratzt ist. Vorbei an Mülltonnen mit Sensoren, die erst geleert werden müssen, wenn sie auch wirklich voll sind. Vorbei an Bildschirmen, auf denen Sozialdienste ablesen können, in welcher Wohnung gerade die Badewanne volläuft und in welcher die Haustüre offen steht. Und dann schließlich zu der Karte von München, auf der man erkennen kann, an welchen Orten die Gefahr für einen Unfall in der nächsten Stunde besonders hoch ist.

Die Polizei habe ihnen Unfalldaten aus den Jahren 2014, 2015 und 2016 überlassen, sagt Kammerer. Diese Daten habe IBM dann mit anderen Informationen aus der gleichen Zeit verbunden, mit Daten zum Wetter, zum Verkehr und zu Events in der Stadt. Die Karte sei nur ein Beispiel, um zu zeigen, was möglich ist - aber sie lässt erahnen, wie die Zukunft der Städte einmal aussehen könnte. Wenn man einen der Mitarbeiter bei IBM fragt, warum er Software-Architekt geworden ist, formuliert Tobias Stöckel es so: ""Ich fand spannend, wie sich die IT mittlerweile in jeden Lebensbereich vorwagt."" Man könnte das auch anders nennen - wie sie sich in jeden Lebensbereich vordrängt. Wenn man wissen will, wie wichtig künstliche Intelligenz in der Wirtschaft gerade wird, muss man nur einmal versuchen, einen Termin mit Andreas Liebl auszumachen. Er leitet an der UnternehmerTUM, dem Gründerzentrum der TUM, eine Initiative für künstliche Intelligenz. Eben das Programm, in das auch Google investiert hat. Liebl hat in diesen Tagen viel zu tun, zu seinen Partnern gehören unter anderem Allianz und Telekom, viele Unternehmen mit einem bekannten Namen. Wenn man am Morgen das Gebäude der UnternehmerTUM betritt, sitzen die ersten schon auf den Sofas, die Laptops auf den Knien. Im Regal sind die Namen der Firmen zu lesen, deren Geschichte auch in Garching begann. Flixbus zum Beispiel oder Tado. Im ersten Stock macht sich Andreas Liebl den ersten Kaffee des Tages, und im Zimmer gegenüber hängt ein Schild an der Türe, auf dem steht: ""Achtung, sensible Daten! Dieser Raum ist vorübergehend gesperrt!"". Liebl lächelt dann nur. Was genau in dem Raum passiert, darf er nicht sagen. Nur, dass dort die wichtigsten Daten einer Firma lagern - und alleine der Rechner 70 000 Euro wert ist.

Viele Unternehmen haben jetzt mit KI begonnen, aber Liebl geht das noch zu langsam. Die meisten Firmen verbesserten erst kleine Dinge, etwa die Kratzer in den Türgriffen, dabei könnte man doch die ganze Produktion neu denken. Die Kühlung der Rechenzentren von Google zum Beispiel steuere jetzt eine künstliche Intelligenz, sagt Liebl. Mit der habe man 40 Prozent Energie eingespart und solche Zahlen schweben ihm auch für deutsche Firmen vor. Liebl und seine knapp 30 Mitarbeiter begleiten Firmen und entwickeln Prototypen mit ihnen, schulen auch Mitarbeiter. Sie wollen das Wissen teilen. Sie veröffentlichen viel im Internet, wollen eine Debatte in der Gesellschaft anstoßen. ""Wenn die Menschen Angst vor einer Superintelligenz in der Zukunft haben, verkennen sie Chancen, die KI jetzt bietet"", sagt Liebl.

Vor ein paar Wochen ging es im Münchner Tatort um solch eine Superintelligenz, um eine Software, die der Mensch nicht mehr unter Kontrolle hat. Die Kommissare Leitmayr und Batic mussten in einem Mordfall dann einen Computer an Stelle eines menschlichen Zeugen befragen. Andreas Liebl hat sich den Tatort nicht angesehen. Einer seiner Mitarbeiter schon. Er sagt, man müsse doch nur mit Siri oder mit Alexa reden, um zu verstehen, dass man von der Superintelligenz noch weit entfernt sei. Wie er den Tatort fand? ""Esoterisch.""";https://www.sueddeutsche.de/muenchen/kuenstliche-intelligenz-forschung-1.4253903;sz.de;Pia Ratzesberger
08.12.2018;Der gläserne Mensch wird durch KI noch transparenter;"Am Thema künstliche Intelligenz (KI) scheiden sich die Geister. Die einen befürworten eine intensive Nutzung von KI, darunter zum Beispiel der Facebook-Gründer Marc Zuckerberg. Er propagiert neue gesellschaftlich wünschenswerte Anwendungen wie etwa den Einsatz in der medizinischen Diagnose und Therapie. Eine große Zahl von Beratungsfirmen baut KI-Abteilungen auf und verdient damit viel Geld.

Der US-amerikanische Wirtschaftswissenschaftler Erik Brynjolfsson vom Massachusetts Institute of Technology bezeichnet künstliche Intelligenz als die bedeutendste ""General Purpose Technology"" (Basistechnologie) unserer Zeit. Die unzähligen Möglichkeiten, KI im Alltag oder branchenübergreifend in Unternehmen anzuwenden, scheinen ihm recht zu geben. Hinzu kommt, dass die Algorithmen viele Jobs in der Regel sehr gut erledigen: Sie analysieren auf Basis von CT-Bildern Krankheiten, sagen Aktienkurse vorher, erkennen Gesichter oder verhindern Cyberangriffe. Auf der anderen Seite gibt es viele Kritiker, Tesla-Chef Elon Musk etwa, und auch das verstorbene Physik-Genie Stephen Hawking gehörte dazu. Die Befürchtung: KI könnte sich eines Tages selbst verbessern, sodass in einem sich verstärkenden Kreislauf eine Superintelligenz entsteht - ein Intellekt also, der dem Menschen in allen Bereichen überlegen ist und ihn beherrscht. Zukunftsforscher Ray Kurzweil ist angeblich sogar in der Lage, ein Datum zu nennen, wann dies passieren wird: im Jahr 2045. Ob die Prognose seriös ist, darf bezweifelt werden.
Algorithmen sind bisher nur sehr beschränkt einsetzbar

Algorithmen oder Roboter, die der Menschheit überlegen sind, sind seit Jahrzehnten ein beliebtes Thema und Gegenstand von Dystopien oder Science-Fiction-Filmen wie ""Alien"", ""Blade Runner"" oder ""Terminator"". Herbert A. Simon, einer der Väter der KI, sagte schon 1965 voraus, dass es nur noch zwanzig Jahre dauern werde, bis Maschinen in der Lage seien, jede Arbeit zu erledigen, die bislang Menschen vorbehalten war. Der Status quo der KI-Anwendung sieht dagegen so aus: Algorithmen sind darauf spezialisiert, bestimmte Probleme zu lösen; darin sind sie kaum zu schlagen, aber kein Algorithmus würde auf die Idee kommen, sein Anwendungsgebiet zu erweitern. Es gibt auch keine erfolgversprechenden Ansätze, die in der Lage wären, eine solche Superintelligenz mit eigenem Bewusstsein zu entwickeln. Ist also alles in bester Ordnung? Können wir sorglos in eine Zukunft blicken, in der KI uns bei vielen Tätigkeiten und Entscheidungen unterstützt und unsere Gesellschaft dadurch verbessert? So einfach ist es leider nicht. Wir müssen beachten, dass es sich bei den meisten KI-Algorithmen um ""Black Boxes"" handelt. Sie geben häufig nicht preis, warum sie wie entschieden haben. Das mag in manchen Fällen unproblematisch sein, in vielen anderen ist es das aber nicht.

In einigen Unternehmen sind heute schon KI-Lösungen bei der Auswahl von Personal im Einsatz. Arbeitet der Algorithmus nach dem Black-Box-Prinzip, können wir die Auswahlentscheidung nicht erklären. Zudem wissen wir nicht, ob der Algorithmus Parameter wie Geschlecht, Hautfarbe oder Religion in seine Entscheidung einbezogen hat. Wollen wir solche Algorithmen? Die Antwort muss heißen: nein. Ein weiterer kritischer Punkt ist das Thema Privatsphäre. KI-Algorithmen können relativ gut Daten verknüpfen und Personen zuordnen. Diese Algorithmen und Technologien könnten Wegbereiter für mehr Überwachung durch Staaten oder Firmen sein. So sind in China bereits erste Modelle im Einsatz, die Menschen anhand ihres Online- und Offline-Verhaltens bewerten. Dieses ""Scoring"" wird für Entscheidungen über die Vergabe von Krediten, Ausreisegenehmigungen oder den Zugang zum Internet herangezogen. Der sprichwörtliche gläserne Mensch wird durch KI noch transparenter. Auch hier lautet die Antwort auf die Frage, ob wir eine solche Gesellschaft wollen: nein.

Was folgt daraus? Ich denke nicht, dass wir KI-Forschung verteufeln und darauf verzichten sollten. Im Gegenteil, ein großes Problem für Europa und Deutschland besteht darin, dass wir im Bereich der künstlichen Intelligenz ins Hintertreffen geraten sind - wie bei anderen Digitalisierungsthemen auch. Die Investitionen in KI-Start-ups sind überwiegend auf zwei Länder verteilt: 48 Prozent dieser Investitionen entfielen im vergangenen Jahr auf China, weitere 38 Prozent auf den langjährigen Spitzenreiter USA. Der Rest der Welt ist weit abgeschlagen. Für Deutschland, eigentlich die Heimat vieler herausragender KI-Forscher, reicht es selbst in Europa nicht zum Spitzenplatz. Den hat Großbritannien inne, wo Start-ups wie Darktrace und Graphcore inzwischen den sogenannten Unicorn-Status erreicht haben, also mit mehr als einer Milliarde Dollar bewertet sind.
Interdsiziplinäre Forschung soll uns auf das KI-Zeitalter vorbereiten

Insofern gehen die Schritte von Europäischer Union und Bundesregierung in die richtige Richtung, mit Investitionen diesen Rückstand aufzuholen. Allerdings dürfen wir dabei nicht die Augen vor gesellschaftlich relevanten Themen wie Transparenz und Privatsphäre verschließen. Vor diesem Hintergrund erscheint insbesondere die Förderung von interdisziplinären Forschungsprojekten wünschenswert. Nur solche fächerübergreifenden Programme sind in der Lage, die potenziellen Folgen für Gesellschaft und Wirtschaft umfassend zu analysieren und soziologisch, historisch oder philosophisch gewonnene Erkenntnisse in die technologischen Entwicklungen zu integrieren.

Darüber hinaus besteht eine wichtige praktische Aufgabe darin, mehr Bewusstsein für die aktuellen Herausforderungen der KI zu schaffen. In einer aktuellen Studie zur KI-Nutzung im Personalbereich zeigte sich, dass vielen Verantwortlichen nicht bewusst war, dass sie mit einer Black Box arbeiten, die aufgrund des Algorithmus oder der Trainingsdaten rassistische oder sexistische Entscheidungen treffen könnte. Den Entscheidern ging es primär darum, Kosten und Zeit einzusparen. Erfreulicherweise gibt es aber bereits Ansätze in der Forschung, die sich um Transparenz bemühen, damit die Entscheidungen der KI für die Menschen nachvollziehbar werden. Intensive Aufklärung könnte den Anstoß dazu geben, dass KI-Nutzer zukünftig auf transparente und faire Algorithmen setzen.

Eine große Zahl von Menschen ist mittlerweile bereit, für Fair-Trade-Produkte einen hohen Preis zu zahlen. Vielleicht lässt sich diese Erfolgsgeschichte auch auf KI-Algorithmen übertragen. Deren Fairness könnte für die Anbieter und für die europäische Wirtschaft insgesamt zu einem Wettbewerbsvorteil werden.";https://www.sueddeutsche.de/digital/kuenstliche-intelligenz-algorithmen-ethik-1.4235816;sz.de;Peter Buxmann
19.12.2018;Was, wenn bald ein Roboter meinen Job macht?;"Nach der Kabinettssitzung am Mittwoch hatte es Bundesarbeitsminister Hubertus Heil (SPD) nicht weit zum nächsten Termin: Vom Kanzleramt hinüber zum Berliner Hauptbahnhof sind es nicht einmal 500 Meter. Dort ließ sich Heil von Auszubildenden der Deutschen Bahn erklären, wie sie zu ihrem Beruf gekommen sind, wie die Bahn durch Weiterbildungs- und Chancenprogramme Jugendlichen Perspektiven bietet. Der Minister erfuhr auch, dass der Konzern schon vor fünf Jahren einen Qualifizierungsfonds über 250 000 Euro aufgelegt hat, um Mitarbeitern ein berufsbegleitendes Studium zu ermöglichen. Das passte gut in Heils Programm, hatte das Kabinett doch gut eine Stunde zuvor einem Gesetzesvorhaben aus seinem Haus zugestimmt: dem Qualifizierungschancengesetz.

Was steckt hinter dem Wortungetüm? Und was haben die Arbeitnehmer hierzulande davon? Die wichtigsten Fragen und Antworten.
Worum geht es?

Angesichts der Digitalisierung und des damit verbundenen Strukturwandels treibt viele Arbeitnehmer die Frage um, ob es ihren Job und vielleicht sogar ihren ganzen Beruf in ein paar Jahren noch geben wird. Und was sie tun müssen, um in einer Arbeitswelt mit intelligenten Robotern, selbstfahrenden Gabelstaplern und Lastern, denkenden Maschinen und vernetzten Produktionssystemen bestehen zu können. Heil setzt auf Qualifizierung und Weiterbildung. Sein Gesetzentwurf, der nun in die Bundestagsberatung gehen wird, sieht deshalb einen erleichterten Zugang zu Weiterbildungsangeboten für Beschäftigte vor und eine großzügigere Förderung von Unternehmen, die ihre Mitarbeiter schulen.
Was ist konkret vorgesehen?

Künftig soll die Weiterbildung von deutlich mehr Arbeitnehmern mit Geld von der Bundesagentur für Arbeit (BA) gefördert werden können - und zwar ""unabhängig von Ausbildung, Lebensalter und Betriebsgröße"", wie es im Gesetzentwurf heißt. Förderkriterium ist nur noch, dass der eigene Job vom Strukturwandel betroffen ist oder gar überflüssig werden könnte. Es geht Heil dabei vor allem um jene berufliche Tätigkeiten, ""die durch Technologien ersetzt werden können"". Ebenfalls förderfähig sind Weiterbildungen für Berufe, in denen der Bedarf an Personal größer ist als das Angebot, beispielsweise Pflegekräfte oder Erzieher. Außerdem soll die Nürnberger BA künftig auch in größerem Umfang Zuschüsse zum Arbeitsentgelt zahlen, wenn ein Mitarbeiter für eine Weiterbildung abwesend ist. Für Mitarbeiter stiegen damit im besten Fall die Erfolgsaussichten, wenn sie beim Chef um eine Weiterbildung bitten - denn die Kosten für die Firma sind im Zweifel geringer.
Sind die Arbeitgeber fein raus?

Das ist in der Tat ein wunder Punkt von Heils Gesetzesvorhaben. Denn die Wirtschaft investiert schon jetzt Milliarden in Weiterbildung; es besteht also die Gefahr von Mitnahmeeffekten. Allerdings ist in dem Entwurf vorgesehen, dass eine Weiterbildung nur dann gefördert wird, wenn sich auch der Arbeitgeber an den Kosten beteiligt. Allerdings gibt es bei dem Thema je nach Betrieb große Unterschiede. Während viele Konzerne ihren Mitarbeitern diverse eigene Programme anbieten, haben kleinere Firmen dazu oft nicht die Kapazitäten. Die Bundesvereinigung der Arbeitgeberverbände beziffert die jährlichen Weiterbildungsausgaben der Unternehmen auf 33 Milliarden Euro. Das sei mehr als der gesamte Haushalt der BA.
Was heißt das für Arbeitnehmer?

Geplant ist auch, die Beratung rund um das Thema Weiterbildung durch die BA auszuweiten. Sie soll sich nicht nur an Arbeitslose richten oder an jene, die von Arbeitslosigkeit bedroht sind, sondern an alle Arbeitnehmer. Sie sollen abklären können, welchen Qualifizierungsbedarf sie haben und welche Optionen.

Von dem Gesetz profitieren sollen zudem Arbeitnehmer, die nur projektweise angestellt sind und dazwischen immer wieder Phasen der Arbeitslosigkeit haben. Sie sollen leichter Zugang zum Arbeitslosengeld I haben. Bislang musste man innerhalb von zwei Jahren zwölf Monate lang in die Arbeitslosenversicherung eingezahlt haben, um diese Leistung zu bekommen; in Zukunft reichen zwölf Monate in zweieinhalb Jahren.
Was kostet das Gesetzespaket?

Laut Entwurf werden die Ausweitung der Arbeitsförderung und der erleichterte Zugang zum Arbeitslosengeld I die BA - und damit die Beitragszahler, also alle sozialversicherungspflichtig beschäftigten Arbeitnehmer und ihre Arbeitgeber - mittelfristig etwa 1,1 Milliarden Euro im Jahr kosten. Los geht es 2019 mit 680 Millionen Euro, danach wird mit steigenden Kosten gerechnet. Allerdings werden die Beitragszahler auch entlastet, durch die Senkung des Beitragssatzes zur Arbeitslosenversicherung um dauerhaft 0,4 Prozentpunkte und um weitere 0,1 Prozentpunkte bis Ende 2022. Für die BA bedeutet das dauerhaft um bis zu 5,1 Milliarden Euro geringere Einnahmen jährlich, bis Ende 2022 kommen 1,2 Milliarden Euro hinzu.
Was sagt die Wirtschaft?

Heils Vorschläge seien ""in ihrer Zielsetzung richtig"", sagte Arbeitgeberpräsident Ingo Kramer. Allerdings verwies er auf die geplante Nationale Weiterbildungsstrategie, die Union und SPD sich eigentlich vorgenommen haben. Was Heil vorhabe, müsse darin eingebettet werden. ""Schließlich ist die BA nicht der einzige und bei Weitem auch nicht der wichtigste Akteur auf diesem Feld."" Außerdem warnen die Arbeitgeber vor Mitnahme- und Verdrängungseffekten und davor, dass das Geld ""mit der Gießkanne"" verteilt werden könnte. Lieber wäre es ihnen deshalb, die BA würde sich mit ihren Angeboten auf Geringqualifizierte, Ältere und Mitarbeiter in mittelständischen Unternehmen mit höchstens 2000 Beschäftigten konzentrieren.";https://www.sueddeutsche.de/karriere/weiterbildung-was-wenn-bald-ein-roboter-meinen-job-macht-1.4136096;sz.de;Henrike Roßbach
25.04.2019;Moral für Maschinen;"Wenn 23 Wissenschaftler von führenden Universitäten in Europa und Amerika - Harvard, Stanford, MIT, Max-Planck-Institut und so weiter - sowie aus Forschungsinstituten der Digitalkonzerne Google, Facebook und Microsoft im Alarmton ein neues Forschungsfeld einfordern, sollte man das sicher ernst nehmen. ""Machine Behaviour"", so lautet der schlichte Titel der Veröffentlichung, die an diesem Donnerstag im führenden Wissenschaftsjournal Nature erscheint. Darin wird erläutert, dass man künstliche Intelligenzen nur mit einer Verhaltensforschung für Maschinen verstehen und so programmieren könne, dass sie den Menschen dienen und nicht schaden.

Nun sind akademische Epauletten gerade in der digitalen Welt ein veraltetes Kriterium für wissenschaftliche Glaubwürdigkeit. Aufschlussreicher ist es, wenn man den Hauptautor des Aufsatzes besucht, den KI-Forscher, Ethiker, Spieltheoretiker und Kognitionsforscher Iyad Rahwan, dessen Biografie mindestens so komplex ist wie das Spektrum seiner Expertisen. Der 41-jährige Wissenschaftler, der sehr viel jünger wirkt, ist im syrischen Aleppo und in den Emiraten aufgewachsen, hat seinen Doktor in Australien gemacht und in Dubai und in Masdar City als Dozent gearbeitet. Im kommenden Sommer wird er Direktor des Forschungsbereichs ""Mensch und Maschine"" am Max-Planck-Institut für Bildungsforschung in Berlin. Derzeit leitet Iyad Rahwan noch eine Forschungsgruppe am Massachusetts Institute of Technology (MIT) im amerikanischen Cambridge. Sein Büro liegt im großartigen E14-Gebäude des MIT Media Labs, das der japanische Pritzkerpreisträger Fumihiko Maki entworfen hat, einem lichtdurchfluteten Glasbau mit weiträumigen Computerlaboratorien. Man sucht sich im vierten Stock seinen Weg durch das Telmex Lab, ein Labyrinth aus Arbeitsflächen, Rechnern, Peripheriegeräten und Kabelbäumen, um die klischeegerecht Comicfiguren und Cola-Dosen drapiert sind. ""Hinter dem gelben Schirm und dem orangenen Sofa"" arbeitet Iyad Rahwan in einem Einzelbüro mit (Chefprivileg) Fenster und Sitzgruppe.
Das Testen der ""Moral Machine"" ist ein unangenehmer Höllenritt in die Abgründe der eigenen Seele

Die Arbeit, mit der er bekannt wurde und die den Ruf nach einer Verhaltensforschung für Maschinen so dringlich macht, ist seine ""Moral Machine"", die er hier entwickelte. Die Studie sollte herausfinden, nach welchen ethischen Grundlagen Menschen in verschiedenen Weltregionen selbstfahrende Autos programmieren würden, deren künstliche Intelligenz in bestimmten Situationen entscheiden muss, wen das Fahrzeug tötet. Passagiere, Passanten, und wenn, dann welche? Eher alte? Eher junge? Ist das Überqueren der Straße bei Rot ein implizites Todesurteil? Auch bei Kleinkindern? Das sind grundlegende Moralfragen, die jetzt geklärt werden müssen, weil sich Maschinen in Zukunft danach verhalten werden. Die Studie wurde im vergangenen Herbst eigentlich abgeschlossen, aber im MIT-Museum und im Netz (moralmachine.mit.edu) kann man die Moralmaschine immer noch bedienen. Das Ding ist ein unangenehmer Höllenritt in die Abgründe der eigenen Seele, was vor allem daran liegt, dass Iyad Rahwan die Fragen gemeinsam mit dem Psychologen Azim Shariff und dem Kognitionsforscher Jean-François Bonnefon nach dem Paradigma der erzwungenen Wahl formulierte. Dieses beruht auf dem Gedankenexperiment des Weichenstellerfalls, das seit Beginn des 20. Jahrhunderts durch die Wissenschaft geistert. Es geht dabei um das Dilemma eines Weichenstellers, der den Unfall eines Zuges mit vielen Toten verhindern könnte, wenn er ihn auf ein Gleis umleitet, auf dem weniger Menschen ums Leben kämen. Die Szenarien der ""Moral Machine"" gehen davon aus, dass die Bremsen eines autonom lenkenden Fahrzeuges vor einem Fußgängerüberweg versagen und nun entschieden werden muss, ob man das Fahrzeug je nach Fall in eine Personengruppe, in eine Mauer oder in eine von zwei Personengruppen lenkt. 40 Millionen Antworten von rund vier Millionen Teilnehmern aus 233 Ländern und Territorien haben Rahwan und sein Team ausgewertet. Die ""Moral Machine"" ist damit die bisher umfassendste Studie zu Maschinenethik.

Auf den ersten Blick sind die Ergebnisse gar nicht so überraschend. Prinzipiell riskieren die Probanden lieber weniger als mehr Leben, lieber Alte als Kinder. Es sind aber die feinen regionalen Unterschiede, die zeigen, dass es keine eindeutig universellen Werte gibt, die als Grundlage für eine Maschinenethik taugen. Die Ergebnisse sind grob in drei Weltregionen unterteilt - die westliche Welt mit Europa und Amerika, der östliche Cluster mit asiatischen und islamischen, sowie die Südregion mit lateinamerikanischen und ein paar frankofonen Ländern. ""Die Ergebnisse decken sich weitgehend mit den vorgefassten Meinungen, die wir über verschiedene Kulturen haben"", sagt Rahwan. ""In Ländern mit ausgeprägtem Rechtsbewusstsein wie Deutschland überfahren die Leute eher mal jemanden, der bei Rot über die Straße geht. Kinder wollen alle schützen, nur in asiatischen Ländern mit hohem Traditionsbewusstsein und kollektivistischen Gesellschaften, wo der Respekt vor Älteren stärker ausgeprägt ist, nicht ganz so stark."" In den Leistungsgesellschaften des Westens verschonen die Probanden Ärzte und Geschäftsleute eher als Obdachlose. In Südamerika werden Frauen stärker geschützt.
Es geht hier nicht um Science Fiction, sondern um die Gegenwart

Maschinenethik ist längst keine Science-Fiction mehr. Fahrzeuge im oberen Preissegment verfügen jetzt schon über halbautonome Fähigkeiten, sei es die Einparkautomatik oder den Autopiloten für Staus. Die Ergebnisse der ""Moral Machine""-Studie widersprechen jetzt schon ersten Leitlinien: ""Deutschland war eines der ersten Länder, das Richtlinien für selbstfahrende Fahrzeuge eingeführt hat. Diese verbieten eine Unterscheidung von Personen auf Grund ihres Geschlechts, ihres ethnischen Hintergrundes und dergleichen. Die starke Präferenz von Kindern zum Beispiel, weil man sie nicht im selben Maß für Fehler beim Überqueren einer Straße verantwortlich machen kann wie Erwachsene, widerspricht den deutschen Richtlinien."" Iyad Rahwan will nur Daten anbieten, keine moralischen Antworten. ""Meine Rolle ist nicht, Gesellschaften zu raten, wie sie die Dilemmata auflösen. Als Wissenschaftler bringe ich Fakten in die Debatte ein. Mehr nicht."" Aber gerade weil diese Fakten so viele moralische Fragen aufwerfen, und gerade weil das Gedankenexperiment des Weichenstellers so viele Faktoren des wirklichen Lebens außer Acht lässt, wird es nicht reichen, solche Fragen in der Theorie ohne die Praxis der technischen Entwicklungen zu wälzen. Und umgekehrt. Womit Iyad Rahwan bei der Verhaltensforschung für Maschinen angelangt ist.
Die nächste große Herausforderung der KI-Forschung ist die Politik

Prinzipiell muss man davon ausgehen, dass Maschinen eigene Verhaltensmuster entwickeln, die sich weder eindeutig programmieren noch schlüssig aus der traditionellen Verhaltensforschung ableiten lassen. ""Maschinen zeigen Verhaltensweisen, die sich fundamental von denen der Menschen und Tiere unterscheiden"", heißt es da. Unzählige Faktoren spielen eine Rolle: der Quellcode, die Lerneffekte künstlicher Intelligenzen, der Auftrag, die Reaktion der Menschen.

Die Dringlichkeit solcher Forschungen steht gleich in der Präambel: ""Maschinen, die von künstlicher Intelligenz getrieben werden, vermitteln zunehmend unsere sozialen, kulturellen, wirtschaftlichen und politischen Interaktionen."" Algorithmen steuern ja nicht nur das Verhalten von Autos. Sie verändern jetzt schon das Liebesleben von Millionen durch Dating-Apps, sie verwalten das Zuhause der Menschen genauso wie ihre Finanzen, ihren Nachrichtenfluss und die öffentliche Debatte. Maschinen werden vermehrt Kinder aufziehen und Alte pflegen. Und in den Steuerelementen autonomer Waffen werden sie in bewaffneten Konflikten darüber entscheiden, wen sie leben und sterben lassen.

Die momentane Forschung kümmere sich aber nur ad hoc um die Auswirkungen der Algorithmen auf Menschen und Gesellschaft, moniert der Nature-Aufsatz. Vor allem aber seien Menschen, die sich mit künstlicher Intelligenz beschäftigen, vor allem Mathematiker und Ingenieure, keine Verhaltensforscher, Psychologen und Soziologen. ""Derzeit sind die Wissenschaftler, die das Verhalten von Maschinen untersuchen, dieselben Wissenschaftler, welche die KIs geschaffen haben. Diese konzentrieren sich aber vor allem auf die Funktionstüchtigkeit ihrer künstlichen Intelligenzen.""

Leicht wird es nicht, dies zu ändern. Die meisten KIs sind Black Boxes, deren Algorithmen das Betriebsgeheimnis der Firmen sind, die sie entwickeln und verkaufen. Das kann ihre Erforschung schon mal juristisch erschweren. Dabei ist die Komplexität von Algorithmen schon diffizil genug. Da tun sich nicht nur in ethischen Einzelfragen Widersprüche auf. ""Die nächste Herausforderung wird die Politik sein"", sagt Iyad Rahwan. ""Denn künstliche Intelligenz ist Politik.""";https://www.sueddeutsche.de/kultur/wissenschaftlicher-aufruf-moral-fuer-maschinen-1.4419702;sz.de;Andrian Kreye
10.07.2020;Wenn Obama plötzlich ein Weißer ist;"Yann LeCun galt als eifriger Twitter-Nutzer. Der renommierte KI-Forscher setzte mehrmals täglich seine Tweets ab. Auch an einem Sonntag Ende Juni schrieb LeCun dort über Forschungsergebnisse, riss einen Witz und kommentierte einen Tweet, der Diskriminierung durch Algorithmen thematisiert hatte. Zu diesem Zeitpunkt ahnte er nicht, dass dies einer seiner letzten Tweets sein sollte. Yann LeCun, KI-Chef von Facebook und Gewinner des berühmten Turing Awards, hat Twitter verlassen. Sein Tweet sorgte für einen heftigen Streit um rassistische Diskriminierung durch maschinelles Lernen, einem Teilgebiet der künstlichen Intelligenz. Es ging um die Frage, woran es liegt, dass künstliche Intelligenz manchmal zu rassistischer Intelligenz wird. Um die Tragweite von LeCuns Rückzug aus Twitter zu verstehen, muss man zwei Dinge wissen. Erstens ist Twitter für viele Informatiker, besonders für junge KI-Forscher, nicht irgendein soziales Medium, sondern ein zentraler Ort des fachlichen Austauschs. Wer Twitter den Rücken kehrt, kehrt einem Großteil der KI-Community den Rücken. Zweitens ist Yann LeCun nicht irgendein KI-Forscher; er hat in den 1990er-Jahren die Convolutional Neural Networks (CNNs) erdacht, die sein Fachgebiet gewaltig umkrempelten. Wenn heute von der Übermacht künstlicher Intelligenz die Rede ist, wenn Chinas Regierung jeden Bürger auf der Straße erkennt und man beim Fast-Food-Restaurant mit dem eigenen Gesicht bezahlen kann, dann liegt das an LeCuns damaligem Durchbruch; Bild- und Teile der Spracherkennung in ihrer heutigen Form wären ohne LeCuns Arbeit undenkbar. Dafür wurde LeCun unter anderem mit dem Turing Award geehrt, der höchsten Auszeichnung in der Informatik.

LeCun hatte auf zwei Bilder reagiert: Links ist ein beinahe bis zur Unkenntlichkeit verpixeltes Bild des ehemaligen US-Präsidenten Barack Obama zu sehen, rechts ein hochaufgelöstes Bild einer weißen Person, die ein bisschen wie Obama aussieht. Die Person auf dem rechten Bild ist nicht real, ein neuronales Netz hat das Bild erzeugt. Ziel der Software war es, Obamas Pixel-Bild eine höhere Auflösung zu verpassen. Solche Upsampling-Algorithmen sind derzeit schwer angesagt; sie können unscharfe Bilder aufpolieren, bis diese gestochen scharf sind. Nur: Sie verändern so wichtige Dinge wie die Hautfarbe. Es sind beileibe nicht die einzigen Modelle im maschinellen Lernen, die Menschen diskriminieren. Immer wieder hört man von Algorithmen, die Menschen afrikanischer oder asiatischer Abstammung nicht erkennen, übersehen oder als kriminell einschätzen.
Der Algorithmus lernte von 50 000 Beispielbildern, die meist weiße Menschen zeigten

Wie im maschinellen Lernen üblich, wurde auch der Upsampling-Algorithmus mit Daten aus der realen Welt trainiert, in diesem Fall mehr als 50 000 Porträtfotos. Das Modell sieht - stark vereinfacht gesprochen - jedes dieser Bilder mal gestochen scharf, mal stark verpixelt und lernt daraus, wie es von unscharfen Bildern auf das Aussehen der abgebildeten Menschen schließen kann. LeCun schrieb dazu: ""Systeme des maschinellen Lernens sind verzerrt, wenn die Daten verzerrt sind."" Er wies darauf hin, dass der Algorithmus vornehmlich mit Bildern hellhäutiger Menschen trainiert wurde - und deshalb hinter Obamas verpixeltem Bild einen ""weißen Obama"" vermutete. ""Man trainiere exakt dasselbe System auf einem Datensatz aus dem Senegal, und jeder wird afrikanisch aussehen."" Daraufhin brach ein Sturm der Entrüstung los. ""Man kann solche Fehler nicht einfach nur auf verzerrte Daten zurückführen"", schrieb Timnit Gebru, Expertin für ethische Fragen im Bereich künstlicher Intelligenz. Sie richtete sich direkt an LeCun: ""Hör einmal auf uns Menschen aus marginalisierten Bevölkerungsgruppen. Wenn nicht jetzt, während der weltweiten Proteste, wann dann?"" Gebru bezog sich auf die Demonstrationen nach dem Tod des Amerikaners George Floyd.

LeCun reagierte mit einer ausführlichen Erklärung in 17 Tweets. Er betonte, dass es ihm lediglich um diesen einen Algorithmus ging. Keinesfalls seien immer nur die Daten schuld, wenn KI rassistische Entscheidungen treffe; die gesamte ""Architektur eines Modells"" könne Vorurteile enthalten. Damit meint LeCun, dass Menschen die Programme schreiben, die mithilfe von Daten lernen. Ihre Vorurteile geben sie dabei nicht selten an die Algorithmen weiter. Im Falle des ""weißen Obamas"" seien nun mal aber die Profilbilder, also die Trainingsdaten, schuld gewesen. Gebru widersprach auf Twitter nicht. Ihr ging es wohl darum, dass diese Sicht nicht falsch, aber verkürzt ist. In ihren Veröffentlichungen und Seminaren weist Gebru immer wieder darauf hin: Auch Daten wie die Profilbilder fallen nicht vom Himmel; sie sind Ausdruck gesellschaftlicher Werte und politischer Machtverhältnisse. ""Hinzu kommt, dass die Architektur eines Modells Verzerrungen in den Daten noch verstärken kann"", sagt Isabel Valera, Professorin für maschinelles Lernen an der Universität des Saarlands. Valera erforscht, wie Algorithmen gerechte Entscheidungen treffen können. Dazu übersetzt sie menschliche Vorstellungen von Fairness in mathematische Formeln und letztlich in Computercode - keine leichte Aufgabe: ""Es ist möglich, ein einzelnes Konzept von Fairness zu erfüllen."" Unmöglich sei es jedoch, sämtliche Normen zu berücksichtigen.
Computermodelle funktionieren im Grunde wie Wanderkarten. Sie vereinfachen die Realität

Das liegt daran, dass Computermodelle im Grunde wie Wanderkarten funktionieren: Sie vereinfachen die Realität, pressen mehrere Quadratkilometer auf eine Holztafel. Dabei findet nicht jeder Trampelpfad seinen Weg auf die Karte, Information geht verloren. Ein Modell, das alle Menschen und Bedürfnisse berücksichtigt, wäre am Ende so hilfreich wie eine Wanderkarte im Maßstab 1:1. Algorithmen werden also unfair bleiben, KI-Forscher können aber diese Schwächen deutlich stärker wahrnehmen und darauf hinweisen. Erst neulich schrieben über 2000 Wissenschaftler einen offenen Brief an einen Verlag mit der Bitte, eine Publikation über eine Software, die angeblich Straftaten vorhersagt, nicht zu veröffentlichen. Sie befürchteten ein rassistisches Ungleichgewicht im Code. Zu den Unterzeichnern gehörte Yann LeCun.";https://www.sueddeutsche.de/wissen/ki-rassismus-obama-1.4962340;sz.de;Julian Rodemann
;;;;sz.de;
;;;;sz.de;
;;;;sz.de;
;;;;sz.de;
;;;;sz.de;
;;;;sz.de;
;;;;sz.de;
;;;;sz.de;
;;;;sz.de;
;;;;sz.de;
;;;;sz.de;
;;;;sz.de;
;;;;sz.de;
;;;;sz.de;
;;;;sz.de;
;;;;sz.de;
;;;;sz.de;
;;;;sz.de;
;;;;sz.de;
;;;;sz.de;
;;;;sz.de;
;;;;sz.de;
;;;;sz.de;
;;;;sz.de;
;;;;sz.de;
;;;;sz.de;
;;;;sz.de;
;;;;sz.de;
;;;;sz.de;
;;;;sz.de;
;;;;sz.de;
;;;;sz.de;
;;;;sz.de;
